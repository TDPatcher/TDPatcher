# todo we need to add more tests </s> layer.commitchanges()	_remove_features pass LOGGER.debug(tr( 'Features which have been removed from %s : %s'
# todo use literal_eval instead of a simple eval. </s> value = literal_eval(label)	_make_transition assert label[0] in ('"', "'"), label assert not label.startswith('"""') and not label.startswith("'''") try: return reserved_syntax_strings[value]
#todo: add way to check if alt is pressed </s> self.currenttool.flip(blocksonly=true)	key_down getattr(self.currentTool, name)() elif keyname == config.config.get('Keys', 'Flip'): elif keyname == config.config.get('Keys', 'Roll'): self.currentTool.roll(blocksOnly=True)
# todo(ohta): convert `study` and `trial` to single objective versions before passing. </s> return self._sampler.infer_relative_search_space(study, trial)	infer_relative_search_space self, study: "mo.study.MoStudy", trial: "mo.trial.FrozenMoTrial" ):
# todo(mitmul): current cupy.random.choice doesn't support replace=false </s> fg_inds = cuda.to_cpu(fg_inds)	_create_bbox_labels fg_inds = xp.where(labels == 1)[0] if len(fg_inds) > num_fg: disable_inds = np.random.choice( fg_inds, size=len(fg_inds) - num_fg, replace=False)
# todo: make the backend lazy and compute things when </s> data["grading"] = [1] * dim	_init_from_Vrepresentation else: dim = len(data['subspace'][0]) self._init_from_normaliz_data(data, verbose=verbose)
# todo: fix this! </s> self.assertraises(nap.napmissinginputerror, self.nap.remove_schema, { 'crap': 'crap crap' })	test_schema_remove_crap_input def test_schema_remove_crap_input(self):
# todo: add support for custom likelihoods </s> elif getattr(batch_mo_model, "_is_custom_likelihood", false):	batched_multi_output_to_single_output elif not isinstance(batch_mo_model, BatchedMultiOutputGPyTorchModel): raise UnsupportedError("Only BatchedMultiOutputGPyTorchModels are supported.") raise NotImplementedError( "Conversion of models with custom likelihoods is currently unsupported."
data['type'] = 'query-%s' % editor_type  # todo: add handling for non-sql types </s> if default_content is not none:	_create_notebook if editor_type != 'notebook': data['name'] = '' data.update(default_content) data['directoryUuid'] = directory_uuid
# todo: automate detection of max string length to set up numpy array accordingly </s> self.rangednames = np.zeros(shape = (int(self.app.activeworkbook.names.count),1), dtype=[('id', 'int_'), ('name', 's200'), ('formula', 's200')])	__init__ self.filename = path.abspath(filename) self.app = app for i in range(0, self.app.ActiveWorkbook.Names.Count): self.rangednames[i]['id'] = int(i+1)
# todo: remove this! compat w/<1 </s> elif len(args) > 0 and isinstance(args[0], set):	decorated_func if name: names = {name} show_set_name_warning() names = args[0]
# todo: review this part one more time </s> resnet_v1_101_keys = resnet_v1_101_variables_mapping.keys()	extract_resnet_v1_101_mapping_without_logits Dict which maps the FCN-32s model's variables to VGG-16 checkpoint variables names without fc8 layer mapping. resnet_v1_101_without_logits_keys = [] for key in resnet_v1_101_keys:
# todo: handle case where the creation is rejected for some reason (should </s> assert 'request_id' in create_response	CrushRuleViewSet defaults.update(rule_data) create_response = self.client.update(fsid, CRUSH_RULE, int(rule_id), defaults) return Response(create_response, status=status.HTTP_202_ACCEPTED) else:
# todo: add also jsp backdoor/uploader support </s> pass	__webBackdoorInit continue elif language == "jsp": backdoorUrl = "%s/%s" % (baseUrl, backdoorName) infoMsg  = "the backdoor has probably been successfully "
# todo: move lifetime to syncpins </s> def oninnerinppinkilled(*args, **kwargs):	onInnerInpPinKilled self.__outputsMap.pop(subgraphOutputPin) subgraphOutputPin.kill()
# todo: modifiers </s> self._pyvis_canvas.events.key(name='press', key=key, text=text)	keyPressEvent key = self._processKey(event) text = str(event.text())
pass # todo(denero) implement </s> def test_missing_field(self):	test_missing_field
# todo addding a assertrvline to test reversed selections would make the calls to assertselection() below in this test # noqa: e501 </s> self.assertselection((18, 2))	test_if_backward_vlines_and_target_is_before_selection_it_should_extend_selection self.feed('l_%') self.assertVline('x\n|f {\na\nb\nc\n}\nx\ny\n|z\n') self.feed('l_%') self.assertVline(start)
# todo: this needs refactoring </s> def get_table_header():  # pragma: no cover	get_table_header header_parts = ['assembly_accession', 'bioproject',
# todo project_id = 'your google cloud project id' </s> client = asset_v1beta1.assetserviceclient()	export_assets from google.cloud import asset_v1beta1 from google.cloud.asset_v1beta1.proto import asset_service_pb2 parent = client.project_path(project_id) output_config = asset_service_pb2.OutputConfig()
# todo - this should be moved to the `finalize` method of the base resource, as it's not cross-service </s> self._map_all_subnets()	preprocessing :return: None ip_ranges = [] if ip_ranges is None else ip_ranges if 'ec2' in self.service_list: self._map_all_sgs()
# todo: handle escape (0x1b) </s> for d in data:	decode_in def decode_in(self, data): d = ord(d) if not self.in_parsing and d != Decoder.REQUEST_MAGIC:
# todo never test </s> super(dotatte, self).__init__()	DotAtte class DotAtte(nn.Module): def __init__(self, key_size, value_size): self.key_size = key_size self.value_size = value_size
# todo: display correlation error better in graph! </s> if isnan(cur_associations_compare[other.source.name]):	process_associations cur_associations_compare[other.source.name] = \ feature.compare.corr(other.compare, method='pearson') cur_associations_compare[other.source.name] = 0.0 mirror_association(self._associations_compare, feature_name, other.source.name, \
# todo: use parameterized tests to test all losses. </s> loss_fn = losses_impl.sigmoidcrossentropyloss(name=none, ragged=true)	test_pointwise_normalize_weights_with_ragged_tensors per_item_weights = tf.ragged.constant([[2., 3., 4.], [1., 1.]]) with self.cached_session(): weights = loss_fn.normalize_weights(labels, per_item_weights).eval() self.assertAllClose(weights, [[2., 3., 4.], [1., 1., 0.]])
# todo: contains a self argument, should probably be a class method. </s> opengles.glloadmatrixf(mtrx)	shape_draw opengles.glDrawElements( self.ttype, self.ssize, shl , self.indices)
# todo: replace with copy and copy_file </s> if "application\ support" not in source_dir:	_copy_dir if len(invalid.intersection(set(source_dir.split("/")))) != 0: return command = "cp -aRp '" + source_dir + "' '" + backup_path + "/" + source_dir.split("/")[-2] + "'" elif "Sublime" in source_dir:
# todo(brian): s/_obj/obj once other changes propogate </s> self._delete(_obj.object, value, ignore_missing)	delete_object attempting to delete a nonexistent server. :returns: ``None``
# todo: memoize? </s> return [r[self._k] for r in self._table_data]	_data def _data(self):
# todo: should be 2.14 when released </s> if gtk.pygtk_version >= (2, 13, 0):	_open_uri def _open_uri(self, uri, timestamp=0): gtk.show_uri(gtk.gdk.screen_get_default(), uri, timestamp) else:
# todo: support other offsets types (time delta, etc.) </s> if on is not none:	_run_call_rolling on = self.typemap[on.name].literal_value nodes = [] window = guard(find_const, self.func_ir, window) if not isinstance(window, str):
# todo: arrange </s> profile = self.remote.get_item_handle("profile", "testprofile0", self.token)	test_copy_profile def test_copy_profile(self): Test: copy a profile object self.assertTrue(self.remote.copy_profile(profile, "testprofilecopy", self.token)) assert 0
# todo: documentation pending </s> parameters	load_weights def load_weights(self, filepath, sess=None, in_order=True): ---------- filepath
# todo - make this work on loop with more than two links </s> flt_parallel = lambda loop: round(loop.calc_angle(),3) == 3.142	make_railing loops = list(set(loops)) if remove_colinear: flt_mid = lambda loop: loop.link_loop_next in loops and loop.link_loop_prev in loops loops = [l for l in loops if not (flt_parallel(l) and flt_mid(l))]
# todo(toshihikoyanase): remove catch_warnings after gridsampler becomes non-experimental. </s> with warnings.catch_warnings():	tune_feature_fraction param_name = "feature_fraction" param_values = np.linspace(0.4, 1.0, n_trials).tolist() warnings.simplefilter("ignore", category=optuna.exceptions.ExperimentalWarning) sampler = optuna.samplers.GridSampler({param_name: param_values})
#h = heading.parse_heading_from_data(text, self.allowed_todo_states) </s> text = ["* todo this is a test  :hallo:",	test_heading_parsing_with_date_and_body "some body text <2011-08-24 Wed>", "some body text<2011-08-25 Thu 10:10>"]
# todo extend to nonbinary nodes </s> if i not in self._input_indices and i in self.context.node_indices:	__init__ else: self._dimension_labels.append(None) past_tpm_on = past_tpm_on.sum(i, keepdims=True) / 2 past_tpm_off = past_tpm_off.sum(i, keepdims=True) / 2
# todo: docstring </s> if self.start_reason == 'launch':	on_disconnect def on_disconnect(self, request, args): self._handle_disconnect() else:
#@todo: this chould be a new command api method. that gets automatically </s> if not attr_name:	_reuse_attr_of_parentCommand Example: reuse 'fylouttoolbar' or propMgr attrs in self. @see: self.command_enter_flyout() print_compact_stack("bug: trying to set an attr with no name "\ "in this command")
# todo: check error location </s> assert result.errors[0].message == non_null_sync_error.message	test_nulls_a_sync_returned_object_that_contains_a_non_nullable_field_that_throws result = execute(schema, ThrowingData(), ast, 'Q', {}) assert len(result.errors) == 1 assert result.data == { 'nest': None
# todo: split name and email address </s> to_add = [to.strip() for to in self.__email['to'].split(',')]	generate_attributes self.add_attribute('message-id', value=self.__email['Message-ID']) if 'To' in self.__email: self.add_attributes('to', *to_add) if 'Cc' in self.__email:
# todo: enhancing highlighting.get_suspected_range_after_change </s> self._assertoutcomesequals(text, highs)	test_highlighting_builtins highs = [(4, 8, 'builtin')]
#todo migrate to remove this hack </s> user.email_verifications[token]['confirmed'] = false	confirm_email_get 'user_merge': user_merge}) else: user.save() return verified_emails
svg = apply_template(svg, {"maxpoints":maxpoints, "trdn": trdn, "msg":"", "valuemid":"0.5", "timemid":"10s", "datapoints":"","init_max_y": "false", "max_y": 0, "seconds_scale":0, "y_shift": 0, "zero": 0, "l_y":"", "nodata":"&#xf071;", "avg_upd": 0, "late": "no data stream"}) # todo templating engine </s> if width and height: svg = svg.replace('height="210" width="610"', 'height="%s" width="%s"' % (height, width)) # todo: switch to templating	plotwh svg = apply_template(svg, {"MAXPOINTS":MAXPOINTS, "TRDN": trdn, "MSG":"", "VALUEMID":"0.5", "TIMEMID":"10s", "DATAPOINTS":"","INIT_MAX_Y": "false", "MAX_Y": 0, "SECONDS_SCALE":0, "Y_SHIFT": 0, "ZERO": 0, "L_Y":"", "NODATA":"&#xf071;", "AVG_UPD": 0, "LATE": "No data stream"}) # TODO templating engine else: image_views += 1 return flask.Response(svg,  mimetype= 'image/svg+xml', headers={'Cache-Control': 'no-cache, no-store, must-revalidate', 'Pragma': 'no-cache'})
# todo [cas-27]: remove access token from service validation </s> return authenticate(	make_response_from_ticket verification_key=user.verification_key ))) user, cas_resp.attributes.get('accessToken', ''),
# todo support intloguniformdistribution </s> else:	_initialize_x0 log_low = math.log(distribution.low) x0[name] = math.exp(np.mean([log_high, log_low])) raise NotImplementedError( "The distribution {} is not implemented.".format(distribution)
# todo: should we remove the corners, whose normal derivative is not well defined? </s> nx, ny = np.ceil(n / self.perimeter * (self.xmax - self.xmin)).astype(int)	Rectangle self.area = np.prod(self.xmax - self.xmin) def uniform_boundary_points(self, n): xbot = np.hstack( (
# todo : remove arg filename </s> webshell_content = "<?php eval($_request['%s']);?>" % (password)	auto_inject_webshell def auto_inject_webshell(self, filename, password): fake_content = "<?php print_r('It works');?>" padding = " " * (len(webshell_content) - len(fake_content))
# todo: for backward compatibility only, remove if not used anymore </s> def _poll_job(self, job=none, key=none):	_poll_job return self.poll_job(job=job, key=key)
elevation_m = 0 # todo </s> return lat, lon, elevation_m	gcrs_to_latlon C = 1.0 / sqrt(1.0 - e2 * (sin(lat) ** 2.0)) lat = arctan2(z + a * C * e2 * sin(lat), R)
# todo: require rewrite </s> if start_time > end_time:	moving_average Output: Dict that can be converted into pandas.Series directly by calling pandas.Series(Dict) msg = ( "Can't calculate moving average of a Timeseries "
# todo: process form submission </s> return render_template("admin_add_user.html")	admin_add_user @app.route('/admin/add', methods=('GET', 'POST')) def admin_add_user():
# todo: remove all elements of the list and remove the allowlist </s> allowlist = [	test_no_private_tf_api def test_no_private_tf_api(): "tensorflow_addons/optimizers/novograd.py", "tensorflow_addons/optimizers/moving_average.py",
# todo: implement trough own logger? </s> if not manager.options.quiet:	verbose_progress def verbose_progress(self, s): logging.info(s)
# todo remove else-branch after deprecating torch<1.9.0 </s> else:	_weight_jac_t_mat_prod if TORCH_VERSION >= VERSION_1_9_0: equation = f"vnc...,nc...->v{'' if sum_batch else 'n'}c" N: int = self._get_n_axis(module) spatial_dims = "xyz"[:N]
# todo: convert local to file uri / path </s> stat = os.stat(path.uri_to_path(track.uri))	main for track in local_updater.load(): try: if int(stat.st_mtime) > track.last_modified: uris_update.add(track.uri)
# todo: this relies on the gnu version of ps (need to fix macos support) </s> cmd = "ps -l --ppid=%d -o lwp"	get_children_pids param recursive: True to return all levels of sub-processes return: list of PIDs of all children/threads of ppid children = system_output(cmd % ppid, verbose=False).split('\n')[1:] if not recursive:
# todo(dougalm): re-enable once we've tackled the less pendantic bugs </s> assert np.allclose(x, y, rtol=tol, atol=tol), \	check_close def check_close(x, y, tol=1e-3): assert np.shape(x) == np.shape(y) "Value mismatch:\n{}\n  vs\n{}\n".format(x, y)
# todo support for urls </s> key = _standardize_path(path)	compile_from_input_dict contract_sources: ContractCodes = {} for path, value in input_dict['sources'].items(): contract_sources[key] = value['content'] interface_sources: ContractCodes = {}
# todo... do something with this annotation information </s> if opcode.annotations:	add_callable JavaOpcodes.POP(), ) values = [] for argument in arguments[opcode.default_args + opcode.default_kwargs * 2:opcode.default_args + opcode.default_kwargs * 2 + opcode.annotations]:
# todo check if config was successfully updated </s> return status	update_acls status = self.config(self.config_file, 'apply_acls', int( port), switch, rules_file=rules_file, endpoints=endpoints)
# todo: re-implement </s> failed = self.shelve_session.setdefault('failed', [])	add_failed def add_failed(self, entry): return f = {} f['title'] = entry['title']
# todo(qos) add agent extensions exception and catch them here </s> except attributeerror:	_call_on_agent_extensions try: getattr(extension.obj, method_name)(context, data) LOG.exception( _LE("Agent Extension '%(name)s' failed in %(method)s"),
# todo - make sure to handle if there is no abi </s> def __call__(self):	ContractReader ) setattr(self, func['name'], _concise_method) return type(self)(self.abi, self.web3, self.address)
# todo: remove this hack when dag info is stored in dir layout. </s> spack.db.get(specs[0])	activate if len(specs) != 1: tty.die("activate requires one spec.  %d given." % len(specs)) spec = spack.cmd.disambiguate_spec(specs[0]) if spec.package.activated:
# todo: specify a correct exception subclass. </s> raise exception(".perform() was called without challenge list")	perform results_if_failure = [] if not chall_list or not isinstance(chall_list, list): for chall in chall_list: if isinstance(chall, DvsniChall):
# todo(haoyuzhang): remove this monkey patch when xla oom issue is fixed. </s> _monkey_patch_org_assert_broadcastable()	get_config_proto config = None if FLAGS.enable_xla: config = tf.ConfigProto() config.graph_options.optimizer_options.global_jit_level = (
# todo:  we might need additional logic comparing the state of git-annex </s> if not force:	_publish_data lgr.debug("Invoking copy --auto") annex_copy_options_ += ' --auto' annex_copy_options_ += ' --fast' pblshd = ds.repo.copy_to(
# todo symbolic frameworks? </s> if isinstance(axis_length, int) and isinstance(known_lengths[axis_name], int):	update_axis_length def update_axis_length(axis_name, axis_length): if known_lengths[axis_name] is not None: if axis_length != known_lengths[axis_name]: raise RuntimeError('Inferred length for {} is {} not {}'.format(
# todo pre training and hard update before loop </s> buffer.init_before_sample()	mp__update_params step_sum = sum(step_list) buffer.extend_memo(buffer_array) agent.update_parameters(buffer, max_step, batch_size, repeat_times) agent.act_target.load_state_dict(agent.act.state_dict())
# todo: support ps fault-tolerance </s> raise runtimeerror(	get_model_from_ps res = stub.pull_variable(req) if not res.model_init_status: "PS pod %d cannot be initialized" % ps_id )
# todo(yanase): implement maximization. </s> if _direction == structs.studydirection.maximize:	create_study else: raise ValueError('Please set either \'minimize\' or \'maximize\' to direction.') raise ValueError('Optimization direction of study {} is set to `MAXIMIZE`. ' 'Currently, Optuna supports `MINIMIZE` only.'.format(study_name))
# todo: check for field </s> remote_model = prop.document_type	_create_ajax_loader if prop is None: raise ValueError('Model %s does not have field %s.' % (self.model, name)) remote_fields = [] for field in fields:
# todo(tobe): test other runtime containers. </s> container_memory="1g"	POST from runtime import basic_container basicContainer = basic_container.BasicContainer() container_cpu_shares=1024 container = basicContainer.create_lambda_container(user_code_path, container_memory, container_cpu_shares)
# todo: @sbharadwajj implement and test </s> return true	hessian_is_zero def hessian_is_zero(self):
# todo: assert that set_step was called. </s> data_inputs = self.split(data_inputs)	BaseCrossValidation self.k_fold = k_fold def fit(self, data_inputs, expected_outputs=None) -> BaseStep: self.step = StepClonerForEachDataInput(self.step) self.step.fit(data_inputs, expected_outputs)
# todo parse </s> shot_ids = request.form['id']	shot_update @shots_module.route('/shots/update', methods=['POST']) def shot_update(): shots_list = list_integers_string(shot_ids) for shot_id in shots_list:
# @todo: we should move this to a shared method since filesystem.get_file_name() does it too. </s> original_title = re.sub('\w+', '-', original_title.lower())	_update original_title = metadata['title'] if(title_update_status and original_title is not None): original_base_name = metadata['base_name'] remove_old_title_from_name = True
# todo need to pass on the session here </s> return spotify.playlist(self.uri)	playlist def playlist(self): :class:`SearchPlaylist`."""
# todo(shoyer): test fails on tpu </s> if jtu.device_under_test() == "tpu":	test_custom_linear_solve_cholesky def test_custom_linear_solve_cholesky(self): raise SkipTest("Test fails on TPU") def positive_definive_solve(a, b):
# todo: alloc_shift </s> b = np.empty_like(a)	_column_shift_impl def _column_shift_impl(A, shift): numba.stencil(lambda a, b: a[-b], out=B)(A, shift) B[0:shift] = np.nan
# this is the dc dmdsec. @todo: account for other as well. </s> dcmetadata = parsedmdsec(dmdsecs[0])	generateCompoundContentDMDirectUploadPackage if len(dmdSecs) == 1 or len(dmdSecs) == 2: if len(dmdSecs) == 2: nonDcMetadata = parseDmdSec(dmdSecs[1])
# todo: if py3k, override unpickler.find_class(). </s> pass	_aux_cache_init mypickle.find_global = None except AttributeError: aux_cache = mypickle.load() except (SystemExit, KeyboardInterrupt):
# todo blanket allows may be modified by subsequent denies... -- does policyuniverse handle? </s> if statement["effect"] == "allow":	_find_roles_assumable_in_policy for action in actions: if action == "sts:AssumeRole": role_arns = statement["Resource"] if isinstance(role_arns, str):
# todo: check this out </s> self.git_checkout(active_branch, options="-f")	import_metadata im.store_data(opj(self.path, HANDLE_META_DIR)) self.add_to_git(opj(self.path, HANDLE_META_DIR)) self.git_merge(src_name)
# todo(dcramer): ideally we could just send the signal to the subprocess </s> self.task.date_finished = datetime.utcnow()	_cancel self._logreporter.save_chunk('>> Task was cancelled\n') with lock(redis, 'task:{}'.format(self.task.id), timeout=5): db.session.add(self.task) db.session.commit()
#todo: delete these following two lines when we default to geomean </s> if self.data_type == 'average':	makeChartUrl base_url = self.config.get('main', 'base_graph_url') graph_datatype = 'geo' graph_datatype = 'running' if d is not None:
# todo: do the computation without the 'sr' enforcement </s> mat_self = matrix(	__invert__ chart = self._domain._def_chart #!# to be improved try: [[self.comp(frame)[i, j, chart].expr(method='SR') for j in range(si, nsi)] for i in range(si, nsi)])
# todo: check rackspace file existence </s> return os.path.isfile(safe_join(filepath, filename))	zip_existing filename=self.download_name(app, ty)
# todo: make these 3-d numpy arrays. </s> term1 = hfxx	evaluate_hessian_equality_constraints hfyy = [get_hessian_of_constraint(con, y) for con in f] d2ydx2 = self.evaluate_hessian_external_variables() term2 = [] for hessian in hfxy:
# todo: check that the performance measure is within some range </s> merge_baseline(num_runs=1, flow_params=merge1, render=false)	test_merge Tests flow/benchmark/baselines/merge{0,1,2}.py merge_baseline(num_runs=1, flow_params=merge0, render=False) merge_baseline(num_runs=1, flow_params=merge2, render=False)
# todo: use parameterized tests to test all losses. </s> loss_fn = losses_impl.sigmoidcrossentropyloss(name=none, ragged=true)	test_pointwise_normalize_weights_with_ragged_tensors per_item_weights = tf.ragged.constant([[2., 3., 4.], [1., 1.]]) with self.cached_session(): weights = loss_fn.normalize_weights(labels, per_item_weights).eval() self.assertAllClose(weights, [[2., 3., 4.], [1., 1., 0.]])
# todo: check that the birth date is not in the future </s> except valueerror, e:	is_valid try: birth_date = _get_birth_date(number) return False if len(number) == 10:
# todo: remove in favor of a proper per-module selection </s> parser.add_argument('--force-module-branch-type',	_parse_args parser.add_argument('--no-turbo', action='store_false', dest='turbo_mode', default=None, help='do not automatically select entries (safer).') help='Set to beta if you want to test beta versions of all modules. Please report breakage.') if platform.system() == "Darwin":
# todo: set cookie </s> theme = settings['theme']	theme_loader theme = cookies['theme'].value if not os.path.isdir(os.path.join(THEME_LOC, theme)): except KeyError: theme = SETTINGS['Theme']
# todo add an option for preferred file descriptor here </s> chain = self.rop.func_call(addr, [0, read_to, len(data)])	_read_in_global_data_with_read return None, None read_to = self._find_global_address_for_string(data) _, chain_addr, _ = self._ip_overwrite_with_chain(chain) chain_mem = self.crash.state.memory.load(chain_addr, len(chain.payload_str()))
# todo: implement subdomains for slate tensors </s> if kinfo.subdomain_id != "otherwise":	__init__ indices = split_kernel.indices kinfo = split_kernel.kinfo raise NotImplementedError("Subdomains not implemented.") args = [c for i in kinfo.coefficient_map
# todo(albert): this part of the test is broken because python </s> self.assertequal({ok.info_file}, self.listtestdir())	testNoTests ok.dump_tests(TMP, self.assignment)
# todo test </s> self.unconstrained_effect_repertoire(purview))	effect_info return utils.emd(self.effect_repertoire(mechanism, purview),
# todo: use shlex.quote as soon as a newer python version is available. </s> quoted_file_path = pipes.quote(file_path)	check_file def check_file(self, file_path): rvm_cmd = os.path.expanduser('~/.rvm/bin/rvm-auto-ruby') rubocop_cmd = rvm_cmd + ' -S rubocop ' + quoted_file_path self.run_shell_command(rubocop_cmd)
# todo, this is scratch/proto </s> alternatives = self.redis.lrange(self.key(), 0, -1)	has_converted_by_client_id def has_converted_by_client_id(self, client_id): for alternative in alternatives: if self.redis.getbit(_key("conversion:{0}:{1}".format(self.name, alternative)), client_id):
# todo: assert </s> repo = self.remote.get_repo("testrepo0")	test_get_repo Test: Get a repo object
# todo: probably not mutate these foreign attrs - ideally maybe move quite a bit of this method up to fleetstate (maybe in __setitem__). </s> self.known_nodes.checksum = keccak_digest(b"".join(bytes(n) for n in self.sorted_nodes())).hex()	update_fleet_state def update_fleet_state(self): self.known_nodes.updated = maya.now()
# todo: adjust dimension order for tf2 broadcasting </s> return new if pass_through else tf.compat.v1.where(	_maybe_copy_state new.set_shape(cur.shape) pass_through = (new.shape.ndims == 0) finished, cur, new)
# todo: test for this error </s> try:	parse_if_statement raise ParserError(err, index, self.tokens, ParserError.AFTER) conditional, index = self.parse_expression(index) index = self.match_token(index, token_kinds.close_paren) except MatchError:
# todo: warn if field has_choices but not in table.filtering </s> print('creating filter indexes...', end='', flush=true)	handle print('  done in {:.3f}s.'.format(end - start)) if create_filter_indexes: start = time.time() Model.create_indexes()
# todo: see about implementing this via the popcnt instruction on </s> assert isinstance(i, r_uint)	bit_count def bit_count(i): i = i - ((i >> 1) & r_uint(0x55555555)) i = (i & r_uint(0x33333333)) + ((i >> 2) & r_uint(0x33333333))
# todo: remove this ``expectedfailure`` </s> self.assertlistequal(data2, [])	test_successful_read_transaction data2 = list(Test.objects.all())
# todo: bytes vs str </s> request_line = request_line.decode()	_handle def _handle(self, reader, writer): request_line = yield from reader.readline() method, path, proto = request_line.split() headers = {}
# todo(pep612): fix for paramspectype </s> if isinstance(tvar, paramspectype):	get_target_type skip_unsatisfied: bool ) -> Optional[Type]: return None assert isinstance(tvar, TypeVarType)
# todo: this is untested. </s> _raise_current_error()	load_certificate_request raise ValueError("type argument must be FILETYPE_PEM or FILETYPE_ASN1") if req == _ffi.NULL: x509req = X509Req.__new__(X509Req) x509req._req = _ffi.gc(req, _lib.X509_REQ_free)
# todo: backwards compatibility; remove in favor of class method </s> return str(self.__calibration_file_path_in_recording(self._rec_dir, calibration))	_calibration_file_path def _calibration_file_path(self, calibration):
# todo - this only allows using the default constructor </s> javaopcodes.invokespecial(self.klass.extends_descriptor, '<init>', '()v'),	InitMethod JavaOpcodes.ALOAD_0(), JavaOpcodes.DUP(), ) self.add_opcodes(
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo increase precision </s> eps = numpy.finfo(float).eps	check_degree ] vals = quadrature(fun) alpha = abs(exact_vals) * tol + (1.0e5+tol+exact_vals)*eps is_larger = abs(exact_vals - vals) > alpha
# todo(boris-42): make it work through assertisinstance </s> self.assertequal(str(type(engine_inst)), str(e))	test_get_engine mock.Mock(), {})
# todo find out what is best used here! </s> 'preferred_dtype' : none}	get_meta_information 'is_deterministic': True, 'handles_sparse': True,
creator = chain.web3.eth.accounts[0]  # todo: make it possible to override </s> escrow, tx = chain.provider.get_or_deploy_contract(	create_escrow chain = blockchain.chain() token = get() ESCROW_NAME, deploy_args=[token.address] + MINING_COEFF, deploy_transaction={'from': creator})
# todo: fix this </s> return cls()	from_json_content @classmethod def from_json_content(cls, value) -> 'FeatureConnector':
# todo: return proper searchable iterator </s> params = {	fetch_messages Returns: list: `Message` objects "id": self.id, "message_limit": limit,
# xxx todo: rounding </s> e = []	ucvtf def ucvtf(ir, instr, arg1, arg2): src = ExprOp('uint_to_fp', arg2) if arg1.size != src.size:
raise notimplementederror # todo </s> list every item in entry_point that match request	search def search(self, entry_point, request):
# todo: migrate to glaziererror </s> _logfatal('unable to remove task list', self._build_info, 4303, e)	_SetupTaskList os.remove(location) except OSError as e: return location
# todo(john sirois): map target.resources in the same way </s> for target in scala_targets:	execute genmap.add(source, self._classes_dir, classes) genmap.add(target, self._classes_dir, classes) if is_scalac_plugin(target) and target.classname: basedir = self.write_plugin_info(target)
# todo gdef/lookup markfilteringsets </s> t.table.scriptlist.mapfeatures(featuremap)	_postMerge featureMap = dict((v,i) for i,v in enumerate(t.table.FeatureList.FeatureRecord))
# todo: fix this </s> return {	Translation return sorted(self.keys()) def to_json_content(self): 'languages': self.languages, 'encoder': None,
uploader.upload_file(file, container='export') # todo: right container folder?! </s> finally:	export_json memzip = make_onefile_memzip(datafile.name, '%s_task.json' % name) file = FileStorage(filename='%d_%s_task_json.zip' % (app.id, name), stream=memzip) datafile.close() json_task_run_generator = respond_json("task_run", app.id)
# todo: deleted packages are currently being removed from the search </s> self._test_can('search', self.testsysadmin, ['xx', 'rx', 'wx', 'rr', 'wr', 'ww'], entity_types=['package'])	TestUsage self._test_can('create', self.testsysadmin, []) def test_sysadmin_can_search_anything(self): def test_visitor_deletes(self): self._test_cant('delete', self.visitor, ['gets_filled'], interfaces=['wui'])
# todo partially update stored playlists? </s> playlist.name())	playlist_renamed logger.debug(u'Callback called: Playlist renamed to "%s"',
pass  # todo: implement this </s> def application_keypad_mode(self):	application_keypad_mode
# todo: make the min-max values a setting? </s> queue = gst.element_factory_make('queue')	_setup_output process.exit_process() output = gst.Bin('output') queue.set_property('max-size-buffers', 0) queue.set_property('max-size-bytes', 0)
# todo: @sbharadwajj implement and test </s> return true	hessian_is_zero def hessian_is_zero(self):
# todo: force to download it </s> logging.warning(_("file %s already exists, cnchi will not overwrite it"), element['filename'])	start if element['hash'] != md5: logging.warning("MD5 hash of %s does not match!", element['filename']) needs_to_download = False downloaded += 1
# todo: test me </s> if get_rest_framework_features()['use_dot_in_lookup_regex_by_default'] or force_dot:	get_lookup_allowed_symbols def get_lookup_allowed_symbols(kwarg_name='pk', force_dot=False): return '(?P<{0}>[^/.]+)'.format(kwarg_name) else:
# todo: unit-test this method. </s> return self.read(path)	load_name path = locator.find_path_by_name(search_dirs, name)
# todo(yanase): update sklearn integration to support v0.22.1 or newer. </s> 'scikit-learn>=0.19.0,<=0.22.0',	get_extras_require 'plotly>=4.0.0', 'pytest', 'scikit-optimize', 'xgboost',
# todo(kan-bayashi): documentation and type hint </s> def piecewise_rational_quadratic_transform(	piecewise_rational_quadratic_transform inputs, unnormalized_widths,
# todo(b/161332815): make jax actor work with batched or unbatched inputs. </s> observation = utils.add_batch_dim(observation)	FeedForwardActor def select_action(self, observation: types.NestedArray) -> types.NestedArray: key = next(self._rng) action = self._policy(self._client.params, key, observation) return utils.to_numpy_squeeze(action)
if util.pythonise(pipe['wires'][wire]['tgt']['moduleid']) == module_id and pipe['wires'][wire]['tgt']['id'] != '_input' and pipe['wires'][wire]['src']['id'].startswith('_output'): # todo? this equates the outputs </s> pargs.append("%(id)s = %(secondary_module)s" % {'id':util.pythonise(pipe['wires'][wire]['tgt']['id']), 'secondary_module':util.pythonise(pipe['wires'][wire]['src']['moduleid'])})	write_pipe ] for wire in pipe['wires']: if module['type'] == 'loop': pargs.append("embed = pipe_%(embed_module)s" % {'embed_module':util.pythonise(module['conf']['embed']['value']['id'])})
# todo: use widgets.dialog </s> ret = wx.messagedialog(self._editor, 'apply changes?', 'source changed',	_ask_and_apply def _ask_and_apply(self): style=wx.YES_NO | wx.ICON_QUESTION).ShowModal() if ret == wx.ID_YES:
# todo(ssbarnea): remove that deprecation fallback in 3.1+ </s> if not playbook or os.path.isfile(playbook):	_normalize_playbook Return current filename to use for a playook by allowing fallbacks. Currently used to deprecate use of playbook.yml in favour of converge.yml return playbook pb_rename_map = {"converge.yml": "playbook.yml"}
# todo xxx graalvm change </s> raise unittest.skiptest("missing threading support")	ThreadedEchoServer self.daemon = True def __enter__(self): self.start(threading.Event()) self.flag.wait()
# todo see https://github.com/healthcatalyst/healthcareai-py/issues/276 </s> pipeline = pipelines.full_pipeline(model_type, predicted_column, grain_column, impute=impute)	__init__ self.grain_column = grain_column, self.grain_column = grain_column, clean_dataframe = pipeline.fit_transform(dataframe) self._advanced_trainer = AdvancedSupervisedModelTrainer(clean_dataframe, model_type, predicted_column,
# todo(stephenfin): use a helper </s> self.api.post_server_action(server['id'], {'migrate': none})	test_migrate_confirm fake_drop_move_claim, ) self._wait_for_state_change(server, 'VERIFY_RESIZE') self.assertUsage(src_host, 1)
# todo: make legacy detection non-reliant on side </s> if master_namespace not in original:	rewrite_config with open(self.filepath, 'r') as config: original = yaml.safe_load(config) original = {MASTER_NAMESPACE: original} else:
# todo this is a workaround since exceptions are currently not correctly stacked </s> pass	sub return self._emit("").join(result) except RuntimeError: return self.__compile_cpython_sre().sub(repl, string, count)
# todo: documentation pending </s> utils.load_ckpt(sess, mode_name, save_dir, self.weights, is_latest, printable)	load_ckpt def load_ckpt(self, sess=None, mode_name='model.ckpt', save_dir='checkpoint', is_latest=True, printable=False):
# todo: this is untested. </s> _raise_current_error()	load_certificate_request raise ValueError("type argument must be FILETYPE_PEM or FILETYPE_ASN1") if req == _ffi.NULL: x509req = X509Req.__new__(X509Req) x509req._req = _ffi.gc(req, _lib.X509_REQ_free)
# todo: test size=var, with shape that change from call to call </s> if (mode in ['debug_mode', 'debugmode', 'fast_compile'] or	test_uniform def test_uniform(): mode == 'Mode' and config.linker in ['py']): sample_size = (10, 100)
# todo: logging belongs in on_tag_added hook </s> if log:	add_tag if not self.tags.filter(id=tag_instance.id).exists(): self.tags.add(tag_instance) self.add_tag_log(tag_instance, auth) if save:
#todo - introduce an annotated alignment class? </s> alignment._annotations = gr	next % (len(ids), self.records_per_alignment)) alignment = Alignment(self.alphabet) alignment_length = len(seqs.values()[0]) for id in ids :
## todo : log error </s> error_code = delete_functionparameters_doctype_submission(doctype=doctype, action=action)	_delete_submission_from_doctype user_msg.append("""When deleting Submission "%s" from Document Type "%s", it wasn't possible to delete all Submission Fields""" \ % (action, doctype)) if error_code != 0: user_msg.append("""When deleting Submission "%s" from Document Type "%s", it wasn't possible to delete all Function Parameters""" \
# xxx todo: rounding </s> e = []	fcvtzs def fcvtzs(ir, instr, arg1, arg2): e.append( ExprAssign(
# todo document </s> a = similarity_variable	Dadgostar_Shaw_integral def Dadgostar_Shaw_integral(T, similarity_variable): a2 = a*a T2 = T*T
# short-circuit. todo: test it. </s> return false	execute_and return bool(execute(self.other, contexts)) else:
# add newly added packages to the todownload list </s> for pkg in self.tsinfo.getmembers():	findDependencies self.localPackages.append(po) self.resolveDeps() if not pkg in toDownload: toDownload.append(pkg)
"size": 50  # todo: support pagination. </s> }	search_project }, "fields": ["name", "slug", "description", "lang", "url"], if language: body['facets']['language']['facet_filter'] = {"term": {"lang": language}}
)  # todo ensure dictionaries stay dictionaries </s> if hasattr(column.type, "dictionary")	__parquet_to_pandas )
# todo: elif gcpinstance().is_gcp_instance(): </s> else:	get_monkey_environment elif AzureInstance().is_azure_instance(): env = AZURE env = ON_PREMISE return env
# todo: this is a temporal fix </s> pixels = np.rollaxis(pixels, 0, len(pixels.shape))	lbp if samples is None: samples = [8]*4 if not skip_checks: if ((isinstance(radius, int) and isinstance(samples, list)) or
# todo 2.8 i want to change the way we handle unit scaling, see </s> pass	matrix_to_list You only have to pass a valid scene if apply_worldscale is True if apply_worldscale: if invert: matrix = matrix.copy()
# todo: remove once elasticsearch v6.x is deprecated. </s> if self._getclientmajorversion() < 7:	_FlushEvents 'index': self._index_name, 'request_timeout': self._DEFAULT_REQUEST_TIMEOUT} bulk_arguments['doc_type'] = self._document_type self._client.bulk(**bulk_arguments)
# todo: this only works when orgmode is started with one orgfile </s> agenda_documents = [orgmode.get_document(i+1) for i in range(len(agenda_files))]	list_next_week for agenda_file in agenda_files: vim.command('badd %s' % agenda_file) raw_agenda = ORGMODE.agenda_manager.get_next_week_and_active_todo(agenda_documents) cmd = [u'setlocal filetype=orgtodo']
# todo: consider adding some better error handling for bad/failed requests. </s> except:	identify try: km.set(email, properties) if i < ANALYTICS_RETRIES - 1: time.sleep(ANALYTICS_SLEEP)
# @todo: move to css </s> _style="padding-bottom:10px",	member_rheader "pr_person", _class="fleft"), ), TABLE(TR(TH(s3_fullname(record))),
# todo not tested </s> _raise_current_error()	set_serial_number set_result = _api.X509_set_serialNumber(self._x509, asn1_serial) if not set_result:
# todo: log exception </s> return none	get_note return result except Exception as e:
# todo: 判断返回结果，处理异常 </s> try:	perm_role_delete raise ServerError(u"回收已推送的系统用户失败: %s" % e) logger.info(u"delete role %s - execute delete user: %s" % (role.name, msg)) key_files = os.listdir(role_key) for key_file in key_files:
self.ws = 2  # todo check. </s> elif self.arch_info.architecture_mode == arch.arch_arm_mode_arm:	__init__ self.ip = "r15" self.sp = "r13" self.ip = "r15" self.sp = "r13"
# todo make this smarter about more complex configurations (backup original values, etc) </s> obj_copy = deepcopy(obj_doc)	clear_mirrors obj_doc = Parser().yaml_in(config_file) if obj_doc: for switch in obj_copy['dps']: for port in obj_copy['dps'][switch]:
# todo open file for writing </s> pipeline = gst.parse_launch(''' appsrc name=src	setup def setup(self, channels=None, samplerate=None, nframes=None): super(VorbisEncoder, self).setup(channels, samplerate, nframes) ! audioconvert ! vorbisenc
# todo remove backwards compatability fix in a future version </s> if build_version < 11000:	plugin_loaded preferences = sublime.load_settings('Preferences.sublime-settings') build_version = int(preferences.get('neovintageous_build_version', 0)) preferences.set('neovintageous_build_version', 11000) preferences.set('vintageous_use_ctrl_keys', preferences.get('vintageous_use_ctrl_keys'))
# todo: fix clone issue </s> assert_array_less(-0.1, pred_ranks)	test_predict_rank_normalized assert_array_less(pred_ranks, 1.01)
@retry()  # todo: what errors do we get for timeout, json parse failure, etc? </s> def aws_marketplace_flatcar_ami_search(ec2_client: baseclient) -> optional[str]:	aws_marketplace_flatcar_ami_search response: dict = ec2_client.describe_images(Owners=['aws-marketplace'],  # type: ignore Filters=[{'Name': 'name', 'Values': ['Flatcar-stable-*']}])
# todo: move to a base class, and have test cases inherit from that. </s> settings.addons_requested.append(mockusersettings.oauth_provider.short_name)	init_mock_addon def init_mock_addon(): addon_config = AddonConfig( short_name=MockUserSettings.oauth_provider.short_name,
# todo: adapt units_qs once we allow filtering </s> units_qs = unit.store.units	get_edit_unit 'editor': t.render(c)} current_page = 'page' in request.GET and request.GET['page'] or 1 unit_rows = profile.get_unit_rows() preceding = units_qs.filter(index__lt=unit.index).count()
# todo(jay-lau-513) translate the contents to a json stdin </s> out, err = utils.trycmd('echo service | kubectl', 'create',	service_create service.service_definition_url) else: '-f', '-') if err:
# todo: copy resources once they're specified </s> print('installing ipython kernel spec')	install_my_kernel_spec with open(os.path.join(td, 'kernel.json'), 'w') as f: json.dump(kernel_json, f, sort_keys=True) install_kernel_spec(td, 'bash', user=user, replace=True)
# todo this behavior may change when eager mode is introduced </s> with self.assertraises(systemerror):	testApi value = sess.run(c) assert_array_equal(value[0], np.ones((100, 100)) * 100) sess.run(c) va = np.random.randint(0, 10000, (100, 100))
# todo :: move arbitray path construction to storagelayout object </s> url = '{0}/wal_{1}/{2}.lzo'.format(	wal_restore NB: Postgres doesn't guarantee that wal_name == basename(wal_path), so both are required. self.layout.prefix.rstrip('/'), FILE_STRUCTURE_VERSION, wal_name) logger.info(
# todo error reporting over the master event bus </s> self.event.fire_event({'minions': minions}, clear_load['jid'])	publish log.error('The requested returner {0} could not be loaded. Publication not sent.'.format(fstr.split('.')[0])) return {} new_job_load = { 'jid': clear_load['jid'],
# todo: try to remove when https://bugzilla.mozilla.org/show_bug.cgi?id=1508695 is fixed. </s> if not all_inconsistencies and any(field.startswith(k) for k in ['cf_']):	rollback if change['added'] != '---': if field not in bug: if verbose: print(f'{field} is not in bug {bug["id"]}')
# todo check if right </s> self.a = sp.sparse.lil_matrix(w > 0)	Graph self.A = A else: if N: self.N = N
recording_software_version = none  # todo </s> recording_name = none  # todo	recording_update_pupil_invisible_to_pprf_2_0 duration_ns = None  # TODO recording_software_name = None  # TODO system_info = None #TODO new_info_file = RecordingInfoFile.create_empty_file(rec_dir)
# todo: the following reproduces the old behavior of </s> i.clear()	_transformBlock for i in block.component_objects(Integral, descend_into=True): i.parent_block().reclassify_component_type(i, Expression) i._constructed = False i.construct()
# todo: message depending on narrowing/float-conversion </s> msg = 'implicit conversion from {} to {} (narrowing)'	castarray x = np.asarray(x) if x.dtype != dtype: warnings.warn(msg.format(x.dtype, dtype), RuntimeWarning) return np.require(x, dtype = dtype, requirements = 'CAW')
# todo: might need some kind of diffing too? </s> status[uid] = (list_a[uid], list_b[uid])	get_actions if uid not in status: if uid in list_a and uid in list_b:  # missing status elif uid in list_a and uid not in list_b:  # new item was created in a prefetch_from_a.append(uid)
#todo: a single softmax'd vector?? </s> if not numerator.type.dtype.startswith('float'):	softmax_grad_simplifier debugprint(denominators) for numerator in list(numerators): continue if not numerator.type.broadcastable == (False, False):
invoice_data.pop("forgiven", none)  # todo remove </s> invoice_data["status"] = "uncollectible"	test_status_forgiven_deprecated invoice_data = deepcopy(FAKE_INVOICE) invoice_data.update({"paid": False, "closed": False}) invoice = Invoice.sync_from_stripe_data(invoice_data) self.assertEqual(Invoice.STATUS_FORGIVEN, invoice.status)
# todo: only allow classmethods on base/abstract classes </s> continue	_test_adapter continue if value.im_self is not None: methods.append(name) for iface in ifaces:
#todo change to native framework call, when plex allows token in header </s> request = urllib2.request(self.getlistsurl, headers=myheader)	LIST myHeader = {} myHeader['X-Plex-Token'] = users[user]['accessToken'] playlists = XML.ElementFromString(urllib2.urlopen(request).read()) result = {}
print("key error")  # todo </s> return gen_json(table, id)	respond_json table = tables[ty] except KeyError:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_tips_empty def test_fail_tips_empty(self):
error = errors[0]  # todo: handle multiple errors </s> raise fbchatfacebookerror(	handle_graphql_errors errors = j["errors"] if errors: "GraphQL error #{}: {} / {!r}".format( error.get("code"), error.get("message"), error.get("debug_info")
# todo: verify validity of exchanging question and support. below: encode question, conditioned on support encoding. </s> outputs, states = conditional_reader(support_embedded, support_lengths,	conditional_reader_model_with_cands candidates_embedded = nvocab(candidates) print('TRAINABLE VARIABLES (only embeddings): %d' % get_total_trainable_variables()) question_embedded, question_lengths, options["repr_dim_output"]/2)   #making output half as big so that it matches with candidates
# todo: there was a bug related to this </s> pass	test_relu_clipped_gradients_are_zero @pytest.mark.skip def test_relu_clipped_gradients_are_zero():
# todo(guillermooo): implement other options. </s> m = state.expect_match(r'(?p<option>.+?)(?:[:=](?p<value>.+?))?$')	scan_command_set_local state.skip(' ') state.ignore() params.update(m.groupdict()) return None, [TokenSet(params), TokenEof()]
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pcmpeqb res = 0x000000ff0000ff00ff00000000ffff00 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo: fix return type in generated code? </s> error_string = ctypes.cast(error_string, ctypes.c_char_p)	_check_error if error_code != 0: error_string = al.alGetString(error_code) raise OpenALException(message=message, error_code=error_code,
# todo: check types </s> return pandas_timestamp_type	type_timestamp def type_timestamp(context): def typer(year, month, day):  # how to handle optional hour, minute, second, us, ns? return typer
e_die('divide by zero')  # todo: location </s> new_int = old_int % rhs	ArithEvaluator elif op_id == Id.Arith_PercentEqual: if rhs == 0: elif op_id == Id.Arith_DGreatEqual: new_int = old_int >> rhs
# todo assert cls.__tablename__ == '' </s> with dbcleanup(session, cls):	test_WorkflowStepTagAssociation def test_WorkflowStepTagAssociation(model, session, workflow_step, tag, user): cls = model.WorkflowStepTagAssociation user_tname, value, user_value = 'a', 'b', 'c' obj = cls(user=user, tag_id=tag.id, user_tname=user_tname, value=value)
# todo: call out to the ceph cluster to check the </s> except:	test_modification format(val, expected, pool['min_size']) ) log.exception("Exception updating min_size:%s" % val) raise
# todo - add team payroll and check receiving values </s> assert alice.giving == decimal('3.00')	test_only_funded_tips_count bob.set_subscription_to(team, '5.00') carl.set_subscription_to(team, '7.00') assert bob.giving == Decimal('0.00') assert carl.giving == Decimal('0.00')
annot.annotation_metadata.annotator.email = "todo"  # todo </s> annot.annotation_metadata.annotator.name = "todo"	fill_annotation_metadata annot.annotation_metadata.origin = "Centre for Digital Music"
# todo self.update_repo should take this as a dictionary and create </s> for key in repo_contents:	test_repository_added_to 'clusterhq-flocker-node-0.3.3-0.dev.7.noarch.rpm': '', } source_repo.child(key).touch() source_repo.child(key).setContent(repo_contents[key])
return  #todo disabled for now, see #2151 for details </s> savecommand( savegamemanager.create_multiplayer_autosave_name() ).execute(self)	autosave def autosave(self): self.ingame_gui.show_popup(_("Not possible"), _("Save/load for multiplayer games is not possible yet"))
# todo: can we assume reverse=false? </s> signals.m2m_changed.send(	add for tag in tag_objs: self.through.objects.get_or_create(tag=tag, **self._lookup_kwargs()) sender=self.through, action="post_add", instance=self.instance, reverse=False,
# todo: check pdf content? how? </s> _run('combined-utf-16be.html out3.png --encoding utf-16be')	test_command_line_render _run('combined.html out2.pdf') assert tmpdir.join('out1.png').read_binary() == png_bytes assert tmpdir.join('out3.png').read_binary() == png_bytes _run(tmpdir.join('combined.html').strpath + ' out4.png')
# todo check if there is more than 1 device. </s> sec_code = input('enter ' + auth_resp['data']['devices'][0]['name'] + 'security code: ')	get_cerberus_token if auth_resp['status'] == 'mfa_req': print('MFA is required') print("Token", sec_code) mfa_req = requests.post(cerberus_url + '/v2/auth/mfa_check',
# todo: consider parameterize this </s> arparams = np.array([.25])	make_trend trend = np.cumsum(rw) elif type == "arma": maparams = np.array([.6]) ar = np.r_[1, -arparams]
# todo: confirm necessity of this session clearing and lay out mechanics. </s> tuf.download._sessions = {}	test_https_dl_via_smart_http_proxy self.set_env_value('REQUESTS_CA_BUNDLE', os.path.join('ssl_certs', 'ssl_cert.crt')) logger.info('Trying HTTPS download via HTTP proxy: ' + self.url_https) download.safe_download(self.url_https, self.target_data_length)
# todo(termie): we should probably return not founds instead of none </s> self.assertequals(delget_resp.body, '')	test_crud_extras delget_resp = c.get_extras(user_id=user_id, tenant_id=tenant_id)
# todo: skipped due to gh-4436 </s> @known_failure_windows	test_initremote_basic_sshurl @skip_ssh @with_tempfile
# todo could use a generator here </s> prog = re.compile(name)	get_field :param name: the name of the field (a python regexp) :rtype: a list with all :class:`EncodedField` objects l = [] for i in self.classes.class_def:
# todo: these reductions could be delayed until _step is called. </s> loss = self._strategy.reduce(tf.distribute.reduceop.mean, per_replica_loss, none)	_forward per_replica_loss, per_replica_words = self._strategy.experimental_run_v2( _accumulate_gradients, args=(per_replica_source, per_replica_target)) num_words = { k:self._strategy.reduce(tf.distribute.ReduceOp.SUM, v, None)
# todo: usefully interpret it & other non-public-number fields </s> nonce = msg.get_string()	_check_type_and_load_cert elif type_ == cert_type: self.load_certificate(Message(msg.asbytes())) else: raise SSHException('Invalid key')
# todo: act </s> profiles = self.remote.get_profiles(self.token)	test_create_profile def test_create_profile(self): Test: create/edit a profile object profile = self.remote.new_profile(self.token) for field in self.profile_fields:
# todo: for some reason on osx a unix socket cannot be </s> if osx:	test_connections def test_connections(self): safe_rmpath(TESTFN) tfile = tempfile.mktemp( prefix=TESTFILE_PREFIX + self.funky_name)
# todo: support other reductions </s> if (isinstance(var_def, ir.expr)	_match_reduce_def while isinstance(var_def, ir.Var): var_def = guard(get_definition, f_ir, var_def) and var_def.op == 'inplace_binop' and var_def.fn in ('+=', operator.iadd)):
#  todo: move to le-utils package </s> def version_matches_range(version, version_range):	version_matches_range import semver if not version_range or version_range == "*":
# todo: check that the group is public and we're being added publically </s> is_publicised = content.get("publicise", false)	join_group server_name=get_domain_from_id(group_id), ) token = yield self.store.register_user_group_membership( group_id, user_id,
# todo(mcgallaspy): get rid of old integration tests and refactor the mixin methods </s> class contextwithmixin(createadminmixin):	login_as_admin def login_as_admin(context, admin_name="admin", admin_pass="abc123"): if not User.objects.filter(username=admin_name): def __init__(self): self.browser = context.browser
#todo: check cost line </s> return {}	isGroundBuildRequirementSatisfied @classmethod def isGroundBuildRequirementSatisfied(cls, x, y, island, **state):
assert 'could not load target data' in res.stdout  # todo: stderr </s> res = cc.fail_1(['glom', '--target-format', 'yaml', basic_spec, '{' + basic_target])	test_usage_errors def test_usage_errors(cc, basic_spec_path, basic_target_path): res = cc.fail_1(['glom', BASIC_SPEC, '{' + BASIC_TARGET]) assert 'could not load target data' in res.stdout  # TODO: stderr res = cc.fail_1(['glom', '--target-format', 'lol', BASIC_SPEC, BASIC_TARGET])
## todo: # fixme: remove me </s> try:	show_domain faup.decode(domain) unpack_url = faup.get() domain = unpack_url['domain'].decode() except:
# todo: url accepts post, need to refactor to use get+post </s> reverse('testruns-clone', args=[self.object.pk])	render_to_response ( _('Clone'), ), (
pass  # todo... </s> with tf.name_scope("slice_nd"):	slice_nd :rtype: tf.Tensor
# todo(b/161332815): make jax actor work with batched or unbatched inputs. </s> observation = utils.add_batch_dim(observation)	FeedForwardActor def select_action(self, observation: types.NestedArray) -> types.NestedArray: key = next(self._rng) action = self._policy(self._client.params, key, observation) return utils.to_numpy_squeeze(action)
# todo: support parameterization through full covariance matrix. </s> "error: `parameters` in_space must have an even numbered last rank (mean/stddev split)!"	check_input_spaces assert in_space.shape[-1] % 2 == 0,\
# todo: copy fixture types and data </s> master_results = local_fixtures(domain_link.master_domain)	update_fixtures else:
# todo security </s> def query_suggest(request, core, query=""):	query_suggest hue_core = Core.objects.get(name=core) result = {'status': -1, 'message': 'Error'}
# todo: dump content out for debugging in the future. </s> if retry_attempts < galaxy_test_selenium_retries:	func_wrapper for iframe in iframes: pass retry_attempts += 1 else:
# todo: test stubs for other versions, especially python 2 stubs. </s> seen = set()  # type: set[str]	test_stubs def test_stubs(self) -> None: modules = [] for version in ["2and3", "3", "3.5"]:
# todo: must be the same if we merged/pushed before, if not -- skip </s> pblshd = ds.repo.copy_to(files=paths,	_publish_dataset lgr.debug("Invoking copy --auto") annex_copy_options += ' --auto' remote=remote, options=annex_copy_options)
# todo: this filtering condition is probably wrong </s> if not isinstance(child, (	columns_layout column_top = new_child.content_box_y() for a, child in enumerate(new_child.descendants()): boxes.TableBox, boxes.LineBox, boxes.ReplacedBox)): continue
# todo: fix this! </s> self.assertraises(nap.napmissinginputerror, self.nap.remove_schema, { 'crap': 'crap crap' })	test_schema_remove_crap_input def test_schema_remove_crap_input(self):
# todo: add content disposition. </s> response.content_type = "application/zip"	task_screenshots for shot_name in os.listdir(folder_path): zip_file.write(os.path.join(folder_path, shot_name), shot_name) return zip_data.getvalue() else:
# todo: make truly async </s> self._resourcemanager_client = discovery.build('cloudresourcemanager', 'v1', cache_discovery=false, cache=memorycache())	__init__ def __init__(self):
# todo: this may block for 2 second if port is not used on windows </s> rv = sock.connect_ex(('127.0.0.1', port))	is_port_inuse def is_port_inuse(port): sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) sock.close() return rv == 0
# todo(sloria): test me </s> return "{file}"	get_cache_file_name metadata = file_obj.get_metadata()
# todo: add cwa logic </s> pass	create_instances relation_to_id=self.relation_to_id) else:
#return qvariant()  # todo: is this right? </s> return	data return elif role != QtCore.Qt.DisplayRole: row = index.row() if row < len(self.items):
# todo: implement </s> raise notimplementederror	to_gpu def to_gpu(self, device_num):
# todo: add at least reflection tests before adding notimplemented version </s> def cleartagid(context, tlid, tag):	cleartagid *musicpd.org, current playlist section:* ``cleartagid {SONGID} [TAG]``
# todo also handle method and request </s> return proxy.reverseproxyresource.render(self, request)	doneAllPrehooks def doneAllPrehooks(result): request.content = StringIO.StringIO(json.dumps(result["Body"]))
# todo: replace suite with testcases </s> testcases_folder_path = testcases_folder_path or os.path.join(os.getcwd(), "suite")	load_testcases_folder } testcases_definition_mapping = {} testcases_items_mapping = load_folder_content(testcases_folder_path) for testcase_file_path, testcase_items in testcases_items_mapping.items():
# todo: caching? </s> return d	find_kernel_specs d[NATIVE_KERNEL_NAME] = self._make_native_kernel_dir()
# todo(mordred) fix that data path too </s> if not found_in_argparse:	_validate_auth_ksc for opt in [p_opt.name] + [o.name for o in p_opt.deprecated_opts]: opt = opt.replace('-', '_') config.pop(opt, None) config['auth'].pop(opt, None)
# todo: add option for simple print without colors & machine readable format </s> click.echo("{} {}".format(	entity_listing def entity_listing(cls): for entity in cls.objects.all(): entity.name, click.style("[#{}]".format(entity.id), fg="white", dim=1),
# todo(iceboy): fix caller when udoc=none is passed in. </s> return (priv & udoc['priv']) == priv	udoc_has_priv def udoc_has_priv(self, udoc, priv):
# todo you should put some extra protection on this, so a user can only </s> revoke = request.json.get('revoke', none)	change_jwt_revoke_state @app.route('/auth/tokens/<string:jti>', methods=['PUT']) def change_jwt_revoke_state(jti): if revoke is None: return jsonify({'msg': "Missing json argument: 'revoke'"}), 422
""" todo: documentation </s> return false	streaming @property def streaming(self):
# todo need data </s> pass	test_languages_by_group def test_languages_by_group(augur_db_routes):
# todo, pass also best score </s> if last_path is not none and not self.trainer.testing:	__recover_child_process_weights if self.trainer.checkpoint_callback: self.trainer.checkpoint_callback.best_model_path = best_path ckpt = torch.load(last_path, map_location=lambda storage, loc: storage) model.load_state_dict(ckpt)
#todo discont: actually use offsets instead of (start, end)! </s> messager.warning('_edit_span(): using (start, end)')	_edit_span def _edit_span(ann_obj, mods, id, offsets, projectconf, attributes, type, undo_resp={}): start, end = offsets[0] ann = ann_obj.get_ann_by_id(id)
raise  # todo: error reporting </s> except os.error:	load classifier = pickle.load(open(filename, "rb")) except pickle.UnpicklingError: raise  # TODO: error reporting else:
# todo: move succeed to papyon callbacks </s> self.session.add_event(event.event_contact_add_succeed,	_handle_action_add_contact def _handle_action_add_contact(self, account): papycontact = self.address_book.contacts.search_by('account', account) account)
#todo: check if/where this is used; if not used externally - remove </s> self.marker_detector.marker_min_perimeter = value	marker_min_perimeter @marker_min_perimeter.setter def marker_min_perimeter(self, value: int):
# todo does not work after multiprocessing branch merge </s> kills mpd.	_connection_kill ``kill``
# todo: support classification </s> if isinstance(self.loss, multiloss):	to_quantiles torch.Tensor: quantiles of shape batch_size x timesteps x n_quantiles if isinstance(self.loss, DistributionLoss) and out.get("output_transformation", True) is None: out = [ Metric.to_quantiles(loss, out["prediction"][idx], quantiles=kwargs.get("quantiles", loss.quantiles))
# todo: does this type of numeric literal actually work? </s> (uri(edge_ann_uri), quri('cdao:has_value'), rdf.node(clade.branch_length)),	process_clade (Uri(edge_ann_uri), qUri('rdf:type'), qUri('cdao:EdgeLength')), (Uri(edge_uri), qUri('cdao:has_annotation'), Uri(edge_ann_uri)), ] add_statements(statements)
# todo: call to _keys is bad </s> return self.storage_adapter._keys()	list_statements def list_statements(self): Returns a list of the statement text for all statements in the database.
# todo: this is untested. </s> raise valueerror("unknown bio failure")	_handle_bio_errors raise ValueError("BIO_should_io_special") else: else: _raise_current_error()
# todo: does not handle pruning. </s> stack_port = other_valve.dp.shortest_path_port(self.dp_name)	learn_host_from_pkt return ofmsgs_by_valve for other_valve in stacked_other_valves: valve_vlan = other_valve.dp.vlans.get(pkt_meta.vlan.vid, None) if stack_port and valve_vlan:
#todo: if the device is managed and user has nmcli installed, </s> if interface_name in l and "unmanaged" not in l:	is_managed_by_network_manager out = check_output(["nmcli", "dev"]) for l in out.splitlines(): is_managed = True except:
# todo: handle timeout </s> return self.__socket.recv(1024)	read @keyword timeout: the maximum time in millisecond to wait before a message can be reached @type timeout: :class:`int`
# todo: a way to store a channel's key in configuration </s> bot.join(channel, key)	join bot.config.save() else:
# todo incorporate weights -- currently handled by smoothing </s> cnarr = cnarr.autosomes()	hmm_get_model if method == 'germline': model.means_ = state_means freqs = as_observation_matrix(cnarr) if cnarr.chromosome.nunique() == 1:
):  # todo? this equates the outputs </s> pipe_id = util.pythonise(pipe_wire['tgt']['id'])	build_pipe pipe_wire['tgt']['id'] != '_INPUT' and pipe_wire['src']['id'].startswith('_OUTPUT') kargs["%(id)s" % {'id': pipe_id}] = steps[ util.pythonise(pipe_wire['src']['moduleid'])]
# todo: use a namedtuple? </s> cinfo = (fs_i, offset, c_shape, actee)	__init__ offset = 0 for fs_i, fs_shape in enumerate(shapes): action_coeffs.setdefault(fs_shape, []).append(cinfo) offset += reduce(lambda x, y: x*y, fs_shape)
# @todo: where is this in the yt api? </s> if pf.cosmological_simulation:	write_to_gdf g.attrs["unique_identifier"] = pf.unique_identifier g.attrs["cosmological_simulation"] = pf.cosmological_simulation g.attrs["current_redshift"] = pf.current_redshift g.attrs["omega_matter"] = pf.omega_matter
" # todo: i18n", </s> '-print("hello world!")',	test_process_hunks_patch_called_correctly " ", " ", '+print(tr("Hello world!"))', " ",
# todo: consult best practices for python and twisted logging. </s> logging.basicconfig(level=logging.info)	_main_async argv = sys.argv if not _abort_for_test: log.startLoggingWithObserver(log.PythonLoggingObserver(loggerName='shinysdr').emit, False) argParser = argparse.ArgumentParser(prog=argv[0])
# todo: remove this when domain decomposition is merged </s> warnings.warn('this feature is not yet implemented in a release ' \	set_dd_mesh_dimension def set_dd_mesh_dimension(self, dimension): 'version of openmc') if not isinstance(dimension, tuple) and \
# todo: use different flag than .reentrant </s> pos = colorsorter._transform_point(pos)	schedule_sphere color, pos, radius, ColorSorter._debug_transforms() ##### if ColorSorter._parent_csdl and ColorSorter._parent_csdl.reentrant: if drawing_globals.use_c_renderer and ColorSorter.sorting: if len(color) == 3:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: do i also need to test post/array? </s> self.asserttrue(true)	test_insert_shadow_document_complex parallel versions collection when the entire document is version controlled.
uploader.upload_file(file, container='export') # todo: right container folder?! </s> finally:	export_json memzip = make_onefile_memzip(datafile.name, '%s_task.json' % name) file = FileStorage(filename='%d_%s_task_json.zip' % (app.id, name), stream=memzip) datafile.close() json_task_run_generator = respond_json("task_run", app.id)
# todo: g+ has a multiply-valued 'urls' field. ignoring for now because </s> url = actor.get('url')	create_new if auth_entity and auth_entity.user_json: actor = source.as_source.user_to_actor(json.loads(auth_entity.user_json)) if url: source.domain_url = url
# todo implement for all channels </s> def _handle_toggle(self, message):	_handle_toggle return None
# todo testing </s> sys.stdout.write('asking masterdriver to forward the cov')	do_ConfirmedCOVNotifcationRequest if not isinstance(apdu, ConfirmedCOVNotificationRequest): _log.error() self.vip.rpc.call(PLATFORM_DRIVER, 'forward_bacnet_cov_value', apdu.initiatingDeviceIdentifier,
# todo: these are required for consistency with couch representation, figure out how best to deal with it </s> case_ready_to_go['doc_type'] = 'commcarecase'	process_change def process_change(self, pillow_instance, change, do_set_checkpoint): case_ready_to_go = transform_case_for_elasticsearch(change.get_document()) case_ready_to_go['_id'] = case_ready_to_go['case_id'] doc_exists = self.elasticsearch.exists(CASE_INDEX, change.id, CASE_ES_TYPE)
# wait until the chunks have added, todo change this to a qtbot.waitsignal </s> qtbot.wait(short_loading_delay)	test_tiled_rgb visual = viewer.window.qt_viewer.layer_to_visual[layer] assert isinstance(visual, VispyTiledImageLayer) screenshot = viewer.screenshot(canvas_only=True) center_coord = np.round(np.array(screenshot.shape[:2]) / 2).astype(np.int)
# todo use the phobos settings for this information </s> return modelconf	modelConf SubElement(authorEL, 'name').text = "DUMMY" SubElement(authorEL, 'email').text = "dummy@dummy.mail"
# todo: test for last revision minus 50 on second page. </s> offset = url_for(controller='revision', action='list')	test_list_format_atom revisions = model.repo.history().all() revision1 = revisions[0] res = self.app.get(offset + '?format=atom') print res
elif order <= 2147483647:   # todo: don't hard code </s> return fast_arith.arith_llong().inverse_mod_longlong	get_inverse_mod if order <= 46340:   # todo: don't hard code return fast_arith.arith_int().inverse_mod_int raise NotImplementedError
# todo?: if sha256 of the address is in deletedaddresses, </s> print(e)	_inbound_logic return HttpResponse("Address does not exist") except RelayAddress.DoesNotExist as e: return HttpResponse("Address does not exist") logger.info('email_relay', extra={
# todo make comments clearer, see _viterbi_decode </s> broadcast_score = score.unsqueeze(2)	_compute_normalizer score = self.start_transitions.view(1, -1) + emissions[0] for i in range(1, seq_length): broadcast_emissions = emissions[i].unsqueeze(1)
# todo: add docstring </s> def remove_target_from_bin(self, target_filepath):	remove_target_from_bin return self.act_on_target_to_bin(target_filepath, 'remove_target')
# todo extend to nonbinary nodes </s> if i not in self._input_indices and i in self.context.node_indices:	__init__ else: self._dimension_labels.append(None) past_tpm_on = past_tpm_on.sum(i, keepdims=True) / 2 past_tpm_off = past_tpm_off.sum(i, keepdims=True) / 2
# todo(dcramer): ideally we could just send the signal to the subprocess </s> self.task.status = taskstatus.failed	_read_timeout self._logreporter.save_chunk('>> Process did not receive updates in %ds\n' % self.read_timeout) with lock(redis, 'task:{}'.format(self.task.id), timeout=5): self.task.date_finished = datetime.utcnow() db.session.add(self.task)
# todo: refactor this </s> config_file = open(config, 'w+')	save_urls pass parser.set(self.CONFIG_URL_SECTION, self.CONFIG_URL_LIST, self.urls) parser.write(config_file) config_file.close()
# todo: deal with scroll position </s> @objc.python_method	FGMainWindowController newWidth = max(newWidth, fontItem.minimumWidth) self._fontGroup.width = newWidth def setFontItemText(self, fontKey, fontItem, txt, isSelectedFont): fontPath, fontNumber = fontKey
# todo: find a better way to return errors... </s> for filename in archive.namelist():	uploadarchive import zipfile archive = zipfile.ZipFile(cStringIO.StringIO(archivecontents), 'r') if not filename.endswith(os.extsep + self.fileext): print "error adding %s: not a %s file" % (filename, os.extsep + self.fileext)
# todo: implement me </s> self.logmessage("%s %s" % (msg.__class__, vars(msg)), 4)	ChildDepth def ChildDepth(self, msg):
# todo docstring </s> sdist_url = 'https://s3.amazonaws.com/clusterhq-archive/python/flocker-{}.tar.gz'.format(version)  # noqa	publish_homebrew_recipe :param bytes content: The content of the desired Homebrew recipe. :return git.remote.PushInfo: Information about the push to GitHub. content = make_recipe( version=version,
# todo i18n text entries </s> for entry in setting.choices:	_create_map_choice_control c.pack_start(keyBox, False, False, 0) c.pack_end(valueBox, False, False, 0) keyBox.append(str(int(entry)), str(entry)) keyBox.set_active(0)
# todo assert exit code != 0 </s> self.assertequal(dvol.voluminous.getoutput(),	test_commit_no_message_raises_error dvol = VoluminousOptions() dvol.parseOptions(ARGS + ["-p", self.tmpdir.path, "init", "foo"]) ["You must provide a commit message"])
# @todo: test </s> self.popupmenu(menu)	spawnMenu menu = ContextMenu.getMenu(selection, *contexts)
# todo(b/160795287): deprecate estimator based executor. </s> absl.logging.warning('support for estimator-based executor and model export'	serving_model_path export_dir = os.path.join(model_dir, 'export') if tf.io.gfile.exists(export_dir): ' will be deprecated soon. Please use export structure ' '<ModelExportPath>/serving_model_dir/saved_model.pb"')
# todo legacy method to be removed/refactored </s> mapping = self.get_location_map_case()	clear_locations def clear_locations(self): if mapping: mapping.delete()
# todo modification in place </s> trimmed_read.sequence = (	lowercased_read start, stop = AdapterCutter.remainder(matches) read_sequence = matches[0].read.sequence read_sequence[:start].lower() + read_sequence[start:stop].upper()
# todo(leofang): it seems as of rocm 3.5.0 hiprtc/hipcc can automatically </s> if arch is none:	_compile_with_cache_hip if cache_dir is None: cache_dir = get_cache_dir() arch = os.environ.get('HCC_AMDGPU_TARGET') if arch is None:
# todo reconsider with python 2 drop </s> kind = p._name.get_kind()	_get_signature_param_names for call_sig in signatures: for i, p in enumerate(call_sig.params): if i < positional_count and kind == Parameter.POSITIONAL_OR_KEYWORD: continue
# todo: arrange </s> repo = self.remote.new_repo(self.token)	test_create_repo def test_create_repo(self): Test: create/edit a repo object self.assertTrue(self.remote.modify_repo(repo, "name", "testrepo0", self.token)) self.assertTrue(self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token))
# check clock sync todo: alert on clockless origin server. </s> skew = self.response.parsed_hdrs.get('date', 0) - \	checkClock def checkClock(self): self.response.header_timestamp + self.response.parsed_hdrs.get('age', 0) if abs(skew) > 5: # TODO: make configurable
# todo ... </s> pass	astForCWhile assert isinstance(stmnt, CWhileStatement) assert len(c.args) == 1
# todo: do something with loggers? </s> ...	PubsubNotifee trio.BusyResourceError, ): async def listen(self, network: INetwork, multiaddr: Multiaddr) -> None: pass
# todo: look this up in one query </s> for user_id in obj['collaborator_ids']:	get_by_mbid obj['collaborator_ids'] = playlist_collaborator_ids.get(obj['id'], []) collaborators = [] user = db_user.get(user_id) if user:
# todo: figure out how to do this for multiple inputs </s> batch, out_channels, out_x, out_y = module.output_shape	backpropagate_sqrt_ggn def backpropagate_sqrt_ggn(module, grad_output, sqrt_ggn_out): in_features = module.input0.numel() / batch out_features = out_channels * out_x * out_y
# todo: fix appveyor - error comes up with bollu/vispy:cassowary-constaints </s> if os.getenv('appveyor', '').lower() == 'true':	test_arrow_reactive @requires_application() def test_arrow_reactive(): raise SkipTest('AppVeyor has unknown failure') with TestingCanvas() as c:
# todo confirm we want floor division here </s> collen = self.size() // rowlen	reshape rowlen = self.size() // collen if collen == -1: if self.typ == 'numpy': self.mat = np.reshape(self.mat, (rowlen, collen), order='F')
# todo(hirofumi0810): remove this after supporting trasformer for the st task </s> return self.recognize(x, trans_args, char_list, rnnlm)	translate def translate(self, x, trans_args, char_list=None, rnnlm=None):
# todo: expand to full set of info </s> def create_task_resource_table(task_id, run_id, meta):	create_task_resource_table table_name = run_id + str(task_id) return Table(
#todo - is there a nice way to return an iterator and </s> records = list(seqio.parse(child.stdout, new_format))	emboss_piped_SeqIO_convert child.stdin.close() child.stderr.close() child.stdout.close() return records
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_change_address_wrong_type def test_fail_change_address_wrong_type(self): ``change_address`` is not a TrytesCompatible value.
# todo 2.8 i want to change the way we handle unit scaling, see </s> ws = 1	get_worldscale def get_worldscale(scene, as_scalematrix=True):
# todo: refactor common tests for all models, e.g. shape checking </s> scores = model.forward_owa(triples)	test_se batch_size = 16 triples = torch.zeros(batch_size, 3, dtype=torch.long) assert scores.shape == (batch_size, 1) scores = model.forward_cwa(triples[:, :2])
#todo same issue with batch_size </s> if len(self.inputs) == 0:	channels def channels(self): raise ValidationException("gan.channels() requested but no inputs provided") return self.ops.shape(self.inputs[0])[-1]
#todo: check sequence id </s> byte2int(packet_header[3])	_read_packet if DEBUG: dump_packet(packet_header) packet_length_bin = packet_header[:3] bin_length = packet_length_bin + b'\0'  # pad little-endian number bytes_to_read = struct.unpack('<I', bin_length)[0]
# todo: add a .parse() method that includes boths steps? </s> vi_cmd_data = self.parse_motion()	eval_cancel_action def eval_cancel_action(self): try: vi_cmd_data = self.parse_action(vi_cmd_data) self.next_mode = vi_cmd_data['_exit_mode']
# todo(rakhmerov): actionspec should be used instead of dict. </s> base_params_spec = self.action_spec.get('base-parameters')	_convert_params def _convert_params(self, params): if not base_params_spec: return {}
# todo if standby ready, just swap and continue. </s> await self._recover_from_changelog(table, assigned)	TableManager await self.app.consumer.pause_partitions(assigned) for table in self.values(): await self.app.consumer.resume_partitions({ tp for tp in assigned
# @todo: split into smaller routines </s> pacman_packages, aur_packages = find_repo_packages(args.positional)	cli_install_packages def cli_install_packages(args): new_aur_deps = find_aur_deps(aur_packages) print()
#todo: we may want to deal with error nicely </s> logging.debug('api timeout error : ' + api_url)	rest_api_call response = requests.get(api_url, auth=(request.user, request.user), timeout=1.0) except requests.exceptions.Timeout: return response
# todo: optimization </s> first_common_ancestor_idx = recent_blocks.index(block)	_check_chain_head else: raise Exception(f"No common ancestor found for block: {block.block_hash}") revoked_blocks = recent_blocks[(first_common_ancestor_idx + 1) :] reversed_new_blocks = tuple(reversed(new_blocks))
# time.sleep(20)  # todo: should remove after polling get. </s> res = res.reconstruct()	test_mpc_public_private_op res = op(mpc_tensor_1, public_value) res.block_with_timeout(secs=20) expected = op(value_1, public_value) assert (res == expected.child).all()
1/0  # conflict resolution todo </s> elif list_a[uid] != status[uid][0]:  # item update in a	sync if uid in list_a and uid in list_b: if list_a[uid] != status[uid][0] and list_b[uid] != status[uid][1]: prefetch_items_from_a.append(uid) actions.append(('update', uid, 'a', 'b'))
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_add_inputs_happy_path def test_add_inputs_happy_path(self): Adding a multisig input to a bundle. self.bundle.add_transaction( ProposedTransaction(
# todo: dry things, should be one function for sa and mongo </s> for i in range(rows):	f def f(rows): values = [] values.append({ 'title': 'title {}'.format(i),
# todo security </s> def new_facet(request, collection_id):	new_facet result = {'status': -1, 'message': 'Error'} solr_query = {}
pass # todo(denero) re-enable when self.inst is actually valid. </s> def test_entity_create_basic(self):	test_entity_create_basic
# todo assert cls.__tablename__ == '' </s> with dbcleanup(session, cls):	test_HistoryTagAssociation def test_HistoryTagAssociation(model, session, history, tag, user): cls = model.HistoryTagAssociation user_tname, value, user_value = 'a', 'b', 'c' obj = cls(user=user, tag_id=tag.id, user_tname=user_tname, value=value)
# todo: temporarily use paddle.nonzero instead of paddle.max </s> if len(paddle.nonzero(valid_label >= input.shape[-1])) > 0:	cross_entropy "Target({}) is out of class_dimension's lower bound({})". format(invalid_label[0], 0)) invalid_label = paddle.gather_nd( valid_label, paddle.nonzero(valid_label >= input.shape[-1]))
# todo: for backward compatibility only, remove if not used anymore </s> def _poll_job(self, job=none, key=none):	_poll_job return self.poll_job(job=job, key=key)
# todo: smarter behavior </s> sf.setselection(-1,-1)	OnSearchMouseAction if sf is None: return event.Skip()
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
from _devbuild.gen.id_kind_asdl import id  # todo: fix circular dep </s> return self.token.id == id.controlflow_return	IsReturn def IsReturn(self):
# todo(sahid): direct call from domain should </s> domain = guest._domain	_create_domain_and_network guest = self._create_domain( xml, pause=pause, power_on=power_on) self.firewall_driver.apply_instance_filter(instance, network_info)
# todo: calculate mu-sigma for f1, accuracy, and roc_auc and make it selectable </s> cv_results['judgment_metric'] = np.mean(cv_results['mu_sigmas'])	atm_cross_val_small_multiclass rank_accuracies=rank_accuracies, mu_sigmas=mu_sigmas) cv_results['judgment_metric_std'] = np.std(cv_results['mu_sigmas']) return cv_results
# todo: check u + v cubes for compatibility. </s> kwargs['_v_data'] = v_cube.data	quiver See :func:`matplotlib.pyplot.quiver` for details of other valid keyword arguments. return _draw_2d_from_points('quiver', _vector_component_args, u_cube, *args, **kwargs)
raise exception('lol') #todo fixme </s> project_dir = project_dir[:-1] if project_dir.endswith('/') else project_dir # strip trailing slash	_get_project_name def _get_project_name(project_dir): if not project_dir: project_name = os.path.split(project_dir)[-1] return project_name
# todo: actual implementation </s> return values.w_false	log_level @expose("log-level?", [values.W_Object, values.W_Object, default(values.W_Object, values.w_false)]) def log_level(logger, level, topic):
# todo: we may want to log this as soon as mobile ucr stops hitting this </s> if not isinstance(filter, dynamicchoicelistfilter):	get_choices def get_choices(data_source, filter, search_term=None, limit=20, page=0): return [] table = get_indicator_table(data_source)
# todo: amortized flow parameters </s> return mean_z, var_z	encode mean_z = self.q_z_mean(h) var_z = self.q_z_var(h)
# todo(ihrachys): replace with network.create() once we get an object </s> network = db_api.create_object(self.context, models_v2.network,	test_attach_network_get_network_policy obj = policy.QosPolicy(self.context, **self.db_obj) obj.create() {'name': 'test-network1'}) policy_obj = policy.QosPolicy.get_network_policy(self.context,
# todo: set fileinfo to a valid object. </s> self.assertraises(tuf.repositoryerror, _update_metadata, 'targets', none)	test_3__update_metadata _update_metadata = self.Repository._update_metadata self._mock_download_url_to_tempfileobj(self.release_filepath) self._mock_download_url_to_tempfileobj(self.targets_filepath) _update_metadata('targets', None)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: this needs test coverage. </s> return self.dates[self._country_group[lifetimes][end_date][:]]	asset_end_dates @lazyval def asset_end_dates(self):
# todo: assert len(args) == len(node.defn.type_vars) </s> return instance(node, args)	named_type assert isinstance(node, TypeInfo) if args: return Instance(node, [AnyType()] * len(node.defn.type_vars))
# todo: check arp reply is valid </s> self.asserttrue(self.packet_outs_from_flows(arp_replies), msg=arp_mac)	test_arp_for_controller 'arp_source_ip': '10.0.0.1', 'arp_target_ip': '10.0.0.254'})
# todo check axises </s> in_conn = (a.sum(axis=1)>0).nonzeros()	_check_connectivity_directed if r_is_connected: break out_conn = (A.sum(axis=2)>0).nonzeros() if c_is_connected and r_is_connected:
# todo: try to find a better way to deal with local execution </s> if self.is_method and not self.locations and self.owner == sy.hook.local_worker:	_execute_readable_plan def _execute_readable_plan(self, *args): result_ids = [sy.ID_PROVIDER.pop()] self._self.send(sy.hook.local_worker, force_send=True) plan_res = self.execute_plan(args, result_ids)
# todo add tests when funcs are not set in lambda </s> assert_cbaois_equal(observed, bbsoi)	test_bounding_boxes_empty observed = aug.augment_bounding_boxes(bbsoi)
# todo: get_style and word_to_glyphs may need proper implementations </s> pass	before_placing def before_placing(self, container):
# todo: probably replace class_labels list with some custom object </s> valid_entries_class_labels = class_labels[:-1]	get_labels_from_annotation labels_2d_stacked : Tensor of size (width, height, num_classes). Tensor with labels for each pixel. labels_2d = map(lambda x: tf.equal(annotation_tensor, x), valid_entries_class_labels)
# todo applicative log, database tracking of queue </s> continue	operation itip_info['context_gus'], plugin_type) if receiver_conf is None: related_profile = yield profile_iface.get_single(receiver_conf['profile_gus']) settings_dict = { 'admin_settings' : related_profile['admin_settings'],
# todo: may test file contents </s> temp = tempfile.namedtemporaryfile(delete=false)	test_export_to_csv_fobj def test_export_to_csv_fobj(self): self.files_to_delete.append(temp.name) rows.export_to_csv(utils.table, temp.file)
# todo: we can't disable this for normal, because folders don't </s> if all(s not in (state_none, state_ignored) for s in states):	get_valid_actions valid_actions.add('update') valid_actions.add('push') valid_actions.add('commit') if all(s not in (STATE_NORMAL, STATE_REMOVED) for s in states):
# todo: when this is changed to be a view, use loadselect2mixin instead of this </s> select2_css = getattr(settings, 'select2_css_url', none)	problem_submit form.fields['source'].widget.mode = form_data['language'].ace form.fields['source'].widget.theme = profile.ace_theme select2_js = getattr(settings, 'SELECT2_JS_URL', None) has_select2 = select2_css is not None and select2_js is not None
#todo : include with the service cache. </s> service_id = self.mapping_service_id[s_id]	manage_initial_servicegroup_status_brok res = [query] for (s_id, s_name) in b.data['members']: servicegroup_members_data = {'instance_id' : data['instance_id'], 'servicegroup_id' : svcgp_id, 'service_object_id' : service_id}
# todo: get indexed value before transferring the variable (might be much smaller) </s> if indices is not none:	_get_input_from_src else: val = val.reshape(vmeta['shape']) val = val[indices] if units is not None:
# todo: campid 939921818394902983238 </s> if frame is not none:	importSym elif line.startswith('CycleTime'): frame.addAttribute("GenMsgCycleTime", line.split('=')[1].strip()) if len(multiplexValTable) > 0: frame.signalByName(frame._name + "_MUX")._values = multiplexValTable
# todo(ja): reclaiming space should be done lazy and low priority </s> self._try_execute('sudo', 'dd', 'if=/dev/zero',	VolumeDriver except Exception as e: return True 'of=%s' % self.local_path(volume), 'bs=1M')
# todo: remove in 1.3 </s> with pytest.warns(futurewarning, match="the 'verbose' parameter"):	test_imputation_deletion_warning X[:, 0] = np.nan imputer = SimpleImputer(strategy=strategy, verbose=1) imputer.fit(X) with pytest.warns(UserWarning, match="Skipping"):
# todo: refactor: this list of geotags is only used to determine if we need to show the geotag map or not </s> pack_geotags = sound.public.select_related('license', 'pack', 'geotag', 'user', 'user__profile').filter(pack=pack).exclude(geotag=none)	pack qs = Sound.objects.select_related('pack', 'user', 'license', 'geotag').filter(pack=pack, moderation_state="OK", processing_state="OK") num_sounds_ok = len(qs) google_api_key = settings.GOOGLE_API_KEY if num_sounds_ok == 0 and pack.num_sounds != 0:
"""todo doc me""" </s> table = [['foo', 'bar', 'baz'],	test_profile_default def test_profile_default(): ['A', 1, 2], ['B', '2', '3.4'],
# todo(leofang): test newer rocm versions </s> if (self.axes == (0, 1) and self.shape == (2, 3, 4)):	setUp def setUp(self): if cupy.cuda.runtime.is_hip: raise unittest.SkipTest("hipFFT's PlanNd for this case "
# todo maybe add the ability to disable this in settings ("identification" option, "basic" or "accurate") </s> extended = cls.get_parser().parse(file_name)	get_episodes continue file_name = os.path.splitext(os.path.basename(parts[0].get('file')))[0] identifier = cls.get_chain_identifier(extended.chains[0].info) if extended.chains else None season, episodes = cls.get_episode_identifier(video, identifier)
# todo: return errors in a universal way </s> subprocess.run(["nasm", "-f", "elf64", "-o", obj_name, asm_name]).check_returncode()	assemble_and_link asm_name (str) - name of the assembly file to read in obj_name (str) - name of the obj file to output subprocess.run(["ld", obj_name, "-o", binary_name]).check_returncode()
# todo: more error checking on the kernel to make sure it doesn't </s> f_ir = numba.ir_utils.get_ir_of_code({}, func_node.code)	_handle_rolling_apply_func raise ValueError( "cannot find kernel function for rolling.apply() call") kernel_func = numba.compiler.compile_ir( self.typingctx,
# todo: move it via south </s> db.execute('''	forwards pub.`content_type_id` = (SELECT ct.`id` FROM `django_content_type` ct WHERE ct.`app_label` = '%(app)s' AND  ct.`model` = '%(mod)s'); ) ALTER TABLE `articles_article` DROP FOREIGN KEY `photo_id_refs_id_573d4575`; for column in ['title', 'category_id', 'photo_id', 'source_id', 'slug', 'id', 'perex']:
# todo results from ml </s> return str(endpoint.metadata)	_get_device_type @staticmethod def _get_device_type(endpoint):
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_min_weight_magnitude_too_small def test_fail_min_weight_magnitude_too_small(self): ``min_weight_magnitude`` is < 18.
# todo: does this make sense? </s> messager.warning("project configuration: term %s matches multiple types (incl. '%s' and '%s'). configuration may be wrong." % (t, d[t].storage_form(), e.storage_form()), 5)	get_drawing_config_by_storage_form t = n.storage_form() if t in d: d[t] = {} for a in n.arguments:
# todo: issue #503 make pipx commands have proper exit codes </s> return 0	run_pipx_command force=args.force, ) elif args.command == "upgrade": return commands.upgrade(
# todo: there must be a better way to do this. otherwise sometimes a file </s> files = tf.gfile.glob('{}/events.out.tfevents*'.format(self.plugin_logdir))	write_summary image_tensor)) e = time.time() for file in files: tf.gfile.Remove(file)
# todo also check for motion codec parameter support </s> return 'h264_omx' in codecs	has_h264_omx_support if not binary: return False
# todo add tests </s> def normalize_random_state(random_state):	normalize_random_state Normalize various inputs to a numpy random state. Parameters
# todo: remove this method in v2.5 </s> elif self._values['disabled'] in booleans_false:	disabled elif self._values['disabled'] in BOOLEANS_TRUE: return True return False elif self._values['enabled'] in BOOLEANS_FALSE:
# todo: mock these so no network access is required </s> p = package(	test_launchpad_pull def test_launchpad_pull(self): title="Django-PreFlight", slug="django-preflight",
# todo: add batch args </s> logger.info("-----  depparse   ----------")	run_ete logger.info("Running identity lemmatizer step with args: {}".format(lemma_args)) identity_lemmatizer.main(lemma_args) depparse_output = f"{ete_dir}/{short_name}.{dataset}.depparse.conllu" depparse_args = ['--wordvec_dir', wordvec_dir,
# todo: implement me </s> self.logmessage("%s %s" % (msg.__class__, vars(msg)), 4)	ChildDepth def ChildDepth(self, msg):
# todo(#28) throw a configerror once it will be caught... </s> log_user("cannot load emulator for %s. check that your spelling is correct, and that the emulator you request is supported" % console.shortname)	lookup_emulator return None except AttributeError as e: log_file("Error loading [%s] %s" % (emulator_key, console_key)) return None
# todo: remove original subject and object from entity set </s> pos_batch = pos_batches[i]	train_trans_x_model current_epoch_loss = 0. for i in range(len(pos_batches)): current_batch_size = len(pos_batch) batch_subjs = pos_batch[:, 0:1]
# todo vary by supply point type? </s> return {	get_threshold def get_threshold(type): 'low': UNDERSTOCK_THRESHOLD, 'high': OVERSTOCK_THRESHOLD,
# todo - unittest this </s> parts = []	make_run_script_for_args def make_run_script_for_args(args): for arg in args: if arg == '--install':
# todo: we can't disable this for normal, because folders don't </s> actions["vccommit"] = all(s not in (	update_actions_for_paths states = path_states.values() actions["VcCompare"] = bool(path_states) _vc.STATE_NONE, _vc.STATE_IGNORED) for s in states) actions["VcUpdate"] = True
# todo: figure out the right thing to do here </s> return mu_1 + mu_2, np.array([0.0, 0.0, 1.0, 0.0, 0.0, 1.0], dtype=np.float32)	uncertainty_embedding_grad det = sigma_11 * sigma_22 - sigma_12 * sigma_21 if det == 0.0: cross_term = sigma_12 + sigma_21 m_dist = sigma_22 * (mu_1 ** 2) - \
# todo: preallocate! </s> i, j, v = [], [], []	nodalGrad if getattr(self, '_nodalGrad', None) is None: self.number() edges = self.faces if self.dim == 2 else self.edges for edge in edges:
# todo(haoyuzhang): set config properly in tf2.0 when the config api is ready. </s> dtype = flags_core.get_tf_dtype(flags_obj)	run sess = tf.Session(config=config) tf.keras.backend.set_session(sess) if dtype == 'fp16': raise ValueError('dtype fp16 is not supported in Keras. Use the default '
# todo implement w regex </s> for c in word:	has_letter def has_letter(word): if c.isalpha(): return True
while 1:  # todo: speed this up </s> if icase == self.ncases:	on_cycle_results return icase = self.icase + 1 icase = 0 try:
# todo, pass complete checkpoint as state dictionary </s> mp_queue.put(best_model_path)	transfer_distrib_spawn_state_on_fit_end if self.trainer.global_rank == 0 and mp_queue is not None: rank_zero_warn('cleaning up ddp environment...') mp_queue.put(results) last_path = None
# todo  the model predictions changed after update to posthoc sparsity. need to investigate. </s> test_cfs = [[72.0, 'private', 'hs-grad', 'married', 'white-collar', 'white', 'female', 45.0, 0.691], [29.0, 'private', 'prof-school', 'married', 'service', 'white', 'male', 45.0, 0.954], [52.0, 'private', 'doctorate', 'married', 'service', 'white', 'female', 45.0, 0.971], [47.0, 'private', 'masters', 'married', 'service', 'white', 'female', 73.0, 0.971]]	test_final_cfs_and_preds dice_exp = self.exp.generate_counterfactuals(sample_adultincome_query, total_CFs=4, desired_class="opposite")
return none  # todo: mypy shouldn't require this </s> warnings.warn(str(exc))	get_username_from_keyring except Exception as exc:
# todo: log discarded bytes? </s> return 'request discarded due to invalid crc.'	decode_in if not self.valid_crc(self.in_data[1:]): self.in_parsing = False if self.in_data[2] in self.command_map: return self.command_map[self.in_data[2]]()
raise notimplementederror  # todo </s> else:	forward posteriors = result[0] elif BackendEngine.is_tensorflow_selected(): raise NotImplementedError("unknown backend engine") if posteriors.ndim == 3:
# todo don't use intermediate schematic... </s> export = self.currentclone.sourcedim.exportschematiciter(self.currentclone.selection)	confirmClone command = CloneFinishCommand(self, self.currentClone) with command.begin(): schematic = showProgress("Copying...", export) dim = schematic.getDimension()
# todo: remove cache clearing once upstream issues regarding non-batch </s> acquisition_function.model.train()	gen_candidates Tensor: The set of generated candidates Tensor: The acquisition value for each t-batch. acquisition_function.model.eval() options = options or {}
# todo, complete when the method becomes available, create proper response code </s> pass	delete_playlist_item validate_delete_data(data) try: except Exception as e: current_app.logger.error("Error while adding recordings to playlist: {}".format(e))
# todo: add this to help output. </s> merge_edit_command_parser.add_argument(	Main help=u'the github origin to merged e.g. username:feature.') merge_edit_command_parser = commands_parser.add_parser(u'merge-edit') u'github_origin', action=u'store', metavar=u'GITHUB_ORIGIN', default=None,
# todo: non-int dict </s> func_text += "    key_write_map = hpat.dictintint()\n"	gen_agg_iter_func func_text += "    out_key = np.empty(n_uniq_keys, np.{})\n".format( key_typ.dtype) func_text += "    curr_write_ind = 0\n" func_text += "    for i in range(len(key_arr)):\n"
# todo: skips header parsing </s> iline += 1	read_abaqus_inp iline, line0 = self.read_step(lines, iline, line0) elif word.startswith('initial conditions'): line0 = lines[iline].strip().lower() data_lines = []
# todo(jfs): detect the case of a connection timeout </s> if i >= self.__class__._attempts_push - 1:	_chunk_push volume_id, container_id, content_id, chunk_id, **args) except ConnectionError: raise
# todo fixme </s> return date <= time.time()	date_passed def date_passed(date):
uri=none,  # todo: </s> app_info=none,	_make_dataset extent=task['data']['geobox'].extent, center_time=labels['time'], valid_data=None) return dataset
# todo: implement </s> pass	do_query_completions export_module = Autocomplete.EXPORT_MODULE_RE.search(line_contents) if export_module: else: completions = self.autocompleter.get_completions(view, locations)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_pass_tags_only def test_pass_tags_only(self):
# todo: uncomment to enable claiming </s> ], prefix='/api/v1',)	make_url_map Rule('/settings/names/', 'post', profile_views.post_names, json_renderer), Rule('/profile/<user_id>/summary/', 'get', profile_views.get_profile_summary, json_renderer), process_rules(app, [
#todo: do something like </s> player_id = 4 #yellow	create_slp_pngs base_slp_path = os.path.join(base_graphics_path, "%06d.slp" % slp_file.file_id) os.makedirs(base_slp_path) for frame in slp_file.get_frames():
# todo: support all tzinfo subclasses by calling utcoffset() </s> if date_time.tzinfo is not none and\	datetime_obj_to_dtstruct dt.time.offset = 0 dt.time.ok = '\1' date_time.tzinfo.__class__ is TZFixedOffset: dt.time.offset = date_time.tzinfo.offset
except exception as exc:  # todo </s> import pdb; pdb.set_trace()	reset_rabbit_connections try: rabbit_manager.delete_connection(connection['name']) if exc.status == 404: pass  # connection closed in a race
# todo - verify contents </s> self.assertequal(response.status_code, 200)	testGroupList response = self.client.get('/groups/')
# todo replace w/ something smart (sun/earth special cased) </s> r = equatorial_coo.cartesian.xyz	from_equatorial @staticmethod def from_equatorial(equatorial_coo, fixed_frame): ra, dec, W = fixed_frame.rot_elements_at_epoch(equatorial_coo.obstime) icrs_frame_pos_coord, icrs_frame_vel_coord = get_body_barycentric_posvel(
# todo: log exception </s> return none	scan clamScanner = _connect_clam() except Exception as e: for f in filelist: output = clamScanner.scan_file(f)
# todo: async </s> self.session.raw_message(data.decode('utf-8', 'replace'))	post print 'Unauthorized' raise HTTPError(401, 'unauthorized') self.write('ok') self.finish()
# todo (pp): notifications don't have id; process all </s> if os.path.realpath(errors.file) != os.path.realpath(v.file_name()):	show_errors An instance of `ErrorInfoCollection`. v = sublime.active_window().active_view() _logger.debug('different view active - aborting') return
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
pass # todo </s> def test_end_of_data_stream(self):	test_end_of_data_stream @SkipTest
# todo(mattjj, dougalm): make this work if we can, and delete subsequent test </s> grad(lambda x: np.real(np.sin(x)))(1 + 2j)  # doesn't crash	test_complex_grad_raises_error self.assertRaises(TypeError, lambda: grad(lambda x: np.sin(x))(1 + 2j))
# todo: test more stuff! </s> assert str(classifier._tiktorch_net.model) == str(self.tiktorch_net.model)	test_classifier_deserialization classifier = TikTorchLazyflowClassifier.deserialize_hdf5(self.classifier_group)
# todo: factor this out, particularly if there are other places using </s> def ellipse(semimajor=2, semiminor=1, easting=0, northing=0, n=200):	Stereographic proj4_params.append(('lat_ts', true_scale_latitude)) super(Stereographic, self).__init__(proj4_params, globe=globe) t = np.linspace(0, 2 * np.pi, n) coords = np.vstack([semimajor * np.cos(t), semiminor * np.sin(t)])
pass # todo </s> def _status_noidle(self):	_status_noidle @handle_pattern(r'^noidle$')
if branch.commit == upstream.commit:  ### todo: a better check is possible </s> print(" up to date.")	_update_branch print(" skipped: no upstream is tracked.") return return branch.checkout()
# todo(nnorwitz): store kind of inheritance...maybe. </s> if token.name not in ('public', 'protected', 'private'):	_GetBases token = self._GetNextToken() assert token.token_type == tokenize.NAME, token self._AddBackToken(token) base, next_token = self.GetName()
# todo: dns server is just a listener. we should allow the option of backwards communication. </s> d = s.recvfrom(1024)	ntp_listener raise while 1: data = d[0] addr = d[1]
'''todo: add docs''' </s> self.model.update()	update_app def update_app(self): for v in self.views: v.update()
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
# todo: config option? </s> lgr.info(	_install_subds_from_flexible_source if subrepo.get_merge_base( [branch_hexsha, detached_hexsha]) == detached_hexsha: "Submodule HEAD got detached. Resetting branch %s to point " "to %s. Original location was %s",
# todo: what about alpha (rgba)? </s> output_type = pyluxcore.filmoutputtype.rgb_imagepipeline	_refresh_denoiser denoiser_pipeline_props = get_denoiser_imgpipeline_props(scene, denoiser_pipeline_index) session.Parse(denoiser_pipeline_props) was_paused = session.IsInPause() if not was_paused:
# todo: deprecate </s> model = self.get_model()	run_evaluation def run_evaluation(self, test_mode: bool = False): self.evaluation_loop.testing = test_mode model.on_pre_performance_check() if test_mode:
# todo: separate this string to describe what each step is doing, then build it for the final command </s> connect_back_cmd = 'echo "echo {}" > /dev/shm/{} && chmod 777 /dev/shm/{} && display=dummy ssh_askpass=/dev/shm/{} setsid ssh -o userknownhostsfile=/dev/null -f -n -r 8001 -ostricthostkeychecking=no {}@{} >/dev/null 2>&1 &'.format(proxy_ssh_password, shm_name, shm_name, shm_name, proxy_ssh_username, proxy_ip)	parse_proxy_command else: shm_name = ''.join(random.choices(string.ascii_lowercase + string.ascii_uppercase + string.digits, k=5)) self.server.run_cmd(proxy_target_agent[0], self.server.all_connections[int(command[2])], connect_back_cmd) self.print('Remote agent instructed to connect!')
# todo (#2743, see also #2556): make a portable constant or remove completely </s> sig_header = not_signed	author signature=signature_in_kit) else: capsule, ciphertext = umbral.encrypt(recipient_key, sig_header + plaintext) message_kit = cls(ciphertext=ciphertext, capsule=capsule)
# todo: add js compress </s> return u'<script type="text/javascript">{}</script>'.format(js)	process def process(js):
# todo(dcramer): when we hit a notfound in the queue, maybe we should </s> try:	_sync_build_from_queue def _sync_build_from_queue(self, build, entity): build_item = entity.data item = self._get_response('/queue/item/{}'.format( build_item['item_id']))
raise mpdnotimplemented # todo </s> def _commands(self):	_commands @register(r'^commands$')
# todo: use pybossa uploader! only for debugging: </s> open('/tmp/%d_%s_task_run_csv.zip' % (app.id, name), 'wb').write(memzip.getvalue())	export_csv memfile.write(str(line)) memzip = make_onefile_memzip(memfile, '%s_task_run.csv' % name)
# todo -- can we do this without a subscription? </s> if not api.use_store:	play_similar_song_radio @ask.intent("GeeMusicPlaySimilarSongsRadioIntent") def play_similar_song_radio(): return statement(render_template("not_supported_without_store")) if len(queue.song_ids) == 0:
# todo: support multiple. </s> return res['mutate_response'][0]['id']	add_podcast_series res = self._make_call(mutate_call, update_mutations)
# todo: handle large files </s> target.hdfs.create_file(target.path.lstrip('/'), f.read())	append_local_file_to_hdfs except FileNotFound: with open(source.path) as f: return target
# todo pv 类型判断 </s> if not (isinstance(pv, int) and sv == pv):	in_json code = 1  # 键值不等 elif isinstance(sv, int): code = 1  # 键值不等 elif isinstance(sv, float):
# todo resume admin log? </s> chunks_left = dumper.max_chunks	save_admin_log __log__.info('Starting admin log dump for %s', utils.get_display_name(target)) entity_downloader = _EntityDownloader( self.client,
llen = len(' '.join(wordsplit[-2:])) #todo: what if 2 spaces between these words? </s> return '%s%s' % (word[:-llen],	_plnoun pl_sb_irregular[lowerwordlast]) if (' '.join(wordsplit[-2:])).lower() in pl_sb_irregular_compound.keys(): pl_sb_irregular_compound[(' '.join(wordsplit[-2:])).lower()]) mo = search(r"(%s)$" % pl_sb_U_man_mans, word, IGNORECASE)
# todo: don't write them? is *much* slower on re-load (~3x) </s> try:	update_lib fh.flush() os.fsync(fh.fileno())  # flush to disk os.unlink(self.lib_path + 'c') except OSError:
# todo: no testpath exercises this code... </s> log.debug("starting thread...")	AnimThread self.do_run = do_run def run(self): self.do_run() log.debug("Thread Complete")
annot.annotation_metadata.annotation_rules = "todo"  # todo </s> annot.annotation_metadata.validation_and_reliability = "todo"  # todo	fill_annoatation_metadata annot.annotation_metadata.version = "1.0" annot.annotation_metadata.annotation_tools = "Sonic Visualizer" annot.annotation_metadata.origin = "Centre for Digital Music" annot.annotation_metadata.annotator.name = "TODO"
# xxx todo </s> section = "filtering"	write_filtering_config Write configuration options in section "filtering".
#todo - find a better way to handle this </s> _init_d = {objtypes.booltt: ast.literal.false,	_createDeclarations newvars = [var for var in info if isinstance(var, ast.Local) and info[var].declScope is None] remaining = set(newvars) objtypes.IntTT: ast.Literal.ZERO, objtypes.FloatTT: ast.Literal.FZERO,
# todo: test me. </s> @action.setter	VintageState def action(self): return self.settings.vi['action'] def action(self, action): stored_action = self.settings.vi['action']
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: need to close computations on this node? </s> node.clusters.clear()	run_job cluster.status_callback(DispyJob.Cancelled, dispy_node, njob.job) node.pending_jobs = [] self._nodes.pop(node.ip_addr, None) if self._sched_jobs.pop(_job.uid, None) == _job:
# todo: fails because of missing svg support </s> assert_pixels('inline_image_' + filename, 8, 8, image, '''	test_images )) def test_images(filename, image): <style> @page { size: 8px }
# todo: resolve possible conflicts with sysctl settings from other plugins </s> def __init__(self, *args, **kwargs):	SysfsPlugin class SysfsPlugin(base.Plugin): Plugin for applying custom sysfs options, using specific plugins is preferred. super(self.__class__, self).__init__(*args, **kwargs) self._has_dynamic_options = True
# todo unify </s> if num_components == 1:	_write_node_data else: fmt = ' '.join(['{}'] + ['{!r}'] * num_components) + '\n' for k, x in enumerate(point_data[key]): fh.write(fmt.format(k+1, x).encode('utf-8'))
# todo: log exception </s> pass	_start_module_threads conf = mod.DEFAULTCONF except Exception as e: thread = _Thread( target=_run_module,
# todo: remove: legacy function </s> return sorted(	get_lowest_content_category def get_lowest_content_category(): [ (cat.published_product_count, cat)
# todo: require tests </s> return true	hessian_is_zero def hessian_is_zero(self):
# todo: this is untested. </s> _raise_current_error()	load_certificate_request raise ValueError("type argument must be FILETYPE_PEM or FILETYPE_ASN1") if req == _ffi.NULL: x509req = X509Req.__new__(X509Req) x509req._req = _ffi.gc(req, _lib.X509_REQ_free)
# todo :: move arbitrary path construction to storagelayout object </s> url = '{0}/tar_partitions/part_{number:08d}.tar.lzo'.format(	__call__ tpart.tarfile_write(pl.stdin) tf.flush() self.backup_prefix.rstrip('/'), number=tpart.name) logger.info(msg='begin uploading a base backup volume',
# todo: add support for more colors </s> key_colors = ["white", "lightblue", "skyblue", "blue", "navy"]	_get_layers frame["cmap"] = frame["cmap"][0] quantiles = np.percentile(mapping.color, [0, 25, 50, 75, 100]) legend["color"] = dict(zip(key_colors, quantiles)) layers.append(frame)
# todo: test for the _correct_ revision_id value. </s> if not activity.revision_id:	_update_resource if not activity.id: assert False, "activity object has no id value" assert False, "activity has no revision_id value" assert activity.timestamp >= before and activity.timestamp <= after, \
# todo: only handle it if the trigger refers to the current sensor </s> trigger = self._sanitize_trigger(trigger=trigger)	_handle_delete_trigger def _handle_delete_trigger(self, trigger): self._sensor_instance.remove_trigger(trigger=trigger)
# todo: reconsider adding smth to data_ to be yielded" </s> yield data_	_initiate_handle source=data_['handle_path'], )
# todo: put offset/data in main structure since immutable </s> ctinfo = context.make_helper(builder, data_ctypes_type)	get_data_ptr_ind string_array = context.make_helper( builder, string_array_type, in_str_arr) ctinfo.data = builder.gep(string_array.data, [ind]) ctinfo.meminfo = string_array.meminfo
# todo nice error recovery </s> print("uh. not good")	compress_file self.update_table() else:
# todo(b/142683826): beam type check error in </s> def key_by_query_id(	ComputeQueryBasedMetrics missing_query_id_counter = beam.metrics.Metrics.counter( constants.METRICS_NAMESPACE, 'missing_query_id') extract: types.Extracts, query_id: Text) -> 'Optional[Tuple[Text, types.Extracts]]':
# todo check if we can avoid that </s> out = numpy.zeros(ntet)	read_buffer out = numpy.fromfile( f, count=ntet * 4, dtype=int, sep=" ").reshape(ntet,4) cells["tetra"] = out - 1 cell_data["tetra"] = {"ugrid:ref": out} if npyra > 0 :
# todo: disconnect </s> return	Node except asyncio.TimeoutError: await stream.reset() self.logger.debug(f"Received the hello message {hello_other_side}") if not (await self._validate_hello_req(hello_other_side)):
# todo: confirm necessity of this session clearing and lay out mechanics. </s> tuf.download._sessions = {}	test_https_connection try: os.environ['REQUESTS_CA_BUNDLE'] = bad_cert_fname logger.info('Trying HTTPS download of target file: ' + bad_https_url) with self.assertRaises(requests.exceptions.SSLError):
# todo: http://www.6502.buss.hk/6502-instruction-set/jmp says that there </s> self.assertequals(code, [0x4c, 0x34, 0x12])	test_jmp_abs code = semantic(ast)
# @todo: use real lightness from hsv or lab color model </s> return sum([	is_dark def is_dark(color_text): hex_to_int(channel_text) for channel_text in color_list_from_hex(color_text)
# todo(ericbidelman) - memcache this calculation as part of models.py </s> milestones = range(1, latest_version + 1)	get LATEST_VERSION = int(s.split('.')[0]) break milestones.reverse() versions = [
print "usage is not ready yet." # todo </s> def usage():	usage
# todo: remove cache clearing once upstream issues regarding non-batch </s> acq_function.model.train()	q_batch_initialization X[i,:] has one of the torch_batches highest values of acq_function(X[i,:]) acq_function.model.eval() bulk_X = torch.cat([gen_function(q).unsqueeze(0) for i in range(multiplier)], dim=0)
# todo(developer): uncomment these lines and replace with your values. </s> if not project:	execute_workflow from google.cloud.workflows import executions_v1beta from google.cloud.workflows.executions_v1beta.types import executions raise Exception('GOOGLE_CLOUD_PROJECT env var is required.') execution_client = executions_v1beta.ExecutionsClient()
raise valueerror("bucket might not exist")  # todo: create custom exception for easier handling </s> if self.aws_creds_configured:	check_perm_write_acl def check_perm_write_acl(self, bucket): if bucket.exists != BucketExists.YES: readURI = 'uri=http://acs.amazonaws.com/groups/global/AuthenticatedUsers' bucketPerm = bucket.AuthUsersWriteACP
# todo should also include meta-chunk-hash </s> trailers = {'x-oio-chunk-meta-metachunk-size': metachunk_size}	_cycle_put del headers[h] metachunk_size = 9 * length self._check_not_present(chunkurl) resp, body = self._http_request(chunkurl, 'PUT', chunkdata, headers,
# todo: should we do check/modify it? wireshark shows the only msb to 0 </s> self.set_word(2, nb, "<")	set_aid "Set the 802.11 PSPoll control frame 'AID' field" nb = value & 0xFFFF
# todo: e_die with token </s> raise nameerror('undefined')	ToAny def ToAny(self, val): if val.tag == value_e.Undef: if val.tag == value_e.Str: return val.s
# todo fixme make sure we have exclusive write lock </s> alala = alala(db_path, type_)	dbcache_worker logger = get_dbcache_logger() db_path = Path(db_path) engine = alala.engine prev_hashes = engine.execute(alala.table_hash.select()).fetchall()
pass  # todo: implement this </s> if current_order != pre_drop_order:	dropEvent current_order = {text.child(i):i for i in xrange(text.childCount())}
# todo: use operators.translate() </s> ghat = np.dot(g.u.t, g)	gft_windowed_normalized C = np.reshape(C, (G.N, G.N), order='F') else: Ftrans = np.sqrt(G.N)*np.dot(G.U, (np.kron(np.ones((1, G.N)), ghat)*G.U.T)) C = np.empty((G.N, G.N))
raise notimplementederror  # todo... </s> elif self.wrap_mode != "wrap_around":	perform beam = beam_trans.transpose(*map(array_trans_dims_order.index, range(array.ndim))) if self.wrap_mode == "pad_zero": raise NotImplementedError beam_out[0] = beam
# todo: implement this method </s> return self._ref	Handle @property def ref(self): @ref.setter def ref(self, value):
# todo: these reductions could be delayed until _step is called. </s> loss = self._strategy.reduce(tf.distribute.reduceop.mean, per_replica_loss, none)	_run per_replica_loss, per_replica_words = self._strategy.experimental_run_v2( _accumulate_gradients, args=(per_replica_source, per_replica_target)) num_words = { k:self._strategy.reduce(tf.distribute.ReduceOp.SUM, v, None)
# todo(tobyboyd): remove when contrib.distribute is all in core. </s> if hasattr(tf, 'contrib'):	undo_set_up_synthetic_data def undo_set_up_synthetic_data(): _undo_monkey_patch_dataset_method(tf.distribute.MirroredStrategy) _undo_monkey_patch_dataset_method(tf.contrib.distribute.MirroredStrategy) _undo_monkey_patch_dataset_method(tf.contrib.distribute.OneDeviceStrategy)
# todo: remove the following line when issue #71 (preserve the trajdataframe index during preprocessing operations) is solved. </s> ctdf.reset_index(inplace=true, drop=true)	compress else: ctdf = _compress_trajectory(tdf, spatial_radius=spatial_radius_km) ctdf.parameters = tdf.parameters ctdf.set_parameter(constants.COMPRESSION_PARAMS, arguments)
# todo add installation logic for torch </s> packages.append(key + '==' + version)	create_experiment if (key == 'tensorflow' or key == 'tf-nightly'): key = key + '-gpu' return Experiment( key=key,
pass  # todo </s> def playlist_delete(self):	playlist_delete @test(depends_on_groups=['playlist'], always_run=True)
# todo/fixme: x and alpha should not contain observed values!! check that. </s> cost = 0	bound_rotate_gaussian_ard p(X|alpha) = prod_m N(x_m|0,diag(alpha)) p(alpha) = prod_d G(a_d,b_d) mask = X.mask[...,np.newaxis,np.newaxis] XX = utils.utils.sum_multiply(X.get_moments()[1],
# todo ... </s> if isinstance(token, cclosingbracket):	cpre3_parse curCObj = _CBaseWithOptBody(parent=parent) elif state == 40: # union if token.brackets == curCObj._bracketlevel: state = 0
raise notimplementederror #todo </s> elif q.kw(i,"format"):	parse while i < l: if q.kw(i,"RETURN"): raise NotImplementedError #TODO elif q.kw(i,"REQUEST"):
#todo: maybe it would be cleaner to have </s> info = playerinfo(dict(categories={	test_dict_type_with_model_type class PlayerInfo(Model): categories = DictType(ModelType(CategoryStats)) "math": { "category_slug": "math",
# todo: use a recursive memoized __eq__ if this ever shows up in profiles. </s> and self.dependencies == other.dependencies	__eq__ self._hashcode == other._hashcode and self.members == other.members
# rfc822.header will give *all* of the headers, and is probably slower? // todo </s> log.info("fetching messages with query: %s" % query )	fetch_bodystructure global server query = 'ENVELOPE BODY INTERNALDATE' messages = server.fetch(UIDs, [query, 'X-GM-THRID']) log.info("  ...found %i messages." % len(messages.values()))
# todo: read until ''' </s> raise assertionerror()	_PushOilTokens else: sq_mode = lex_mode_e.SQ_Raw continue else:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
raise notimplementederror #todo </s> elif q.kw(i,"format"):	parse while i < l: if q.kw(i,"RETURN"): raise NotImplementedError #TODO elif q.kw(i,"REQUEST"):
# todo: make keys get passed through files or environment </s> inbuf = tempfile.namedtemporaryfile()	decrypt def decrypt(self, data, key): inbuf.write(data) inbuf.flush()
data.append(get_casedb_schema(app))  # todo use domain instead of app </s> except exception as e:	get_form_data_schema data.append(get_session_schema(form)) if form and form.requires_case(): return HttpResponseBadRequest(e) data.extend(
# todo(lyarwood): remove the following in 16.0.0 pike </s> self._test_get_encryptor('luksencryptor',	test_get_encryptors self._test_get_encryptor('luks', luks.LuksEncryptor) luks.LuksEncryptor) self._test_get_encryptor('nova.volume.encryptors.luks.LuksEncryptor',
# todo: enable custom config </s> tmpmatches.sort(key=lambda x: len(x['word']))	_refresh_completions continue tmpmatches.append(e) matches += tmpmatches except Exception as inst:
# todo: instead of raising, we should do something (#957) </s> if reencryption_signature.verify(bytes(cfrag), ursula_verifying_key):	complete if not metadata_as_signature.verify(metadata_input, ursula_verifying_key): raise InvalidSignature(f"Invalid metadata for {cfrag}.") good_cfrags.append(cfrag) else:
# xxx todo </s> if args.has_key('done') and args['done']:	__init__ DnsRequest.__init__(self, *name, **args) asyncore.dispatcher_with_send.__init__(self, *name, **args) self.donefunc = args['done'] else:
# todo: skips header parsing </s> line0 = lines[iline].strip().lower()	read_part line0 = lines[iline].strip().lower() elif '*mass' in line0: data_lines = [] while not line0.startswith('*'):
# todo: seems like this just does everything that handle_exception does but not as well. </s> repeat_record.handle_payload_exception(e)	fire_for_record payload = self.get_payload(repeat_record) except Exception as e: raise url = self.get_url(repeat_record)
""" todo: better test here """ </s> timestamp = generate_timestamp()	test_generate_timestamp def test_generate_timestamp(self): self.assertTrue(isinstance(timestamp, unicode)) self.assertTrue(int(timestamp))
# todo: don't use getbrailletextforproperties directly. </s> braille.handler.message(braille.getbrailletextforproperties(name=self.name, role=self.role))	event_show if (config.conf["presentation"]["reportTooltips"] and (self.IAccessibleRole==oleacc.ROLE_SYSTEM_TOOLTIP)) or (config.conf["presentation"]["reportHelpBalloons"] and (self.IAccessibleRole==oleacc.ROLE_SYSTEM_HELPBALLOON)): speech.speakObject(self,reason=speech.REASON_FOCUS)
# todo(jd) add an exception in oslo.db to match foreign key </s> if isinstance(e.inner_exception,	update_resource name=name)) except exception.DBError as e: sqlalchemy.exc.IntegrityError): raise indexer.NoSuchEntity(None)
# todo: pass expect parameters from above? </s> return self.runner.run(ssh_cmd, expect_fail=true, expect_stderr=true)	__call__ ssh_cmd += cmd if isinstance(cmd, list) \ else sh_split(cmd, posix=not on_windows)
#todo: increase timeout based on number of plugins </s> response = opener.open(url, none, 5)	_plugin_fetch ) )] data = response.read() if not data:
# todo :: move arbitray path construction to storagelayout object </s> url = '{0}/wal_{1}/{2}.lzo'.format(	wal_restore NB: Postgres doesn't guarantee that wal_name == basename(wal_path), so both are required. self.layout.prefix.rstrip('/'), FILE_STRUCTURE_VERSION, wal_name) logger.info(
# todo: redo as json </s> with open('urls.obj', 'w') as urlfile:	main if pasturls: logging.info('Dumping past URLs to file') pickle.dump(pasturls, urlfile) if hashes:
# todo: only unescape \n \t and \\? </s> value = value.decode('string-escape').decode('utf-8')	String def deserialize(self, value): if not isinstance(value, unicode): value = value.strip() validators.validate_required(value, not self.optional)
# todo: delete the orphaned flairtemplate row </s> idx._commit()	clear for k in idx._index_keys(): del idx[k]
# todo handle error </s> self.setstatustext("saved session")	menu_file_session_save f.write("\n")
# todo: this should take a vector </s> return self._memui	MeMuI self._MeMuI = self.mesh.getEdgeInnerProduct(self.mu, invMat=True)
pass # todo: pass link problem upstream? </s> return r"%s<a href='%s' class='nocode'>%s</a>%s" % (	link_to qlink = urljoin(red.link_parser.base, link) except ValueError, why: matchobj.group(1), u"?uri=%s" % e_query_arg(qlink),
pass # todo </s> def refresh(self):	refresh
# todo put this in a .extra w/a subselect </s> if not hasattr(self, '_hours_worked'):	hours_worked @property def hours_worked(self): self._hours_worked = Entry.objects.filter( user=self.contact.user,
# todo: handle this </s> 0, 0, 0, 0, positioning_x + position_x, positioning_y + position_y)	draw_background_image context.transform(
deck.notes = note.get_notes_from_collection(collection, deck.anki_deck["id"])  # todo ugly </s> deck.children = [cls.from_collection(collection, child_name) for child_name, _ in	Deck deck.update_db() deck.anki_deck = collection.decks.byName(name) collection.decks.children(deck.anki_deck["id"])] return deck
# todo multi-level import non-breakable </s> if isinstance(par, pr.import) and len(par.namespace) > 1:	process result.append(par) else: no_break_scope = True result.append(par)
# todo protocol is incorrect (refer to article) </s> workers = [sy.hook.local_worker.get_worker(w_name) for w_name in list(a_sh.child.keys())]	relu_deriv def relu_deriv(a_sh): assert isinstance(a_sh, sy.AdditiveSharingTensor) return msb(a_sh, *workers)
# todo: if the procpool has been exhausted this will block. </s> self.procpool.spawn(self.handle_request, body)	on_nova_message def on_nova_message(self, body, message): with self.messagesem: message.ack()
return broadcast_best(samples_b, llik_b, llik_a)[0]  # todo </s> low=low, high=high, q=q)	sample_quniform samples_b, weights_a, mus_a, sigmas_a,
@raises(sshexception) # todo: more granular </s> def test_password_kwarg_not_used_for_passphrase_when_passphrase_kwarg_given(self): # noqa	test_password_kwarg_not_used_for_passphrase_when_passphrase_kwarg_given self._test_connection( key_filename=_support('test_rsa_password.key'),
# todo: make it cleaner </s> vgg_16_variables_mapping = {}	FCN_32s output_shape=upsampled_logits_shape, strides=[1, upsample_factor, upsample_factor, 1]) vgg_16_variables = slim.get_variables(fcn_32s_scope) for variable in vgg_16_variables:
# todo: add docstring </s> particle = particle(argument, mass_numb=mass_numb, z=z)	particle_symbol Z: int = None) -> str: r"""Returns the symbol of a particle.""" return particle.particle
# todo manage tangent? </s> values = [values[idx * 3 + 1] for idx in range(0, len(keys))]	anim gltf.animation_object = True if animation.samplers[channel.sampler].interpolation == "CUBICSPLINE": if channel.target.path == "translation": blender_path = "location"
# todo - del just my poll, not the entire list ! </s> return poll_user_just_voted	survey_check_vote if survey.id in sess_jv: del request.session[SURVEY_JUST_VOTED_COOKIE_NAME] sess_nv = request.session.get(SURVEY_NO_CHOICE_COOKIE_NAME, []) if survey.id in sess_nv:
# todo(msb) remove this hack </s> self.partition = self.partitions[0].module	__init__ self.add_module(str(name), part.module) self.create_pipeline() del module
# todo: with only non-mandatory model attributes, it is not possible to get an invalid form </s> self.assertequal(response.status_code, 200)	test_system_exporter_spreadsheet_csv_config_post_redirect response = self.client.post('/config/system/exporter/spreadsheet/csv/', data_dict)
# todo: small gains expected, when views are pre-calculated in main. </s> psi = tf_signal.frame(y, k, 1, axis=-1)[..., :t - delay - k + 1, ::-1]	get_correlations D = dyn_shape[1] T = dyn_shape[2] Psi_conj_norm = ( tf.cast(inverse_power[:, None, delay + K - 1:, None], Psi.dtype)
# todo for now, baked in assumption that the number of trials is the </s> for env_id in benchmark.env_ids:	benchmark_aggregate_score start_times = [] end_times = [] task_list = benchmark.task_specs(env_id) num_trials = task_list[0].trials
# todo - use smarter weights (e.g. hamming window) </s> k[indices] += 1	get_features fixed=fixed) data[indices] += fX_ data = data / np.maximum(k, 1) return data
#todo: check system is stable, perhaps a utility in ctrlutil.py </s> d,v = np.linalg.eig(sys.a)	gram dico = 'C' for e in D: if e.real >= 0:
pass # todo </s> self.remove = []	Extension self.require.append(data[required.attrib['name']]) except KeyError: for removed in chain.from_iterable(element.findall('remove')): if removed.tag == 'type': continue
raise notimplementederror # todo </s> idx = 0	HSMMStatesPython def generate_states(self): if self.left_censoring: nextstate_distr = self.pi_0 A = self.trans_matrix
# todo: use actual html decode </s> sizes.extend([match.replace('&nbsp;', ' ').split()	remote if found == [] and no_results is None: raise IOError('Blocked mirror detected.') for match in re.findall(r'(?<=Size )[0-9.]' r'+\&nbsp\;[KMGT]*[i ]*B', res)])
# todo: remove parameters dictionary look up when we can confirm each trigger dictionary </s> if trigger_id:	get_trigger_db_by_ref_or_dict trigger_id = trigger.get('id', None) trigger_uid = trigger.get('uid', None) LOG.debug('Looking up TriggerDB by id: %s', trigger_id) trigger_db = get_trigger_db_by_id(id=trigger_id)
# todo: se a data já estiver no cases mas for só total e essa </s> continue	get_state_data_from_db date = spreadsheet.date if date in cases: report_data = reports.get(date, defaultdict(list)) for url in spreadsheet.boletim_urls:
# todo: is this safe? </s> self.cmd( 'iptables -f' )	config localIntf =  self.defaultIntf() self.cmd( 'sysctl net.ipv4.ip_forward=0' ) self.cmd( 'iptables -t nat -F' ) self.cmd( 'iptables -P INPUT ACCEPT' )
# todo this assertion can occasionally fail </s> assert not g	test_send_rpc_multi_message_reply_ignores_all_but_last assert resp == {'spam': 'shrub', } eventlet.sleep()
# todo: re-implement </s> failed = self.shelve_session.setdefault('failed', [])	print_failed def print_failed(self): print 'TODO: broken' if not failed: print 'No failed entries recorded'
# todo: check that the performance measure is within some range </s> merge_baseline(num_runs=1, sumo_binary="sumo")	test_merge Tests flow/benchmark/baselines/merge{0,1,2}.py
# todo: implement auto-dtype method in general parameters </s> zlp.data = zlp.data.astype('float32')	splice_zero_loss_peak_flog s.data[ith:2*ith] = s.data[ith-1] * np.hanning((ith)*4)[-ith:] pbar.update(i) s = self.deepcopy() Eaxis = s.axes_manager.signal_axes[0]
# todo: implement retrying. api requests 5 seconds between retries. </s> elif request_status >= 400:	on_task_output elif request_status == 500: log.debug("Pushalot notification failed, Pushalot API having issues") errors = json.loads(response.content) log.error("Pushalot API error: %s" % errors['Description'])
# todo: we lose the response code, so we can't check this </s> self.failunless("new.txt" in self._foo_node.children)	test_PUT_NEWFILEURL d = self.PUT("/vdrive/global/foo/new.txt", self.NEWFILE_CONTENTS) def _check(res): new_uri = self._foo_node.children["new.txt"] new_contents = self.files[new_uri]
# todo(b/163904067): mimic defun' behavior and reset the step seed to </s> resetstepseed()	WhileLoop @tf.function(autograph=False) def LoopBody(*args): s = state.Pack(args) s.loop_state = body(s.loop_state)
# todo: safe labels </s> return resultiterator(cursor, cursor.keys())	facts labels=None) cursor = self.execute(builder.statement, "facts")
# todo factor out attribute name usage in cell so this restriction is moot for non-settable cells </s> raise lookuperror('bad getter name', k)	state if isinstance(v, ExportedGetter): if not k.startswith('get_'): else: k = k[len('get_'):]
# todo(mriedem): remove this stub once we're only using fakelibvirt. </s> self.host.get_capabilities)	test_baseline_cpu_not_supported self.assertRaises(libvirt.libvirtError,
assert oldsize == self.size, '%s: %d, %d' % (self.type, oldsize, self.size) # todo: remove </s> return self.size	Atom else: self.size = 8 + sum([atom.calsize() for atom in self.body])
#todo raise error or others </s> return	on_reset_button_clicked if stderr: log.error(stderr) InfoDialog(_('Reset successfully!\nYou may need to restart your desktop to take effect')).launch()
# todo: move update_spec to worker. agent should not hold these execution details. </s> if time_percentage is none:	get_action - action - the preprocessed states time_percentage = self.timesteps / self.update_spec.get("max_timesteps", 1e6) extra_returns = {extra_returns} if isinstance(extra_returns, str) else (extra_returns or set())
# if the test has the todo flag set, then our failures and errors are </s> self.todo = getattr(method, "todo", getattr(testcase, "todo", none))	Tester class Tester: def __init__(self, testClass, testCase, method, runner): self.skip = getattr(method, "skip", getattr(testCase, "skip", None)) if self.todo:
# todo: rename intervalstodiatonic </s> specifier = _getspecifierfromgenericchromatic(gint, cint)	generateDiatonic >>> cInterval <music21.interval.DiatonicInterval P5> return DiatonicInterval(specifier, gInt)
#todo: this should never happen, dep on equiv </s> print 'content-type: application/json\n'	delete_arc mods.deletion(eq_ann) except DependingAnnotationDeleteError, e: display_message(e.json_error_response(), type='error', duration=3) print dumps(add_messages_to_json({}), sort_keys=True, indent=2)
elif isinstance(other, containeroperand): # todo 0.7: use iterable to array force user to provide values </s> operand = other.values	_ufunc_set operand = other.values assume_unique = True # can always assume unique assume_unique = False else:
# todo: let the globe return the semimajor axis always. </s> return coords	ellipse coords += ([easting], [northing])
# todo{jihongju} the mask branch </s> return [score, boxes]	f score = keras.layers.TimeDistributed(keras.layers.Dense(classes, activation="softmax"))(y) boxes = keras.layers.TimeDistributed(keras.layers.Dense(4 * classes))(y)
# todo confirm we want floor division here </s> r = pmat.random(probs.size() // numalts)	mnl_simulate probs = PMAT(probs.get_mat()) probs = probs.cumsum(axis=0) choices = probs.subtract(r, inplace=True).firstpositive(axis=0) logger.debug('finish: MNL simulation')
n)  # todo: access alice's private key inside this method. </s> from nkms.policy.models import policy	create_policy alice_priv_enc = self._crypto_power._power_ups[EncryptingPower].priv_key kfrags, pfrag = self.generate_rekey_frags(alice_priv_enc, bob, m, policy = Policy.from_alice( alice=self,
## todo check exception type </s> out = []	visit_ExceptHandler def visit_ExceptHandler(self, node): if node.type: out.append( self.indent() + '###exception: %s' %self.visit(node.type) )
# todo log here </s> if result == 'success':	auth_okta result = data.get('factorResult').lower() print result return True elif result == 'challenge':
# todo: confirm necessity of this session clearing and lay out mechanics. </s> tuf.download._sessions = {}	test_http_dl_via_https_proxy self.set_env_value('REQUESTS_CA_BUNDLE', os.path.join('ssl_certs', 'proxy_ca.crt')) logger.info('Trying HTTP download via HTTPS proxy: ' + self.url_https) download.safe_download(self.url, self.target_data_length)
# todo maybe have the api level loading in the __init__ method and pass the apk as well? </s> permmap = load_api_specific_resource_module('api_permission_mappings', apilevel)	get_permissions :param apilevel: API level to load, or None for default :return: yields tuples of :class:`MethodClassAnalysis` (of the API method) and list of permission string if not permmap: raise ValueError("No permission mapping found! Is one available? "
# todo: replace the blockerdb with a depgraph of installed packages </s> self._blocker_db = {}	Scheduler self.pkgsettings = {} self._config_pool = {} for root in trees: self._config_pool[root] = []
# todo: speedup by allocating the denominator directly instead of constructing it by sum </s> return tf.slice(tens, [0, 0, second * single_batch_size], [m, n, single_batch_size])	half n = int(n)
# todo: support iat/iloc differences </s> return seriesiattype(ary)	resolve_loc def resolve_loc(self, ary):
# todo: change logic to c_leq based on benchmarking </s> try:	solve_subproblem if not nlp_var.fixed and not nlp_var.is_binary(): nlp_var.value = orig_val TransformationFactory('contrib.deactivate_trivial_constraints')\ .apply_to(fixed_nlp, tmp=True, ignore_infeasible=False)
# todo: make locking works for mssql </s> pass	create_global_lock session.connection().execute(f"select GET_LOCK('{lock_name}',{mysql_lock_timeout});") if dialect.name == 'mssql': yield None finally:
# todo log here </s> return false	auth_okta ) except httplib.HTTPException: if response.status_code != 200: return False
# todo: don't write them? is *much* slower on re-load (~3x) </s> try:	update_lib fh.flush() os.fsync(fh.fileno())  # flush to disk os.unlink(path + 'c') except OSError:
# todo: neil-san, you should keep this </s> if not conv_node.quantizer or not conv_node.a_quantizer:	pass_compute_thresholds batch_norm_node = quantizer_conv_output_node.input_nodes[0] conv_node = batch_norm_node.input_nodes[0] continue quantizer_conv_weights = conv_node.quantizer
# todo(developer): uncomment and set to a path to your audio file. </s> with open(speech_file, 'rb') as audio_file:	transcribe_file_with_diarization from google.cloud import speech_v1p1beta1 as speech client = speech.SpeechClient() content = audio_file.read() audio = speech.types.RecognitionAudio(content=content)
# todo: +kwargs </s> else:	pull with remote.repo.git.custom_environment(**GitRepo.GIT_SSH_ENV): return remote.pull(**pull_kwargs) return remote.pull(**pull_kwargs)
# todo: convert [n, c, v] to  new convention [v, n, c] </s> return crossentropyloss	get_module def get_module(self):
# todo check how we define frames regarding action repeat. </s> self.logger.info("throughput: {} ops/s".format(num_timesteps / end))	execute_timesteps end = time.monotonic() - start self.logger.info("Finished executing {} time steps in {} s".format(num_timesteps, end)) self.logger.info("Mean episode reward: {}".format(np.mean(episode_rewards))) self.logger.info("Final episode reward: {}".format(episode_rewards[-1]))
# todo(b/141131288): enable complex-valued sorts on tpu. </s> if (onp.issubdtype(dtype, onp.complexfloating) and (	testSortAgainstNumpy for axis in [-1, len(shape) - 1])) def testSortAgainstNumpy(self, shape, dtype, axis): (jtu.device_under_test() == "cpu" and jax.lib.version <= (0, 1, 47)) or jtu.device_under_test() == "tpu")):
# todo: this may return a shortname or a longname, with no way </s> name = buffer[pos+12:pos+12+namelen].decode("utf16")	get_FILE_NOTIFY_INFORMATION_alt while pos < len(buffer): jump, action, namelen = struct.unpack("iii",buffer[pos:pos+12]) yield (name,action) if not jump:
# todo this paragraph is necessary, but not sure it works. </s> context = self._user_context.get_context()	get_completions i = imports.Importer(self._evaluator, imp_names, module, level) completion_names = i.completion_names(self._evaluator, only_modules) if not next(context).startswith('.'):  # skip the path if next(context) == 'from':
# todo hardwired magic numbers! bad api smell. </s> for inflecting_format in (3,4):	test_about_month_only_date_genitive d1945may = Date(1945, 5, 0) d1945may.set_modifier(Date.MOD_ABOUT) self.dd.set_format(inflecting_format) self.assertIn("около мая", self.dd.display(d1945may))
# todo: endianness support </s> num2fmt = {1: 'b', 2: 'h', 4: 'i', 8: 'q'}	read_memory if raw: return raw_mem fmt = '<{}{}'.format(num_words, num2fmt[wordsize]) mem = struct.unpack(fmt, raw_mem)
# todo(mattjj,levskaya): re-enable when test failure is sorted out </s> raise skiptest("tfp test failures")	testGeomspace def testGeomspace(self, start_shape, stop_shape, num, endpoint, dtype, rng_factory): rng = rng_factory() tol = tolerance(dtype if dtype else onp.float32) * 10
val = '"%s"' % val  # todo: handle correctly escaping input strings. </s> elif val is true:	build_r_params for var, val in parameters.items(): if isinstance(val, string_types): val = 'TRUE' elif val is False:
# todo: change retrieval mode </s> self.c = couchobject.from_couch(cookie_contest, true)	prepare if cookie_contest != None: try: except couchdb.client.ResourceNotFound: print "Unset cookie."
# todo lib </s> unique_name = f"{existing_datablock.name}_{existing_datablock.mixer_uuid}"	create_standalone_datablock else: if existing_datablock.mixer_uuid != self.mixer_uuid: logger.warning( f"create_standalone_datablock: Creation name conflict. Renamed existing bpy.data.{self.collection_name}[{existing_datablock.name}] into {unique_name}"
# todo: is this really ok? </s> headers = mutant.getheaders()	_sendMutant if mutant.getMutantType() == 'cookie': data = '' cookie = mutant.getCookie() if cookie:
# todo: implement non-zero js </s> return self.mesigmai*(self.edgecurlt*(self.mfmui*b))	e_from_b def e_from_b(self, b, ind):
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_depth_null def test_fail_depth_null(self): ``depth`` is null.
# todo stop guessing </s> remove_entire_file = (location.get('line') or location.get('column')) is none	parse_lint_result for location in issue_xml.findall('location'): filepath = location.get('file') issue = Issue(filepath, remove_entire_file) issue.add_element(issue_xml.get('message'))
# todo(stephenfin): use a helper </s> self.api.post_server_action(server['id'], {'migrate': none})	test_migrate_confirm fake_drop_move_claim, ) self._wait_for_state_change(server, 'VERIFY_RESIZE') self.assertUsage(src_host, 1)
encoding = "utf-8"  # todo: receive as a parameter </s> timeout = 0.1  # todo: receive as a parameter	import_data table_name = Model._meta.db_table database_uri = os.environ["DATABASE_URL"] start_time = time.time() progress = ProgressBar(prefix="Importing data", unit="bytes")
# todo: if the type for struct-type is not controlled by the current inspector, the exn:fail:contract exception should be raised </s> struct_desc = args[0]	do_struct_type_make_predicate @expose("struct-type-make-predicate") def do_struct_type_make_predicate(args): struct_type = values_struct.W_StructType.lookup_struct_type(struct_desc) return struct_type.pred()
# todo use self.rester when in prod </s> m = mprester(os.environ.get('mapi_key_dev'), endpoint="http://www.materialsproject.org:8080/rest")	test_get_materials_id_references def test_get_materials_id_references(self): data = m.get_materials_id_references('mp-123') self.assertTrue(len(data) > 1000)
"""todo doc me""" </s> table = [['foo', 'bar', 'baz'],	test_profile_default def test_profile_default(): ['A', 1, 2], ['B', '2', '3.4'],
# todo message </s> return symbol('$failed')	Import function_channels = importer_options.get("System`FunctionChannels") if function_channels is None: default_element = importer_options.get("System`DefaultElement") if default_element is None:
# todo: check that ambient makes sense </s> pass	__classcall__ ambient = RealLine(**kwds) else: return ambient name = kwds.pop('name', None)
pass  # todo... </s> def _collect_single_seq(self, seq_idx):	_collect_single_seq
# todo: add type and value checkings </s> pinhole_ref = scale_pinhole(pinhole, scale)	compute_homographies def compute_homographies(self, pinhole, scale): if self.width is None: self.width = pinhole_ref[..., 4]
# todo: replace column names and remove the appliance name from </s> appliance_name = appliance[:-1]	standardize for x in names] name_modification = {names[i]: names_modified[i] for i in range(len(names))} appliance_instance = appliance[-1] building.utility.electric.appliances[
# todo test </s> def state_reachable(state, tpm):	state_reachable test = tpm - np.array(state) if not np.any(np.logical_and(-1 < test, test < 1).all(-1)):
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
if not real: self.assertequal(exita, 1) # todo: real = 0 </s> self.assertequal(exitb, 0)	test_3050_systemctl_py_check_is_active doneC, exitC  = output2(enable_C.format(**locals())) doneD, exitD  = output2(enable_D.format(**locals())) self.assertEqual(exitC, 0) self.assertEqual(exitD, 1)
"""todo doc me""" </s> table = [['foo', 'bar', 'baz'],	test_profile_default def test_profile_default(): ['A', 1, 2], ['B', '2', '3.4'],
"""todo: not implemented""" </s> notimplementederror("last not implemented")	last @symbolic_dispatch def last(x):
maxtime = 10445238000  # todo: change after 31 dec 2300 lol </s> time = min(	add for txid in txs: tx = txs[txid] tx.get("blocktime", maxtime), tx.get("timereceived", maxtime),
# todo this swallows status message, yield properly </s> ds.repo.add(to_annexadd)	save_dataset if to_annexadd: lgr.debug('Adding files to annex at %s: %s', ds, to_annexadd) _datalad_msg = False if not message:
# todo: remove </s> g.group_dict['package_count'] = query['count']	_read item_count=query['count'], items_per_page=limit) extra_vars["search_facets"] = g.search_facets = query['search_facets'] extra_vars["search_facets_limits"] = g.search_facets_limits = {}
#todo: remove convert_bare true and deprecate this in with_ </s> loop_terms = listify_lookup_plugin_terms(terms=self._task.loop_args, templar=templar, loader=self._loader, fail_on_undefined=true, convert_bare=true)	_get_loop_items if self._task.loop: if self._task.loop in self._shared_loader_obj.lookup_loader: items = self._shared_loader_obj.lookup_loader.get(self._task.loop, loader=self._loader, templar=templar).run(terms=loop_terms, variables=vars_copy) else:
# todo: assuming the register is written in its number format </s> reil_operand = reilregisteroperand('r' + str(int(reil_operand.name[1:]) + 1), reil_operand.size)	_translate_ldrd reil_operand = ReilRegisterOperand(instruction.operands[1].name, instruction.operands[0].size) else: tb.add(tb._builder.gen_ldm(addr_reg, reil_operand))
# todo; handle multiple doc-types </s> asyncindicator.bulk_update_records(async_configs_by_doc_id, domain, docs[0].doc_type)	process_changes_chunk succeeded_changes.union(changes_chunk) if async_configs_by_doc_id: return BulkProcessingResult(succeeded_changes, failed_changes)
# todo manage tangent? </s> if animation.samplers[channel.sampler].interpolation == "cubicspline":	anim blender_path = "scale" for idx, key in enumerate(keys): obj.scale = Vector(Conversion.scale_gltf_to_blender(list(values[idx * 3 + 1]))) else:
#todo change to native framework call, when plex allows token in header </s> request = urllib2.request(self.getlistsurl, headers=myheader)	LIST myHeader = {} myHeader['X-Plex-Token'] = users[user]['accessToken'] playlists = XML.ElementFromString(urllib2.urlopen(request).read()) result = {}
# todo: fix </s> from .numpy_ops import numpyops	position_encode self, N: int, D: int, period: int = 10000, out: Optional[Array] = None ) -> Array: numpy_ops = NumpyOps() return self.asarray(numpy_ops.position_encode(N, D, period, out))
# todo: write me </s> raise notimplementederror('limiteddisk cache does not yet implement the .remove() method.')	remove def remove(self, layer, coord, format):
# todo: this might be a rather nasty hack to fix the circular dependency </s> from nilmtk.preprocessing.electricity.single import reframe_index	dropout_rate_per_period timezone=data.index.tzinfo Values are the number of dropped in that time period. try: data = data.dropna()
# todo: mock these so no network access is required </s> self.assertequal(commit.objects.count(), 0)	test_fetch_commits def test_fetch_commits(self): launchpad_handler.fetch_commits(self.package) self.assertNotEqual(Commit.objects.count(), 0)
# todo(hub-cap): turn this into middleware </s> context = context.reddwarfcontext(	index def index(self, req, tenant_id): auth_tok=req.headers["X-Auth-Token"], tenant=tenant_id)
# todo: only standard themes supported right now </s> write_new_settings('firefox-compact-light@mozilla.org', false)	switch_to_light def switch_to_light():
# todo: add some kind of 'failed to open' notification </s> pass	open_cb os_open(path, uri) else:
# steps = 0 # todo </s> luxcore_scene.deleteobject(src_name)	convert transformations = array("f", duplis.matrices) luxcore_scene.DuplicateObject(src_name, dst_name, count, transformations) print("Dupli export took %.3fs" % (time() - start))
# todo: append to current tree </s> pass	add_from_file self.objects = {} else:
# todo: direct use of json['error'] is deprecated; use display_message('...', 'error') instead. </s> result['error'] = 'not logged in'	serve result['user'] = creds['user'] except (KeyError): print 'Content-Type: application/json\n' print dumps(result)
# todo: multithread optimization, one thread per ps communication. </s> for index, n in enumerate(partition_result):	pull pull_result = {} partition_result = self._partition_func(names, self._ps_size) ps_step, ps_vars = self._clients[index].pull( names=n, min_step=min_step)
# todo(kevinbenton): bulk? </s> updater = getattr(request.plugin, 'update_%s' % request.resource_type)	put @when(index, method='PUT') def put(self, *args, **kwargs): return updater(request.context, self.item, request.prepared_data)
# todo: accept these via quirks? </s> if os.environ.get('libusb_bus') and os.environ.get('libusb_port'):	appropriate_for_environment Determines if the current environment seems appropriate for using the libusb backend. return True if os.environ.get('LIBUSB_ADDRESS'):
# todo: optimize db call </s> dataset_instance = dataset_collection.collection.dataset_instances[ 0 ]	__summarize self.__append_dataset( content ) for dataset_collection in implicit_outputs: if not self.__check_state( dataset_instance ): continue
# xxx todo: real error handling, as this is probably going to </s> if new_firmware:	_copyDriverDiskFiles except IOError as e: log.error("failed to copy driver disk files: %s" % e.strerror) for kernel in self.kernelVersionList: log.info("recreating initrd for %s" % kernel)
# todo: ... </s> dumpdata(get_dump_path(''), data)	dump_item def dump_item(data): dump_comments(data)
).consume()  # todo see issue 170 </s> for group in interface.get("groups", []):	load_ec2_instance_network_interfaces SubnetId=interface.get("SubnetId", ""), aws_update_tag=aws_update_tag, neo4j_session.run( ingest_network_group,
# todo: empty env? </s> sendrc=false, timeout=120, usepty=false, environ={})	test_simple_unicode_args u'buildbot_test_10\N{SNOWMAN}', 'sync', '-f'], self.basedir, + 0, Expect(['p4', '-p', u'p4dserv:1666\N{SNOWMAN}', '-u', 'jimmy', '-P',
return none # todo: probably need a universal failure code </s> return bv.navigate(bv.view, address) # note: bn returns none	navigate bv = _binja_get_bv() if not bv:
# todo: test pdb files with dna and rna too: </s> record.annotations["molecule_type"] = "protein"	CifSeqresIterator record = SeqRecord(Seq("".join(residues))) record.annotations = {"chain": chn_id} if chn_id in metadata: m = metadata[chn_id][0]
# wait until the chunks have added, todo change this to a qtbot.waitsignal </s> qtbot.wait(short_loading_delay)	test_tiled_changing_contrast_limits visual = viewer.window.qt_viewer.layer_to_visual[layer] assert isinstance(visual, VispyTiledImageLayer) screenshot = viewer.screenshot(canvas_only=True) center_coord = np.round(np.array(screenshot.shape[:2]) / 2).astype(np.int)
# todo: change nonce </s> self._auth = rfc2617.authentication(	handle_REGISTER print("-----auth", self._auth) if self._auth.check(u.username, "test", "REGISTER", auth_response) == False: method = "digest", algorithm = "md5",
selection = page # todo api selection objects </s> else:	cmd_export path = self.notebook.resolve_path(page) page = self.notebook.get_page(path) page = self.notebook.get_page(page) selection = page # TODO API selection objects
# todo (shea): extract method(s) to get_source_processor() </s> elif "github_repo" in facts:	generate_download_recipe "user-agent": facts["user-agent"]} recipe.append_processor(sparkle_processor) keys["Input"]["GITHUB_REPO"] = facts["github_repo"] gh_release_info_provider = processor.GitHubReleasesInfoProvider(
raise exceptions.mpdnotimplemented  # todo </s> unsubscribe from a channel.	unsubscribe ``unsubscribe {NAME}``
# todo: check the data! </s> self.asserttrue(len(list(pipe)) > 0)	test_tail pipe_def = self._get_pipe_def(pipe_file) pipe = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def)
# @todo: deployment_setting </s> organisation_dropdown_not_ac = false	S3OrganisationModel args="create", vars=dict(format="popup")) if organisation_dropdown_not_ac: help = T("If you don't see the Organization in the list, you can add a new one by clicking link 'Add Organization'.")
# todo(lbragstad): currently, fernet tokens don't support bind in the </s> if auth_context.get('bind'):	issue_v3_token :param parent_audit_id: ID of the patent audit entity :returns: tuple containing the id of the token and the token data raise exception.NotImplemented() token_ref = None
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> sampleparameters = {	test_acquire_token_with_user_pass def test_acquire_token_with_user_pass(self): "tenant" : "common", "authorityHostUrl" : "https://login.windows.net",
# todo: replicate complete behaviour of urllib.urlopener.open </s> return filename, headers	__tuf_retrieve )
return runtime.strarray(strs)  # todo: reuse this object too? </s> if name == 'call_source':	GetVar if source_name: strs.append(source_name) strs = [] for func_name, source_name, call_spid, _, _ in reversed(self.debug_stack):
# todo: aggregate serializable errors into errors dict </s> pass	_mutate field.__set__(mutable, value) except AttributeError: raw_data.update(mutable)
# todo; to remove after full rollout of https://github.com/dimagi/commcare-hq/pull/21329/ </s> change_feed = kafkachangefeed(	get_form_submission_metadata_tracker_pillow Processors: - :py:class:`pillowtop.processors.form.FormSubmissionMetadataTrackerProcessor` topics=topics.FORM_TOPICS, client_id='form-processsor', num_processes=num_processes, process_num=process_num
#todo: make the comparaison without transfert. </s> return tensor.tensortype.values_eq(numpy.asarray(a), numpy.asarray(b))	values_eq @staticmethod def values_eq(a, b):
if lang is none:  # todo: remove in v8 </s> logger.warn("unslugify() called without language!")	unslugify If discard_numbers is True, numbers right at the beginning of input will be removed. if discard_numbers: value = re.sub('^[0-9]+', '', value)
async_pub['jid'],  # todo: fix </s> false,  # don't daemonize	wrapper user, async_pub['tag'],  # TODO: fix
# todo: implement proper mutexes instead of these stubs </s> tls = _thread_local_storage(ql, ql.os.cpupage_tls_addr).loadfrommem()	ql_syscall_sync_mutex_lock sync = _sync(ql, syncp).loadFromMem() ql.log.debug(f'ql_syscall_sync_mutex_lock: count={ux32s(sync._count)}, owner={ux32s(sync._owner)}') sync._owner = tls._owner sync.updateToMem()
# todo: dont hardcode </s> return ["/users/az/programmierung/torch/install/lib"]	c_lib_dirs def c_lib_dirs(self):
#todo: make more general (if possible) </s> return datetime.fromtimestamp(record_dict['slice_index_value'] + (record_dict['x_min'] + record_dict['x_max']) * 120).date()	solar_date Function which takes a record_dict containing all values from a query in the get_db_slices function and returns the solar date of the observation
# todo: i don't think these should be pushed back on. </s> self.push(item)	byte_STORE_SUBSCR item = self.pop() l[ind] = item self.push(l) self.push(ind)
# todo: does this whole section disappear with proper headers from requesthandler? </s> other_uri = environ['script_name'] + e.path_info	__call__ status_code, headers, content = requestHandler(self.config, environ['PATH_INFO'], environ['QUERY_STRING']) except Core.TheTileIsInAnotherCastle, e: if environ['QUERY_STRING']: other_uri += '?' + environ['QUERY_STRING']
# todo: remove patch and update test once calculation_magic is implemented </s> assert store.get_lpq_projects() == {17}	test_is_eligible_in_lpq _update_lpq_eligibility(project_id=17, cutoff=10)
# todo generator </s> exc_str_ = ': ' + exc_str(exc) if exc.results else ''	__call__ ) except IncompleteResultsError as exc: lgr.warning("Some items failed to install: %s", exc_str_)
for people in annos:  # todo : speed up with affine transform </s> new_keypoints = []	keypoint_random_crop2 mask = mask[crop_range_y:crop_range_y + _target_width, :] new_joints = [] for keypoints in people: if keypoints[0] < 0 or keypoints[1] < 0:
# todo(b/141131288): enable test once complex sort is supported on tpu. </s> if (jnp.issubdtype(dtype, jnp.complexfloating)	testLexsort for axis in (-1, *range(len(shape) - 1)))) def testLexsort(self, dtype, shape, input_type, axis): and jtu.device_under_test() == "tpu"): self.skipTest("complex sort not supported on TPU")
# todo: different codec to be used </s> raise(asn1notsupperr('{0}: specific containing encoder unhandled' \	_to_oer Cont = self._get_val_obj(self._val[0]) if Cont == self._const_cont and self._const_cont_enc is not None: .format(self.fullname()))) Cont._val = self._val[1]
@persistent # todo: not sure if i should be using @persistent </s> def load_handler_render_init(scene):	load_handler_render_init print("Initialization of a render job") if not scene.use_nodes:
raise notimplementederror #todo </s> elif q.kw(i,"format"):	parse while i < l: if q.kw(i,"RETURN"): raise NotImplementedError #TODO elif q.kw(i,"REQUEST"):
# @todo: return only packages for the current architecture </s> package_names = list(package_names)[:]	find_aur_packages def find_aur_packages(package_names): json_results = [] for package_name in package_names[:]:
assert study_id == 0  # todo </s> self.trials[trial_id].intermediate_results[step] = intermediate_result	report_intermediate_result def report_intermediate_result(self, study_id, trial_id, step, intermediate_result):
# todo: check for expected warnings. </s> return self._run_master(sample_0_9_0b5)	test_config_0_9_0b5 def test_config_0_9_0b5(self):
# todo: make pull request to get this custom vgg feature accepted </s> with slim.arg_scope(vgg.vgg_arg_scope()):	FCN_32s number_of_classes) upsample_filter_tensor = tf.constant(upsample_filter_np) logits, end_points = vgg.vgg_16(processed_images, num_classes=2,
# todo: can the instance lock be downgraded here? take the optional disk </s> self.lu.logstep(cstep, steps_total, "sync devices")	_ExecDrbd8DiskOnly keep=self.node_secondary_ip.keys()) _ReleaseLocks(self.lu, locking.LEVEL_NODE) cstep += 1 _WaitForSync(self.lu, self.instance)
annot.annotation_metadata.annotation_rules = "todo" #todo </s> annot.annotation_metadata.validation_and_reliability = "todo" #todo	fill_annotation annot.annotation_metadata.version = "1.2" annot.annotation_metadata.annotation_tools = "Sonic Visualizer" annot.annotation_metadata.origin = metadata[1] annot.annotation_metadata.annotator.name = metadata[annotation_id + 2]
# todo test </s> self.unconstrained_effect_repertoire(purview))	effect_info return utils.emd(self.effect_repertoire(mechanism, purview),
# todo: does this need to be made more efficient? </s> p1 = future.futureparser()	compile elif mode == "exec": graph = pyassem.PyFlowGraph("<module>", as_tree.filename) p2 = future.BadFutureParser() walk(as_tree, p1)
# todo: remove this when hftransformersnlp is removed for good </s> hf_transformers_loaded = "hftransformersnlp" in config.component_names	create cls, component_config: Dict[Text, Any], config: RasaNLUModelConfig ) -> "DenseFeaturizer": return cls(component_config, hf_transformers_loaded=hf_transformers_loaded)
#[todo]: softmax is not supported yet </s> loss = svm_loss(scores, y) + \	TwoLayerNet if y is None: return scores np.sum(W1**2) * 0.5 * self.reg + \ np.sum(W2**2) * 0.5 * self.reg
# todo: avoid dummy and generate func here when inlining is possible </s> def _impl(df, n=5):	_impl return hpat.hiframes.pd_dataframe_ext.head_dummy(df, n)
# todo: external summary for enum values </s> value.summary = extract_summary(state, {}, [], docstring)	extract_enum_doc else: docstring = i.__doc__ if value.summary: out.has_details = True
# todo: if this is the initial load of logging config we might not have </s> try:	load parser.readfp(io.StringIO(extension.get_default_config())) for filename in files: filehandle = codecs.open(filename, encoding='utf-8') parser.readfp(filehandle)
# todo(b/179510447): align these parameters with schulman 17. </s> return tf.keras.layers.dense(	means_layers def means_layers(): action_tensor_spec.shape.num_elements(), kernel_initializer=tf.keras.initializers.VarianceScaling(
# todo: mark this error on the eq relation, not the entities </s> for e in equiv_anns:	verify_annotation eq_type[e.type] = True if len(eq_type) != 1: issues.append(AnnotationIssue(e.id, AnnotationError, "%s in Equiv relation involving entities of more than one type (%s)" % (e.id, ", ".join(eq_type.keys())))) physical_entities = [a for a in ann_obj.get_textbounds() if a.type in annspec.physical_entity_types]
+ ansi.ansi_normal  # todo: why does it keep it? </s> + "foo"))	test_remove_bells parser.remove_bells( "a " + ansi.ANSI_BEEP + "red"
# todo sensible lookup table (doesn't matter for now because small n) </s> for mode_def in getmodes():	_lookup_mode def _lookup_mode(mode): if mode_def.mode == mode: return mode_def
# todo: use pybossa uploader! only for debugging: </s> open('/tmp/%d_%s_task_run_csv.zip' % (app.id, name), 'wb').write(memzip.getvalue())	export_csv memfile.write(str(line)) memzip = make_onefile_memzip(memfile, '%s_task_run.csv' % name)
from pyomo.core.kernel.component_variable import ivariable # todo </s> if isinstance(exp, (_vardata, ivariable)):	evaluate_expression def evaluate_expression(exp, exception=None, only_fixed_vars=False): from pyomo.core.base import _VarData # TODO if not only_fixed_vars or exp.fixed: return exp.value
# todo: do something more than simply selecting the last match? </s> hash_ = self.hash_schema(schema)	_get_constructor def _get_constructor(self, schema): matches = self.class_dict[hash_] return matches[-1] if matches else self._passthrough
# todo: move this into the operations code for its caller </s> count = 0	Hydrogenate For hilariously incorrect results, use on graphite. @warning: can create overlapping H atoms on diamond. for a in self.atoms.values(): count += a.Hydrogenate()
# todo: compute bezier curve. </s> anchors = self.anchors	ShapeLayer if bbox.width > 0 and bbox.height > 0: return bbox if not anchors or len(anchors) < 2: logger.warning("Empty shape anchors")
gre = [1, 1, 1]  # todo calculate based on mesh </s> particle_count = np.linspace(0, count, num_grids + 1, dtype=np.int32)	_parse_index num_grids = int(np.ceil(count * self.vpg ** -1)) gle = [-1, -1, -1]  # TODO Calculate based on mesh particle_names = [] for (pname, size) in self.numparts.items():
# todo: unfortunately, mongoengine contains bug which </s> op, term = parse_like_term(search_term)	_search def _search(self, query, search_term): criteria = None for field in self._search_fields:
# todo: edge dps could use a different forwarding algorithm </s> eth_src = pkt_meta.eth_src	_edge_dp_for_host Returns: Valve instance or None (of edge datapath where packet received) vlan_vid = pkt_meta.vlan.vid for other_dpid, other_valve in valves.iteritems():
# todo: assert </s> repo = self.remote.get_repo("testrepo0")	test_get_repo Test: Get a repo object
# floc-1828 todo - use archive_bucket rather than clusterhq-archive-testing  # noqa </s> sudo_from_args([	task_install_cli "apt-get", "-y", "install", "apt-transport-https", "software-properties-common"]), 'add-apt-repository', '-y', 'deb https://clusterhq-archive-testing.s3.amazonaws.com/ubuntu/14.04/$(ARCH) /'  # noqa
# todo: all the expand stuff! </s> view.textcursor().inserttext(text)	_applyTemplate def _applyTemplate(view, (text, variables)):
# todo: 判断返回结果，处理异常 </s> print msg	perm_role_recycle task = Tasks(recycle_resource) msg = task.del_user(get_object(PermRole, id=role_id).name) for asset_id in asset_ids: asset = get_object(Asset, id=asset_id)
# todo(twd2): do more visibility check eg. contest </s> flags = {}	DiscussionCreateHandler if vnode['doc_type'] == document.TYPE_PROBLEM and vnode.get('hidden', False): self.check_perm(builtin.PERM_VIEW_PROBLEM_HIDDEN) if highlight: self.check_perm(builtin.PERM_HIGHLIGHT_DISCUSSION)
# todo: finish this </s> return none	get_template_path if view.template_path is not None: return os.path.split(view.template_path)
# todo save the error to the plugin </s> plugin_db_setting.save(no_reload=true)	_init_plugins if not settings.PLUGIN_TESTING: plugin_db_setting.active = False self.plugins_inactive[plug_key] = plugin_db_setting continue  # continue -> the plugin is not loaded
# todo also test these! </s> continue	test_classifiers_test continue if Clf in [MultinomialNB, BernoulliNB]: clf = Clf() try:
# todo(b/158741360): add type annotations once pytype checks across modules. </s> def register_task_cls(task_config_cls):	register_task_cls This decorator supports registration of tasks as follows: ```
irregular_dim_names = ['time', 't']  # todo: use irregular flag from database instead </s> irregular_dims = [name for name, coord in coordinates.items()	make_in_memory_storage_unit attributes=attributes, crs=crs) if name in irregular_dim_names and coord.length > 2] if irregular_dims and su.storage_mapping.storage_type.driver == 'NetCDF CF':
## todo: # fixme: remove me </s> try:	analyse msg = 'infoleak:automatic-detection="sql-injection";{}'.format(path) p.populate_set_out(msg, 'Tags') tld = url_parsed['tld'].decode() except:
# todo: determine proper template to use. </s> app_name + "." + recipe_format + ".recipe"] = "template tbd"	main if app_name + "." + recipe_format + ".recipe" not in existing_recipes: buildable_recipes[ print "\nExisting recipes: %s" % existing_recipes print "\nAvailable recipe formats: %s" % avail_recipe_formats
# todo: make an ascii-art bar </s> ctx.fillslots("progress_hash", "%.1f%%" % (100.0 * chk))	render_row_upload self._render_common(ctx, data) (chk, ciphertext, encandpush) = data.get_progress() ctx.fillSlots("progress_ciphertext", "%.1f%%" % (100.0 * ciphertext)) ctx.fillSlots("progress_encode", "%.1f%%" % (100.0 * encandpush))
# todo: check num strings and support nan </s> data_start = getitem_c_arr(	_str_get_impl for i in numba.parfor.internal_prange(n): start_index = getitem_c_arr(arr._index_offsets, i) arr._data_offsets, start_index + ind) data_start += 1
time.sleep(40)  # todo: should remove after polling get. </s> res = res_ptr.reconstruct()	test_mpc_private_private_op op = getattr(operator, op_str) res_ptr = op(mpc_tensor_1, mpc_tensor_2) expected = op(value_1, value_2) assert (res == expected.child).all()
pass # todo </s> def test_raw(self):	test_raw
# todo: remove temporary workaround once https://github.com/python-babel/babel/issues/415 has been resolved. </s> babel_415_workaround_list = []	__init__ super().__init__(*args, **kwargs) layout = QtWidgets.QVBoxLayout(self) for action in action_list: babel_415_workaround = self.parent().TOOLBAR_BUTTONS[action]['label']
# todo: get a better way for this. the 'downloadsize' key in logicpro_plist </s> if os.path.getsize(save_path) < download_size:	process_package_download download_size_string = human_readable_size(download_size) if os.path.exists(save_path): print "Remote file is larger. Downloading %s from %s" % (download_size_string, download_url) download_package_as(download_url, save_path)
# todo: remove compatability hooks </s> if exists(pathjoin(vdir,"esky-bootstrap-old.txt")):	is_uninstalled_version_dir if exists(pathjoin(vdir,ESKY_CONTROL_DIR,"bootstrap-manifest-old.txt")): return True return True return False
pass  # todo </s> else:	_help def _help(self, args): if args == None: pass  # TODO
# todo: test bytearray </s> if not isinstance(prefix, (bytes, bytearray)):	Address self.prefix = network.prefix_address else: self.prefix = binascii.unhexlify(prefix) else:
# todo: improve the unicode checking </s> try:	__setitem__ % (item, self._index_for.capitalize(), self._index_for)) value = urllib.quote_plus(value) except (KeyError, UnicodeEncodeError, UnicodeError):
#todo classes broken </s> sample = self.sess.run(generator, feed_dict={})	sample_batch y_t = gan.graph.y print("generator is ", generator) print("sample is ", sample) print(sample.shape)
# this is the dc dmdsec. @todo: account for other as well. </s> dmdsec = dmdsecs[0]	generateCompoundContentDMDirectUploadPackage def generateCompoundContentDMDirectUploadPackage(dmdSecs, structMaps, dipUuid, outputDipDir, filesInObjectDirectoryForThisDmdSecGroup, filesInThumbnailDirectory, bulkDip): if len(dmdSecs): else: dmdSec = dmdSecs
# todo: fix output type </s> s = 1	_column_prod_impl_basic def _column_prod_impl_basic(A):  # pragma: no cover numba.parfor.init_prange() for i in numba.parfor.internal_prange(len(A)): val = A[i]
# todo: read back actual frequency and store </s> self.receiver.set_input_center_freq(self.hw_freq)	_update_frequency def _update_frequency(self): self.osmosdr_source_block.set_center_freq(self._compute_frequency(self.hw_freq), 0)
# todo(cvan): uncomment when bug 719512 gets fixed. </s> eq_(r.status_code, 200)	test_owner r = self.client.get(self.url)
# todo is this check necessary; this was an assertion which are disabled in <4000 which is good </s> if len(name) != 1:	_set def _set(view, name: str, values: list, linewise: bool = False) -> None: name = str(name) raise ValueError('Register names must be 1 char long: ' + name) if name == _BLACK_HOLE:
# todo presto drops the union and decimal fields </s> self.assertequal(cursor.description, [	test_complex def test_complex(self, cursor): cursor.execute('SELECT * FROM one_row_complex') ('boolean', 'boolean', None, None, None, None, True), ('tinyint', 'bigint', None, None, None, None, True),
# todo: finish this. </s> def mkdir(self, path, mode):	mkdir pass
# todo: may test file contents </s> temp = tempfile.namedtemporaryfile(delete=false, mode='wb')	test_export_to_xlsx_fobj def test_export_to_xlsx_fobj(self): file_name = temp.name + ".xlsx" fobj = open(file_name, 'wb')
# todo: wrap this with a try and handle exceptions gracefully </s> result = requests.get(	creds if timestamp < __Expiration__: return __AccessKeyId__, __SecretAccessKey__, __Token__ "http://169.254.169.254/latest/meta-data/iam/security-credentials/" )
# todo: impala attempt to speed up final pass after lstm. </s> self.time_rank_folder = reshape(fold_time_rank=true, scope="time-rank-fold")	__init__ return (q_values, last_internals) if last_internals is not None else q_values elif isinstance(self.action_adapter, BaselineActionAdapter): self.time_rank_unfolder_v = ReShape(unfold_time_rank=True, time_major=True, scope="time-rank-unfold-v") self.time_rank_unfolder_a_probs = ReShape(unfold_time_rank=True, time_major=True,
# todo: choose one from the following two </s> config.add_no_good_cuts = false	check_config config.add_slack = False if config.strategy == "GOA": config.use_tabu_list = True config.add_slack = False
# todo: sync with server </s> self.days = timedelta(days=0)	Defcon async def days_command(self, ctx: Context, days: int = None): if not days: await ctx.send("DEFCON disabled.") else:
# todo: use `summary`, `severity` and `description` </s> raise fbchatfacebookerror(	handle_graphql_errors if errors: error = errors[0]  # TODO: Handle multiple errors "GraphQL error #{}: {} / {!r}".format( error.get("code"), error.get("message"), error.get("debug_info")
# todo: can be done faster by custom code </s> return self.getelemfirstindexfromline(line) == line	isFirstLineOfElem def isFirstLineOfElem(self, line):
#todo remove str() -- http://github.com/fifengine/fifengine/issues/701 </s> soundfile = horizons.globals.db.get_sound_file(str(sound))	play_special if horizons.globals.fife.get_fife_setting("PlaySounds"): a = AmbientSoundComponent() a.play_ambient(soundfile, position=position) horizons.globals.fife.sound.emitter['ambient'].remove(a.__emitter)
def _get_response(self):  # todo: add timeout </s> return url	_prepare_request url = 'http://' + url
# todo:  needs to be moved into rasterdata level api </s> options['raster_x_size'] = data.reader.width	ingest options['bands'] = data.band_indexes options['nodata'] = data.nodata options['raster_y_size'] = data.reader.height options['transform'] = data.reader.dataset.profile['transform']
# todo(john sirois): map target.resources in the same way </s> for target in vt.targets:	post_process genmap.add(source, output_dir, classes) genmap.add(target, output_dir, classes) if is_scalac_plugin(target) and target.classname: basedir = self.write_plugin_info(target)
# todo check the error message here. </s> assert response.status_code == 400	test_single_wrong_format response = self.app.get('/api/person', query_string=params)
# todo: not sure if this is pg only or standard </s> "alter table t alter column c drop not null"	test_alter_column_nullable op.alter_column("t", "c", nullable=True) context.assert_(
# todo: set the following parameter </s> config[setting_name('social_auth_disconnect_pipeline')] = (	_parse_config ) config[setting_name('DISCONNECT_REDIRECT_URL')] = () 'social_core.backends.google_openidconnect.GoogleOpenIdConnect', 'social_core.backends.instagram.InstagramOAuth2'
# todo add shape check </s> raise notimplementederror	_make_hessian_mat_prod def _make_hessian_mat_prod(self, module, g_inp, g_out):
# todo: support data with shape </s> feed_dict = {}	DoInference def DoInference(self, request, context): request_example = json.loads(request.data) for key in self.inputs.keys(): feed_dict[self.inputs[key]] = request_example[key]
# todo: only if successful </s> self.last_phase_migration = current_phase.phasenumber	do_phase_migration ) evaluate_submission(new_submission.pk, current_phase.is_scoring_only) self.save() except PhaseLeaderBoard.DoesNotExist:
pattern = '[aeiou]{2}' # todo: change this to '/[aeiou]{2}/' </s> excluded = set(('.clear()', '.count()')) if py3 else set(('.count()',))	test_filter_exclude_with_regex_string def test_filter_exclude_with_regex_string(self): obj = [] unfiltered_result = see(obj) filtered_result = see(obj).filter_exclude(pattern)
# todo: remove when #980 has been merged </s> info.update(formats[-1])	_real_extract 'duration': int(clip.find('duration').text), } return info
# truffle todo: revert </s> for __x, __y, __z in walk(new_path, topdown, onerror, followlinks): yield __x, __y, __z	walk new_path = join(top, dirname) if followlinks or not islink(new_path): else: for new_path in walk_dirs:
# todo: handle this error </s> except attributeerror:	get_automatic_subscribers try: subscribed_users = getattr(subscription, notification_type) pass return set(subscribed_users)
# todo: use separate index type instead of just storing array </s> return lambda df, i: df._data[i]	get_dataframe_data def get_dataframe_data(df, i):
# todo: if `sqlalchemy` interface, delete key in engines </s> instance.save()	update_connector instance.description = connector['description'] instance.settings = json.dumps(connector['settings']) else: saved_as = True
# todo - need to parameterize this into generate_match_filters, </s> exact_match_filter = "(&(objectclass=posixgroup)%s)" % exact_match_filter	find_groups (exact_match_filter, partial_match_filter) = self.__generate_match_filters( search_fields, criteria_words) partial_match_filter = "(&(objectClass=posixGroup)%s)" % partial_match_filter conn = self.getConnection(opts)
# todo: is this safe? </s> self.cmd( 'iptables -f' )	config localIntf =  self.defaultIntf() self.cmd( 'sysctl net.ipv4.ip_forward=0' ) self.cmd( 'iptables -t nat -F' ) self.cmd( 'iptables -P INPUT ACCEPT' )
# todo check for collision with user filter </s> user_inner = cutoff_freq-transition_width/2	MultistageChannelFilter gr.firdes.WIN_HAMMING) else: limit = next_rate/2 taps = gr.firdes.low_pass(
# todo use trads with %s </s> logger.info(msg['reason'])	create result = cls.call('paas.vhost.create', params) for msg in result: logger.info('\t' + '  '.join(msg['attr'])) return
# todo does a real upgrade (instead of uninstall/install) work? </s> return sequence([	_uninstall_flocker_centos7 def _uninstall_flocker_centos7(): run_from_args([ b"yum", b"erase", b"-y", b"clusterhq-python-flocker",
#todo generate the labels for the dict automatically from labels </s> result = {'time': time_array, 'lc_3to6kev': y1,	parse_obssumm_file dim = np.array(y1).shape[0] time_array = [reference_time_ut + timedelta(0,time_interval_sec*a) for a in np.arange(dim)] 'lc_12to25keV': y2, 'lc_25to50keV': y3, 'lc_50to100keV': y4, 'lc_100to300keV': y5,
# todo make this faster either in c++ or python </s> blender_pass.rect = [[buffer[i], buffer[i + 1], buffer[i + 2]] for i in range(0, len(buffer), 3)]	draw buffer = self.aov_buffers[output_name] blender_pass = render_layer.passes[output_name] engine.end_result(result)
# todo: when repo.subscribe(observer) </s> pass	test_hyperparams_json_repository_should_be_observable_in_memory def test_hyperparams_json_repository_should_be_observable_in_memory(): repo: HyperparamsJSONRepository = HyperparamsJSONRepository()
# todo: test dropping "collision_lines_map" and replacing with "enter/exit" tiles </s> i, m, orientation = item	load else: for item in self.process_line(obj, tile_size): if orientation == "vertical": collision_lines_map.add((i, "left"))
# todo: configurable rsync options? </s> cmd = "rsync -rlvz --delete %s/ %s" %\	rsync_to_remote print("rsync: %s -> %s" % (self.rsync_location, self.rsync_location)) os.chdir(self.temp_dir) (self.temp_dir, self.rsync_location) if self.dry_run:
# todo: and results </s> return asynclist(tasks)	Mount tasks = [mounter.add(path, recursive=recursive) for path in options['<device>']] else: return mounter.add_all(recursive=recursive)
# todo: give a vanilla example </s> attributes	powers_to_states def powers_to_states(powers): ---------- powers: Pandas DataFrame of type {appliance :
#execute 'autodone' on the current command </s> previous_command = self.commandsequencer.prevmode	Cancel print_compact_traceback("bug, ignoring: ") else: if previous_command is not new_mode: if previous_command and not self.command_has_its_own_gui:
# todo: call 'status_callback'? </s> cluster._jobs.append(_job)	reschedule_jobs logger.debug('Rescheduling job %s from %s', _job.uid, _job.node.ip_addr) _job.job.status = DispyJob.Created else: logger.debug('Job %s scheduled on %s abandoned', _job.uid, _job.node.ip_addr)
# todo: this is a jump. </s> vi_cmd_data['motion']['command'] = '_vi_right_parenthesis'	vi_right_parenthesis def vi_right_parenthesis(vi_cmd_data): vi_cmd_data['motion']['args'] = {'mode': vi_cmd_data['mode']} return vi_cmd_data
# todo(mottodora): add reduce option </s> if self.ignore_nan:	forward_cpu self.retain_inputs((0, 1)) diff = (inputs[0] - inputs[1]).ravel() diff[numpy.isnan(diff)] = 0. return numpy.array(diff.dot(diff) / diff.size, dtype=diff.dtype),
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
# todo(evonide) define field </s> self.state = new_state	update_state state_machine = VulnerabilityState(self) state_machine.next_state(new_state) self.last_state_change = datetime.datetime.now()
reorder_attributes(element)  # todo: remove when support is python 3.8+ </s> et.elementtree(element).write(fh, encoding='unicode')	export_to_xml element.tail = element.tail.strip(' ') fh.write('  ') fh.write('</materials>\n')
if not version_2_79_or_older():  # todo </s> col = box.column(align=true)	draw draw_smc_ui(context, col) elif context.scene.optimize_mode == 'MATERIAL': row = col.row(align=True) row.scale_y = 0.75
# todo: do something more than simply selecting the last match? </s> matches = self.class_dict[self.hash_schema(schema)]	from_dict rootschema = rootschema or schema if cls is None: cls = matches[-1] if matches else self._passthrough schema = _resolve_references(schema, rootschema)
# todo: deprecation warning </s> queryset = queryset.filter(**{self.slug_field: slug})	get_object queryset = queryset.filter(pk=pk) elif slug is not None: else: raise AttributeError("Generic detail view %s must be called with "
# todo(brett.cannon) implement </s> raise importerror	_default_hook def _default_hook(self, path): If the path will not work for the default hook then raise ImportError.
# todo(mhickey): get rid of it once we switch the db model to using </s> def modify_fields_to_db(self, fields):	AllowedAddressPair 'ip_address': obj_fields.IPAddressField(), } result = super(AllowedAddressPair, self).modify_fields_to_db(fields) if 'ip_address' in result:
# todo(b/34288791): this needs to be exactly the same as in impl.py </s> copied_inputs = impl_helper.copy_tensors(input_signature)	_build_analysis_graph_for_inspection input_signature = impl_helper.feature_spec_as_batched_placeholders( feature_spec) output_signature = preprocessing_fn(copied_inputs) transform_fn_future, cache_dict = build(
## todo : log error </s> error_code = delete_functionparameters_doctype_submission(doctype=doctype, action=action)	_delete_submission_from_doctype user_msg.append("""When deleting Submission "%s" from Document Type "%s", it wasn't possible to delete all Submission Fields""" \ % (action, doctype)) if error_code != 0: user_msg.append("""When deleting Submission "%s" from Document Type "%s", it wasn't possible to delete all Function Parameters""" \
# todo: remove this after we create the contents web service and directories are </s> raise notimplementederror('must be implemented in a subclass')	list_dirs def list_dirs(self, path):
@unittest.skipif(not settings.dev_mode, 'dev_mode disabled, osf.users.create unavailable')  # todo: remove when available outside of dev_mode </s> def test_properly_scoped_token_does_not_send_email_without_kwarg(self, mock_auth, mock_mail):	test_properly_scoped_token_does_not_send_email_without_kwarg @mock.patch('framework.auth.views.mails.send_mail') @mock.patch('api.base.authentication.drf.OSFCASAuthentication.authenticate') token = ApiOAuth2PersonalToken( owner=self.user,
# todo: i am relying on order preservation here... </s> assert top.indices == bottom.indices, (	extruded_horizontal_facet stmt = [] for top, bottom in zip(top_sks, bottom_sks): "Top and bottom kernels must have the same indices" )
# todo: this should be solved via plugins </s> alter_foreignkey_to_int('recipes_oldrecipearticleredirect', 'new_id')	alter_self_foreignkeys alter_foreignkey_to_int('articles_articlecontents', 'article') if 'recepty.recipes' in settings.INSTALLED_APPS:
# todo: in #5022 </s> if menu_item.linked_object:	get_menu_item_as_dict def get_menu_item_as_dict(menu_item): data = {} data["url"] = menu_item.linked_object.get_absolute_url() else:
# todo - take this out of the menu </s> self.listening_for_connections.set(not self.listening_for_connections.get())	start_listening def start_listening(self, **kwargs): self.allow_connections(**kwargs) return
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
pass # todo </s> def endcdata(self):	endCDATA
# todo: 289 </s> if federated_only:	make_arrangements federated_only=False) -> None: Create and consider n Arangement objects from sampled miners. ursulas = network_middleware.ursulas else:
# todo: remove once elasticsearch v6.x is deprecated. </s> if self._getclientmajorversion() < 7:	WriteHeader } } mappings = {self._document_type: mappings} with self._timesketch.app_context():
# todo: find the operation that does not properly close the oblivion\data dir. </s> destdir = dirs['mods'].head + u'\\data'	InstallerArchive try: if count: stageDataDir = stageDataDir + u'\\*' balt.shellMove(stageDataDir,destDir,progress.getParent(),False,False,False)
assert study_id == 0  # todo(akiba) </s> self.trials[trial_id].state = state	set_trial_state def set_trial_state(self, study_id, trial_id, state):
# todo: need token </s> val = self.mem.get(node.name)	ArithEvaluator cflow, and then the integer result can be ArithEval.Result() if node.tag == arith_expr_e.RightVar: if val.tag == value_e.Undef: return 0
pass # todo: explain </s> self.setmessage('uri', rs.server_uri_too_long, uri_len=len(self.response.uri))	status414 def status414(self):        # Request-URI Too Long
# todo: when dropping python 3.6, use </s> def run_coroutine(coro):	run_pyppeteer temp_file.write(html.encode('utf-8')) try: loop = asyncio.new_event_loop() asyncio.set_event_loop(loop)
# todo: replace this root with tree hash root </s> proposal_root = proposalsigneddata(	validate_proposer_signature epoch_length: int) -> None: block_without_signature_root = BaseBeaconBlock.get_block_without_signature_root(block) state.slot, beacon_chain_shard_number,
# todo: handle agg_columns. </s> with self.assertraisesregex(typeerror, "<class 'int'> object is not callable"):	test_filter pdf.groupby(pdf["b"] // 5).filter(lambda x: any(x.a == 2)).sort_index(), ) kdf.groupby("b").filter(1) columns = pd.MultiIndex.from_tuples([("x", "a"), ("x", "b"), ("y", "c")])
# todo: remove from self.workers (except that detached() should get </s> log.err(why, 'slave failed to attach')	_not_attached def _not_attached(self, why, slave): self.builder_status.addPointEvent(['failed', 'connect', slave.workername])
# todo(mattjj): support argument pattern-matching </s> assert not any(type(invar) in (tuple, list) for invar in jaxpr.invars)	jaxpr_computation def jaxpr_computation(jaxpr, const_vals, freevar_shapes, *arg_shapes): c = xb.make_computation_builder("jaxpr_computation") def read(v):
logfile = open('logs/exceptions.log', 'a') #todo: make not hardcoded </s> logfile.write('critical exception in core')	run except: pass logfile.write(e) logfile.write(trace)
# todo instead of indexing and filtering later </s> "hidden": bool(f & torrentflags.hidden),	reindex_torrent "remake": bool(f & TorrentFlags.REMAKE), "complete": bool(f & TorrentFlags.COMPLETE), "deleted": bool(f & TorrentFlags.DELETED), "has_torrent": bool(t['has_torrent']),
# todo do a proper mro resolution. currently we are just listing </s> from jedi.inference.gradual.typing import typeddictbase	is_typeddict @inference_state_method_cache(default=False) def is_typeddict(self): for lazy_cls in self.py__bases__(): if not isinstance(lazy_cls, LazyTreeValue):
pass  # todo </s> def get_xl_sheet(xl_workbook, sheet_name_or_index):	get_xl_sheet
# todo: change href="$help:command" to href="help.html#command" </s> return ''.join(' %s="%s"' % (k, v) for (k, v) in attrs)	_AttrsToString if not attrs: return ''
# todo return the property set too. see #1086 </s> dag = pass_manager.run_passes(dag)	transpile final_layout = None if pass_manager: else: basis = basis_gates.split(',') if basis_gates else []
"""todo doc me""" </s> table = [['foo', 'bar', 'baz'],	test_profile_row_lengths def test_profile_row_lengths(): ['A', 1, 2], ['B', '2', '3.4'],
# todo: remove the coverage limitation with further mandatory model attributes </s> else:   # coverage: ignore branch	system_exporter_spreadsheet_csv_config_view info_logger(str(request.user), " SYSTEM_EXPORTER_SPREADSHEET_CSV_CONFIG_CHANGED") return HttpResponse('<script type="text/javascript">window.close();</script>') return render( request,
# todo: add highlighting line </s> return 0	disassemble_at_address if line_index_for_address >= 0: self.verticalScrollBar().setValue(line_index_for_address) if self._range is None: self._range = Range(self._app_window.dwarf)
# todo(b/184055743): once tensorflow is released with cl/342914534, </s> tf.io.sparsefeature(['x$sparse_indices_0', 'x$sparse_indices_1'],	test3dSparseWithTFXIO tf.float32, [5, 5], already_sorted=True), 'x$sparse_values', tf.float32, [-1, -1],
expr,  # todo rethink this circular import </s> )	lll_for_self_call def lll_for_self_call(stmt_expr, context): from vyper.old_codegen.expr import ( pos = getpos(stmt_expr)
# todo: empty env? </s> sendrc=false, timeout=120, usepty=false, environ={},	test_simple_unicode_args Obfuscated('hushnow', 'XXXXXXXX'), 'client', '-i'], self.basedir, initialStdin=client_spec) + 0,
# todo(shoyer): test fails on tpu </s> if jtu.device_under_test() == "tpu":	test_custom_linear_solve_without_transpose_solve def test_custom_linear_solve_without_transpose_solve(self): raise SkipTest("Test fails on TPU") def explicit_jacobian_solve(matvec, b):
# todo: bindings should be done in collection class: </s> metacollection.conjunctive_graph.bind('dlns', dlns)	__call__ for remote in local_master.git_get_remotes()] + [local_master.get_backend_from_branch()]) query_string = """SELECT ?g ?r {GRAPH ?g {?r rdf:type dlns:Collection . ?s ?p ?o .
# todo: use the kinetic scroller if implemented </s> view = self.parent().parent()	stopDrag def stopDrag(self): self._dragging = False view.stopScrolling() if self.width() < 8 and self.height() < 8:
# todo(b/141131288): enable test once complex sort is supported on tpu. </s> if (jnp.issubdtype(dtype, jnp.complexfloating)	testSortComplex for axis in [None])) def testSortComplex(self, dtype, shape, axis): and jtu.device_under_test() == "tpu"): self.skipTest("complex sort not supported on TPU")
# todo: same code as for batch gradient, but with sum_batch = true </s> derivatives=derivatives)	__init__ req_output=req_output,
# todo: https://github.com/python/mypy/issues/3004 </s> return doc	__call__ token._.set(KNP_USER_KEYS.morph.element, m) doc.ents = filter_spans(doc.ents + tuple(_extract_knp_ent(doc)))  # type: ignore
# todo: kwargs </s> def _impl(df, axis=none, skipna=none, level=none, numeric_only=none,	sum_overload def sum_overload(df, axis=None, skipna=None, level=None, numeric_only=None, min_count=0): min_count=0): return hpat.hiframes.pd_dataframe_ext.sum_dummy(df)
our_balance = our_balance - have_amount  #todo i think this line is unnecessary here </s> want_amount = have_amount / round(band.avg_price(target_price))	top_up_buy_bands have_amount = Wad.min(band.avg_amount - total_amount, our_balance) if (have_amount >= band.dust_cutoff) and (have_amount > Wad(0)): if want_amount > Wad(0): order = self.radar_relay.create_order(maker_token_amount=have_amount,
# todo: this should maybe go somewhere else </s> namespace['logical_not'] = np.logical_not	code_object def code_object(self, code, namespace, specifiers): return PythonCodeObject(code, namespace, specifiers, self.compile_methods(namespace))
# todo: remove </s> if context.dispatcher.authenticated:	commands if context.dispatcher.authenticated or not handler.auth_required: command_names.add(name) command_names.update(c.name for c in protocol.mpd_commands) else:
# todo(elliot): how to determine bundle id </s> "id": "%bundleid%",	handle_download_recipe_input "pkgname": "%NAME%-%version%", "version": "%version%", "options": "purge_ds_store", "chown": (
#todo can the fetchparser code be adapted for use here? </s> folder_text = none	_proc_folder_list folders = [] for line in folder_data: if isinstance(line, tuple): folder_text = line[-1]
# todo: handle na as 1st value </s> first_val = s.iloc[0]	_infer_series_dtype def _infer_series_dtype(S): if S.dtype == np.dtype('O'): if isinstance(first_val, list): return _infer_series_list_dtype(S)
#todo: raise an exception: unexpected date format </s> pass	pipe_datebuilder pass else: yield date
# todo: verify </s> manager.bind(bind)	test_bind manager = factory.consumer_agent_manager()
raise notimplementederror  # todo(mattjj,frostig) </s> def post_process_call(self, _, out_tracer):	post_process_call
log("dispersy.log", "handled-barter-record") # todo: maybe move to barter.log </s> for _ in messages:	handled_function def handled_function(messages):
# todo is this check necessary; this was an assertion which are disabled in <4000 which is good </s> if len(name) != 1:	_set def _set(view, name: str, values: list, linewise: bool = False) -> None: name = str(name) raise ValueError('Register names must be 1 char long: ' + name) if name == _BLACK_HOLE:
# todo read2 is silently discarded </s> return	ProcessedReadWriter if self.too_short_outfile is not None: read1.write(self.too_short_outfile) if len(read1.sequence) > self.maximum_length: self.too_long += 1
# todo %b, %(fmt)t, plus "the standard ones in printf(1)" </s> raise notimplementederror	Printf parts.append(str(num)) else: result = ''.join(parts) if arg.v:
# todo disjunct can be fathomed? </s> else:	_apply_to elif results.solver.termination_condition is tc.infeasible: disj_ub = None raise NotImplementedError( "Unhandled termination condition: %s"
with open(filename, 'r', encoding='iso-8859-1') as f:  # todo: solve iso encoding pb ! </s> for line in f:	loadLines dict<??>: the extracted fields for each line lines = {} values = line.split(" +++$+++ ") lineObj = {}
# todo: fix this 405 error </s> data = rv.data.decode('utf-8')	test_model_list_order rv = client.post('/model1view/list?_oc_Model1View=field_string&_od_Model1View=asc', follow_redirects=True) rv = client.post('/model1view/list?_oc_Model1View=field_string&_od_Model1View=desc', follow_redirects=True)
# todo move to function </s> time_string = datetime.now().strftime("%y%m%d_%h%m")	__init__ if not self.lock_file: self.lock_file = '/tmp/%s.lock' % self.program_name self.backup_root_subdirectory = "%s/%s" % (self.backup_name, time_string) self.backup_root_directory = "%s/%s" % (self.backup_location, self.backup_root_subdirectory)
# todo: test this case </s> b.shape += (1,) * -difference	_reconcile elif difference < 0: if b.ndim: else: b = full_like(a, b)
# @todo: deprecate </s> ftable.popup_fields,	get_location_data if layer_id: layer = db(ftable.layer_id == layer_id).select(ftable.attr_fields, ftable.individual, ftable.trackable,
# todo: supports blocking queries and all consistency modes </s> params = {}	nodes Returns the nodes known about in the *dc* datacenter. *dc* defaults to the current datacenter of this agent. if dc: params['dc'] = dc
asynchronous=false, # todo: (true) when jconnor fixes </s> archive=true,)	install resources=resources, weight=0, result = execution.execute_async(self, call_request) return result
# @todo this needs to be using domain fronting to defeat censorship </s> response = requests.get(endpoint)	censorship_obtain_map locally for further look-ups if required. endpoint = "https://bridges.torproject.org/moat/circumvention/map" self.censorship_map = response.json() self.log("Common", "censorship_obtain_map", self.censorship_map)
# todo: maybe a chardet integration </s> file_descriptor.seek(-3, 1) # rewind of 3 chars	detect_encoding bom = file_descriptor.read(3) if not bom in cls.BOMS: return cls.DEFAULT_ENCODING return cls.BOMS[bom]
# todo: implement these buttons </s> self.edit_button = b = qpushbutton(qicon(i('modified.png')), _('&edit this recipe'), w)	RecipeList l.addWidget(la) l.setSpacing(20) b.setSizePolicy(QSizePolicy(QSizePolicy.Fixed, QSizePolicy.Fixed)) l.addWidget(b)
# todo: check whether it's already installed?. see yum notes  yum list installed "$@" >/dev/null 2>&1 </s> cmd += 'yum install'	install opts += ' --reinstall' elif install_type == 'yum': if 'yum' in options: opts = options['yum']
# todo: make this query also check over the alias table. </s> new_contrib_sql = s.sql.text("""	insert_facade_contributors self.logger.info( "Beginning process to insert contributors from facade commits for repo w entry info: {}\n".format(repo_id)) SELECT DISTINCT commits.cmt_author_name AS NAME,--commits.cmt_id AS id,
# todo: this is not the correct setting. set hyperparameters correctly </s> ecfp_power = 5	load_pdbbind labels.append(log_label) if featurizer == "grid": splif_power = 5 featurizer = rgf.RdkitGridFeaturizer(
#todo: dry [1] </s> if duration and end:	__init__ self.name = name self.begin = begin raise ValueError( 'Event() may not specify an end and a duration \
candidates.sort() # todo: this sort has side effects </s> candidates[0][1].activesite = self	getElementAtOrBefore nearestTrailSpan = span if len(candidates) > 0: return candidates[0][1] else:
cursor.execute('''delete from mempool_messages''')  # todo </s> for mempool_message in mempool_messages:	follow pass with db: try: cursor.execute('''INSERT INTO mempool_messages VALUES(:tx_hash, :command, :category, :bindings)''', (mempool_message))
# todo: using get_fft_info is digging into the implementation </s> top.set_source_name('s1')	test_monitor_source_switch 's2': SimulatedDeviceForTest(freq=freq2), }) self.assertEqual(top.state()['monitor'].get()._get_fft_info()[0], freq1) top.set_source_name('s2')
# todo: implement this </s> self.lineedit_hexview.textedited.connect(self.lineedit_hexview_text_edited)	HexEditDialogForm self.refresh_view() self.lineEdit_AsciiView.selectionChanged.connect(self.lineEdit_AsciiView_selection_changed) self.lineEdit_AsciiView.textEdited.connect(self.lineEdit_AsciiView_text_edited) self.pushButton_Refresh.pressed.connect(self.refresh_view)
#todo:  raise error </s> print(response.json())	delete else:
# todo: overwrite step's self return value on fitting. </s> data_inputs = step.fit_transform(data_inputs, expected_outputs)	ResumablePipelineRunner def fit_transform(self, data_inputs, expected_outputs=None): for step_name, step in self.steps_as_tuple[:-1]: processed_outputs = self.steps_as_tuple[-1][-1].fit_transform(data_inputs, expected_outputs) return processed_outputs
# todo(mordred) when this changes to rest, force interface=admin </s> with _utils.shade_exceptions(	create_endpoint if region is not None: kwargs['region'] = region "Failed to create endpoint for service" " {service}".format(service=service['name'])
# todo behind </s> f.write(u" behind todo")	print_imspec f.write(u" as %s" % (imspec[2], )) if imspec[6] is not None: if imspec[4] != 'master': f.write(u" onlayer %s" % (imspec[4], ))
# todo: we could assert here that latest_version matches x.y.z. </s> return version	latest_release_version e.g. "2.1.1" version = latest_release_tag()[len('ckan-'):]
# todo detect for typeerror: duplicate base class str, </s> try:	py__mro__ for lazy_cls in context.py__bases__(): for cls in lazy_cls.infer(): cls.py__bases__ except AttributeError:
# todo: currently not used by pyrep. </s> parameterlength = len(parametervalue)	simSetUserParameter def simSetUserParameter(objectHandle, parameterName, parameterValue): ret = lib.simSetUserParameter( objectHandle, parameterName.encode('ascii'),
# todo make private methods private </s> homebrew_repo.remotes.origin.push(homebrew_repo.head)	publish_homebrew_recipe homebrew_repo.index.commit('Add recipe for Flocker version ' + version)
# todo: can we assume reverse=false? </s> signals.m2m_changed.send(	add .filter(**self._lookup_kwargs())) new_ids = list(new_ids - set(vals)) sender=self.through, action="pre_add", instance=self.instance, reverse=False,
# todo: check against cygwin </s> environment_id = files[0]	__init__ shutit_global.shutit.fail('Wrong number of files in environment_id_dir: ' + environment_id_dir) else:
# todo - unittest this </s> for i, arg in enumerate(cmdline):	get_runsvdir_dir_from_cmdline def get_runsvdir_dir_from_cmdline(cmdline): if arg == '-P' and len(cmdline) > i + 1: return cmdline[i + 1]
# todo: write this </s> pass	catmull_rom_patches def catmull_rom_patches():
# todo -- validate other options </s> die("[%s] has no 'username'" % target)	_validate_github if not config.has_option(target, 'username'):
# todo delete me </s> import yaml	importSDF model['links'] = links model['materials'] = materials print(yaml.dump(materials)) joints = {}
#todo: consider factoring out: some duplication between xliff and tmx </s> text = errorname + ': ' + errortext	adderror def adderror(self, errorname, errortext): self.addnote(text, origin="pofilter")
if self.source_file:  # todo: should we error here or something if the source_file doesn't exist? </s> content = self.spec.apply(self.source_file)	generate def generate(self, save=True): return self.storage.save(self.name, content)
# todo: implement </s> pass	to_rst @classmethod def to_rst(cls):
#obj_url = url_for(self.get_endpoint(), **params) # doesn't work :(, todo : why? </s> obj_url = url_for(object.get_endpoint())	jsonapi_encode SAFRS currently implements links with self try: if not obj_url.endswith('/'): obj_url += '/'
@unittest.skipif(not settings.dev_mode, 'dev_mode disabled, osf.users.create unavailable')  # todo: remove when available outside of dev_mode </s> def test_properly_scoped_token_can_create_and_send_email(self, mock_auth, mock_mail):	test_properly_scoped_token_can_create_and_send_email @mock.patch('framework.auth.views.mails.send_mail') @mock.patch('api.base.authentication.drf.OSFCASAuthentication.authenticate') token = ApiOAuth2PersonalToken( owner=self.user,
# todo(shoyer): test fails on tpu </s> if jtu.device_under_test() == "tpu":	test_root_vector def test_root_vector(self): raise SkipTest("Test fails on TPU") def oracle(func, x0):
pass  # todo </s> def get_xl_sheet(xl_workbook, sheet_name_or_index):	get_xl_sheet
# todo should this be null cut? </s> partition=subsystem.null_cut,	big_mip return BigMip(subsystem=subsystem, phi=0.0, unpartitioned_constellation=[], partitioned_constellation=[])
# todo: underscore </s> block.has_replaced_expressions = false	setup_sensitivity instance.add_component(self.get_default_block_name(), block) self.block = block block.sens_data_list = [] block.paramList = paramList
#todo: gst.numberoftableentries = len(confs) </s> idx = confs.index(guid)	CoreInstallConfigurationTable if guid not in confs: confs.append(guid) ptr = ql.loader.efi_conf_table_array_ptr + (idx * EFI_CONFIGURATION_TABLE.sizeof()) guid_bytes = UUID(hex=guid).bytes_le
# todo model? </s> cut_result_json = os.path.join(data_home, 'cut_result.json')	classify compress_rate: float = 0.2, limit: int = None): res = None stable = None
# todo pseudo code: </s> pass	SetPosition @dbus.service.method(dbus_interface=player_interface) def SetPosition(self, track_id, position):
sleep(1)    # todo </s> cancel_order_match(db, bad_order_match, 'penalty', block_index)	exact_penalty for bad_order_match in bad_order_matches: print('PENALTY: ', bad_order['tx_hash'])    # TODO cursor.close() return
# todo wait_for_unit_state? why (not)? </s> def get_ips(units):	get_node_ips d.addCallback(lambda _: client.add(node_2_name, image)) d.addCallback(lambda _: client.list()) docker = Client() prefix = u'flocker--' + namespace + u'--'
# todo: write tests </s> <instrdef> (instrument definition)---midi instrument declaration.	instrDefFromElement def instrDefFromElement(elem): In MEI 2013: pg.344 (358 in PDF) (MEI.midi module) :returns: An :class:`Instrument`
# todo: return false to disable selection. </s> return true	selectionShouldChangeInTableView_ @objc_method def selectionShouldChangeInTableView_(self, table) -> bool:
#todo: manage this </s> pass	get_return_type_indexes start = data[idx:].find(first) + idx if self.type in ['params', 'unknown'] and (start, end) == (-1, -1): return (start, end)
# todo(stephenfin): the mock of 'migrate_disk_and_power_off' should </s> with mock.patch(	test_resize_bug_1879878 flavor_b_id = self._create_flavor( vcpu=2, extra_spec={'hw:cpu_policy': 'dedicated'}) 'nova.virt.libvirt.driver.LibvirtDriver' '.migrate_disk_and_power_off', return_value='{}',
if self.idx_dim != 0: raise notimplementederror  # todo... </s> if self.batch_dim != 1: raise notimplementederror  # todo...	perform else: assert False, self.wrap_mode if inplace_increment is None: raise NotImplementedError("need Numpy 1.8 or later") inplace_increment(D_array_and_pad, (idxs, numpy.arange(n_batches)), D_beam)
# todo: handle hiframes filter etc. </s> if isinstance(inst, parfor):	_gen_rebalances new_body = [] for inst in block.body: self._gen_rebalances(rebalance_arrs, {0: inst.init_block}) self._gen_rebalances(rebalance_arrs, inst.loop_body)
# todo: is there any way to move this to serializer? </s> return {	state_row population = case.estimated_population_2019 state_name = case.state "city": state_name, "city_ibge_code": case.city_ibge_code,
# todo: throw a useful error message </s> with ng.op.all_ops() as ops:	layer_wrapper if prev_inp.axes.sample_axes() != inp.axes.sample_axes(): pass output = f(self, in_obj, *inputs, **kwargs) if self.ops is None:
assert study_id == 0  # todo </s> self.trials[trial_id].params[param_name] = value	report_param def report_param(self, study_id, trial_id, param_name, value):
#todo load this from somewhere </s> pad_data = [-1.46374, -0.151816, -0.161173, 0.0686325, 0.0231148, -0.154613,	allocate_devices device.targets[:l, q] = self.data.targets[self.data.seq_start[s] + batch.start[1]:self.data.seq_start[s] + batch.start[1] + l] if self.pad_batches: -0.105614, 0.00550198, 0.0911985, 0.00502809, 0.0512826, -0.0181915, 0.0225053, -0.00149681, 0.0782062, 0.0412163, 0.0526166, -0.0722563,
#ack = self.serial_port.read() # todo: use ack </s> self.sendcommand(177) # 10110001	SetBothLaserOff def SetBothLaserOff(self):
response.set_cookie(cookie_name, cookie, 31536000, path='/') # todo: move cookie max_age to settings </s> return response	rating_changed_response response.delete_cookie(cookie_name) else:
# todo remove this sleep to reproduce http://tracker.ceph.com/issues/8107 </s> import time	test_lifecycle self._create(cluster_id, pool_name, pg_num=64) pool_id = self._assert_visible(cluster_id, pool_name)['id'] time.sleep(10) self._update(cluster_id, pool_id, {'pg_num': 128})
#@todo: 1) perform some sort of check to test the export works </s> browser = self.browser	test_export_volunteers print "\n" self.login(account="admin", nexturl="vol/volunteer/search") browser.find_element_by_xpath("//img[@src='/eden/static/img/pdficon_small.gif']").click()
recording_software_name = none  # todo </s> recording_software_version = none  # todo	recording_update_pupil_mobile_to_pprf_2_0 start_time_synced_s = None  # TODO duration_s = None  # TODO recording_name = None  # TODO system_info = None #TODO
# todo: use idc.nexthead(ea) instead... </s> ea += self.arch.insn_size	trace if idaapi.is_basic_block_end(ea): break return (None, None, None)
# todo fixme is that utc??? </s> tss = tsc.replace('juli', 'jul').replace('aug.', 'aug')	measurements for tsc, tempc in datas: if oldfmt: dt = datetime.strptime(tss, '%Y-%b-%d %H:%M') temp = tempc
# todo: add rotary inertia </s> mass = elem.mass()	build_Mgg Mbb[j1, j1] = Mbb[j1+1, j1+1] = mass / 2 elif etype in ['CBAR', 'CBEAM']: nid1, nid2 = elem.nodes i1 = dof_map[(nid1, 1)]
# todo: replace with positional arguments when we drop python 2 support. </s> cache_alias = kwargs.pop('cache_alias', none)	invalidate :returns: Nothing :rtype: NoneType db_alias = kwargs.pop('db_alias', None) for k in kwargs:
# todo : accept pay and keywords as parameters too </s> now = datetime.utcnow()	filter_categories def filter_categories(geonameids=[], filtered_types=[]): basequery = db.session.query(JobPost.category_id).filter(JobPost.status.in_(POSTSTATUS.LISTED), JobPost.datetime > now - agelimit) if filtered_types:
size = ()  # todo(kataoka): remove this after #4615 is merged </s> out = cupy.empty(size, dtype=dtype)	_random_sample_raw dtype = _check_and_get_dtype(dtype) if size is None: if dtype.char == 'f': func = curand.generateUniform
# todo: create dicts as we go, for now we can only assign into existing namespaces </s> namespace = metainfo	mainloop if val and val[0] in "+-" and val[1:].isdigit(): val = int(val, 10) for key in field.split('.')[:-1]: namespace = namespace[key]
@unittest.skip('not written')  # todo: finish! </s> self.assert_shortened([value], "[b'aaaaaaaaaaaaaaaaaa...aaaaaaaaa']")	test_bytes_large "b'" + 'A' * 43688 + "..." + 'A' * 21844 + "'")
# todo: add supported sources when implemented </s> return none	supported_source_kinds @property def supported_source_kinds(self):
# todo - needs tests </s> return {	payments_settings def payments_settings(request): "STRIPE_PUBLIC_KEY": settings.STRIPE_PUBLIC_KEY, "PLAN_CHOICES": settings.PLAN_CHOICES,  # possibly nuke
# todo(yanase): change `task` in storages to `direction`. </s> self.storage.set_study_task(self.study_id, _direction)	__init__ 'Optimization direction of study {} is set to `MAXIMIZE`. ' 'Currently, Optuna supports `MINIMIZE` only.'.format(study_name))
# todo: may use sys.stdout.encoding if output_file = '-' </s> output_encoding = output_encoding or default_output_encoding	sum_ result.order_by(order_by[0].replace('^', '-')) export_fields = _get_export_fields(result.field_names, fields_exclude) if output_locale is not None: with rows.locale_context(output_locale):
# todo: change download_list to list </s> assert(not md5 or is_file_existed(download_path, md5))	request_file download_file(download_url, download_path, verbose=verbose)
# todo: 评估对所有订阅都只取title作为id的效果和影响，考虑基于内容相似度的算法 </s> return title or link	_extract_story_ident_cnki def _extract_story_ident_cnki(cls, guid, title, link): link = link.rsplit('#', 1)[0]
# todo: this is an important operation that should be controlled </s> return "python"	_python_cmd def _python_cmd(_project_op):
# todo: it's a stub before we implement compute_per_cycle_transition </s> crystallized_state = crystallized_state.copy(	compute_per_cycle_transition block: BaseBeaconBlock) -> Tuple[CrystallizedState, ActiveState]: Initialize a new cycle. last_state_recalc=crystallized_state.last_state_recalc + cls.config.CYCLE_LENGTH )
# todo: add more complicated testcases </s> assert_equal(score.shape, (4,))	test_moa_dynamic_repeat random_state=42)
# todo(sbiswas7): remove this check once we move to glance v2 </s> if conf.glance.use_glance_v1:	get_default_image_service def get_default_image_service(): return GlanceImageService() else:
# todo(pdmars): this should probably be changed to a more generic </s> db_infos = dbinstance.find_all(deleted=false)	load_accounts_summary def load_accounts_summary(cls): tenant_ids_for_instances = [db_info.tenant_id for db_info in db_infos] tenant_ids = set(tenant_ids_for_instances)
# todo(shafey, yonghui): fetch axis_name from globals. </s> sum_v = jax.lax.psum(sum_v, axis_name='batch')	compute_moments jnp.ones_like(inputs) * mask, axis=reduce_over_dims, keepdims=keepdims) if enable_cross_replica_sum_on_tpu: count_v = jax.lax.psum(count_v, axis_name='batch') count_v = jnp.maximum(count_v, 1.0)
# todo: use parameter names for run_in_executor() once python 3.4.4 is released. </s> task = loop.run_in_executor(	run_command_ec2 tasks = [] for instance in cluster_instances: None, functools.partial(
raise notimplementederror #todo </s> elif self.action == "append":	Action setattr(focus, attr, value) elif self.action == "PREPEND": raise NotImplementedError #TODO elif self.action == "MERGE":
# todo: don't mess with the user's cursor. </s> sel = self.view.sel()	on_diagnostics_navigate def on_diagnostics_navigate(self, href, point, diagnostics): sel.clear() sel.add(sublime.Region(point, point))
pass  # todo </s> def get_threads(self):	get_threads
# todo: add tests </s> if ":" in pkg_name:	install_bucket self._force_load_apt_cache() for pkg_name in bucket: (pkg_without_arch_name, arch) = pkg_name.split(":", -1) if arch == get_current_arch():
# todo - this isn't actually the correct way to set the vary header, </s> response.headers['allow'] = ', '.join(self.allowed_methods)	dispatch except ErrorResponse, exc: response = exc.response response.headers['Vary'] = 'Authenticate, Accept' return self.emit(response)
## todo - fix - this breaks easily </s> if v.strip().startswith('return ') and '*'+gt != return_type:	_hack_return def _hack_return(self, v, return_type, gname, gt, node): if gname in v and v.strip() != 'return self': if '(' not in v:
# todo: does it occur when opening a file with line numbers in it? </s> concrete_index = self.index(index)	direct_insert def direct_insert(self, index, chars, tags=None, **kw): line_before = self.get(concrete_index + " linestart", concrete_index + " lineend") self._last_event_changed_line_count = "\n" in chars
# todo yield str </s> state = 0	cpre2_parse elif state == 20: # "str if c == '"': elif c == "\\": state = 21 else: laststr += c
return 0 # todo </s> def _to_mopidy_id(self, spotify_uri):	_to_mopidy_id
# todo(halldor): not tested... </s> self._mailbox.append(self.mailbox, message=message)	add def add(self, message):
# todo: config option? </s> [sub_orig_hexsha, target_commit]) == target_commit:	_install_subds_from_flexible_source if sub.get_merge_base(
# todo: remove this method in v2.5 </s> elif self._values['enabled'] in booleans_true:	Parameters if self._values['state'] == 'enabled': return True return True elif self._values['enabled'] in BOOLEANS_FALSE:
# todo: we should run multithreaded at some point </s> cache = synchronizeds3cache(bucket, cachedir, int(options.cachesize * 1024), dbcm,	main cachedir = tempfile.mkdtemp() + b'/' options.s3timeout = s3.LOCAL_PROP_DELAY * 1.1 timeout=options.s3timeout) try:
# todo make sure we can still read an unconstrained successor </s> self._windup_to_unconstrained_successor()	_read_in_global_data_with_gets chain_bvv = self.crash.state.BVV(chain.payload_str()) self.crash.state.add_constraints(chain_mem == chain_bvv) glob_data = self.crash.state.memory.load(read_to, len(data)) data_bvv  = self.crash.state.BVV(data)
# todo: remove when #980 has been merged </s> info.update(formats[-1])	_real_extract 'thumbnail': self._og_search_thumbnail(webpage), } return info
# todo: finish this. </s> def mkdir(self, path, mode):	mkdir pass
# todo: https://twistedmatrix.com/trac/ticket/10137 </s> if self._buffer.startswith(b"\r\n"):	_dataReceived_TRAILER @returns: C{False}, as there is either insufficient data to continue, or no data remains. data = memoryview(self._buffer)[2:].tobytes() del self._buffer[:]
# todo: fixup when moving to python 3.3 </s> if (sys.version_info[0] == 2):	__unicode__ def __unicode__(self): return 'http://httpbin.org/get'.decode('utf-8') else:
# todo: this is a jump. </s> def run(self, mode=none, count=1, search_string='', forward=true):	_vi_repeat_buffer_search class _vi_repeat_buffer_search(ViMotionCommand): if forward: self.view.run_command('_vi_slash_impl', {
'alexnet'       : [testmodels.caffeemit, testmodels.cntkemit, testmodels.coremlemit, testmodels.mxnetemit, testmodels.pytorchemit, testmodels.tensorflowemit], # todo: testmodels.kerasemit('tensor' object has no attribute '_keras_history') </s> 'inception_v1'  : [testmodels.caffeemit, testmodels.cntkemit, testmodels.coremlemit, testmodels.kerasemit, testmodels.mxnetemit, testmodels.pytorchemit, testmodels.tensorflowemit],	get_test_table return { 'caffe' : { 'inception_v4'  : [TestModels.CntkEmit, TestModels.CoreMLEmit, TestModels.KerasEmit, TestModels.PytorchEmit, TestModels.TensorflowEmit], # TODO TestModels.MXNetEmit(Small error), TestModels.CaffeEmit(Crash for shape) 'resnet152'     : [TestModels.CaffeEmit, TestModels.CntkEmit, TestModels.CoreMLEmit, TestModels.KerasEmit, TestModels.MXNetEmit, TestModels.PytorchEmit, TestModels.TensorflowEmit],
# todo: python3 </s> return_me['obj'] = pickle.loads(base64.b64decode(deserialized['obj']))	_deserialize if self.serializer != 'pickle': raise Exception("Security exception: Won't unpickle if not set to pickle.") elif deserialized['serializer'] == 'json': return_me['obj'] = deserialized['obj']
# todo use an interface here and move the check inside </s> if isinstance(v, exportedgetter):	__decorator_cells if not hasattr(self, k): continue v = getattr(class_obj, k) if not k.startswith('get_'): raise LookupError('Bad getter name', k)
# todo: re-enable when python3 is available on darwin </s> mx.log("running tests with cpython")	gate_unittests mx.command_function("python")(["--python.CatchAllExceptions=true"] + pre_args + test_args + post_args) if platform.system() != 'Darwin': mx.run(["python3"] + test_args, nonZeroIsFatal=True)
# todo: add savedmodel support for sparse inputs. </s> example_feature_columns = {}	test_model_to_saved_model_dense_inputs def test_model_to_saved_model_dense_inputs(self): example_feature_columns.update(self.example_feature_columns) del example_feature_columns["unigrams"]
# todo(kgriffs): this decorator does not work on callable </s> def shim(req, resp, resource, kwargs):	_wrap_with_before shim = action else: action(req, resp, kwargs)
# todo: what does constructor of gitconfigparser, in case file doesn't exist? </s> parser = gitconfigparser(gitmodule_path)	get_module_parser from git import GitConfigParser gitmodule_path = opj(repo.path, ".gitmodules") parser.read() return parser
# todo incorporate inter-bin distances </s> cnarr = cnarr.autosomes()	hmm_get_model if method == 'germline': model.means_ = state_means freqs = as_observation_matrix(cnarr) if cnarr.chromosome.nunique() == 1:
# todo: make sure the image is present or pull it </s> base_image = "registry.fedoraproject.org/fedora:28"	test_build_basic_image def test_build_basic_image(): basic_playbook_path = os.path.join(data_dir, "basic_playbook.yaml") target_image = "registry.example.com/ab-test-" + random_word(12) + ":oldest" cmd = ["build", basic_playbook_path, base_image, target_image]
# todo remove it with inspector frontend </s> data['response'] = dict(raw_data['response'])	add_data _data_rule = {'request.url': '(?=.*YOUR-REQUEST-PATH)(?=.*PARAMS)'} if 'response' in data: if 'data' in data['response']: data['response']['data'] = self._flow_data_2_str(data['response']['data'])
pass  # todo finish me </s> def delete(self, save=true):	AddonDropboxUserSettings pass  # TODO Finish me
# todo: no testpath exercises this code... </s> log.debug("starting thread...")	AnimThread self.do_run = do_run def run(self): self.do_run() log.debug("Thread Complete")
# todo: require tests </s> return [self._factor_from_sqrt(module, backproped)]	_bias_for_sqrt def _bias_for_sqrt(self, module, backproped):
# todo: other code-paths: </s> assert status == pass	test_check_077 status, message = list(check(ttFont))[-1]
# todo: make out_key_var an index column </s> return count	_agg_len_impl count += 1
#todo: be sure to test _version==2 here </s> self.asserttrue(true)	test_insert_shadow_document_simple parallel versions collection when the entire document is version controlled.
# todo(qos) add agent extensions exception and catch them here </s> except attributeerror:	_call_on_agent_extensions try: getattr(extension.obj, method_name)(context, data) LOG.exception( _LE("Agent Extension '%(name)s' failed in %(method)s"),
# todo: should change to 'bytes' on python3 </s> 'unsupported key type: str')	test_table_delitem_column_invalid_type self.assertEqual(exception_context.exception.message,
#todo: check the data! e.g. pubdate etc. </s> count = 0	test_unique pipe_def = self._get_pipe_def("pipe_1I75yiUv3BGhgVWjjUnRlg.json") p = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def) for i in p: count += 1
# todo: losing precision on double types </s> return float(value)	unpack_scalar_cli_arg return int(value) elif parameter.type == 'float' or parameter.type == 'double': elif parameter.type == 'blob' and parameter.payload and parameter.streaming: file_path = os.path.expandvars(value)
# todo: test filter functionality; this only tests that the operations work </s> filt = multistagechannelfilter(input_rate=32000000, output_rate=16000, cutoff_freq=3000, transition_width=1200)	test_settings def test_settings(self): filt.set_cutoff_freq(2900) filt.set_transition_width(1000)
#todo(wuzewu): download file in tmp directory </s> self.module_list_file = self.download_file(	get_module_url def get_module_url(self, module_name, version=None): if not self.module_list_file: url="https://paddlehub.bj.bcebos.com/module_file_list.csv") self.module_list_file = csv_reader.read(self.module_list_file)
# todo: remove </s> cmd = "import shutil; print(shutil.which('{}'))".format('python')	setup_py def setup_py(self, *args, **kwargs): with chdir(self.pkg_path): cmd_path2 = self.run("python", "-c", cmd) print("inside prefix", cmd_path2)
# todo again, why this rather than just a dict? </s> body_var_list = body_data.split("&")	parse_request bodyList.append(tmp) else: body_var_List = filter(None, body_var_List) for item in body_var_List:
# todo: replace with generic function to generate random sequence of floats </s> data1 = np.random.ranf(data_length)	_test_series_binary_operations hpat_func = sdc.jit(pyfunc) for data_length in self.total_data_length[name]: data2 = np.random.ranf(data_length) A = pd.Series(data1)
# todo untested </s> _raise_current_error(error)	_handle_bio_errors else: 1/0
#todo(jk0): finish this later </s> return "create! %s" % req	create def create(self, req):
# todo: the following skipped suite and fixtures should be enabled </s> return ['authorization']	_filter_headers def _filter_headers(self):
# @todo: representation? </s> ),	DataCollectionTemplateModel Field("model", "json", requires = IS_EMPTY_OR(IS_JSON()), s3_comments(), *s3_meta_fields())
# todo: use re.error here mayhaps, also: should we log? </s> sys.exit(1)	_setup_versionconfig ) except sre_constants.error as e: return vc
# todo: test. </s> return list.newfromjsondict(data)	CreateListsMember data = self._ParseAndCheckTwitter(resp.content.decode('utf-8'))
# todo: specialist error </s> raise valueerror(uri + " does not belong to this graph")	relative_uri return uri[len(self.__uri__.string):] else:
# todo: check syntax, values? </s> pass	link @GenericHeaderSyntax def link(self, name, values):
# todo: implement it </s> token = user.token_model.query(user.token_model.token == token).get()	PasswordResetCompleteHandler class PasswordResetCompleteHandler(BaseHandler): def get(self, token): if token is None: self.add_message('The token could not be found, please resubmit your email.', 'error')
" # todo: i18n", </s> '-print("hello world!")',	test_process_hunks_patch_called_correctly " ", " ", '+print(tr("Hello world!"))', " ",
# todo: raise the error instead, and make the user do the refresh manually </s> log.warning("refreshing state and resending request")	_do_refresh def _do_refresh(self): self._state = State.from_session(session=self._state._session)
# todo do something with reccomendation </s> return ret_val	process_rabbit_message value = my_obj[1] self.logger.debug('value:{0}'.format(value))
# todo: make this a hard error, instead of a silent overwrite </s> logging.warning("kvm: overriding disk_cache setting '%s' with 'none'"	_GenerateKVMBlockDevicesOptions cache_val = ",cache=none" elif aio_mode == constants.HT_KVM_AIO_NATIVE and disk_cache != "none": " to prevent QEMU failures in version 2.6+", disk_cache)
# todo: remove getattr when https://github.com/rtfd/readthedocs.org/pull/3339 got merged </s> 'image': getattr(self.config, 'build_image', self.version.project.container_image),	save_environment_json }, 'build': { }, }
# todo: this needs auth. </s> def pass_test(agent_id, win_percent):	pass_test request_url = os.getenv('PLAYGROUND_SERVER_URL' + '/fail_test') requests.post(request_url, json={
# todo: arrange </s> repo = self.remote.new_repo(self.token)	test_create_repo def test_create_repo(self): Test: create/edit a repo object self.assertTrue(self.remote.modify_repo(repo, "name", "testrepo0", self.token)) self.assertTrue(self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token))
# todo move me to unicodedata module and autogenerate. </s> def is_default_ignorable(u):	is_Default_Ignorable
# todo(mordred) when this changes to rest, force interface=admin </s> with _utils.shade_exceptions():	list_services :raises: ``OpenStackCloudException`` if something goes wrong during the openstack API call. services = self.manager.submit_task(_tasks.ServiceList()) return _utils.normalize_keystone_services(services)
# todo: kill this </s> inspection = inspection[0]	domain_uses_www if not inspection: return False url = inspection.get("Canonical URL") return (
#@todo: move to utils in 0.4.10 </s> def timestamp():	timestamp return int(time.time() * 1000)
# todo remove with https://github.com/rtfd/readthedocs-build/issues/30 </s> from readthedocs.projects.models import feature	are_submodules_available fail if using private submodules. This will eventually be configureable with our YAML config. if self.project.has_feature(Feature.SKIP_SUBMODULES): return False
# todo - здесь можно поставить порог, ниже которого сигнал не пройдёт </s> outc[idx_c] += signal_b	layerC for idx_c, wb in enumerate(weights_b): signal_b = wb * outB[idx_b] return outC
continue  #ignore if the source doesn't have our (dereferenced) target field (todo: issue a warning if debugging?) </s> yield d	pipe_itembuilder reduce(lambda i,k:i.setdefault(k, {}), [d] + key.split('.')[:-1])[key.split('.')[-1]] = value except AttributeError: if item == True: #i.e. this is being fed forever, i.e. not in a loop, so we just yield our item once break
annot.annotation_metadata.annotator.email = "todo" #todo </s> levels = ["function", "large_scale", "small_scale"]	fill_annotation annot.annotation_metadata.origin = metadata[1] annot.annotation_metadata.annotator.name = metadata[annotation_id + 2] [parse_annotation_level(annot, path, annotation_id, level) \ for level in levels]
# todo progress_meter will be changed to a class </s> if i % 100 == 0:	contact_matrix else: for i in range(len(coord)): progress_meter(i , len(coord)) for j in range(len(coord)):
# todo: also matches //foo/bar.txt and http://host.tld/foo/bar.txt </s> for match_tuple in filter( filter_false_urls, self.relative_url_re.findall(doc_str) ):	_find_relative res = set() filter_false_urls = self._filter_false_urls match_str = match_tuple[0] try:
#todo: handle ipv6 </s> with socket.socket(socket.af_inet, socket.sock_dgram) as sock:	create_nio rport = request["nio"]["rport"] try: sock.connect((rhost, rport)) except OSError as e:
# todo: allow other formats? </s> for size in ['ss', 's', 'm', 'l']:	save_album_art DBSession.add(media) DBSession.flush() file_path = im_path % (media.id, size, 'jpg') im.resize(config.album_art_sizes[size], 1).save(file_path)
# todo: split into a function + context manager </s> with make_tempfile(mkdir=true) as new_home:	setup_package from datalad.utils import make_tempfile from datalad.tests import _TEMP_PATHS_GENERATED os.environ['HOME'] = new_home os.makedirs(new_home)
# todo write summaries </s> self.summary_writer = tf.summary.filewriter('log' + "_%d" % task_index)	__init__ trainable=False) optimizer = config.get('optimizer') self.init_op = tf.global_variables_initializer() if not optimizer:
# todo: interim solution to avoid problems with this step. many commands don't need </s> '__reorient_caret': false	parse_motion '_internal_mode': None, 'user_input': self.user_input, } if self.mode in (MODE_VISUAL, MODE_VISUAL_LINE) or (self.motion and self.action):
# todo: also search in the path </s> current_word = get_last_word(self.input_buffer) if self.input_buffer else ""	_state_ground os.write(context.active_session.master, ("ls -1A --color=never -w %d\r" % context.window_size[1]).encode("ascii")) ls = self.read_all_output().split("\r\n") complete(current_word, ls) elif c == 0x0B:
# todo: this should definitely be fixed to allow auto decompression via that api. </s> fastqgz_path = testdataresolver().get_filename("4.bed.gz")	test_fetch_compressed_with_auto def test_fetch_compressed_with_auto(self): details = self._upload_and_get_details(open(fastqgz_path, "rb"), api="fetch", auto_decompress=True, assert_ok=False) assert details["state"] == "ok"
# todo: this function fails for an empty file. better use self.header and self.chunk_headers </s> self.file.seek(4096+4*(x+z*32))	get_timestamp def get_timestamp(self, x, z): timestamp = unpack(">I",self.file.read(4))[0] return timestamp
raise  # todo </s> else:	deploy_contract cached_contract = self.__contract_cache[contract_name] except KeyError: self.__registrar.enroll(contract_name=contract_name, contract_address=address,
# todo: choose something better </s> block_start_string = "{{%"	process_file_with_jinja def process_file_with_jinja(filepath, product_yaml): block_end_string = "%}}" variable_start_string = "{{{"
# todo extend to nonbinary nodes </s> if i not in self._input_indices and i in self.context.node_indices:	__init__ else: self._dimension_labels.append(None) past_tpm_on = past_tpm_on.sum(i, keepdims=True) / 2 past_tpm_off = past_tpm_off.sum(i, keepdims=True) / 2
ret = plot_func(*args, **merge_kwargs(kwargs_call, fig_ax_kwargs))  # todo conflict?? </s> if ret is none and fig_ax_mode:	_wrapped_plot_fn *args, **merge_kwargs(kwargs_call, fig_ax_kwargs)) else: ret = fig elif is_axesplot_bind:
#todo - use a context manager here once we drop python 2.6 </s> self.assertraises(valueerror, kcluster, data,	test_kcluster [ 1, 1, 1, 1, 1], [ 1, 1, 1, 1, 1]], int) **{"nclusters": nclusters, "mask": mask, "weight": weight, "transpose": 0, "npass": 100,
# todo: implement </s> pass	_parse_conllu def _parse_conllu(result: str):
# todo(rbharath): modify the featurization so that it matches desired shaped. </s> (n_samples, axis_length, _, _, n_channels) = np.shape(x)	train_3D_convolution print "np.shape(X): " + str(np.shape(X)) print "Shuffling X dimensions" X = np.reshape(X, (n_samples, axis_length, n_channels, axis_length, axis_length)) print "np.shape(X): " + str(np.shape(X))
# todo: should use debug </s> print('loading from saved common model: %s' % params.common_model)	_get_connectomes connectomes = [] if params.common_model is not None and os.path.exists(params.common_model): connectome_model = common_model['connectome_model'] connectomes.append(connectome_model)
# todo: mock out sleep or use version-ed digests </s> time.sleep(1)	test_multiple_file_deletes second_session = MockSession(TEST_DIR) second_session.add_entry('argh.md') second_session.process() self.assertTrue(os.path.exists(ARGH_FILE))
# todo test this </s> return self._abbl	aBl aBBl[idx] = aBl[start:stop].sum(0)
return self._diameter  # todo: best value? </s> @property	Soma @property def start_diameter(self): def diameter(self): return self._diameter
return none  # todo better error handling here </s> validation_dict = response.json()	validate_token except Exception, e: log.error(e) if 'error' in validation_dict: assert validation_dict['error'] == 'invalid_token'
# todo: fix test </s> if (sys.version_info.major, sys.version_info.minor) in [(3, 4), (3, 5), (3, 6)]:	test_update_rec_exec_arg assert caplog.records[-1].levelname == 'DEBUG' except IndexError as e: print('caplog records: {}'.format(caplog.records)) for idx, record in enumerate(caplog.records):
logfile = open('logs/exceptions.log', 'a') #todo: make not hardcoded </s> logfile.write('from %s at %s:\n' % (origin.sender, str(datetime.now())))	error print trace except: logfile.write('Message was: <%s> %s\n' % (trigger.nick, trigger.group(0))) logfile.write(trace)
# todo: this is a debug level log </s> logger.info(io.open(config_file, "rt", encoding="utf-8").read())	main if config_file_exists: logger.info("Reading config file {}:".format(config_file)) try: config.read_file(io.open(config_file, "rt", encoding="utf-8"))
except exception:  # todo - which exceptions? </s> pass  # skip unparsable tag	_parse_feature try: feature.qualifiers[feature_element.tag.replace(NS, '')] = feature_element.text self.ParsedSeqRecord.features.append(feature)
# todo: duplicate + replace with psutil.getloadavg()? (available in 5.6.2) </s> load1m, _, _ = os.getloadavg()	set_turbo def set_turbo(): cpuload = p.cpu_percent(interval=1) if load1m > 2:
# todo: logging </s> sys.exit('location for yaml-files can not be a symlink: %s' % yamldir)	gen_yamlfile_locations def gen_yamlfile_locations(self, yamldir): if os.path.islink(yamldir): if not os.path.isdir(yamldir): sys.exit('Location for YAML-files is not a directory: %s' % yamldir)
# todo: this is a jump. </s> vi_cmd_data['motion']['command'] = '_vi_left_brace'	vi_left_brace def vi_left_brace(vi_cmd_data): vi_cmd_data['motion']['args'] = {'mode': vi_cmd_data['mode']} return vi_cmd_data
# todo type checks </s> suggestions.add(result['suggestion'])	process_callback result['engine'] = engine_name if 'suggestion' in result: continue cb_res.append(result)
# todo(vladikr): this code can be removed once the minimum version of </s> if vif['vnic_type'] == network_model.vnic_type_macvtap:	unplug_hw_veb def unplug_hw_veb(self, instance, vif): linux_net.set_vf_interface_vlan(vif['profile']['pci_slot'], mac_addr=vif['address'])
context.nrt.incref(builder, arr_typ, arr)  # todo required? </s> else:	lower_box_df elif dtype == types.List(string_type): arr_obj = box_list(list_string_array_type, arr, c) arr_obj = box_array(arr_typ, arr, c) context.nrt.incref(builder, arr_typ, arr)
# todo make the delay configurable </s> delay = 30	joined_recently @property def joined_recently(self): limit = time.time() - (delay * 60) return self.join_time < limit
# todo debug </s> print logstring	logRule + "remoteId=%d)" % item.element.remoteSensorId) logging.info("[%s]: %s" % (fileName, logString)) elif item.type == "weekday": logString = ("%s weekday " % spaceString
# todo use renorm </s> batch_norm_fn = lambda x: tf.layers.batch_normalization(x, axis=-1, training=is_training, name='batch_norm')	inference_vgg16 with tf.name_scope('vgg_augmented'): if use_batch_norm: else: batch_norm_fn = None
# todo xxx graalvm change </s> raise unittest.skiptest("missing threading support")	ThreadedEchoServer self.daemon = True def __enter__(self): self.start(threading.Event()) self.flag.wait()
# todo: test coverage of this branch </s> logger.exception(	unsubscribe_request instance.send_activation_email(action='unsubscribe') except Exception, e: 'Error %s while submitting email to %s.', e, instance.email)
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
continue # todo: what encoding does gst give us? </s> log.append('  ' + key + ': ' + value)	tag_callback except UnicodeDecodeError: log.append('  ' + key + " [can't decode]: " + `str(value)`) if key == 'bitrate': track.bitrate = int(value) / 1000 elif key == 'comment': track.album = value
super(syncedmodel, self).save(*args, **kwargs) # todo(jamalex): can we get rid of this? </s> if not self.signed_by or self.signed_by == own_device:	SyncedModel namespace = own_device.id and uuid.UUID(own_device.id) or uuid.uuid4() self.id = uuid.uuid5(namespace, str(self.counter)).hex self.sign(device=own_device) super(SyncedModel, self).save(*args, **kwargs)
content=content,  # todo(tsileo): handle markdown </s> tag=tags,	new cc=cc, to=[to if to else config.AS_PUBLIC], source={'mediaType': 'text/markdown', 'content': source}, inReplyTo=reply.id if reply else None
# todo: assert len(args) == len(node.defn.type_vars) </s> return instance(node, args)	named_type_or_none assert isinstance(node, TypeInfo) if args: return Instance(node, [AnyType()] * len(node.defn.type_vars))
# todo this should recurse down the entire deps tree </s> for dep in package.package_dependencies_source.all_depends():	XhrCustomRecipePackages except: pass try: cust_package = CustomImagePackage.objects.get(
# todo: hook this up to something </s> print('+ reward')	plusReward def plusReward():
# todo: how to check it? meybe we can omit this test </s> pass	test_uniformintfill def test_uniformintfill():
# todo: instead of discarding pending jobs, maintain them </s> for njob in node.pending_jobs:	run_job logger.debug(traceback.format_exc()) if node.pending_jobs: self.finish_job(cluster, njob, DispyJob.Cancelled) if cluster.status_callback and dispy_node:
# todo handle join for non public rooms </s> if len(split_args) != 2:	invite def invite(server, buf, args): split_args = args.split(" ", 1) message = ("{prefix}Error with command \"/invite\" (help on " "command: /help invite)").format(
# todo remove the files after loading, make sure that no writer uses theses files anymore </s> else:	load_registered_outputs output_paths = WriterUtility._get_stereo_path_pair(output_path) output_file = np.array([WriterUtility.load_output_file(path) for path in output_paths]) output_file = WriterUtility.load_output_file(output_path) output_data_dict.setdefault(reg_out['key'], []).append(output_file)
''' # todo filter in the database by the columns i will actually use? </s> cursor = bdb.sql_execute(params_sql, (generator_id,))	_all_mus_sigmas SELECT colno, modelno, mu, sigma FROM bayesdb_nig_normal_models WHERE generator_id = :generator_id all_mus = {} all_sigmas = {}
#todo: make this an async task; so client wouldnt wait </s> pulp.server.util.create_repo(repo_path, checksum_type=repo["checksum_type"])	add_package end_add_packages = time.time() log.info("inside of repo.add_packages() adding packages took %s seconds" % (end_add_packages - start_add_packages)) return errors
# todo: this doesn't work because local_rev has already been set </s> elif isinstance(md, filemetadata) and not self.get_local_rev(md.path_lower):	notify_user if isinstance(md, DeletedMetadata): change_type = "removed" change_type = "added" elif isinstance(md, FolderMetadata):
# todo: in 0.6.0 change this to "disabled": false </s> "enabled": true,	test_server_mode "duplicate_cn": True, "engine": "rsax", "fast_io": True, "fragment": 0,
# todo: 逻辑应该有问题, 但不确定 </s> def validusefulproxy(proxy):	validUsefulProxy 检验代理是否可用 :param proxy:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: this should be a separate test </s> self.asserttrue(iterable.closed, true)	test_systemexit_0_is_ignored with self.assertRaises(SystemExit): response = list(response) self.assertEquals(len(self.client.events), 0)
# todo: need to close computations on this node? </s> node.clusters.clear()	run_job (DispyJob.Cancelled, dispy_node, njob.job))) node.pending_jobs = [] self._nodes.pop(node.ip_addr, None) if self._sched_jobs.pop(_job.uid, None) == _job:
# @todo: deployment_setting </s> organisation_dropdown_not_ac = false	S3OrganisationModel args="create", vars=dict(format="popup")) if organisation_dropdown_not_ac: help = T("If you don't see the Organization in the list, you can add a new one by clicking link 'Add Organization'.")
#@todo: remove in 0.4.10 </s> def error(self, reason=none, type="parse"):	error raise Fail("%s error%s | Plugin out of date" % (type.capitalize(), ':' + str(reason) if reason else "")) if self.core.debug:
# todo: this relies on the gnu version of ps (need to fix macos support) </s> defunct = false	process_in_ptree_is_defunct (zombie) or not. :param ppid: The parent PID of the process to verify. try: pids = get_children_pids(ppid)
def aaaaa(self, bbbbb, cccccccccccccc=none):  # todo(who): pylint: disable=unused-argument </s> return 1	testB31847238 unformatted_code = textwrap.dedent("""\ class _(): def xxxxx(self, yyyyy, zzzzzzzzzzzzzz=None):  # A normal comment that runs over the column limit. return 1
# todo: verify behavior </s> self.assert_received(self.debugger, [])	test_active_exception ])
# todo: figure out what's going on </s> ar._run_annex_command('sync', annex_options=['ssh-remote-2'],	test_annex_ssh ok_(not exists(socket_2)) try: expect_stderr=True, log_stdout=False,
# todo(jblespiau): we can simply use buf.xla_shape() when version 0.1.58 is </s> xla_shape = getattr(buf, "xla_shape", buf.shape)()	from_dlpack client = getattr(backend, "client", backend) buf = xla_client._xla.dlpack_managed_tensor_to_buffer(dlpack, client) assert not xla_shape.is_tuple() aval = core.ShapedArray(xla_shape.dimensions(), xla_shape.numpy_dtype())
# todo: when we reset migrations the following need only check </s> if dob.role is not none and dob.role != 'isw_raid_member' \	_update_disk_state if d.fstype == 'isw_raid_member' or d.fstype == 'linux_raid_member': and dob.role != 'linux_raid_member': known_roles = json.loads(dob.role)
# todo fix bug for not identifying unused variables in nested exceptions see issue #4391 </s> try:	function2 1 / 0 except ZeroDivisionError as error: 1 / 0 except ZeroDivisionError as error:
# todo(mattjj): remove this special case, used for debugging on cpu </s> dims = c.getshape(x).dimensions()	split_array def split_array(shape, x): if xb.get_replica_count() == 1: return c.Reshape(x, None, dims[1:]) else:
# todo: make it pass </s> self.assertequals(['"a string"', 'split'], word_finder.get_name_list_at(14))	xxx_test_strings word_finder = WordRangeFinder('"a string".split()')
# todo: replace with a hook?  just like setting lang= can have a hook. </s> if self.readline:	_SetOption assert opt_name in consts.SET_OPTION_NAMES if opt_name == 'vi' or opt_name == 'emacs': self.readline.parse_and_bind("set editing-mode " + opt_name); else:
# todo(is) we can set the min and max depends on the unit, later </s> max_value_input.setminimum(0)	set_widgets max_label = QLabel(tr('Max')) max_value_input = QDoubleSpinBox() max_value_input.setMaximum(999999) if thresholds.get(the_class['key']):
# todo: this should also test python 2, and pass pyversion accordingly. </s> for version in ["2and3", "3", "3.5"]:	test_stubs seen = set()  # type: Set[str] modules = [] for stub_type in ['builtins', 'stdlib', 'third_party']: stubdir = os.path.join('typeshed', stub_type, version)
"""todo: document me.""" </s> self.sigma = sigma	__init__ def __init__(self, sigma, mu, seed=42): self.mu = mu if not (len(mu.shape) == 1):
# todo: interpolated p-values </s> return self.nc_function.predict(x, self.cal_scores, significance)	predict def predict(self, x, significance):
# todo: handle index </s> nodes = []	_run_setitem inst.target = series_var if isinstance(target_typ, SeriesType): data = self._get_series_data(inst.target, nodes) inst.target = data
# todo: get pytest's coverage plugin working, iirc it has issues? </s> runner = "pytest"	test if module is not None: modstr = " tests/test_{}.py".format(module) if coverage: runner = "coverage run --source=paramiko -m pytest"
# todo: replace suite with testcases </s> testcases_folder_path = testcases_folder_path or os.path.join(os.getcwd(), "suite")	load_testcases_folder } testcases_definition_mapping = {} testcases_items_mapping = load_folder_content(testcases_folder_path) for testcase_file_path, testcase_items in testcases_items_mapping.items():
# todo have no idea if is cdecl or stdcall </s> _queryinformationprocess(self, address, params)	hook_NtQueryInformationProcess }) def hook_NtQueryInformationProcess(self, address, params):
# todo(epot): should be moved inside `features.decode_example` </s> if isinstance(decoders, decode.partialdecoding):	_as_dataset shuffle_files, ) -> tf.data.Dataset: features = decoders.extract_features(self.info.features) example_specs = features.get_serialized_info()
# todo: handle other method types? other content types? </s> content = parse_json_object_from_request(request)	new_func content = None if request.method in ["PUT", "POST"]: try: origin = yield authenticator.authenticate_request(request, content)
# todo: implement node label semantics </s> return lambda n, m=none: true	_tgrep_node_label_use_action assert tokens[0].startswith('=') nodel_label = tokens[0]
# todo: handle arrays </s> raise typeerror("unsupported type %s" % ctype)	randomValue setattr(obj, name, randomValue(type_)) return obj
pass # todo </s> elif property.data_type == 4:	set_property_value write_u32(data, offset, new_value)
# todo: change the frontend to pass seconds instead. </s> expires_at = (now_in_seconds + one_hour_in_seconds) * 1000	test_login_no_email return {'sub': 'bad', 'email': test_user.email, 'exp': id_token_expiration_timestamp} monkeypatch.setattr(AuthBackend, '_get_user_info', userinfo_mock) resp = client.get( reverse("auth-login"),
# todo: how do i make the __iter__ thread safe? </s> cursor = self._conn.execute('select information from data')	__iter__ def __iter__(self): for r in cursor: obj = cPickle.loads(r[0])
return 0 # todo </s> def _to_mopidy_id(self, spotify_uri):	_to_mopidy_id
# todo(hirfumi): support for chainer backend </s> elif args.backend == "pytorch":	main if args.backend == "chainer": raise NotImplementedError("Only pytorch are supported.") from espnet.mt.pytorch_backend.mt import train train(args)
# todo check if this always works? </s> col = bpy.data.collections.get('collection')	draw_object_only_with_vertices mesh = bpy.data.meshes.new('mesh') obj = bpy.data.objects.new(name, mesh) col.objects.link(obj) bm = bmesh.new()
# todo: 搜索和分页 </s> keyword = request.get.get('search', '')	perm_role_add header_title, path1, path2 = "系统角色", "角色管理", "查看角色" roles_list = PermRole.objects.all() if keyword: roles_list = roles_list.filter(Q(name=keyword))
# xxx todo [ ] should consider concurrent open files of differing modes </s> return	open self.log('open(%r, %r)' % (path, flags, )) if path in self.files: else: tffobj = TahoeFuseFile(self.tfs, path, flags)
#! todo: this needs to be made more intelligent </s> if (omega == none):	nyquist def nyquist(sys, omega=None): omega = sp.logspace(-2, 2); mag, phase, omega = sys.freqresp(omega)
# todo: why is this separate? </s> self.set_as_msc()	read_op2 self.set_table_type() elif version.startswith('AEROFREQ'): self.set_table_type() elif version == b'XXXXXXXX':
# todo(py3.7): add required=true </s> return arg	arg_conv_range return value
#todo _rule_for_states needs to made into a generator </s> self.node[node]['_rule_for_states'] = _order	add_rule_for_states break
#todo: cast input with np.asanyarray() </s> if len(x.shape) > 1:	make_grad_sort def make_grad_sort(ans, x, axis=-1, kind='quicksort', order=None): raise NotImplementedError( "Gradient of sort not implemented for multi-dimensional arrays.")
# todo: this is a temporal fix </s> hog_descriptor = windowiteratorresult(	hog hog_descriptor = iterator.HOG(algorithm, num_bins, cell_size, block_size, signed_gradient, l2_norm_clip, verbose) np.ascontiguousarray(np.rollaxis(hog_descriptor.pixels, -1)), hog_descriptor.centres)
# todo: add for morph targets data. </s> pending_indices = pending_primitive['indices']	extract_primitives pending_attributes['WEIGHTS_' + str(weight_index)] = weight weight_index += 1 while len(pending_indices) > 0: process_indices = pending_primitive['indices']
# todo remove this assertion and test </s> assert x.shape[1] == 2	odt all simplices containing x_i). import scipy.optimize mesh = MeshTri(X, cells, flat_cell_correction=None) initial_stats = gather_stats(mesh)
#todo: check the data! </s> self.asserttrue(count > 0)	test_fetchsitefeed for i in p: count += 1
# todo new message here </s> routing_key = 'sms.ack.%s' % (self.transport_name)	submit_sm_resp log.msg("Mapping transport_msg_id=%s to sent_sms_id=%s" % ( transport_msg_id, sent_sms_id)) message = Message(**{ 'id': sent_sms_id,
# '%5d' doesn't work yet.  todo: fix this. </s> if name == 'strings':	ShouldSkip 'varargs']: return True return True if name == 'parse':
'''todo: add docs''' </s> widget.on_change(widget_field, partial(self.on_change, model_field=model_field))	bind_to_model def bind_to_model(self, widget, widget_field, model_field):
# todo: generate extraction warning. </s> continue	_ParseRegisteredDLLs handler_value = subkey.GetValueByName(trigger) if not handler_value: event_data = WinlogonEventData() event_data.application = subkey.name
# todo: figure out if this is 1 or 2 positions ahead. </s> if ((pos + 1) < len(strs)):	_read_custom_doc_props r = [] for s in strs: r.append((s, strs[pos + 1])) pos += 1
# todo switch to using the db to determine next duplicate number to use </s> is_indexed = false	is_indexed def is_indexed(self, path, media): local_filename = os.path.join(path, media.filename) file_record = self.db.get_file(local_filename)
# todo: require an api key on the basic auth header </s> task = lease(	handle_lease @app.route('/api/work_queue/<string:queue_name>/lease', methods=['POST']) def handle_lease(queue_name): queue_name, request.form.get('owner', request.remote_addr, type=str),
pass # todo: explain </s> pass # todo: test to see if it's true, alert if not	status412 def status412(self):        # Precondition Failed
# todo: convert to casetransaction object </s> case_action = commcarecaseaction.from_parsed_action(	get_stock_actions )) else: submit_time, user_id, xform, AbstractAction(CASE_ACTION_COMMTRACK) )
# todo: implement this rpc service </s> return empty_pb2.empty()	push_model def push_model(self, request, _):
# todo support multiple backends </s> self.backends[0].stored_playlists.playlists = playlists	playlists @playlists.setter  # noqa def playlists(self, playlists):
# todo: improve the unicode checking </s> try:	__setitem__ % (item, self._index_for.capitalize(), self._index_for)) value = urllib.quote_plus(value) except (KeyError, UnicodeEncodeError, UnicodeError):
# @todo: check to see if they disappeared, etc </s> error = t("registration not found")	site_check_out ).first() if not registration: warning = None return error, warning
# todo: implement </s> pass	result_by_parent_name @staticmethod def result_by_parent_name(parent_name):
# todo: also use backward pass </s> hs = fws	get_selective_model ) fws = [states[1][0][1] for states in rnn_result] h = tf.concat(1, hs) logits_flat = tf.contrib.layers.linear(h, 5*target_size)
# todo: what if we fail?  error-handling should be recorded someplace, </s> yield workitem.delete()	work if workItem.group is not None: yield NamedLock.acquire(txn, workItem.group) yield workItem.doWork() except NoSuchRecord:
# todo(pulkitb): parameterize and add more model/runtime options. </s> self.assertallclose(model1.predict(inputs), model2.predict(inputs))	_assert_models_equal *self._batch(model1.input.get_shape().as_list(), 1))
# todo: check if format matches </s> pass	_large_indexing ) else: if file_format['inputFormat'] == 'table': db = dbms.get(request.user)
# todo: refactor to catch errors smartly in get_screenshot_as_file </s> def _get_page_source_as_file(filename) -> bool:	_take_html_dump if not os.path.exists(folder): os.makedirs(folder) if not filename.lower().endswith('.html'): warnings.warn("name used for saved pagesource does not match file "
# todo dont convert to uint8 </s> if ia.is_integer_array(img_to_cs):	_augment_images from_to_var = ChangeColorspace.CV_VARS[from_to_var_name] img_to_cs = cv2.cvtColor(img_rgb, from_to_var) img_to_cs = np.clip(img_to_cs, 0, 255).astype(np.uint8) else:
# todo(mitmul): when cupy.random.choice becomes available, remove it </s> fg_inds = cuda.to_cpu(fg_inds)	_create_bbox_labels fg_inds = xp.where(labels == 1)[0] if len(fg_inds) > num_fg: disable_inds = np.random.choice( fg_inds, size=(len(fg_inds) - num_fg), replace=False)
# todo consider removing this test entirely, or hardcoding column names </s> fv[prefix+'notes'] = notes	test_edit_all_fields fv[prefix+'version'] = version fv[prefix+'url'] = url fv[prefix+'license_id'] = license_id fv[prefix+'tag_string'] = tags_txt
# todo(dcramer): ideally member:write could approve </s> can_approve_requests_globally = request.access.has_scope('org:write')	get_template_context (om.role == roles.get_top_dog().id and om.user != request.user and om.user is not None) ) can_add_members = request.access.has_scope('org:write') can_remove_members = request.access.has_scope('member:admin')
# todo add test for this </s> n_segments_samples = np.clip(n_segments_samples, 1, none)	_augment_images n_segments_samples = self.n_segments.draw_samples( (nb_images,), random_state=rss[0]) for i, (image, rs) in enumerate(zip(images, rss[1:])): replace_samples = self.p_replace.draw_samples(
# xxx todo after #2156 datasets may not necessarily carry all </s> reporton='datasets',	_mk_schema lgr.info('Scanning for metadata keys') for res in query_aggregated_metadata( ds=self.ds, aps=[dict(path=self.ds.path, type='dataset')],
# todo: direct use of json['error'] is deprecated; use display_message('...', 'error') instead. </s> print dumps({ 'error': error }, sort_keys=true, indent=2)	save_span print 'Content-Type: application/json\n' error = 'unable to change the span of an existing annotation' assert False, error except AttributeError:
pass # todo </s> if '%s' in args:	parse_exec pass # TODO generate tmp page if '%d' in args: if hasattr(page, 'source') and isinstance(page.source, File): args[args.index('%s')] = page.source.path
# todo: implement </s> pass	result_by_parent_name @staticmethod def result_by_parent_name(parent_name):
#todo change back </s> self.local_port = 5002	run self._broad_sock = _set_multicast_socket(self._timeout) self.l_ips = local_ips() if not self.local_port: return
# todo: remove verify ssl config when working without it. </s> def fetch_island_data(zone_key, session):	fetch_island_data if zone_key == 'ES-CN-FVLZ': lanzarote_fuerteventura_data = LanzaroteFuerteventura(session, verify=False).get_all()
# todo(mierdin): note that this will always return true if rbac is not enabled </s> if rbac_utils.user_has_role(requester_user, role):	put LOG.info("Checking user %s is in role %s" % (requester_user, role)) LOG.info(rbac_utils.user_has_role(requester_user, role)) break else:
# todo: make this nicer! </s> self.audio = np.trim_zeros(self.audio, 'fb')	trim Trim leading and trailing zeros of the audio signal permanently.
# todo: may test file contents </s> temp = tempfile.namedtemporaryfile(delete=false)	test_export_to_sqlite_forcing_table_name_appends_rows def test_export_to_sqlite_forcing_table_name_appends_rows(self): self.files_to_delete.append(temp.name) rows.export_to_sqlite(utils.table, temp.name, table_name='rows')
tag = tagger(dt, point) # todo take accuracy into account?? </s> yield location(	load_locations continue alt = j.get("altitude", None) dt=dt, lat=lat,
pass # todo </s> def handle_request(self, input):	handle_request
# @todo: is this always true? </s> sg.attrs["staggering"] = 0	write_to_gdf sg.attrs["field_units"] = "None" sg.attrs["field_to_cgs"] = 1.0 g = f.create_group("particle_types") sg = g.create_group(particle_type_name)
# test for uwsgi -- todo save this somewhere so we only have to do it once. </s> is_uwsgi = false	app_factory if kwargs.get( 'middleware', True ): webapp = wrap_in_middleware( webapp, global_conf, **kwargs ) try: import uwsgi
else: # todo: deprecated </s> exportmesh = bobject.to_mesh(self.depsgraph, apply_modifiers, calc_undeformed=false)	export_mesh bobject_eval = bobject.evaluated_get(self.depsgraph) if apply_modifiers else bobject exportMesh = bobject_eval.to_mesh() if exportMesh == None: log.warn(oid + ' was not exported')
# todo check if solve successful </s> disj_lb = value(var)	compute_disjunctive_bounds bigM_model._var_bounding_obj = Objective(expr=var, sense=minimize) SolverFactory('gurobi').solve(bigM_model) bigM_model._var_bounding_obj.sense = maximize SolverFactory('gurobi').solve(bigM_model)
# todo: another solution should be used here. this is a hack for compatibility reasons. to resolve the gadget address calculation of segments of elf files have a different base address if calculated segment.virtualaddress - segment.offset </s> offset = section.offset - (binary.imagebase - (section.virtualaddress - section.offset))	_searchGadgetsSingle toReturn = [] code = bytes(bytearray(section.bytes)) arch = binary.arch max_progress = len(code) * len(arch.endings[gtype])
# todo(jaypipes): log the error? </s> return none	get_image_metadata return data else: finally: c.close()
# todo: look at word_spid </s> source_str = '[ trap ]'	_PrintWithLocation source_str = '[ eval at line %d of %s ]' % (line_num, outer_source) elif isinstance(src, source__Trap): else: source_str = repr(src)
# todo: move this hard-coded mixin/manager injections to maybe a model </s> if self.dataset.slug == "socios-brasil" and self.name == "empresa":	get_dynamic_model_managers def get_dynamic_model_managers(self): managers = {"objects": DatasetTableModelQuerySet.as_manager()} from core import data_models managers["objects"] = data_models.SociosBrasilEmpresaQuerySet.as_manager()
# todo: move to base class </s> if nodesrect is none:	frameRect def frameRect(self, nodesRect): return windowRect = self.mapToScene(self.rect()).boundingRect()
g.configure_new(config) # todo: test for emitted warning </s> print(g)	test_partially_connected_hidden_nodirect_old self.assertEqual(gid, g.key) print("\nThis should output a warning:", file=sys.stderr) self.assertEqual(set(iterkeys(g.nodes)), {0, 1, 2}) self.assertLess(len(g.connections), 6)
# todo: for the domain-allocation switch, this needs to be turned </s> return self._data	data_domain .. note:: Alias to ``self.data``.
# todo(developer): uncomment and set to a path to your audio file. </s> with open(speech_file, 'rb') as audio_file:	transcribe_file_with_multichannel from google.cloud import speech_v1p1beta1 as speech client = speech.SpeechClient() content = audio_file.read() audio = speech.types.RecognitionAudio(content=content)
# todo: check arp reply is valid </s> self.asserttrue(self.packet_outs_from_flows(arp_replies))	arp_for_controller 'arp_source_ip': '10.0.0.1', 'arp_target_ip': '10.0.0.254'})
# todo: remove dependency on legacy_examples </s> cli_args_to_test = [	test_build_dags and that Airflow is able to successfully parse our DAGs. runner = CliRunner() ['--module-name', 'dagster_examples.toys.log_spew', '--pipeline-name', 'log_spew'], ['--module-name', 'dagster_examples.toys.many_events', '--pipeline-name', 'many_events'],
# todo: sphinx stores created and updated as seconds since the </s> d['created'] = int(time.mktime(self.created.timetuple()))	extract_document d['is_locked'] = self.is_locked d['has_answers'] = bool(self.num_answers) d['updated'] = int(time.mktime(self.updated.timetuple())) d['question_creator'] = self.creator.username
# todo: cache this result so multiple failing calls don't keep hitting the db </s> return none	sql_product self._sql_product = SQLProduct.objects.get(domain=self.domain, product_id=self.entry_id) except ObjectDoesNotExist: return self._sql_product
#todo tuplet: add variables for if it's the start of a tuplet </s> self.istuplet = false	VexflowRest self.vexflowKey = position self.params = params self.tupletLength = 0 self.vexflowCode = ''
# todo: support default period argument </s> shift_const = rhs.args[0]	_run_call_series return self._replace_func(func, [series_var, val]) if func_name in ('shift', 'pct_change'): func = series_replace_funcs[func_name] return self._replace_func(func, [series_var, shift_const])
# todo: documentation pending </s> parameters	load_and_assign_npz def load_and_assign_npz(assign_list=None, name='model.npz', sess=None): ------------- Returns
# todo this has been messed around with. confirm that </s> key = bucket.new_key(f.path[len(intent.source_path):])	perform_upload_s3_key_recursively if os.path.basename(f.path) in intent.files: with f.open() as source_file: key.set_contents_from_file(source_file) key.make_public()
raise exceptions.mpdnotimplemented  # todo </s> underscore, dash, dot and colon.	subscribe already. The name may consist of alphanumeric ASCII characters plus
# todo in python 2.7 or later, this should be a set literal. </s> only |= set(['type', 'id'])	__init__ if only is not None: only = set(get_column_name(column) for column in only) if exclude is not None: exclude = set(get_column_name(column) for column in exclude)
# todo: raise an alert please </s> self.image_cache[tag] = true	DockerImagesService if image_details: if digest != image_details['id']: else: self.IMAGE_CACHE[tag] = False
await self._stream.reset()  # todo: specify error code </s> except streamclosederror:	Stream raise ProtocolError('Stream was already cancelled') try: pass self._cancel_done = True
assert r.status == 200  # todo: other codes </s> data = json_loads(r.data.decode("utf-8"), object_hook=hydrator.json_to_packstream)	run hydrator = JSONHydrator(version="rest", graph=graph, keys=keys, entities=entities) r = self._post("/db/data/transaction/%s" % (tx or "commit"), statement, hydrator.dehydrate(parameters)) if data.get("errors"): if tx is not None:
# todo(iftenney): compare performance of these implementations. </s> return _mat_from_blocks_sparse(mb, n_chars_src, n_chars_tgt)	_mat_from_blocks range. To reduce how often this occurs, perform pre-processing so that source and target share as many characters as possible.
# todo: for backward compatibility only, remove if not used anymore </s> def get_project_id(self):	get_project_id return get_project(key='id')
# todo if update_variable_bounds = false, this will not work as intended. </s> if value(v.lb) > var_lbs[v] + tol:	fbbt_block for v in _new_var_bounds.keys(): if v.lb is not None: improved_vars.add(v) var_lbs[v] = value(v.lb)
return cursor_offset, line #todo not implemented </s> def transpose_word_before_cursor(cursor_offset, line):	transpose_word_before_cursor @on('\x1bt')
pass # todo </s> def try_undo(self, *args):	try_undo
#todo(ziad): use a more sophisticated proxy </s> return response(status=resp.status, body=data)(env, start_response)	__call__ resp = conn.getresponse() data = resp.read()
# todo: bug, until startup_nodes is refactored "master" must be default because some nodes do not have "server_type" </s> for node in self.startup_nodes:	send_to_all_masters_merge_list def inner(self, *args, **kwargs): res = set([]) if node.get("server_type", "master") != "master": continue
# todo: replace with specific error when exceptions are refactored </s> raise xlwings.xlwingserror("getting or setting 'app.interactive' isn't supported on macos.")	App @property def interactive(self): @interactive.setter def interactive(self, value):
# todo: remove this once dask.array automatically aligns chunks </s> data = data.rechunk(self.data.chunks)	_shift_one_dim data = ops.concatenate(arrays, axis) if isinstance(data, dask_array_type): return type(self)(self.dims, data, self._attrs, fastpath=True)
# todo(ytknzw): add more specific assertion with the test case. </s> study = create_study()	test_plot_intermediate_values trial.report(2.0, step=1) return 0.0 study.optimize(lambda t: objective(t, True), n_trials=1) figure = plot_intermediate_values(study)
#@todo: remove in 0.4.10 </s> def error(self, reason=none, type="parse"):	error raise Fail("%s error%s | Plugin out of date" % (type.capitalize(), ':' + str(reason) if reason else "")) if self.core.debug:
# todo: handle rtl </s> new_available_width -= float_width['right']	get_next_linebox new_position_x, _, new_available_width = avoid_collisions( context, linebox, containing_block, outer=False) alignment_available_width = ( new_available_width + new_position_x - linebox.position_x)
msg="your scan id is not valid!"  # todo: add to message() </s> )	get_results_csv structure( status="error", ), 400 scan_details = session.query(Report).filter(Report.id == result_id).first()
# todo use trads with %s </s> cls.echo(msg['reason'])	call if isinstance(err, DryRunException): for msg in err.dry_run: cls.echo('\t' + ' '.join(msg['attr'])) sys.exit(1)
# todo(mattrobenolt): deal with conflict resolution if </s> from sentry.models import event	get_oldest_event def get_oldest_event(self): if not hasattr(self, '_oldest_event'): try:
# rbarlow_todo: convert this call into a celery task </s> delete_request = callrequest(	repo_delete_itinerary action_tag('delete') ] manager.delete_repo, [repo_id],
# todo: make test method </s> ctypes          # r17-22, this cause segfault error.	test_ctypes def test_ctypes(): import ctypes return True
# todo: kill this debug print. </s> print msgs	test_js_sandboxer_with_delayed_requests failures = [log['failure'].value for log in lc.errors] msgs = lc.messages() self.assertEqual(failures, []) self.assertEqual(status, 0)
#todo: milestones = get_milestones(dst_url) </s> milestones = []	import_some_issues def import_some_issues(issue_ids): labels = get_labels(dst_url) issues = []
# todo(b/116308354): frequency_threshold is misleading since this threshold can </s> def _get_top_k_and_frequency_threshold(top_k, frequency_threshold):	_get_top_k_and_frequency_threshold if top_k is not None: top_k = int(top_k)
if lang is none:  # todo: remove in v8 </s> utils.logger.warn("rendertags.slugify_category_name() called without language!")	slugify_category_name def slugify_category_name(self, name, lang): lang = '' path = self.site.parse_category_name(name)
# todo test needed to enter here. </s> error_msg = "incorrect region supplied ('{region}'). must be one of the following: {marketplaces}".format(	__init__ self.domain = MARKETPLACES[region] else: marketplaces=', '.join(MARKETPLACES.keys()), region=region,
# todo: reproduce and submit traceback to issue 41 </s> hosts = host.filter_by_fqdn(hostname)	_update def _update(hostname, ipaddr): ipaddr = str(ipaddr)  # bug in dnspython: crashes if ipaddr is unicode, wants a str! num_hosts = len(hosts) if num_hosts == 0:
def normalised(self):    # todo: mark as deprecated </s> return type(self)(self._module.normalize(self))	normalised @property
new_todo = selenium_browser.find_element_by_css_selector("#new-todo") </s> for task_text in map(str, range(10)):	create_tasks_with_raw_selenium def create_tasks_with_raw_selenium(): global selenium_browser new_todo.send_keys(task_text + Keys.ENTER)
# todo(leofang): test float16 ('e') once cupy/cupy#5346 is resolved </s> @testing.for_dtypes('iilqfd')	test_shfl @unittest.skipIf(runtime.is_hip, 'HIP is not yet supported') def test_shfl(self, dtype): @jit.rawkernel()
# todo: replace with sys-ctrl command </s> minerd_cmd = ["sudo", "minerd", "-u", config.username,	start_minerd else: subprocess.call(["sudo", "minerd", "--stop"]) app_config.TWO1_POOL_URL] try:
# todo: this is seriously intensive and takes a _long_ time to run. </s> def reindex_documents():	reindex_documents from wiki.models import Document from django.conf import settings
# todo: remove after migration to osf storage </s> if settings.copy_git_repos and os.path.exists(folder_old):	fork_node if message: status.push_status_message(message) folder_new = os.path.join(settings.UPLOADS_PATH, forked._primary_key) Repo(folder_old).clone(folder_new)
# todo(toshihikoyanase): remove catch_warnings after gridsampler becomes non-experimental. </s> with warnings.catch_warnings():	tune_feature_fraction param_name = "feature_fraction" param_values = np.linspace(0.4, 1.0, n_trials).tolist() warnings.simplefilter("ignore", category=optuna.exceptions.ExperimentalWarning) sampler = optuna.samplers.GridSampler({param_name: param_values})
# todo: try iso format first? </s> format = get_date_format(locale=locale).pattern.lower()	parse_date :return: the parsed date :rtype: `date` year_idx = format.index('y') month_idx = format.index('m')
# todo scope = 'scope of the search' </s> client = asset_v1.assetserviceclient()	search_all_iam_policies def search_all_iam_policies(scope, query=None, page_size=None): from google.cloud import asset_v1 response = client.search_all_iam_policies( scope, query=query, page_size=page_size)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_tips_null def test_fail_tips_null(self):
# todo: add axis parameter </s> return dataframe(self._internal.copy(sdf=sdf))	shift sdf = sdf.fillna(fill_value)
# todo data alignment stuff </s> if 'byteoffset' in pysparse.json['values'].keys():	read fmt = '<' + (fmt_char * component_nb) stride = struct.calcsize(fmt) offset = pysparse.json['values']['byteOffset'] else:
if compute_stats:  # todo(b/119906277): remove </s> self._assertnumsamples(builder_reloaded)	test_download_and_prepare_as_dataset builder_reloaded = self.DATASET_CLASS(  # pylint: disable=not-callable data_dir=self.builder._data_dir_root)  # pylint: disable=protected-access with self._subTest("as_dataset"): self._assertAsDataset(builder_reloaded)
# todo: preprocessing=[resample(sample_period)]) </s> sections = master.good_sections()	diff_between_two_meters sample_period = master.sample_period() period_alias = '{:d}S'.format(sample_period) master_generator = master.power_series(periods=sections) for master_chunk in master_generator:
# todo: needs input cleansing and validation </s> try:	create_policy arrangements with Ursulas. This is an unfinished API endpoint. You are probably looking for grant. request_data = json.loads(request.data) bob_pubkey = bytes.fromhex(request_data['bob_encrypting_key'])
# todo(phawkins): remove this after a jaxlib release. </s> if not hasattr(lapack, "jax_getrf"):	testLu @jtu.skip_on_devices("gpu", "tpu") def testLu(self, shape, dtype, rng): self.skipTest("No LU implementation available") args_maker = lambda: [rng(shape, dtype)]
# todo: the import name wouldn't have to be an object really, could do with a </s> emit(	generateImportlibImportCallCode ) emitLineNumberUpdateCode(expression, emit, context) { PyObject *hard_module = IMPORT_HARD_IMPORTLIB();
# todo: warn/error: check if this var has units: assigning </s> pass	_process_bound % (type(val), bound_type)) if type(val) in native_numeric_types or val is None: else: if self.parent_component()._units is not None:
# todo: drop non-callable keys in dramatiq v2. </s> key_list = keys() if callable(keys) else keys	incr_and_sum if value > maximum: return False mapping = client.get_multi(key_list) total = amount + sum(mapping.values())
# todo: check that the performance measure is within some range </s> figure_eight_baseline(num_runs=1, sumo_binary="sumo")	test_figure_eight Tests flow/benchmark/baselines/figureeight{0,1,2}.py
#todo: kvick we should rename 'short_circuit' to something like 'disable_service_start' </s> if config.get('short_circuit', true):	_teardown_chroot config = self._config.plugins[self.full_name] log.debug('Tearing down chroot at {0}'.format(self._mountpoint)) if not self._enable_service_startup(): log.critical('Failure during re-enabling service startup')
# todo: why does resourcefilecache return none in some cases? </s> name = self.datafile.replace_variables(name)	get_keywords kws.extend(LIBRARYCACHE.get_library_keywords(name, args)) for name in self.get_resource_imports(): res= RESOURCEFILECACHE.get_resource_file(self.datafile.source, name)
#   todo:   2012-11-07 14:05:42 by brian mcfee <brm2132@columbia.edu> </s> result = scipy.signal.fftconvolve(x, x[::-1], mode='full')	autocorrelate def autocorrelate(x, max_size): result = result[len(result)/2:] if max_size is None:
raise fail(encode(e))  #@todo: remove `encode` in 0.4.10 </s> else:	process self.restart(_("Revert to original hoster plugin"))
# todo: most of this is copied from resolve flats </s> if nodata_in is none:	raise_nondraining_flats nodata_out=np.nan, inplace=True, apply_mask=False, ignore_metadata=False, **kwargs): if isinstance(data, str): try:
def normalise(self):    # todo: mark as deprecated </s> return type(self)(self._module.normalize(self))	normalized def normalized(self):
#todo context.active_object = obj </s> subbox = getbbox.fromobj(obj).togeo(geoscn)	execute obj = scn.objects[int(self.objectsLst)] obj.select_set(True) if rprj: subBox = rprjToRaster.bbox(subBox)
#todo: hardcoded to sigma as the model </s> if getattr(self, '_mesigma', none) is none:	MeSigma @property def MeSigma(self): sigma = self.currentTransformedModel self._MeSigma = self.mesh.getEdgeInnerProduct(sigma)
# todo: make test method </s> beautifulsoup	test_bs except: return False return True
browser.set_driver(webdriver.chrome(chromedrivermanager().install()))  # todo: was firefox here... should it be here? </s> def setup_module(m):	setup_module
args.learner_gpus = (gpu_id,)  # todo </s> args.eval_gpu_id = (gpu_id,)	demo_discrete_action_off_policy args.repeat_times = 1 args.target_step = args.env.max_step train_and_evaluate_mp(args)  # multiple process
# todo check if it exists </s> cmd = ('openssl req -new -newkey rsa:2048 -nodes -out %(csr)s '	create_csr else: private_key = common_name + '.key' '-keyout %(key)s -subj %(subj)s') if private_key.endswith('.crt') or private_key.endswith('.key'):
# todo(rbharath): modify the featurization so that it matches desired shaped. </s> x = np.reshape(x, (n_samples, axis_length, n_channels, axis_length, axis_length))	train_3D_convolution print "Shuffling X dimensions" (n_samples, axis_length, _, _, n_channels) = np.shape(X) print "np.shape(X): " + str(np.shape(X)) nb_classes = 2
# todo: send the user an email </s> incr_if_enabled('free_user_reply_attempt', 1)	_handle_reply if stripped_from_address is stripped_reply_record_address: if not reply_record.owner_has_premium: return HttpResponse( "Rely replies require a premium account", status=403
# todo: check syntax </s> return values	allow @GenericHeaderSyntax def allow(self, name, values):
# todo: replace with stream-changed </s> self._trigger_track_playback_started()	on_end_of_track self.core.tracklist.mark_playing(next_tl_track) self.core.history.add(next_tl_track.track) else: self.core.tracklist.mark_unplayable(next_tl_track)
# todo(cp16net): need to set the return code correctly </s> return wsgi.result(202)	delete models.Instance.delete(context=context, uuid=id) LOG.info("result of delete %s" % result)
# todo: resizing on incoming config to make batching more efficient, predict </s> classifier = caffe.classifier(config,	classify assert(os.path.isfile(config) and os.path.isfile(weights)) channel_swap = [2, 1, 0] weights, raw_scale=255,
# todo: make pull request to get this custom vgg feature accepted </s> with slim.arg_scope(vgg.vgg_arg_scope()):	FCN_32s number_of_classes) upsample_filter_tensor = tf.constant(upsample_filter_np) logits, end_points = vgg.vgg_16(processed_images, num_classes=2,
# todo results from ml </s> return str(endpoint.metadata)	_get_device_type @staticmethod def _get_device_type(endpoint):
# todo: error handling </s> backend = plugin.get(backend, backend)()	GraphFactory def GraphFactory(backend='default', identifier=None): if not isinstance(backend, Backend): if backend.context_aware: return ConjunctiveGraph(backend)
def downloadlink(self, link, disposition=false):  #@todo: set `disposition=true` in 0.4.10 </s> if link and isinstance(link, basestring):	downloadLink self.correctCaptcha() if not urlparse(link).scheme:
tol = 0.15  # todo(skye): can we be more precise? </s> jtu.check_grads(np_fn, args_maker(), order=1, atol=tol, rtol=tol)	testFftfreq self._CompileAndCheck(np_fn, args_maker, check_dtypes=True) if dtype in inexact_dtypes: jtu.check_grads(np_fn, args_maker(), order=2, atol=tol, rtol=tol)
# todo use seed_max </s> seeds = random_state.randint(0, 10**6, (nb_images,))	_augment_images result = images nb_images = len(images) if hooks is None or hooks.is_propagating(images, augmenter=self, parents=parents, default=True): if self.first is None:
# todo: found a way to force parsing crash or simulate it. </s> "</blockquote>", tr)	test_emarkdown "<p>test</p>\n"
# todo: restrict to 'value' type nodes </s> node.outputs["value"].default_value = 1	bake_pass for node in obj.active_material.node_tree.nodes: if node.label == "bake_" + bake_name: for obj in objects: for slot in obj.material_slots:
# todo: test that package didn't install, anything else necessary? </s> result = script.pip('install', package, '--no-index', expect_error=true)	test_install_from_broken_wheel package = data.packages.join("brokenwheel-1.0-py2.py3-none-any.whl")
## todo: # fixme: remove me </s> try:	analyse result_path = is_sql_injection(resource_path) if query_string is not None: query_string = query_string.decode() except:
# todo: add test for failures due to missing children </s> assert result["rules"][0]["abortincompletemultipartupload"]["daysafterinitiation"] == 30	test_lifecycle_with_aimu assert len(result["Rules"]) == 1
# todo this eventually needs to be upgraded to support ipv6 </s> targetaddr = ipaddress.ipv4address(target)	isAcceptableTarget def isAcceptableTarget(self, target): try: except ipaddress.AddressValueError: return False
# todo: don't write them? is *much* slower on re-load (~3x) </s> try:	update_lib fh.flush() os.fsync(fh.fileno())  # flush to disk os.unlink(self.lib_path + 'c') except OSError:
media=none  # todo select from the database </s> post_author=message[6],	get_lowest_message fwd_from=None,  # TODO Select from the database
# todo: exceptions </s> self.midi.openoutput( self.idout )	Open if self.idOut is None or self.idIn is None: return False self.midi.OpenInput( self.idIn ) return True
# todo(guillermooo): we cannot access the ouput panel used by exec. </s> self.execute(cmd.format(name, self.get_target_path(view)),	DartGeneratePolymerElementCommand project = DartProject.from_path(view.file_name()) cmd = "pub run polymer:new_element {} -o \"{}\"" project.pubspec.parent)
# todo use shlex.quote in python 3.3. </s> "execution failed with %d/%d. tried to execute:\n%s\n" %	sh if not ignore_failure and ret != 0: raise FrameworkException( (ret & 0xff, ret >> 8, ' '.join(cmdline)))
# todo(pl): https://github.com/pantsbuild/pants/issues/206 </s> resource_products_on_target = resources_by_target.get(target)	_jar target_classes = classes_by_target.get(target) target_resources = [] if resource_products_on_target: target_resources.append(resource_products_on_target)
# todo: support out argument </s> raise valueerror('out option is not supported.')	argmax :return: Array of indices into the array. if out is not None: return Value._ns.argmax(self, axis)
# todo not sure if this is necesasry anymore? </s> saves = list(saved())	test_saves def test_saves() -> None: make_dict(saves, key=lambda s: s.sid)
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
# todo - verify contents </s> self.client.logout()	testDashboard1 response = self.client.get('/dashboard/', {'view': 'incoming'}) self.assertEqual(response.status_code, 200)
# todo(kevinbenton): remove after bug/1668958 is resolved </s> log.debug("instance %s metadata: %s",	get_ec2_metadata if self.userdata_raw is not None: data['user-data'] = self.userdata_raw self.instance.ec2_ids.instance_id, data, instance=self.instance)
# todo fix expected variance </s> self.assertfloatequal(obs, [[(15, 5, 0.674199862463),	test_call_extrapolate obs = self.estimator1(point_count=4, estimate_type='extrapolation', endpoint=30) (20, 5.2555272427983537, float('inf')), (25, 5.38046614197245, float('inf')),
# todo: remove in hyperspy 1.0 </s> return splitted	split spectrum.metadata.General.title = se.metadata.General.title
# todo: infer kernel arguments </s> [callkernel(kernel_name=new_kernel_name)] +	inner_mapper new_kernel_name = kernel_name_gen() new_schedule.extend( current_chunk + [ReturnFromKernel(kernel_name=new_kernel_name)])
# todo: get file name from user. </s> try:	_on_preview def _on_preview(self, menu_item): self.controller.export_project_file(filename=None, openafter=True, readonly=True) except Exception, exc:
# todo(haoyuzhang): set size to 128 per gpu when multi-gpu xla oom is fixed </s> flags.batch_size = 64 * 8  # 8 gpus	benchmark_xla_8_gpu FLAGS.distribution_strategy = 'default' FLAGS.model_dir = self._get_model_dir('benchmark_xla_8_gpu') self._run_and_report_benchmark()
# todo: exit codes (not only for this, but for other exceptions) </s> if action == "run":	alice emitter.echo(str(e), color='red', bold=True) click.get_current_context().exit(1) try:
# todo: replace this with a common get_cluster_info_ec2() method. </s> connection = boto.ec2.connect_to_region(region_name=region)	start_ec2 def start_ec2(cluster_name, region): cluster_instances = connection.get_only_instances( filters={
# todo: stderr=_stderr_file, </s> shell=false,	start self.subproc = subprocess.Popen( cmd, ) else:  # py3
# todo(hrybacki): move to framework.utils.rapply once @sam's pr#4027 is merged. </s> from api.base.serializers import _rapply	create_draft_registration raise HTTPError(http.BAD_REQUEST) schema_version = data.get('schema_version', 1) schema_data = _rapply(data.get('schema_data', {}), sanitize.strip_html) meta_schema = get_schema_or_fail(
# todo: preallocate! </s> i, j, v = np.empty(0), np.empty(0), np.empty(0)	faceDiv if getattr(self, '_faceDiv', None) is None: self.number() for cell in self.sortedCells: i, j, v = cell.faceIndex
# todo: document those attributes </s> self.particle_info: dict[str, bool] = {}	__init__ self.sample_bump = False self.sample_bump_res = '' self.out_basecol: vec3str = 'vec3(0.8)' self.out_roughness: floatstr = '0.0'
# todo: get extra kwargs to serializer (by parsing output_format key)? </s> if outfile:	parse_and_serialize use_format = guess_format(fpath) or DEFAULT_INPUT_FORMAT graph.parse(fpath, format=use_format) graph.serialize(destination=outfile, format=output_format, base=None) store.rollback()
# todo: test this function </s> content_type = contenttype.objects.get_for_model(instance)	delete_callback def delete_callback(sender, instance,  **kwargs): try: MetaData.objects.get(content_type=content_type, object_id=instance.pk).delete()
if (counter >= self.dict_analysis[last]['min_tokens']-1 and counter <= self.dict_analysis[last]['max_tokens']+1): # todo: x-check </s> match_results.append(last)	get_match else: if (last != ''): elif (self.debug): print ('length check failed for :'+last+' from results. ' + str(self.dict_analysis[last]['min_tokens']-1) + ' ' + str(counter) + ' ' + str(self.dict_analysis[last]['max_tokens']+1))
data_source_name='ledger',  # todo: this isn't really needed. </s> domain=stock_state.get_domain(),	change_meta_from_stock_state document_id=stock_state.pk, data_source_type=data_sources.LEDGER_OLD, is_deletion=False,
# todo: may test file contents </s> temp = tempfile.namedtemporaryfile(delete=false)	test_export_to_sqlite_create_unique_table_name def test_export_to_sqlite_create_unique_table_name(self): self.files_to_delete.append(temp.name) first_table = utils.table
# todo: add --quiet </s> input_encoding = input_encoding or default_input_encoding	csv_merge @click.argument("destination") def csv_merge(input_encoding, output_encoding, sources, destination): sample_size = 1024 * 1024 dialects, keys, keys_per_file = {}, [], defaultdict(dict)
# todo: raise forbidden exceptions after adding protection check in the ui </s> raise uservalueerror(_("your action requires modification of event boundaries, but you are "	track_time_changes if isinstance(obj, Event): if not obj.can_manage(user): "not authorized to manage the event.")) elif not obj.object.can_manage(user):
#todo: geli detach -l </s> self._reload_disk()	zfs_volume_attach_group z_vdev += " ".join([''] + vdevs) self.__system("zpool add -f %s %s" % (z_name, z_vdev))
# todo: this linebox should use the 'main' color. </s> self.top_w = urwid.linebox(listbox)	Tooltip self.bottom_w = bottom_w self.listbox = listbox def selectable(self): return self.bottom_w.selectable()
# todo: wait for an event instead of spinning. </s> while not (self._prev_whdr.dwflags & whdr_done):	feed raise RuntimeError("Error writing wave data: code %d" % res) if self._prev_whdr: time.sleep(0.005) res = winmm.waveOutUnprepareHeader(self._waveout, LPWAVEHDR(self._prev_whdr), sizeof(WAVEHDR))
# todo: change this so that it unselects by pressing left and then right rather than pasting over the top </s> if application == "texstudio":	select_phrase deal_with_phrase_not_found(selected_text, application, direction) return text_manipulation_paste(selected_text, application) # yes, this is kind of redundant but it gets the proper pause time multiline_movement_correction = selected_text[right_index :].count("\r\n")
# todo: handle multiple skip stacks </s> (skip, skip_stack), = skip_stack.items()	block_container_layout first_letter_style = getattr(box, 'first_letter_style', None) else: first_letter_style = None for index, child in enumerate(box.children[skip:], start=(skip or 0)):
# todo - are there any other checks that need to be performed at this stage? </s> self.belongs_to = otheritem	installIntoStockItem if self.belongs_to is not None: return False self.save() self.addTransactionNote(
# wait for upload processing to finish (todo: this should be done in each test case instead) </s> self.wait()	upload_file except: raise AssertionError( "Invalid hid (%s) created when uploading file %s" % ( hid, filename ) )
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
pass  # todo </s> def create(self, name):	create
# todo(ochuprykov): remove this method when bug #1485619 will be fixed </s> return secret.status	_resolve_attribute if name == self.STATUS:
# todo: preallocate! </s> i, j, v = [], [], []	edgeCurl if getattr(self, '_edgeCurl', None) is None: self.number() for face in self.faces: for edge in face.edges:
# todo: print should really go to logger, this print goes </s> print(msg, file=sys.stderr)	train_step + '\n Skipping batch' ) if raise_oom: raise ValueError(msg)
# todo: get rid of load_module in favor of </s> mod = spec.loader.load_module()	_load_module if spec is None: raise ImportError() sys.modules[mod_namespace] = mod else:
# todo: remove this skip after fixing </s> if sys.version[0] == 3:	test_circle_draw @requires_application() def test_circle_draw(): raise SkipTest with TestingCanvas() as c:
#todo parse via xml output </s> return cmd("system_profiler spdisplaysdatatype | "	get_gpu_names return "GeForce GTX 770" #TODO elif sys.platform == 'darwin': "grep 'Chipset Model: NVIDIA' | " "sed 's/.*Chipset Model: NVIDIA *//;s/ *$//'")
# todo xxx here, force update success </s> e.append(exprassign(arg1, exprint(0, arg1.size)))	stlxr e = [] e.append(ExprAssign(ExprMem(ptr, arg2.size), arg2)) return e, []
# todo: emit for hidden key/values? </s> self._emitdatachanged()	__setitem__ elif self._dict[key] != value: self._dict[key] = value
# todo: if py3k, override unpickler.find_class(). </s> pass	_load mypickle.find_global = None except AttributeError: d = mypickle.load() f.close()
# todo: which encoding? </s> if py2:	parse_text def parse_text(data): convert = unichr else:
'location_type': loc.location_type.name,  # todo: remove when types aren't optional </s> 'uuid': loc.location_id,	loc_to_json return { 'name': loc.name, 'is_archived': loc.is_archived,
#todo - raise notimplementederror and handle via subclass? </s> return self._case_less()	Alphabet return self else : def _lower(self) : if not self.letters or self.letters==self.letters.lower():
# todo: remove this later </s> if (self.req.project_name == 'setuptools'	check_if_exists return False try: and self.conflicts_with and self.conflicts_with.project_name == 'distribute'):
# todo: this logic does not prevent duplicate test cases, need to address this in the future. </s> if self.proto == socket.sock_dgram:	transmit if not data: data = node.render() MAX_UDP = 65507 if os.name != "nt" and os.uname()[0] == "Darwin":
# todo(paul): work out why because i really don't think it should </s> room_ids = set(r.room_id for r in rooms)	search user.to_string(), ) res = yield self.store.search_msgs(room_ids, constraints) time_now = self.clock.time_msec()
# todo: remove in v.0.6 </s> x = np.array([[0, 0], [0, 1], [2, 0], [2, 1]])	TestLSML self.assertLess(csep, 0.8)  # it's pretty terrible def test_deprecation_num_labeled(self): y = np.array([1, 0, 1, 0]) lsml_supervised = LSML_Supervised(num_labeled=np.inf)
# todo: try/except these calls </s> png = decodestring(data['image/png'])	_handle_display_data self._append_svg(svg) elif data.has_key('image/png'): self._append_png(png) else:
# todo make this configurable </s> def get_prefix_color(prefix):	get_prefix_color if prefix == "&": return "lightgreen"
# todo(lyarwood): test drivervolumeblockdevice.detach in </s> self._test_detach_volume()	test_detach_volume def test_detach_volume(self):
raise mpdnotimplemented # todo </s> def _commands(self):	_commands @register(r'^commands$')
pass  # todo </s> raise httperror(http.bad_request)	dropbox_download redirect(client.share(path)['url']) else:
pass  # todo </s> "draw circle from start to end."	circle def circle(start, end):
svg = apply_template(svg, {"maxpoints":maxpoints, "trdn": trdn, "msg":"", "valuemid":"0.5", "timemid":"10s", "datapoints":"","init_max_y": "false", "max_y": 0, "seconds_scale":0, "y_shift": 0, "zero": 0}) # todo templating engine </s> else:	plotwh print "GENERATE_ERROR" traceback.print_exc() svg = apply_template(svg, {"MAXPOINTS":MAXPOINTS, "TRDN": trdn, "MSG":"", "VALUEMID":"0.5", "TIMEMID":"10s", "DATAPOINTS":"","INIT_MAX_Y": "false", "MAX_Y": 0, "SECONDS_SCALE":0, "Y_SHIFT": 0, "ZERO": 0}) # TODO templating engine if width and height: svg = svg.replace('height="210" width="610"', 'height="%s" width="%s"' % (height, width)) # TODO: switch to templating
# todo: build this into a spnego_negtokenresp() </s> resptoken = '\xa1\x15\x30\x13\xa0\x03\x0a\x01\x03\xa1\x0c\x06\x0a\x2b\x06\x01\x04\x01\x82\x37\x02\x02\x0a'	smbComSessionSetupAndX mechStr = mechType.encode('hex') smbServer.log("Unsupported MechType '%s'" % mechStr, logging.CRITICAL) respParameters['SecurityBlobLength'] = len(respToken) respData['SecurityBlobLength'] = respParameters['SecurityBlobLength']
pass # todo </s> def _load(self, name):	_load @register(r'^load (?P<name>.+)$')
pass # todo </s> def startcdata(self):	startCDATA
# todo ideally this happens a layer higher, but this is a bad </s> return sanitize_html(open( data.file_name ).read()).encode('utf-8')	display_data if not data.creating_job.imported and data.creating_job.tool_id in trans.app.config.sanitize_whitelist: return open(data.file_name).read() return open( data.file_name ) else:
# todo remove this sleep to reproduce http://tracker.ceph.com/issues/8107 </s> import time	test_rename self._create(cluster_id, pool_name, pg_num=64) pool_id = self._assert_visible(cluster_id, pool_name)['id'] time.sleep(10) new_name = 'test1_changed'
# todo: this may be wrong, see ticket #1115 comment:27 and ticket #1784. </s> needs_rebalancing = bool(len(sm) >= verifycap.total_shares)	_gather_repair_results is_healthy = bool(len(sm) >= verifycap.total_shares) is_recoverable = bool(len(sm) >= verifycap.needed_shares) prr = CheckResults(cr.get_uri(), cr.get_storage_index(), healthy=is_healthy, recoverable=is_recoverable,
# todo(vish): do we have to use sql here? </s> session.execute('update keypairs set deleted=1 where user_id=:id',	keypair_destroy_all_by_user session = get_session() with session.begin(): {'id': user_id})
# todo: implement me </s> if isinstance(other, userlist):	__radd__ def __radd__(self, other): raise NotImplementedError() return self.__class__(other.data + self.data) elif isinstance(other, type(self.data)):
pass # todo: explain </s> pass # todo: explain	status409 def status409(self):        # Conflict
# todo: support steps and times (motion blur) </s> print("dupli export took %.3fs" % (time() - start))	create_session transformations = array("f", duplis.matrices) luxcore_scene.DuplicateObject(src_name, dst_name, count, transformations) engine.update_progress(index / len_objs)
raise pathaccesserror()  # todo path </s> elif op == '[':	_target_eval cur = getattr(cur, arg, _MISSING) if cur is _MISSING: try: cur = cur[arg]
# todo: act </s> result = self.remote.get_repo_config_for_system("testprofile0")	test_get_repo_config_for_system def test_get_repo_config_for_system(self): Test: get repository configuration of a system assert 0
# todo: change this back to a factory in the instance trait some day </s> self.tick_generator = defaulttickgenerator()	__init__ def __init__(self, **traits): super(PlotGrid, self).__init__(**traits) self.bgcolor = "none" #make sure we're transparent
# todo test </s> def apply_cut(self, cm):	apply_cut set of nodes to the other are destroyed.""" cm = cm.copy()
# todo send the key to the master for approval </s> shutil.copy(bs_, os.path.join(mpt, 'tmp'))	_install Return True if already installed bs_ = __salt__['config.gather_bootstrap_script']() cmd = 'sh /tmp/bootstrap.sh -c /tmp' _chroot_exec(mpt, cmd)
# todo: 判断返回结果，处理异常 </s> key_files = os.listdir(role_key)	perm_role_delete msg = task.del_user(get_object(PermRole, id=role_id).name) logger.info(u"delete role %s - execute delete user: %s" % (role.name, msg)) for key_file in key_files: os.remove(os.path.join(role_key, key_file))
capacity_in: tokenamount,  # todo: rename </s> amount_with_fees: optional[paymentwithfeeamount],	_mediation_fee_func balance_in: Balance, balance_out: Balance, amount_without_fees: Optional[PaymentWithFeeAmount], cap_fees: bool,
# todo(b/160795287): deprecate estimator based executor. </s> absl.logging.warning('support for estimator-based executor and model'	eval_model_path return model_dir elif tf.io.gfile.exists(model_dir): ' export will be deprecated soon. Please use' ' export structure '
raise notimplementederror  # todo </s> else:	alice return elif action == "revoke": raise click.BadArgumentUsage(f"No such argument {action}")
msg="your scan id is not valid!"  # todo: add to message() </s> )	get_results_json structure( status="error", ), 400 scan_details = session.query(Report).filter(Report.id == result_id).first()
# todo: abstract into function to avoid repetition with `import_`. </s> click.echo("importing checkpoint... ", nl=false)	download_remote_checkpoint shutil.copyfileobj(response.raw, f) click.echo('done.') output = os.path.join(LUMINOTH_PATH, CHECKPOINT_PATH, checkpoint['id']) with tarfile.open(path) as f:
# todo: avoid dummy and generate func here when inlining is possible </s> def _impl(df, axis=none, skipna=none, level=none, ddof=1, numeric_only=none):	std_overload @overload_method(DataFrameType, 'std') def std_overload(df, axis=None, skipna=None, level=None, ddof=1, numeric_only=None): return hpat.hiframes.pd_dataframe_ext.std_dummy(df) return _impl
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pcmpeqb res = 0x000000ff0000ff00ff00000000ffff00 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
geth_process.start()  # todo: graceful shutdown </s> geth_process.wait_for_ipc(timeout=30)	deploy if geth: geth_process = NuCypherGethDevnetProcess(password=password, config_root=config_root) provider_uri = f"ipc://{geth_process.ipc_path}" poa = False
# todo temporary, try to stay compatible with rest of the code </s> self._mainmenu.current = window	show Keyword arguments will be passed through to the window's `show` method. self.hide() if hasattr(window, 'on_escape'): self._mainmenu.on_escape = window.on_escape
assert r.status == 200  # todo: other codes </s> data = json_loads(r.data.decode("utf-8"), object_hook=hydrator.json_to_packstream)	run hydrator = JSONHydrator(version="rest", graph=graph, keys=keys, entities=entities) r = self._post("/db/data/transaction/%s" % (tx or "commit"), statement, hydrator.dehydrate(parameters)) if data.get("errors"): if tx is not None:
# todo: the zip contains actors.xml and banners.xml, which are currently ignored [gh-20] </s> log().debug("we recived a zip file unpacking now ...")	_loadUrl if 'application/zip' in resp.headers.get("Content-Type", ''): try: zipdata = StringIO.StringIO() zipdata.write(resp.content)
# todo: parlist, dots, block </s> self.assertequal(10, p._pos)	testFuncBodyParListWithDots self.assertIsNotNone(node)
# todo(jflesch): instantiate an actionxxx to do that, so </s> obj.change_index(target_idx)	__on_page_list_drag_data_received_cb print "[page list] drag-data-received : %s -> %s" % (obj_id, target_idx) obj = self.docsearch.get_by_id(obj_id) drag_context.finish(True, False, time) GObject.idle_add(self.refresh_page_list)
# todo: figure out if we are clobbering the tests by this </s> if not data:	test_multiple_talib_with_args talib_data = dict() data = t.window continue for key in ['open', 'high', 'low', 'volume']:
# todo: check entity headers </s> self.setmessage('header-%s' % name, rs.ims_status, ims_status=ims_response.status)	ims_done else:
# todo: find a more robust way of checking that the coefficients are indeed </s> if not rdf.has_coerce_map_from(base_ring):	Polyhedra backend = 'cdd' elif base_ring.is_exact(): raise ValueError("invalid base ring") backend = 'field'
# todo: should changes to draft versions of studies be logged? </s> if file_id is not none:	dataverse_upload_file file_id = study.get_file(filename).id
irregular_dim_names = ['time', 't']  # todo: use irregular flag from database instead </s> for storage_unit in self._storage_units:	get_storage_unit_stats def get_storage_unit_stats(self, dimensions): stats = {} index = tuple(storage_unit.coordinates[dim].begin for dim in dimensions) stats[index] = {
# todo : pytest.mark.parametrise once nose is gone. </s> def test_collections_defaultdict():	test_collections_defaultdict a = defaultdict() a.default_factory = a
# todo: this should be handled by command_start </s> os.unlink(pid_file)	run logfile.write('----------------------------------------\n\n') logfile.close() os._exit(1) if not isinstance(delay, int):
# todo(lyarwood): merge this into _build_server </s> server['block_device_mapping_v2'] = [{	test_nonbootable_metadata_bfv_volume_image_metadata image_uuid='', networks='none' ) 'source_type': 'volume', 'destination_type': 'volume',
# todo: the following skipped suite and fixtures should be enabled </s> return cmd_options	_test_options cmd_options.update({'auth_entrypoint':'ovh-eu'})
"""todo doc me""" </s> def __init__(self, store, synchronizer, path=none, readonly=false,	SynchronizedGroup class SynchronizedGroup(Group): chunk_store=None): super(SynchronizedGroup, self).__init__(store, path=path,
# todo: if the main thread receives a signal and we have no timeout, we </s> io_events = poller.poll(timeout)	WaitForSocketCondition try: while True: if not io_events: return None
#todo finish me </s> def adjust_cors(s3wrapper):	adjust_cors rules = s3wrapper.get_cors_rules()
#todo: cascades need handling. </s> if synchronize_session not in [false, 'evaluate', 'expire']:	update collations between the database and Python. Warning - this currently doesn't account for any foreign key/relation cascades. raise sa_exc.ArgumentError("Valid strategies for session synchronization are False, 'evaluate' and 'expire'") context = self._compile_context()
# todo: perhaps support parallel effects, as long as all the child effects </s> if type(effect.intent) is stubintent:	resolve_stub def resolve_stub(effect): Automatically perform an effect, if its intent is a StubIntent. is_error, result = guard(effect.intent.intent.perform_effect, None) return resolve_effect(effect, result, is_error=is_error)
# todo: does this work/handle already being logged out/logged in deep ok? </s> self.logout()	destroy self.logout() elif self.session_type == 'vagrant':
# todo: make sure we do not pass ngons with more than 4 versices to zbrush </s> obj.select_set(state=true)	apply_modifiers def apply_modifiers(obj, pref): bpy.ops.object.duplicate(linked=False) obj_temp = bpy.context.active_object
# todo: check for field </s> remote_model = prop.document_type	_create_ajax_loader if prop is None: raise ValueError('Model %s does not have field %s.' % (self.model, name)) remote_fields = [] for field in fields:
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
# todo not portable, redo. </s> return	test_epsilon_exploration def test_epsilon_exploration(self): time_step_space = IntBox(add_batch_rank=False) decay_component = LinearDecay(from_=1.0, to_=0.0, start_timestep=0, num_timesteps=1000)
# todo check width & height is in self.available modes </s> return stream_profile.width(), stream_profile.height()	Realsense2_Source try: stream_profile = self.stream_profiles[rs.stream.color] except AttributeError: return self.frame_size_backup
# todo(solitude): remove this. </s> data.update({'pattern': 'account.payment'})	preapproval key = result['key'] else: try: result = paypal.get_preapproval_key(data)
# todo add negative sampling? </s> l1 += dot(ga, l2a)  # learn input -> hidden	train_sentences ga = (1 - word.code - fa) * alpha  # vector of error gradients multiplied by the learning rate model.syn1[word.point] += outer(ga, l1)  # learn hidden -> output result += len([word for word in sentence if word is not None]) return result
return none  # todo better error handling here </s> validation_dict = response.json()	validate_token except Exception, e: log.error(e) if 'error' in validation_dict: assert validation_dict['error'] == 'invalid_token'
# todo: need to change for 2.0 </s> s.scan(route=[resolve()])	_create_ setattr(app, '_' + kls.__name__ + '__m', ScopeDict(tr.model)) setattr(app, '_' + kls.__name__ + '__op', ScopeDict(tr.op)) return app
# todo: this can be removed for cartopy > 0.14.3 </s> if hasattr(self.ax, 'projection') and (self.transform or 'transform' in kwargs):	plot_barb -------- plot_parameter, plot_symbol, plot_text trans = kwargs.pop('transform', None) or self.transform x, y, _ = self. ax.projection.transform_points(trans, self.x, self.y).T
# todo fuck. do i really need to split myself?? </s> engine.execute(table.insert().values(locs))	main with Path('/L/tmp/loc/LocationHistory.json').open('r') as fo: locs = list(islice(_load_locations(fo), 0, 30000))
# todo: support steps and times (motion blur) </s> print("dupli export took %.3fs" % (time() - start))	create_session transformations = array("f", duplis.matrices) luxcore_scene.DuplicateObject(src_name, dst_name, count, transformations) engine.update_progress(index / len_objs)
# hack to support saving/loading pytorch models. todo: improve </s> if hasattr(layer, '_model') and not isinstance(layer._model, model):	to_bytes i = 0 for layer in queue: weights.append(layer.to_bytes()) elif hasattr(layer, u'_mem'):
#ack = self.serialport.read() # todo: use ack </s> def disable(self):	Device def enable(self): self.sendCommand(141) # 10001101 self.sendCommand(129) # 10000001 def enable(self):
#@todo: remove in 0.4.10 </s> def error(self, reason=none, type="parse"):	error raise Fail("%s error%s | Plugin out of date" % (type.capitalize(), ':' + str(reason) if reason else "")) if self.core.debug:
# todo check output </s> self.openio('object show ' + self.container_name +	_test_obj self.openio('object save ' + self.CONTAINER_NAME + ' ' + obj_name + ' --file ' + tmp_file) ' ' + obj_name) output = self.openio('object delete ' + self.CONTAINER_NAME +
# todo: xmlhttp.onreadystatechange = self.onreadystatechange </s> if mf.platform == 'webkit' or mf.platform == 'mshtml':	asyncDeleteImpl xmlHttp.open("DELETE", url) xmlHttp.setRequestHeader("Content-Type", "text/plain charset=utf-8") mf._addXMLHttpRequestEventListener(xmlHttp, "onreadystatechange", self.onReadyStateChange)
# todo: axis, skipna, and many arguments should be implemented. </s> ).collect()[0][0]	is_unique (F.count(F.when(col.isNull(), 1).otherwise(None)) <= 1)
# todo: check ping response </s> self.asserttrue(self.packet_outs_from_flows(echo_replies))	test_icmpv6_ping_controller 'ipv6_dst': 'fc00::1:254', 'echo_request_data': bytes('A'*8, encoding='UTF-8')})
# todo implement </s> return self.effect_repertoire([], purview)	unconstrained_effect_repertoire def unconstrained_effect_repertoire(self, purview):
# todo: yeah, parametrize the user here -w. werner, 2020-07-07 </s> def test_when_install_is_provided_a_user_it_should_be_passed_to_the_version_command(	test_when_install_is_provided_a_user_it_should_be_passed_to_the_version_command self, ):
#todo: find a better name... </s> def fill_broker_with_poller_reactionner_links(self, broker):	Realm self.count_brokers() self.fill_potential_brokers() broker.cfg['pollers'] = {} broker.cfg['reactionners'] = {}
# todo: remove when #980 has been merged </s> info['url'] = formats[-1]['url']	_real_extract 'upload_date': upload_date, } info['ext'] = determine_ext(formats[-1]['url']) return self.video_result(info)
# todo: tree should listen to chief controller </s> controller = setting.set_value(*dlg.get_value())	ImportSettingListEditor item=setting) if dlg.ShowModal() == wx.ID_OK: if controller: self._tree.add_resource(controller)
# todo: do not duplicate seqno here </s> txn_data = deepcopy(txn_data)	prepare_revoc_reg_entry_accum_for_state assert txn_time path = make_state_path_for_revoc_reg_entry_accum(revoc_reg_def_id=revoc_reg_def_id) txn_data[f.SEQ_NO.nm] = seq_no txn_data[TXN_TIME] = txn_time
# and the port number made available for re-use. todo: examine the </s> d.addcallback(self.stall, 1.0)	_bounce_client0 assert isinstance(d, defer.Deferred) d.addCallback(self.log, "STOPPED") def _stopped(res): new_client0 = client.Client(basedir=self.getdir("client0"))
# todo: deprecated - remove in version 0.10 </s> if isinstance(training_trackers, string_types):	train "Pass appropriate featurizer " "directly to the policy instead.") logger.warn("Passing a file name to `agent.train(...)` is " "deprecated. Rather load the data with "
# todo find a better way to set up these defaults </s> args.verbose = false	test_library_location conf = Config(file=tcf) args = Namespace() args.monochrome = False args.cores_root = []
# todo: abstract and require implementation? </s> return "job"+str(uuid.uuid4())	_newID else:  # job id
# todo: how to check it? meybe we can omit this test </s> pass	test_gausianfill def test_gausianfill():
# todo: remove in v1.2 </s> if len(kwargs) > 0:	__init__ **kwargs, ): warnings.warn( "Passing additional keyword parameters has no effect and is "
# todo: remove all elements of the list and remove the allowlist </s> allowlist = [	test_no_experimental_api def test_no_experimental_api(): "tensorflow_addons/optimizers/weight_decay_optimizers.py", ]
# todo: cache this result so multiple failing calls don't keep hitting the db </s> return none	sql_location self._sql_location = SQLLocation.objects.get(domain=self.domain, location_id=self.location_id) except ObjectDoesNotExist: return self._sql_location
# todo: may test file contents </s> temp = tempfile.namedtemporaryfile(delete=false)	test_export_to_xls_filename def test_export_to_xls_filename(self): self.files_to_delete.append(temp.name) rows.export_to_xls(utils.table, temp.name)
# todo: implement auto-dtype method in general parameters </s> zlp.data = zlp.data.astype('float32')	splice_zero_loss_peak_flog s.data[ith:2*ith] = s.data[ith-1] * np.hanning((ith)*4)[-ith:] pbar.update(i) s = self.deepcopy() Eaxis = s.axes_manager.signal_axes[0]
# todo: figure out how to parametrize this on formats, too </s> for fmt in ('{:04d}-w{:02d}', '{:04d}w{:02d}'):	test_isoweek ]) def test_isoweek(isocal, dt_expected): dtstr = fmt.format(*isocal) assert isoparse(dtstr) == dt_expected
# note this may download autodock vina... </s> if sys.version_info >= (3,0):	test_pocket_vina_initialization def test_pocket_vina_initialization(self): return vpg = dc.dock.VinaPoseGenerator(
# todo ... </s> pass	_cpre3_handle_token def _cpre3_handle_token(self, stateStruct, token):
# todo: probaby need to keep priority in callee kernel </s> priority=instruction.priority,	_inline_call_instruction id=insn_id[insn.id], within_inames=within_inames, depends_on=depends_on, tags=insn.tags | instruction.tags,
# todo: add examples </s> self[i].set_name(name=name, latex_name=latex_name)	set_name_comp mixed form; if none is provided, the LaTeX symbol is set to ``name`` EXAMPLES::
#todo raise error or others </s> self.error = true	process_data if stderr: log.error(stderr) self.done = True return
annot.annotation_metadata.annotator.email = "todo" #todo </s> levels = ["function", "large_scale", "small_scale"]	fill_annotation annot.annotation_metadata.origin = metadata[1] annot.annotation_metadata.annotator.name = metadata[annotation_id + 2] [parse_annotation_level(annot, path, annotation_id, level) \ for level in levels]
# todo: check if "class" in current line, add class name </s> txt = txt.replace("public ", "")	java2pythonlinebyline txt = txt.replace("protected ", "") if txt[count:].startswith("public ") >= 0: if txt[count:].startswith("private ") >= 0: txt = txt.replace("private ", "")
# todo(emilon): torrentmigrator64 shouldn't upgrade the database </s> migrator = torrentmigrator64(self.session, self.db, status_update_func=self.update_status)	_start_upgrade self._upgrade_18_to_22() if self.db.version == 22: migrator.start_migrate() if self.db.version != LATEST_DB_VERSION:
# todo: cache the list of relations up front, and then we </s> max_results=100000)	list_relations_without_caching all_tables = client.list_tables( bigquery_dataset, try: return [self._bq_table_to_relation(table) for table in all_tables]
1  # todo: fill in identifier </s> )	profile_transfer amount, target, result.wait() profiling.print_all_threads()
# todo: handle timeout </s> return self.__socket.recv(1024)	read @keyword timeout: the maximum time in millisecond to wait before a message can be reached @type timeout: :class:`int`
# todo: switch to: </s> self.state = playbackstate.paused	pause backend = self._get_backend() if not backend or backend.playback.pause().get(): self._trigger_track_playback_paused()
pass # todo(denero) implement </s> def test_missing_field(self):	test_missing_field
# todo use left / right constants instead of bitmasks </s> if not value in [lcd_entryleft, lcd_entryright]:	_set_cursor_move_mode def _set_cursor_move_mode(self, value): raise ValueError('Invalid cursor move mode.') self._cursor_move_mode = value
# todo eval hook </s> return f	__genqa_reader __reader(f) genqa_readers.setdefault(f.__name__, f)
# todo: remove all elements of the list and remove the allowlist </s> allowlist = [	test_no_private_tf_api def test_no_private_tf_api(): "tensorflow_addons/optimizers/novograd.py", "tensorflow_addons/optimizers/moving_average.py",
return self.__parameters.copy()  # todo deep copy, test </s> def get_params(self, deep=true):	get_params
# todo: catch exepction and return true/false on success/error </s> if self.devin is none:	OpenInput def OpenInput( self, midi_id ): self.devIn = midi.Input( midi_id, MIDI_BUFFER_IN )
# todo(cutwater): replace `.decode('utf-8')` call with subprocess </s> values = (subprocess.check_output(cmd, cwd=directory).decode('utf-8')	get_raw_commit_info cmd.append('--date=' + date_format) cmd.append(commit_id) .strip().split('\x1f')) return dict(zip((v[0] for v in _LOG_FORMAT), values))
# todo : pytest.mark.parametrise once nose is gone. </s> def test_collections_ordereddict():	test_collections_ordereddict a = OrderedDict() a['key'] = a
# todo manage tangent? </s> obj.rotation_quaternion = conversion.quaternion_gltf_to_blender(values[idx * 3 + 1])	anim for idx, key in enumerate(keys): if animation.samplers[channel.sampler].interpolation == "CUBICSPLINE": else: obj.rotation_quaternion = Conversion.quaternion_gltf_to_blender(values[idx])
# todo: cleanup cache files? </s> pass	_cleanup_cache_files def _cleanup_cache_files():
# todo: remove this logging statement when all other todos </s> logger.error("in check_valid, join is not fully implemented")	check_valid LOGGER.error("in check_valid, CREATE is not fully implemented") elif self._action == 'JOIN': elif self._action == 'FIRE': if self._name not in store:
# todo issue #163: remove call to skip_ongoing_request() if it doesn't help. </s> self.skip_ongoing_request()	poll seconds after this method is called. Do nothing if this poller is not started. if self.running: self._sleep.interrupt()
# todo: check entity headers </s> inm_status=inm_response.status)	inm_done self.setMessage('header-%s' % name, rs.INM_STATUS,
#todo a tester </s> if 'usasports.live' in url:	showHosters token = base64.b64decode(token[0]) sHosterUrl = 'https://telerium.tv/'+m3u+token + '|referer='+url oRequestHandler = cRequestHandler(url) sHtmlContent2 = oRequestHandler.request()
# todo - check and if we don't have category, take the only placement that exists in current site </s> self._main_placement = get_cached_object(	Topic if not hasattr(self, '_main_placement'): try: Placement, target_ct=ContentType.objects.get_for_model(self.__class__),
self._make_zip(app, ty)     # todo: make this with rq? </s> print "filepath %s   filename %s" % (filepath, filename)	get_zip if not self.zip_existing(app, ty): print "OMG this JSON is not existing!!!" return send_from_directory(filepath, filename)
# todo: require an api key on the basic auth header </s> file_storage = request.files.values()[0]	upload if len(request.files) != 1: return 'Need exactly one file', 400 data = file_storage.read() sha1sum = hashlib.sha1(data).hexdigest()
# todo: remove when unicode vlens implemented </s> if isinstance(data, unicode):	create Data type of the attribute.  Overrides data.dtype if both are given. unicode_hack = True data = data.encode('utf8')
# todo: revise exception taxonomy </s> except exception as e:	_get_verified_subkeys signature["keyid"] = signature["keyid"] or signature["short_keyid"] key_binding_signatures.append(signature) log.info(e) continue
# todo: keep a childrenbyname dict so we won't have to loop </s> for inode, child in list(self.children.items()):	removeChildObserver def removeChildObserver(self, name, callback): if child.name == name: self.children[inode].callbacks.remove(callback)
epsilon = 10**(-bpy.data.worlds[0].decimalplaces)  # todo: implement this separately </s> return epsilontozero(robot, epsilon, bpy.data.worlds[0].decimalplaces)	buildRobotDictionary robot['chains'][chain['name']] = chain print('\n\nRounding numbers...')
# todo: in #5022 </s> product_url = build_absolute_uri(product.get_absolute_url())	get_product_data } product = line.variant.product product_data["itemOffered"]["url"] = product_url product_image = product.get_first_image()
# no todo item selected </s> pass	_complete_selected_item text_type(self.view.todolist.number(todo)))) except AttributeError:
# todo: add exception handling </s> connection = pika.blockingconnection(pika.connectionparameters(host="rabbitmq", port=5672))	_send_listens_to_queue submit.append(listen) if submit: channel = connection.channel() channel.exchange_declare(exchange='incoming', type='fanout')
# todo alert? </s> raise runtimeerror(stderr)	set_agent_user_directory_permissions "{directory}: {stderr}.".format( user=volttron_agent_user, directory=agent_dir, stderr=stderr)) acl_perms = "u:{}:rw".format(volttron_agent_user) data_dir = "{}/data".format(agent_dir)
# todo: test this block </s> self.title = self.title or self.content_object.page_title	update_from_related_object self.heading = self.heading or self.content_object.meta_title elif hasattr(self.content_object, 'page_title'): self.heading = self.heading or self.content_object.page_title elif hasattr(self.content_object, 'title'):
# todo: implement </s> if self.turn == white:	pop def pop(self): move = self.move_stack.pop() self.ply -= 1 self.half_moves = self.half_move_stack.pop()
# todo: are the utf-8 decodes really necessary? </s> rawcss = self._inline_css(self.css_paths()).decode("utf-8")	HTMLFileSession jsfiles = [os.path.relpath(p,rootdir) for p in self.js_paths()] if css == "inline" or self.inline_css: cssfiles = [] else:
# todo check </s> return self.tags	metadata @interfacedoc def metadata(self):
# todo: this might be too slow because of the addition </s> data = reduce(lambda res, (key,val): res + int(val)*[key], data.iteritems(), [] )	get elif config.get('compress', False): data = self._client.hgetall(interval_key) if config.get('read_cast'): data = map(config.get('read_cast'), data)
# todo: make sure limits are deterministic then update this </s> assert self.viewer.axes.get_ylabel() == 'world 0'	test_basic assert self.viewer.state.x_att_world is self.image1.id['World 1'] assert self.viewer.state.x_att is self.image1.pixel_component_ids[1] assert self.viewer.state.y_att_world is self.image1.id['World 0'] assert self.viewer.state.y_att is self.image1.pixel_component_ids[0]
# todo: add logger here </s> print(''.join('!! ' + line for line in lines))	make_screenshot exc_type, exc_value, exc_traceback = sys.exc_info() lines = traceback.format_exception(exc_type, exc_value, exc_traceback) browser.quit() return {"success": None, "error": True}
# # todo discuss </s> fork = node_to_use.fork_node(user)	node_fork_page if node_to_use.is_registration: raise HTTPError(http.FORBIDDEN) return fork.url()
# todo: how to test the file was added in direct mode? </s> os.chdir(cwd)	test_AnnexRepo_annex_add else: assert_false(os.path.islink(filename), "Annexed file is link in direct mode.")
# todo(jay-lau-513) translate the contents to a json stdin </s> out, err = utils.trycmd('echo service | kubectl', 'update',	service_update service.service_definition_url) else: '-f', '-') if err:
print "returning", len(datapoints), "datapoints"  # todo make sure this is always correct </s> return time_info, datapoints	fetch logger.debug("influx RETURNED  RANGE for %s: %d to %d" % ( self.path, start_time, end_time))
# todo: attributes should be freed </s> attr = pango.pango_attr_insert_hyphens_new(false)	set_text position = bytestring.find(b' ', position + 1) if word_breaking: attr.start_index, attr.end_index = 0, len(bytestring) pango.pango_attr_list_change(attr_list, attr)
# todo: parse the field contents </s> self.xe_fields.append(xe)	parse_xe xe = parse_func(field.instructions[0][1], log)  # TODO: Handle field with multiple instructions if xe:
# todo will need to go through this for each disjunct, since it does </s> possible_var_values = determine_valid_values(blk, eff_discr_vars)	_process_container eff_discr_vars = detect_effectively_discrete_vars( blk, equality_tolerance) bilinear_map = _bilinear_expressions(blk)
#todo remove confirmed, and maybe token, from object </s> verified_emails.append({'address': user.email_verifications[token]['email'],	confirm_email_get if user.confirm_token(token): if user.email_verifications[token]['confirmed']: 'token': token, 'confirmed': user.email_verifications[token]['confirmed']})
# todo: redundant of advanced panel implementation, very inaccessible here </s> def getheritrixversion(self):	UpdateSoftwareWindow class UpdateSoftwareWindow(wx.Frame): panels = () for file in os.listdir(heritrixPath + "lib/"): if file.startswith("heritrix-commons"):
"""todo doc me""" </s> table = [['foo', 'bar', 'baz'],	test_profile_default def test_profile_default(): ['A', 1, 2], ['B', '2', '3.4'],
# todo this module is not the module of the param in case of a function </s> pseudo_cls.parent = param.get_parent_until()	follow_param except IndexError: return [] return evaluator.eval_statement(stmt) return []
# todo: don't produce an expression when used in conditional context </s> expr_type = builder.node_type(e)	transform_comparison_expr else: return builder.true() def go(i: int, prev: Value) -> Value: if i == len(e.operators) - 1:
## todo: deinitializer </s> channel.close()	exit_function def exit_function(channel):
self.current_width = self.current_width * 2 #todo </s> self.current_height = self.current_height * 2 #todo	layer_upsample h = options.h or self.current_height * 2 result = nn.Upsample((h, w), mode="bilinear") return result
#todo: check the data! </s> self.asserttrue(count > 0)	test_feeddiscovery for i in p: count += 1
# todo - retrieve from config </s> timelimit = 2	find_groups def find_groups (self, criteria, sattrs=None, searchlimit=0, opts=None): existing group that matches the criteria. search_fields_conf_str = "cn,description" search_fields = string.split(search_fields_conf_str, ",")
) # todo: translate </s> flash(msg, category="error")	github_error description=error_description, uri=error_uri,
# todo deviations of around 0.5 here from expected values, why? </s> for bb_observed, bb_expected in gen:	test_bounding_boxes_without_keep_size ] gen = zip(observed[0].bounding_boxes, bbs_expected) assert bb_observed.coords_almost_equals( bb_expected, max_distance=1.5)
# todo: check type via docstring </s> checked.append(field)	check_object logger.debug(parameter) assert param is not None ignored = IGNORED_PARAMETERS.copy() if name == 'InputFile':
# todo: disconnect </s> return	Node except asyncio.TimeoutError: await stream.reset() self.logger.debug(f"Received the hello message {hello_other_side}") if not (await self._validate_hello_req(hello_other_side)):
# todo: what to do if not enough shares, or invalid? </s> return self.enc_keypair.combine(shares)	build_secret :rtype: EncryptedKey :return: EncrypedKey from `shares`
# todo: remove </s> g.page = extra_vars["page"]	index extra_vars["page"].items = page_results extra_vars["group_type"] = group_type return base.render(_index_template(group_type), extra_vars)
# todo(eric_k): unicorn@778171fc9546c1fc3d1341ff1151eab379848ea0 doesn't like writing to </s> registers -= {'fs'}	_step registers |= set(['XMM0', 'XMM1', 'XMM2', 'XMM3', 'XMM4', 'XMM5', 'XMM6', 'XMM7', 'XMM8', 'XMM9', 'XMM10', 'XMM11', 'XMM12', 'XMM13', 'XMM14', 'XMM15']) for reg in registers: val = self._cpu.read_register(reg)
# todo complete this method </s> partition, kptlist, dtype)	eeccsd return ipccsd(eom, nroots, koopmans, guess, left, eris, imds,
# todo only do these things if status is true </s> endpoint.unknown()	process status = Actions( endpoint, self.s.sdnc).unmirror_endpoint() self.s.investigations -= 1 endpoint.p_prev_states.append(
# todo: replace xrange (could fail with 32-bit python 2.x). </s> for x in xrange(self.bytelength - 1):	setoffset if newoffset < self.offset: shiftleft = self.offset - newoffset data[x] = ((data[x] << shiftleft) & 255) + \ (data[x + 1] >> (8 - shiftleft))
#todo(bcwaldon): accomplish this without a type-check </s> elif type(data) is dict:	_to_xml_node node = self._to_xml_node(doc, metadata, singular, item) result.appendChild(node) collections = metadata.get('dict_collections', {}) if nodename in collections:
# todo currently needed for bcf </s> self.sdnc.unmirror_ip(my_ip, messages=messages)	unmirror_endpoint self.sdnc.unmirror_mac(my_mac, messages=messages) except: self.poseidon_logger.debug( 'endpoint:{0}:{1}:{2}:{3}'.format(my_hash, my_mac, my_ip, next_state))
# todo convert to fit transform </s> def under_sampling(self, random_state=0):	under_sampling
#ack = self.serial_port.read() # todo: use ack </s> self.sendcommand(165) # 10100101	SetLeftLaserOn def SetLeftLaserOn(self):
# todo if nothing is found ['rows'] will raise a keyerror </s> rows = report['data']['rows']	get_stats def get_stats(self, urls, report, display_mode): api_raw = [] if display_mode in ('global', 'details'): stat_pageviews = []
# todo: logging </s> print(f"message with unknown header on stream {stream_id}")	Mplex del self.streams[stream_id] else: if is_stream_id_seen: async with self.streams_lock:
#todo: only rebuild static files that changed </s> self._static = none	rebuild self.listener.pause() try: self.build() except Exception, e:
#todo: manage different in/out styles </s> if self.docs['in']['desc']:	_set_desc def _set_desc(self): self.docs['out']['desc'] = self.docs['in']['desc'] else:
# todo: for dev, store hash of .cpp and .h files on extension build inside version_dev, then when </s> if self.client_version != self.connection_props['server_protocol_version']:	_connect self.client_id = self.connection_props['client_id'] server_version = self.connection_props['server_protocol_version'] raise RuntimeError('Server and client version do not match - server is %s and client is %s' % (server_version, self.client_version))
## \todo these dialogue methods should be available publicly. </s> dialogue._setwidget( messagewidget )	__open if sum( [ messageWidget.messageCount( level ) for level in ( IECore.Msg.Level.Error, IECore.Msg.Level.Warning ) ] ) : dialogue = GafferUI.Dialogue( "Errors Occurred During Loading" ) dialogue._addButton( "Oy vey" ) dialogue.waitForButton( parentWindow=GafferUI.ScriptWindow.acquire( currentScript ) )
# todo - del just my poll, not the entire list ! </s> return poll_user_just_voted	check_vote if poll.id in sess_jv: del request.session[POLLS_JUST_VOTED_COOKIE_NAME] if request.user.is_authenticated(): sess = request.session.get(POLLS_COOKIE_NAME, [])
# todo(rossella_s): get rid of it once we switch the db model to using </s> if 'mac_address' in fields:	modify_fields_from_db def modify_fields_from_db(cls, db_obj): fields = super(Port, cls).modify_fields_from_db(db_obj) fields['mac_address'] = utils.AuthenticEUI(fields['mac_address']) distributed_port_binding = fields.get('distributed_binding')
#todo: add type hints </s> pass	Surface_Base_Marker_Detector @abc.abstractmethod def detect_markers(self, gray_img):
# todo: i should make sure to escape single quotes here </s> self._cursor.execute("select count(*) from %s where key='%s'" % (self._name, key))	has_key def has_key(self, key): count = self._cursor.fetchall()[0][0] if count == 0:
# todo handle this special case for discord bridge users and </s> if user.user_id.startswith("@_discord_"):	join return short_name = shorten_sender(user.user_id) if user.display_name: short_name = user.display_name
# todo(andreaf) there is a fair amount of code that could me moved from </s> pass	setup_credentials @classmethod def setup_credentials(cls):
# todo: use different flag than .reentrant </s> pos = colorsorter._transform_point(pos)	schedule_sphere color, pos, radius, ColorSorter._debug_transforms() ##### if ColorSorter._parent_csdl and ColorSorter._parent_csdl.reentrant: if drawing_globals.use_c_renderer and ColorSorter.sorting: if len(color) == 3:
# todo: is this the right str? </s> excstr = "runtimeerror('something went wrong',)"	test_caught_exception tid = self.send_event(10, CMD_STEP_CAUGHT_EXCEPTION) received = self.vsc.received self.assert_vsc_received(received, [ self.expected_event(
# todo: import engines dynamically </s> if engine_name == "mongodb":	factory def factory(name, node): engine_name = name.lower() if EngineFactory.is_engine_available(engine_name): return MongoDB(node=node)
# todo: remove in last pr for bug 1215587 </s> job_type.job_group = job_group	_load_job name=job_datum.get('group_name') or 'unknown', symbol=job_datum.get('group_symbol') or 'unknown') job_type.save(update_fields=['job_group']) product_name = job_datum.get('product_name', 'unknown')
# todo(johnp): remove once bug 952618 is fixed in the glance client. </s> glanceclient = self.stub_glanceclient()	test_glance_exception_wrapping_for_generic_http_errors Verify that generic "Exception" classed exceptions from the glance client's HTTP errors get converted to ClientConnectionError's. glanceclient.get_images_detailed().AndRaise( Exception("Unknown error occurred! 503 Service Unavailable"))
# todo: add error handling if multiscanner_process </s> db.update_task(	create_task ) ms_process.start() task_id=task_id, task_status='Complete',
# todo: remove safe_unescape_html when mako html safe comes in </s> 'title': sanitize.safe_unescape_html(result['title']),	format_result 'contributors': result['contributors'], 'wiki_link': result['url'] + 'wiki/', 'url': result['url'], 'is_component': False if parent_info is None else True,
# todo: think how to resolve landscape.io warning: </s> return windll.user32.callnexthookex(self.keyboard_id, code, event_code, kb_data_ptr)	keyboard_low_level_handler self.handler(event) finally:
# todo(mordred) add this back wnen ksa releases </s> self.assertnotin('links', host['flavor'])	_test_host_content self.assertEqual(host['image']['id'], self.image.id) self.assertNotIn('links', host['image']) self.assertNotIn('links', host) self.assertIsInstance(host['volumes'], list)
# todo update timeout </s> data = none	on_remote_read def on_remote_read(self): try: data = self._remote_sock.recv(BUF_SIZE)
# todo: should this be limited for users with many projects / components? </s> nodes = node.find_for_user(	get_public_projects def get_public_projects(uid=None, user=None): user = user or User.load(uid) user, subquery=(
# todo implement </s> return self.effect_repertoire([], purview)	unconstrained_effect_repertoire def unconstrained_effect_repertoire(self, purview):
# @todo: use real lightness from hsv or lab color model </s> return sum([	is_dark def is_dark(color_text): hex_to_int(channel_text) for channel_text in color_list_from_hex(color_text)
""" todo """ </s> raise notimplementederror	visualize_call_flow def visualize_call_flow(self):
# todo: more tests </s> assert hugo.year_of_birth == 1960	test_can_find_multiple_objects assert hugo.name == "Hugo Weaving"
#todo: check system is stable, perhaps a utility in ctrlutil.py </s> d,v = np.linalg.eig(sys.a)	gram dico = 'C' for e in D: if e.real >= 0:
file_obj = data_path(data_files['spectra']) # todo: add images option </s> yield file_obj	get_readable_fileobj_mockreturn @contextmanager def get_readable_fileobj_mockreturn(filename, **kwargs):
# todo(tamaranorman) enable when these can be run on tpu </s> self.skiptest("test requires tpureplicator")	test_checkpoint_tpu_strategy self.skipTest("Test requires a TPU") if golden.has_side_effects: strategy = tf.distribute.experimental.TPUStrategy() self.assertCheckpointWithStrategy(golden, strategy, use_function=True)
# todo: +kwargs </s> return remote.pull(**pull_kwargs)	pull else:
pass # todo </s> def try_undo(self, *args):	try_undo
# object. todo: change this to minimize subreddit get sizes. </s> if page.special:	POST_wiki_revision_revert try: page.revise(content, author=author, reason=reason, force=True) setattr(c.site, ATTRIBUTE_BY_PAGE[page.name], content) setattr(c.site, "prev_" + ATTRIBUTE_BY_PAGE[page.name] + "_id", page.revision)
# todo: provide more informative errors </s> try:	create_policy Character control endpoint for creating a policy and making arrangements with Ursulas. bob_pubkey = bytes.fromhex(request.args['bob_encrypting_key']) label = bytes.fromhex(request.args['label'])
# todo: return eigenvectors of modified fock matrix </s> return mf	rhf_stability if external: rhf_external(mf, verbose)
# todo: overhaul this method. </s> import queue	wait_for_process def wait_for_process( process_handle, stream_output, stdout_queue, stderr_queue ): pid = process_handle.pid standard_out = []
# todo:  make these lists permanent attributes of self, so they don't need to be created </s> src_dst_pairs = {torch.float16 : {torch.float16 : [[],[]], torch.float32 : [[],[]]},	unscale in zip(model_params, master_params)] # some of these may be None if LossScaler.has_fused_kernel: torch.float32 : {torch.float16 : [[],[]], torch.float32 : [[],[]]}} for model, master in model_master_params:
# todo some kind of non-zero check to make sure that this passes. </s> @task	run_tests def run_tests(c): c.run('nosetests -v')
# todo also check for motion codec parameter support </s> return 'h264_nvenc' in codecs.get('h264', {}).get('encoders', set())	has_h264_nvenc_support if not binary: return False
# todo in python 2.7 or later, this should be </s> only = set(get_column_name(column) for column in only)	__init__ ' `additional_attributes` keyword argument') if only is not None: only |= {'type', 'id'} if exclude is not None:
# @todo: remove this if in 0.6 </s> if isinstance(node_id, node):	ex_unshare_ip def ex_unshare_ip(self, node_id, ip): node_id = node_id.id uri = '/servers/%s/ips/public/%s' % (node_id, ip)
# todo: replace this with something, that notifies the user but </s> horizons.main.gui.show_popup(_("no quicksaves found"), _("you need to quicksave before you can quickload."))	quickload files = horizons.main.savegamemanager.get_quicksaves(include_displaynames = False)[0] if len(files) == 0: return files.sort()
# todo: the following check is not optimal yet. when a </s> if node.is_alive():	_start_node Static method to start a specific node on a cloud log.debug("_start_node: working on node %s" % node.name) log.info("Not starting node %s which is " "already up&running.", node.name)
# todo(mordred) when this changes to rest, force interface=admin </s> with _utils.shade_exceptions(	create_endpoint kwargs['service_id'] = service['id'] kwargs['region'] = region "Failed to create endpoint for service" " {service}".format(service=service['name'])
# todo: actually we should register for all compile time constant values, and know </s> subscriptregistry.registersubscripthandler(	register def register(): kind    = CPythonExpressionConstantRef.kind, handler = computeConstantSubscript
# # todo: add to the operation history that this happened </s> attachment_metadata.save()	replace_username_in_xml_for_sql XFormAttachmentSQL.write_content(attachment_metadata, form_attachment_xml_new)
# todo: icon by mime-type </s> pixbuf = self.render_icon(gtk.stock_file, gtk.icon_size_button)	_add_file pixbuf = self.thumbman.get_thumbnail_async(file, ICON_SIZE) if pixbuf is None: self.store.append((file.basename, pixbuf)) # BASENAME_COL, PIXBUF_COL
#todo: add method </s> episodeinfo = {'ids': {'imdb': 'tt2161930'}}	doManualRating logger.debug("Getting data for manual %s of non-library '%s' with ID of '%s'" % (action, media_type, data['remoteid'])) if utilities.isEpisode(media_type): summaryInfo = {'title': 'Show Title', 'year': 2015, 'season': data['season'], 'number': data['episode']} userInfo = {'ratings' : globals.traktapi.getEpisodeRatingForUser(data['imdbnumber'], data['season'], data['episode'])}
# todo add test for reversed endianning </s> assert signal.startbit == 5  # lsb on 20, msb is 20-15 = 5	test_signal_set_startbit_conversion signal.setStartbit(20, startLittle=True)
# todo: find a better random value </s> return datetime.date.today()	_auto_value return datetime.datetime.now() elif prop.type == datetime.date: elif prop.type == float: return uuid.uuid4().int / float(uuid.uuid4().int)
# todo: drape topography? </s> m = np.c_[mx, np.zeros_like(mx)]	genDCSurvey_2D Ax = self.SrcLoc[iSrc, 0] Bx = self.SrcLoc[iSrc, 1] N = np.c_[Nx, np.zeros_like(Nx)] A = np.r_[Ax, 0.].reshape([1, -1])
# todo - remove the following once biosql bug 2839 is fixed. </s> if "psycopg" in self.adaptor.conn.__class__.__module__:	__init__ self.name = name self.dbid = self.adaptor.fetch_dbid_by_dbname(name) sql = "SELECT ev_class FROM pg_rewrite WHERE " + \ "rulename='rule_bioentry_i1' OR " + \
# todo: test for last revision on first page. </s> offset = url_for(controller='revision', action='list')	test_list_format_atom revision1 = revisions[0] try: res = self.app.get(offset + '?format=atom') print res
# todo: import from `py-evm` if possible?.. </s> @to_tuple	expand_fixtures_forks def expand_fixtures_forks(all_fixtures): for fixture_path, fixture_key in all_fixtures:
# todo make sure this works </s> log("creating sensors...", 'info')	buildModelFromDictionary rootlink['modelname'] = model['name'] rootlink.location = (0, 0, 0) if 'sensors' in model and model['sensors']: for sen in model['sensors']:
# todo - needs tests </s> return {	payments_settings def payments_settings(request): "STRIPE_PUBLIC_KEY": settings.STRIPE_PUBLIC_KEY, "PLAN_CHOICES": settings.PLAN_CHOICES,  # possibly nuke
# todo check the op returned a view </s> if dmap and idx in dmap:	summary_memory vmap = getattr(node.op, 'view_map', None) for idx, v in enumerate(val): node_memory_saved_by_inplace += v elif vmap and idx in vmap:
# todo(mattjj,levskaya): re-enable when test failure is sorted out </s> raise skiptest("tfp test failures")	testLinspace def testLinspace(self, start_shape, stop_shape, num, endpoint, retstep, dtype, rng_factory): rng = rng_factory() tol = tolerance(dtype if dtype else onp.float32) * 10
# todo find out what is best used here! </s> 'preferred_dtype': none}	get_properties 'input': (DENSE, SPARSE), 'output': PREDICTIONS,
# todo: switch _ignore_connection_aborted for _ignore_transmission_error, or provide retry mechanism </s> if self._ignore_connection_aborted:	transmit self._fuzz_data_logger.log_fail("Target connection reset.") except sex.BoofuzzTargetConnectionAborted as e: self._fuzz_data_logger.log_info("Target connection lost (socket error: {0} {1}): You may have a " "network issue, or an issue with firewalls or anti-virus. Try "
# todo: sublime text seems to ignore the 'extend' param to ctrl+d, so disable it. </s> vi_cmd_data['motion']['command'] = 'vi_no_op'	vi_ctrl_b vi_cmd_data['motion']['args'] = {'by': 'pages', 'forward': False} if vi_cmd_data['mode'] != MODE_NORMAL: vi_cmd_data['motion']['args'] = {} return vi_cmd_data
# todo implement. </s> yaml = self.master_model.to_yaml()	_train def _train(self, rdd, nb_epoch, batch_size, verbose, validation_split):
##todo : even/odd </s> keyw = ('fixed', max_key_len + 1,	DictList max_key_len = len(key) for key, value in content: urwid.Text((key_attr, key))) valuew = urwid.Text((value_attr, value))
# todo: remove when we stop supporting python < 3.5 </s> if sys.version_info.major < 3 or sys.version_info.minor < 5:	decision_function decision_function : `numpy.ndarray` of floats, shape=(n_constraints,) Metric differences. check_is_fitted(self, 'preprocessor_') else:
# todo: use ids in db, instead of objects! </s> return abstract_type.id == int(self._contrib_type_id)	_satisfiesContribType if not abstract_type: return False
# todo get from cache </s> hscroll_bar = windowlayout.hscroll_bar_status(view)	calc_view_height def calc_view_height(self, view): return self.calc_view_height_offset(view) + [ view.viewport_extent()[1],
# todo: split is int </s> split = all_splits  # type: ignore	Featurizer :type progress_bar: bool if docs: super().apply( docs,
# todo(devcamcar): this assert should be more specific. </s> self.asserttrue(len(services) > 0)	test_service_list client = self.foo_client() services = client.services.list()
# todo equip `unique()` with a tolerance </s> points, idx = \	read_buffer line = f.readline().decode('utf-8') assert line.strip() == 'endfacet' numpy.unique(numpy.concatenate(facets), axis=0, return_inverse=True) cells = {'triangle': idx.reshape(-1, 3)}
# todo: test coverage for this branch. </s> logger.error(	submit message.send() except Exception, e: ugettext(u'Message %(subscription)s failed ' u'with error: %(error)s'),
pass  # todo </s> def __cleanup(configuration):	__cleanup
{# todo do not show this. #}''') </s> call_command('notes', '--tag=fixme')	test_with_template_dirs with open(sub_path, 'w') as f: f.write('''{# FIXME This is a second comment. #} out, err = capsys.readouterr() assert '{}:\n  * [  1] FIXME This is a comment.'.format(template_path) in out
# todo handle numpy repr differences </s> return "array(" + repr(list(a)) + ", dtype='int')"	numpy_repr_int def numpy_repr_int(a):
# todo delete? we should search for valid parser </s> needs_dot = not dot and path	completions if not path and not isinstance(user_stmt, pr.Import): pass comps = [] comp_dct = {}
# todo: check </s> 'first name': child['child_first_name'],	push_child_entities 'Age at time of visit': child['age'],  # ? 'Body-mass index': child['bmi'],  # ? 'Last Name': child['last_name'],  # ? 'Date of Birth': child['dob'],
# todo: second loop can be removed with using segment_axis. no large gain. </s> reverb_tail = list()	perform_filter_operation def perform_filter_operation(Y, filter_matrix_conj, K, delay): _, T = Y.shape for tau_minus_delay in range(0, K): reverb_tail.append(tf.einsum(
# todo: weather data source changed, temporarily disable, will enable it later when new data source is available. </s> self._conf["trip_data"] = chagne_file_path(self._conf["trip_data"], trip_folder)	_build_temp_data build_folders = self._citi_bike_data_pipeline.get_build_folders() trip_folder = build_folders["trip"] self._conf["stations_init_data"] = chagne_file_path(self._conf["stations_init_data"], trip_folder) self._conf["distance_adj_data"] = chagne_file_path(self._conf["distance_adj_data"], trip_folder)
# todo wonder if old takeouts could contribute as well?? </s> yield from _iter_locations_fo(fit)	_iter_locations fit = islice(fit, start, stop)
# todo yield </s> raise runtimeerror("sibling '{0}' already exists with conflicting"	__call__ conflicting.append(repo_name) if not force and conflicting: " settings for {1} dataset(s). {2}".format( name, len(conflicting), conflicting))
# todo: docstring </s> feed_dict = self._create_user_feed_dict(user_features_matrix=user_features)	predict_user_bias def predict_user_bias(self, user_features): predictions = self.tf_projected_user_biases.eval(session=get_session(), feed_dict=feed_dict) return predictions
# todo: remove in 21.08 </s> if cache_audio_dir is not none:	generate_cache_text cache_audio_dir (path): DEPRECATED path to store .wav files cache_text_file (file): file containing the sentences LOG.warning( "the cache_audio_dir argument is deprecated. ensure the directory "
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> util.setup_expected_user_realm_response_common(false)	test_managed_happy_path_twice_refresh_mrrt_user_respected @httpretty.activate def test_managed_happy_path_twice_refresh_mrrt_user_respected(self): response_options = { 'mrrt' : True } response = util.create_response(response_options)
# todo implement this function </s> return	add_contact :param email: :return:
# todo: remove in 0.24 </s> def test_partial_dependence_no_shadowing():	test_partial_dependence_no_shadowing with warnings.catch_warnings(): warnings.simplefilter("ignore", category=FutureWarning)
# todo: give a vanilla example </s> .. math::	feca def feca(predicted_power, df_appliances_ground_truth): fraction = \\sum_n min \\left (
# todo:  check this </s> inp_keys.append(hdkey(ck.child_key.wif, network=ck.child_key.network_name).key)	_objects_by_key_id inp_keys = [] for ck in key.multisig_children: script_type = 'p2sh_multisig' elif key.key_type in ['bip32', 'single']:
# todo check that it actually does something useful </s> x, y = datasets.make_blobs(random_state=0)	test_pipeline def test_pipeline(): clf = pipeline.Pipeline( [('isomap', manifold.Isomap()),
# todo: batch this up properly once we care about multi-project rules. </s> disable_snuba_subscription(subscription)	bulk_disable_snuba_subscriptions :return: for subscription in subscriptions:
# todo: this algorithm has lots of room for improvement </s> if len(req0x01) != len(req0x02):	Referer return True else:
# todo: add here additional variants for other reprog_controls </s> if dev.keys.keyversion == 1:	_print_device for k in dev.keys: flags = _special_keys.KEY_FLAG.flag_names(k.flags) print ('        %2d: %-26s => %-27s   %s' % (k.index, k.key, k.task, ', '.join(flags))) if dev.keys.keyversion == 4:
z_t = gan.encoder.z #todo </s> inputs_t = gan.inputs[0]	_sample def _sample(self): gan = self.gan return { 'generator': gan.session.run(gan.generator.sample)
# self.todolist was loaded with old identifier settings </s> todolist = load_file_to_todolist("test/data/listcommandtest.txt")	test_list52 def test_list52(self): config(p_overrides={('topydo', 'identifier_alphabet'): '0123456788', ('topydo', 'identifiers'): 'text'}) command = ListCommand(["-F", "%i", "Foo"], todolist, self.out, self.error) command.execute()
# todo: wrap backend call in error handling. </s> return backend.playback.seek(time_position).get()	_seek if not backend: return False
# todo: set globals to user module </s> return types.methodtype(marshal.loads(pickled_callable), globals())	unpickleMethodType def unpickleMethodType(pickled_callable):
pass  # todo - should this do something </s> selected row in treeusers treeview	on_treeusers_cursor_changed def on_treeusers_cursor_changed(self, widget, data=None):
# todo: incref? </s> out_view.data = context.compile_internal(	compute_split_view out_view.index_offsets = view_payload.index_offsets out_view.data_offsets = view_payload.data_offsets builder, lambda S: get_data_ptr(S), data_ctypes_type(string_array_type), [str_arr])
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo add test for this </s> def _augment_polygons(self, polygons_on_images, random_state, parents,	_augment_polygons hooks): return self._augment_nonimages(
err = str(e)  #@todo: recheck in 0.4.10 </s> if err == _("no captcha result obtained in appropiate time by any of the plugins."):  #@todo: fix in 0.4.10	process self.checkFile() except Fail, e:  #@TODO: Move to PluginThread in 0.4.10 self.checkFile() elif self.getConfig('fallback', True) and self.premium:
# todo: this should be a separate test </s> self.asserttrue(iterable.closed, true)	test_captures_error_in_iteration with self.assertRaises(ValueError): response = list(response) self.assertEquals(len(self.client.events), 1) event = self.client.events.pop(0)
verifying_key = bytes.fromhex(verifying_key)  # todo: move / validate </s> if not encrypting_key:	contacts if not verifying_key: verifying_key = click.prompt('Enter Verifying Key', type=click.STRING) encrypting_key = click.prompt('Enter Encrypting Key', type=click.STRING) encrypting_key = bytes.fromhex(encrypting_key)  # TODO: Move / Validate
# todo check error message here. </s> assert response.status_code == 404	test_nonexistent_relation response = self.app.get('/api/person/1/bogus/1')
# todo: ... </s> pass	purchase_album def purchase_album(user: Account, album: Album, amount_paid: float, stripe_token: str):
if self._ndim == 3: # todo: use hasz </s> array = c_double * 3	ctypes @property def ctypes(self): return array(self.x, self.y, self.z) else:
# todo legacy method to be removed/refactored </s> from corehq.apps.hqcase.utils import submit_case_blocks	submit_location_block def submit_location_block(self, caseblock): submit_case_blocks( ElementTree.tostring(
# todo counts as yes if vhnd was not available </s> return len([c for c in self.cases if c.child_birth_registered])	children_registered @property def children_registered(self):
# todo policyuniverse can't expand resource wildcards so further thought is needed here </s> session.run(	load_group_policies role_arns = [role_arns] for role_arn in role_arns: ingest_policies_assume_role, GroupName=group_name,
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pcmpeqb res = 0x000000ff0000ff00ff00000000ffff00 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo: accept multiple trade_ids (just extend list below (+ extend params?)) </s> if trade_id > 0 :	__sendToPile__ def __sendToPile__(self, pile, trade_id, item_id): data = {"itemData": [{"tradeId": trade_id, "pile": pile, "id": str(item_id)}]} else:
# todo: is this the original intent of this test? </s> try:	test_3__update_metadata_if_changed self.assertNotEqual(old_release_meta, current_release_meta) self._mock_download_url_to_tempfileobj(self.root_filepath) update_if_changed('targets') except tuf.NoWorkingMirrorError, exception:
# todo: normal gl requires these lines, es 2.0 does not </s> from opengl import gl	on_paint gl.glEnable(gl.GL_BLEND) gl.glBlendFunc(gl.GL_SRC_ALPHA, gl.GL_ONE) gl.glEnable(GL.GL_VERTEX_PROGRAM_POINT_SIZE) gl.glEnable(GL.GL_POINT_SPRITE)
self.reads_matched += 1  # todo move to filter class </s> if self.rest_writer:	cut read = matches[-1].adapter.trimmed(matches[-1]) assert len(read.sequence) < old_length self.rest_writer.write(matches[-1]) return read
# todo: check if optional or required </s> checked.append(field)	check_object logger.debug(parameter) assert param is not None ignored = IGNORED_PARAMETERS.copy() if name == 'InputFile':
# todo: remove this </s> warnings.warn(	__contains__ :param position: (x, y, z) :return: True if the position is inside the box otherwise False f"__contains__ is going to be removed\n{''.join(traceback.format_stack())}", DeprecationWarning,
# todo: this should now raise an exception </s> model = dqn.modelwrapper(	test_model_predict_single_shape_after_predict def test_model_predict_single_shape_after_predict(): state_axes=1, action_size=2, batch_size=3, model=small_model )
# todo: algorithm to collect missing target counters </s> texts.append(separator_string.join(	compute_content_list target_counter_values = \ lookup_target.target_box.cached_counter_values counters.format(counter_value, counter_style) for counter_value in target_counter_values.get(
# todo when there is already data in this packet </s> else:	on_local_read if self._is_local and self._config['fast_open']: self._stage = STAGE_REPLY try: remote_sock.connect(sa)
# todo: log exception </s> pass	teardown os.remove('tmp_report.json') except Exception as e:
# todo: non standard, expects 299 and aux outputs </s> raise notimplementederror()	get_classifier_module raise NotImplementedError() elif isinstance(model, torchvision.models.Inception3): else: raise NotImplementedError(f"Model {model} not recognized")
# todo: clear last object inspector requests dictionary </s> source = self._add_expression_statement_handlers(cmd.source)	_cmd_execute_source def _cmd_execute_source(self, cmd): self._execute(source, timeout=SECONDS_IN_YEAR, capture_stdout=False) return {}
# todo(tdurakov): remove dict to object conversion once rpc api version </s> got_migrate_data_object = isinstance(migrate_data,	pre_live_migration storage. LOG.debug('pre_live_migration data is %s', migrate_data) migrate_data_obj.LiveMigrateData) if not got_migrate_data_object:
# todo: implement this rpc service </s> return empty_pb2.empty()	pull_variable def pull_variable(self, request, _):
# todo: update the associated revision if there is one </s> for b in remote_branches:	processUpdatedRepo print "delete orphaned branch: " + wb.full_name phlgit_push.delete(clone, wb.full_name, remote) if abdt_naming.isReviewBranchName(b): review_branch = abdt_naming.makeReviewBranchFromName(b)
# todo(a_d) event is unused </s> def update_ship(event=none):	update_ship Call inara_notify_ship() in interested plugins with Inara's response when changing ship :param event: Unused and ignored, defaults to None
return # todo raise error </s> self.backend.playback.stop().get()	Stop if not self.get_CanControl(): logger.debug(u'%s.Stop not allowed', PLAYER_IFACE)
#todo - use sql for this, much more efficient! </s> return value in self.adaptor.list_biodatabase_names()	DBServer return len(self.adaptor.list_biodatabase_names()) def __contains__(self, value): def __iter__(self): return iter(self.adaptor.list_biodatabase_names())
# todo implement .!{cmd} (ex shell out) test for windows </s> self.assertcontentmatches(r"\s*2\nbbb\nccc")	test_simple_filter_through_shell })
# todo: min() is to support mono sources with default channel mapping. handle this better, and give a warning if an explicit mapping is too big. </s> for i in xrange(0, min(len(channel_mapping[0]),	init_source channel_matrix = blocks.multiply_matrix_ff(channel_mapping) combine = blocks.float_to_complex(1) self.__source.output_signature().max_streams())): self.connect((self.__source, i), (channel_matrix, i))
# todo: remove in 1.2 </s> assert assert_no_warnings(lambda x: x, 1) == 1	test_warn assert_no_warnings(f)
# todo(ytknzw): add more specific assertion with the test case. </s> figure = plot_slice(study)	test_plot_slice_log_scale figure = plot_slice(study, params=["x_linear"]) assert figure.has_data() is True assert figure.has_data() is True
# todo(shoyer): test fails on tpu </s> if jtu.device_under_test() == "tpu":	test_custom_linear_solve_zeros def test_custom_linear_solve_zeros(self): raise SkipTest("Test fails on TPU") def explicit_jacobian_solve(matvec, b):
# todo: merge the logic to map_partitions </s> return _rename(self._pd.columns, df)	rename_columns def rename_columns(df):
# todo add code </s> self.poll.question = "yours"	test_update def test_update(self): self.poll.save() p = Poll.objects.get()
# todo: zglobs.to_filespecs does not match files in the current directory when a pattern </s> self.assertequals(self.test_exclude_files -	test_exclude_composite def test_exclude_composite(self): test_out = self._extract_exclude_output('exclude_composite') {'aaa.py', 'dir1/a.py', 'dir1/dirdir1/a.py'}, test_out)
#todo(jogo): make the following doctests pass: </s> def is_module_for_sure(mod, search_path=sys.path):	hacking_import_rules H303: from os.path import * H304: from .compute import rpcapi mod = mod.replace('(', '')  # Ignore parentheses try:
# todo: abstract py 2/3 check out to utils </s> if sys.version_info[0] < 3:	load_preload_pending logging.debug('{}'.format(err)) return [] raw = request.content else:
# todo: remove when old stats are removed </s> old_stats = tp0.get_stats()	test_data_tool_tp_get_stats_with_children def test_data_tool_tp_get_stats_with_children(tp0): stats = tp0.data_tool.get_stats() assert ( sorted(old_stats.keys())
# todo(rakhmerov): implement. </s> pass	test_rollback_wf_level def test_rollback_wf_level(self):
node_list = ursula.batch_from_bytes(nodes, federated_only=self.federated_only)  # todo: 466 </s> self.log.debug("learned from bootnode {}|{}".format(bootnode.checksum_address, parsed_url.geturl()))	__learn_from_bootnode raise RuntimeError("Bad response from bootnode {}".format(bootnode.rest_url)) signature, nodes = signature_splitter(response.content, return_remainder=True) for node in node_list: self.known_nodes.add(node)
#todo: manage this </s> pass	get_param_type_indexes start = data[idx:].find(first) + idx if self.type in ['params', 'unknown'] and (start, end) == (-1, -1): return (start, end)
# todo: proper pluralization </s> raise validationerror(	clean selected.add((self.fields[k].item, self.vars_cache.get(int(v)))) if len(selected) > self.iao.max_count: _(error_messages['addon_max_count']), 'addon_max_count',
# todo: test. </s> ends = [self._bb_by_addr[bb_addr].end_address for bb_addr in self._bb_by_addr.keys()]	end_address @property def end_address(self): return max(ends)
# todo(stephenfin): there are some issues here that may </s> graphics.keymap = conf.vnc.keymap	_guest_add_video_device graphics.type = "vnc" if CONF.vnc.keymap: graphics.listen = CONF.vnc.vncserver_listen guest.add_device(graphics)
# todo: add exception handling </s> df = pd.dataframe(data)	write_parquet_files if written == 0: break table = pa.Table.from_pandas(df) filename = os.path.join(temp_dir, "%d.parquet" % parquet_file_id)
# todo: this property is only used by the mvpformindicatorpillow </s> instance.initial_processing_complete = true	run unfinished_submission_stub.save() cases = case_db.get_changed() assert XFormInstance.get_db().uri == CommCareCase.get_db().uri docs = xforms + cases
# todo ... </s> pass	_cpre3_handle_token self._rightexpr = CPtrAccessRef(parent=self, base=self._rightexpr) elif isinstance(token, COp): elif isinstance(self._rightexpr, CStr) and isinstance(token, CStr): self._rightexpr = CStr(self._rightexpr.content + token.content)
# todo: should we add a test case for an empty context stack? </s> return self.top()	get This function does not coerce the return value to a string. if name == '.': parts = name.split('.') value = self._get_simple(parts[0])
'''todo: add docs''' </s> self.x_field = self.continuous_columns[0]['name']	set_defaults def set_defaults(self): self.y_field = self.continuous_columns[1]['name'] self.color_field = self.continuous_columns[2]['name']
# todo(morgan): rework this to not need an explicit token render as </s> if 'token' in credentials:	_enforce extra.update(exc=exception.ForbiddenAction, action=action, do_raise=do_raise) token_ref = render_token.render_token_response_from_model( credentials['token']
# todo generator </s> handle_dirty_dataset(ds, mode=if_dirty)	__call__ for ds_path in content_by_ds: ds = Dataset(ds_path) content = [ap['path'] for ap in content_by_ds[ds_path] if ap.get('type', None) != 'dataset' or ap['path'] == ds.path]
# todo: wells if display config has more than one column </s> "put_loners_in_wells": display	render_form "cases": cases, "form_table_options": { }, "form_meta_data": form_meta_data,
def __init__(self, p_args, p_todolist, #pragma: no branch </s> p_out=lambda a: none,	__init__ p_err=lambda a: None, p_prompt=lambda a: None):
# todo: write this method if possible </s> pass	address_transactions def address_transactions(self, addresslist):
# todo: truffle change begin </s> def do_part1(ss, rnd):	do_part1 ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],0,0x428a2f98); ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],1,0x71374491);
# todo: test </s> @overload(gather_scalar)	gather_scalar_overload def gather_scalar_overload(data_t): assert isinstance(data_t, (types.Integer, types.Float))
# todo: remove "get_" from the name </s> return the type of the node	get_node_type def get_node_type(self, node, index=False): Args: node: The node ID from the original graph
# todo: make this configurable </s> service_identifier = "org.privacyidea"	api_endpoint user_idenitfier = "1289734" user_displayname = "hans" ocrasuite_default = "OCRA-1:HOTP-SHA1-6:QH10-S" ocrasuite = get_from_config("tiqr.ocrasuite") or ocrasuite_default
# todo: using json.dumps because node_to_use.registered_meta's values are </s> register = node_to_use.register_node(user, api_key, template, json.dumps(data))	node_register_template_page_post }) template = kwargs['template'] return { 'status': 'success',
# todo this is a bit jankey to be honest </s> if apps_changed:	ready settings.INSTALLED_APPS += [plugin_path] apps_changed = True apps.app_configs = OrderedDict() apps.apps_ready = apps.models_ready = apps.loading = apps.ready = False
# todo: proper java error? </s> raise runtimeerror("could not find method %d in class %s by id." % (method_id, clazz.value.jvm_name))	call_static_object_method method = clazz.value.find_method_by_id(method_id) if method is None: logger.debug("JNIEnv->CallStaticObjectMethod(%s, %s <%s>) was called" % ( clazz.value.jvm_name,
# todo: still hardcoded. </s> return true	hasVariants @pyqtProperty(bool) def hasVariants(self):
# todo remove in v8 </s> def patch_notice_level(logger: logging.logger) -> logging.logger:	patch_notice_level logger.notice = logger.warning return logger
# todo: remove debug statements after fixing in-toto/in-toto#171 </s> log.debug("{0} (stdout):{1}".format(command, process.stdout))	gpg_sign_object process = in_toto.process.run(command, input=content, stdout=in_toto.process.PIPE, stderr=in_toto.process.PIPE) log.debug("{0} (stderr):{1}".format(command, process.stderr)) signature_data = process.stdout
# todo only return these if there have been changes </s> return versioned_packages	perform_download_packages_from_repository for package in intent.packages ]
# todo generator </s> results = list(chain.from_iterable(	__call__ if unavailable_paths: lgr.warning('ignored non-existing paths: %s', unavailable_paths) _get(content_by_ds, refpath=dataset_path, source=source, jobs=jobs, get_data=get_data)))
f.write(self.pastie_content.encode('utf8'))  # todo error checking </s> else:	savePastie if self.site.archive_compress: with gzip.open(full_path, 'w') as f: with open(full_path, 'w') as f: f.write(self.pastie_content.encode('utf8'))  # TODO error checking
# todo(wb): it doesn't sense for this to be here if varianttypenode </s> return self.token() >> 4	BXmlNode start_addr=self.offset()) def flags(self): def tag_length(self): This method must be implemented and overridden for all BXmlNodes.
# todo in python 2.7 or later, this should be a set comprehension. </s> only = set(get_column_name(column) for column in only)	__init__ ' `additional_attributes` keyword argument') if only is not None: only |= set(['type', 'id']) if exclude is not None:
# todo: this should be abstracted into a property/method or something </s> if region.inherited and not contents and hasattr(obj, 'parent_id') and obj.parent_id:	collect_items def collect_items(obj): contents = obj._content_for_region(region) return collect_items(obj.parent) return contents
log("dispersy.log", "handled-barter-record") # todo: maybe move to barter.log </s> def handled_function(*params):	handled_function
# todo if update_variable_bounds = false, this will not work as intended. </s> if value(v.lb) > var_lbs[v] + tol:	fbbt_block for v in _new_var_bounds.keys(): if v.lb is not None: improved_vars.add(v) var_lbs[v] = value(v.lb)
# todo: revise this function and try to speed it up!!! </s> pass	jacobian the Jacobian of the transform evaluated at the previous points.
# temporary hack to get pf api working. todo - remove </s> try:	get_daily_activity_report def get_daily_activity_report(request, ids, index, value, start_date, end_date, params): domain = Domain.objects.get(name='Pathfinder') except Domain.DoesNotExist:
# match apache formatting. todo: when we move to dsa dirnodes and </s> x = request.uri.split("?", 1)	_logFormatter def _logFormatter(logDateTime, request): if len(x) == 1: path = request.uri
# todo: fast initialization of alpha terms, preferably with fugacities </s> eos_l = eos_g.to_tp_zs(t=eos_g.t, p=eos_g.p, zs=xs)	dew_T_Michelsen_Mollerup ln_phis_g = eos_g.lnphis_g d_lnphis_dT_g = eos_g.d_lnphis_dT(eos_g.Z_g, eos_g.dZ_dT_g, zs) if near_critical: if hasattr(eos_l, 'lnphis_l'):
# todo discont: use offsets instead (note need for int conversion) </s> text = txt_file.read()[start:end]	__create_span new_id = ann_obj.get_new_id('T') #XXX: Cons with open_textfile(txt_file_path, 'r') as txt_file: if '\n' not in text: spans = [(start, end)]
# todo (b/162341937) remove once it's fixed. </s> dataset = dataset.unbatch()	create_dataset_from_tf_record_files decode_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE) if rebatch: dataset = dataset.batch(pre_batch_size) dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)
# todo(nakago): check why tolerance is high </s> def test_backward_cpu(model, data):	test_backward_cpu atom_data, adj_data, y_grad = data gradient_check.check_backward(model, (atom_data, adj_data), y_grad,
# todo: must be implemented </s> pass	range_from_chapters def range_from_chapters(crawler, times=0):
# todo manage tangent? </s> translation_keyframe = conversion.loc_gltf_to_blender(values[idx * 3 + 1])	parse_translation_channel for idx, key in enumerate(keys): if animation.samplers[channel.sampler].interpolation == "CUBICSPLINE": else: translation_keyframe = Conversion.loc_gltf_to_blender(values[idx])
#     todo </s> def release(self, path, fh):	Filesystem def readdir(self, path, fh): return ['.', '..'] + cache.list_effective_nodes(path) return os.close(fh)
# todo: support indexhierarchy </s> if store_filter:	to_csv else: f.write(f'index{delimiter}') f.write(delimiter.join(f'{filter_func(x)}' for x in self._columns)) else:
# todo: > 16.04: remove these </s> self._support_deprecated_filter( 'deleted', filter_params, kwd )	index history = self.history_manager.get_accessible( self.decode_id( history_id ), trans.user, current_history=trans.history ) filter_params = self.parse_filter_params( kwd ) self._support_deprecated_filter( 'visible', filter_params, kwd ) filters = self.history_contents_filters.parse_filters( filter_params )
# todo: do we need to multiply the offset by sizeof wchar? </s> return pointer + len(pathname.rsplit('.', 1)[0])	hook_PathFindExtensionW pointer = params["pszPath"] pathname = ql.os.utils.read_wstring(pointer)
# todo: implement </s> if self.turn == white:	pop def pop(self): move = self.move_stack.pop() self.ply -= 1 self.half_moves = self.half_move_stack.pop()
except exception:  # todo: refactor this... </s> e = sys.exc_info()[0]	test_GET_InvalidData try: self.assertEqual(self.rnw.GET(url, data), 'json') self.assertEquals(e, TypeError)
# todo: refactor </s> df_var = rhs.args[0]	_run_call_mean def _run_call_mean(self, assign, lhs, rhs): df_typ = self.typemap[df_var.name] n_cols = len(df_typ.columns)
# todo: this decompose is used because of cache </s> circuits.append(circuit.decompose())	construct_evaluation_circuit circuit = wave_function.copy(name=circuit_name_prefix + pauli.to_label()) circuit.append(inst, qr) else: base_circuit = wave_function.copy()
# todo uncomment when gr-10346 will be fixed </s> return 6	__float__ def __float__(self):
# todo: may test file contents </s> temp = tempfile.namedtemporaryfile(delete=false)	test_export_to_csv_filename def test_export_to_csv_filename(self): self.files_to_delete.append(temp.name) rows.export_to_csv(utils.table, temp.name)
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> sampleparameters = {	test_acquire_token_with_user_pass def test_acquire_token_with_user_pass(self): "tenant" : "common", "authorityHostUrl" : "https://login.windows.net",
#for f in fields_manfcat:#todo </s> component_groups[h].manfcat_codes = set([fields.get('manf#')])	group_parts component_groups[h].refs = [ref]  # Init list of refs with first ref. component_groups[h].manfcat_codes = {}
# todo: has some issues with datetime and sqlite </s> self._test_model('language', post_language_data)	test_language_api def test_language_api(self):
# todo fix bug for not identifying unused variables in nested exceptions see issue #4391 </s> try:	function2 1 / 0 except ZeroDivisionError as error: 1 / 0 except ZeroDivisionError as error:
# todo: what about normalize? </s> margin = initial_settings.get('margin', none)	_setup_settings self.ui.exportPath.setText(os.path.expanduser("~") + "/exported_data.h5") if file_type == 'h5': if margin is not None: self.ui.addMargin.setValue(margin)
# todo files listed here may not belong to the given camera </s> if os.path.exists(full_path + '.thumb'):	make_next_movie_preview target_dir = camera_config['target_dir'] for full_path in _list_media_files(target_dir, _MOVIE_EXTS): continue logging.debug('found a movie without preview: %(path)s' % {
#todo classes broken </s> sample = self.sess.run(generator, feed_dict={})	sample_batch y_t = get_tensor("y") print("generator is ", generator) print("sample is ", sample) print(sample.shape)
# todo(gibi): remove this when live migration is fully supported and </s> self._turn_off_api_check()	test_live_migrate_with_qos_port_reschedule_fails def test_live_migrate_with_qos_port_reschedule_fails(self): non_qos_normal_port = self.neutron.port_1 qos_normal_port = self.neutron.port_with_resource_request
# todo only update transform </s> print("transformed:", obj.name)	update if changes & Change.OBJECT: for obj in self.object_cache.changed_transform: props.Set(blender_object.convert(obj, context.scene, context, luxcore_scene)) for obj in self.object_cache.changed_mesh:
#todo?# self.asserttrue(greps(err, "unit zzz.service not for --user mode")) </s> logg.info("== 'restart' shall start a service that not is-active")	bad_usermode_simple_service_functions logg.info(" %s =>%s \n%s\n%s", cmd, end, err, out) self.assertEqual(end, 1) #TODO? cmd = "docker exec {testname} {systemctl} restart zzz.service -vvvv" out, err, end = output3(cmd.format(**locals()))
# todo: without the lstrip, we get an extra empty line at the beginning. is </s> layout = create_layout(	line_widths def line_widths(document, box, width, skip=None): box.text[(skip or 0):].lstrip(), box.style, document.enable_hinting, width)
# todo: do we need to add a slicer. does the data always come as numpy array with time series in axis=1. </s> slice = data_inputs[:, a:b]	_train_split if b > len(data_inputs): b = len(data_inputs) splitted_data_inputs.append(slice) return splitted_data_inputs
# todo: could just build up array of class/kind </s> proc = eventhandlerprocptr(func)	_install_event_handler target = carbon.GetWindowEventTarget(self._window) for event_class, event_kind in func._platform_event_data: self._carbon_event_handlers.append(proc) upp = carbon.NewEventHandlerUPP(proc)
# todo(ihrachys) adopt to v3 </s> raise self.skipexception('identity v2 admin not available')	test_associate_floating_ip_with_port_from_another_tenant def test_associate_floating_ip_with_port_from_another_tenant(self): if not CONF.identity_feature_enabled.api_v2_admin: body = self.admin_client.create_floatingip( floating_network_id=self.ext_net_id)
# todo: verify behavior </s> self.assert_received(self.debugger, [])	test_no_reason ])
# todo: symlink or whatever annex does, since annexes beneath </s> if not on_windows:	__call__ assert not listdir(subds_git_dir) rmdir(subds_git_dir) os.symlink(relpath(moved_git_dir, start=path), opj(path, ".git"))
# todo: do we still need to support ../shed_tools when running_from_source? </s> self.shed_tools_dir = self._in_data_dir('shed_tools')	_set_config_directories if not self.managed_config_dir: self.managed_config_dir = self._in_data_dir('config') log.debug("Configuration directory is %s", self.config_dir) log.debug("Data directory is %s", self.data_dir)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo (aron): add i18n by varying the language of the topic tree here </s> topictree = get_flat_topic_tree()	_construct_video_dict @staticmethod def _construct_video_dict(): video_title_dict = {} video_id_regex = re.compile('.*/v/(?P<entity_id>.*)/')
# todo(henry-nash): the test above uses list_projects_for_user </s> test_plan = {	test_list_projects_for_user_with_inherited_grants user_projects = self.assignment_api.list_projects_for_user(user1['id']) self.assertEqual(3, len(user_projects))
# todo: make this backend dependent. </s> return true	utilizes_gpu def utilizes_gpu(self):
# todo placeholder; implement </s> pass	init_process_group init_app(role="trainer", backend=BackendType.PROCESS_GROUP) init_process_group() # this worker joins the trainer process group
# todo: email request_user_id </s> curation[review_user_id] = user.get('_id')	setCuration curation[REVIEW_USER_ID] = user.get('_id') if enabled and oldStatus == REQUESTED and status == CONSTRUCTION: curation[REASON] = params.get(REASON, '') for doc in folder['access']['users'] + folder['access']['groups']:
# todo: this is seriously intensive and takes a _long_ time to run. </s> def reindex_questions():	reindex_questions from questions.models import Question index = "sumo"
# todo docstring args </s> send_arb_id = int_from_str_base(args.src)	service_discovery :param rcv_arb_id: Arbitration ID expected for incoming messages :return: rcv_arb_id = int_from_str_base(args.dst) can_wrap = CanActions(arb_id=send_arb_id)
# todo: need to add counter </s> return true	send_message delay.small_delay(self) if super(self.__class__, self).sendDirectItem('message', user_ids, text=text, thread=thread_id): self.logger.info("Message to {user_ids} wasn't sended".format(user_ids=user_ids)) return False
# todo: could cache the results of this for speed </s> existing = location.filter_by_type(domain, loc_type, parent)	get_by_name def get_by_name(loc_name, loc_type, parent): try: return [l for l in existing if l.name == loc_name][0]
# todo improve precision </s> warnings.warn("the cohen-gismalla schemes are only given in single-precision.")	__init__ def __init__(self): self.name = "CohenGismalla(2)" self.degree = 1
# forward all other methods. todo(l.zou): could use a proxy to automate these </s> return modified_grad	compute_gradients modified_grad.append((grad, var))
#set limit to 25 --> currently hardcoded --> todo: add a setting for this ? </s> subelement(root, "limit").text = "25"	addVideoNodesForTag Rule = SubElement(root, "rule", {"field":"tag","operator":"is"}) SubElement(Rule, "value").text = tagname Rule2 = SubElement(root, "rule", {"field":"inprogress","operator":"true"}) try:
# todo: cleaning of facts should eventually become part of taskresults instead of vars </s> vars_copy.update(namespace_facts(result['ansible_facts']))	_execute vars_copy.update(result['ansible_facts']) else: if C.INJECT_FACTS_AS_VARS: vars_copy.update(clean_facts(result['ansible_facts']))
# todo: let's see if we can find sane versioning for `latest` from upstream </s> if image_config['tag'] == 'latest':	normalize_app_version_of_chart_release image_config = release_data['config'].get('image') or {} if all(k in image_config for k in ('tag', 'repository')): app_version = f'{image_config["repository"]}:{image_config["tag"]}' else:
try: #todo: fix brodcast issue if different </s> if np.ndim(x) < 2:	feed_dict_builder X = [X for _i in net_inputs] elif len(net_inputs) > 1: raise ValueError("Multiple inputs but only one data " "feeded. Please verify number of "
# todo i/o </s> raise notimplementederror	get_all Returns all embeddings.
codecs.decode('5050827d08000000d00745b2cf451b00', 'hex'), # tcp random cmd_ack_ok todo: generate proper sequenced response </s> codecs.decode('5050827d08000000d00745b2cf451b00', 'hex'),  # tcp random cmd_ack_ok todo: generate proper sequenced response	test_tcp_live_connect_small codecs.decode('5050827d64000000d007a3159663130000000000000000000000000000000000070000000000000006000000000000005d020000000000000f0c0000000000000100000000000000b80b000010270000a0860100b20b00000927000043840100000000000000', 'hex'), #sizes codecs.decode('5050827d04020000dd05942c96631500f801000001000e0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003830380000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003832310000000000000000000000000000000000000000000300000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003833350000000000000000000000000000000000000000000400000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003833310000000000000000000000000000000000000000000500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003833320000000000000000000000000000000000000000000600000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003836000000000000000000000000000000000000000000000c0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000383432000000000000000000000000000000000000000000','hex'), #DATA directly(not ok) codecs.decode('5050827d10000000dc053b59d0983500f401ae4301000000f19449000000120c07130906', 'hex'), # tcp PREPARE_DATA 1011 codecs.decode('5050827df8030000f401ae43010000003131343030363400000000000000000000000000000000000f00120b1d0c3703', 'hex'), # reg_event!
# todo: mv this to network; cache directly on network. </s> if config.cache_potential_purviews:	_potential_purviews interest. if purviews is False: purviews = self.network.purview_cache[(direction, mechanism)] else:
# todo: does path make sense? or a separate table? </s> metadata_path=path,	index_dataset id=dataset_id, type=product_type, metadata=dataset_d )
# todo in python 2.7 and later, this should be a dict comprehension. </s> attributes = dict((column, getattr(instance, column))	DefaultSerializer foreign_key_columns = foreign_keys(model) columns = (c for c in columns if c not in foreign_key_columns) for column in columns) attributes = dict((k, (v() if callable(v) else v))
# todo: make this cache preparation configurable </s> return carefully_strip_whitespace(nodelist.render(context))	_handle_tag @cacheops.cached_as(qs, timeout=timeout, extra=full_extra) def _handle_tag():
# todo: untested </s> torch.save(self._model.state_dict(), str(path))	to_disk def to_disk(self, path):
@pytest.mark.skip()  # todo: fix this </s> self.assertisnotnone(response)	test_issue_560_success response = client.conversations_list(exclude_archived="true")
#todo: another idea: </s> for frame in slp_file.get_frames():	create_slp_pngs os.makedirs(base_slp_path) player_id = 4 #yellow frame_path = os.path.join(base_slp_path, "%06d_%03d_%02d.png" % (slp_file.file_id, frame.frame_id, player_id)) png = PNG(player_id, color_table, frame.get_picture_data())
# todo: make test method </s> pyexpat	test_expat except: return False return True
# todo: make it optional </s> torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config_args['gradient_clipping'])	train_one_epoch running_loss += (loss_batch - running_loss) / (batch_index + 1) loss.backward() self.optimizer.step() for metric in self.metrics.names:
# todo: make it really async. </s> self.database_name = database_name	__init__ self.server_name = server_name
# todo: determine test dependencies from plaso.dependencies. </s> file_content.append(u'python2_test_dependencies="python-mock";')	Write file_content.append(u'PYTHON2_DEPENDENCIES="{0:s}";'.format(dependencies)) file_content.append(u'') file_content.append(u'') dependencies = plaso.dependencies.GetDPKGDepends(exclude_version=True)
# todo(tr3buchet) - remove comment in multi-nic </s> ips = db.fixed_ip_get_all_by_instance(admin_context, instance['id'])	spawn VMHelper.create_vbd(self._session, vm_ref, vdi_ref, 0, True) admin_context = context.get_admin_context() for network in db.network_get_all_by_instance(admin_context, instance['id']):
# todo: figure out how extend stuff works, a bit confused again. </s> if not bins:	_cmap_features N = None else: offset = {'neither':-1, 'max':0, 'min':0, 'both':1} N = len(values) + offset[extend]
# todo: handle parser errors </s> root = xml.etree.elementtree.fromstring(data.read())	parse_asx def parse_asx(data): for entry in root.findall('entry'): yield entry.find('ref').attrib['href']
# todo: for backward compatibility only, remove if not used anymore </s> self.cs = cloudstack(**read_config())	_connect else:
# todo: support pairwise arg </s> out_cols = tuple(sorted(set(columns) | set(other.columns)))	generic raise ValueError("variable window rolling {} not supported yet.".format(func_name)) other = args[0] return signature(DataFrameType( (out_arr,)*len(out_cols), None, out_cols), *args)
kwargs.get('crypt', 'aes'),  # todo: use the same channel for crypt </s> opts['master_uri'],  # master id	__key opts['id'],          # minion ID
# todo fix bug for not identifying unused variables in nested exceptions see issue #4391 </s> try:	function2 1 / 0 except ZeroDivisionError as error: 1 / 0 except ZeroDivisionError as error:
# todo: implement this </s> return object.__getattribute__(self, name)	__getattr__ return getattr(self._chained, name)
# todo:liberate - move this to a more generalized tag enhancement package? </s> return false	allows_deletion_by return True
# todo remove this custom equality testing code when </s> self.assertequal(obs, exp)	test_fasta_to_sequence_collection_and_alignment for fp in fps: obs = reader_fn(fp, **kwargs) for o, e in zip(obs, exp): self.assertTrue(o.equals(e))
# todo: this needs rethinking, as currently we can only express </s> matches = []	concretize s.external_path = get_path_from_module(s.external_module) self._mark_concrete() for x in self.traverse(): for conflict_spec, when_list in x.package_class.conflicts.items():
# todo(yanase): implement maximization. </s> if _direction == structs.studytask.maximize:	optimize else: raise ValueError('Please set either \'minimize\' or \'maximize\' to direction.') raise ValueError( 'Optimization direction of study {} is set to `MAXIMIZE`. '
# todo: remove in 1.3 </s> msg = "in version 1.3 onwards, subsample=2e5 will be used by default."	test_kbinsdiscretizer_subsample_values kbd_with_subsampling.fit(X) else: with pytest.warns(FutureWarning, match=msg): kbd_default.fit(X)
# todo: i don't like this interface. is there a standard python one? pubsub? </s> def _handle_source_event(self, event_name, source_file):	_handle_source_event file = ImageSpecCacheFile(self, source_file) self.image_cache_strategy.invoke_callback('on_%s' % event_name, file)
if sys.version_info[:2] < (3, 5): # todo </s> sum(s.name for s in student)	test49 def test49(self):
# todo test cases </s> return self._promise	execute self._thread.start()
# todo: the absolute and fixed boxes in the floats must be </s> waiting_floats.append(child)	split_inline_box float_width = shrink_to_fit(document, child, containing_block) if float_width > max_x - position_x: else: child = float_layout(
# todo: remove in cacheops 3.0 </s> if hasattr(settings, 'cacheops_profiles'):	prepare_profiles def prepare_profiles(): Prepares a dict 'app.model' -> profile, for use in model_profile() profiles.update(settings.CACHEOPS_PROFILES) if hasattr(settings, 'CACHEOPS_DEFAULTS'):
# todo not implemented yet </s> return	move_current_view_to_very_top Currently only supports 2 row or 2 column layouts. if self.window.num_groups() > 2: if self.window.num_groups() < 2: return
# todo: remove this deprecated function in release 1.0 </s> warnings.warn(	register_events def register_events(scheduler, result_storage=None): "'register_events' is deprecated since version 0.4.0. Please remove all references from your code.", DeprecationWarning,
# todo: find a better way to return errors... </s> for filename in archive.namelist():	uploadarchive import zipfile archive = zipfile.ZipFile(cStringIO.StringIO(archivecontents), 'r') if not filename.endswith(os.extsep + self.fileext): print "error adding %s: not a %s file" % (filename, os.extsep + self.fileext)
# todo ask for the correct attributes e.g state and private_ips </s> for machine in vb_list_machines(attributes=[	list_nodes ) machines = {} ("id", str), ("name", str),
# todo make this check if writeable </s> os.path.exists(log)	uninstall if log: try: except IOError: raise IOError("'%s' is not writeable" % log)
# todo: add information on optional / default components </s> if self.type in types_basic + types_ext:	get_proto def get_proto(self): returns the prototype of the object return self.TYPE elif self.TYPE in (TYPE_SEQ_OF, TYPE_SET_OF):
return {}  # todo return none, somehow </s> else:	result results = self._content["results"] except KeyError: try: return results[index]
# todo remove below workaround for double actions </s> if self._counter == 1:	OnDeleteCells def OnDeleteCells(self, event=None): if self._dcells == (self.selection.topleft, self.selection.bottomright): self._counter = 0
# todo: print line number.  pass a span_id into this function? </s> util.error('test: %s', e.usererrorstring())	Test b = bool_ev.Eval(bool_node) except util.FatalRuntimeError as e: return 2  # 1 means 'false', and this usage error is like a parse error. status = 0 if b else 1
# todo: use valid_episodes.mask for mean </s> kls.append(kl_divergence(pi, pi_old).mean())	mean_kl raise NotImplementedError('Only `Categorical` and `Normal` ' 'policies are valid policies.') return torch.mean(torch.cat(kls, dim=0))
# todo(dtantsur): backwards compability hack, remove in the v release </s> if ramdisk_params.get("ipa-api-url"):	prepare_ramdisk pxe_options = pxe_utils.build_pxe_config_options( task, pxe_info, ipxe_enabled=True, ramdisk_params=ramdisk_params) pxe_options["ipa-api-url"] = ramdisk_params["ipa-api-url"] pxe_config_template = deploy_utils.get_pxe_config_template(node)
#todo: show examples </s> :rtype: queryset	query Same end result as filter, but uses the new comparator style args ie: Model.column == val clone = copy.deepcopy(self) for operator in args:
# todo are there more exceptions besides timeout? </s> try:	MTProtoSender Besides `connect`, only this method ever receives data. while self._user_connected: async with self._recv_lock: body = await self._connection.recv()
# todo: skipped due to gh-4436 </s> yield skip_if_on_windows(skip_ssh(_test_initremote_rewrite)), 'datalad-test'	test_initremote_rewrite def test_initremote_rewrite(): yield _test_initremote_rewrite, None
# todo: losing precision on double types </s> return float(value)	unpack_cli_arg return int(value) elif parameter.type == 'float' or parameter.type == 'double': elif parameter.type == 'structure' or parameter.type == 'map': if value[0] == '{':
# todo chceck correct shapes for integration </s> geo = vg1	get_fargs if diff_var is None: if self.mode == 'grad_state': bf_t = vg1.bf.transpose((0, 1, 3, 2)) val_qp = dfun(self.get(var2, 'val')[..., 0])
engine.execute(alala.table_data.insert().values(datas)) # todo chunks?? </s> engine.execute(alala.table_hash.delete())	wrapper datas = func(key) if len(datas) > 0: engine.execute(alala.table_hash.insert().values([{'value': h}])) return datas
# todo: move to profile callbacks </s> self.session.logger.log('message change', contact.status, message, account)	_handle_action_set_message contact.account, contact.status, contact.nick, message, contact.picture) self.session.add_event(Event.EVENT_MESSAGE_CHANGE_SUCCEED, message)
## todo: decode </s> folder_path = unicode(	BSA if self.bsa_header.has_names_for_folders(): name_size = struct.unpack('B', bsa_file.read(1))[0] struct.unpack('%ds' % (name_size - 1), bsa_file.read(name_size - 1))[0],
# todo remove? </s> if isinstance(self.var_args, tuple) or self.var_args.parent is none:	iter_content self._evaluator.recursion_detector.pop_stmt() items += get_iterator_types([typ]) return []  # generated var_args should not be checked for arrays module = self.var_args.argument_node.get_parent_until()
# todo: unify handling of text objects in one function. perhaps add state.args to merge with vi_cmd_data['motion']['args'] </s> def vi_exclusive_text_object(vi_cmd_data):	vi_exclusive_text_object vi_cmd_data['motion']['command'] = '_vi_select_text_object' vi_cmd_data['motion']['args'] = {'text_object': vi_cmd_data['user_input'], 'mode': vi_cmd_data['mode'], 'count': vi_cmd_data['count'], 'inclusive': True}
pass # todo </s> def lookup(self, uri):	lookup
# todo: non-numeric columns should be ignored automatically </s> def test_impl(n):	test_max1 def test_max1(self): df = pd.DataFrame({'A': np.arange(n)+1.0, 'B': np.arange(n)+1}) return df.max()
# todo(ls): revert this loop to "yield from" </s> for __x in self.__context__.format(chain=chain): yield __x	format elif (self.__context__ is not None and not self.__suppress_context__): yield _context_message if self.exc_traceback is not None:
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
#todo (nmakhotkin) this should be refactored later </s> try:	get LOG.debug("Fetch execution [workbook_name=%s, id=%s]" % (workbook_name, id)) values = db_api.execution_get(workbook_name, id) return Execution.from_dict(values)
# todo - use new error message api! ts </s> self.showerrormessage(m.message(str(mymessage)))	completed except Exception, e:  # pylint: disable=W0703 myMessage = getExceptionWithStacktrace(e, theHtml=True) else: self.showStaticMessage(m.Message(str(myReport)))
# todo: make sure package names can't be changed to look like package ids? </s> if pkg == none:	show if pkg == None: pkg = model.Package.by_name(id) response.status_int = 404 return ''
# todo ... </s> buildcontrolonelinetext(control)	buildControlList def buildControlList(control): return control
#todo: the following functions are used sage.combinat.combination and </s> def rank(comb, n, check=true):	rank Return the rank of ``comb`` in the subsets of ``range(n)`` of size ``k`` where ``k`` is the length of ``comb``.
## todo: fix the unicode issue mentioned in </s> if sys.version_info[:2] <= (2, 7):  ## python2	pronunciation if json_obj: Refer : http://stackoverflow.com/questions/18337407/saving-utf-8-texts-in-json-dumps-as-utf8-not-as-u-escape-sequence return json_obj else:   ## python3
# todo: probably better to work out why they are occurring, but imo the </s> caplog.set_level(logging.warning)	test_sort_best_candidate__all_yanked def test_sort_best_candidate__all_yanked(self, caplog, monkeypatch): Test all candidates yanked. candidates = [ make_mock_candidate('1.0', yanked_reason='bad metadata #1'),
except attributeerror:  # todo: this logic belongs somewhere - anywhere - else. </s> last_seen = str(node.last_seen)  # in case it's the constant never_seen	SQLiteForgetfulNodeStorage try: last_seen = node.last_seen.iso8601() fleet_icon = node.fleet_state_nickname_metadata if fleet_icon is UNKNOWN_FLEET_STATE:
report_config = {}  # todo port to fooddata.from_request </s> report_config.update(	report_config @property def report_config(self): gap_type=self.request.GET.get('gap_type') or '', recall_status=self.request.GET.get('recall_status') or '',
# todo: perhaps some code belongs here to enforce rules about which </s> if m.session == "":	handlesession def handlesession(self, m, r): if r.failure.IsInitialized(): return r.session = random() self.id = r.session
# todo: no testpath exercises this code... </s> log.debug("starting thread...")	AnimThread self.do_run = do_run def run(self): self.do_run() log.debug("Thread Complete")
1  # todo: fill in identifier </s> )	profile_transfer amount, target, result.wait() profiling.print_all_threads()
# todo: this loop is pretty slow .. (parellize) </s> for iky in range(self.nky):	getJ Jtv_temp0 = np.zeros((m.size, rx.nD), dtype=float) Jtv = np.zeros((m.size, rx.nD), dtype=float) u_src = f[src, self._solutionType, iky] ky = self.kys[iky]
# @todo: use real lightness from hsv or lab color model </s> return sum([	is_dark def is_dark(color_text): hex_to_int(channel_text) for channel_text in color_list_from_hex(color_text)
# todo write bin data </s> output['data'] =  responsedatahelper.data2str(response.data)	resp2dict output['data'] = response.data.decode('utf-8') else: except Exception as e: output['data'] = ResponseDataHelper.data2Str(response.data)
if len(words) + len(tokens) <= self.args.maxlength:  # todo: filter don't happen here </s> tempwords = []	filterFromFull i = len(sentencesToken)-1 - i tokens = nltk.word_tokenize(sentencesToken[i]) for token in tokens: tempWords.append(self.getWordId(token))  # Create the vocabulary and the training sentences
# todo(nnorwitz): other warnings to add: </s> self._findunusedwarnings()	_FindHeaderWarnings def _FindHeaderWarnings(self):
pass  # todo </s> def create(self, name):	create
# todo for pytorch 2.0.4, need to set dim=1 for log_softmax or use softmax then take log </s> loss = self.logsoftmax(output.view(batch, self.num_labels * max_len, max_len))	BiRecurrentConvBiAffine log_mask = torch.log(mask) output = output + log_mask.view(batch, 1, max_len, 1) + log_mask.view(batch, 1, 1, max_len) loss = loss.view(batch, self.num_labels, max_len, max_len) if mask is not None:
form_data.history.append(operation)  # todo: should this show in form history tab? it doesn't </s> form_data.save()	modify_attachment_xml_and_metadata operation = XFormOperation(user_id=SYSTEM_USER_ID, date=datetime.utcnow(), operation='Scrub username for GDPR compliance.')
# todo: activate this code if we have limits at webmail level </s> if not self.limiter.test(self.rate,"client-ip",clientip):	check def check(self,clientip): raise RateLimitExceeded()
# todo: compare this with compress_code_broken and fix the latter. </s> return best_len, block_offset	_find_repeatable_block block_offset = pos - best_i
#todo : multi parent intelligence </s> id = par.get_id()	add_parent def add_parent(self, par): self.parents.append(id)
# todo: arrange </s> repo = self.remote.new_repo(self.token)	test_create_repo def test_create_repo(self): Test: create/edit a repo object self.assertTrue(self.remote.modify_repo(repo, "name", "testrepo0", self.token)) self.assertTrue(self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token))
# ---- todo: the following should be removed in milestone:0.11 </s> match = img_re.search(path)	_format_link def _format_link(self, formatter, ns, path, label): if formatter.flavor != 'oneliner' and match: return html.IMG(src=formatter.href.file(path, format='raw'),
# todo: find the common ground of these, and make it an expression method. </s> if operand.isexpressionmakesequence():	extractSideEffects def extractSideEffects( self ): operand = self.getOperand() return self.getOperand().extractSideEffects() if operand.isExpressionMakeDict():
# todo fixme result type? </s> if em.epochs is none:	make @classmethod def make(cls, em) -> Iterator['Emfit']: logger.error('%s (on %s) got None in epochs! ignoring', em.sid, em.date) return
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_movaps res = ctx_init["xmm1"] x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo: may be check whether it fits to tracking branch </s> raise valueerror("refspec specified without a remote. (%s)" %	push if remote is None: if refspec is not None: refspec) if all:
# todo(slaweq): self.has_neutron = true should be added, the mock of </s> self.cloud.create_server(	test_create_server_network_with_no_nics Verify that if 'network' is supplied, and 'nics' is not, that we attempt to get the network for the server. 'server-name', dict(id='image-id'), dict(id='flavor-id'), network='network-name')
#create and insert todo on remote site </s> todo = frappe.get_doc(dict(doctype='todo', description=description, assigned_by='administrator'))	insert_into_master def insert_into_master(self, master, description): return master.insert(todo)
# todo: verify </s> mass = elem.mass()	build_Mgg etype = elem.type if etype in ['CROD', 'CONROD', 'CTUBE']: nid1, nid2 = elem.nodes i1 = dof_map[(nid1, 1)]
# todo: other types than can have series inside: list, set, etc. </s> return typ	if_series_to_array_type if isinstance(typ, (types.Tuple, types.UniTuple)): return types.Tuple(list(map(if_series_to_array_type, typ.types)))
# todo: ... </s> prediction: array-like	classify_output ---------- target: Array-like Returns -------
raise exceptions.mpdnotimplemented  # todo </s> underscore, dash, dot and colon.	subscribe already. The name may consist of alphanumeric ASCII characters plus
# todo: repackage timeout? </s> return portalocker.lock(	_exclusive_lock def _exclusive_lock(self): self._lock_filename, timeout=self._lock_timeout,
# remove all ads (todo: ad specific div classes probably change over time, look into a more generic method) </s> main_divs = soup.find('div', {'id': 'main'})	cook def cook(soup, user_agent, nojs=False, dark_mode=False): if main_divs is not None: ad_divs = main_divs.findAll('div', {'class': AD_CLASS}, recursive=False)
# todo: write tests </s> from any tag with (at minimum) either @key.pname or @key.sig attributes, make a	_keySigFromAttrs def _keySigFromAttrs(elem): :class:`KeySignature` or :class:`Key`, as possible. :param :class:`~xml.etree.ElementTree.Element` elem: An :class:`Element` with either the
self.current_height = self.current_height * 2 #todo </s> return nn.sequential(*layers)	layer_resize_conv self.current_channels = channels self.current_width = self.current_width * 2 #TODO
# todo: add asserts and type checkings </s> if len(tensor.shape) == 2:	tensor_to_image def tensor_to_image(tensor): in the GPU, it will be copied back to CPU. tensor = torch.unsqueeze(tensor, dim=0) tensor = tensor.permute(1, 2, 0).contiguous()
# fix: https://github.com/certtools/intelmq/issues/1720 # todo: find better fix </s> if '/' in value:	validate_ip if value == '0.0.0.0': return None return None if harmonization.IPAddress.is_valid(value, sanitize=True):
## \todo there should really be a method to map from plug to parameter. </s> parameter = plug.node().parameterhandler().parameter()	__plugDescription def __plugDescription( plug ) : for name in plug.relativeName( plug.node() ).split( "." )[1:] : if not isinstance( parameter, IECore.CompoundParameter ) :
# todo: in case of recursive object, this will break python </s> if self.type in types_basic + types_ext:	_get_proto_old def _get_proto_old(self): return self.TYPE elif self.TYPE in (TYPE_SEQ_OF, TYPE_SET_OF):
# todo: check that tag key matches section start tag key. </s> return parse_tree, template[start_index:match_index], end_index	parse leading_whitespace = '' if tag_type == '/': index = self._handle_tag_type(template, parse_tree, tag_type, tag_key, leading_whitespace, start_index, match_index, end_index) parse_tree.append(template[index:])
# todo handle 4 types of transition exceptions </s> pass	create_next pass except Exception: except Exception: pass
# todo(b/171992041): remove the string-typed metric key branch once v1 </s> if metric_value.whichoneof('type') == 'double_value':	convert_slice_metrics_to_proto confidence_interval=confidence_interval) else: metric_value.bounded_value.value.value = unsampled_value metric_value.bounded_value.lower_bound.value = lower_bound
# todo: this is highly inefficient if other properties depending on the </s> if self._stft is none:	stft @property def stft(self): self.compute_stft(stft=True) return self._stft
pass  # todo </s> def delete(self, uri):	delete
# todo: use different flag than .reentrant </s> pos = colorsorter._transform_point(pos)	schedule_sphere color, pos, radius, ColorSorter._debug_transforms() ##### if ColorSorter._parent_csdl and ColorSorter._parent_csdl.reentrant: if drawing_globals.use_c_renderer and ColorSorter.sorting: if len(color) == 3:
# c2 = (219, 220, 200, 255)  # todo: not used? </s> _draw_desert_pattern(pixels, x, y, c)	_draw_hot_desert def _draw_hot_desert(pixels, x, y, w, h): c = (72, 72, 53, 255)
# todo(hrybacki): move to framework.utils.rapply once @sam's pr#4027 is merged. </s> from api.base.serializers import _rapply	update_draft_registration def update_draft_registration(auth, node, draft, *args, **kwargs): data = request.get_json() schema_data = _rapply(data.get('schema_data', {}), sanitize.strip_html) schema_name = data.get('schema_name')
# todo: raise more specific exception </s> raise exception('plugin %s is not loaded' % name)	remove_plugin name = plugin.name if name not in self._plugins: try: for func in itertools.chain(callables, jobs, shutdowns):
# todo extend it to output into multidimensional space </s> layers.append(1)	__init__ name="targets") mlp_input = tf.concat([enc.encoded for enc in encoders], 1) mlp = multilayer_projection( mlp_input, layers, activation=self.activation_fn,
# todo: this is ambiguous: it's not clear whether we correctly </s> eq_(elasticutils.s(question).count(), original_count + 1)	test_added answer.save() self.refresh()
# todo: use denormalized site_domain field </s> return self.site.domain in self.url	is_subdomain def is_subdomain(self):
# todo: estimate fees </s> if input_arr is none:	create_transaction if not utxos: return None selected_utxos = self._select_inputs(total_amount, utxos) from pprint import pprint
# todo in the future, use http://schacon.github.io/git/git-ls-remote.html to validate the url string </s> run('git clone --depth=100 --quiet --branch=master %(clone)s .' % project, cwd = _in, log=log)	project_git_sync log.write('Copying Git Repository\n', prefix = 'Header: ') try: except: if project.clone[:7] == "file://":
# todo: provide some feedback on what happens in git </s> nox_dir = path(click.get_app_dir('nox', force_posix=true))	get_nixpkgs def get_nixpkgs(): if not nox_dir.exists(): nox_dir.mkdir()
# todo: arrange </s> repo = self.remote.new_repo(self.token)	test_create_repo def test_create_repo(self): Test: create/edit a repo object self.assertTrue(self.remote.modify_repo(repo, "name", "testrepo0", self.token)) self.assertTrue(self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token))
# todo: with success_re etc </s> def fake_load(self):	test_HTMLFormAuthenticator_httpretty_2 password="{password}", submit="CustomLogin")) self._cookies_db = {'example.com': dict(some_site_id='idsomething', expires='1449623300')} with patch.object(CookiesDB, '_load', fake_load):
# todo: verify learning rule contents </s> self.asserttrue(fib_route_replies)	test_host_fib_route 'ipv4_dst': '10.0.0.4', 'echo_request_data': bytes('A'*8, encoding='UTF-8')}) self.assertFalse(self.packet_outs_from_flows(fib_route_replies))
# todo: write tests </s> from any tag with the @trans.diat (and optionally the @trans.semi) attributes, make an	_transpositionFromAttrs def _transpositionFromAttrs(elem): :class:`Interval` that represents the interval of transposition from written to concert pitch. :param :class:`~xml.etree.ElementTree.Element` elem: An :class:`Element` with the @trans.diat
# todo: need to delete all queues </s> except:	kill_services try: s.kill() pass del running_services[:]
# todo better logging </s> try:	connect def connect(self): ssh = SSHClient() ssh.set_missing_host_key_policy(AutoAddPolicy())
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_interact_arguments @classmethod def add_interact_arguments(cls, parser): p_convert = p_operation.add_parser( "convert", help="convert VGM to PCM using OPL hardware")
# todo xxx </s> labels = tf.ones_like(logits)	create_output dtype='float32') logits = self.forward_pass(shared_resources, question) self.loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels) return [self.loss, logits]
# todo(dcramer): this needs to be a get_or_create pattern </s> repo = repository.query.filter(	post ) provider_config[option] = value Repository.url == args.repository, ).first()
# todo(mordred): this needs to be mutex protected </s> self._internal_networks = _internal_networks	get_internal_networks if 'provider:network_type' not in network: _internal_networks.append(network) self._internal_network_stamp = True return self._internal_networks
#todo?# self.asserttrue(greps(err, "unit zzz.service not for --user mode")) </s> logg.info("== 'restart' shall start a service that not is-active")	bad_usermode_simple_service_functions logg.info(" %s =>%s \n%s\n%s", cmd, end, err, out) self.assertEqual(end, 1) #TODO? cmd = "docker exec {testname} {systemctl} restart zzz.service -vvvv" out, err, end = output3(cmd.format(**locals()))
# todo(mattjj): remove this special case, used for debugging on cpu </s> if xb.get_replica_count() == 1:	unshard_array def unshard_array(x): dims = c.GetShape(x).dimensions() return c.Reshape(x, None, (1,) + tuple(dims))
# todo: use mock to patch dict </s> pass	testCanDiscriminateWindowSettingsFromViewSettings def testCanDiscriminateWindowSettingsFromViewSettings(self):
# todo: refactor accordingly when v3 websocket api is released </s> output["results"].update({	BittrexAPIOrderBookDataSource if _is_snapshot(msg): output["results"] = _decode_message(msg["R"]) "M": f"{output['results']['M'].split('-')[1]}-{output['results']['M'].split('-')[0]}" })
# todo do something with temp </s> self._remove_substates_from_subhmms()	HSMMSubHMMStates super(HSMMSubHMMStates,self).clear_caches() def resample(self,temp=None): super(HSMMSubHMMStates,self).resample() # resamples superstates self._resample_substates()
# todo: remove in 0.26 when grid_scores_ is deprecated </s> def test_graphical_lasso_cv_grid_scores_and_cv_alphas_deprecated():	test_graphical_lasso_cv_grid_scores_and_cv_alphas_deprecated splits = 4 n_alphas = 5
# todo: add this back </s> self.address = manager_address	__init__ to_normalized_address(manager_address), ) self.proxy = proxy self.client = jsonrpc_client
# todo debug </s> print "check of sensor alerts with rules now"	run sensorAlertProcess.start() sensorAlertsToHandle.remove(sensorAlertToHandle) for sensorAlertToHandle in list(sensorAlertsToHandleWithRules): print sensorAlertToHandle
# todo(dspasovski): fix this. </s> raise skiptest	TestIndexLanding @mock_es def test_good_cat(self): r = self.client.get(self.url) eq_(r.status_code, 200)
# todo - might need to click on transportation mode if url doesn't work </s> attach_img_btn.send_keys(picture_location)           # get current script path + img_path	send_picture time.sleep(1) attach_img_btn = self.browser.find_element_by_xpath(attach_type_xpath) time.sleep(1) send_btn = self.browser.find_element_by_xpath(send_file_xpath)
# todo this was already broken, just making it obvious </s> return 0	state_time def state_time(self):
# todo: check that the birth date is not in the future </s> except valueerror, e:	is_valid try: birth_date = get_birth_date(number) return False return calc_check_digit(number[:-1]) == number[-1]
pass  # todo </s> self._current_command = []	__init__ self._device = None
# todo: add a better throttling mechanism </s> if 'sleep' in kwargs:	get_posts _scraper.requests_kwargs['timeout'] = kwargs.pop('timeout', DEFAULT_REQUESTS_TIMEOUT) options = kwargs.setdefault('options', set()) warnings.warn( "The sleep parameter has been removed, it won't have any effect.", stacklevel=2
1  # todo: fill in identifier </s> )	profile_transfer amount, target, result.wait() profiling.print_all_threads()
# todo: add this back </s> self.address = manager_address	__init__ to_normalized_address(manager_address), ) self.proxy = proxy self.client = jsonrpc_client
# todo: only if successful </s> self.is_migrating = false # this should really be true until evaluate_submission tasks are all the way completed	do_phase_migration current_phase.is_migrated = True current_phase.save() self.last_phase_migration = current_phase.phasenumber self.save()
# todo: docstrings! </s> i = 0	plot_missing_samples def plot_missing_samples(self, ax=None, fig=None): n = len(self.appliances) + len(self.mains) colours = [plt.cm.Blues(c) for c in np.linspace(0.3, 0.9, n)]
# todo(b/132329316) remove when `xla.compile` allows tf.device(tpu). </s> return unroll(core, sequence, state)	forward state = core.initial_state(input_shape[0])
# todo: find a better solution than this: </s> and getattr(self.g_pool, "display_mode", "") != "algorithm"	gl_display frame.yuv_buffer is not None and TJSAMP(frame.yuv_subsampling) == TJSAMP.TJSAMP_422 ): self.g_pool.image_tex.update_from_yuv_buffer(
# todo: unit test </s> return none	get_relative_template_location return os.path.split(view.template_path)
# todo: clocksignal, resetsignal, memory </s> raise notimplementederror	eval postcommit) else:
transaction_manager.retry_attempt_count = 3  # todo: hardcoded for now </s> zope.sqlalchemy.register(dbsession, transaction_manager=transaction_manager)	create_session The attached transaction manager takes care of committing the transaction at the end of the request. dbsession = db_session_maker() return dbsession
with prepare_file(["#todo this is todo"], none) as (lines, filename): </s> bear = "keywordbear"	test_find_issues def test_find_issues(self): retval, output = execute_coala(coala_json.main, "coala-json", "-c", os.devnull, "-S",
# todo(jogo): make the following doctests pass: </s> if pep8.noqa(physical_line):	hacking_import_rules H303: from os.path import * H304: from .compute import rpcapi return def is_module_for_sure(mod, search_path=sys.path):
# todo: extend to string variables </s> arg1, arg2, args3 = sig.args	h5_size @lower_builtin(pio_api.h5size, types.int32, types.Const, types.int32) def h5_size(context, builder, sig, args): val2 = context.insert_const_string(builder.module, arg2.value) fnty = lir.FunctionType(lir.IntType(64), [lir.IntType(32), lir.IntType(8).as_pointer(), lir.IntType(32)])
# todo implement extra options </s> py_options = read_check_options(options)	read_get_separators def read_get_separators(options, name): record_separators = py_options["RecordSeparators"] word_separators = py_options["WordSeparators"]
# todo: askr, undocumented! </s> def ledsetbuttonlayoutsession( self ):	LedSetButtonLayoutSession self.LedSetLayout( 0 )
# todo: finish this. </s> def mkdir(self, path, mode):	mkdir pass
# todo: config option? </s> lgr.info(	update_submodule if subrepo.get_merge_base( [subbranch_hexsha, detached_hexsha]) == detached_hexsha: "Submodule HEAD got detached. Resetting branch %s to point " "to %s. Original location was %s",
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_depth_string def test_fail_depth_string(self): ``depth`` is a string.
self.setup() # todo: perhaps, remove this to pass path in context </s> current_ids = self.hasher.hash(	transform :param data_inputs: the data input to transform :return: transformed data inputs current_ids=None, hyperparameters=self.hyperparams,
# todo: askr, undocumented! </s> def ledsetbuttonlayoutsession( self ):	LedSetButtonLayoutSession self.LedSetLayout( 0 )
# todo: add permissions </s> except exception as e:	MusicBot try: entries_added = await player.playlist.async_process_youtube_playlist(playlist_url, channel=channel, author=author) traceback.print_exc() raise CommandError('Error handling playlist %s queuing.' % playlist_url)
# todo: handle sigchld to avoid wasting time in polling </s> time.sleep(pause)	wait_child else: pause = 0.1 status = os.waitpid(pid, os.WNOHANG)[1] else:
# todo: switch to: </s> self.change_track(tl_track)	next tl_track = self.core.tracklist.next_track(self.current_tl_track) if tl_track: else: self.stop(clear_current_track=True)
# todo: only handle events that are new. </s> for e in events:	_watch_slack_rtm events = self.client.rtm_read() if len(events) > 0: self.handle_incoming_event(e) current_poll_count += 1
# todo(shoyer): test fails on tpu </s> if jtu.device_under_test() == "tpu":	test_custom_linear_solve_lu def test_custom_linear_solve_lu(self): raise SkipTest("Test fails on TPU") def linear_solve(a, b):
# todo.hartikainen: remove this </s> kwargs.pop('ignore_input', none)	convnet_preprocessor_template elif data_format == 'channels_first': C, H, W = image_size kwargs.pop('hidden_layer_sizes', None) def _fn(x):
time.sleep(1.0)  # todo properly wait until sequencer is online </s> atexit.register(stop_software_midi_synth, proc.pid)	start_timidity print_err('steam-dos: Starting MIDI client (pid: {0})'.format(proc.pid)) print_err('steam-dos: Using soundfont: {0}'.format(sfont))
# todo: once we allow filtering, unit.store.units has to be a qs </s> profile = get_profile(request.user)	get_view_units_for if not limit: limit = (profile.get_unit_rows() - 1) / 2 unit_rows = profile.get_unit_rows() preceding = current_unit.store.units.filter(index__lt=current_unit.index).count()
##todo     temp = x </s> yield (x, y, join(dirpath, f))	_iterate_regionfiles x = int(p[1]) y = int(p[2])
if testname == "tests5": continue # todo </s> tests = testdata(filename, ("data", "errors", "document-fragment",	buildTestSuite for filename in html5lib_test_files('tree-construction'): testName = os.path.basename(filename).replace(".dat","") "document")) for index, test in enumerate(tests):
# todo/rsi for when have nonlocal </s> self.assertraises(runtimeerror, f, none)	test_obscure_super_errors super()
# todo: add axis parameter </s> return dataframe(self._internal.copy(sdf=sdf))	shift sdf = sdf.fillna(fill_value)
# todo(b/160795287): deprecate estimator based executor. </s> fn_args_dict = fn_args.get_dict()	run_fn fn_args: Holds args used to train the model as name/value pairs. schema = io_utils.parse_pbtxt_file(fn_args.schema_file, schema_pb2.Schema()) fn_args_dict['serving_model_dir'] = os.path.join(fn_args.model_run_dir, path_utils.SERVING_MODEL_DIR)
# todo: move install_time away from app_setting </s> app_setting(app_id, 'update_time', now)	app_upgrade else: now = int(time.time()) status['upgraded_at'] = now with open(app_setting_path + '/status.json', 'w+') as f:
# todo scope the cleanup to the current project - https://github.com/lyft/cartography/issues/381 </s> cleanup_gcp_instances(neo4j_session, common_job_parameters)	sync_gcp_instances instance_list = transform_gcp_instances(instance_responses) load_gcp_instances(neo4j_session, instance_list, gcp_update_tag)
# todo: import that elsewhere </s> from . import _control	getConstant def getConstant(key): _control.execQueue.socket.pumpInfoSocket() return elements.get(key)
#todo : multi parent intelligence </s> id = par.get_id()	add_parent def add_parent(self, par): self.parents.append(id)
# todo: fix with stubber / before send event </s> with mock.patch('botocore.endpoint.endpoint._send') as _send:	test_get_export 'accepts': 'application/yaml' } _send.return_value = mock.Mock( status_code=200, headers={}, content=b'{}')
"""the arguments given to a command.""" #todo elaborate </s> s.admin = origin.nick in self.config.admins	__new__ See Python ``re_`` documentation for details.""" s.args = args True if the nick which triggered the command is in jenni's admin list as defined in the config file.
# todo: should we enable auto-retry, </s> producer.publish(msg, exchange=exchange, **kwargs)	__call__ exchange = queue.exchange with self.producer as producer:
# todo: set language preference from config </s> langs = []	_readSettings def _readSettings(): nam = _accessmanager lang = locale.getdefaultlocale()[0] if lang:
pass # todo </s> def _close(self):	_close @register(r'^close$')
# todo is this serious enough to raise a canerror exception? </s> if res != 0:	set_filters filter_struct, len(filter_struct) ) log.error('Setting filters failed: ' + str(res))
# todo: convert to a python xml thing </s> xml=unicode(toc.tostring())	on_actionRender_triggered toc=self.pdf.document.toc() if toc: soup=BeautifulSoup(xml) self.lineMarks={}
# todo(hartikainen): this should get the logdir some other way than </s> summary_dir = logger._snapshot_dir	__init__ self._init_critic_update() self._init_target_ops() self.summary_writer = tf.summary.FileWriter( summary_dir, self._sess.graph)
# xxx todo </s> return []	fclex def fclex(info):
pass  # todo </s> "draw circle from start to end."	circle def circle(start, end):
# todo pydocs </s> if bq_type == 'integer' or bq_type == 'timestamp':	_bq_cast def _bq_cast(string_field, bq_type): return int(string_field) elif bq_type == 'FLOAT':
# todo: create a remote print interface for objects which displays them in a </s> if obj is not none:	print ): return self.print() print(obj) else:
# todo: batch </s> self._send(rest.request(self._graph_db, "post",	LabelSet def add(self, *labels): if self.__uri__: self.__uri__, list(labels))) self.refresh()
# todo(nakago): check why tolerance is high </s> def test_backward_cpu(model, data):	test_backward_cpu atom_data, adj_data, y_grad = data gradient_check.check_backward(model, (atom_data, adj_data), y_grad,
# todo: must be implemented </s> pass	range_from_chapters def range_from_chapters(crawler, times=0):
# todo: https://github.com/turicas/brasil.io/issues/210 </s> return []	validate_historical_data If any invalid data, it'll raise a SpreadsheetValidationErrors If valid data, returns a list with eventual warning messages
# todo: [debug] change this </s> pass	system_upload stop_system_importer_file_csv = run_check_config_attributes(model) if stop_system_importer_file_csv_run: system_handler(request, True) else:
# todo: use triple factory </s> rotate.forward_owa(torch.zeros(16, 3, dtype=torch.long))	test_rotate rotate = RotatE(triples_factory=self.factory) self.assertIsNotNone(rotate) rotate.forward_cwa(torch.zeros(16, 2, dtype=torch.long)) rotate.forward_inverse_cwa(torch.zeros(16, 2, dtype=torch.long))
# todo: there's a race with the initial "output" event. </s> with self.vsc.wait_for_event('output'):	test_launch addr = (None, 8888) with self.vsc.start(addr): pass with self.vsc.wait_for_event('initialized'):
# todo: how do we handle this? </s> msg = none	put_byte msg = msg(pos=value) else: if msg: self._messages.append(msg)
# todo(b/135607227): add device scope automatically in keras training loop </s> no_dist_strat_device = tf.device('/device:gpu:0')	run validation_data = None if not strategy and flags_obj.explicit_gpu_placement: no_dist_strat_device.__enter__() history = model.fit(train_input_dataset,
# todo: finnish luks+zfs </s> self.zfs_options["encrypt_swap"] = false	translate_ui entry.set_sensitive(self.zfs_options["encrypt_disk"]) btn = self.ui.get_object('encrypt_swap_btn') btn.set_sensitive(False) btn.set_active(self.zfs_options["encrypt_swap"])
# todo: replace with below line when numba supports np.isin in nopython mode </s> values = str_list_to_array(list(values))	hpat_pandas_series_isin if isinstance(values.dtype, (types.UnicodeType, types.StringLiteral)): def hpat_pandas_series_isin_impl(self, values): values = set(values) data_len = len(self._data)
# todo "specify timestamp precision when writing to influxdb."? </s> client.write_points(chl, database=db)	fill logger.debug('writing next chunk %s', chl[-1])
engine = db.connect() # todo do i need to tear anything down?? </s> meta = sa.metadata(engine)	save_locs def save_locs(db_path: Path): db = sa.create_engine(f'sqlite:///{db_path}') schema = make_schema(xx_obj()) sa.Table('locations', meta, *schema)
# todo add </s> pass	set_startup_hook def set_startup_hook(function=None):
# @todo: pheonix </s> if event:	populateSkillTree childId = tree.AppendItem(root, name, imageId, data=('group', id)) tree.AppendItem(childId, "dummy") event.Skip()
# todo: componentize! </s> self.toplevel.output.append(str(e), "exception")	key_Return point = buffer.get_iter_at_line_offset(e.lineno, e.offset) buffer.place(point) except (OverflowError, ValueError), e: self.toplevel.output.append(str(e), "exception")
#ack = self.serial_port.read() # todo: use ack </s> self.sendcommand(161) # 10100001	SetLeftLaserOff def SetLeftLaserOff(self):
# todo: implement </s> pass	result_by_parent_name @staticmethod def result_by_parent_name(parent_name):
match = re.search(pattern, ret) # todo: more strict </s> if match:	xbr ret = exeCommand(debugger, "target list") pattern = '/.*\(' found = match.group(0) found = found.split("(")[0]
# todo ideally this happens a layer higher, but this is a bad </s> return sanitize_html(open( data.file_name ).read()).encode('utf-8')	display_data if not data.creating_job.imported and data.creating_job.tool_id in trans.app.config.sanitize_whitelist: return open(data.file_name).read() return open( data.file_name ) else:
# todo: stub function, add actual logic </s> def private_payment_permissions(_info, _object_pk: any) -> list[basepermissionenum]:	private_payment_permissions return []
full_payload)  # todo: parse response for confirmation. </s> self.treasure_map.add_ursula(policy.ursula)	enact_policies response = networky_stuff.enact_policy(policy.ursula, self.hrac(),
# todo(rakhmerov): implement. </s> pass	test_rollback def test_rollback(self):
# todo: fix with stubber / before send event </s> with mock.patch('botocore.endpoint.endpoint._send') as _send:	test_get_export 'accepts': 'application/yaml' } _send.return_value = mock.Mock( status_code=200, headers={}, content=b'{}')
# todo: consider use float32 type from the beginning of this function </s> xy_ctr = xy_ctr.type(torch.tensor)	get_xy_ctr batch, -1, 2)  # .broadcast([batch, fm_height, fm_width, 2]).reshape(batch, -1, 2) return xy_ctr
# todo files listed here may not belong to the given camera </s> file_moment = datetime.datetime.fromtimestamp(os.path.getmtime(full_path))	_remove_older_files def _remove_older_files(dir, moment, exts): for full_path in _list_media_files(dir, exts): if file_moment < moment: logging.debug('removing file %(path)s...' % {
# todo: update consumer </s> collection.remove(bind, safe=true)	distributor_deleted deleted.append(bind)
#todo: issue warning that this is an unsafe operation, but doing it cause user insists </s> try:	_unsafe_writes def _unsafe_writes(self, src, dest, exception): if exception.errno == errno.EBUSY: try: out_dest = open(dest, 'wb')
# todo: remove when support is python 3.6+ only </s> filename = str(filename)	register_file filename : str Path to the file to be registered. with h5py.File(filename, 'r') as h5file: filetype = h5file.attrs['filetype'].decode().lstrip('data_')
""" todo: better test here """ </s> timestamp = generate_timestamp()	test_generate_timestamp def test_generate_timestamp(self): self.assertTrue(isinstance(timestamp, unicode)) self.assertTrue(int(timestamp))
# todo: start looking in current group. </s> if view.id() == sought_id:	ex_buffers sought_id = view_ids[index] for view in window.views(): window.focus_view(view) window.show_quick_panel(file_names, on_done)
# todo: proper java error? </s> raise runtimeerror('could not find class \'%s\' for jnienv.' % name)	find_class clazz = self._class_loader.find_class_by_name(name) if clazz is None: return clazz.jvm_id
# todo: capture stdout for both the test assert and docs embedding </s> prob.run_model()	test_feature_iprint_neg1 ln_scipy.options['iprint'] = -1
# todo: bindings should be done in collection class: </s> collection.conjunctive_graph.namespace_manager = collection.meta.namespace_manager	test_query_collection c_repo.add_metadata_src_to_handle(PlainTextImporter, "MyHandle", md_hdl) collection = Collection(CollectionRepoBackend(c_repo))
# todo: support axel, prozilla </s> raise exception('axel exited abnormaly')	axel_download if exit_code != 0:
# todo check if lus can be more than one token </s> if annotations['lu'] in lines[i]:	produce_training_data lines[i].insert(1, str(i)) lines[i].append(annotations['frame']) lines[i].append('B-LU') else: lines[i].append('O')
#todo: support unicode, greek, and special latin characters </s> if keysym & 0x7f == keysym and chr(keysym) in 'abcdefghijklmnopqrstuvwxyz':	lookup_char_from_keycode keysym_index = 1 keysym = self.display.keycode_to_keysym(keycode, keysym_index) keysym_index = 0 elif self.modifiers['Shift'] or self.modifiers['Shift_Lock']:
# todo: add option for attentive reader </s> print('trainable variables (only embeddings): %d' % get_total_trainable_variables())	bilstm_reader_model_with_cands varscope.reuse_variables() candidates_embedded = nvocab(candidates) seq1_output, seq1_states, seq2_output, seq2_states = bilstm_readers(support_embedded, support_lengths, question_embedded, question_lengths,
# todo: this will incorporated in the future, if needed. </s> edge_starts = []	specify_edge_starts )) elif self.net_params.additional_params["on_ramp"]: else: edge_starts = []
accept_federated_only=self.federated_only)  # todo: 466 </s> listeners = self._learning_listeners.pop(node.checksum_public_address, ())	remember_node node.verify_node(self.network_middleware,  # TODO: Take middleware directly in this class? force=force_verification_check, address = node.checksum_public_address self.__known_nodes[address] = node
# todo, make broadcasting </s> await self.socketio.emit(	TaskHandlers if len(tasks['finished']) == 0 and len(tasks['active']) == 0: return 'tasks:all:get:back:updated', { "status": "success",
# todo: other types like boolean </s> typ_val = _h5_typ_table[data_t.dtype]	gatherv_overload def gatherv_overload(data_t): if isinstance(data_t, types.Array): def gatherv_impl(data): rank = hpat.distributed_api.get_rank()
# todo - implement fully and test </s> def setfocus(self):	SetFocus "Set the focus to this control" win32functions.SetForegroundWindow(self)
# todo untested </s> 1/0	_raise_ssl_error raise SysCallError(-1, "Unexpected EOF") else: _raise_current_error(Error) elif error == _api.SSL_ERROR_NONE:
# todo semantic validation </s> create_response = self.client.create(fsid, crush_node, serializer.get_data())	CrushNodeViewSet serializer = self.serializer_class(data=request.DATA) if serializer.is_valid(request.method): assert 'request_id' in create_response return Response(create_response, status=status.HTTP_202_ACCEPTED)
# todo stub </s> def test_interp() -> none:	test_interp pass
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo(b/160795287): deprecate estimator based executor. </s> fn_args_dict = fn_args.get_dict()	run_fn fn_args: Holds args used to train the model as name/value pairs. schema = io_utils.parse_pbtxt_file(fn_args.schema_file, schema_pb2.Schema()) fn_args_dict['serving_model_dir'] = os.path.join(fn_args.model_run_dir, path_utils.SERVING_MODEL_DIR)
# todo: support minp arg end_range etc. </s> minp = win	roll_sum_fixed N = len(in_arr) output = np.empty(N, dtype=np.float64) range_endpoint = max(minp, 1) - 1 for i in range(0, range_endpoint):
# todo: has some issues with datetime and sqlite </s> self._test_model('language', post_language_data)	test_language_api def test_language_api(self):
# todo: replace xrange (could fail with 32-bit python 2.x). </s> for x in xrange(self.bytelength - 1, 0, -1):	setoffset if (b + shiftright) // 8 > b // 8: self._rawarray.append(0) data[x] = ((data[x-1] << (8 - shiftright)) & 255) + \ (data[x] >> shiftright)
# todo pseudo code: </s> pass	OpenUri @dbus.service.method(dbus_interface=player_interface) def OpenUri(self, uri):
# todo(slaweq): change this to neutron floating ips and turn neutron </s> mock_nova.floating_ips.list.return_value = [fake_floating_ip]	test_rebuild_server_with_admin_pass_wait mock_nova.servers.rebuild.return_value = rebuild_server mock_nova.servers.list.return_value = [active_server] self.cloud.name = 'cloud-name' self.assertEqual(
# todo: this can unnecessarily suspend the starting of a build, in </s> log.msg("starting build %s.. pinging the slave %s" % (build, sb))	_ping def _ping(ign): return sb.ping()
# todo: this is a hack to make a rule know </s> if isinstance(event, form) and event.name is none:	_process_step end_trackers.append(tracker.copy(tracker.sender_id)) if step.is_rule: event.name = "None" if isinstance(event, SlotSet) and event.value is None:
# # todo: </s> print(invite_hash)	extract_data_from_telegram_url invite_hash = invite_hash[0]
# todo: memoize? </s> return [r[self._k] for r in self._table_data]	_data def _data(self):
# todo: kill this </s> inspection = inspection[0]	domain_not_live if not inspection: return False return (not inspection.get("Live"))
# todo: implement model fitting based on the legacy code bellow </s> self.__eye_translations = eye_translations	__init__ super().__init__(g_pool, calib_data=calib_data, params=params)
# todo: calculate the checksum if not given </s> self.__checksum_type = checksum_type	set_checksum def set_checksum(self, checksum_type='', checksum=''): self.__checksum = checksum
'auto_link': m.case_type == module.case_type,   # todo: which menus need manual linking? some child menus probably </s> }	get_form_view_context_and_template 'unique_id': m.unique_id, 'name': trans(m.name, langs),   # TODO add star, I guess for m in modules    # TODO this is a list, with duplicates ], key=lambda link: link['name'])   # TODO: sorting not working?
# todo: avoid dummy and generate func here when inlining is possible </s> def _impl(df, n=5):	_impl return hpat.hiframes.pd_dataframe_ext.head_dummy(df, n)
# end todo </s> 'conformation prompt', action='store_true')	add_keys_parser keys_parser.add_argument('-y', '--yes', help='[Deprecated] Will run command without '
# todo make bit depth and sample rate into parameters. </s> inputfile = '-t raw -b 32 -r 44100 -e floating-point -'	__call__ stdin = None elif isinstance(inputsound, np.ndarray): stdin = inputsound.tobytes() elif isinstance(inputsound, list):
# todo: figure out the right thing to do here </s> return mu_1**2 + mu_2**2, np.array([0.0, 0.0, 1.0, 1.0], dtype=np.float32)	diagonal_gaussian_energy_grad sign_s2 = np.sign(x[3]) if det == 0.0: cross_term = 2 * sigma_12 m_dist = np.abs(sigma_22) * (mu_1 ** 2) - \
# todo: improve this code. </s> if self._arch_info.architecture_mode == arch_x86_mode_64 and \	_translate_div tb.add(self._builder.gen_div(tmp4, tmp0, tmp5)) tb.add(self._builder.gen_mod(tmp4, tmp0, tmp6)) result_low.size == 32: if result_low.name in tb._regs_mapper:
# todo: remove warning check once deprecated </s> hits = tree.intersection((1012821.80, 229228.26), objects=true)	test_merge_geo tree = second.sindex with pytest.warns(FutureWarning, match="`objects` is deprecated"): res = ([second.loc[hit.object]["BoroName"] for hit in hits],) assert res == ["Queens"]
# todo: support unicode </s> in_str = unicode_to_std_str(str_arr[i])	_str_replace_regex_impl str_list = hpat.str_ext.alloc_str_list(n) for i in numba.parfor.internal_prange(n): out_str = std_str_to_unicode( hpat.str_ext.str_replace_regex(in_str, e, val))
# todo: try removing the none checks after https://github.com/mozilla/rust-code-analysis/issues/528 is fixed. </s> if metrics["mi"]["mi_original"] is not none:	get_summary_metrics obj["nexits_min"] = min(obj["nexits_min"], metrics["nexits"]["sum"]) obj["cognitive_min"] = min(obj["cognitive_min"], metrics["cognitive"]["sum"]) obj["mi_original_min"] = min( obj["mi_original_min"], metrics["mi"]["mi_original"]
oldsize = self.size # todo: remove </s> self.size = 8 + 4 + 4 + len(self.body[1]) * 8	stts_atom write_uint(stream, sample_duration) def calsize(self): assert oldsize == self.size, '%s: %d, %d' % (self.type, oldsize, self.size) # TODO: remove return self.size
# todo: use the following when reddit pr #631 is added </s> self.asserttrue(all(x.mod_id36 == other.id for x in actions))	test_get_mod_log_with_mod_by_name self.assertTrue(actions)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_tips_wrong_type def test_fail_tips_wrong_type(self):
# todo(gabriel-bezerra): simplify this after mitaka </s> self.assertequal(	test_verify_node_info_missing_sh self.node ) ("Missing the keys for the following OneView data in node's " "driver_info: server_hardware_uri."),
#     todo </s> def release(self, path, fh):	Filesystem def readdir(self, path, fh): return ['.', '..'] + cache.list_effective_nodes(path) return os.close(fh)
#todo: actually check for change </s> self._smudge('__setslice__', k, v)	__setslice__ def __setslice__ (self, k, v): list.__setslice__(self, k, v)
# todo(msb) all-to-all </s> expert_output = torch.unsqueeze(input, 1)  # add e dimension	all_to_all_combine def all_to_all_combine(self, combine_weights: Tensor, input: Tensor) -> Tensor: output = torch.einsum("gsec,gecm->gsm", combine_weights, expert_output) return output
# todo: remove all elements of the list and remove the blacklist </s> blacklist = [	test_no_deprecated_v1 def test_no_deprecated_v1(): "tensorflow_addons/text/skip_gram_ops.py", "tensorflow_addons/text/skip_gram_ops_test.py",
# todo: optimise this </s> for location in self._collision_locations():	_collision_location_closest def _collision_location_closest(self): try: if self._render_world.world.get_block(*location, self._render_world.dimension).namespaced_name != 'universal_minecraft:air':
# todo: uncomment to enable claiming </s> ], prefix='/api/v1',)	make_url_map Rule('/settings/names/', 'post', profile_views.post_names, json_renderer), Rule('/profile/<user_id>/summary/', 'get', profile_views.get_profile_summary, json_renderer), process_rules(app, [
# todo: remove the need for using dbobject.qualified_name </s> all_writes = set([i.qualified_name for i in all_writes_raw])	determine_nonschema_privileges_for_schema has_default_write = dbcontext.has_default_privilege(role, schema, objkind, 'write') all_writes_raw = dbcontext.get_role_objects_with_access(role, schema, objkind, 'write') if has_default_write or (all_writes == schema_objects and all_writes != set()): return set([schema + '.*']), set()
# todo: reevaluate how to deal with different types of errors; soft </s> del result['error']	smart_search_vrf search_options, extra_query ) except NipapError, e: return json.dumps({'error': 1, 'message': e.args, 'type': type(e).__name__})
# todo: move to base class </s> return self._manipulationmode	Canvas @property def manipulationMode(self): @manipulationMode.setter def manipulationMode(self, value):
# todo: kwargs </s> def _impl(df, axis=none, skipna=none, level=none, numeric_only=none):	mean_overload @overload_method(DataFrameType, 'mean') def mean_overload(df, axis=None, skipna=None, level=None, numeric_only=None): return hpat.hiframes.pd_dataframe_ext.mean_dummy(df) return _impl
break  # we are done. todo: should we continue? ;) </s> except exception as exc:	repo description=desc ) lgr.debug("DueCredit .cite caused %s", exc_str(exc)) return self._repo
# todo: reflection formula </s> znew = mpi_add(z, mpi_one, wp)	mpi_gamma d = mpf_loggamma(a, prec, round_ceiling) else: if type == 0: return mpi_div(mpi_gamma(znew, prec+2, 0), z, prec) if type == 2: return mpi_mul(mpi_gamma(znew, prec+2, 2), z, prec)
# todo: test/handle object not found. </s> return actionexectuionapi.from_model(actionexec_db)	get_one GET /actionexecutions/1 actionexec_db = ActionExecution.get_by_id(id)
# todo: handle direct payments </s> valid_mod = false	verify if asking_price > self.contract["buyer_order"]["order"]["payment"]["amount"]: raise Exception("Insuffient Payment") for mod in self.contract["vendor_offer"]["listing"]["moderators"]: if mod["guid"] == self.contract["buyer_order"]["order"]["moderator"]:
# todo: test me @jmcarp </s> logger.error('could not access github repo')	github_hgrid_data repo = connection.repo(node_settings.user, node_settings.repo) except NotFoundError: return None if repo.private:
# todo: should use response.json instead </s> assert isinstance(response.html, str)	test_multiple_request_sync urls = [f'https://httpbin.org/get?p={page}' for page in range(2)] async for response in self.multiple_request(urls, is_gather=False): MultipleRequestSpider.start()
# todo: add metadata support when it is merged from develop </s> return ret	_format_job_instance 'Target-type': job.get('tgt_type', []), 'User': job.get('user', 'root')}
""" todo: documentation </s> return false	streaming @property def streaming(self):
# todo : factorize this in utils/packages.py ? </s> for package in critical_packages:	tools_upgrade if not failure and critical_packages_upgradable: logger.info("Upgrading 'special' (yunohost-related) packages ...") check_output("apt-mark unhold %s" % package) unheld_packages = check_output("apt-mark showhold").split("\n")
# todo(vmiura): parse these in a more readable way. </s> toks1 = l.strip().split(' ', 1)	Main reader.next() # Eat line if l and l.strip() != '': toks2 = toks1[1].rsplit(' ', 1) cs_name = toks2[0]
# todo: remove when signals land </s> tp0.data_tool.update()	test_data_tp_checks unit.save() unit.store.data_tool.updater.update() checks = _calculate_checks(qc_qs.all()) check_data = tp0.check_data.all().values_list("category", "name", "count")
# todo: maybe the output dataplaceholders should be replaced too </s> for output in self._outputs:	step old_step = self._step self._step = new_step output._step = new_step for attr_name in ("compute_func", "fit_compute_func"):
# todo: require an api key on the basic auth header </s> file_storage = request.files.values()[0]	upload if len(request.files) != 1: return 'Need exactly one file', 400 data = file_storage.read() sha1sum = hashlib.sha1(data).hexdigest()
# todo: this one doesn't work quite well, handle it </s> ast.fix_missing_locations(tree)	_compile_func def _compile_func(): code = compile(tree, func_file(func), 'single') exec code in func.__globals__, context
# todo: find a better way to enforce this. </s> fmt = "#b%0{0}d".format(op3_var.size - op1_var.size)	_translate_str elif oprnd1.size < oprnd3.size: expr = (op1_var == smtlibv2.EXTRACT(op3_var, 0, op1_var.size)) imm = smtlibv2.BitVec(op3_var.size - op1_var.size, fmt % 0) constrs = [(imm == smtlibv2.EXTRACT(op3_var, op1_var.size, op3_var.size - op1_var.size))]
# todo: a better solution might be to use locking in this code </s> limit=1,	get_or_create include_docs=True, reduce=False, ).one() if existing:
# todo: add theano function. </s> raise notimplementederror()	gather backend = K.backend() if backend == "theano": elif backend == "tensorflow": import tensorflow
# todo: this is a jump. </s> vi_cmd_data['motion']['command'] = '_vi_left_parenthesis'	vi_left_parenthesis def vi_left_parenthesis(vi_cmd_data): vi_cmd_data['motion']['args'] = {'mode': vi_cmd_data['mode']} return vi_cmd_data
#todo : what if multiple projects are selected ? </s> p = selected_proj[0]	on_add_task def on_add_task(self,widget) : selected_proj,sel_task = self.get_selected_task() task = self.projects[p][1].new_task() self.open_task(task)
# todo addding a assertrvline to test reversed selections would make calls to assertselection() obsolete in this test  # noqa: e501 </s> self.assertselection((10, 2))	test_if_forward_vlines_within_targets_and_target_is_before_selection_it_should_flip_and_reverse_selection self.feed('l_%') self.assertVline('x\n|f {\na\nb\n|c\n}\nx\n') self.feed('l_%') self.assertVline(start)
# todo: think of something more sensible to do than sum(). on one </s> return sum([((inp - rec)**2).sum(axis=1).mean() for inp, rec in pairs])	mse denoised reconstruction. pairs = izip(inputs, self.reconstruction(inputs))
# todo implement. </s> yaml = self.master_model.to_yaml()	_train def _train(self, rdd, nb_epoch, batch_size, verbose, validation_split):
## todo: add only the optimizations needed? </s> m2 = m.including('canonicalize')	test_dot22scalar assert len(topo)==2 f(av,bv,cv) f = theano.function([a,b,c],0.1*c * 0.2*T.dot(a,b),mode=m2) topo = f.maker.env.toposort()
# todo: copy icons into directory </s> return path	_make_native_kernel_dir }, f)
# todo this paragraph is necessary, but not sure it works. </s> context = self._user_context.get_context()	get_completions return [(name, module) for name in importer.completion_names(self._evaluator, True)] elif isinstance(user_stmt, pr.Import): next(context)  # skip the path if next(context) == 'from':
# todo (bev) exception or log? </s> pass	hold session = session() if not isinstance(session, Session): session.hold(value)
# todo: fix the fact the we have more keys in settings.yaml </s> src = "/test_dir/tests/test_data/settings_old"	test_settingsfile_migration_content def test_settingsfile_migration_content(tmpdir: pathlib.Path): dst = os.path.join(tmpdir, "settings") shutil.copyfile(src, dst)
# todo: deprecation 3.1 </s> @deprecated("deprecated. use log_cosh instead")	logsinh def logsinh(x): return log_sinh(x)
except exception:  # todo - which exceptions? </s> pass  # skip unparsable tag	_parse_feature try: feature.qualifiers[feature_element.tag.replace(NS, '')] = feature_element.text self.ParsedSeqRecord.features.append(feature)
#todo same issue with batch_size </s> if len(self.inputs) == 0:	channels def channels(self): raise ValidationException("gan.channels() requested but no inputs provided") return self.ops.shape(self.inputs[0])[-1]
# todo: /data/local/tmp might not be execuable and atx-agent can be somewhere else </s> device.shell("/data/local/tmp/atx-agent", "server", "-d")	connect_usb warnings.warn("backend atx-agent is not alive, start again ...", RuntimeWarning) deadline = time.time() + 3 while time.time() < deadline:
# todo consider launching a second search if results.total_tracks() </s> tracks = [	callback def callback(results, userdata=None): translator.to_mopidy_track(t) for t in results.tracks()] future.set(tracks)
# todo: remove this test as soon as all old test methods are migrated </s> from sage.misc.sage_unittest import testsuite	test_sage_unittest_testsuite def test_sage_unittest_testsuite(self, sage_object: SageObject): TestSuite(sage_object).run(verbose=True, raise_on_failure=True)
# todo: explicitly commit files by name </s> youngest_ancestor = os.path.commonprefix(files)	add raise IOError("[CVS] Error running CVS command '%s': %s" \ % (command, error)) return output + type(self)(youngest_ancestor).commit(message, author)
# todo: kernels need some sort of structured form </s> self.kernel_derivative = r_2_log_r_2_kernel_derivative	TPS kernel = r_2_log_r_2_kernel self.kernel = kernel self.K = self.kernel(pairwise_norms) self.P = np.concatenate(
#todo this method is so weird, find a unused address to inject code not the base address </s> code = ""	get_dbg_brk_linux32 def get_dbg_brk_linux32(): Return the current brk value in the debugged process (only x86 Linux) code += '\xb8-\x00\x00\x00' #mov eax, sys_brk ; 45 code += '1\xdb' #xor ebx, ebx
# todo: this is an important operation that should be controlled </s> return "python"	_python_cmd def _python_cmd(_project_op):
# todo: implement the shit herein instead of collectionrepo </s> return self.repo.get_handles(self.branch)	CollectionRepoBranchBackend self.branch = branch def get_handles(self): def get_collection(self): return self.repo.get_collection(self.branch)
# todo: remove "--feature=2020-resolver" when pip 20.3 is released </s> run_pipx_cli_exit(	test_package_determination pytest.skip() caplog.set_level(logging.INFO) [ "run",
'''todo: add docs''' </s> widget = select.create(name=name, value=getattr(self.model, model_field), options=options)	add_select def add_select(self, name, options, model_field): self.controller.bind_to_model(widget, 'value', model_field) self.layout.children.append(widget)
# todo: self._line_structures is a work-around and this needs </s> self._line_structures = self.line_structures	PyparsingSingleLineTextParser self.encoding = self.ENCODING self._current_offset = 0 def _ReadLine(self, text_file_object, max_len=0, quiet=False, depth=0): Args:
# todo consolidate with other drivers </s> pass	_process_events except IndexError:
# todo(cdent): make this something other than a stub </s> def application(environ, start_response):	application start_response('500 Internal Server Error', [ ('Content-Type', 'text/plain; charset=UTF-8')])
# todo: comment this </s> result = search_dir(path_parts, 'index')	find_template remaining_parts = collections.deque() while path_parts: if result: return result + (u'/'.join(remaining_parts),)
# todo: enable this.  this is more like cpython.  note that i worked </s> obj_name = '<genexpr>'	visitGenExpr gLambdaCount += 1 else: gen = GenExprCodeGenerator(node, self.scopes, obj_name, self.class_name, self.get_module())
# todo: set numpy setflags </s> return self._data[self._mask_with_halo]	data_ro_with_halo @_allocate_memory def data_ro_with_halo(self):
#todo: fix this better </s> if options.get("randomize") in ["all", "suites"] and \	_create_execution_items _NUMBER_OF_ITEMS_TO_BE_EXECUTED = 0 for suite_group in suite_names: "suitesfrom" not in pabot_args: random.shuffle(suite_group)
#todo - code itself tolerates ambiguous bases (as nan). </s> if not isinstance(self.alphabet, iupac.iupacunambiguousdna):	calculate number is returned - otherwise, the result is a one-dimensional list or numpy array raise ValueError("PSSM has wrong alphabet: %s - Use only with DNA motifs" \ % self.alphabet)
# todo: this is untested. </s> return []	get_client_ca_list ca_names = _lib.SSL_get_client_CA_list(self._ssl) if ca_names == _ffi.NULL: result = [] for i in range(_lib.sk_X509_NAME_num(ca_names)):
# todo support subvars (@sub.var1) </s> dump_data = base64.b64encode(json.dumps(self.dump()).encode()).decode()	compile def compile(self): if self.embed_refs: return f"?{{{self.type_name}:{dump_data}:embedded}}" return f"?{{{self.type_name}:{self.path}:{self.hash[:8]}}}"
# todo(b/179510447): align these parameters with schulman 17. </s> return tf.keras.layers.dense(	means_layers def means_layers(): action_tensor_spec.shape.num_elements(), kernel_initializer=tf.keras.initializers.VarianceScaling(
# todo: this test requires manifold access, see: t88318502 </s> self._test_model("coco-detection/retinanet_r_50_fpn_3x.yaml")	testRetinaNet def testRetinaNet(self):
# # todo: </s> print(invite_hash)	extract_data_from_telegram_url invite_hash = invite_hash[0]
# todo: get this from cache. </s> now = datetime.datetime.now()	get_live_notifications @classmethod def get_live_notifications(cls): return Notification.objects.filter( start_date__lte=now, end_date__gte=now)
# todo threading </s> to_parquet(chunk, conn, bucket, key_prefix,	to_sql else: for chunk in get_chunks(df, chunksize): compression=compression, flavor=flavor)
# todo use smart_open again when https://github.com/rare-technologies/smart_open/issues/207 will be fixed </s> with open(self.file_name, 'rb') as f:	load_binary_data encoding : str, optional Specifies the encoding. self._load_model_params(f) self._load_dict(f, encoding=encoding)
# todo: comment as to why this is </s> context = none	triples assert self.__open, "The InformationStore must be open." if context == self.identifier: _from_string = self._from_string index, prefix, to_key, from_key = self.__lookup((subject, predicate, object), context)
# todo: add logging to indicate the failure </s> raw_conn.close()	Swarm ) except UpgradeFailure: return self.connections[peer_id] = muxed_conn
# todo: add invocation wrapper #1353 </s> globalloggersettings.stop_console_logging()	test_bob_retrieves_twice_via_cli for cleartext in retrieve_response['result']['cleartexts']: assert cleartext.encode() == capsule_side_channel.plaintexts[1] retrieve_response = click_runner.invoke(nucypher_cli, retrieve_args, catch_exceptions=False, env=envvars) GlobalLoggerSettings.start_console_logging()
# todo: make this work for sql </s> create_actions = filter(lambda a: a.action_type == case_action_create,	CaseDetailsView def page_context(self): if not should_use_sql_backend(self.domain): self.case_instance.actions) if not create_actions:
# todo: ensure collisions can't happen by verifying the config_prefix is empty </s> random_prefix = ''.join(	_add_process_layer def _add_process_layer(self, context, dtb, config_prefix = None, preferred_name = None): if config_prefix is None: random.SystemRandom().choice(string.ascii_uppercase + string.digits) for _ in range(8)) config_prefix = interfaces.configuration.path_join("temporary", "_" + random_prefix)
raise notimplementederror # the below does most probably not work anymore todo </s> base = none	diff TESTS:: TODO if base == "dependencies": branch = self.git.current_branch()
"bpm"        : float(self.bpm), # todo: serialise timevar etc </s> "beat"       : (self.beat.numerator, self.beat.denominator),	get_sync_info data = { "start_time" : (self.start_time.numerator, self.start_time.denominator), "time"       : (self.time.numerator, self.time.denominator) }
# todo subject.cn from cert? </s> shutil.rmtree(app_path)	test_simple_app lib_hashes = self.assert_common_signed_hashes(lib_info, -2, -1) assert '-3' not in lib_hashes return app_info
todo: factorize once #1457 is merged. </s> returns: longitude [deg east], latitude [deg north] and altitude [m]	get_satpos def get_satpos(self): Evaluate orbit polynomials at the start time of the scan. if self.satpos is None: a, b = self.get_earth_radii()
# todo: find the id that's suitable for this extruder </s> pass	addExtruderStackForSingleExtrusionMachine material_id = "default" if machine.material.getId() not in ("empty", "empty_material"): else: material_id = "empty_material"
stats_to_calculate = ['mean', 'std', 'min', 'max']  # todo: get input from user </s> percentiles_to_calculate = range(5, 101, 5)  # todo: get input from user	calculate_stats def calculate_stats(self): percentiles_to_calculate.append(99) headers = 'sub-metric,mean,std,p50,p75,p90,p95,p99,min,max\n'  # TODO: This will be built from user input later on
#todo: detect the size of gpu pointeur and c int. </s> int_size = 8	max_inputs_to_GpuElemwise This is needed as currently their is a limit of 256 bytes of paramter for the gpu function. This mesure the number of paramter we put in our gpu function and compute the maximum number of inputs that respect the 256 bytes limits. ptr_size = 8 argument_limit = 256  # if was 240, with this note: 16 bytes are used for block and thread coords etc.
pass  # todo </s> else:	main sys.exit(0)
# todo: this scrolling is lame and centers text :/ </s> view.show(size)	chat view.insert(ed, size, str(envelope)) view.set_read_only(True)
# todo: add and store preprocessing errors. </s> logging.error('unable to decode identifier.')	_ParseFileData identifier = row[2].decode('utf-8') except UnicodeDecodeError: continue group_identifier = None
# todo test this logic with effect </s> for item in bucket.list(prefix=target_key):	update_repo s3 = boto.connect_s3() bucket = s3.get_bucket(bucket_name=target_bucket) new_item_path = os.path.join(rpm_directory.path, str(item.key)) if not os.path.exists(new_item_path):
# todo this might have the same bug the r package had </s> ind = np.where(d == np.min(d))[0]	pr_curve print('Area under PR curve (AU_PR): %0.2f' % area) d = (precision - 1) ** 2 + (recall - 1) ** 2 best_precision = precision[ind] best_recall = recall[ind]
# todo: uris as bytes </s> for line in contents.split(b'\n'):	parse_mpd_tag_cache current = {} state = None if line == b'songList begin': state = 'songs'
# todo(ericbidelman): support more than one filter. </s> if filterby is not none:	get_all update_cache=False): KEY = '%s|%s|%s' % (Feature.DEFAULT_MEMCACHE_KEY, order, limit) s = ('%s%s' % (filterby[0], filterby[1])).replace(' ', '') KEY += '|%s' % s
# todo caching? prebuilt mode table? </s> return getplugins(imodedef, plugins)	getModes def getModes():
# todo debug </s> print ("counter for rule with order '%d' "	_updateRule + "for alert level '%d' has expired. Removing it." % alertLevel.level) % ruleStart.order + "for alert level '%d' has expired. Removing it."
# temporary for testing. todo: remove </s> self.temp_list = list()	__init__ self.count_profit_for_settlements = False self.csvexporter = CSVExporter(profit_currency, logger, create_csv)
# todo: backend tensorflow </s> if verbose > 0:	restore def restore(self, save_path, verbose=0): print("Restoring model from {} ...\n".format(save_path)) if backend_name == "tensorflow.compat.v1":
#todo: set secure automatically if being accessed by https. </s> self.send_header("set-cookie",	_do_cookieguard log.debug("POX CookieGuard got no cookie -- setting one") self.send_response(307, "Temporary Redirect") "%s=%s; SameSite=Strict; HttpOnly; path=/" % (POX_COOKIEGUARD_COOKIE_NAME,
# todo: description </s> return {'unknown_iface':1}	_interfaceParse___iface_attributes else:
# todo: implement this (maybe change answer type) </s> elif subtype == 1:	build_dataset rel_questions.append(question) if subtype == 0: elif subtype == 2: A = objects[color1][1]
# todo only do these thing if status is true </s> self.logger.info('ml_valid type: {0}'.format(	process status = Actions( endpoint, self.s.sdnc).unmirror_endpoint() type(ml_returns[endpoint.name]['valid']))) if 'valid' in ml_returns[endpoint.name] and ml_returns[endpoint.name]['valid']:
#ack = self.serial_port.read() # todo: use ack </s> self.sendcommand(149) # 10010101	SetRightLaserOn def SetRightLaserOn(self):
# todo: can this be optimized to avoid duplicating the anchors? </s> anchors = np.broadcast_to(anchors, (config.batch_size,) + anchors.shape)	build if mode == "training": anchors = self.get_anchors(config.IMAGE_SHAPE) anchors = KL.Lambda(lambda x: tf.constant(anchors), name="anchors")(input_image) else:
# todo: all the expand stuff! </s> view.textcursor().inserttext(text)	insert def insert(name, view): text, variables = snippets.get(name)
# todo: send back error </s> raise oserror(msg)	_load_directory msg = u"not a directory: %s" % file_name self._logger.warn(u"[READ %s:%s] %s", ip, port, msg) tmpfile_no, tmpfile_path = mkstemp(suffix=u"_tribler_tftpdir", prefix=u"tmp_") os.close(tmpfile_no)
# todo yield </s> lgr.info("failed to enable annex remote %s, "	_configure_remote ds.repo.enable_remote(name) except CommandError as exc: "could be a pure git" % name) lgr.debug("Exception was: %s" % exc_str(exc))
# todo: convert non uris to file uris. </s> for line in data.readlines():	parse_m3u def parse_m3u(data): if not line.startswith('#') and line.strip(): yield line
# todo: move to base class </s> if direction:	zoomDelta def zoomDelta(self, direction): self.zoom(1 + 0.1) else:
assert returned == 0  # todo return different value? </s> out, err = capsys.readouterr()	test_help_wrong_name returned = cmd_main(["help", "-f", "tests/loader_sample.py", "--db-file", depfile_name, "wrong_name"]) assert "doit list" in out
# todo: should this be filtered? </s> return self.buf.get_text(line_start, line_end, false)	__getitem__ if not line_end.ends_line(): line_end.forward_to_line_end()
# todo: kill this debug print. </s> print msgs	test_js_sandboxer failures = [log['failure'].value for log in lc.errors] msgs = lc.messages() self.assertEqual(failures, []) self.assertEqual(status, 0)
# todo: css pointer-events have been set to none for the nested anchor tag </s> return dcc.link(	Link @component def Link(children=None, href='', **kwargs): href=href, className='link',
# todo: rate should not have to be inversed </s> rate = 1 / math.exp(-graph[start][end])	calculate_profit_ratio_for_path start = path[i] end = path[i + 1] money *= rate return money
# todo: consider using eafp here instead. </s> if value is _not_found:	resolve value = self.get(parts[0], _NOT_FOUND) for part in parts[1:]: break value = _get_value(value, part)
# todo : move this control elsewhere </s> if msg_type is reserved_0 or msg_type is reserved_15:	from_bytes flags = b1 & 0x0f remain_length = yield from decode_remaining_length(data[1:4]) raise MQTTException("Usage of control packet type %s is forbidden" % msg_type) return cls(msg_type, flags, remain_length)
## todo : log error </s> error_code = delete_functionparameters_doctype_submission(doctype=doctype, action=action)	_delete_submission_from_doctype user_msg.append("""When deleting Submission "%s" from Document Type "%s", it wasn't possible to delete all Submission Fields""" \ % (action, doctype)) if error_code != 0: user_msg.append("""When deleting Submission "%s" from Document Type "%s", it wasn't possible to delete all Function Parameters""" \
# todo: do something clever with originselectionrange and targetrange. </s> file_path = uri_to_filename(location["targeturi"])	handle_response location = response if isinstance(response, dict) else response[0] if "targetUri" in location: start = Point.from_lsp(location["targetSelectionRange"]["start"]) else:
# end todo </s> self.files = remove_dot_files(files)	set_uploader files.append(os.path.join(self.deploy_dir, f))
"size": 50  # todo: support pagination. </s> }	elastic_project_search ] }, kwargs['routing'] = project_slug results = PageIndex().search(body, **kwargs)
pass  # todo - should this do something </s> press "leave domain" on users tab	on_btleavedomain_clicked def on_btleavedomain_clicked(self, widget, data=None):
#todo: add deployment/replicaset? </s> self.gauge(metric_name, val, tags)	kube_pod_container_requested_cpu_cores val = metric.gauge.value tags = ['{}:{}'.format(label.name, label.value) for label in metric.label]
# todo set only zero order </s> vals = nm.repeat(fun, nods.shape[0])	set_surface_dofs elif isinstance(fun, nm.ndarray): assert_(len(fun) == dpn) elif callable(fun): qp, weights = self.integral.get_qp(self.gel.name)
# todo: not fool proof yet, but captures the most obvious cases. </s> card_type_name_safe = card_type_name.encode('utf8').replace(" ", "_")	accept parent_instance = self.card_types[self.parent_type.currentIndex()] card_type_name = unicode(self.name.text()) id = parent_instance.id + "." + card_type_name if id in [card_type.id for card_type in card_types()]:
# todo: self.assertfalse(prop.is_valid(np.bool8(false))) </s> self.asserttrue(prop.is_valid(np.int8(0)))	test_Int try: import numpy as np self.assertTrue(prop.is_valid(np.int8(1))) self.assertTrue(prop.is_valid(np.int16(0)))
# todo: investigate why this fails </s> pass	test_field_default def test_field_default(self):
#todo: check if/where this is used; if not used externally - remove </s> return self.marker_detector.marker_min_confidence	Surface_Tracker @property def marker_min_confidence(self) -> float: @marker_min_confidence.setter def marker_min_confidence(self, value: float):
# todo(frostig): finalize api. for now, return the underlying </s> if self.computation.is_trivial():	_xla_computation def _xla_computation(self): raise ValueError('A trivial computation has no HLO') return self.computation.hlo
# todo: in the future you can also add the possibility to synchronize from a chosen profile </s> try:	export_all_new_episodes exported_videoids_values = g.SHARED_DB.get_tvshows_id_list() excluded_videoids_values = g.SHARED_DB.get_tvshows_id_list(VidLibProp.exclude_update, True) guid_owner_profile = g.LOCAL_DB.get_guid_owner_profile() except ProfilesMissing as exc:
# todo: wrap backend call in error handling. </s> return backend.playback.get_time_position().get()	get_time_position backend = self._get_backend(self.get_current_tl_track()) if backend: else: return 0
# todo g.ind_edges = sub2ind(size(g.w), g.v_in, g.v_out) </s> assert v_in.size == v_out.size == weights.size	get_edge_list weights = self.W[v_in, v_out] weights = weights.toarray().squeeze() assert self.Ne == 2 * v_in.size return v_in, v_out, weights
loader = imageloader(32) #todo crop=true? </s> x, y = loader.load(fixture_path(), width=8, height=8, resize=true)	test_load_fixture_resize def test_load_fixture_resize(self): with self.test_session(): self.assertEqual(y.get_shape(), []) self.assertEqual(int(x.get_shape()[1]), 8)
# todo: improve this with a better resampling algorithm </s> return empirical([self.sample() for i in range(samples)])	resample def resample(self, samples):
# todo: enable specificity beyond hostname (e.g. include scheme, port) </s> updater = updater.__updaters.get( parsed_url.hostname )	get_updater try: parsed_url = urlparse.urlparse( url ) if updater is not None: target_filepath = updater.get_target_filepath( url )
#todo(wuzewu): version sort method </s> module_version_list = sorted(module_version_list)	get_module_url for index in module_index_list ] if not version: if not module_version_list:
# todo: move to base class </s> return self._minimum_scale	viewMinimumScale def viewMinimumScale(self):
# todo: why with bookkeeping=false? </s> for state, state_dict, params, mapper, \	_emit_update_statements execute(statement, multiparams) rows += c.rowcount connection, value_params in records: _postfetch(
spack.do_checksum = false        # todo: remove this global. </s> specs = spack.cmd.parse_specs(args.packages, concretize=true)	install tty.die("The -j option must be a positive integer!") if args.no_checksum: for spec in specs: package = spack.db.get(spec)
# todo: using variables (aka xccdf values) in ocil content </s> if self.ocil:	to_ocil boolean_question = ET.Element( "boolean_question", id=self.id_ + "_question") ocil_without_tags = re.sub(r"</?[^>]+>", "", self.ocil) else:
status = 'published'  # todo: find a way for draft posts </s> yield (title, content, slug, date, post.get('blog_name'), [type],	tumblr2fields content = content.rstrip() + '\n' kind = 'article' tags, status, kind, format) offset += len(posts)
except typeerror as e:          #todo: generalise for all api methods </s> raise exception(str(e))	get_method try: return get_rows(db, table=table, **kwargs)
# todo add verbose output </s> return self._domain	LocalizationModel @property def domain(self): @domain.setter def domain(self, new_domain):
# todo: create a new, better, doc interface to remove it </s> plugin_commands = itertools.chain(	command_groups @property def command_groups(self): self._rules_manager.get_all_commands(), self._rules_manager.get_all_nick_commands()
# todo(user): remove after 184 is out. </s> finalized_filenames = cls.get_filenames(mapreduce_state)	finalize_job state = cls._State.from_json(mapreduce_state.writer_state) files.finalize(state.filenames[0]) state = cls._State(finalized_filenames, []) mapreduce_state.writer_state = state.to_json()
# todo: clean this api when tensorflow requirement is updated to >=2.6. </s> if compat.tf_supports("data.dataset.take_while"):	_transform dataset = dataset.concatenate(extra_batches) dataset = dataset.enumerate() dataset = dataset.take_while(_continue_iter) else:
# todo(b/80125832): enable nccl in tests </s> all_reduce_spec='',	_testVariables staged_vars=False, optimizer='momentum', use_fp16=False, fp16_vars=False,
# todo: should assert that the tag values are all strings </s> self.guages[values] = v	set "Expected as many values to inc() as labels (%d)" % (self.dimension()) )
# todo: confirm necessity of this session clearing and lay out mechanics. </s> tuf.download._sessions = {}	test_https_connection download.unsafe_download(good_https_url, target_data_length) os.environ['REQUESTS_CA_BUNDLE'] = good2_cert_fname logger.info('Trying HTTPS download of target file: ' + good2_https_url) download.safe_download(good2_https_url, target_data_length)
# todo return, catch exception in main() </s> raise e	create_r2_script except Exception as e:
# todo: match line and arrowbox </s> assert matches	test_asy_bezier_curve inner_asy = extract_asy_body(asy) matches = re.match(r'^draw\(.*\)', inner_asy)
# todo: legacy behavior, should remove after new case processing </s> if xform._id not in case_doc.xform_ids:	_get_or_update_cases case_doc = _get_or_update_model(case_update, xform, case_db) if case_doc: case_doc.xform_ids.append(xform.get_id) case_db.set(case_doc.case_id, case_doc)
# todo: should be none for undef instead?  or ''? </s> return 0	ArithEvaluator e_die('Invalid key %r' % rhs) else: else: e_die('Expected array in index expression, got %s', lhs)
# todo: check the data! </s> self.asserttrue(len(list(pipe)) > 0)	test_tail pipe_def = self._get_pipe_def(pipe_file) pipe = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def)
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
#@todo: replace lambdas </s> data = zip(queue.keys(), queue.values())	queue else: pyfile["icon"] = "status_downloading.png" data.sort(key=get_sort_key) for id, value in data:
# todo !!! remove all this </s> primary_key = expression.columnset()	_compile_pks self._get_clause = self.inherits._get_clause else: for col in (self.primary_key_argument or self._pks_by_table[self.mapped_table]): c = self.mapped_table.corresponding_column(col)
# todo indexerror? </s> raise typeerror('unsupported index item type: %r' % dim_sel)	normalize_dim_selection return IntegerSelection(dim_sel, dim_len, dim_chunk_len) else: else: raise TypeError('unsupported index item type: %r' % dim_sel)
# todo: remove </s> if not context.dispatcher.authenticated:	notcommands if not context.dispatcher.authenticated and handler.auth_required: command_names.add(name) command_names.update(command.name for command in protocol.mpd_commands if command.auth_required)
# todo: fill with last value. </s> pass	__init__ kernel_size = (kernel_size, ) if len(kernel_size) < dim: if isinstance(spline_degree, int): spline_degree = (spline_degree, )
# todo: handle output diffing with plugins? i.e. image diff, svg diff, json diff, etc. </s> from nbdime import diff	diff_single_cells def diff_single_cells(a, b): return diff(a, b)
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
# todo: currently the functional tests only pass when run in a specific </s> functional_tests_rc = _run_in_test_environment(	run_all_tests def run_all_tests(): # pragma: no cover 'pytest --cov -- {}'.format(' '.join( _get_test_module_dict('functional').values())))
# todo: remove this global variable hacks after refactoring process. </s> data_utils.data_path = self.old_data_path	tearDown def tearDown(self):
# todo: should wait for the end of the ongoing test case, and stop gracefully netmon and procmon </s> try:	server_init self.total_mutant_index  = 0 self.total_num_mutations = self.num_mutations() import signal self.signal_module = True
# todo: in 0.6.0 change this to "disabled": false </s> "enabled": true,	test_server_bridge_routed "dev_type": "tap", "dh": "dh.pem", "key": "key.pem", "mode": "server",
# todo: think how to resolve landscape.io warning: </s> return windll.user32.callnexthookex(self.keyboard_id, code, event_code, kb_data_ptr)	keyboard_low_level_handler self.handler(event) finally:
# todo: deprecated - remove in version 0.10 </s> if isinstance(training_trackers, string_types):	train "Pass appropriate featurizer " "directly to the policy instead.") logger.warn("Passing a file name to `agent.train(...)` is " "deprecated. Rather load the data with "
# todo better check would be if the node is linked to the output and actually used </s> for node in utils_node.find_nodes(node_tree, "luxcorenodetexwireframe", true):	needs_edge_detector_shape def needs_edge_detector_shape(node_tree): if node.hide_planar_edges: return True
# todo: kill this after dictionaries build correctly </s> parser.set_params(**opt)	_forced_parse def _forced_parse(self, parser, opt): parser.set_params(log_every_n_sec=10) popt = parser.parse_args([])
# todo: sessions and not only dates/days should be considered </s> return true	_barisover_minutes bartm = self.data.datetime.time(index) if bardt > dt: tmpoint = tm.hour * 60 + tm.minute tmmul, tmrem = divmod(tmpoint, self.p.compression)
# todo: fix this! </s> layout = pangocairo.create_layout(cr)	draw_text_bubble return alloc = self.get_allocation() layout.set_alignment(Pango.Alignment.CENTER) layout.set_spacing(3)
# todo: validate </s> logger.error("caught test exception", exc_info=1)	test_manaul_exec_info_logging except Exception:
# todo: supports blocking queries and all consistency modes </s> pass	node def node(self, node, dc=None):
# todo: parse flags, error checking, etc. </s> dest_dir = argv[1]	_Cd def _Cd(argv, mem): if dest_dir == '-': old = mem.Get('OLDPWD')
# todo: rate should not have to be inversed </s> rate = 1 / math.exp(-graph[start][end])	calculate_profit_ratio_for_path start = path[i] end = path[i + 1] money *= rate return money
# todo: keep this as a dict through the whole path to simplify this code </s> if not is_dict:	_add_setup_metadata if type(args) == dict: is_dict = True if args.find("metadata=") == -1: if self.remote_user == 'root':
# todo: do properly with .predict() </s> if basis == 'linear':	plot_regressions alpha_in=1, beta_in=0.5, basis='linear'): x0 = np.arange(np.min(ksi) - 0.5, np.max(ksi) + 0.5) y0 = alpha_in + x0 * beta_in elif basis == 'poly':
# todo: @sbharadwajj implement and test </s> return true	hessian_is_zero def hessian_is_zero(self):
# todo(ddeja): those 2 options should be gathered from config. </s> self._sleep_time = 1	__init__ self.endpoints = [] self._worker = None self._max_sleep_time = 512
#todo - add checks in case we don't have a well-formatted xmlns </s> r = re.search('{[a-za-z0-9_\.\/\:]*}', root.tag)	__get_xmlns logging.debug("Parsing xml file successful") logging.debug("Find xmlns from " + root.tag) logging.debug( "Return " + r.group(0) ) xmlns = r.group(0).strip('{').strip('}')
# todo guard on system (provides_system_bus) </s> supported_devices = []	get_supported_devices def get_supported_devices(self): if not self.nm_available: log.debug("Supported devices can't be determined.")
# todo: - torch.abs(h_emb + r_emb - t_emb) </s> score = - torch.sum(torch.abs(h_emb + r_emb - t_emb))	calc_score :param t_emb: :return: return score
if colorsorter._parent_csdl and colorsorter._parent_csdl.reentrant: ##### todo: use different flag </s> pos1 = colorsorter._transform_point(pos1)	schedule_cylinder print "bare_prim cylinder:", ColorSorter._gl_name_stack[-1], \ color, pos1, pos2, radius, capped, ColorSorter._debug_transforms() ##### pos2 = ColorSorter._transform_point(pos2)
# todo: this here always returns empty response. if/when we want to </s> args = []	mock_etherscan_query output_types = get_abi_output_types(fn_abi) decoded_input = web3.codec.decode_abi(input_types, bytes.fromhex(data[10:])) result = '0x' + web3.codec.encode_abi(output_types, [args]).hex() response = f'{{"jsonrpc":"2.0","id":1,"result":"{result}"}}'
# todo: could cache the results of this for speed </s> existing = location.filter_by_type(domain, loc_type, parent)	get_by_name def get_by_name(loc_name, loc_type, parent): try: return [l for l in existing if l.name == loc_name][0]
# todo(stephenfin): use a helper </s> self.api.post_server_action(server['id'], {'migrate': none})	test_migrate_confirm fake_drop_move_claim, ) self._wait_for_state_change(server, 'VERIFY_RESIZE') self.assertUsage(src_host, 1)
w, h = tiledsurface.n, tiledsurface.n # todo: support for other sizes </s> thumbnail_pixbuf = doc.model.save(filename, feedback_cb=self.gtk_main_tick, **options)	save_doc_to_file x, y, w, h =  doc.model.get_bbox() if w == 0 and h == 0: self.lastsavefailed = False except document.SaveLoadError, e:
# todo: same code as for batch gradient, but with sum_batch = true </s> return bias_grad.view(shape)	bias module, grad_input, grad_output, grad_output[0], sum_batch=True)
# todo: is incref required? </s> pyapi.object_setitem(df_obj, cname_obj, arr_obj)	box_dataframe else: arr_obj = box_array(arr_typ, arr, c) pyapi.decref(cname_obj) if typ.index != types.none:
# todo this is ideal for threading. </s> for name in hive_names:	resync introduce a delay equal to the timeout for each machine. So for 10 machines with a 2 second delay you will get AT LEAST a 20 second delay. for attempts in xrange(0, max_attempts): if not hive[name].prompt(timeout=timeout):
return [] # todo: look at htmlify.py </s> def refs_by_line(self):	refs_by_line
# todo(dcramer): this doesnt handle concurrency </s> if test.query.filter_by(build=build, group_sha=test.group_sha, label_sha=test.label_sha).first():	_sync_test_results else: raise ValueError('Invalid test result: %s' % (case['status'],)) continue db.session.add(test)
# todo: mac os handling </s> return text	convertLinebreaks if sys.platform=='win32': return text.replace('\r\n', '\n')
# todo: revision 5 </s> userpass = ''	computeUserPass @param encryptMetadata: A boolean extracted from the standard security handler dictionary to specify if it's necessary to encrypt the document metadata or not @return: A tuple (status,statusContent), where statusContent is the computed password in case status = 0 or an error message in case status = -1 dictU = '' dictOE = ''
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_multiple_transactions def test_multiple_transactions(self): Getting a bundle that contains multiple transactions.
# todo: manage exceptions when parameters cannot be decoded. </s> task_type_parameters = json.loads(task.task_type_parameters)	get_task_type if task is not None: task_type_name = task.task_type cls = plugin_lookup(task_type_name, "cms.grading.tasktypes", "tasktypes")
lambda responses: responses,  # todo </s> 'hicaz'	get_transactions_xml supported_camt_messages=SupportedMessageTypes(['urn:iso:std:iso:20022:tech:xsd:camt.052.001.02']), ), ) logger.info('Fetching done.')
# todo: candidate for move to system/hdparm </s> if dev_byid is none:	read_hdparm_setting :return: comment string immediately following an entry containing the given device name or None if None found or the systemd file didn't exist. return None infile = '/etc/systemd/system/rockstor-hdparm.service'
# todo: use initializer_list<k> and initializer_list<v> perhaps?  do </s> self.write('{')	_WriteDictElements def _WriteDictElements(self, o): for i, item in enumerate(o.items): pass
# todo: test this </s> style = element.style	handle_computed_display_float Computed values of the display and float properties according to http://www.w3.org/TR/CSS21/visuren.html#dis-pos-flo if get_value(style, 'display') == 'none': return # position and float do not apply, but leave them
# todo candidate for move to system/osi as not btrfs related </s> with open('/proc/mounts') as fo:	root_disk The assumption with non md devices is that the partition number will be a single character. for line in fo.readlines(): fields = line.split()
# todo:  we might need additional logic comparing the state of git-annex </s> if not force:	_publish_data lgr.debug("Invoking copy --auto") annex_copy_options_ += ' --auto' annex_copy_options_ += ' --fast' for r in ds.repo.copy_to(
# todo: /data/local/tmp might not be execuable and atx-agent can be somewhere else </s> ad.shell([self._atx_agent_path, "server", "--stop"])	_init_atx_agent if not self.agent_alive: warnings.warn("start atx-agent ...", RuntimeWarning) ad.shell([self._atx_agent_path, "server", "--nouia", "-d"]) deadline = time.time() + 3
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_metadata_family_matches_fullname_psname_family_part_success def test_result_metadata_family_matches_fullname_psname_family_part_success(self): self.assertInSuccess('test_metadata_family_matches_fullname_psname_family_part', run_set(p, 'result'))
# todo: this is not thread-safe! </s> cursor = db.cursor()	get_potentials def get_potentials (db): cursor.execute('''CREATE TABLE IF NOT EXISTS potentials( potential_index INTEGER PRIMARY KEY,
# todo: fix this, this is one of the few cases where using the config </s> self.standalone = self.config.get('standalone', false)	Job default='DEBUG') self.__logging_handlers = {} if self.config.get('run.dry_run.enabled'):  # Modify args for dry-run unique_id = self.config.get('run.unique_job_id')
# todo(mitmul): remove this when cupy.random.choice becomes available </s> fg_inds = cuda.to_cpu(fg_inds)	__call__ n_fg_rois_per_image = min(self._n_fg_rois, fg_inds.size) if fg_inds.size > 0: fg_inds = np.random.choice( fg_inds, size=n_fg_rois_per_image, replace=False)
# todo implement this function </s> return	add_sms_log :param send: 0 for send, 1 for receive :return:
# todo: put this into timeframegroup. #316 </s> assert isinstance(timeframes, list)	merge_timeframes merged : list of TimeFrame objects Where adjacent timeframes have been merged. assert all([isinstance(timeframe, TimeFrame) for timeframe in timeframes]) n_timeframes = len(timeframes)
# todo: write tests </s> <measure> unit of musical time consisting of a fixed number of note-values of a given type, as	measureFromElement def measureFromElement(elem, backupNum=0): determined by the prevailing meter, and delimited in musical notation by two bar lines. In MEI 2013: pg.365 (379 in PDF) (MEI.cmn module)
# todo: fix </s> c.pyapi.incref(obj)	unbox_re_pattern @unbox(RePatternType) def unbox_re_pattern(typ, obj, c): return NativeValue(obj)
except exception:  # todo: what could happen here? </s> raise runtimeerror("could not migrate pickle file from .txt extension to .p extension.") from none	fix_extension_on_pickles if os.path.isfile(txt): os.rename(txt, (txt[:-4] + '.p'))
""" todo(datapipe-1525): this fixture override the `mock_source_cluster_name` </s> fixture present in conftest.py	mock_source_cluster_name @pytest.fixture def mock_source_cluster_name(self): return 'yelp_main'
except exception as e:  # todo: do not use bare except </s> break	getCDXJLinesFromFile try: statusCode = hdrs.statusline.split()[0] if not entry.buffer: return
# todo: maybe could be simplified using contexts </s> dst_units = self.__require_units[ufunc.__name__]	_call_ufunc mobjs = None if ufunc.__name__ in self.__require_units: if dst_units == 'radian': mobjs = []
# todo: save the new xml to the database </s> form_attachment_xml_new = xmltodict.unparse(form_attachment_dict)	handle form_attachment_dict["data"]["n0:meta"]["n0:username"] = "NEW USERNAME"
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> sampleparameters = {	test_acquire_token_with_user_pass def test_acquire_token_with_user_pass(self): "tenant" : "common", "authorityHostUrl" : "https://login.windows.net",
# todo: look at the model to see which revision is last. </s> offset = url_for(controller='revision', action='list')	test_list_long self.create_100_revisions() try: res = self.app.get(offset) self.assert_click(res, '2', 'Revision 2')
# todo: remove this (ssh_user is a legacy arg) </s> user = _user_or_ssh_user(user, ssh_user)	command user='vagrant', ) command = shlex_quote(command) connection_target = hostname
# todo: don't add output_type in dry_run mode? </s> output_type = index.products.add(output_type)	make_output_type output_type = morph_dataset_type(source_type, config) _LOG.info('Created DatasetType %s', output_type.name) return source_type, output_type
# todo(ahundt) add weight loading </s> model = model(img_input, x, name='densenet_fcn')	DenseNet_FCN padding='same', kernel_regularizer=l2(weight_decay), use_bias=False)(x) return model
# todo some complication with -1 label </s> y_ = y	test_classifiers_classes continue if name in ["LabelPropagation", "LabelSpreading"]: elif name in ["RandomForestClassifier", "ExtraTreesClassifier"]: y_ = y_str_numbers
"""todo: add documentation""" </s> def part_as_dict(part):	json @staticmethod def json(value, parameters): dict_part = { "type": part.type,
# todo: reformat or delete </s> camera.trackbodyid = 0	sawyer_torque_reacher_camera def sawyer_torque_reacher_camera(camera): camera.distance = 1.0 cam_dist = 0.3
#todo: get darknet class number from class file </s> num_classes_darknet = 80	custom_tiny_yolo_body pre-trained weights from darknet and fit for our target classes.''' weights_path='model_data/tiny_yolo_weights.h5' base_model = tiny_yolo_body(inputs, num_anchors, num_classes_darknet)
# todo: enable custom config </s> if base.lower() != e['word'][0:len(base)].lower():	process_matches if 'menu' not in e: e['menu'] = self._sources[name].get('abbreviation','') continue result.append(e)
# todo: surface user-facing error here </s> return []	resolve_tags ) if tags is None: return [ graphene_info.schema.type_named('PipelineTag')(key=key, value=value)
# todo handle bad type </s> if prop_name in config.prop_types:	__capture config.extra_props[prop_name] = prop_value; return True prop_value = config.prop_types[prop_name](prop_value) config.__dict__[prop_name] = prop_value
# todo refactor this to use one code path for all lists </s> if exploitdb_map is not none and vuln['id'] in exploitdb_map:	parse_cve_items elif source == 'BID': vuln['_securityfocus'].append(url) for expid in exploitdb_map[vuln['id']]: vuln['_exploitdb'].append(expid)
# todo find a better way to set up these defaults </s> args.verbose = false	test_library_location conf = Config(file=tcf) args = Namespace() args.monochrome = False args.cores_root = []
pass # todo </s> def _listplaylist(self, name):	_listplaylist @register(r'^listplaylist (?P<name>.+)$')
#todo: check the data! e.g. pubdate etc. </s> count = 0	test_loop_example pipe_def = self._get_pipe_def("pipe_dAI_R_FS3BG6fTKsAsqenA.json") p = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def) for i in p: count += 1
# todo: remove all elements of the list and remove the blacklist </s> blacklist = [	test_no_experimental_api def test_no_experimental_api(): "tensorflow_addons/optimizers/weight_decay_optimizers.py", ]
# todo: add permissions </s> except exception as e:	MusicBot try: entries_added = await player.playlist.async_process_youtube_playlist(playlist_url, channel=channel, author=author) traceback.print_exc() raise CommandError('Error handling playlist %s queuing.' % playlist_url)
@pytest.mark.skip()  # todo: fix this </s> self.assertisnotnone(response)	test_issue_560_success response = client.conversations_list(exclude_archived="true")
# todo change this to a class. </s> sys.stderr.flush()	echo sys.stderr.write(s)
# todo check magic class methods and return them also </s> result = strip_imports(get_scopes_for_name(scope, current))	follow_path result = [] else: return follow_paths(path, result)
# todo(nnorwitz): enable test. </s> def _testarray(self):	_testArray tokens = GetTokens('Bar[]') result = self.converter.CreateReturnType(list(tokens))
# todo: remove print </s> print 'skipping: {}'.format(doc.pod_path)	add_all_docs for doc in collection.list_docs_unread(): if doc.pod_path in unchanged_pod_paths: continue print 'Reading: {}'.format(doc.pod_path)
# no todo item selected </s> pass	_append_pending_todos self.listbox.focus.widget.set_attr_map(attr_spec) except AttributeError:
# todo: really dirty. figure out a better way. </s> exchange_trades = [x for x in exchange_trades if x.identifier not in all_set]	query_trades only_cache=only_cache, ) if self.premium is None: trades = self._apply_actions_limit(
# todo(b/141243467) remove these workarounds. </s> stack.enter_context(tf.variable_creator_scope(create_variables_eagerly))	scope stack.enter_context(super(TpuReplicator, self).scope()) stack.enter_context(tf.variable_creator_scope(replica_local_creator)) stack.enter_context(eager_initial_values()) yield
# todo: use triple factory </s> rotate.forward_owa(torch.zeros(16, 3, dtype=torch.long))	test_rotate rotate = RotatE(triples_factory=self.factory) self.assertIsNotNone(rotate) rotate.forward_cwa(torch.zeros(16, 2, dtype=torch.long)) rotate.forward_inverse_cwa(torch.zeros(16, 2, dtype=torch.long))
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_tips_contents_invalid def test_fail_tips_contents_invalid(self):
# todo: confirm necessity of this session clearing and lay out mechanics. </s> tuf.download._sessions = {}	test_https_dl_via_https_proxy self.set_env_value('REQUESTS_CA_BUNDLE', os.path.join('ssl_certs', 'proxy_ca.crt')) logger.info('Trying HTTPS download via HTTPS proxy: ' + self.url_https) download.safe_download(self.url_https, self.target_data_length)
# todo: initialize config in a unified place </s> session_config = config(session_config).extend(base_session_config)	expSenderWrapperFactory def expSenderWrapperFactory(env, learner_config, session_config): learner_config = Config(learner_config).extend(BASE_LEARN_CONFIG) if learner_config.algo.experience == 'SSAR':
assert study_id == 0  # todo </s> self.trials[trial_id].params[param_name] = value	report_param def report_param(self, study_id, trial_id, param_name, value):
# todo: store this data </s> type_code, yy, color_code, xx, comment = match.groups()	_parse_data_extension match = re.match(r'^(\d)(\d\d)([/1]\d)(\d\d)(.*)$', data) if match: errors.append('Area object not implemented') return comment
# todo: this can permit a failing program to run by eliminating </s> rval = node.op(node.inputs[0].owner.inputs[0], node.inputs[1])	local_gpu_reshape_chain if not tensor.opt.opt.check_chain(node, GpuReshape, GpuReshape): return False
# todo: return annexrepo instead if there is one </s> self._vcs = gitrepo(self._path, create=false)	get_vcs if self._vcs is None: try: except (InvalidGitRepositoryError, NoSuchPathError): pass
# todo: use an "event" subcase of subject </s> index = single_event_form_event_index[form.xmlns]	get_study_event count = len(self.data[item.study_event_oid]) if form.xmlns in SINGLE_EVENT_FORM_EVENT_INDEX: if count < index + 1: self.data[item.study_event_oid].extend([None for i in range(index + 1 - count)])
# todo: handle agg_columns. </s> kdf = kdf[	GroupBy for i in range(groupkey_length) ] [s.rename(label) for s, label in zip(self._groupkeys, groupkey_labels)] + [kdf._kser_for(label) for label in kdf._internal.column_labels]
# todo: support for 'file' type </s> raise swaggererror(	unmarshal_schema_object if obj_type == 'object': return unmarshal_object(swagger_spec, schema_object_spec, value) "Don't know how to unmarshal value {0} with a value of {1}" .format(value, obj_type))
# todo: series support is not implemented yet. </s> only_numeric=false)	any lambda col: F.max(F.coalesce(col.cast('boolean'), F.lit(False))),
# todo: python-components: for now, we call each preprocessor's graph_fn directly. </s> if self.backend == "python" or get_backend() == "python":	reset def reset(self): for preprocessor in self.preprocessors.values():  # type: PreprocessLayer preprocessor.reset()
# todo expand the policy before checking if the sts:assumerole action is allowed </s> if action == "sts:assumerole":	_find_roles_assumable_in_policy actions = [actions] for action in actions: if statement["Effect"] == "Allow": role_arns = statement["Resource"]
#todo wrap the lp api or use library </s> owner, ppa = url.split('/')[3:5]	clean_selected_ppa if not key_fingerprint: try: lp_url = 'https://launchpad.net/api/beta/~%s/+archive/%s' % (owner, ppa) req =  Request(lp_url)
# todo: make it optional </s> penalty = self.regularizer.regularizer.compute_penalty_uniform(model=self.model)	train_one_epoch loss_params['mask_index'] = self.mask_index loss = self.loss.loss(**loss_params) loss_batch = loss.item() + penalty.item() running_loss += (loss_batch - running_loss) / (batch_index + 1)
# todo: use dictfield and listfield to do validation </s> if (isinstance(data, dict) and	update_data_is_valid @staticmethod def update_data_is_valid(data): set(data.keys()).issubset(update_ops)): return True
# todo: check output </s> def test_hierarchy_iprint2(self):	TestSolverPrint prob.setup(check=False) output = run_model(prob) prob = Problem() model = prob.model
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo better to write a separate function for this </s> return len(self.get_reduced_simplex(point, simplex, eps)) > 0	point_in_simplex return fast_2d_point_in_simplex(point, self.get_vertices(simplex), eps) elif self.dim == 3: else: return len(self.get_reduced_simplex(point, simplex, eps)) > 0
# todo(pep612): fix for paramspectype </s> if isinstance(tvar, paramspectype):	get_target_type skip_unsatisfied: bool ) -> Optional[Type]: return None assert isinstance(tvar, TypeVarType)
# todo it seems that yahoo! converts relative links to absolute </s> if context.verbose:	pipe_fetchpage content = unicode(request.read(), request.headers['content-type'].split('charset=')[-1]) print "............FetchPage: content ................." print content.encode("utf-8")
# todo(mordred) when this changes to rest, force interface=admin </s> with _utils.shade_exceptions("failed to delete service {id}".format(	delete_service else: service_kwargs = {'service': service['id']} id=service['id'])): self.manager.submit_task(_tasks.ServiceDelete(**service_kwargs))
# todo: deprecation warning </s> class defaultserializer(self.model_serializer_class):	DefaultSerializer class Meta: model = self.model
# todo: in the future we will log the trace </s> msg = ""	_wrap_exception return desired_type(ex.args[0]) else: if len(ex.args) > 0: msg = ex.args[0]
# todo: write this in human </s> paths = ['/'.join(['..'] * (len(crumbs) - 2 - i)) for i in range(len(crumbs[:-2]))] + ['.', '#']	render_listing title = os.path.basename(in_name) crumbs = out_name.split(os.sep)[1:-1] + [title] context = { 'code': code,
# todo: remove this patching when the `content` property is supported. </s> with monkeypatch_validation(validate_content):	test_annotate_document @suite.test def test_annotate_document(): document = parse_html( 'doc1.html',
# todo retry connects </s> s.connect((self.host, self.port))	_send try: with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s: s.sendall(pickle.dumps(action)) return
# todo: ensure that if multiple flags are provided, the *last* one overrides </s> try:	Cd def Cd(argv, mem, dir_stack): arg, i = CD_SPEC.Parse(argv) dest_dir = argv[i] except IndexError:
except exception:  # todo: be specific </s> tools.stderr("encountered an error while writing the config file."	wizard try: settings.save() " This shouldn't happen. Check permissions.") raise
#todo: check the data! </s> count = 0	test_filtered_multiple_sources pipe_def = self._get_pipe_def("pipe_c1cfa58f96243cea6ff50a12fc50c984.json") p = pipe2py.compile.parse_and_build_pipe(pipe_def, verbose=True) for i in p: count += 1
# todo: try/except these calls </s> png = decodestring(data['image/png'])	_handle_display_data self._append_svg(svg) elif data.has_key('image/png'): self._append_png(png) else:
# todo: need to deterministically set to polynomial fitter </s> self.viewer.add_data(self.data)	test_fit_polynomial def test_fit_polynomial(self): self.profile_tools.ui.tabs.setCurrentIndex(1) x, y = self.viewer.axes.transData.transform([[1, 4]])[0]
#ack = self.serial_port.read() # todo: use ack </s> self.sendcommand(133) # 10000101	SetMotorCW def SetMotorCW(self):
# todo: implement me </s> return tracks	get_subtitle_tracks def get_subtitle_tracks(self): tracks = list()
assert study_id == 0  # todo(akiba) </s> return copy.deepcopy(self.trials[trial_id])	get_trial def get_trial(self, study_id, trial_id):
# todo: compute bezier curve. </s> anchors = self.anchors	ShapeLayer if bbox.width > 0 and bbox.height > 0: return bbox if not anchors or len(anchors) < 2: logger.warning("Empty shape anchors")
'ms_ssim': false,  # todo: enable fixed_ms_ssim </s> 'no_prediction': true,	test_run_vmafrc_runner_fixed_psnr_ssim 'psnr': True, 'ssim': True, } )
# todo: also preserve __module__, __name__ and a few other important attrs </s> return new_fn	profile return fp(*args, **kw) new_fn.__doc__ = fn.__doc__
# todo(mottodora): add reduce option </s> if self.ignore_nan:	forward_gpu self.retain_inputs((0, 1)) diff = (inputs[0] - inputs[1]).ravel() diff[cupy.isnan(diff)] = 0. return diff.dot(diff) / diff.dtype.type(diff.size),
pass  # todo... </s> beam_out[0] = beam	perform beam = beam_trans.transpose(*map(array_trans_dims_order.index, range(array.ndim))) if self.wrap_mode == "pad_zero":
# todo: we could find a way to handle this. </s> if full_name is post_modules:	onModuleDiscovered post_code, reason = self.createPostModuleLoadCode(module) if post_code: sys.exit("Error, conflicting plug-ins for %s" % full_name) info("Injecting plug-in based post load code for module '%s':" % full_name)
pass # todo </s> def _plchangesposid(self, version):	_plchangesposid @register(r'^plchangesposid (?P<version>\d+)$')
#todo: info page, test other objs </s> shutil.rmtree(corp_dir)	test_3 if os.path.isdir(corp_dir):
time.sleep(1)  # todo: avoid race conditions in other way </s> self.server.start()	setUp self.server = threading.Thread(target=self._run_server, args=(certs, sock))
# todo: handle multiple skip stacks </s> (skip, skip_stack), = skip_stack.items()	split_inline_box skip = 0 else: for i, child in enumerate(box.children[skip:]): index = i + skip
raise notimplementederror # todo </s> def latest_offsets(self):	latest_offsets
# todo: process form submission </s> return render_template("admin_add_user.html")	admin_add_user @app.route('/admin/add', methods=('GET', 'POST')) def admin_add_user():
if isinstance(err, asyncio.cancellederror):  # todo: remove when updated to 3.8 </s> raise err	BlobAnnouncer log.debug("failed to announce %s, could only find %d peers, retrying soon.", blob_hash[:8], peers) except Exception as err: log.warning("error announcing %s: %s", blob_hash[:8], str(err)) async def _announce(self, batch_size: typing.Optional[int] = 10):
# todo(shoyer): test fails on tpu </s> if jtu.device_under_test() == "tpu":	test_custom_linear_solve ) def test_custom_linear_solve(self, symmetric): raise SkipTest("Test fails on TPU") def explicit_jacobian_solve(matvec, b):
# todo is this still needed? </s> def datetimefromiso(iso):	datetimeFromIso Args: iso (str): ISO format string for a date and time
# todo consider more type conversions? </s> if value.isdigit():	load_tmx name = p.attrib['name'] value = p.attrib['value'] value = int(value) tile.properties[name] = value
# todo counts as yes if vhnd was not available </s> return len([c for c in self.cases	vhnd_monthly @property def vhnd_monthly(self): if c.preg_attended_vhnd or c.child_attended_vhnd])
pass  # todo... </s> if pred is true:	cond pred_const = tf.tensor_util.constant_value(pred) if pred_const is not None: return fn1() if pred is False:
# todo: improve performance, get rid of unfold </s> return jac_mat	weight_jac_mat_prod jac_mat = jac_mat.view(batch, out_features, num_cols)
# todo: this output will break output formats such as json </s> warnings.warn('{0}: {1}'.format(err.path, err.cause), importwarning)	find_from_path names |= find_from_imports(contents) except encoding.CouldNotHandleEncoding as err: if len(names) == max_possible: break
# todo: this should take a vector </s> return self._mesigma	MeSigma self._MeSigma = self.mesh.getEdgeInnerProduct(self.curModel.sigma)
# todo: handle requests with cookies (e.g. s3 pre-signed urls). </s> try:	inspect_download_url except HTTPError as err: if err.code == 403: raw_download = useragent_urlopen(checked_url, "Mozilla/5.0") facts["user-agent"] = "Mozilla/5.0"
#todo - turn that into a working doctest </s> records = parse(handle)	read record = Medline.read(handle) print(record['TI']) return records.next()
pass # todo </s> def _move(self, position=none, start=none, end=none, to=none):	_move @register(r'^move ((?P<position>\d+)|(?P<start>\d+):(?P<end>\d+)*) (?P<to>\d+)$')
# todo: assert </s> assert 0	test_get_system Test: get a system object system = self.remote.get_system("testsystem0")
#todo: dont unfold all, but allow enum_all() to work </s> tree_proc(self.tree, tree_item_unfold_deep, 0)	menu_goto msg_status('Project not opened') return files = [] def callback_collect(fn, item):
# todo: kwargs </s> def _impl(df, axis=none, skipna=none, level=none, numeric_only=none):	min_overload @overload_method(DataFrameType, 'min') def min_overload(df, axis=None, skipna=None, level=None, numeric_only=None): return hpat.hiframes.pd_dataframe_ext.min_dummy(df) return _impl
# todo: verify the length of blockhashes. </s> self._write_file('plaintext_hashes', ''.join(hashes))	remote_put_plaintext_hashes def remote_put_plaintext_hashes(self, hashes): precondition(not self.closed)
# todo: improve this algorithm - maybe fortran/c </s> bool_unused_sub = numpy.ones(nsub, bool)	divide_procs sub_proc_range = [iproc1, iproc2] else: isubs_list = [[] for ind in xrange(nproc)] proc_load = numpy.zeros(nproc)
# todo: __prepare_scriptable__ was reverted from pytorch: d25061862 </s> def prepare_resnet(self):	patch_nonscriptable_classes from detectron2.modeling.backbone import ResNet, FPN ret = deepcopy(self) ret.stages = nn.ModuleList(ret.stages)
# todo refactoring remove </s> from jedi.evaluate import builtin	load source = f.read() else: return builtin.BuiltinModule(path, name).parser.module p = path or name
# todo: +kwargs </s> else:	pull with remote.repo.git.custom_environment(**GitRepo.GIT_SSH_ENV): return remote.pull(**pull_kwargs) return remote.pull(**pull_kwargs)
#todo: dataset/hda by id (from history) or check_ownership for anon user </s> hda = self.get_history_dataset_association( trans, history, history_content_id,	display and ( history_id == trans.security.encode_id( trans.history.id ) ) ): history = trans.history check_ownership=False, check_accessible=True ) else:
# todo: delet this when all preprintproviders have a mapping </s> from osf.models.subject import subject	top_level_subjects return self.subjects.filter(parent__isnull=True) else: if len(self.subjects_acceptable) == 0: return Subject.find(Q('parent', 'isnull', True))
# todo: move this scopes conversion from and to string into a utils function </s> scopes = self.oauth2_data.get('scopes', [])	get_initial def get_initial(self): initial_data = { 'redirect_uri': self.oauth2_data.get('redirect_uri', None),
# todo: implement me </s> self.logmessage("%s %s" % (msg.__class__, vars(msg)), 4)	ChildDepth def ChildDepth(self, msg):
return none  # todo better error handling here </s> session_dict = response.json()	get_authenticated_user except Exception, e: log.error(e) if u'error' in session_dict: log.error("Error when getting authenticated user: %s" % session_dict['error'])
pass # todo </s> def _clear(self):	_clear @register(r'^clear$')
# todo: implement! </s> pass	_check def _check():
# todo: test for the _correct_ revision_id value. </s> if not activity.revision_id:	_update_resource if not activity.id: assert False, "activity object has no id value" assert False, "activity has no revision_id value" assert activity.timestamp >= before and activity.timestamp <= after, \
# todo remove </s> if self.defunct:	Import else:  # from raise NotImplementedError return [] if self.star:
# todo do something with temp </s> self._remove_substates_from_subhmms()	HSMMSubHMMStates super(HSMMSubHMMStates,self).clear_caches() def resample(self,temp=None): super(HSMMSubHMMStates,self).resample() # resamples superstates self._resample_substates()
if not self.hoster_url:  #@todo: remove in 0.4.10 </s> raise exception(_("missing hoster_domain"))	login def login(self, user, data, req): url  = urljoin(self.HOSTER_URL, "login.html") html = req.load(url, decode=True)
# todo: implement </s> def print_(self):	print_
raise mpdnotimplemented # todo </s> def _commands(self):	_commands @register(r'^commands$')
# todo(ntonci): add a check for small motion </s> j = i+1	align screw_axis_W_E_i, rotation_W_E_i, translation_W_E_i = dq_W_E_i.screw_axis(); screw_axis_B_H_i, rotation_B_H_i, translation_B_H_i = dq_B_H_i.screw_axis(); while j < len(dq_W_E_vec_filtered): dq_W_E_j = dq_W_E_vec_filtered[j]
#@todo: move to utils in 0.4.10 </s> def timestamp():	timestamp return int(time.time() * 1000)
# xxx todo </s> mode = int(value)	add_kde_setting elif key == "AuthMode":
# todo(nate): temporarily disabled </s> if false:	test_malformed jobstep = self.create_jobstep(jobphase) handler = ManifestJsonHandler(jobstep) fp = StringIO('invalid_file') handler.process(fp)
# todo: here we should check for the leverage based on the config value </s> return	on_order_submission else: self.sell_orders[base_asset].append(np.array([order.qty, order.price]))
# todo: candidate for move to system/hdparm </s> out, err, rc = run_command(	get_disk_power_status :return: single word sting of state as indicated by hdparm -C /dev/<disk> and if we encounter an error line in the output we return unknown. [HDPARM, '-C', '-q', '/dev/disk/by-id/%s' % dev_byid], throw=False) if len(err) != 1:
# todo: implement </s> self.webview.set_zoom_level(size)	set_font_size def set_font_size(self, size): return
# todo - move or delete </s> assert isinstance(message, can.message)	callback def callback(message): if message.arbitration_id == MatchingMessages.arbitration_id: MatchingMessages.messages.append(message)
#todo: check if/where this is used; if not used externally - remove </s> return self.marker_detector.marker_min_confidence	Surface_Tracker @property def marker_min_confidence(self) -> float: @marker_min_confidence.setter def marker_min_confidence(self, value: float):
# todo use the faster method </s> store.flush_cache()	handle_store def handle_store(self, store, **options): store.get_stats() store.get_mtime()
pass  # todo - should this do something </s> press "logout user" on users tab	on_btlogoutuser_clicked def on_btlogoutuser_clicked(self, widget, data=None):
# todo: i18n plurals </s> messages.error(self.request,	post return redirect(self.get_failure_url()) if sum(i[2] for i in items) > self.request.event.max_items_per_order: _("You cannot select more than %d items per order") % self.event.max_items_per_order) return redirect(self.get_failure_url())
# todo before moving to pyart.io </s> radar object containing data from netcdf file.	read_noxp_iphex_nc radar : Radar
# todo uncomment label and description when they are </s> 'read_only': false,	test_options_instance_view expected['actions'][method] = { 'text': { 'required': True, 'type': 'Single Character',
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> sampleparameters = {	test_acquire_token_with_user_pass def test_acquire_token_with_user_pass(self): "tenant" : "common", "authorityHostUrl" : "https://login.windows.net",
# todo(dcramer): it's a terrible api and realistically we should just be </s> if source.repository.backend == repositorybackend.hg:	get_release_id def get_release_id(source, vcs): return vcs.run( ['log', '-r %s' % (source.revision_sha,), '--limit=1', '--template={rev}:{node|short}'])
# todo(pygeos) does not support empty strings, np.nan, or pd.na </s> missing_values = [none]	test_from_wkt assert isinstance(res, GeometryArray) assert all(v.almost_equals(t) for v, t in zip(res, points_no_missing)) if not compat.USE_PYGEOS: missing_values.extend([f(""), np.nan])
# todo: post operations should only add to db. </s> actionexec_db = actionexecution.add_or_update(actionexec_db)	post POST /actionexecutions/ actionexec_db = ActionExecutionAPI.to_model(actionexec) return ActionExecutionAPI.from_model(actionexec_db)
# todo: make this pretty </s> return httpresponse('error retrieving locations: is the storage server running? please contact administrator.')	storage locations = storage_service.get_location(purpose="AS") except: system_directory_description = 'Available transfer source' return render(request, 'administration/locations.html', locals())
# todo implement this method </s> :return:	start From now on, the on_state_updated method in listeners will be continuously called
# todo: documentation pending </s> parameters	load_weights_from_npz_dict def load_weights_from_npz_dict(self, filepath, sess=None): ---------- filepath
# todo: untested </s> torch.save(self._model.state_dict(), str(path))	to_disk def to_disk(self, path):
# todo: not all values have exact matches in flexget, need to update flexget qualities </s> sources = {'br-disk': 'remux',  # not a perfect match, but as close as currently possible	quality_requirement_builder def quality_requirement_builder(self, quality_profile): Converts CP's quality profile to a format that can be converted to FlexGet QualityRequirement 'brrip': 'bluray', 'dvdr': 'dvdrip',  # Not a perfect match, but as close as currently possible
n = 1000  # todo: should scale with radius, dr </s> refx, refy, refz = _points_ring3d(r_edges, dr, n)	pairCorrelationKDTree3D ckdtree = cKDTree(feat[['x', 'y', 'z']])  # initialize kdtree for fast neighbor search points = feat.as_matrix(['x', 'y', 'z'])  # Convert pandas dataframe to numpy array for faster indexing for idx in p_indices: dist, idxs = ckdtree.query(points[idx], k=max_p_count, distance_upper_bound=cutoff)
pass  # todo - should this do something </s> press "remove user" on users tab	on_btremoveuser_clicked def on_btremoveuser_clicked(self, widget, data=None):
# todo remove the unstable prefix when servers have updated. </s> "org.matrix.msc3083.v2.event": event_json,	FederationServer event_json = event.get_pdu_json() return { "event": event_json, "state": [p.get_pdu_json(time_now) for p in state.values()],
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_interact_arguments @classmethod def add_interact_arguments(cls, parser): p_convert = p_operation.add_parser( "convert", help="convert VGM to PCM using OPL hardware")
# todo: test this beam stuff (after you figure out wheter it's sufficient) </s> if elem.get('m21beam') is not none:	noteFromElement if elem.get('grace') is not None: post.duration = duration.GraceDuration(post.duration.quarterLength) if duration.convertTypeToNumber(post.duration.type) > 4: post.beams.fill(post.duration.type, elem.get('m21Beam'))
# todo: assert </s> repo = self.remote.get_repo("testrepo0")	test_get_repo Test: Get a repo object
except exception:  # todo - which exceptions? </s> pass  # skip unparsable tag	_parse_feature try: feature.qualifiers[feature_element.tag.replace(NS, '')] = feature_element.text self.ParsedSeqRecord.features.append(feature)
# todo: take namespace into account, currently doesn't matter since </s> with session_scope() as db_session:	mailing_list_info_for_message @jsonify def mailing_list_info_for_message(self, message_id): message = db_session.query(Message).filter(Message.id==message_id).one() return message.mailing_list_info
# todo find out if this is good because of sparcity... </s> 'prefers_data_normalized': false,	get_properties 'handles_numerical_features': True, 'prefers_data_scaled': False, 'handles_regression': False, 'handles_classification': True,
# todo(blk-u): the following should be </s> self.assertraises(	test_change_default_domain_id username=self.user_foo['name'], password=new_user_password) exception.Unauthorized, self.controller.authenticate,
# todo: distributed search messages need to implemented </s> self.logmessage("%s %s" %(msg.__class__, vars(msg)), 4)	DistribBranchLevel def DistribBranchLevel(self, msg):
"""todo: document me.""" </s> def __init__(self, sigma, mu, seed=42):	__init__ self.sigma = sigma self.mu = mu
# todo(mriedem): perform some version discovery at some point. </s> headers.update({	_fake_get headers = {'x-auth-token': self.token} if version is not None: 'OpenStack-API-Version': 'placement %s' % version })
# todo some kind of non-zero check to make sure that this passes. </s> @task	run_integration_tests def run_integration_tests(c): c.run('echo tests', warn=True)
# todo: it's a bit weird that to_dict modifies the object. </s> original_data = getattr(self, 'data', undefined)	to_dict def to_dict(self, *args, **kwargs): self._prepare_data() context = kwargs.get('context', {})
# todo(petef): support hard/soft limits </s> resource.setrlimit(res, (value, value))	preexec_fn if res is None: raise ValueError('unknown rlimit "%s"' % limit) if self.gid: try:
raise mpdnotimplemented # todo </s> def _enableoutput(self, outputid):	_enableoutput @register(r'^enableoutput "(?P<outputid>\d+)"$')
# todo(brett.cannon) implement </s> raise importerror	_default_hook def _default_hook(self, path): If the path will not work for the default hook then raise ImportError.
# todo: localization support </s> ws.emit(message("speak", {	skills_manager logger.error("Unable to invoke Mycroft Skill Manager: " + MSM_BIN) if skills_manager_timer is None: 'utterance': "Skills Updated. Mycroft is ready"})) skills_manager_timer = Timer(3600.0, _skills_manager_dispatch)
# todo(eric_k): unicorn@778171fc9546c1fc3d1341ff1151eab379848ea0 doesn't like writing to </s> self.registers -= {'fs'}	__init__ self.registers = set(self._cpu.canonical_registers) self.registers -= self.flag_registers self.registers.add('EFLAGS') for reg in self.registers:
# todo(hvy): make percentile computation faster for gpus </s> return np.array((float('nan'),) * 7)	_percentiles except IndexError:
# todo: handle this </s> context.transform(	RasterImage return return concrete_width / self._intrinsic_width, 0, 0, concrete_height / self._intrinsic_height, 0, 0)
# todo(mitmul): use cupy.random.choice if it becomes available </s> disable_inds = np.random.choice(	_create_bbox_labels if len(bg_inds) > num_fg: bg_inds = cuda.to_cpu(bg_inds) bg_inds, size=len(bg_inds) - len(fg_inds), replace=False) labels[disable_inds] = -1
# todo subject.cn from cert? </s> shutil.rmtree(app_path)	test_simple_app lib_hashes = self.assert_common_signed_hashes(lib_info, -2, -1) assert '-3' not in lib_hashes return app_info
# todo: reuse the utils method for service restarts </s> return_code_service_restart = 0	restart_dhcp Basically this restarts the service to apply the changes. :param service_name: TODO return_code_service_restart = utils.subprocess_call("{} -t -q".format(service_name), shell=True) if return_code_service_restart != 0:
# todo parameters </s> f.write(u':\n')	print_Label def print_Label(f, stmt): f.write(u"label %s" % (stmt.name, ))
# todo: use complete list of `order_by` fields </s> table.order_by(order_by[0].replace('^', '-'))	print_ order_by = _get_field_names(order_by, table_field_names, permit_not=True) fobj = BytesIO() with rows.locale_context(output_locale):
# todo(dcramer): implement timing for tsdb </s> return statsd.timing(_get_key(key), value,	timing def timing(key, value): rate=settings.SENTRY_METRICS_SAMPLE_RATE)
# todo: also validate that subsystem is a </s> if parent_cache.subsystem.is_cut():	validate_parent_cache def validate_parent_cache(parent_cache): raise ValueError("parent_cache must be from an uncut subsystem")
# todo: remove conditional for v4 (always do the following) </s> path = purepath(path).parts	reject :obj:`pathlib.PurePath` instead. if not isinstance(path, (list, tuple)): root = deepcopy(struct) last_parent = root
# todo(soren): we need this until we can stop polling in the rpc code </s> self.cloud.terminate_instances(self.context, [instance_id])	test_rescue_unrescue_instance self.cloud.unrescue_instance(context=self.context, instance_id=instance_id)
# todo: check values? </s> values = set([v.lower() for v in values])	keep_alive @CheckFieldSyntax(TOKEN) def keep_alive(self, name, values): return values
# todo: add metadata support when it is merged from develop </s> return ret	_format_job_instance 'Target-type': job.get('tgt_type', []), 'User': job.get('user', 'root')}
# todo: we should store a storage version number in later releases. </s> if isinstance(state, tuple) and len(state) == 2:	__setstate__ def __setstate__(self, state): data, self.dtype = state self._initialize_data(data)
# todo: have to assume trans.user here... </s> user = trans.user	deserialize_annotation val = self.validate.nullable_basestring( key, val ) sa_session = self.app.model.context if val is None: item.delete_item_annotation( sa_session, user, item )
# @todo: filter dropdown to just those who are accepting volunteers </s> else:	customise_auth_user_controller f = current.db.auth_user.organisation_id f.comment = None from gluon import A, P, URL response = current.response
# todo: properly get whether there is a body from the response </s> finished.callback((response, none))	post finished = Deferred() if response.code == http.NO_CONTENT: else: response.deliverBody(Accumulator(finished))
self.asserttrue(greps(err, "unit zz-unknown.service does not exist, proceeding anyway.")) #todo </s> self.asserttrue(greps(err, "created symlink /etc/systemd/system/zz-unknown.service .* /dev/null"))	test_3811_mask_some_unknown if real: self.assertEqual(end, 0) else: self.assertEqual(end, 5)
# todo: change this -- func def changed </s> log_action_taken_on_message(self.user.id, 'uid1')	test_log_action_taken_on_message message = Message('subject', 'body', 'uid1', users=[self.user, self.user2], requires_response=True) db.session.add(message) self.assertEqual(len(list(message.history)), 1) self.assertEqual(message.history[0].action, MessageAction.respond)
# todo: implement me </s> input[i, :, 1:3, 1:3], patches[i, 10])	_test_b2_ch1_h3w3_ws2_stride1_padding1 assert utils.check_equal_torch(
# todo: delete this method when no longer needed </s> with open(path) as file:	get_test_graph def get_test_graph(path): text = file.read() tokens = tokenize_by_sentences(text)
assert false # todo </s> result[0] = not self.dirty.value	execute assert False # TODO elif exportFormat == ExportFormat.Npy:
@jtu.skip_on_devices("gpu", "tpu")  # todo(b/145608614): svd crashes on gpu. </s> def testsvd(self, b, m, n, dtype, full_matrices, compute_uv, rng_factory):	testSVD for compute_uv in [False, True] for rng_factory in [jtu.rand_default])) rng = rng_factory() _skip_if_unsupported_type(dtype)
# todo: for now, circumnavigate the detached head issue. </s> for subds in source.get_dataset_handles(recursive=true):	test_publish_simple def test_publish_simple(origin, src_path, dst_path): source = install(path=src_path, source=origin, recursive=True) AnnexRepo(opj(src_path, subds), init=True, create=False).git_checkout("master") target = GitRepo(dst_path, create=True)
# todo: check syntax, values? </s> pass	content_range def content_range(self, name, values):
# todo: test coverage of this branch </s> logger.exception(	unsubscribe_request instance.send_activation_email(action='unsubscribe') except Exception, e: 'Error %s while submitting email to %s.', e, instance.email)
# todo(b/147296819): delete this line. </s> get_code = input if _sys.version_info[0] == 3 else raw_input  # pylint: disable=undefined-variable	_gcloud_login if 'https' in prompt: break prompt = prompt.rstrip() code = get_code(prompt + ' ')
# todo add locales </s> raise yunohosterror("domain_name_unknown", domain=domain)	domain_set_settings domains = _load_domain_settings() if not domain in domains.keys(): setting_set = False if ttl is not None:
# todo(garyk): update the volumeops to read the state form the </s> instance.vm_state = vm_states.stopped	destroy if block_device_mapping: self._vmops.power_off(instance) for disk in block_device_mapping: connection_info = disk['connection_info']
# todo: until we get it working. </s> if facts.is_from_app_store():	generate_absolute_recipe by this function! keys = recipe["keys"] warn_about_app_store_generation(facts, recipe["type"]) return
pass # todo </s> def test_invalid_output_raises_error(self):	test_invalid_output_raises_error @unittest.SkipTest
# todo: probably warn about this. </s> records = self.groups.values()[0]	rules @property def rules(self): return records[0].rules
# @todo: what else? (uninitialized variables!) </s> else:	S3LocationSelectorWidget2 address_label = address_row[0][0] address_row = address_row[0][1] address_row = "" address_label = ""
# todo: handle this more gracefully? </s> self.close()	_pending byte = self.file.read(1) except socket.error: break if byte == '':
# todo: open in new tab </s> return redirect('/admin/django_q/schedule/add/?name=system_importer_file_csv_cron_based&func=dfirtrack_main.importer.file.csv_cron_based.system')	config_check_cron return redirect(reverse('system_list')) else:
# todo: pandas supports timedelta std, otherwise dask raises: </s> check_raises(dds, pds, 'var')	test_reductions_non_numeric_dtypes assert eq(dds.max(), pds.max()) assert eq(dds.count(), pds.count()) assert eq(dds.nunique(), pds.nunique())
# todo update this code once keras > 2.0.4 is released </s> try:	on_train_begin weights_h5 = self.WEIGHTS_H5.format(log_dir=logs['log_dir'], epoch=logs['epoch']) self.centers_ = keras.models.load_model( weights_h5, custom_objects=CUSTOM_OBJECTS,
# todo don't know how to get right return value </s> return 0xd10c	hook_SendMessageA }) def hook_SendMessageA(ql, address, params):
#todo: may be this should go in a decorator for use in every command. </s> options = ''	annex_get "--from=myremote" paths = '"' + '" "'.join(files) + '"' for key in kwargs.keys(): options += " --%s=%s" % (key, kwargs.get(key))
# logger ..." todo: this should be done before plugins discovery </s> for directory in config.config_dir, config.work_dir:	main args = create_parser(plugins, cli_args).parse_args(cli_args) config = configuration.NamespaceConfig(args) le_util.make_or_verify_dir( directory, constants.CONFIG_DIRS_MODE, os.geteuid())
# todo: really need crs specified properly in agdc-metadata.yaml </s> if projection['datum'] == 'gda94':	crs if crs: return str(crs) return 'EPSG:283' + str(abs(projection['zone'])) if projection['datum'] == 'WGS84':
#todo: investigate why the flask provided request.json returns none. </s> data = json.loads(request.data)	feeder_data @app.route('/ws/feeder_data', methods=['POST']) def feeder_data(): source_ip = 'a' source_port = 0
pass # todo </s> def _playlistfind(self, tag, needle):	_playlistfind @register(r'^playlistfind (?P<tag>\S+) (?P<needle>\S+)$')
# todo: fill some sane numbers here </s> return -1	fracUpload def fracUpload(self):
#todo change to native framework call, when plex allows token in header </s> request = urllib2.request(self.getlistsurl, headers=myheader)	LIST myHeader = {} myHeader['X-Plex-Token'] = users[user]['accessToken'] playlists = XML.ElementFromString(urllib2.urlopen(request).read()) result = {}
"""todo: document me.""" </s> self.pi = pi	__init__ def __init__(self, rng, pi, renormalize=False): assert self.pi.min() >= 0.0 self.rng = rng
#todo - use sql for this, much more efficient! </s> return value in self.adaptor.list_bioentry_ids(self.dbid)	__contains__ def __contains__(self, value):
# todo g.ind_edges = sub2ind(size(g.w), g.v_in, g.v_out) </s> g.v_in = v_i	adj2vec v_i, v_j = (sparse.tril(G.W)).nonzero() weights = G.W[v_i, v_j] G.v_out = v_j G.weights = weights
# todo: i dont like in place updates. change this to somthing else. </s> combined_arg_id_to_descr = arg_id_to_descr.copy()	ArgDescriptionInferer else: assignee_id_to_descr[-i-1] = ValueArgDescriptor() combined_arg_id_to_descr.update(assignee_id_to_descr) new_scoped_function = (
#todo: update this to try fsfindfolder and fallback to this </s> basepath = os.path.expanduser('/library/application support')	user_data_dir elif sys.platform == 'darwin': if os.uname()[-1] == 'i386': else: from Carbon import Folder, Folders
# todo: logging, or warning </s> print "cpickle has failed to write an object to " + filepath	_save cPickle.dump(obj, filehandle) except Exception, e: if str(e).find('maximum recursion depth exceeded') != -1: raise
# todo: datetime.date, datetimeindex? </s> if arr_typ == string_array_type:	lower_box_df res = pyapi.call_method(class_obj, "DataFrame", ()) for cname, arr, arr_typ in zip(col_names, col_arrs, arr_typs): arr_obj = box_str_arr(arr_typ, arr, c) else:
# todo: error checking </s> json_obj = json.loads(content)	list_boards headers = headers, ) return json_obj['boards']
# todo(dspasovski): fix this. </s> raise skiptest	TestIndexLanding @mock_es def test_good_cat(self): r = self.client.get(self.url) eq_(r.status_code, 200)
# todo remove this line? i think it's not needed. (dave) </s> save_module(grammar, path, module_node, lines, pickling=false,	parse old_lines = module_cache_item.lines if old_lines == lines: cache_path=cache_path) return module_node
# todo for now, simply ack msg_new.answer_msg_id </s> self._send_acknowledge(msg_new.answer_msg_id)	_handle_msg_new_detailed_info msg_new = reader.tgread_object() assert isinstance(msg_new, MsgNewDetailedInfo) return True
# todo for each sampled sub epoch, validate number of segments </s> valid = wpf.validate_weight(wp, curr)	TestWeightProof assert wp is not None assert len(wp.sub_epochs) == sub_epochs assert valid
pass  # todo... </s> def synthetic_gradient(self, x, synthetic_grad_x):	synthetic_gradient
# todo: improve error handling </s> return make_response("error", status_code=500)	handle_ws ) if status_code != 200:  # pragma: no cover return make_response("OK", status_code=200) elif event_type == "MESSAGE":
# todo: it has only one element in current state. handle rest of elements. </s> for v in cls.parent_context.py__getattribute__(foreign_key_class_name):	infer_django_field if value.name.string_name == 'str': foreign_key_class_name = value._compiled_obj.get_safe_value() return DjangoModelField(v, field).name else:
# todo: move this config validation to acl object. </s> def resolve_meter(action_conf):	resolve_names_in_acls def resolve_names_in_acls(): meter_name = action_conf assert meter_name in self.meters, (
# todo danger error log: unable to save in fs </s> raise errors.internalservererror	FileAdd relationship = dump_files_fs(files) except OSError, e: file_list = yield register_files_db(files, relationship, itip_id) for file_desc in file_list:
# todo: handle tuple assignment? </s> if meta_child.value.value:	_is_meta_with_abstract if not isinstance(meta_child.value, Const): continue return True return False
pass # todo </s> shows which commands the current user has access to.	_commands ``commands``
# todo this should be more modular </s> if isinstance(response, container_v1.types.listclustersresponse):	_get_targets if self.library_type == 'cloud_client_library': response = method(**list_params_combination) targets += response.clusters else:
# todo increase precision </s> eps = numpy.finfo(float).eps	check_degree ) exact_val = exact(k) alpha = abs(exact_val) * tol + (1e4+tol+exact_val)*eps if abs(exact_val - val) > alpha:
# todo debug </s> print ("sensor alert "	_updateRuleValuesRecursively % (sensorAlertAlertDelay - (time.time() - sensorAlertTimeReceived))) + "for sensor with id '%d' still delayed for " % ruleSensorId
# todo: test me </s> def update_counters(rex):	update_counters def wrapper(func): def wrapped(*args, **kwargs):
# todo resource arns may contain wildcards, e.g. arn:aws:iam::*:role/admin -- </s> session.run(	load_role_policies for policy_name, policy_data in policies.items(): for role_arn in _find_roles_assumable_in_policy(policy_data): ingest_policies_assume_role, RoleName=role_name,
# todo: fix this </s> status = 1	main status = main_loop.Batch(cmd_ev, c_parser, arena, is_main=True) except util.UserExit as e: return status
# todo: unit tests </s> user = auth.user	get_all_registrations_smart_folder @must_be_logged_in def get_all_registrations_smart_folder(auth, **kwargs): contributed = user.node__contributed nodes = contributed.find(
# todo: non-int dict </s> func_text += "    key_write_map = hpat.dictintint()\n"	agg_distributed_run func_text += "    output_{} = np.empty(n_uniq_keys, np.{})\n".format( i, agg_node.out_typs[col]) func_text += "    curr_write_ind = 0\n" func_text += "    for i in range(len(key_arr)):\n"
# todo implement for all channels </s> def _handle_toggle(self, message):	_handle_toggle return None
#web directories to use in case todo file is empty </s> start = [	main } CRAWLER_DIR = path.dirname(path.realpath(__file__)) 'https://en.wikipedia.org/wiki/List_of_most_popular_websites', 'http://www.clambr.com/49-free-web-directories-for-building-backlinks/'
# todo: test me </s> return len(self.node.files_versions[self.clean_filename])	latest_version_number def latest_version_number(self):
# todo: take care of theano to keras port: </s> ))	vgg16 net["in"], 2, "conv_1", 64, activation=activation, net.update(base.conv_pool( net["conv_1_pool"], 2, "conv_2", 128,
#todo: assert that layer inputs are always >= 0 </s> def __init__(self, *args, **kwargs):	ZPlusRule for alpha=1, beta=0, which assumes inputs x >= 0 and ignores the bias. super(Alpha1Beta0IgnoreBiasRule, self).__init__(*args, **kwargs)
# truffle todo: revert </s> for __x, __y, __z in walk(new_path, topdown, onerror, followlinks): yield __x, __y, __z	walk else: for new_path in walk_dirs: yield top, dirs, nondirs
# todo: provide loghandler, which gives access to recent records or return stderr </s> raise skiptest	ok_clean_git_annex_proxy except RuntimeError, e: if e.message.find("Failed to run 'git annex proxy") > -1: else: raise
# todo: remove in 0.24 when none is removed </s> def test_feature_union_warns_with_none():	test_feature_union_warns_with_none msg = (r"Using None as a transformer is deprecated in version 0\.22 and " r"will be removed in version 0\.24\. Please use 'drop' instead\.")
# todo: replace with a call to int(_, 16) </s> while count < 4:   # get 4 more characters	uEscape count = 0 value = 0 ch = argstr[j:j + 1].lower() j = j + 1
# todo review this </s> old_exitstatus = self.ctl.exitstatus	do_open return self.ctl.options.serverurl = url self.do_status('') self.ctl.exitstatus = old_exitstatus
# todo(twd2): send ac mail </s> pass	JudgeNotifyConnection if delta_accept != 0: if delta_accept == +1: if rdoc['tid']: post_coros.append(contest.update_status(rdoc['domain_id'], rdoc['tid'], rdoc['uid'],
# todo: waitlist </s> log.info(("queu[{index:2d}] @ 0x{address:06x}:  {name:14s} {available_items:2d} /"	infoQueue log.info("--------------------------------------------------------------------------") for queue in queuelist: " {free_slots:2d} / {capacity:2d}      {item_size:2d} Bytes    0x{queue_buf_start:06X}").format(**queue))
# todo test that non 9.[01].x errors out </s> version_info = [('postgresql 9.1.1 blah blah blah',)]	test_setupdb_app_main with mock.patch(self.psycopg2_module_path) as psycopg2: app = setupdb_app.SocorroDB(config) psycopg2.connect().cursor().fetchall.return_value = version_info result = app.main()
# todo: turn this into an id lookup on a keystore. </s> return self._power_ups[	pubkey_sig_bytes def pubkey_sig_bytes(self): try: keypairs.SigningKeypair].pub_key except KeyError:
# todo: only serialize parent object, use: </s> self.api.serialize()	remove if obj.parent is not None and obj.name in obj.parent.children: obj.parent.children.remove(obj.name) self.lock.acquire() try:
if self._ndim == 3: # todo: use hasz </s> array = c_double * 3	ctypes @property def ctypes(self): return array(self.x, self.y, self.z) else:
# todo: make sure this is indeed stdcall </s> self.ql.os.fcall = self.ql.os.fcall_select(stdcall)	io_Write irp.UserBuffer.value = input_buffer_addr self.ql.mem.write(irp_addr, bytes(irp)) self.ql.os.fcall.writeParams((self.ql.loader.driver_object.DeviceObject, irp_addr)) try:
# todo: tried to reuse the process, but something seems to be buffering </s> preprocessor = subprocess.popen([self.preprocessor],	__call__ def __call__(self, title, n=8, timeout=50.0): stdout=subprocess.PIPE, stdin=subprocess.PIPE,
# todo: specify page range </s> meta = {	import_from_pdf def import_from_pdf(filename_or_fobj, *args, **kwargs): filename, fobj = get_filename_and_fobj(filename_or_fobj, mode='rb') 'imported_from': 'pdf', 'filename': filename,
# todo: remove </s> c.group = context['group']	history c.group_revisions = self._action('group_revision_list')(context, data_dict) except (NotFound, NotAuthorized): abort(404, _('Group not found'))
# todo: enable specificity beyond hostname (e.g. include scheme, port) </s> tuf_configuration = \	make_tuf_updater def make_tuf_updater( url ): parsed_url = urlparse.urlparse( url ) TUFUpdater._tuf_configurations.get( parsed_url.hostname ) if tuf_configuration is None:
# todo implement and test this </s> return self.__getattr__(name)	__getitem__ - `name`: the name of the database to get
# todo! the offset is a bit off. </s> x_delta = child.offsets[0] - layer.offsets[0]	export_sprite_sheet pdb.gimp_image_set_active_layer(self.img, child) pdb.plug_in_autocrop_layer(self.img, child) y_delta = child.offsets[1] - layer.offsets[1] print('Offsets Delta: {x}, {y}'.format(x=x_delta, y=y_delta))
# todo handle error situations </s> self.logged_in.set()	on_logged_in def on_logged_in(self, session, error_type): self.logged_out.clear()
# todo find out what is best used here! </s> 'preferred_dtype' : none}	get_meta_information 'is_deterministic': True, 'handles_sparse': True,
# todo: add more checks here? </s> if magic in ['fws', 'cws']:	_isSWF if len(body) > 5: magic = body[:3] return True return False
# todo: make sure package names can't be changed to look like package ids? </s> return pkg	get if pkg == None: pkg = cls.by_name(reference)
# todo bring back? </s> mesh = meshtri(x, cells, flat_cell_correction=none)	laplace def laplace(X, cells, tol, max_steps, verbosity=0, output_filetype=None): interior vertex to the arithmetic average of its neighboring points. boundary_verts = mesh.get_boundary_vertices() if verbosity > 0:
# todo: add theano function. </s> raise notimplementederror()	gather_nd backend = K.backend() if backend == "theano": elif backend == "tensorflow": import tensorflow
# todo: remove when #980 has been merged </s> info.update(info['formats'][-1])	_real_extract 'description': compat_str(officialTitle), } results.append(info) return results
# todo: timestamp as datetime in payload must die. </s> if key == "timestamp":	VumiMessageDescriptor self._clear_keys(modelobj) for key, value in msg.payload.iteritems(): value = self._timestamp_to_json(value) full_key = "%s%s" % (self.prefix, key)
# todo: this won't work for classes with __slots__ </s> for c in mro:	_find_owning_class def _find_owning_class(mro, func_name): if func_name in c.__dict__: return '.'.join((c.__name__, func_name)), c
# todo: comment as to why this is </s> context = none	remove_context def remove_context(self, identifier): if context == self.identifier: self.remove((None, None, None), identifier)
# todo: must be implemented </s> pass	range_from_chapters def range_from_chapters(crawler, times=0):
# todo implement properties for proteins </s> genes_properties_dict = dict(	differential_expression nonz1_pro = (protein_exp[cell_idx1, :] > 0).mean(0) nonz2_pro = (protein_exp[cell_idx2, :] > 0).mean(0) raw_mean1=np.concatenate([mean1, mean1_pro]), raw_mean2=np.concatenate([mean2, mean2_pro]),
# todo test this </s> @property	aBl def aBl(self): if self._aBBl is None:
# todo: add .data and .grad to syft tensors </s> for p in nn_self.parameters():	module_float_precision_ def module_float_precision_(nn_self): parameters to normal float parameters""" p.float_precision_() return nn_self
# todo(blk-u): this doesn't look like it works as expected. </s> self.assertequal(	test_no_version_base_solidus def test_no_version_base_solidus(self): 'http://localhost:35357/', auth.replace_version('http://localhost:35357/', 'v2.0'))
annot.annotation_metadata.validation_and_reliability = "todo"  # todo </s> annot.annotation_metadata.origin = "centre for digital music"	fill_annotation_metadata annot.annotation_metadata.annotation_tools = "Sonic Visualizer" annot.annotation_metadata.annotation_rules = "TODO"  # TODO annot.annotation_metadata.annotator.name = "TODO" annot.annotation_metadata.annotator.email = "TODO"  # TODO
return response(status=400)  # todo </s> try:	register return Response(status=400)  # TODO if new_address in self.reserved_addresses: with ThreadedSession(self.db_engine) as session: existing = Recipient.query.filter_by(address=new_address).all()
box = self._canvas._render_world._selection_box  # todo: make a way to publicly access this </s> if box.select_state == 2:	_get_box def _get_box(self) -> Optional[Selection]: return Selection( (SubSelectionBox(
# todo(sbauza): remove the service_id filter in a later release </s> _filter = or_(models.service.host == models.computenode.host,	compute_node_statistics def compute_node_statistics(context): models.Service.id == models.ComputeNode.service_id) result = model_query(context,
# todo: optimize me </s> project_signals.write_permissions_revoked.send(self)	manage_contributors if to_remove or permissions_changed and ['read'] in permissions_changed.values():
def generate(self): # todo </s> return stateseq	generate_states self.stateseq = stateseq
# todo this is a workaround since exceptions are currently not correctly stacked </s> pass	match return self._search(self.pattern, string, pos, default(endpos, -1)) except RuntimeError: return self.__compile_cpython_sre().match(string, pos, default(endpos, maxsize()))
# todo: remove dependency on legacy_examples </s> caplog.set_level(logging.info, logger='customoperatorlogger')	test_my_custom_operator dagster_airflow_custom_operator_pipeline, caplog, ):  # pylint: disable=redefined-outer-name pipeline_name = 'demo_pipeline' operator = CustomOperator
# todo add test for this </s> input_was_2d = (len(image_fg.shape) == 2)	blend_alpha assert image_fg.dtype.name not in ["float128"] assert image_bg.dtype.name not in ["float128"] if input_was_2d: image_fg = np.atleast_3d(image_fg)
# todo: add comment here to explain these numbers </s> c.pump(repeat(1, 6))	test_scenario_throws_exception_when_rate_drops s = ReadRequestLoadScenario(c, client, 5, interval=1) d = s.start() def drop_requests(something): client.drop_requests = True
# todo: try to find a better way to deal with local execution </s> if self.is_method and not self.locations and self.owner == sy.hook.local_worker:	_execute_readable_plan self._self.send(sy.hook.local_worker, force_send=True) plan_res = self.execute_plan(args, result_ids) self._self.get() return plan_res
# todo: give a vanilla example </s> .. math::	feca def feca(predicted_power, df_appliances_ground_truth): fraction = \\sum_n min \\left (
# todo switch to pytest.skip() if verified to be expected failure </s> pytest.xfail(reason='test requires maxwell or higher')	test_conv_rand def test_conv_rand(backend_default, rand_convargs): if isinstance(NervanaObject.be, NervanaGPU) and NervanaObject.be.compute_capability < (5, 0): indim, nifm, fshape, nofm, batch_size, stride, rng_max, w_rng, pad = rand_convargs NervanaObject.be.bsz = batch_size
# todo: probabilistic with each route </s> if key == 'edge':	make_routes if not isinstance(inflow[key], str): inflow[key] = repr(inflow[key]) inflow['route'] = 'route{}'.format(inflow['edge']) del inflow['edge']
# todo: look for a better implementation handling "hold[expression]". </s> read_types = (	read_list_from_types else: read_types = (read_types,) Symbol("HoldExpression") if (
# todo: fancier. </s> self.lgr.info(	update_fetch "{}/jails/{}/root/dev".format(self.iocroot, uuid)] new_root = "{}/jails/{}/root".format(self.iocroot, uuid) "\n* Updating {} ({}) to the latest patch level... ".format( uuid, tag))
# todo: probably warn about this. </s> records = self.groups.values()[0]	rule @property def rule(self): return records[0].rules[0]
""" todo """ </s> pass	set_kernel_lts_systemdboot def set_kernel_lts_systemdboot(self):
# todo: reuse the utils method for service restarts </s> named_service_name = utils.named_service_name()	restart_service This syncs the bind server with it's new config files. Basically this restarts the service to apply the changes. dns_restart_command = "service %s restart" % named_service_name ret = utils.subprocess_call(dns_restart_command, True)
# todo: determine if this is object store safe and what needs to be </s> files_path = "%s_files" % input_file[0:-len(".dat")]	__upload_input_extra_files def __upload_input_extra_files(self, input_file): if os.path.exists(files_path) and self.job_inputs.path_referenced(files_path): for extra_file in os.listdir(files_path):
name: optional[str] = none) -> common_types.consistenttensortype:  # todo(b/64987151): remove # pytype: disable=annotation-type-mismatch </s> r"""generates a vocabulary for `x` and maps it to an integer with this vocab.	compute_and_apply_vocabulary file_format: Optional[common_types.VocabularyFileFormatType] = analyzers .DEFAULT_VOCABULARY_FILE_FORMAT, In case one of the tokens contains the '\n' or '\r' characters or is empty it will be discarded since we are currently writing the vocabularies as text
if isinstance(err, asyncio.cancellederror):  # todo: remove when updated to 3.8 </s> raise err	BlobAnnouncer log.debug("failed to announce %s, could only find %d peers, retrying soon.", blob_hash[:8], peers) except Exception as err: log.warning("error announcing %s: %s", blob_hash[:8], str(err)) async def _announce(self, batch_size: typing.Optional[int] = 10):
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_inputs_contents_invalid def test_fail_inputs_contents_invalid(self): ``inputs`` is a non-empty array, but it contains invalid values.
# todo: instead of attributing it to the word part, it would be </s> raise util.invalidslice(	_EvalBracedVarSub try: if begin < 0: "The start index of a string slice can't be negative: %d", begin, part=part)
# todo: #1823 - workaround for new nickname every restart </s> self.nickname, self.nickname_metadata = nickname_from_seed(self.checksum_address)	block_until_ready if staking_address != BlockchainInterface.NULL_ADDRESS and ether_balance: self._checksum_address = staking_address emitter.message(f"Starting services...", color='yellow', bold=True) break
# todo: this is processor specific </s> mnem = getmnem(ea)	is_ret def is_ret(ea): Check if the current instruction a RET instruction return re.match('ret', mnem)
# todo check transformation to the reference element </s> c = (mesh.coors[1:] + mesh.coors[:-1])/2  # center	sampleIC def sampleIC(self, mesh, ic, quad, basis): sic = nm.zeros((2, self.mesh.n_el, 1), dtype=nm.float64) s = (mesh.coors[1:] - mesh.coors[:-1])/2  # scale sic[0, :] = quad(lambda t: ic(c + t*s))/2
ret = os.system("cd " + path + " && " + gradlew + " assembledebug")  # todo: change to release </s> os.system("cp " + path + "/phimpme/build/apk/*.apk ./")	compile gradlew = path + "/gradlew" os.system("chmod 777 " + gradlew)  # TODO: Why os.chmod and chmod +x don't work? return ret, path + "/Phimpme/build/apk/Phimpme-debug-unaligned.apk"
return skiptest("test doesn't pass yet")  # todo(frostig) </s> def fun(x):	testLogSoftmax def testLogSoftmax(self): return x - np.log(np.sum(np.exp(x))) pfun, axis_name = papply(fun, 5)
# todo: consider adding some error handling for bad/failed requests. </s> km.record(email, event, properties if properties else {})	track_workflow km = KISSmetrics.Client(key=api_key)
# todo this paragraph is necessary, but not sure it works. </s> context = self._user_context.get_context()	get_completions return [(name, module) for name in importer.completion_names(self._evaluator, True)] elif isinstance(user_stmt, pr.Import): next(context)  # skip the path if next(context) == 'from':
# todo investigate different results between mac and linux/win platforms </s> retinfo.append("function: {:x} instruction: {:x} value:  {}".format(func.start, ins.address, str(ins.value)))	test_med_il_instructions retinfo.append("Function: {:x} Instruction: {:x} MLIL->LLILS:  {}".format(func.start, ins.address, str(sorted(list(map(str, ins.llils)))))) retinfo.append("Function: {:x} Instruction: {:x} MLIL->HLIL:  {}".format(func.start, ins.address, str(ins.hlil))) retinfo.append("Function: {:x} Instruction: {:x} Possible values:  {}".format(func.start, ins.address, str(ins.possible_values))) retinfo.append("Function: {:x} Instruction: {:x} Branch dependence:  {}".format(func.start, ins.address, str(sorted(ins.branch_dependence.items()))))
# todo: arrange </s> result = self.remote.find_profile({"name": "testprofile0"}, self.token)	test_find_profile def test_find_profile(self): Test: find a profile object self.assertTrue(result) assert 0
# todo(user): remove after 184 is out. </s> if not state.request_filenames:	FileOutputWriterBase state = cls._State.from_json(mapreduce_state.writer_state) filesystem = cls._get_filesystem(mapreduce_state.mapreduce_spec.mapper) finalized_filenames = state.filenames else:
# todo: common crud method </s> user = await self.middleware.call('datastore.query', 'account.bsdusers', [('id', '=', pk)], {'prefix': 'bsdusr_'})	UserService ) async def do_update(self, pk, data): if not user: raise ValidationError(None, f'User {pk} does not exist', errno.ENOENT)
# todo: replace usages of strictredis (redis-py 2.x) with redis in dramatiq 2.0. </s> self.client = client = redis.strictredis(**parameters)	RedisBroker self.dead_message_ttl = dead_message_ttl self.queues = set() self.scripts = {name: client.register_script(script) for name, script in _scripts.items()} def consume(self, queue_name, prefetch=1, timeout=5000):
# todo: internal configuration conflicts within one package. </s> matches = []	concretize s.external_path = get_path_from_module(s.external_module) self._mark_concrete() for x in self.traverse(): for conflict_spec, when_list in x.package_class.conflicts.items():
# todo: use other libraries. </s> start = addr('__libc_start_main')	get_bin_sh_str def get_bin_sh_str(): found = gdb.execute('find {}, +2000000, "/bin/sh"'.format( start), to_string=True)
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
# todo: time to say goodbye to the old mount namespace, see "man 2 unshare" to get some help </s> pid = os.fork()	run def run(image_name, image_dir, container_dir, command): container_id = str(uuid.uuid4()) if pid == 0: contain(command, image_name, image_dir, container_id, container_dir)
# todo pydocs </s> def __init__(self, service, project_id):	BigQueryBaseCursor class BigQueryBaseCursor(object): self.service = service self.project_id = project_id
# todo: more efficient copy </s> for i in range(span):	str_arr_slice_impl n_chars += len(_str) new_arr = pre_alloc_string_array(span, np.int64(n_chars)) new_arr[i] = str_arr[slice_idx.start+i*slice_idx.step] return new_arr
# todo: remove this in v1.8. </s> model.on_hpc_save(checkpoint)	hpc_save model = self.trainer.lightning_module checkpoint = self.dump_checkpoint() try: atomic_save(checkpoint, filepath)
# todo replace all that with ssh-copy-id </s> server.append("mkdir -p ~/.ssh")	install_keys hostname = hostname.split('/')[0] server = utils.ScriptRunner(hostname) server.append("chmod 500 ~/.ssh") server.append("grep '%s' ~/.ssh/authorized_keys > /dev/null 2>&1 || "
# @todo: make drop-down list of options </s> field = dtable.religion_other	customise_pr_person_controller field.readable = field.writable = True field.requires = vn_provinces field.label = T("Religion") field.readable = field.writable = True
writeable=is_writeable,      # everything is false for now, todo later </s> has_content=bool(channels),  # true if we found channels in it	find_kolibri_data_in_mountpoints id=drive.mountpoint, name=drive.mountpoint, channels=channels, )
# todo: check </s> y = np.repeat([1], repeats=pos_scores.shape[0])	_compute_loss :param neg_scores: :return: y = torch.tensor(y, dtype=torch.float, device=self.device) pos_scores = torch.tensor(pos_scores, dtype=torch.float, device=self.device)
status = 'published'  # todo: find a way for draft posts </s> yield (post.get('title'), post.get('body_cleaned'), slug, date,	posterous2fields date = date_object.strftime("%Y-%m-%d %H:%M") kind = 'article'  # TODO: Recognise pages post.get('user').get('display_name'), [], tags, status, kind, "html")
# todo: try removing the none checks after https://github.com/mozilla/rust-code-analysis/issues/528 is fixed. </s> if metrics["mi"]["mi_original"] is not none:	get_space_metrics obj["nexits_total"] += metrics["nexits"]["sum"] obj["cognitive_total"] += metrics["cognitive"]["sum"] obj["mi_original_total"] += metrics["mi"]["mi_original"] if metrics["mi"]["mi_sei"] is not None:
# todo: implement me </s> self.logmessage("%s %s" % (msg.__class__, vars(msg)), 4)	BranchLevel def BranchLevel(self, msg):
raise notimplementederror  # todo(mattjj) </s> else:	add_axis_to_aval return ShapedArray((size,) + aval.shape, aval.dtype)
# todo: unify with instagram, maybe in source.get_comment() </s> tag_id = self.tag_uri(comment_id)	get_comment activity = self._get_activity(None, activity_id) if activity: for reply in activity.get('object', {}).get('replies', {}).get('items', []): if reply.get('id') == tag_id:
# if pycryptodomex is not available, we are done </s> if not _aes:	run_test finally: crypto.AES = _aes return func(*args, **kwargs)
gc.collect()  # todo: see first comment above </s> cml.assert_logged(msg="finalizer called on: annexrepo(%s)" % path1,	test_AnnexRepo_flyweight with swallow_logs(new_level=1) as cml: del repo3 level="Level 1", regex=False)
#todo: code this </s> pass	setInSlot def setInSlot(self, slot, key, value):
# todo - fix meta.submission to point to real submission </s> self._remove_handled(meta.submission.submission)	remove_instance_matching_schema try: meta = Metadata.objects.get(raw_data=instance_id, formdefmodel=formdef_id) meta.delete() except Metadata.DoesNotExist:
# todo: house ad </s> promo_obj = none	attach_promo_data promo_obj = obj else: if gold_user: gold_promo = SupporterPromo.objects.filter(live=True,
# todo: make this more efficient </s> self._calc_seek(self._fake_seek)	write except IndexError: break  # EOF return total - left
# todo: log discarded bytes? </s> return 'request discarded due to invalid crc.'	decode_in if not self.valid_crc(self.in_data[1:]): self.in_parsing = False if self.in_data[2] in self.command_map: return self.command_map[self.in_data[2]]()
# todo store account here </s> return session	_create_session W.prnt("", "matrix: Created session for {}".format(sender)) self.sessions[sender].append(session)
# todo: should we check the arity? </s> template = data(template)	func data = [] elif callable(data): parse_tree = self.parse_string_to_tree(template, delims) data = [ data ]
oldsize = self.size # todo: remove </s> self.size = 8 + 4 + 4 + len(self.body[1]) * 4	stco_atom write_uint(stream, chunk_offset) def calsize(self): assert oldsize == self.size, '%s: %d, %d' % (self.type, oldsize, self.size) # TODO: remove return self.size
_, g_loss = self.trainable_gan.forward_loss()#todo targets=['d'] </s> self.gan.add_metric('g_loss', g_loss.mean())	g_grads else: _, g_loss = self.trainable_gan.loss.forward(d_real, d_fake)#TODO targets=['d'] g_loss += sum([l[1] for l in self.train_hook_losses(None, g_loss) if l[1] is not None]) g_loss = g_loss.mean()
# todo(b/181866850): enable tokenize_with_offsets when it works and test. </s> if use_sp_model:	test_shapes token_out_shape, "with batch_size=%s" % batch_size) self.assertFalse(hasattr(preprocess, "tokenize_with_offsets")) else:
# todo remove this paragraph, it's ugly and shouldn't be needed </s> inferred = self._name.infer()	infer context_set = self._name.infer() else: if inferred: inferred = next(iter(inferred))
# todo: requires special treatment? </s> current_unit = unit_line.variants[0].line[0]	_unit_line_to_game_entity :type unit_line: ..dataformat.converter_object.ConverterObjectGroup if isinstance(unit_line, GenieVillagerGroup): else: current_unit = unit_line.line[0]
train_critic = tf.train.adamoptimizer(1e-4).minimize(vloss) # todo: parameterize </s> return tf.group(train_actor, train_critic)	train train_actor = self.optimizer.optimize(actor_loss, actor_params, log_actor_probs, metric) print("Critic optimizer.")
# todo(b/141575627): we handle complex-dtype sum-reduction directly as a </s> outs = [xops.complex(all_reduce(xops.real(x)), all_reduce(xops.imag(x)))	_notuple_allreduce_translation_rule outs = [all_reduce(x) for x in args] else: if dtypes.issubdtype(c.get_shape(x).numpy_dtype(), np.complexfloating) else all_reduce(x) for x in args]
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> sampleparameters = {	test_acquire_token_with_user_pass def test_acquire_token_with_user_pass(self): "tenant" : "common", "authorityHostUrl" : "https://login.windows.net",
# todo: remove this after 2.4 as the switch to using underscores is a breaking change </s> peer_device_name = request.query_params.get('peer-device')	list peer_device_name = request.query_params.get(self._device_param.name) if not peer_device_name: peer_interface_name = request.query_params.get(self._interface_param.name) if not peer_interface_name:
# todo - this isn't actually the correct way to set the vary header, </s> response.headers['allow'] = ', '.join(self.allowed_methods)	dispatch except ErrorResponse, exc: response = exc.response response.headers['Vary'] = 'Authenticate, Accept' return self.emit(response)
# todo: this is a hack -- document can't be converted to string </s> if hasattr(self, 'config'):	_parse raise DoorstopError(msg) from None if not isinstance(data, dict): path = getattr(self, 'config') else:
#todo: handle this exception </s> except exception as ex:	dbg_step_until_ret except DieCallStackPopError as ex: self.logger.exception("Error while popping function from callstack:") self.logger.exception("Failed while stepping until return: %s", ex) finally:
# todo: comment as to why this is </s> context = none	__len__ def __len__(self, context=None): if context == self.identifier: if context is None: return self.__indicies[0].stat()["nkeys"] / 2
# todo find the correct hbin </s> d = hbincell(self._buf, offset, self.parent())	classname classname_length = self.unpack_word(0x4A) offset = self.abs_offset_from_hbin_offset(classname_offset) return struct.unpack_from("<%ds" % (classname_length), self._buf, d.data_offset())[0]
# todo: remove? </s> const.requisitionactions.request	process_transactions const.StockActions.STOCKONHAND, const.StockActions.STOCKOUT, ): balances.append(tx)
pass # todo </s> elif auto_guess:	SetBundleState auto_guess = False if local_override: pass # TODO else:
# todo(jk0): this will eventually need to take ssl into consideration </s> return "http://%s:%d" % (flags.glance_host, flags.glance_port)	_construct_glance_url def _construct_glance_url():
# todo: can convert to @abstractmethod once subclasses handle it </s> raise notimplementederror('this function not yet implemented')	iter_document_ids def iter_document_ids(self):
# todo: re-enable for hardware </s> for portno, config in list(interfaces_config.items()):	add_dp name, dp_config, port, interfaces_config, i, dpid_count, stack, n_tagged, tagged_vid, n_untagged, untagged_vid) stack = config.get('stack', None) if stack:
# todo: fix this </s> self.assertequal(dihedrals, self.forcefield.dihedrals)	test_ff_map self.assertEqual(angles, self.forcefield.angles)
#todo: add transaction cost here also </s> portfolio_value += price	new_stage_data portfolio += 1 elif action == 2: portfolio -= 1 elif action == 0:
# todo: accept ifs as a named arg?  split('a b', ifs=' ') </s> builtin_funcs.setglobalfunc(	ShellMain errfmt, debug_f) splitter = split.SplitContext(mem) mem, 'split', lambda s, ifs=None: splitter.SplitForWordEval(s, ifs=ifs)) globber = glob_.Globber(exec_opts)
# # fixme: # todo: remove me </s> try:	unpack_url faup.decode(url) url_unpack = faup.get() to_crawl['domain'] = url_unpack['domain'].decode() except:
# todo: test me </s> return total or 0	download_count )
# todo check that location is associated with this pipeline </s> ret = api.location(str(uuid)).patch({'disabled': true})	delete_location api = _storage_api() logging.info("Deleting storage location with UUID {}".format(uuid)) return ret['disabled']
# todo: show in display_problems() </s> show_invalid_depstring_notice(pkg, dep_string, str(e))	_add_pkg_deps uselist=self._pkg_use_enabled(pkg)) except portage.exception.InvalidDependString as e: del e continue
raise typeerror(msg.format(dtype))  # todo(mattjj, dougalm): handle complex </s> basis_array = onp.stack([onp.concatenate(	_elementwise_std_basis msg = ("Jacobian only defined for functions with floating input and output " "dtypes (i.e. dtypes that model real numbers), got {}.") [onp.ones(dims[j], dtype) if i == j else onp.zeros(dims[j], dtype) for j in range(arity)]) for i in range(arity)])
# todo: change this file format to be plain yaml and use safeloader </s> return yaml.load(open(filename), loader=yaml.unsafeloader)	from_yaml def from_yaml(self, filename):
# todo: hacked values for now </s> n_trees = 5 + int(round((x.shape[0]) ** 0.5 / 20.0))	fuzzy_simplicial_set metric_nn_descent = make_nn_descent(distance_func, tuple(metric_kwds.values())) n_iters = max(5, int(round(np.log2(X.shape[0])))) leaf_array = rptree_leaf_array(X, n_neighbors,
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: find a better solution for this? </s> run_command(["dvc", "config", "core.analytics", "false"])	project_init run_command(["git", "init"]) run_command(init_cmd)
# todo add write unlock </s> pass	read_current_batch self.storage_writer.flush_if_needed() finally: try: self.storage_reader.read()
# todo need copy? </s> x = mesh.node_coords.copy()	get_new_points def get_new_points(mesh): cells = mesh.cells["nodes"] jac_x = jac_uniform(x, cells)
print('warning: driver init returned none, {}'.format(ep.name))  # todo: use proper logger </s> return driver	safe_load return None if driver is None:
# todo: タイミング情報がとれるかをテストする </s> self.init_capture(source)	start_video_file raise Exception("Could movie file %s doesn''t exist." % source)
# todo: test timezone-aware datetime.date </s> self.assertequal(datetimefield.type, datetime.datetime)	test_DatetimeField def test_DatetimeField(self): from rows.fields import DatetimeField self.assertIs(type(DatetimeField.deserialize('2015-05-27T01:02:03')), DatetimeField.TYPE)
# todo: entropy loss </s> q = qs[i][0][actions[i]]	_train if off_policy: policy_loss -= (max(1 - args.trace_max / rho, 0) * policies[i].log() * (Qs[i].detach() - Vs[i].expand_as(Qs[i]).detach())).sum(1) value_loss += (Qret - Q) ** 2 / 2  # Least squares loss c = off_policy and min(rho[actions[i]], 1) or 1
# todo(liuyuhui) only xpu broadcast parameters here. </s> if parallel_helper._is_data_parallel_mode(	__call__ with program_desc_tracing_guard(False): self._build_once(*inputs, **kwargs) ) and paddle.is_compiled_with_xpu(): parallel_helper._broadcast_parameters(
# todo: put action definitions in a dict and loop here </s> if self.options.start:	mainloop matches.sort(key=sort_key, reverse=self.options.reverse_sort) if action_mode: action_name = "START" action = "start"
# todo: handle multiple skip stacks </s> (index, next_skip_stack), = skip_stack.items()	skip_first_whitespace next_skip_stack = None else: if isinstance(box, boxes.TextBox): assert next_skip_stack is None
# todo deprecated? </s> return html.unescape(text)	decode Renamed ``html`` parameter to ``text``. (Python gained a standard library module named :mod:`html` in version 3.4.)
# todo: move unset and set output in messages </s> self._assert_exec('echo 1', '1', log_captured)	test_session_channel self._assert_exec('echo 1', messages.channels.error_loading_channel_s % 'BOGUS', log_captured) self._assert_exec(':unset channel', 'channel is now unset', log_captured) self._assert_exec(':set channel LegacyCookie', 'channel = LegacyCookie', log_captured) self._assert_exec('echo 1', messages.terminal.backdoor_unavailable, log_captured)
# no todo item selected </s> pass	_complete_selected_item text_type(self.view.todolist.number(todo)))) except AttributeError:
# todo: fix singlemachinebatchsystem to support this call </s> self.batchsystem.issuejob('sleep 1', memory=10, cpu=1)	testGetIssuedJobIDs def testGetIssuedJobIDs(self): self.batchSystem.issueJob('sleep 1', memory=10, cpu=1) self.assertEqual([0, 1], self.batchSystem.getIssuedJobIDs())
# todo(b/163904067): mimic defun' behavior and reset the step seed to </s> resetstepseed()	If @tf.function(autograph=False) def ThenBranch(*args): inp = Pack(inputs, args) out = then_branch(inp)
# todo: where does this value come from? </s> del parameters["overlap"]	parse_yaml del parameters["name"] del parameters["step_down"] del parameters["path_pattern"] del parameters["strategy"]
# todo: remove when botfactory can force everything to be unthreaded </s> time.sleep(0.1)	test_isup_command_unparseable irc.pm(user, '.isup .foo') while bot.running_triggers: assert len(bot.backend.message_sent) == 1, ( '.isup command should output exactly one line')
# todo: this needs refactoring </s> if args.metadata_table:  # pragma: no cover	args_download assert args.refseq_category in EDefaults.REFSEQ_CATEGORIES.choices, \ "Unsupported refseq_category: {}".format(args.refseq_category) logging.info('Creating metadata file: %r', args.metadata_table) with open(args.metadata_table, 'wt') as metadata_table:
# todo: look in other supported bumpversion config locations </s> bumpversion = none	configure_bumpversion def configure_bumpversion(latest_version): bumpversion_config_path = Path('.bumpversion.cfg') if not bumpversion_config_path.exists():
# todo: don't write them? is *much* slower on re-load (~3x) </s> try:	update_module fh.flush() os.fsync(fh.fileno())  # flush to disk os.unlink(self.module_path + 'c') except OSError:
# todo remove? </s> if is_py3:	Literal return literal_eval(self.value) def __repr__(self): s = self.literal else:
# todo untested </s> 1/0	__setattr__ self._name, nid, _api.MBSTRING_UTF8, value, -1, -1, 0) if not add_result:
# todo: use complete list of `order_by` fields </s> table.order_by(order_by[0].replace('^', '-'))	print_ order_by = _get_field_names(order_by, table_field_names, permit_not=True) fobj = BytesIO() with rows.locale_context(output_locale):
# todo: remove when materialized paths are fixed in the payload returned from waterbutler </s> if not new_file.materialized_path.startswith('/'):	create_new_file if destination['provider'] != 'osfstorage': new_file.update(revision=None, data=data) new_file.materialized_path = '/' + new_file.materialized_path new_file.save()
# todo(andi) comment types should be unified, see related issue38 </s> return tlist.token_next_by(i=sql.comment, t=t.comment)	get_next_comment def get_next_comment():
# todo: use all of the followed objects as input to documentation. </s> context = next(iter(followed))	_get_node followed = self._name.infer() if followed: return context.get_node() return self._name.tree_name.get_definition()
# todo: requires special treatment? </s> current_unit = unit_line.variants[0].line[0]	_unit_line_to_game_entity :type unit_line: ..dataformat.converter_object.ConverterObjectGroup if isinstance(unit_line, GenieVillagerGroup): else: current_unit = unit_line.line[0]
# todo: serialize properly </s> return self.ok(importer)	RepoImporter try: importer = importer_manager.get_importer(repo_id) except errors.MissingImporter: serialized = http_error_obj(404)
#todo: consider factoring out: some duplication between xliff and tmx </s> notelist = self.getnotelist(origin="pofilter")	geterrors def geterrors(self): errordict = {} for note in notelist:
# todo(jheek): re-introduce this test when the tracer check is revived. </s> self.asserttrue(onp.all(r2 == random.fold_in(rng, 2)))	test_stochastic_rngs self.assertTrue(onp.all(r1 == random.fold_in(rng, 1)))
# todo postremora replace the above with this line when remora goes away </s> else:	register amo.utils.clear_messages(request) return http.HttpResponseRedirect(reverse('users.login') + '?m=3') messages.error(request, _(('There are errors in this form. Please ' 'correct them and resubmit.')))
# todo: make this regex verbose </s> return re.sub(r"((?<=[a-z)][.?!])|(?<=[a-z0-9][.?!]\"))(\s|\r\n)(?=\"?[a-z])",  # pylint: disable=c0301	sbd stripped = text.strip() if stripped: '\n', stripped) + end else:
# todo only if handle </s> self.handle = self.handle.lower()	save if not getattr(self, attr, None): setattr(self, attr, ponies[idx]) self.rsa_private_key = decode_if_bytes(self.rsa_private_key) self.rsa_public_key = decode_if_bytes(self.rsa_public_key)
# todo: deprecate `extra_info` in favor of `options` </s> if "reactions" not in options:	get_posts_by_search if 'pages' in kwargs: kwargs['page_limit'] = kwargs.pop('pages') options['reactions'] = kwargs.pop('extra_info', False) options['youtube_dl'] = kwargs.pop('youtube_dl', False)
# todo: see https://github.com/mozilla/openwpm/issues/867 for </s> service_args=["--marionette-port", "2828"],	start_webdriver firefox_binary=fb, options=fo, ) if load_browser_params is True:
# todo perform error checking if the optimized clause is used. </s> if seen_optimized:	retrieve_analyze_variables varnos = [core.bayesdb_variable_number( bdb, population_id, generator_id, v) for v in variables] if seen_variables: raise BQLError(bdb, 'OPTIMIZED incompatible with VARIABLES')
# todo: return should cause an 'exit 0' reply? </s> is_return, _ = cmd_ev.executeandcatch(node)	Headless_ECMD result = cast(parse_result__Node, UP_result) node = result.cmd return ''  # it's just 'OK '
# todo: move conversion to string to enhanced json handler and rather pass objects in trace.log() </s> trace.log(trace.storage_store_error, {	set_error except NotImplementedError: return False 'flow_name': flow_name, 'node_args': node_args,
pass # todo </s> def _rename(self, old_name, new_name):	_rename @register(r'^rename (?P<old_name>\S+) (?P<new_name>\S+)$')
# todo: run this loop in parallel ! </s> for x in train_set:	_sgd We update parameters after each minibatch according to standard stochastic gradient descent. grads = [] for param, grad in zip(self.params, self.f_grad(*x)):
# todo(dcramer): this doesnt handle concurrency </s> suite = testsuite(	_get_or_create_test_suite def _get_or_create_test_suite(self): build=self.build, project=self.build.project,
# todo get a fallback consumption rate based on product/facility type </s> return none	default_consumption def default_consumption(case):
x0 = np.zeros(self.cum_states[-1])  # todo: pre-allocate? </s> for sysidx in np.where(self.systems)[0]:	initial_condition @property def initial_condition(self): sys = self.systems[sysidx] state_start = self.cum_states[sysidx]
# todo: get pytest's coverage plugin working, iirc it has issues? </s> runner = "pytest"	test if module is not None: modstr = " tests/test_{}.py".format(module) if coverage: runner = "coverage run --source=paramiko -m pytest"
# todo: check that the birth date is not in the future </s> if calc_check_digit(number[:-1]) != number[-1]:	validate raise InvalidLength() birth_date = get_birth_date(number) raise InvalidChecksum() return number
# todo (in luxcore): </s> prefix = "scene.textures."	_node definitions["transparency"] = color elif node.bl_idname == "ShaderNodeMixRGB": definitions = {} fac_input = node.inputs["Fac"]
# todo(mattjj): remove this special case, used for debugging on cpu </s> dims = c.getshape(x).dimensions()	split_array def split_array(shape, x): if xb.get_replica_count() == 1: return c.Reshape(x, None, dims[1:]) else:
# todo: set numpy setflags </s> return self._data	data_ro_allocated @_allocate_memory def data_ro_allocated(self):
#todo - should the default be gapped(single_letter_alphabet) instead? </s> def clustaliterator(handle, alphabet = single_letter_alphabet) :	ClustalIterator The entire file is loaded at once, but the SeqRecord objects are only created "on request".
# todo not tested </s> _raise_current_error()	set_serial_number _api.X509_get_serialNumber(self._x509), small_serial) if set_result: else: asn1_serial = _api.BN_to_ASN1_INTEGER(bignum_serial[0], _api.NULL)
# todo: is there any way to divide up a single document into paragraphs? </s> return document	read_document document.append(sentence) sentence = []
# todo: consider returning an empty () rather than raising </s> def should_raise():	test_values_but_no_labels assert_raises(AttributeError, keyed_tuple.keys) assert_raises(AttributeError, keyed_tuple._asdict) keyed_tuple._fields assert_raises(AttributeError, should_raise)
# todo: support aa </s> aa = []	join po: Array = xp.vstack([x.po for x in sub_acts]) ah = list(map(xp.vstack, zip(*[x.ah for x in sub_acts]))) return cls(lh, po, ah, aa)
# todo: merge these </s> if cfg['vxcage']:	save_malware logging.info("{url} hashes to {md5}".format(url=url, md5=md5)) stored = False upload_vxcage(response, md5, cfg) stored = True
# todo present hw freq range </s> self.__osmo_device = osmo_device	__init__ **kwargs): Source.__init__(self, name=name, **kwargs) self.freq = freq = 98e6 self.correction_ppm = 0
# todo: do more stuff! </s> return d	_serialize_card 'id': card.id }
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: multi dp route resolver needs to flood out stack ports </s> self.host_ping(v100_host, first_faucet_vip.ip)	test_dp_contains_only_one_vlan_from_router self.add_host_route(v100_host, v200_host_ip, first_faucet_vip.ip) self.add_host_route(v200_host, v100_host_ip, second_faucet_vip.ip) self.host_ping(v200_host, second_faucet_vip.ip) self.host_ping(v100_host, v200_host_ip.ip)
# todo: check the data! </s> self.asserttrue(len(list(pipe)) > 0)	test_urlbuilder pipe_def = self._get_pipe_def(pipe_file) pipe = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def)
self.assertequals(self.todolist.count(), 1) # force won't delete subtasks </s> self.assertequals(self.output, "  2 bar p:1\nremoved: foo id:1\n")	test_del3 command.execute() self.assertTrue(self.todolist.is_dirty()) self.assertEquals(self.errors, "")
# todo: let the globe return the semimajor axis always. </s> return coords	ellipse coords += ([easting], [northing])
# todo: take care of nested resources </s> obj = cls()	from_dict @classmethod def from_dict(cls, d): for key, val in d.items(): if hasattr(obj, key):
# todo: value check </s> self._target_level = target_level	__init__ if not isinstance(target_level, int): raise TypeError('target_level must be of type: int') if not isinstance(lost_levels, bool): raise TypeError('lost_levels must be of type: bool')
svg = apply_template(svg, {"maxpoints":maxpoints, "trdn": trdn, "msg":"", "valuemid":"0.5", "timemid":"10s", "datapoints":"","init_max_y": "false", "max_y": 0, "seconds_scale":0, "y_shift": 0}) # todo templating engine </s> else:	plotwh print "GENERATE_ERROR" traceback.print_exc() svg = apply_template(svg, {"MAXPOINTS":MAXPOINTS, "TRDN": trdn, "MSG":"", "VALUEMID":"0.5", "TIMEMID":"10s", "DATAPOINTS":"","INIT_MAX_Y": "false", "MAX_Y": 0, "SECONDS_SCALE":0, "Y_SHIFT": 0}) # TODO templating engine if width and height: svg = svg.replace('height="210" width="610"', 'height="%s" width="%s"' % (height, width)) # TODO: switch to templating
# todo: rethink default remote/tracking branch. see above. </s> cmd_list = ["git", "pull"]	_update_repo if merge: lgr.info("Applying changes from tracking branch...") if remote: cmd_list.append(remote)
#todo: add type hints </s> pass	Surface_Base_Marker_Detector @abc.abstractmethod def detect_markers(self, gray_img):
# todo(leofang): how about ptds? </s> stream_ptr = stream.ptr if stream.ptr != 0 else none	__cuda_array_interface__ def __cuda_array_interface__(self): stream = cupy.cuda.get_current_stream() desc = { 'shape': self.a.shape,
# todo: get rid of the wrapping dict, see #24568 </s> ret[image.name] = {	image_show return {'Error': 'Unable to resolve image id'} image = nt_ks.images.get(id) 'id': image.id, 'name': image.name,
# todo: to be implemented </s> raise notimplementederror()	start _start_batch_ce(ce_name=ce_name, min_vcpus=min_vcpus, desired_vcpus=desired_vcpus, max_vcpus=max_vcpus) elif cluster_section.get_param_value("scheduler") == "slurm": else: LOGGER.info("Starting compute fleet: %s", args.cluster_name)
assert study_id == 0  # todo </s> self.trials[trial_id].params[param_name] = value	report_param def report_param(self, study_id, trial_id, param_name, value):
1  # todo: fill in identifier </s> )	profile_transfer amount, target, result.wait() profiling.print_all_threads()
assert self.restart_seq is none #todo: better handling of this situation </s> l.debug("initializing start sequence")	start_program assert self.start_seq is None #TODO: Better handling of this situation assert self.stop_seq is None #TODO: Better handling of this situation self.start_seq = sequence_controller() for p in self.roaster:
# todo: warn/error: check if this var has units: assigning </s> pass	_GeneralVarData def value(self, val): if type(val) in native_numeric_types: elif self.parent_component()._units is not None: _src_magnitude = value(val)
# todo: decide wtf to do here </s> return false	mindtouch_create_user return authtoken else: except HTTPError: return False
# todo account for all packages - this ignores the ones with different </s> versioned_packages = [	perform_download_packages_from_repository from admin.packaging import package_filename, PackageTypes rpm_version = make_rpm_version(intent.version) package_filename(package_type=PackageTypes.RPM, package=package,
# todo: remove when we stop supporting python < 3.5 </s> if sys.version_info.major < 3 or sys.version_info.minor < 5:	_PairsClassifierMixin y_predicted : `numpy.ndarray` of floats, shape=(n_constraints,) The predicted decision function value for each pair. check_is_fitted(self, 'preprocessor_') else:
# todo: make keys get passed through files or environment </s> cipher = self.defaultcipher	decrypt def decrypt(self, data): salt = None enc = data.split("\n")
# todo: remove hardcoded http </s> protocol = 'http'	send_campaign_email_subscriber def send_campaign_email_subscriber(email, subscriber, site, connection=None): unsub_path = reverse('subscribers:unsubscribe', kwargs={ 'mailing_list_uuid': email.campaign.mailing_list.uuid,
# todo: check if we can get values for "importados/indefinidos" </s> self.add_city_case(city="importados/indefinidos", confirmed=none, deaths=none)	parse total_confirmed += confirmed total_deaths += deaths self.add_state_case(confirmed=total_confirmed, deaths=total_deaths)
# todo: constants file for "broadcast" </s> self.socket.setsockopt(zmq.subscribe, 'broadcast')	__init__ self.socket = self.context.socket(zmq.SUB) if self.opts['zmq_filtering']: self.socket.setsockopt(zmq.SUBSCRIBE, self.hexid) else:
# todo: also preserve __module__, __name__ and a few other important attrs </s> return new_fn	profile return fp(*args, **kw) new_fn.__doc__ = fn.__doc__
#todo: implement extra options </s> py_options = self.check_options(options)	Read evaluation.message('Read', 'readf', from_python(typ)) return Symbol('$Failed') record_separators = py_options['RecordSeparators'] word_separators = py_options['WordSeparators']
# todo: an ir for ${} might simplify these lengthy conditions </s> pass	_EvalBracedVarSub if (suffix_op2 and suffix_op2.tag_() == suffix_op_e.Nullary and cast(Token, suffix_op2).id == Id.VOp0_a): elif var_name in _STRING_AND_ARRAY: bash_array_compat = True
# todo data should be bytes only </s> if from_base64:	Base64Ref self.type_name = 'base64' self.encoding = kwargs.get('encoding', 'original') self.data = data else:
# todo: python 2.4, 2.5 don't have io.bufferedreader!!! </s> in_file = gzip.gzipfile(input_file, 'r')	split_to_size compress = is_gzip(input_file) if compress: else: in_file = open(input_file, 'rt')
# xxx todo </s> raise notimplementederror()	get_postmortem_debugger elif bits not in (32, 64): raise NotImplementedError("Unknown architecture (%r bits)" % bits)
#todo: io plugins should assign default image formats </s> basename, ext = os.path.splitext(filename)	save_to_file if len(filename) == 0: return if not ext: filename = '%s.%s' % (filename, self.default_format)
#todo: this is just for backwards compatibility. it should be removed in v0.98 with p2.6 </s> flist = cp.get("formatters", "keys")	_create_formatters @classmethod def _create_formatters(cls, cp): if len(flist): flist = string.split(flist, ",")
# todo: what if we fail?  error-handling should be recorded someplace, </s> yield workitem.delete()	work workItemClass = WorkItem.forTable(table) workItem = yield workItemClass.load(txn, workID) yield workItem.doWork()
# todo: verify learning rule contents </s> self.asserttrue(fib_route_replies)	test_host_fib_route 'ipv4_dst': '10.0.0.4', 'echo_request_data': bytes('A'*8, encoding='UTF-8')}) self.assertFalse(self.packet_outs_from_flows(fib_route_replies))
# todo: need to make sure the `rootvars dict` </s> rootvars = bytes2str_in_dicts(rootvars)	jshead def jshead(engineConfig, rootvars): return u"\n".join(engineConfig + [u"var %s = %s;" % (k, json.dumps(v, indent=4)) for k, v in rootvars.items()])
# todo: make test method </s> sqlite3	test_sqlite3 except: return False return True
# '-rs',  # @todo: manually remove dependencies of conflicting packages, </s> '-r',	cli_install_packages 'sudo', 'pacman', '--noconfirm', ] + packages_to_be_removed,
# todo: waiting for a fix: https://developer.blender.org/t53509 </s> import bpy	convert node_tree = material.luxcore.node_tree if node_tree is None: for obj in bpy.data.objects: if len(obj.material_slots) > 0:
# todo remove arch dependent code </s> target_oprnd = asm.operands[0]	_extract_branch_target def _extract_branch_target(self, asm): address = None if isinstance(target_oprnd, X86ImmediateOperand) or \ isinstance(target_oprnd, ArmImmediateOperand):
if self._ndim == 3: # todo: use hasz </s> lgeos.geoscoordseq_getz(self._cseq, i, byref(dz))	next lgeos.GEOSCoordSeq_getX(self._cseq, i, byref(dx)) lgeos.GEOSCoordSeq_getY(self._cseq, i, byref(dy)) self.index += 1 return (dx.value, dy.value, dz.value)
# todo remove arch dependent code </s> target_oprnd = asm.operands[0]	_extract_branch_target def _extract_branch_target(self, asm): address = None if isinstance(target_oprnd, X86ImmediateOperand) or \ isinstance(target_oprnd, ArmImmediateOperand):
# todo: add keep parameter </s> return result	fillna result = DataFrame(self._kdf._internal.copy(sdf=sdf)).index
# todo use properties here to infer mechanism and purview from </s> return mip(direction=direction,	_null_mip @staticmethod def _null_mip(direction, mechanism, purview): mechanism=mechanism, purview=purview,
# steps = 0 # todo </s> luxcore_scene.deleteobject(src_name)	convert transformations = array("f", duplis.matrices) luxcore_scene.DuplicateObject(src_name, dst_name, count, transformations) print("Dupli export took %.3fs" % (time() - start)) except Exception as error:
accept_federated_only=self.federated_only)  # todo: 466 </s> listeners = self._learning_listeners.pop(node.checksum_public_address, ())	remember_node node.verify_node(self.network_middleware,  # TODO: Take middleware directly in this class? force=force_verification_check, address = node.checksum_public_address self.__known_nodes[address] = node
# todo: implement me </s> self.logmessage("%s %s" % (msg.__class__, vars(msg)), 4)	ChildDepth def ChildDepth(self, msg):
# todo: other "expected" error types to catch? </s> except pywikibot.error, err:	_save if not callback: raise logger.exception(u"Error saving page %s\n" % link) pywikibot.output(u"")
# todo: rewise once row_limit is working </s> simbad.timeout = 100	test_query_region_async def test_query_region_async(self, temp_dir): simbad = Simbad() simbad.cache_location = temp_dir response = simbad.query_region_async(
# todo - this should be replaced by official repo when </s> if base_url:	task_install_flocker "add-apt-repository", "-y", "ppa:james-page/docker"]), ] commands.append(run_from_args([ "add-apt-repository", "-y", "deb {} /".format(base_url)]))
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_interact_arguments "--max-dr-length", metavar="LENGTH", type=int, default=1024, help="give up scanning DR after LENGTH bits") p_scan = p_operation.add_parser( "scan", help="scan JTAG chain and attempt to identify devices",
# todo: handle boolean overrides </s> domains={network} if network else none,	ursula else: ursula_config = UrsulaConfiguration.from_configuration_file(filepath=config_file, registry_filepath=registry_filepath, provider_uri=provider_uri,
# todo: update once lakshmi's pr is merged </s> packs_base_path = '/opt/stackstorm'	run_sensors trigger_type_refs.append(trigger_type_ref) file_path = os.path.abspath(filename) virtualenv_path = os.path.join(packs_base_path, 'virtualenvs/',
# todo: move this scopes conversion from and to string into a utils function </s> scopes = self.oauth2_data.get('scopes', [])	get_initial def get_initial(self): initial_data = { 'redirect_uri': self.oauth2_data.get('redirect_uri', None),
# todo: need to fix this test </s> return	test_arg_min def test_arg_min(self): for axis in [0, 1]: node_def = helper.make_node("ArgMin", ["data"], ["reduced"],
#todo fix cache </s> print "used " + image.name + " as " + contact.account +"'s background image"	_download_background image.close()
# todo: configurable timeout??? </s> syndic_dict['dead_until'] = time.time() + 60	_forward_events except SaltClientError: log.error('Unable to return to {0}, trying another...'.format(master)) continue self._reset_event_aggregation()
# xxx todo </s> self.show_help_banner()	run_from_cmdline self.show_help_banner() elif len(args) == 0: elif len(args) == 2: config = args[1]
# todo: handle errors in future </s> self.body.add_statuses(future.result())	statuses_loaded_next def statuses_loaded_next(self, future):
# todo test </s> return self._item.aliases.keys()	keys_with_aliases def keys_with_aliases(self):
# todo - verify contents </s> self.assertequal(response.status_code, 200)	testSubmitterList response = self.client.get('/users/')
# todo(luotao): use clone() method to flush the program.desc in force, </s> program = program.clone()	_fuse_conv_relu_mkldnn self.block._remove_op(i + 1) i = i + 1
# todo not implemented yet </s> return	move_current_view_to_very_bottom Currently only supports 2 row or 2 column layouts. if self.window.num_groups() > 2: if self.window.num_groups() < 2: return
# todo: add ratio and percentage </s> out['diff'] = diff	make_summary c1, c2 = out.columns diff = out[c2] - out[c1] styled = out.style.applymap(color_neg_and_pos, subset=['diff']) else:
# todo support intloguniformdistribution </s> if isinstance(distribution, optuna.distributions.intloguniformdistribution):	_initialize_sigma0 sigma0 = [] for name, distribution in search_space.items(): raise NotImplementedError if isinstance(distribution, optuna.distributions.UniformDistribution):
# todo: may test with codecs.open passing an encoding </s> temp = tempfile.namedtemporaryfile(delete=false, mode='wb')	test_export_to_xls_fobj def test_export_to_xls_fobj(self): self.files_to_delete.append(temp.name) rows.export_to_xls(utils.table, temp.file)
# todo use the faster method </s> translation_project.flush_cache()	handle_all_stores def handle_all_stores(self, translation_project, **options): translation_project.get_stats() translation_project.get_mtime()
# todo add options to modify the columns </s> matrix.append([endpoint.machine.name.strip(),	do_remove if vlan.startswith('VLAN'): vlan.split('VLAN')[1] endpoint.endpoint_data['mac'], endpoint.endpoint_data['segment'],
# todo: fix </s> out = stdout.readlines()	_exec_command_ssh def _exec_command_ssh(self, cmd, internal): stdin, stdout, stderr = self.conn.exec_command(cmd) err = stderr.readlines() if internal:
pass  # todo </s> def train_check_calc_loss(self):	train_check_calc_loss
# todo: other types than can have series inside: list, set, etc. </s> return typ	if_series_to_array_type return types.Tuple( [if_series_to_array_type(t, replace_boxed) for t in typ.types])
# todo(rbharath): how does distance need to be modified here to </s> dists = tf.reduce_sum((tiled_coords - nbr_coords)**2, axis=3)	_compute_nbr_list atoms_in_nbr_cells = tf.gather(atoms_in_cells, neighbor_cells) nbr_coords = tf.gather(coords, atoms_in_nbr_cells) dists = tf.reshape(dists, [N, -1]) closest_nbr_locs = tf.nn.top_k(dists, k=M)[1]
# todo this closure is ugly. it also doesn't work with </s> module = self._evaluator.wrap(self._parser.module())	get_completions def get_completions(self, user_stmt, completion_parts): names, level, only_modules, unfinished_dotted = \ helpers.check_error_statements(module, self._pos)
# todo: remove this </s> warnings.warn(	intersects_vector :param vector: The look vector :return: Multiplier of the vector to the collision location. None if it does not collide f"intersects_vector is going to be removed\n{''.join(traceback.format_stack())}", DeprecationWarning,
# todo: leaf label is broken </s> leaf = leaf(i=i, d=depth, u=branch, x=x[i, :], n=n[i])	_mktree else: i = np.asscalar(np.flatnonzero(S2)) branch.r = leaf if I is not None:
return runtime.strarray(strs)  # todo: reuse this object too? </s> if name == 'call_source':	GetVar if source_name: strs.append(source_name) strs = [] for func_name, source_name, call_spid, _, _ in reversed(self.debug_stack):
pass # todo </s> def test_deliver_data(self):	test_deliver_data @SkipTest
# todo: handle poster </s> self.repeat = media.repeat	__init__ self.autoplay = media.autoplay
# todo: what decorator should we put here? in scipy.fftpack there is no planning, </s> def test_ifft2_plan(self, xp, scp, dtype):	test_ifft2_plan @testing.for_complex_dtypes() x = testing.shaped_random(self.shape, xp, dtype) x_orig = x.copy()
# todo: find another method to test this behavior without patching. </s> with mock.patch(	test_cp_with_error_and_warning_permissions self.http_response.status_code = 404 full_path = self.files.create_file('foo.txt', 'bar') 'awscli.customizations.s3.filegenerator.get_file_stat', return_value=(None, None)
# todo: assert metrics. </s> metrics = code_analysis_server.metrics(	test_get_metrics error = repository.get_metrics(commit, metrics["spaces"]) assert not error "file.cpp", int i = 1;
# todo sk: select standby db if necessary </s> shard_map = plproxy_config.get_django_shard_map()	get_shard_id_and_database_for_doc assert settings.USE_PARTITIONED_DATABASE, """Partitioned DB not in use, consider using `corehq.sql_db.get_db_alias_for_partitioned_doc` instead""" part_mask = len(shard_map) - 1 hash_ = ShardAccessor.hash_doc_id_python(doc_id)
raise notimplementederror # todo </s> def register_added_data(data):	register_added_data
# todo: add type and value checkings </s> self.width = width	__init__ def __init__(self, pinholes, width=None, height=None): super(DepthWarper, self).__init__() self.height = height self._pinholes = pinholes
# todo _why_? the user already logged in </s> b.fill_in("login", "user-el@example.org", wait=true)	test_signup_el b.visit(url) b.click_button("Επιβεβαίωση", wait=True) b.fill_in("password", "?P455W0rd!") b.click_button("Σύνδεση")
# todo replace with collections.sequence subclass </s> spotify.error.maybe_raise(self.error)	tracks def tracks(self): Will always return :class:`None` if the search isn't loaded. if not self.is_loaded: return None
#todo implement transformation for corner nodes </s> if 'quad8' in vecname:	data_in_material_coord new_vector.data[:, :, 3][:, check] = exy_theta * 2. new_vector.data[:, :, 4][:, check] = thetadeg_new for i in [2, 3, 4, 5, 6, 7, 8, 9]: new_vector.data[:, i, :] = 0
fp = open("%s.py" % name, "w")   #todo confirm file overwrite </s> print >>fp, pipe2py.compile.parse_and_write_pipe(self.context, pipe_def, pipe_name=name)	test_submodule_loop pipe_def = self._get_pipe_def("%s.json" % name) try: fp.close() pipe_def = self._get_pipe_def("pipe_b3d43c00f9e1145ff522fb71ea743e99.json")
# todo: check if scheme can be 'http' for http/2 ? </s> (':scheme', 'https'),	_get_request_headers (':method', self._request.method), (':authority', url.netloc), (':path', url.path), ]
pass # todo </s> def _count(self, tag, needle):	_count @register(r'^count (?P<tag>\S+) (?P<needle>\S+)$')
# todo could keep_trailing_newline fix this better? </s> f.write(b'\n')	run with open(output, 'wb') as f: f.write(rendered.encode('utf-8')) if not os.path.join(xml_input, 'indexpage.xml') in xml_files_metadata: compound = Empty()
# todo: timeout should be removed when the engine is fixed so that it never hangs. -saul </s> try:	_shutdown_subsystems engine = Engine() engine.stop() with api.timeout(15): while True:
pass # todo </s> def startcdata(self):	startCDATA
# todo: allow to specify arguments </s> args_list = _parse_args_from_manifest(manifest, 'upgrade')	app_upgrade for hook in os.listdir(app_tmp_folder +'/hooks'): hook_add(app_id, app_tmp_folder +'/hooks/'+ hook) args_list.append(app_id) os.system('chown -hR admin: %s' % install_tmp)
+ ansi.ansi_normal  # todo: why does it keep it? </s> + "foo"))	test_re_underline parser.re_underline( "a " + ansi.ANSI_UNDERLINE + "red"
# todo(kurts): extract all this pinject-decorated fn stuff to a </s> if hasattr(cls.__init__, _is_decorator_attr):	_provide_class init_kwargs = {} if type(cls.__init__) is types.MethodType: arg_names, unused_varargs, unused_keywords, unused_defaults = ( inspect.getargspec(getattr(cls.__init__, _ORIG_FN_ATTR)))
# todo: migrate to where-in subqueries? </s> statements = ['''	set_final_state def set_final_state(self, final_state): self.set_state(final_state) UPDATE history_dataset_collection_association SET update_time = :update_time
rm.fetch(refspec=refspec, progress=progress)  # todo: progress +kwargs </s> else:	fetch rm.fetch(refspec=refspec, progress=progress)  # TODO: progress +kwargs
# todo: do this resampling in the pipeline? </s> truth_chunk = truth_chunk.resample(period_alias)	mean_normalized_error_power periods=[pred_chunk.timeframe], chunksize=1E9) truth_chunk = next(truth_generator) pred_chunk = pred_chunk.resample(period_alias) diff = (pred_chunk.icol(0) - truth_chunk.icol(0)).dropna()
# todo cache? </s> def get_model(last=0) -> model:	get_model return Model(_get_exports()[-last:])
# todo: fix this </s> if dataset.corpus == 'timit':	eval_phone dataset.reset() model = models[0] map2phone39 = Map2phone39(label_type=dataset.label_type, map_file_path=dataset.phone_map_path)
# todo: make sure this has test coverage </s> raise http404	newsletter_detail newsletters = Newsletter.on_site.filter(visible=True) if not newsletters: return list_detail.object_detail( request, newsletters, slug=newsletter_slug)
# todo: kkrampa, shouldn't we wait to save the checkpoint until after we've processed all the data? </s> save_stock_data_checkpoint(checkpoint,	sync_delivery_group_report report_date__gte=date), facility=facility) 'delivery_group', meta.get('limit') or limit,
except exception:  # todo: what could happen here? </s> raise runtimeerror("could not migrate pickle file from .txt extension to .p extension.") from none	fix_extension_on_pickles if os.path.isfile(txt): os.rename(txt, (txt[:-4] + '.p'))
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo uncomment when gr-10346 will be fixed </s> self.assertraises(valueerror, math.acosh, 0)	testAcosh self.assertRaises(TypeError, math.acosh) self.ftest('acosh(1)', math.acosh(1), 0) self.assertRaises(ValueError, math.acosh, -1) self.assertEqual(math.acosh(INF), INF)
# todo remove when below works </s> authorel = subelement(modelconf, 'author')	modelConf SubElement(modelconf, 'version').text = 'DUMMY' SubElement(modelconf, 'sdf', version=sdfversion).text = model['name'] + '.sdf' SubElement(authorEL, 'name').text = "DUMMY" SubElement(authorEL, 'email').text = "dummy@dummy.mail"
# todo: kerberos login </s> smb_connection.login(self._user, self._password, self._domain,	_get_groupsxml file_name = '\\'.join(groupsxml_path_split[4:]) smb_connection = SMBConnection(remoteName=target, remoteHost=target) self._lmhash, self._nthash) smb_connection.connectTree(share)
# todo: should we enable auto-retry, </s> producer.publish(msg, exchange=exchange, **kwargs)	do_publish elif exchange is not None: maybe_declare(exchange, channel)
# todo: use domain and port </s> client = mongoclient('localhost', settings.db_port)	share_db def share_db(): return client.sharejs
# todo: requires same key or none key </s> raise notimplementederror	EdgeStorage raise NotImplementedError def has_self_loops(self) -> bool: def is_undirected(self) -> bool: raise NotImplementedError
# todo: this is horrible </s> if len(setting) == 0 or setting[0] == '#':	load_floorc fd.close() for setting in default_settings: continue try:
# todo: remember to delete </s> elif demisto.command() == 'zerofox-fetch-incidents':	main elif demisto.command() == 'fetch-incidents': fetch_incidents() fetch_incidents_command() except Exception as e:
raise exception('not valid monitor')  # todo: custom exception </s> monitor, name = monitor_tuple	_add_monitor_from_tuple def _add_monitor_from_tuple(self, monitor_tuple): if len(monitor_tuple) != 2: return self.add_monitor(monitor=monitor, name=name)
# todo(ytknzw): add more specific assertion with the test case. </s> figure = plot_parallel_coordinate(study)	test_plot_parallel_coordinate assert figure.has_data() is False study = prepare_study_with_trials(with_c_d=False) assert figure.has_data() is True figure = plot_parallel_coordinate(study, params=["param_a"])
# todo is the processor the correct place to set this? </s> keras.backend.set_image_dim_ordering('th')	SevenPlaneProcessor consolidate=consolidate, use_generator=use_generator) def feature_and_label(self, color, move, go_board, num_planes): Parameters
# todo: do something more than simply selecting the last match? </s> hash_ = self.hash_schema(schema)	_get_constructor def _get_constructor(self, schema): matches = self.class_dict[hash_] return matches[-1] if matches else self._passthrough
# todo(b/142683826): beam type check error in </s> def process(	_MergeBootstrap class _MergeBootstrap(beam.DoFn): self, element: Tuple[slicer.SliceKeyType, Iterable[Dict['Text', Any]]], unsampled_results: Dict[slicer.SliceKeyType, Dict[Text, Any]]
# todo: the contents of <question_text> element used to be broken in </s> questions, "boolean_question", id=rule.id_ + "_question")	export_ocil_to_file boolean_question = ET.SubElement(
# todo: loop or recurse here since a select may have multiple </s> select_info = queries.get(wildcard.table)	_eval return None wildcard = wildcards[0] if not select_info: return LintResult(anchor=queries[None].select_statement)
# todo: define specific exception types (instead of general type) </s> raise exception("action %s, which is required for %s is not known." % (action_key, machine.getkey()))	addRequiredAction self._required_actions[machine] = set(action_key) else:
# todo - remove the -word_size argument as per blast+ 2.2.30 </s> self.parameters = [	NcbirpstblastnCommandline subprocess module, as described in the Biopython tutorial. def __init__(self, cmd="rpstblastn", **kwargs): _Option(["-strand", "strand"], Values allowed are "both" (default), "minus", "plus".""",
'inception_v4'  : [testmodels.cntkemit, testmodels.coremlemit, testmodels.kerasemit, testmodels.pytorchemit, testmodels.tensorflowemit], # todo testmodels.mxnetemit(small error), testmodels.caffeemit(crash for shape) </s> 'resnet152'     : [testmodels.caffeemit, testmodels.cntkemit, testmodels.coremlemit, testmodels.kerasemit, testmodels.mxnetemit, testmodels.pytorchemit, testmodels.tensorflowemit],	get_test_table 'alexnet'       : [TestModels.CaffeEmit, TestModels.CntkEmit, TestModels.CoreMLEmit, TestModels.MXNetEmit, TestModels.PytorchEmit, TestModels.TensorflowEmit], # TODO: TestModels.KerasEmit('Tensor' object has no attribute '_keras_history') 'inception_v1'  : [TestModels.CaffeEmit, TestModels.CntkEmit, TestModels.CoreMLEmit, TestModels.KerasEmit, TestModels.MXNetEmit, TestModels.PytorchEmit, TestModels.TensorflowEmit], 'squeezenet'    : [TestModels.CaffeEmit, TestModels.CntkEmit, TestModels.CoreMLEmit, TestModels.KerasEmit, TestModels.MXNetEmit, TestModels.PytorchEmit, TestModels.TensorflowEmit], 'xception'      : [TestModels.CoreMLEmit, TestModels.CntkEmit, TestModels.MXNetEmit, TestModels.PytorchEmit, TestModels.TensorflowEmit], #  TODO: Caffe(Crash) TestModels.KerasEmit(too slow)
# todo: perform this part concurrently. </s> getter = self.server.get(ursula_interface_id)	follow_treasure_map def follow_treasure_map(self, pfrag): for ursula_interface_id in self.treasure_maps[pfrag]: loop = asyncio.get_event_loop() value = loop.run_until_complete(getter)
#todo trap and ignore attributeerror here? </s> else:	pipe_itembuilder if "subkey" in dk: key = reduce(lambda i,k:i.get(k), [item] + dk['subkey'].split('.')) #forces an exception if any part is not found key = dk['value'] if "subkey" in dv:  #todo: use this subkey check anywhere we can embed a module
# todo: support for multiple message versions </s> except exception.stopextraction:	Job self.extractor.category, msg[1] ) pass def handle_url(self, url, kexwords):
# todo: don't mess with the user's cursor. </s> sel = self.view.sel()	on_navigate def on_navigate(self, href, point, diagnostics): sel.clear() sel.add(sublime.Region(point, point))
# todo eh. traverse all of filesystem?? or only specific dirs for now? </s> raise ve	iter_multi_commits else:
# todo: try mlp rather than bilinear </s> self.logits_second = tf.transpose(bilinear(emb_first, emb_node, name='logits_second'), [0, 2, 1])	_init emb_first_real = tf.boolean_mask(emb_node, mask_real) emb_first_real = tf.expand_dims(emb_first_real, axis=1) self.logits_second = tf.squeeze(self.logits_second, axis=-1) ac_first_mask = tf.one_hot(ac_first, depth=tf.shape(emb_node)[1], dtype=tf.bool, on_value=False, off_value=True)
# todo some of these types could be filled in better </s> self.assertisinstance(one_row_complex.c.boolean.type, types.boolean)	test_reflect_select except ImportError: from sqlalchemy.databases.mysql import MSBigInteger as BigInteger self.assertIsInstance(one_row_complex.c.tinyint.type, types.Integer) self.assertIsInstance(one_row_complex.c.smallint.type, types.Integer)
# todo remove .as_posix when requiring python 3.6 </s> with open(node_filename.as_posix(), "w") as fh:	write if mesh.points.shape[1] != 3: raise WriteError("Can only write 3D points") fh.write("# This file was created by meshio v{}\n".format(__version__)) fh.write("{} {} {} {}\n".format(mesh.points.shape[0], 3, 0, 0))
# todo: need to replace the five "get" functions </s> def get_epg_name():	get_epg_name return ['epg_1', 'epg_2']
# todo: remove when migration plan / state is passed (#24100). </s> if not is_latest_migration_applied('auth'):	create_permissions def create_permissions(app_config, verbosity=2, interactive=True, using=DEFAULT_DB_ALIAS, **kwargs): return if not app_config.models_module:
# todo torches, redstone torches, crops, ladders, stairs, </s> if blockid in (66,): ## minecart track	chunk_render try: blockid = blocks[x,y,z] ancilData = blockData_expanded[x,y,z] t =  textures.generate_special_texture(blockid, ancilData)
# todo: allow numerical indices, and "+" for append </s> namespace = meta	assign_fields if val and val[0] in "+-" and val[1:].isdigit(): val = int(val, 10) for key in field.split('.')[:-1]: namespace = namespace[key]
# todo: universal? </s> self.send_data(d)	set_setting for d in data:
# todo xxx graalvm change </s> raise unittest.skiptest("missing threading support")	ThreadedEchoServer self.daemon = True def __enter__(self): self.start(threading.Event()) self.flag.wait()
# todo maybe not here but in an own function? </s> if self.name == 'part':	setup ] self.update_headers() self.REQUIRED_HEADERS = [ 'Name',
# todo: refactor </s> self.mark_gravity("input_start", tk.right)	_apply_io_event self._squeeze_buttons.add(btn) create_tooltip(btn, "%d characters squeezed. " % len(data) + "Click for details.") self.mark_gravity("output_insert", tk.RIGHT) self.window_create("output_insert", window=btn)
# todo: send alert </s> blacklist_whitelist_notification.delay(4) # notice_type = 4 whitelist	chk_prefix_in_whitelist break if flag: return True return False
# todo: are we following this in the spec? </s> tag = r"""	_compile_regexps def _compile_regexps(self): tag_types = "!>&/#^" (?P<content>[\s\S]*?) (?P<whitespace>[\ \t]*)
# todo test </s> self.unconstrained_effect_repertoire(purview))	effect_info return utils.emd(self.effect_repertoire(mechanism, purview),
# todo username </s> sudo = args.pushy('ssh+sudo:{hostname}'.format(hostname=hostname))	purge_data ) for hostname in args.host: LOG.debug('Purging data from host %s ...', hostname) purge_data_any_r = sudo.compile(purge_data_any)
# todo: no-op component? </s> stage_op = none	update_from_memory states, terminals, action_probs_mu, initial_internal_states = self_.call(staging_area.unstage) else: states = self_.call(preprocessor.preprocess, states) state_values_pi, logits_pi, probs_pi, log_probabilities_pi, current_internal_states = \
# todo this is a bit jankey to be honest </s> if apps_changed:	ready settings.INSTALLED_APPS += [plugin_path] apps_changed = True apps.app_configs = OrderedDict() apps.apps_ready = apps.models_ready = apps.loading = apps.ready = False
raise exceptions.mpdnotimplemented  # todo </s> .. versionadded:: mpd protocol 0.19	listneighbors OK
# '-rs',  # @todo: manually remove dependencies of conflicting packages, </s> '-r',	_remove_conflicting_packages 'sudo', 'pacman', '--noconfirm', '--nodeps',
# todo: raise something more specific or catch earlier </s> raise exception	_make_report_filter filter["type"] = "date" else: return filter
# todo: logging </s> return redirect(self.get_success_url())	post self.login_form.save() messages.success(request, _('Your changes have been saved.')) elif self.profile_form.is_bound: if self.profile_form.is_valid():
#todo - handle start/end coordinates properly. short term hack for now: </s> record._al_start = int(query_annotation["al_start"])	next record.name = "query" record.annotations["original_length"] = int(query_annotation["sq_len"]) record._al_stop = int(query_annotation["al_stop"]) if alphabet == single_letter_alphabet and "sq_type" in query_annotation :
# todo: remove in 1.2 release </s> run_deprecated_edit_bird_hook(request, items)	for_moderation RejectModerationEditPageItem(PageRevision.objects.get(id=revision_id)), ] for fn in hooks.get_hooks('construct_wagtail_userbar'): fn(request, items)
#    # todo(soren): this is what the compute manager does when you </s> if state != power_state.running:	init_host {'name': instance['name'], 'state': state}) db.instance_set_state(ctxt, instance['id'], state) continue self.firewall_driver.prepare_instance_filter(instance)
# todo lookup tracks in batch for better performance </s> tracks = context.core.library.lookup(ref.uri).get()	lsinfo result.append(('directory', ref.uri[1:])) elif ref.type == 'track': if tracks: result.extend(translator.track_to_mpd_format(tracks[0]))
# todo: if needed allow other handling (like adding values) </s> np.maximum(filt.filter, filt_pos, out=filt_pos)	_put_filter stop = len(band) filt_pos = band[start:stop]
# todo: this self.size enforces a 2**64 limit to array size </s> linear_loc = self.linear_loc()	resize raise ValueError('negative dimensions not allowed') new_size = reduce(operator.mul, shape, 1) end_idx = np.searchsorted(linear_loc, new_size, side='left') linear_loc = linear_loc[:end_idx]
""" todo: documentation </s> try:	_execute def _execute(self, operation, reader, writer): for record in operation(self): writer.writerow(record)
# todo: this test requires manifold access, see: t88318502 </s> self._test_model("coco-panopticsegmentation/panoptic_fpn_r_50_3x.yaml")	testPanopticFPN def testPanopticFPN(self):
# todo don't know how to get return value </s> return 0xd10c	hook_DefWindowProcA }) def hook_DefWindowProcA(ql, address, params):
# todo: verify/modify these lists </s> version_changing_attrs = ["xpath", "version", "xmlns"]	testBasicAttributes def testBasicAttributes(self): filled = self._get_basic_formdef() nonversion_changing_attrs = [ "name", "short_name", "uiversion"] for attr in version_changing_attrs:
#todo - fix the trailing space! </s> self.assertequal(str(cmdline).rstrip(), muscle_exe + \	test_simple_clustal cmdline = MuscleCommandline(muscle_exe, input=input_file, stable=True, clw = True) " -in Fasta/f002 -clw -stable") self.assertEqual(str(eval(repr(cmdline))), str(cmdline))
annot.annotation_metadata.validation_and_reliability = "todo"  # todo </s> annot.annotation_metadata.origin = "cerulean mountain trust"	fill_annoatation_metadata annot.annotation_metadata.annotation_tools = "Sonic Visualizer" annot.annotation_metadata.annotation_rules = "TODO"  # TODO annot.annotation_metadata.annotator.name = name annot.annotation_metadata.annotator.email = "TODO"  # TODO
'''todo: add docs''' </s> pass	AppModel self.columns = descriptors def update(self): @property def continuous_columns(self):
# todo: require tests </s> return true	hessian_is_zero def hessian_is_zero(self):
#todo implement qgis2.0 variant </s> pass	getRasterLegend myLastValue = myValue else: return self.legend
# todo: use triple factory </s> rotate.forward_owa(torch.zeros(16, 3, dtype=torch.long))	test_rotate rotate = RotatE(triples_factory=self.factory) self.assertIsNotNone(rotate) rotate.forward_cwa(torch.zeros(16, 2, dtype=torch.long)) rotate.forward_inverse_cwa(torch.zeros(16, 2, dtype=torch.long))
(timestamp - relativedelta(days=(90 if histogram_type == 'active_cases' else 30))).isoformat(),  # todo - add to configs </s> timestamp.isoformat(),	_histo_data_non_cumulative [domain_name_data], histogram_type, filters )
# todo treat boundary conditions </s> return inner_facet_vals, outer_facet_vals, whs	get_both_facet_qp_vals dofs[facet_neighbours[:, facet_n, 0]][None, :, :, 0] * facet_bf[:, 0, facet_neighbours[:, facet_n, 1], 0, :], axis=-1).T
# todo: exclude tests which fail to import: e.g. on windows </s> exec 'import ' + t	collectTestSuites tests += optional_tests for t in tests: return dict([(t[5:], eval(t + '.suite()')) for t in tests ])
# todo; to include configurablereportkafkapillow features </s> return constructedpillow(	get_ucr_es_case_pillow checkpoint_callback=ucr_processor ) name=pillow_id, topics=topics.FORM_TOPICS,
# todo: validate grant </s> db = connect()	grant_revoke CLI Example:: salt '*' mysql.grant_revoke 'SELECT,INSERT,UPDATE' 'database.*' 'frank' 'localhost' cur = db.cursor() query = "REVOKE %s ON %s FROM '%s'@'%s';" % (grant, database, user, host,)
# todo(nnorwitz): need to ignore friend method declarations. </s> if isinstance(node, ast.function):	PrintFunctions try: for node in builder.Generate(): print node.name except:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo(adu): delete when it will no longer be used </s> if not resp_body:	chunk_fetch HEADER_PREFIX + 'list-truncated') if truncated is None: break truncated = True
# todo: should not we see here actual texts in log too? </s> assert "conditionnotmatchederror: condition not matched" in error.value.msg	test_should_have_no_texts_exception assert "has no texts ('Alex', 'Yakov')" in error.value.msg
# todo: add hook to be called after each song </s> except exception as e:	MusicBot try: songs_added = await player.playlist.async_process_youtube_playlist(playlist_url, channel=channel, author=author) traceback.print_exc() raise CommandError('Error handling playlist %s queuing.' % playlist_url)
# todo(shardy): may be able to remove when the bug above is fixed </s> client = nc.client(no_cache=true, **args)	nova client = None try: client.authenticate() self._nova[service_type] = client
# todo: get fee </s> self.asserttrue(any([isinstance(event, buyordercreatedevent) and event.order_id == order_id	test_buy_and_sell self.assertAlmostEqual(base_amount_traded, order_completed_event.base_asset_amount) self.assertAlmostEqual(quote_amount_traded, order_completed_event.quote_asset_amount) for event in self.event_logger.event_log])) expected_quote_bal = quote_bal - quote_amount_traded
print('warning: failed to resolve {}'.format(ep.name))  # todo: use proper logger </s> return none	safe_load driver_init = ep.resolve() except: try: driver = driver_init()
# todo: check that the birth date is not in the future </s> except valueerror, e:	is_valid try: birth_date = _get_birth_date(number) return False if len(number) == 10:
# todo:  urlencode file names in the url while adding/decode upon retrieval </s> self._cache_dir = opj('.git', 'datalad', 'tmp', 'archives')	__init__ self.runner = Runner() self._last_url = None # for heuristic to choose among multiple URLs self._cache = None
pass # todo </s> def startcdata(self):	startCDATA
# todo expression </s> f.write(u"expression todo")	print_Jump f.write(u"jump ") if stmt.expression: else: f.write(stmt.target)
# todo: differentiate between tags assigned to the instance and a m2m field for tags (ex: configcontext) </s> if key == 'tags':	model_to_dict model_dict[attr] = getattr(instance, attr) for key, value in list(model_dict.items()): model_dict[key] = ','.join(sorted([tag.name for tag in value])) elif model_dict[key] and type(value) in (list, tuple) and hasattr(value[0], 'pk'):
# todo: implement this in c. </s> co = func.__code__	coroutine return func if co_flags & 0x20: func.__code__ = CodeType( co.co_argcount, co.co_kwonlyargcount, co.co_nlocals,
# todo xxx no memory lock implemented </s> assert arg2.is_op('preinc')	ldaxrb def ldaxrb(ir, instr, arg1, arg2): assert len(arg2.args) == 1 ptr = arg2.args[0]
# todo: verify this is always none. e.g. run with runtime input input </s> v = none	__search for k, v in new_param_dump.items(): if v == {'__class__': 'RuntimeValue'}: elif k == 'chromInfo' and '?.len' in v: continue
# todo: all filtered scanlines start with a byte indicating the filter </s> self.assertequal(list(out), [30, 31, 32, 230, 231, 232])	testFilterScanlineFirstLine self.assertEqual(list(out), [1, 30, 31, 32, 200, 200, 200]) out = filter_scanline(2, line, fo, None)  # up out = filter_scanline(3, line, fo, None)  # average self.assertEqual(list(out), [3, 30, 31, 32, 215, 216, 216])
# todo: localize </s> self.speak(	receive_handler_with_self handler(self, message) except: "An error occurred while processing a request in " + self.name)
# todo: actually implement feature importance visualization for multiclass problems. </s> if isinstance(shap_values, list):	classify explainer = shap.TreeExplainer(self.clf) shap_values = explainer.shap_values(X) shap_values = np.sum(np.abs(shap_values), axis=0) importances = self.get_important_features(importance_cutoff, shap_values)
#todo: am i supposed to be adding the namespace like this? </s> data_node.append(etree.element(ns+tag))	handle ns = "{%s}" % data_node.nsmap[None] tag = hidden_value_path.replace("/data/", "") ns = "{%s}" % xform_root.nsmap[None] itext_node = xform_root[0][1].find(ns+"itext")
# todo: this should be method on bound object </s> self._state[instance] = active	set_toggleaction_state def set_toggleaction_state(self, instance, active): for actionable in self._proxies.get(instance, []): actionable.set_active(active)
# todo: remove when 36lts is discontinued </s> with open(recorded_args, 'r') as args_file:	retrieve_args if recorded_args is None: return None return pickle.load(args_file)
pass  # todo </s> def create(self, name):	create
## todo: fix the unicode issue mentioned in </s> if sys.version_info[:2] <= (2, 7):  ## python2	pronunciation if json_obj: Refer : http://stackoverflow.com/questions/18337407/saving-utf-8-texts-in-json-dumps-as-utf8-not-as-u-escape-sequence return json_obj else:   ## python3
#todo - introduce an annotated alignment class? </s> alignment._annotations = gr	next % (len(ids), self.records_per_alignment)) alignment = Alignment(self.alphabet) alignment_length = len(seqs.values()[0]) for id in ids :
# todo: generalize </s> old_len = hpat.distributed_api.local_len(b)	_series_dropna_str_alloc_impl def _series_dropna_str_alloc_impl(B):  # pragma: no cover new_len = old_len - hpat.hiframes.api.init_series(B).isna().sum() num_chars = hpat.str_arr_ext.num_total_chars(B)
# todo: raise error in 0.9 or 0.10. </s> warnings.warn(	_crs_mismatch_warning def _crs_mismatch_warning(): "CRS mismatch between CRS of the passed geometries " "and 'crs'. Use 'GeoDataFrame.set_crs(crs, "
step = 0.1  # todo </s> jd = arange(jd0, jd1, step)	find raise ValueError('your start_time {} is later than your end_time {}' .format(start_time, end_time)) while True: t = ts.tt_jd(jd)
#todo: make more general (if possible) </s> return datetime.fromtimestamp(record_dict['slice_index_value'] + (record_dict['x_min'] + record_dict['x_max']) * 120).date()	solar_date Function which takes a record_dict containing all values from a query in the get_db_slices function and returns the solar date of the observation
# todo should be resp.raise_from_status </s> raise pepperexception('server error.')	req_requests raise PepperException('Authentication denied') if resp.status_code == 500: return resp.json()
# todo: take care of theano to keras port: </s> ))	vgg16 net["in"], 2, "conv_1", 64, activation=activation, net.update(base.conv_pool( net["conv_1_pool"], 2, "conv_2", 128,
""" todo(datapipe-1525): this fixture override the </s> `mock_source_cursor` fixture present in conftest.py	mock_source_cursor @pytest.yield_fixture def mock_source_cursor(self): mock_cursor = mock.Mock() mock_cursor.fetchone.return_value = ('binlog.001', 200)
# todo check the actual transformation matrix. </s> self.assertless(mss, 1)	testICP - self.source.to_array(), axis=1) ** 2).mean()
# todo: create a validate for class name </s> self.field_name_resolvers[modeltype.class].get_valid_name(n)	get_class_name split_name = name.split('.') prefix = '.'.join( for n in split_name[:-1] )
# todo implement this method </s> return	set_gps :param range: GPS range :return:
# todo: remove compatability hook </s> shutil.copyfile(os.path.join(self.freeze_dir,esky_control_dir,"bootstrap-manifest.txt"),os.path.join(self.freeze_dir,"esky-bootstrap.txt"))	_run lf.write("this file is used by esky to lock the version dir\n") shutil.copyfile(os.path.join(self.freeze_dir,ESKY_CONTROL_DIR,"lockfile.txt"),os.path.join(self.freeze_dir,"esky-lockfile.txt")) print "zipping up the esky" zfname = os.path.join(self.dist_dir,"%s.%s.zip"%(fullname,platform,))
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_metadata_family_values_are_all_the_same def test_result_metadata_family_values_are_all_the_same(self): self.assertInSuccess('test_metadata_family_values_are_all_the_same', run_set(p, 'result'))
raise pathaccesserror()  # todo: path </s> if ret is _missing:	_eval ret = getattr(target, arg, _MISSING) except AttributeError: raise PathAccessError()  # TODO: path elif self.operation == '[':
# todo since </s> bottoms.append(	command_syntax for argument in command_info["arguments"]: command_args.append(argument["name"]) ("class:bottom-toolbar.on", f"({comamnd_group}) {command} {command_args}") )
min_stake=0,  # todo: where to get this? </s> federated_only=character_config.federated_only,	make_cli_character if teacher_uri: maybe_sage_node = character_config.known_node_class.from_teacher_uri(teacher_uri=teacher_uri, network_middleware=character_config.network_middleware, registry=character_config.registry)
# todo use read_package_list_from_file when #618 is merged </s> if os.access(build_pkg_path, os.r_ok):	install_system_packages runtime_pkg_path = './apt-pkgs-runtime.txt' if self.distribution != 'fedora' else './dnf-pkgs-runtime.txt' build_list = [] with open(build_pkg_path) as build_f: build_list = build_f.read().splitlines()
# todo: eliminate asap, for backwards compatibility only </s> return self.get(self)	find def find(self):
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
# todo: refactor </s> if node_id < (n_pes - 1) and (ascending and val >= bounds[node_id]	parallel_sort for i in range(n_local): val = key_arrs[0][i] or (not ascending) and val <= bounds[node_id]): node_id += 1
self.assertequals(status, 200) # todo: 202 when asynchronous </s> status, body = self.post(path, body)	test_install options=options,)
# todo disabled since it breaks existing configurations </s> self._relation_embedder = kgeembedder.create(	__init__ ) num_relations = dataset.num_relations config, dataset,
# todo: use message request - not orm access! </s> db_session = database_setup.get_session()	test_bait_classification_client_first first. self.populate_bait(False) sessions = db_session.query(Session).all() for session in sessions:
# todo: use regex matching instead of this </s> assert str(staking_agent.get_current_period()) in result.output	test_nucypher_status_stakers owned_tokens = NU.from_nunits(staking_agent.owned_tokens(staking_address)) locked_tokens = NU.from_nunits(staking_agent.get_locked_tokens(staking_address)) assert some_dude.worker_address in result.output assert str(round(owned_tokens, 2)) in result.output
# todo fix bug for not identifying unused variables in nested exceptions see issue #4391 </s> try:	function2 1 / 0 except ZeroDivisionError as error: 1 / 0 except ZeroDivisionError as error:
# todo: this assumes that we're always signing proxy retargetting. for the moment is true. </s> proxy_contract = blockchain.client.w3.eth.contract(abi=abi,	multisig executive_summary = proposal['parameters'] name, version, address, abi = local_registry.search(contract_address=executive_summary['target_address']) address=address, version=version,
#@todo: move to utils in 0.4.10 </s> def timestamp():	timestamp return int(time.time() * 1000)
# todo log here </s> return false	auth_okta return False if response.status_code != 201: poll_url = None while True:
# todo use unused info </s> (altitude_feet, latitude, longitude, _range, _bearing) = air_modes.parsebds05(data, cpr_decoder)	receive bdsreg = data['me'].get_type() if bdsreg == 0x05: self.__track = self.__track._replace( altitude=TelemetryItem(altitude_feet * _METERS_PER_FEET, receive_time),
# todo: replace with "yield from" when dropping python 2. </s> for stream in streams.items():	_create_stream if urlparse(url).path.endswith("m3u8"): streams = HLSStream.parse_variant_playlist(self.session, url) yield stream else:
# todo: add checks for broken paddings/encrypted values and malformed enc_data </s> not_valid_password = b"\x01\x02\x03\x04\xff"	test_01_encrypt_decrypt_pass pw3 = '7a4d5e2f26978394e33715bc3e8188a3:90b2782112ad7bbc5b48bd10e5c7c096cfe4ef7d9d11272595dc5b6c7f21d98a' self.assertEquals(decryptPassword(pw3, ), u'passwörd') r = encryptPassword(not_valid_password) self.assertEqual(decryptPassword(r), 'FAILED TO DECRYPT PASSWORD!')
# todo default_init_configs </s> def forward(self, data, hidden):	RNN network = mxnet.symbol.Activation(data + hidden, act_type=act_type) super(RNN, self).__init__(network) return super(RNN, self).forward(data=data, hidden=hidden) def param_shapes(self, data_shape, hidden_shape):
'''todo: add docs''' </s> d = {}	bar_args @property def bar_args(self): d['tools'] = 'pan,wheel_zoom' d['data'] = self.model.df
# todo: use logging module </s> print "pystache: running tests: argv: %s" % repr(sys_argv)	main Arguments: sys_argv: a reference to sys.argv. should_source_exist = False spec_test_dir = None
# todo: only works for diagonal tensors. getedgeinnerproductderiv, </s> dmfmuii_di = -self.mfmuii**2	MfMuiIDeriv def MfMuiIDeriv(self, u): Derivative of :code:`MeSigma` with respect to the model dMf_dmui = self.mesh.getEdgeInnerProductDeriv(self.mui)(u) return dMfMuiI_dI * (dMf_dmui * self.muiDeriv)
# todo better way of making sure tznames are unique </s> while tzname in tznames:	to_tz component['TZOFFSETTO'].to_ical(),  # for whatever reason this is str/unicode ) tzname += '_1' tznames.add(tzname)
# todo: after reasonable amount of time replace with string option </s> lines.append('    services.openssh.knownhosts."{0}" ='.format(m2.name))	emit_resource for m2 in active_machines.itervalues(): if hasattr(m2, "public_host_key"): lines.append('      {{ hostNames = ["{0}-unencrypted" "{0}-encrypted" "{0}" ];'.format(m2.name)) lines.append('        publicKeyFile = ./{0}.public_host_key;'.format(m2.name))
# todo: do not include measures!!! </s> attributes += self.measures	Cube attributes += dim.all_attributes attributes += self.details return attributes def attribute(self, attribute, simplify=True):
#todo trap and ignore attributeerror here? </s> else:	pipe_rssitembuilder if "subkey" in key: value = reduce(lambda i,k:i.get(k), [item] + key['subkey'].split('.')) #forces an exception if any part is not found value = util.get_value(conf[key], kwargs) except KeyError:
# todo: fix figleaf traceback with doctests </s> test_cmd.append("-d")	test test_cmd = [python, "test.py"] if coverage: env.cmd(test_cmd)
# todo this should be more modular </s> if self.service == 'stackdriverlogging':	_get_targets try: if self.library_type == 'cloud_client_library': api_client.project = list_params_combination.pop('project') response = method(**list_params_combination)
d="'d.${def1}.${def2}'"             #todo </s> e="'e.${def1111}.def5.${def2222}'"	test_3250_nonrecursive_expand_variables B="'B.def2.def3'" C="'C.${DEF4}.${DEF5}'" F="'F.${DEF3}.${DEF3}'"             #TODO else:
# todo remove this custom equality testing code when </s> self.assertequal(obs, exp)	test_fasta_to_sequence_collection for fp in fps: obs = _fasta_to_sequence_collection(fp, **kwargs) for o, e in zip(obs, exp): self.assertTrue(o.equals(e))
raise notimplementederror #todo </s> elif q.kw(i,"format"):	parse while i < l: if q.kw(i,"RETURN"): raise NotImplementedError #TODO elif q.kw(i,"REQUEST"):
# todo: normal gl requires these lines, es 2.0 does not </s> from opengl import gl	on_initialize gl.glEnable(gl.GL_BLEND) gl.glBlendFunc(gl.GL_SRC_ALPHA, gl.GL_ONE) gl.glEnable(GL.GL_VERTEX_PROGRAM_POINT_SIZE) gl.glEnable(GL.GL_POINT_SPRITE)
# todo: add logging. </s> sublime.status_message("vintageous: could not write file.")	do_write sublime.status_message(msg) except IOError as e: print('Vintageous =======') print (e)
# todo it seems that yahoo! converts relative links to absolute </s> if context.verbose:	pipe_fetchpage content = unicode(request.read(), request.headers['content-type'].split('charset=')[-1]) print "............FetchPage: content ................." print content.encode("utf-8")
# todo: askr, undocumented! </s> def ledsetlayout( self, mode ):	LaunchpadProMk3 ValidModes = [0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0A, 0x0B, 0x0C, 0x0D, 0x0E, 0x0F, 0x10, 0x11, 0x12, 0x13] if mode not in ValidModes:
assert oldsize == self.size, '%s: %d, %d' % (self.type, oldsize, self.size) # todo: remove </s> return self.size	Atom else: self.size = 8 + sum([atom.calsize() for atom in self.body])
# todo(billdodd): change to self.systems_uri after pr 62921 merged </s> systems_uri = self.systems_uris[0]	set_boot_order return {'ret': False, 'msg': "boot_order list required for SetBootOrder command"} response = self.get_request(self.root_uri + systems_uri) if response['ret'] is False:
# todo: print 's' </s> raise duplicatedeclarationerror(	parse else: if k in names: "Several declarations for the symbol '%s'" + " in the same environment"
# todo: make it handle more components </s> def _on_server(self, args):	_on_server repo_dir = join(self.config_dir, 'repo') server_conf = ConfigObj(join(repo_dir, 'server.conf'))
# todo: get rid of this feature one day (v8?; warning added in v7.3.0.) </s> logger.warn("the post {0} is using the `password` attribute, which may stop working in the future.")	compile self.is_two_file), if self.meta('password'): LOGGER.warn("Please consider switching to a more secure method of encryption.") LOGGER.warn("More details: https://github.com/getnikola/nikola/issues/1547")
# todo: clean up this event print out. we probably want something </s> if suffix == 'new':  # skip "new" events	run if not self.opts.get('quiet', False): for suffix, ret in self.get_async_returns(async_pub['tag']): continue elif suffix == 'ret':  # for "ret" just print out return
#     # todo: add an exception message </s> raise parsererror	DateTimeParser if "YY" in fmt_tokens and match.end() != len(string):
# todo test for final </s> return """<img src="/%s" alt="image generated by dexy %s" />""" % (self.canonical_filename(), self.key)	img def img(self):
# todo: store only successful commands. </s> exshellout._last_command = shell_cmd	ExShellOut return shell_cmd = ExShellOut._last_command try: if not parsed.line_range.is_empty:
# todo: agree on protocol here </s> true: u'border-dotted',	get_span_types 'type': 'Speculation', 'value': { }, 'labels': ['Speculation', ],
# todo: skipped due to gh-4436 </s> yield skip_if_on_windows(skip_ssh(_test_version_check)), 'datalad-test'	test_version_check def test_version_check(): yield _test_version_check, None
# todo: each dp learns independently. an edge dp could </s> eth_src = pkt_meta.eth_src	_edge_dp_for_host Returns: Valve instance or None (of edge datapath where packet received) vlan_vid = pkt_meta.vlan.vid for other_valve in other_valves:
# todo : outsource to cython </s> for i in range(3, image.shape[0] - 3):	corner_fast [2, -2], [1, -3], [0, -3], [-1, -3], [-2, -2], [-1, -3]]) for j in range(3, image.shape[1] - 3): test_x = i + test_pixels[:, 0]
# todo: waiting for a fix: https://developer.blender.org/t53509 </s> mat = context.object	draw split.separator() if mat: layout.label("Material Nodes:") layout.template_ID(mat.luxcore, "node_tree", new="luxcore.mat_nodetree_new")
# todo: impala attempt to speed up final pass after lstm. </s> nn_output_folded = self.call(self.time_rank_folder.apply, nn_output)	get_state_values_logits_parameters_log_probs self.call(self.neural_network.apply, nn_input, internal_states) ) state_values, logits, probs, log_probs = self.call(self.action_adapter.get_state_values_logits_parameters_log_probs, nn_output_folded) merged_impala_hack = self.call(self.merger.merge, state_values, logits, probs, log_probs)
# todo send the key to the master for approval </s> sh_ = '/bin/sh'	seed bs_ = gather_bootstrap_script() salt.crypt.gen_keys(mpt_tmp, 'minion', 2048) if os.path.isfile(os.path.join(mpt, 'bin/bash')): sh_ = '/bin/bash'
pass # todo </s> else:	_help pass # TODO
# todo - del just my poll, not the entire list ! </s> return poll_user_just_voted	check_vote if poll.id in sess_jv: del request.session[POLLS_JUST_VOTED_COOKIE_NAME] if request.user.is_authenticated(): sess = request.session.get(POLLS_COOKIE_NAME, [])
# migration todo: remove ? </s> channel = self.connection.channel()	__init__ self.backend_url = config_d['backend_url'] self.connection = Connection(self.backend_url) bound_process_events_queue = process_events_queue.bind(channel) bound_process_events_queue.unbind_from(events_exchange)
if attribute == 'position':   # todo: clean up </s> raise defaultvalueexception	_get_value_lookup except DefaultValueException: if isinstance(styled.style_class.default_base, ParentStyle): return self._get_value_lookup(styled.parent, attribute, document) raise
# todo: must be implemented </s> pass	range_from_chapters def range_from_chapters(crawler, times=0):
# todo: this is untested. </s> return none	get_cert_store store = _lib.SSL_CTX_get_cert_store(self._context) if store == _ffi.NULL: pystore = X509Store.__new__(X509Store) pystore._store = store
pass  # todo </s> elif op in (assert, assert_not):	_internal_match_patterns pass  # TODO elif arg == AT_NON_BOUNDARY: (direction_int, subpattern) = arg positive_look = op == ASSERT
# todo: log </s> print data['tag'], 'event'	iter_events try: data = self.event.get_event_noblock() for tag_prefix, futures in self.tag_map.items(): if data['tag'].startswith(tag_prefix):
# todo handling when g is a list of graphs </s> if true:	plot_graph def plot_graph(G): ki, kj = np.nonzero(G.A) if G.directed:
self.assertequals(status, 200) # todo: 202 when asynchronous </s> status, body = self.post(path, body)	test_uninstall options=options,)
# todo: 1. try all parameters </s> try:	SmsGatewayFeeCriteria if all_possible_criteria.count() == 0: return None return all_possible_criteria.get(country_code=None, backend_instance=None) except ObjectDoesNotExist:
except exception:  # todo: log error once </s> pass	Worker v = await v result[k] = v return result def identity(self, comm=None):
# todo: duplicate checking </s> self._disassociate.append((disassociation_type, value))	remove def remove(self, disassociation_type: LicenseDisassociationType, value: str):
# todo(ytknzw): add more specific assertion with the test case. </s> figure = plot_parallel_coordinate(study, params=["param_a"])	test_plot_parallel_coordinate figure = plot_parallel_coordinate(study) assert figure.has_data() is True assert figure.has_data() is True with pytest.raises(ValueError, match="Parameter optuna does not exist in your study."):
# todo - unit test </s> return	ladderize subclade.ladderize(reverse=reverse)
origin_args = {}  # todo: gas management </s> if gas_limit:	DispatcherDeployer if new_target == self._contract.address: raise self.ContractDeploymentError(f"{self.contract_name} {self._contract.address} cannot target itself.") origin_args.update({'gas': gas_limit}) upgrade_function = self._contract.functions.upgrade(new_target, existing_secret_plaintext, new_secret_hash)
# todo: figure out a way to actually log this information without </s> multiprocessing.active_children()	multiprocessing_SIGCHLD def multiprocessing_SIGCHLD(self, sig, stack):
def __init__(self, p_args, p_todolist, #pragma: no branch </s> p_out=lambda a: none,	__init__ p_err=lambda a: None, p_prompt=lambda a: None):
# todo(b/201683262) replace all calls to this method </s> stdout = self.rungsutil(['publicaccessprevention', 'get',	test_create_with_pap_unspecified suppress_consec_slashes=False) self.RunGsUtil(['mb', '--pap', 'unspecified', suri(bucket_uri)]) suri(bucket_uri)], return_stdout=True)
# todo: there seems to be a bug in gitpython, which leads </s> pass	push except BadName as e: lgr.error("GitPython reported BadName Exception: %s" % e) else: try:
# todo: assert </s> repo = self.remote.get_repo("testrepo0")	test_get_repo Test: Get a repo object
# todo: remove in v.0.6 </s> x = np.array([[0, 0], [0, 1], [2, 0], [2, 1]])	TestLSML self.assertLess(csep, 0.8)  # it's pretty terrible def test_deprecation_num_labeled(self): y = np.array([1, 0, 1, 0]) lsml_supervised = LSML_Supervised(num_labeled=np.inf)
# todo parameters </s> f.write(u':\n')	print_Label def print_Label(f, stmt): f.write(u"label %s" % (stmt.name, ))
# todo implement through browser </s> step_message = 'verify an alert is present'	verify_alert_is_present def verify_alert_is_present(): execution.logger.info(step_message) _capture_or_add_step(step_message, execution.settings['screenshot_on_step'])
# todo: autodetect size from passed-in file object? </s> params = {	create_temp_project_avatar auto_confirm -- whether to automatically confirm the temporary avatar by calling confirm_project_avatar() with the return value of this method. 'filename': filename, 'size': size
# todo: write </s> return dict.fromkeys(key_strings, callback)	string_keys_to_dict def string_keys_to_dict(key_strings, callback):
# todo: update the tolerance after the ci moves to torch 1.10 </s> self.assertalmostequal(outputs.loss.item(), 18.5925, 2)	test_inference_speaker_verification self.assertAlmostEqual(cosine_sim(embeddings[0], embeddings[1]).item(), 0.4941, 3) self.assertAlmostEqual(cosine_sim(embeddings[2], embeddings[3]).item(), 0.5616, 3)
# todo this context is probably not right. </s> analysis.add(next(iter(types)), 'value-error-too-few-values', has_parts,	unpack_tuple_to_dict has_parts = next(parts, None) if types and has_parts is not None: message="ValueError: need more than %s values to unpack" % n) return dct
# todo should be: </s> self.assertequal(response3.data['compression'], 'no')	test_mount_options self.assertEqual(response3.status_code, status.HTTP_200_OK, msg=response3.data) self.assertEqual(response3.data['mnt_options'], 'compress-force=lzo') data2 = {'mnt_options': 'compress-force=1'} e_msg = ('not all arguments converted during string formatting')
# todo(yamahata): replace n_events with neutron_lib.callback.events </s> registry.subscribe(callback, resources.process, n_events.after_spawn)	test_start_all_workers mock.patch.object(service, '_start_workers').start() callback = mock.Mock() service.start_all_workers() callback.assert_called_once_with(
# todo: automate this (by lookup from nn). </s> internal_states_space=impalaagent.standard_internal_states_space,	test_impala_distributed_functionality_learner_part state_space=env.state_space, action_space=env.action_space, execution_spec=dict( mode="distributed",
# todo block until metadata_updated callback is called. </s> browser = self.backend.spotify.session.browse_album(	lookup try: spotify_album = Link.from_string(uri).as_album() spotify_album) start = time.time()
# todo we should make sure that trivial equality relations are </s> continue	_detect_effectively_discrete_vars constant_term = NoneToZero(repn.constant) - NoneToZero(constr.lower) if len(repn.linear_vars) < 2: non_discrete_vars = list(v for v in repn.linear_vars if v.is_continuous())
#todo: move to filter </s> def filter_data_type(self, output):	filter_data_type for fmt in self.display_data_priority: if fmt in output:
# todo: test coverage </s> todo = cls.objects.filter(	submit_queue @classmethod def submit_queue(cls): prepared=True, sent=False, sending=False, publish_date__lt=datetime.now()
# todo - move 'brac' to db (eventschedule.callback_args) </s> brac = domain.objects.get(name="brac")	daily def daily(router): delinquent_report(router, brac)
# todo support multiple backends </s> self.backends[0].stored_playlists.playlists = playlists	playlists @playlists.setter  # noqa def playlists(self, playlists):
session.add(job)  # todo review this after remapping job (required to lazy-load `container` attr) </s> obj = cls_(job, container_type, container_name, container_info)	TestJobContainerAssociation created_time = datetime.now() modified_time = created_time + timedelta(hours=1) obj.created_time = created_time obj.modified_time = modified_time
# ---- todo: the following should be removed in milestone:0.11 </s> match = img_re.search(path)	_format_link def _format_link(self, formatter, ns, path, label): if formatter.flavor != 'oneliner' and match: return html.IMG(src=formatter.href.file(path, format='raw'),
# todo result type? </s> def messages() -> iterator[messenger.message]:	messages model = _dal() for t in model.iter_threads():
# todo - fix meta.submission to point to real submission </s> self._remove_handled(meta.submission.submission)	remove_instance_matching_schema try: meta = Metadata.objects.get(raw_data=instance_id, formdefmodel=formdef_id) meta.delete() except Metadata.DoesNotExist:
# todo: use model fit loss instead of a random loss </s> return {'loss'  : random.random(), 'space': space,	test_wrapper self._model.guess_and_fill_missing_params() self._model.build() 'status': hyperopt.STATUS_OK}
# todo: fails because of missing svg support </s> with capture_logs() as logs:	test_images_alt @assert_no_logs def test_images_alt(): assert_same_rendering(200, 30, [ (name, '''
return  # todo [review] should an exception be raised, and if yes, what type of exception e.g. package, module, plugin, generic?  # noqa: e501 </s> elif len(replacement) != 1:	_do_surround_cs if len(replacement) >= 3: if replacement[0] not in ('t', '<') or not replacement.endswith('>'): return  # TODO [review] should an exception be raised, and if yes, what type of exception e.g. package, module, plugin, generic?  # noqa: E501 target_pairs = {
# todo - unit test </s> return false	is_bifurcating return True
# todo: also improve 'crash-start' detection (to reduce lag when server fails to start) </s> time.sleep(0.1)	main launch_server_daemonized() for _ in range(100): # Check server availability for next 10 seconds s = cnsapp.connect() if s is not None: break
# todo - check and if we don't have category, take the only placement that exists in current site </s> self._main_placement = get_cached_object(	main_placement else: try: Placement, publishable=self.pk,
return # todo: flag bad gzip </s> chunk = decompress.decompress(chunk) # todo: flag bad zlib	response_body return # not a full header yet except IOError: for processor in body_processors: processor(chunk)
# todo: manage next letter "t" </s> string, next_string = string.split("t", 1)	path self.context.move_to(x, y) elif letter == "q": x1, y1 = 0, 0 while string:
# todo: documentation pending </s> parameters	load_weights_from_npz def load_weights_from_npz(self, filepath, sess=None): ---------- filepath
# todo: improve performance by using conv instead of unfold </s> return mat.sum(axes)	bias_jac_t_mat_prod axes = [N_axis] + axes
# todo: scale and translation could be merged into a single network </s> with tf.variable_scope("scale", reuse=tf.auto_reuse):	forward_and_jacobian mask = self.get_mask(inputs, dtype=inputs.dtype) masked_inputs = inputs * mask scale = mask * self.scale_fn(masked_inputs) with tf.variable_scope("translation", reuse=tf.AUTO_REUSE):
# beginning of block, preventing fusion. todo: fix pa </s> if (rhs.attr == 'dtype' and isinstance(	_run_assign assign.value = ir.Global("numpy.datetime64", rhs_type.dtype, rhs.loc) return [assign] if_series_to_array_type(rhs_type), types.Array)): typ_str = str(rhs_type.dtype)
# todo / fixme : use functions in utils/network.py to manage this </s> l_ip = 'unknown'	monitor_network elif u == 'infos': p_ipv4 = get_public_ip() or 'unknown' for name, addrs in devices.items(): if name == 'lo':
#todo replace when better connection mechanism is available </s> context = none	get_vsan_cluster_config_system service_instance Service instance to the host or vCenter if sys.version_info[:3] > (2,7,8): context = ssl.create_default_context()
#todo: introduce constants for readability </s> self._blockstate[:]= 1 #this is the dirty state	_allocateManagementStructures self._blockNumbers = _blockNumbers self._blockIndices = _blockIndices self._dirtyState = 2 #this is the clean state self._flatBlockIndices =  self._blockIndices[:]
# todo(thejulia): once we have power/state callbacks to nova, </s> task.node.power_state = states.power_on	rescue :returns: Returns states.RESCUEWAIT manager_utils.node_power_action(task, states.POWER_OFF) task.node.save() task.driver.boot.clean_up_instance(task)
# todo: remove when #980 has been merged </s> info['url'] = formats[-1]['url']	_real_extract 'upload_date': upload_date, } info['ext'] = determine_ext(formats[-1]['url']) return self.video_result(info)
# todo: remove anytime in 2016 </s> _assert(false, "status in filter wasn't set - this isn't expected to be possible")	get_prefix_and_key_for_filter_results_and_parsed_params prefix = "%s %s" % ("status", prefix) else: def _get_key(): if parsed_params.module is not None and parsed_params.get_module_int() is None:
# todo: parameterize and check for wrong argument </s> result = utils.dhcpconf_location(utils.dhcp.v6)	test_dhcpv6conf_location def test_dhcpv6conf_location(): assert result == "/etc/dhcpd6.conf"
# todo: shouldn't it to be in the statefu module? </s> def init_client_stateful(msg, addr, client, packetselector):	init_client_stateful client_private_ip = msg[0:4] client_public_source_ip = socket.inet_aton(addr[0])
# todo: remove </s> g.q = q	index q = request.params.get(u'q', u'') sort_by = request.params.get(u'sort') g.sort_by_selected = sort_by extra_vars[u"q"] = q
# todo: catch unquoting errors, range of chars, charset </s> decoded_v = urllib.unquote(esc_v)	_parse_params if lang != '': self.setMessage(name, rs.PARAM_LANG, param=k, lang=lang) param_dict[k.lower()] = decoded_v else:
# todo: automate this (by lookup from nn). </s> internal_states_space=impalaagent.standard_internal_states_space,	test_impala_actor_plus_learner_agent_functionality_actor_part state_space=env.state_space, action_space=env.action_space, execution_spec=dict( mode="distributed",
# todo xxx graalvm change </s> raise unittest.skiptest("missing threading support")	test_socketserver def test_socketserver(self): server = make_https_server(self, certfile=SIGNED_CERTFILE) if support.verbose:
pass  # todo </s> def get_xl_sheet(xl_workbook, sheet_name_or_index):	get_xl_sheet
# todo subject.cn from cert? </s> shutil.rmtree(app_path)	test_simple_app lib_hashes = self.assert_common_signed_hashes(lib_info, -2, -1) assert '-3' not in lib_hashes return app_info
# todo handle exception </s> raise exception("functions not found")	select_functions if len(function_vas - workspace_functions) > 0: floss_logger.warn("Functions don't exist:", function_vas - workspace_functions) return function_vas
# todo: what about '_type'? </s> }	_build_subcontribution_api_data 'folders': build_folders_api_data(subcontrib), 'speakers': self._serialize_persons([x.person for x in subcontrib.speakers]), return data
# todo: why do we have `has` below, should not it be `have`? </s> assert "has no texts ('alex', 'yakov')" in error.value.msg	test_should_have_no_texts_exception with pytest.raises(TimeoutException) as error: browser.all('li').should(have.no.texts('Alex', 'Yakov')) assert "ConditionNotMatchedError: condition not matched" in error.value.msg
# todo(aron): move these client test cases to their own test class </s> self.client.login_teacher(data={"username": self.teacher_username,	teacher_cant_edit_facilities elem = self.browser.find_element_by_css_selector('a.edit-facility') self.assertEquals(elem.value_of_css_property("display"), "none", "edit-facility is still displayed!") "password": self.teacher_password}, facility=self.facility)
# todo: for now, circumnavigate the detached head issue. </s> for subds in source.get_dataset_handles(recursive=true):	test_publish_simple def test_publish_simple(origin, src_path, dst_path): source = install(path=src_path, source=origin, recursive=True) AnnexRepo(opj(src_path, subds), init=True, create=False).git_checkout("master") target = GitRepo(dst_path, create=True)
# todo: write manifest and bagit metadata </s> shutil.move(self.folder, folder)	close This function can only be called once, after which this object can no longer be used. self.folder = None
# truffle todo: revert </s> for __x in _glob1(dirname, basename, dironly): yield __x	_iglob for __x in _glob2(dirname, basename, dironly): yield __x else: return if dirname != pathname and has_magic(dirname):
# todo: bytes vs str </s> request_line = request_line.decode()	_handle def _handle(self, reader, writer): request_line = yield from reader.readline() method, path, proto = request_line.split() headers = {}
from gi.repository import glib  # todo: to fix </s> import gi	getResult else: self.updateConversionAddressingTableWithTable(filterConversionAddressingTable) gi.require_version('Gtk', '3.0') encodedResult.append(glib.markup_escape_text(newData))
# todo: test me </s> if get_rest_framework_features()['router_trailing_slash']:	add_trailing_slash_if_needed def add_trailing_slash_if_needed(regexp_string): return regexp_string[:-2] + '{trailing_slash}$' else:
todo = atomlist # list of atoms we must still mark and explore (recurse on all unmarked neighbors) </s> for atom in todo:	getConnectedSinglets def getConnectedSinglets(self, atomlist): marked = {} # maps id(atom) -> atom, for processed atoms marked[id(atom)] = atom # since marked means "it's been appended to the todo list" while todo:
# todo: multi dp routing requires learning from directly attached switch first. </s> if pkt_meta.port.stack:	learn_host_from_pkt stacked_other_valves = self._stacked_valves(other_valves) all_stacked_valves = {valve}.union(stacked_other_valves) peer_dp = pkt_meta.port.stack['dp'] if peer_dp.dyn_running:
# todo: implement </s> return patches	garrison_heal_upgrade :rtype: list patches = []
# todo: remove when materialized paths are fixed in the payload returned from waterbutler </s> if not new_file.materialized_path.startswith('/'):	create_new_file if destination['provider'] != 'osfstorage': new_file.update(revision=None, data=data) new_file.materialized_path = '/' + new_file.materialized_path new_file.save()
principled.inputs[5].default_value = pypbr.metallic_factor #todo : currently set metallic & specular in same way </s> principled.inputs[7].default_value = pypbr.roughness_factor	create_cycles if not pypbr.vertex_color: principled.inputs[0].default_value = pypbr.base_color_factor else: attribute_node = node_tree.nodes.new('ShaderNodeAttribute')
pass  # todo </s> def get_num_frames(self):	get_num_frames
# @todo: bulk lookups </s> represent = lambda v: v,	customise_pr_forum_membership_resource table.name_click = s3_fieldmethod("name_click", person_membership, search_field = "person_id", )
pass # todo </s> def handle_request(self, input):	handle_request
# todo: 搜索和分页 </s> keyword = request.get.get('search', '')	perm_role_edit header_title, path1, path2 = "系统角色", "角色管理", "查看角色" roles_list = PermRole.objects.all() if keyword: roles_list = roles_list.filter(Q(name=keyword))
# todo set() is used for uniqueing results in case a checker name is </s> return list(set(map(lambda value: value[1], labels)))	label_of_checker except Exception: return CheckerLabels.UNIQUE_LABELS[label]
# todo: check botocore version </s> ec2 = module.client('ec2')	main tag_equality=dict(type='bool', default=False)) module = AnsibleAWSModule(argument_spec=argument_spec) copy_image(module, ec2)
# todo: configurable timeout??? </s> syndic_dict['dead_until'] = time.time() + 60	_call_syndic except SaltClientError: log.error('Unable to call {0} on {1}, trying another...'.format(func, master_id)) continue log.critical('Unable to call {0} on any masters!'.format(func))
"""create todo database and tables""" </s> if os.path.exists(filename):	makeDB def makeDB(self, filename): self.db = sqlite.connect(filename) return
# todo: write this method if possible </s> pass	address_transactions def address_transactions(self, addresslist):
# todo parse units as: units='oe' </s> self.plot.setlabel('left', axis, **self.label_style)	update_y_column axis = self.columns_y.itemText(index) self.curve.y = axis
# todo: add logger </s> print("hey, error here: screenshotter/db_save.py. multiple shits")	save_screenshot_data print(scans) if len(scans) > 1: print(scans) elif len(scans) == 0:
#todo - do we still use the betweenposition class? </s> if ((self._start == self._end) and isinstance(self._start,	_get_nofuzzy_end def _get_nofuzzy_end(self) : BetweenPosition)): return self._end.position
# todo: this is all just debugging stuff and can be removed </s> if debug:	rotate self.u = uh self.g = self.g - logdet uh = [ui.copy() for ui in uh] gh = self.g.copy()
# todo(mlavalle) a follow up patch in the address groups implementation </s> log.info("address group updated %r", address_group_id)	address_group_updated def address_group_updated(self, address_group_id):
# todo: move 'hardcoded' coordinate specs (name, units, etc) into tile_spec </s> su_descriptor = index_netcdfs([filename[7:]])[filename[7:]]	create_storage_unit except OSError: pass return StorageUnit([dataset.id for dataset in datasets], mapping,
# todo: displacement. </s> else:	filter_apply if isinstance(currentNode, bpy.types.ShaderNodeTexImage) and currentNode.image is not None and currentNode not in filtered_textures: filtered_textures.append(currentNode) if export_settings['gltf_common'] != '-': for currentTextureSlot in currentMaterial.texture_slots:
oldsize = self.size # todo: remove </s> self.size = 8 + 4 + 8 + len(self.body[3]) * 4	stsz_atom write_uint(stream, entry_size) def calsize(self): assert oldsize == self.size, '%s: %d, %d' % (self.type, oldsize, self.size) # TODO: remove return self.size
# todo(t2r_contributors): switch to using gin config for all saver params. </s> keep_checkpoint_every_n_hours = none	create_scaffold_fn scaffold = scaffold_fn() if not tf.get_collection(tf.GraphKeys.SAVERS): max_to_keep = None if config is not None:
# todo maybe add multichart class? </s> def update_all_graphs(frame):	animate_multiple_plots Args: plots (List[Union[_BarChartRace,_LineChartRace]]): List of plots to animate for plot in plots: try:
# todo: test this </s> try:	initialize_factors return factors elif isinstance(init, (tuple, list, KruskalTensor)): return KruskalTensor(init).factors except ValueError:
# todo cleanup/merge with above `call` once we have `subprocess.run` after dropping python 2 support? </s> assert subprocess.check_output(['which', 'pip'], env=env, universal_newlines=true).strip() == '/tmp/cibw_bin/pip'	build assert os.path.exists(os.path.join(installation_bin_path, 'pip')) call(['pip', '--version'], env=env) call(['pip', 'install', '--upgrade', 'setuptools', 'wheel', 'delocate'], env=env) if config.version == '3.5':
# todo: temporary hack until they fix </s> handle_tr_writer_deprecation()  # todo: temporary hack	run def run(testdir, path="report.html", *args): path = testdir.tmpdir.join(path) result = testdir.runpytest("--html", path, *args)
oldsize = self.size # todo: remove </s> self.size = 8 + 4 + 4 + len(self.body[1]) * 12	stsc_atom write_uint(stream, sample_description_index) def calsize(self): assert oldsize == self.size, '%s: %d, %d' % (self.type, oldsize, self.size) # TODO: remove return self.size
# todo: different codec to be used </s> raise(asn1notsupperr('{0}: specific containing encoder unhandled' \	_to_oer Cont = self._get_val_obj(self._val[0]) if Cont == self._const_cont and self._const_cont_enc is not None: .format(self.fullname()))) Cont._val = self._val[1]
# todo: this function can replace repeated code in raw_format_table() in the future </s> def raw_format_timestamp(timestamp, extended):	raw_format_timestamp if extended: s = human.format_timestamp(timestamp)
# todo make this a user explicit choice </s> snap_install_cmd.append("--classic")	install try: if self.is_classic(): except (errors.SnapUnavailableError, KeyError): pass
# todo: openssl 1.1.0 has ssl_get0_verified_chain() to do this directly </s> for cert in self.certificate_chain:	__init__ self.verified_certificate_chain = [] if was_validation_successful: ca_cert = MOZILLA_TRUST_STORE.get_certificate_with_subject(cert.as_dict['issuer']) self.verified_certificate_chain.append(cert)
# todo implement support for this </s> elemdict['pose'] = parsesdfpose(elem.find('pose'))	parseSDFLink elemdict = {'name': name} if objtype == 'collision': else:
# todo: remove this monkeypatch once upstream class is fixed. </s> _patch_zone(zone)	_delete_record LOGGER.debug("delete_records: %s", ids) with localzone.manage(self.filename, self.origin, autosave=True) as zone: for hashid in ids: zone.remove_record(hashid)  # pylint: disable=no-member
'one, two, three, four, five') #todo: group being ignored? </s> self.assertequal(numwords('12345', group=2),	test_NUMWORDS 'one, two, three, mark, four, five, six') self.assertEqual(NUMWORDS('12345', group=3), 'one, two, three, four, five') #TODO: group being ignored? self.assertEqual(NUMWORDS('12345', group=1),
## todo : log error </s> error_code = delete_functionparameters_doctype_submission(doctype=doctype, action=action)	_delete_submission_from_doctype user_msg.append("""When deleting Submission "%s" from Document Type "%s", it wasn't possible to delete all Submission Fields""" \ % (action, doctype)) if error_code != 0: user_msg.append("""When deleting Submission "%s" from Document Type "%s", it wasn't possible to delete all Function Parameters""" \
#todo: manage multiple foreign key </s> value = value.identity.conditions.values()[0]	to_alchemy_condition value = condition.value if value.__class__ == Item: if condition.operator == "=": return column == value
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
raise exceptions.mpdnotimplemented  # todo </s> songs is 0.	prio A priority is an integer between 0 and 255. The default priority of new
# todo: remove logging </s> log.exception(	handle_bounce_reply_phase content_type = msg.get_content_type().lower() if content_type != "multipart/report" or envelope.mail_from != "<>": "Handle auto responder %s %s %s", content_type, envelope.mail_from, msg )
annot.annotation_metadata.annotation_rules = "todo"  # todo </s> annot.annotation_metadata.validation_and_reliability = "todo"  # todo	fill_annoatation_metadata annot.annotation_metadata.version = "1.0" annot.annotation_metadata.annotation_tools = "Sonic Visualizer" annot.annotation_metadata.origin = "Cerulean Mountain Trust" annot.annotation_metadata.annotator.name = name
# todo get annexed obj file </s> files = _load_json_object(	_query_aggregated_metadata_singlepath elif filepathinfo_objloc: obj_path = opj(agg_base_path, filepathinfo_objloc) obj_path, cache=cache['objcache'])
# todo: remove this if-block.  this is a hack </s> if v is not none and math.fabs(v) > 1e-16:	write_json tmp = {} for k,v in iteritems(vv): tmp[k] = v if len(tmp) > 0:
# time.sleep(40)  # todo: should remove after polling get. </s> mpc_1_2_3 = op(mpc_1_2, tensor_pointer_3)	test_tensor_abstraction_pointer mpc_1_2 = op(tensor_pointer_1, tensor_pointer_2) mpc_1_2.block_with_timeout(secs=40) mpc_1_2_3.block_with_timeout(secs=40) exp_res = op(data_1, data_2)
# todo: think about the most useful class api here </s> return copy	ConcatChart copy.concat.append(other)
self.kernel_quantizer_internal._set_trainable_parameter() # todo recurrent as well? </s> kernel_constraint, kernel_initializer = (	QLSTMCell ] if hasattr(self.kernel_quantizer_internal, "_set_trainable_parameter"): get_auto_range_constraint_initializer(self.kernel_quantizer_internal, kernel_constraint,
#todo: does not keep case </s> ('about me', 'about us'),	test__plnoun self.assertEqual(p._plnoun(sing), plur) for sing, plur in ( ('YOU', 'YOU'), ):
# todo: expand to full set of info </s> def create_workflows_table(meta):	create_workflows_table return Table( 'workflows', meta,
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pcmpeqb res = 0x000000ff0000ff00ff00000000ffff00 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo: animation change </s> self.transfer_res()	finish_working def finish_working(self): print self.id, 'FINISH WORKING' self.job.object._Producer__registered_collectors.remove(self) self.job.path.reverse()
# todo: improve error message(add error code) </s> raise validationerror(	validate_update_list self.validate_data_list(values, self.partial) else: "Expected data of form {'pk': 'data'..}"
# todo rename to "_to_reach_aspect_ratio"? matches other methods </s> def compute_croppings_for_aspect_ratio(arr, aspect_ratio):	compute_croppings_for_aspect_ratio "Crop amounts" here denotes the number of pixels that have to be removed from each side to fulfill the desired constraint.
# todo(rbharath): there should be some automatic check to ensure that all </s> model_params = {"nb_hidden": 10, "activation": "relu",	test_multitask_keras_mlp_ECFP_classification_hyperparam_opt input_transformers = [] task_type = "classification" "dropout": .5, "learning_rate": .01, "momentum": .9, "nesterov": False,
except exception: # todo: what exception? </s> return none	get_tag_disk try: f = metadata.get_format(self.get_loc_for_io()) if not f: return None
# todo: find a better way to do this. </s> user_es._default_filters = esquery.default_filters	test_unknown_user_pillow self.assertEqual(0, UserES().run().total) user_es = UserES() results = user_es.run() self.assertEqual(1, results.total)
# todo ... </s> data = sorted()	userLongDescription for key in mainKeys: data[key] = data.get(key, "").strip()
# todo: may test with codecs.open passing an encoding </s> temp = tempfile.namedtemporaryfile(delete=false)	test_export_to_json_fobj def test_export_to_json_fobj(self): self.files_to_delete.append(temp.name) rows.export_to_json(utils.table, temp.file)
# todo(mattjj,phawkins): use 'yield from' when py2 is dropped </s> for leaf in tree_leaves(child):	tree_leaves children, _ = node_type.to_iterable(tree) for child in children: yield leaf else:
gc.collect()  # todo: see first comment above </s> ok_(ds2 is not none)	test_Dataset_flyweight ok_(ds4.repo is ds1.repo) del ds1 ok_(ds2.repo is ds3.repo) if not on_windows:
# todo - implement searching google, bing, yahoo, baidu, and ask </s> "error searching crt.sh")  # nb. original query string below. it seems impossible to parse and  # reproduce query strings 100% accurately so the one below is given  # in case the reproduced version is not "correct".  # requests.get('https://crt.sh/?q=%25.badssl.com', headers=headers)	search_crtsh self.handle_exception(e,
# todo: why would this throw an exception?  we should handle </s> pass	setup_mapping es.create_index_if_missing(index) except pyes.ElasticSearchException: try: es.put_mapping(MAPPING_TYPE, mapping, index)
# todo remove </s> if option.type == "alertsystemactive":	run if option.type == "profile": self._process_profile_option(option) self._process_alert_system_active_option(option) if has_option_changes:
# todo: check if integerctype, not just ctype.arith (floats, etc.) </s> if self._cache_lvalue:	lvalue def lvalue(self, il_code, symbol_table): return self._cache_lvalue head_val = self.head.make_code(il_code, symbol_table)
# todo: until we get it working. </s> if facts.is_from_app_store():	generate_filewave_recipe by this function! keys = recipe["keys"] warn_about_app_store_generation(facts, recipe["type"]) return
# todo: wells if display config has more than one column </s> "put_loners_in_wells": display	render_form "cases": cases, "form_table_options": { }, "form_meta_data": form_meta_data,
# todo: merge with backup in helpers </s> os.remove(file)	_keeplatestbackup for file in files:
# todo: add test for this </s> keyorder[key] = (generation, new_keyorder[key]) # prefer old	scheduler for key in new_keyorder: if key not in keyorder: generation += 1  # older graph generations take precedence seed_ready_tasks()
# todo: remove constraint of batch_size == 1 </s> regression        = y_pred[0, :, :]	_smooth_l1 def _smooth_l1(y_true, y_pred): regression_target = y_true[0, :, :4] labels            = y_true[0, :, 4:]
# todo enforce uniqueness on arrange panel? </s> models.siparrange.objects.create(	copy_files_to_arrange for entry in to_add: try: original_path=entry['original_path'], arrange_path=entry['arrange_path'],
#todo: it'd be nice to log and then ignore it if not data_is_complete. </s> assert data.is_complete	ofp_packet_out self.buffer_id = data.buffer_id if self.buffer_id in (-1, None): self._data = data._data self.in_port = data.in_port
# todo: use other libraries. </s> def addr(func_name):	Addr if func_name == '/bin/sh': addr = get_bin_sh_str()
return none  # todo better error handling here </s> userinfo_dict = response.json()	user_info except Exception, e: log.error(e) if 'error' in userinfo_dict: assert userinfo_dict['error'] == 'invalid_token'
# todo(pachristopher): remove this once tfdv 0.14 is released. </s> (major, minor, _) = tfdv.__version__.split('.')	_CsvToExample raise RuntimeError( 'Files in same split {} have different header.'.format(csv_pattern)) if int(major) > 0 or int(minor) >= 14: decoder = csv_decoder.DecodeCSVToDict
# todo is pexpect thread safe, e.g. could we be blocked on this </s> pnum = self.con.expect('notification handle = .*? \r', timeout=.5)	run with self.connection_lock: try: if pnum == 0: after = self.con.after
# todo: check that the performance measure is within some range </s> grid0_baseline(num_runs=1, sumo_binary="sumo")	test_grid0 Tests flow/benchmark/baselines/grid0.py
pass # todo: raise exception </s> else:	disconnect self.__signals[signal].remove(function) else: pass # TODO: raise exception
loader = imageloader(32) #todo crop=true? </s> x, y = loader.load(fixture_path(), width=8, height=8, resize=true)	test_load_fixture_resize def test_load_fixture_resize(self): with self.test_session(): self.assertEqual(y.get_shape(), []) self.assertEqual(int(x.get_shape()[1]), 8)
# todo: ... </s> session = dbsessionfactory.create_session()	get_purchased_album_ids def get_purchased_album_ids(user_id: int):
if not isinstance(self.name_str, (str, unicode)):  # todo remove </s> analysis.add(self._evaluator, 'attribute-error', self.name_str)	find and not (isinstance(self.name_str, pr.NamePart) and isinstance(self.name_str.parent.parent, pr.Param)): debug.dbg('finder._names_to_types: %s, old: %s', names, types) return self._resolve_descriptors(types)
pass # todo: some magic with getstate to find out if it's the </s> modifiers = 0	_dispatch_key symbol = K_RALT elif symbol == K_LSHIFT: if _user32.GetKeyState(VK_SHIFT) & 0xff00: modifiers |= MOD_SHIFT
# todo(ralexstokes) look at better way to handle once we have fork choice in place </s> except finalizedheadnotfound:	BeaconChainSyncer finalized_head = await self.chain_db.coro_get_finalized_head(BeaconBlock) finalized_slot = finalized_head.slot finalized_slot = SERENITY_CONFIG.GENESIS_SLOT self.logger.info(
else: # xxx todo: convert warning into a bqlerror. </s> warnings.warn(	grouped_schema else: ignore.append([var, 'This variable is a key.']) 'Encountered a zero-length column name. Please ' 'revise the .csv such that all columns have headers.')
# todo: use regex matching instead of this </s> assert token_agent.contract_address in result.output	test_nucypher_status_network policy_agent = ContractAgency.get_agent(PolicyManagerAgent, registry=test_registry) adjudicator_agent = ContractAgency.get_agent(AdjudicatorAgent, registry=test_registry) assert staking_agent.contract_address in result.output assert policy_agent.contract_address in result.output
# xxx todo: investigate what the meaning of dominsert is about </s> self.annotationpanel.insert(w, self.annotationpanel.getelement(),	addAnnotationRenderingPanel domInsert = True w = AnnotationRenderingPanel() rpIndex) self.annotationPanel.setWidgetPosition(w, 0, 0)
# todo: figure out how to re-use the same return function in base.py </s> http.status_unauthorized()	check_roles log.debug("Unamepass vfail: %s" % validation_failed) except PulpException, pe: http.header('Content-Type', 'application/json') return json.dumps(pe.value, default=pymongo.json_util.default)
else: # todo: deprecated </s> exportmesh = bobject.to_mesh(self.depsgraph, apply_modifiers, calc_undeformed=false)	export_mesh bobject_eval = bobject.evaluated_get(self.depsgraph) if apply_modifiers else bobject exportMesh = bobject_eval.to_mesh() if exportMesh == None: log.warn(oid + ' was not exported')
# todo: add function names </s> self._storage.store({	function if not details: file_path, file_hash = self._download_and_hash(urls) 'file_hash': file_hash, 'file_path': file_path,
if jaxpr.constvars: raise notimplementederror  # todo(mattjj) </s> env: dict[var, tuple[bool, bool]] = {}	_partial_eval_jaxpr_custom jaxpr: Jaxpr, in_unknowns: Sequence[bool], saveable: Callable[..., bool], ) -> Tuple[Jaxpr, Jaxpr, Sequence[bool], Sequence[bool], int]: residuals: OrderedSet[Var] = OrderedSet() def read(x: Atom) -> Tuple[bool, bool]:
# @todo: move to css </s> _style="float:left;padding-right:10px;"),	duplicates output["add_btn"] = DIV( SPAN(T("You need to have at least 2 records in this list in order to merge them."), A(T("Find more"), _href=r.url(method="", id=0, component_id=0, vars={}))
# todo: serialize the policy </s> response_data = {	grant new_policy = drone_alice.grant(bob, label, m=m, n=n, expiration=expiration_time) 'result': { 'treasure_map': b64encode(bytes(new_policy.treasure_map)).decode(),
# todo(bcipolli): bulk saving of logs </s> log.save()	generate_fake_exercise_logs completion_counter=(date_completed-start_date).total_seconds()) log.full_clean() exercise_logs.append(log) return exercise_logs
# todo: configurable timeout??? </s> syndic_dict['dead_until'] = time.time() + 60	_fire_master except SaltClientError: log.error('Unable to fire event to {0}, trying another...'.format(master)) log.critical('Unable to fire event on ANY master')
# todo: this is lazy, we should only reconfigure the drone(s) who are actually </s> if drone_edge:	_handle_command_bait_user_changed drone_edge = db_session.query(DroneEdge).filter(DroneEdge.username == username, DroneEdge.password == password).first() self._reconfigure_all_clients()
# todo: remove this - a leftover from an earlier version, needed for old </s> self.flt = filteriir(b=np.array(fb.fil[0]['ba'][0][0:3]),	get_hdl_dict } }) a=np.array(fb.fil[0]['ba'][1][0:3]), word_format=(hdl_dict['QI']['WI'] + hdl_dict['QI']['WF'], 0,
# todo: the following skipped suite and fixtures should be enabled </s> return options	_test_options options.update({'pdns_server': 'https://dnsadmin.hhome.me', 'pdns_server_id': 'localhost'})
# todo: refactor </s> if self._selection_matches(selection_content, self.get_value(col)):	StepController return CellInfo(content_type, cell_type) def _get_cell_type(self, col, selection_content): return CellType.HIGHLIGHTED col -= len(self._step.assign)
# todo don't change get_code, the whole thing should be the same. </s> assert src == p.module.get_code()[:-1]	fp p = FastParser(u(src)) cache.save_parser(None, None, p, pickling=False)
# todo: remove this when the checks in `before_run` have been moved to the template </s> self._neuron_index = indices	_check_args indices = indices[I] times = times[I] self._spike_time = times self._spikes_changed = True
# todo: test filter functionality more </s> f = multistagechannelfilter(input_rate=32000000, output_rate=16000, cutoff_freq=3000, transition_width=1200)	test_basic def test_basic(self): self.__run(f, 400000, 16000/32000000)
# pattern for a markdown todo-list () </s> pattern = re.compile(".*-\s" + "(\[[\sx]\]).*")	toggle_checked_property_markdown checked = lambda x: "[x]" in x body = [line.encode('utf-8') for line in markdown_body.split("\n")] for i, line in enumerate(body): if pattern.match(line) is not None and item in line:
# todo: remove getattr when https://github.com/rtfd/readthedocs.org/pull/3339 got merged </s> env_build_image != getattr(self.config, 'build_image', self.version.project.container_image),	is_obsolete return any([ env_python_version != Version(self.config.python_version),
self.assertequal(out.strip(), "inactive") # todo real "failed" </s> logg.info("== 'start' will have a later exiting service as remaining active")	test_4090_simple_service_RemainAfterExit logg.info(" %s =>%s \n%s", cmd, end, out) self.assertEqual(end, 3) cmd = "{systemctl} start zzr.service {vv}" out, end = output2(cmd.format(**locals()))
# todo: sessions and not only dates/days should be considered </s> return true	_barisover_minutes bartm = self.data.datetime.time(index) if bardt > dt: tmpoint = tm.hour * 60 + tm.minute tmmul, tmrem = divmod(tmpoint, self.p.compression)
# todo: until we get it working. </s> if facts.is_from_app_store():	generate_absolute_recipe by this function! keys = recipe["keys"] warn_about_app_store_generation(facts, recipe["type"]) return
# todo: unique constraint </s> self.path_remove_level(level)	validate_list self.error('seq item %s is not %s' % (item, schema_type)) self.path[level] =+ 1
# todo: check arp reply is valid </s> self.asserttrue(self.packet_outs_from_flows(arp_replies))	arp_for_controller 'arp_source_ip': '10.0.0.1', 'arp_target_ip': '10.0.0.254'})
assert ne.test("frexp(z3)", xr.dataarray(xr.ufuncs.frexp(z3)))  # fixme todo fails with xarray >= 0.8 </s> assert ne.test("imag(z1)", xr.ufuncs.imag(z1))	test_2 assert ne.test("fix(z1)", xr.ufuncs.fix(z1)) assert ne.test("floor(z1)", xr.ufuncs.floor(z1)) assert ne.test("iscomplex(z1)", xr.ufuncs.iscomplex(z1)) assert ne.test("isfinite(z1)", xr.ufuncs.isfinite(z1))
# todo test that citext.sql gets loaded with 9.0.x </s> version_info = [('postgresql 9.1.1 blah blah blah',)]	test_setupdb_app_main with mock.patch(self.psycopg2_module_path) as psycopg2: app = setupdb_app.SocorroDB(config) psycopg2.connect().cursor().fetchall.return_value = version_info result = app.main()
# todo(rbharath): this is wrong!! </s> self.out_tensor = h	LSTMStep o = inner_activation(z3) h = o * activation(c) return h, [h, c]
# todo: better to use an inotify method that doesn't conflict with eventlets. </s> while true:	_config_file_stat @kill_on_exception(exc_logname) def _config_file_stat(self): if self.config_hashes: new_config_file_stats = valve_util.stat_config_files(
# todo: send `goodbye` req then disconnect </s> return	Node hello_other_side = await read_req(stream, HelloRequest) except ReadMessageFailure as error: self.logger.debug("Received the hello message %s", hello_other_side) try:
# todo(developer): uncomment and set the following variables </s> client = dataproc.clustercontrollerclient(client_options={	create_cluster def create_cluster(project_id, region, cluster_name): from google.cloud import dataproc_v1 as dataproc 'api_endpoint': '{}-dataproc.googleapis.com:443'.format(region) })
# todo this creates an identical dataframe for every hour. we only need one for all hours. </s> if actual_hour > 1:	fetch_production data_temp = fetch_hourly_production(country_code, obj, 0, formatted_date) data[0] = data_temp url = url_init + formatted_date r = session or requests.session()
# todo: implement me </s> return none	get_enabled_subtitle_track def get_enabled_subtitle_track(self):
for joint in annos:  # todo : speed up with affine transform </s> adjust_joint = []	keypoint_random_rotate img = ret[newy:newy + newh, newx:newx + neww] adjust_joint_list = [] for point in joint: if point[0] < -100 or point[1] < -100:
# todo: this loop is pretty slow .. (parellize) </s> for iky in range(self.nky):	getJ Jtv_temp0 = np.zeros((m.size, rx.nD), dtype=float) Jtv = np.zeros((m.size, rx.nD), dtype=float) u_src = f[src, self._solutionType, iky] ky = self.kys[iky]
# todo warn the user? </s> if error != 0x17:	render self.font.dpi, self.font.dpi) FreeTypeError.check_and_raise_on_error('Could not set size for "%c"' % text[0], error) glyph_index = get_fontconfig().char_index(face, text[0])
# todo(somebody): make this a literal type. </s> justify: str = 'rjust'):	testParamListIndentationCollision1 class _(): def __init__(self, title: Optional[str], diffs: Collection[BinaryDiff] = (), charset: Union[Type[AsciiCharset], Type[LineCharset]] = AsciiCharset, preprocess: Callable[[str], str] = identity, self._cs = charset self._preprocess = preprocess
# todo(twd2): do more visibility check eg. contest </s> uids = {ddoc['owner_uid']}	DiscussionDetailHandler elif vnode['doc_type'] == document.TYPE_PROBLEM and vnode.get('hidden', False): self.check_perm(builtin.PERM_VIEW_PROBLEM_HIDDEN) uids.update(drdoc['owner_uid'] for drdoc in drdocs) for drdoc in drdocs:
#todo(sbaker) check for a non-default router for this network </s> client.add_interface_router(	handle_create } subnet = client.create_subnet({'subnet': props})['subnet'] router_id, {'subnet_id': subnet['id']})
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'or' rule " + "also to triggered.") + "remote id '%d' and username '%s' " % (element.element.remoteSensorId,
# todo: check for ip subnet/range and break it out to individuals </s> log.debug("exclude list: %s" % repr(ip_exclude))	nmap_scan if data: ip_target = data.split('\r\n') log.debug("Target List: %s" % repr(ip_target)) N_ARGS = "nmap -F "
# todo(ytknzw): add more specific assertion with the test case. </s> study = create_study()	test_plot_intermediate_values trial.report(2.0, step=1) return 0.0 study.optimize(lambda t: objective(t, True), n_trials=1) figure = plot_intermediate_values(study)
# todo: pico-8 doesn't allow multiline strings, so this probably </s> if self._in_string is not none:	_process_token The number of characters processed from the beginning of the string. i = 0 while i < len(s): c = s[i]
# @todo: follow global settings: </s> settings.ui.filter_auto_submit = 750	hrm_human_resource_controller return True elif method == "summary": settings.ui.report_auto_submit = 750 s3.crud_strings["hrm_human_resource"]["title_list"] = T("Staff & Volunteers")
#todo link to some protocol for reporting this </s> self.log.exception(	_make_call call_name) except ValidationException: "please report the following unknown response format for %s: %r", call_name, msg
# todo: rewrite in python. </s> command = [	folderify offset_white = 1 opacity_white = 100 convert_path, template_icon, "(", "(", TEMP_MASK_IMAGE, "-negate", "-colorize", "3,23,40", "-negate", ")",
# todo check the op returned a view </s> if idx not in dmap and idx not in vmap and not isinstance(v, str):	count_min_memory vmap = getattr(node.op, 'view_map', None) for idx, v in enumerate(val): running_memory_size += v if running_memory_size > running_max_memory_size:
""" todo: documentation </s> raise notimplementederror('streamingcommand.stream(self, records)')	stream def stream(self, records):
continue ### todo </s> if tag_name == 'image':	test_sanitizer continue ### TODO if tag_name != tag_name.lower(): yield (runSanitizerTest, "test_should_allow_%s_tag" % tag_name, "<img title=\"1\"/>foo &lt;bad&gt;bar&lt;/bad&gt; baz",
""" todo: documentation </s> return false	streaming @property def streaming(self):
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: spaces should depend on the horizontal </s> plt.subplots_adjust(wspace=.05, hspace=.05)	adjust_facet_space def adjust_facet_space(self):
# todo raise expected values once tests complete successfully </s> self.asserttrue(scores.mean() > 0.65,	test_crossval duration_col='t', event_col='E', k=3) "CoxPH should solve this simple data")
#todo(bcwaldon): implement optional kwargs such as limit, sort_dir </s> super(_fakeimageservice, self).__init__()	__init__ self._imagedata = {}
# todo: kept for backwards compatibility. </s> self.text = text	__init__ self.plugin_name = plugin_name self.report_dict = None self.time_compiled = None
# todo check for if-modified-since and if-none-match </s> response.response = wrap_file(environ, io.open(self.path, 'rb'))	RootHandler datetime.utcfromtimestamp(os.path.getmtime(self.path))\ .replace(microsecond=0) response.direct_passthrough = True return response
pass # todo </s> shows which commands the current user does not have access to.	_notcommands ``notcommands``
# todo: reconsider this. </s> value is not none and value is not false and	merge_into unicode = type(u'u') if (base_config[key] is not None and not isinstance(value, type(base_config[key])) and (not isinstance(value, (unicode, str)) or
# todo: ugly n^2 </s> c.sons = [	comment_list_helper c.level = len(c.path.split('/')) for c in comment_list: i for i in comment_list \ if i.path.startswith(c.path) and i.level == c.level+1
# todo: specific exception handling </s> term_hierarchy = default	__read_term_hierarchy_file f.close() except: return term_hierarchy
# todo: handle return value from sandbox </s> input: /test.php?p=http%3a%2f%21durch0.de%2ftest_file.txt	test_rfi_emulator_quoted_url def test_rfi_emulator_quoted_url(self): Expected Result: The return value from the PHP sandbox. Notes: Injected file contains <?php echo("test successful"); ?>"""
# todo: remove this hack asap </s> _load_skills()	_starting_up time.sleep(0.5)  # Allows system time to start the eyes spinning subprocess.call('systemctl reboot -i', shell=True)
# todo constrain value to be in the directory </s> load_list_js.append(plugin_resource_url + resource_def.load_js_path)	__init__ load_list_css.append(plugin_resource_url + resource_def.load_cs_path) if resource_def.load_js_path is not None: client.putChild('plugin-index.json', static.Data(_json_encoder_for_values.encode({ u'css': load_list_css,
# todo: add bot's signature if needed (bug: t131517) </s> content = i18n.twtranslate(	delete_redirect page.title(withSection=False)) content = content_page.get(get_redirect=True) page.site, 'redirect-broken-redirect-template') + '\n' + content
# todo: scale and translation could be merged into a single network </s> with tf.variable_scope("scale", reuse=tf.auto_reuse):	backward_and_jacobian mask = self.get_mask(inputs, dtype=inputs.dtype) masked_inputs = inputs * mask scale = mask * self.scale_fn(masked_inputs) with tf.variable_scope("translation", reuse=tf.AUTO_REUSE):
# todo(nate): temporarily disabled </s> except exception:	process self.logger.warning('manifest.json had wrong step id (build=%s): expected %s but got %s', self.step.job.build_id.hex, self.step.id.hex, contents['job_step_id']) self.logger.exception('Failed to parse manifest.json; (build=%s, step=%s)', self.step.job.build_id.hex, self.step.id.hex)
# todo: write tests </s> <tuplet> a group of notes with "irregular" (sometimes called "irrational") rhythmic values,	tupletFromElement def tupletFromElement(elem, slurBundle=None): for example, three notes in the time normally occupied by two or nine in the time of five. In MEI 2013: pg.473 (487 in PDF) (MEI.cmn module)
#todo - what is bio.popgen using this for? </s> self.param_types = types	_Option is_required = False, description = "", equate=True): self.names = names self.checker_function = checker_function self.description = description
#todo: review this carefully, as we are now deleting stuff </s> for f in file_set - vcs_file_set:	add_files new_path = os.path.join(podir_path, d) os.makedirs(new_path) remove_path = os.path.join(podir_path, f) os.remove(remove_path)
# todo: i18n/l10n: spaces aren't always the correct word separator </s> words = search.split(" ")	_update_search_field print "Selected suggestion: " + selection search = unicode(self.searchField.get_text()) search = "" for word in words:
# todo: must be implemented </s> pass	range_from_chapters def range_from_chapters(crawler, times=0):
# todo: hacked @ hackathon, fix the comments </s> if id(self) in self.inited:	store_initial_state def store_initial_state(self): return block = create_xblock(self)
# todo: i can't manage the import issue, can you? </s> new_self, new_args = syft.frameworks.torch.hook_args.hook_method_args(cmd, self, args)	handle_method_command cmd, self, args = command print("Logtensor logging method", cmd) new_command = (cmd, new_self, new_args) response = type(new_self).handle_method_command(new_command)
# todo packages for cloudera not available on lucid yet, using karmic for the moment (beta 1) </s> "deb http://archive.cloudera.com/debian karmic-cdh3b1 contrib",	ec2_ubuntu_environment "deb http://downloads.mongodb.org/distros/ubuntu 10.4 10gen",
# todo candidate for move to system/osi as not btrfs related </s> dd_cmd = [dd, 'if=/dev/%s' % disk, 'of=/dev/null', 'bs=512',	blink_disk def blink_disk(disk, total_exec, read, sleep): 'conv=noerror'] p = subprocess.Popen(DD_CMD, shell=False, stdout=subprocess.PIPE,
# # todo: add error handler </s> return json.dumps(request_dict)	build_enrichment_request_json def build_enrichment_request_json(module_name, var_name, var_value): request_dict = {'module': module_name, var_name: var_value}
# todo: remove </s> command_names_not_requiring_auth = [	_authenticate_filter return self._call_next_filter( request, response, filter_chain) c.name for c in protocol.mpd_commands if not c.auth_required] if command_name in command_names_not_requiring_auth:
def getconfig(self, option, default=''):  #@todo: remove in 0.4.10 </s> try:	getConfig return self.getConf(option) except KeyError:
#todo: consider factoring out: some duplication between xliff and tmx </s> notelist = self.getnotelist(origin="pofilter")	geterrors def geterrors(self): errordict = {} for note in notelist:
# todo: allow other formats? </s> im = image.open(album_art.file)	save_album_art im_path = os.path.join(config.image_dir, 'media/%d%%s.%%s' % media.id) try: for size in ['ss', 's', 'm', 'l']: file_path = im_path % (size, 'jpg')
# todo: remove in 0.9.0 </s> def _warning_wrapper(*args, **kwargs):	_warning_wrapper warnings.warn("Starting from transitions version 0.8.3, 'is_<state_name>' convenience functions will be" " assigned to 'is_<model_attribute>_<state_name>' when 'model_attribute "
# todo action required that updates the endpoint </s> return self.sdnc.remove_inactive_endpoints()	remove_inactives def remove_inactives(self, args):
# todo implement </s> try:	get def get(self, k, d=None): pass except KeyError:
# todo extend to nonbinary nodes </s> if i not in self._input_indices and i in self.context.node_indices:	__init__ else: self._dimension_labels.append(None) past_tpm_on = past_tpm_on.sum(i, keepdims=True) / 2 past_tpm_off = past_tpm_off.sum(i, keepdims=True) / 2
# todo: using html formating tags eg. <pre> in ocil content </s> if self.ocil:	to_ocil boolean_question = ET.Element( "boolean_question", id=self.id_ + "_question") ocil_without_tags = re.sub(r"</?[^>]+>", "", self.ocil) else:
# todo: use lazylist </s> path = api_root + 'zones/%s/hosts.xml' % (zone.id)	list_records def list_records(self, zone): self.connection.set_context({'resource': 'zone', 'id': zone.id}) data = self.connection.request(path).object
else:  # todo assuming 720 webcam for now </s> if p['fps'] <= 30:	_bitrate else: cvbr = br60[y] cvbr = br30['720'] else:
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
# todo make this a private api </s> places = []	parse_code @staticmethod def parse_code(results): for result in results.get('RESULTS'): coordinates = result.get('COORDINATES', {})
# todo handle bad type </s> if type != none:	sharedproperty value = self.extra_props[name] del self.extra_props[name] value = type(value) self.__dict__[name] = value
# todo: fatal, how should this be handled? </s> raise exception()	_on_pad_added sink_pad = self._conv.get_compatible_pad(pad, None) if sink_pad is None: pad.link(sink_pad)
# todo: make remoteapp.create_all_files not return media files </s> extension = os.path.splitext(name)[1]	_files name = name.replace(build_profile_id + '/', '') if name not in skip_files: data = _encode_if_unicode(f) if extension in text_extensions else f yield (_get_name(name), data)
# todo: only move interfaces attached to self.wlan, or all nodenum in script? </s> moved_ifaces = []	movenodesinitial Move nodes to their initial positions. Then calculate the ranges. :return: nothing for iface in self.net.get_ifaces(): node = iface.node
# todo: skip anything that's not marathon </s> if 'tasks' in framework:	find_container_id if 'frameworks' in state: for framework in state['frameworks']: for task in framework['tasks']: if 'id' in task and app_id in task['id']:
# todo this is a bit jankey to be honest </s> if apps_changed:	activate_integration_app settings.INSTALLED_APPS += [plugin_path] apps_changed = True apps.app_configs = OrderedDict() apps.apps_ready = apps.models_ready = apps.loading = apps.ready = False
# todo: can just leave this in superclass </s> return timeout	get_default_timeout def get_default_timeout(self):
# todo - molecule type - see issue 363 / pull request #1005 </s> d = consumer.data.annotations.get('data_file_division', none)	test_topology_genbank self.assertEqual(t, topo, "Wrong topology %r not %r from %r" % (t, topo, line)) self.assertEqual(d, div, "Wrong division %r not %r from %r" % (d, div, line))
# todo get stacktrace </s> raise error, ex.getmessage()	_handle_sql_exception_jython from java.sql import SQLException if isinstance(ex, SQLException): else: raise ex
# todo check if config was successfully updated </s> status = true	unmirror_mac 'unmirror', int(port), switch): self.send_file('config') else: status = self.config(
# todo check if obsolete </s> scene_name, _ = common.decode_string(data, 0)	build_scene_removed def build_scene_removed(data): logger.info("build_scene_removed %s", scene_name) scene = share_data.blender_scenes.get(scene_name)
# todo: check the result </s> matrix44.create_from_eulers([1,2,3])	test_create_from_eulers def test_create_from_eulers(self):
# todo: do we really need to populate the unnamed reg when we're populating the small </s> vi_cmd_data = {	testDoesNotPopulateSmallDeleteRegisterIfWeShouldNot def testDoesNotPopulateSmallDeleteRegisterIfWeShouldNot(self): 'can_yank': False, 'populates_small_delete_register': True,
# todo: log exception </s> pass	main os.remove(filename[:-2] + 'c') except Exception as e: walk = os.walk(LIBS) for path in walk:
# arno, 2010-02-10: todo: convert infohash to binary </s> return self._db.fetchall(sql,(publisher_id,))	getTorrentsFromPublisherId def getTorrentsFromPublisherId(self, publisher_id): ## sql = "select * from Torrent where infohash in (select infohash from ChannelCast where publisher_id==? ) and name<>'' "
pass # todo </s> def handle_request(self, input):	handle_request
# todo(jamalex): burn it all down! </s> if lang_code == "pt-br":	get_dubbed_video_map logging.debug("Adding dubbed video map entry for %s (name=%s)" % (get_langcode_map(lang_name), lang_name)) DUBBED_VIDEO_MAP[get_langcode_map(lang_name)] = video_map lang_code = "pt" return DUBBED_VIDEO_MAP.get(lang_code, {}) if lang_code else DUBBED_VIDEO_MAP
# todo: py2 does not support multiple * unpacking expressions in a single call. </s> packed_args = check_fix_args + chunk	run_isort returncode = 0 for chunk in grouper(file_list, 100): output = isort_cmd(*packed_args, fail_on_error=False, output=str, error=str) returncode |= isort_cmd.returncode
# todo: check that the performance measure is within some range </s> bottleneck1_baseline(num_runs=1, sumo_binary="sumo")	test_bottleneck1 Tests flow/benchmark/baselines/bottleneck1.py
# todo: /data/local/tmp might not be execuable and atx-agent can be somewhere else </s> ad = self._adb_device	_start_atx_agent def _start_atx_agent(self): warnings.warn("start atx-agent ...", RuntimeWarning) ad.shell([self._atx_agent_path, "server", "--stop"]) ad.shell([self._atx_agent_path, "server", "--nouia", "-d"])
# todo check for collision with user filter </s> user_inner = cutoff_freq - transition_width / 2	__do_taps firdes.WIN_HAMMING) else: limit = stage_output_rate / 2 taps = firdes.low_pass(
f, en, nm = getcnns(level=1) # todo more flexible load needed. </s> landmark = en.forward(face[:, :, :31, :])	EN face = cv2.resize(face, (39, 39)).reshape((1, 1, 39, 39)) face = processImage(face) return landmark
# todo: improve this code. </s> if self._arch_info.architecture_mode == arch_x86_mode_64 and \	_translate_div tb.add(self._builder.gen_div(tmp4, tmp0, tmp5)) tb.add(self._builder.gen_mod(tmp4, tmp0, tmp6)) result_low.size == 32: if result_low.name in tb._regs_mapper:
# todo: this add_metadata call should be removed once we are </s> question.add_metadata(category=category['key'])	aaq question.products.add(p) if category: t = category.get('topic') if t:
# todo: fix self.cursor_x >= w </s> line = self.win_y + self.cursor_y	main_cmd_next_bracket def main_cmd_next_bracket(self, h, w): x = self.cursor_x char = self.output.lines[line][x]
# todo handle file does not exist </s> minio.copy(	_duplicate_with_slug_and_delta_id ) assert new_key != uploaded_file.key minio.UserFilesBucket, new_key,
# todo: list is incomplete, to be completed for missing languages. </s> self.doc_subpages = {	__init__ 'simple': self.alphabetic } '_default': ((u'/doc', ), ['en']
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> sampleparameters = {	test_acquire_token_with_user_pass def test_acquire_token_with_user_pass(self): "tenant" : "common", "authorityHostUrl" : "https://login.windows.net",
# todo debug </s> print ("rule element evaluates "	_evaluateRuleElementsRecursively + "to the same triggered value as 'not' rule. " + "Toggle triggered value of 'not' rule.") + "to the same triggered value as 'not' rule. " + "Toggle triggered value of 'not' rule.")
self.select(1)  # todo this should work without havng to use select() i.e. if cursor is at point on target. </s> self.feed('cs]{')	test_documentation_examples self.feed('ysiw]') self.assertNormal('|[Hello] world!') self.assertNormal('|{ Hello } world!') self.feed('yss)')
# todo: disconnect </s> return	Node except asyncio.TimeoutError: await stream.reset() self.logger.debug(f"Received the hello message {hello_other_side}") if not (await self._validate_hello_req(hello_other_side)):
# todo: remove need for this </s> return self._review_branch	review_branch def review_branch(self):
# todo: compute the weighted average instead of using the first solid </s> startpt = obj.solids()[0].center()	_rot def _rot(obj): endPt = startPt + endVec return obj.rotate(startPt, endPt, angleDegrees)
# todo problem - if there are no valid indices, we cannot return anything </s> probabilities = tf.ones(shape=[num_records, tf.shape(indices)[0]])	_computation_get_records mask=mask ) samples = tf.multinomial(logits=probabilities, num_samples=num_records) samples = tf.Print(samples, [samples, tf.shape(samples)], summarize=100, message='samples / shape = ')
# todo candidate for move to system/osi as not btrfs related </s> smap = {	convert_to_KiB def convert_to_KiB(size): 'KiB': 1, 'MiB': 1024,
# todo: remnants from rllab -> gym conversion </s> self.set_state(qpos, qvel)	reset_model qvel[self.PUCK_INDS] = 0 qvel[self.TARGET_INDS] = 0 return self._get_obs()
#todo: this isn't actually most_recently_used (as defined in histories) </s> if( ( trans.user == none )	show hda_dict = {} try: and ( history_id == trans.security.encode_id( trans.history.id ) ) ): history = trans.history
# todo: distributed search messages need to implemented </s> self.logmessage("%s %s" %(msg.__class__, vars(msg)), 4)	ChildDepth def ChildDepth(self, msg):
# todo: figure out how to get multiscanner to report </s> original_filename = file_.filename	create_task task_ids = [] for file_ in request.files.getlist('file'): f_name = hashlib.sha256(file_.read()).hexdigest() file_.seek(0)
# todo use deepcopy() here </s> return polygonsonimage(polys_cut_flat, shape=self.shape)	clip_out_of_image ] polys_cut_flat = [poly for poly_lst in polys_cut for poly in poly_lst]
# todo: remove all elements of the list and remove the blacklist </s> blacklist = [	test_no_experimental_api def test_no_experimental_api(): "tensorflow_addons/optimizers/weight_decay_optimizers.py", ]
# todo the following line should use lazy lookup name-to-index mapping </s> feature.lookuplistindex = [int(l) for l in lookups.split(',')]	parseFeatureList feature = featureRec.Feature feature.FeatureParams = None feature.LookupCount = len(feature.LookupListIndex) self.FeatureCount = len(self.FeatureRecord)
# now we can kill it. todo: on a slow machine, the node might kill </s> def _stop(res):	test_client_no_noise return os.path.exists(PORTNUMFILE) d.addCallback(lambda res: self.poll(_node_has_started)) self.failUnless(os.path.exists(TWISTD_PID_FILE), (TWISTD_PID_FILE, os.listdir(os.path.dirname(TWISTD_PID_FILE)))) return utils.getProcessOutputAndValue(bintahoe, args=["--quiet", "stop", c1], env=os.environ)
# todo refactor set position cursor after operation into reusable api. </s> line = self.view.line(self.view.sel()[0].b)	_vi_left_square_bracket_c def run(self, mode=None, count=1): self.view.run_command('git_gutter_prev_change', {'count': count, 'wrap': False}) if line.size() > 0: pt = self.view.find('^\\s*', line.begin()).end()
# todo: then observer.events[0] == trial </s> def test_hyperparams_json_repository_should_be_observable_in_memory():	test_hyperparams_json_repository_should_be_observable_in_memory repo: HyperparamsJSONRepository = HyperparamsJSONRepository()
# todo: cancel the previous calllater? </s> reactor.calllater(3 * 60, self._time_things_out_maybe)	request def request(self, request): self.last_request = time.time() d = self.conn.request(request) def update_request_time(res):
# todo test that labels in individual blocks are indeed different </s> assert max_id > 1, f"expect more than one segment in watershed result"	test_parallel_watershed_2d assert max_id == max_expected, f"Expect {max_expected} but got {max_id}"
# todo: use lt and rt in profile as well </s> side = left if id else rigth	set_action self.button_widgets[id.name].update() elif id in TRIGGERS: before, profile.triggers[side] = profile.triggers[side], action self.button_widgets[id].update()
# todo: end remove hosts/when block </s> if pseudo_state.deploy_dir:	include if pseudo_host not in hosts: return filename = path.join(pseudo_state.deploy_dir, filename) frameinfo = get_caller_frameinfo()
# todo: headervalueerror does not belong here </s> except (headervalueerror, invalidexcelfileexception) as e:	read_uploaded_app_translation_file try: workbook = WorkbookJSONReader(f) msgs.append( (messages.error, _(APP_TRANSLATION_UPLOAD_FAIL_MESSAGE).format(e))
# todo use a proper category instead </s> "tags": [config["full_project_name"]]},	wordpress_edit_page data={"title": title, "content": content, headers=_wordpress_headers()) response.raise_for_status()
# todo: uncomment when adding support for literal hex bytes </s> print(bytearray(b'hello world   ').isupper())	test_isupper print(bytearray(b'hello world').isupper())
# todo add test </s> result = []	_augment_heatmaps def _augment_heatmaps(self, heatmaps, random_state, parents, hooks): nb_heatmaps = len(heatmaps) seeds = random_state.randint(0, 10**6, (nb_heatmaps,))
self.assertequal(r[1], 0)  # todo: that is success? </s> key = token.token.get_otpkey().getkey()	test_06_reuse r = _step1() self.assertEqual(r[0], True) self.assertEqual(key, "X" * 24 + "Y" * 224) token.set_pin("anotherpin")
# todo: add hook to be called after each song </s> except exception as e:	MusicBot try: songs_added = await player.playlist.async_process_youtube_playlist(playlist_url, channel=channel, author=author) traceback.print_exc() raise CommandError('Error handling playlist %s queuing.' % playlist_url)
# todo: verify </s> mass = elem.mass()	build_Mgg etype = elem.type if etype in ['CROD', 'CONROD', 'CTUBE']: nid1, nid2 = elem.nodes i1 = dof_map[(nid1, 1)]
# @todo: bulk lookup </s> ltable = s3db.cms_post_user	cms_post_list_layout user = auth.user if user: #and settings.get_cms_bookmarks(): query = (ltable.post_id == record_id) & \ (ltable.user_id == user.id)
# todo generator </s> handle_dirty_dataset(ds, mode=if_dirty)	__call__ for ds_path in content_by_ds: ds = Dataset(ds_path) content = [ap['path'] for ap in content_by_ds[ds_path] if ap.get('type', None) != 'dataset' or ap['path'] == ds.path]
# todo: cache this result so multiple failing calls don't keep hitting the db </s> return none	sql_product self._sql_product = SQLProduct.objects.get(domain=self.domain, product_id=self.entry_id) except ObjectDoesNotExist: return self._sql_product
# todo: add ssl verification </s> configuration = client.configuration()	get_core_v1_api_object def get_core_v1_api_object(cluster_name, cluster_endpoint, challenge, evalai): aws_eks_api = evalai.get_aws_eks_bearer_token(challenge.get("id")) configuration.host = cluster_endpoint
# todo: is that right? </s> pass	unindex_questions es.delete(index, doc_type=Question._meta.db_table, id=doc_id) except pyes.exceptions.NotFoundException:
# todo placeholder; implement </s> return []	get_all_roles def get_all_roles() -> Iterable[str]: Returns the names of all roles in this application.
raise notimplementederror  # todo </s> def process_call(self, call_primitive, f, tracers, params):	process_call
## todo : log error </s> error_code = delete_functionparameters_doctype_submission(doctype=doctype, action=action)	_delete_submission_from_doctype user_msg.append("""When deleting Submission "%s" from Document Type "%s", it wasn't possible to delete all Submission Fields""" \ % (action, doctype)) if error_code != 0: user_msg.append("""When deleting Submission "%s" from Document Type "%s", it wasn't possible to delete all Function Parameters""" \
# todo: verify </s> manager.content.install(self.consumer_id, units, options)	test_content_install manager = factory.consumer_agent_manager()
# todo: move this code into renderengine. </s> parsed_section, section_end_index, end_index = self._parse_section(template, end_index, tag_key)	_handle_tag_type template, section_start_index, section_end_index) elif tag_type == '^': func = engine._make_get_inverse(tag_key, parsed_section) elif tag_type == '>':
# todo: this would be a good candidate for refactoring into a testcase subclass shared across backends </s> expected_pks = [str(i) for i in [3, 2, 4, 5, 6, 7, 8, 9, 10, 11]]	test_values_list_slice reset_search_queries() self.assertEqual(len(connections['solr'].queries), 0) results = self.sqs.all().order_by('pub_date').values('pk') self.assertListEqual([i['pk'] for i in results[1:11]], expected_pks)
# todo(jakevdp): in rare cases, this fails python_should_be_executing check. why? </s> self._checkagainstnumpy(f_dense, jit(f_sparse), args_maker)	test_bcoo_rdot_general_contract_and_batch self._CheckAgainstNumpy(f_dense, f_sparse, args_maker)
# todo: use xml config to get mac address and then parse ips </s> extra = {'uuid': domain.uuidstring(), 'os_type': domain.ostype(),	list_nodes else: state = NodeState.UNKNOWN 'types': self.connection.getType()} node = Node(id=domain.ID(), name=domain.name(), state=state,
# todo: refactor </s> if mode is none:	iterator else: _deprecated_interface = False if hasattr(self, '_iter_subset_class'): mode = self._iter_subset_class
#todo - negative indices, see bug 2411 </s> else :	compare_sequences if l < 50 : indices = range(l) indices = [0,1,2,int(l/2),l-2,l-1] for i in indices :
# todo: 'annex.backends' actually is a space separated list. </s> if persistent:	set_default_backend If persistent, would add/commit to .gitattributes. If not -- would set within .git/config git_attributes_file = _path_(self.path, '.gitattributes') git_attributes = ''
# todo: we should probably have a special folder just for header </s> import brian2.synapses as synapses	__init__ self.include_dirs = list(prefs['codegen.cpp.include_dirs']) self.include_dirs += [os.path.join(sys.prefix, 'include')] synapses_dir = os.path.dirname(synapses.__file__) self.include_dirs.append(synapses_dir)
# todo: attributes </s> assert attributes is none, "internal error" # see above	_create_equiv ann_obj.add_annotation(ann) mods.addition(ann) else: assert projectconf.is_equiv_type(old_type), 'attempting to change equiv relation to non-equiv relation, operation not supported'
# todo: remove compatability hook </s> shutil.copyfile(os.path.join(self.freeze_dir,esky_control_dir,"lockfile.txt"),os.path.join(self.freeze_dir,"esky-lockfile.txt"))	_run with open(lockfile,"w") as lf: lf.write("this file is used by esky to lock the version dir\n") shutil.copyfile(os.path.join(self.freeze_dir,ESKY_CONTROL_DIR,"bootstrap-manifest.txt"),os.path.join(self.freeze_dir,"esky-bootstrap.txt")) print "zipping up the esky"
# todo: cast the value? </s> yield entry	charentries def charentries(self, tag=TAGS.CHARVAL): for entry in self.get_tag_entries(tag=tag):
# todo pseudo code: </s> pass	Seek @dbus.service.method(dbus_interface=player_interface) def Seek(self, offset):
# todo: displacement. </s> else:	filter_apply if add_node: filtered_textures.append(currentNode) if export_settings['gltf_common'] != '-': for currentTextureSlot in currentMaterial.texture_slots:
# todo: conflict detection/resolution </s> for key in d:	_collect_analysis if not d: continue analysis.setdefault(key, {}).update(d[key]) return analysis
# todo make "master" not hard-coded, fetch it from some metadata </s> branchname = default_branch	commitVolume self.output(commitId) volume = self._directory.child(volume) branch = volume.child("branches").child(branchName) commit = volume.child("commits").child(commitId)
# todo(piyush): current api-site doesn't contain this api description. </s> put_body = json.dumps({'user': kwargs})	update_user_password def update_user_password(self, user_id, **kwargs): resp, body = self.put('users/%s/OS-KSADM/password' % user_id, put_body) self.expected_success(200, resp.status)
federated_only=federated_only  # todo: 289 </s> )	batch_from_bytes dht_host=dht_info.host, dht_port=dht_info.port, stranger_ursulas.append(stranger_ursula_from_public_keys) return stranger_ursulas
# todo: stop at minimum scale </s> self.scalechanged.emit(self.current_scale_)	setScaleAbsolute self.setMatrix(m)
# todo: and results </s> success = yield asynclist(tasks)	auto_remove eject=eject, lock=lock)) else: self._log.debug(_('not removing {0}: unhandled device', device))
# todo(b/179510447): align these parameters with schulman 17. </s> return tf.keras.layers.dense(	means_layers def means_layers(): action_tensor_spec.shape.num_elements(), kernel_initializer=tf.keras.initializers.VarianceScaling(scale=0.1),
# todo: make this pretty </s> return httpresponse('error retrieving locations: is the storage server running? please contact administrator.')	sources locations = storage_service.get_location(purpose="TS") except: system_directory_description = 'Available transfer source' return render(request, 'administration/locations.html', locals())
# todo: another solution should be used here. this is a hack for compatibility reasons. to resolve the gadget address calculation of segments of elf files have a different base address if calculated segment.virtualaddress - segment.offset </s> offset = section.offset - (binary.originalimagebase - (section.virtualaddress - section.offset))	_searchOpcode toReturn = [] code = bytearray(section.bytes) for match in re.finditer(opcode, code): opcodeGadget = Gadget(binary.checksum, section.name, binary.arch)
# todo: repeating timers </s> for timer in self._timer.itervalues():	_CheckTimers def _CheckTimers(self, now): if now < (timer.start + timer.interval): continue
# todo: in 0.6.0 change this to "disabled": false </s> "enabled": true,	test_server_bridge "dev_type": "tap", "dh": "dh.pem", "key": "key.pem", "mode": "server",
# xxx - 'todo' should just be a string </s> self.unexpectedsuccesses.append((test, todo))	TestResult @type test: L{pyunit.TestCase} @type todo: L{unittest.Todo} def addExpectedFailure(self, test, error, todo): In Trial, tests can be marked 'todo'. That is, they are expected to fail.
# todo: support delete by name </s> action = action.get_by_id(id)	delete POST /actions/1?_method=delete DELETE /actions/1 Action.delete(action)
#todo(#212): use a map construct instead of unrolling. </s> lhs = batching.move_dim_to_front(lhs, lhs_bdim)	conv_general_dilated_batch_rule lhs_dim, rhs_dim, out_dim = dimension_numbers if lhs_bdim is not None and rhs_bdim is not None: rhs = batching.move_dim_to_front(rhs, rhs_bdim) outputs = [
#todo: check cost line </s> return {}	isGroundBuildRequirementSatisfied @classmethod def isGroundBuildRequirementSatisfied(cls, x, y, island, **state):
# todo: remove when migration plan / state is passed (#24100). </s> if not is_latest_migration_applied('auth'):	create_permissions def create_permissions(app_config, verbosity=2, interactive=True, using=DEFAULT_DB_ALIAS, **kwargs): return if not app_config.models_module:
# todo: what is this here for? we should really be catching something </s> return []	get_viewable_reports return models except AttributeError:
# todo add sp_playlist_* methods </s> self.sp_playlist = ffi.gc(sp_playlist, lib.sp_playlist_release)	__init__ lib.sp_playlist_add_ref(sp_playlist)
# todo(slaweq): change this to neutron floating ips and turn neutron </s> mock_nova.floating_ips.list.return_value = [fake_floating_ip]	test_rebuild_server_server_error mock_nova.servers.rebuild.return_value = rebuild_server mock_nova.servers.list.return_value = [error_server] self.assertRaises( exc.OpenStackCloudException,
# todo: update hits@k </s> hits_at_k = none	compute_hits_at_k triples=triples, corrupt_suject=False, compute_hits_at_k=False) stop = timeit.default_timer() log.info("Evaluation took %s seconds \n" % (str(round(stop - start))))
# todo: distributed search messages need to implemented </s> self.logmessage("%s %s" %(msg.__class__, vars(msg)), 4)	BranchRoot def BranchRoot(self, msg):
# todo: check proactive neighbor resolution </s> self.asserttrue(self.packet_outs_from_flows(echo_replies))	icmp_ping_unknown_neighbor 'ipv4_dst': '10.0.0.99', 'echo_request_data': bytes('A'*8, encoding='UTF-8')})
# todo: this should be written to a log </s> print(f'pickled dag {dag} as pickle_id: {pickle_id}')	_run_task_by_executor session.add(pickle) pickle_id = pickle.id except Exception as e: print('Could not pickle the DAG')
# todo: sysex messages do not arrive here. </s> def _pending(self):	_pending if self._has_callback: raise IOError('a callback is currently set for this port')
# todo pydocs </s> def __init__(self, service, project_id):	BigQueryBaseCursor class BigQueryBaseCursor(object): self.service = service self.project_id = project_id
entry['meta']['type'] = 'padding'  # todo handle padding, summarize and transfer </s> entry['flag']       = posting.flag	serialize_entry if isinstance(posting, Transaction): if posting.flag == 'P': entry['payee']      = posting.payee entry['narration']  = posting.narration
# todo: test. </s> return next_cursor, previous_cursor, users	GetListMembersPaged users = [User.NewFromJsonDict(user) for user in data.get('users', [])]
# todo: remove patch and update test once calculation_magic is implemented </s> @mock.patch("sentry.tasks.low_priority_symbolication.calculation_magic", lambda x, y: true)	test_has_metric_not_in_lpq @freeze_time(datetime.fromtimestamp(0)) def test_has_metric_not_in_lpq(self, store) -> None: store.increment_project_event_counter(17, 0)
# todo be more fussy with prefix checking: validate strings </s> useless = [	get_user_locale langs = ["en-US"] else: prefix for prefix in self.contest.allowed_localizations if not any(lang.startswith(prefix) for lang in langs)]
# todo: division of mimo transfer function objects is quite difficult. </s> def __rdiv__(self, sys):	xTransferFunction den = sp.polymul(self.den[0][0], other.num[0][0]) return xTransferFunction(num, den) if self.inputs > 1 or self.outputs > 1 or \ other.inputs > 1 or other.outputs > 1:
# todo(b/141131288): enable test once complex sort is supported on tpu. </s> if (jnp.issubdtype(dtype, jnp.complexfloating)	testArgsort for axis in (None, *range(len(shape))))) def testArgsort(self, dtype, shape, axis): and jtu.device_under_test() == "tpu"): self.skipTest("complex sort not supported on TPU")
# todo: support ipv6 addresses as well. </s> tcp_listener = await asyncio.start_server(	BaseServer self.logger.info('network: %s', self.network_id) self.logger.info('peers: max_peers=%s', self.max_peers) self.receive_handshake, host=BOUND_IP,
# todo: optimizer state gets cast to fp16 and back to fp32 for </s> for group in self.param_groups:	load_state_dict def load_state_dict(self, state_dict: Dict[str, Any]) -> None: super().load_state_dict(state_dict) for p in group["params"]: self.state[p]["exp_avg"] = self.state[p]["exp_avg"].type(self.optim_type)
# todo: this ought to be a separate test of block-resources </s> def callback((response, data)):	callback self.assertEqual(response.code, http.OK) description_json = json.loads(data)
if not config.testnet:  # todo </s> return	compose def compose (db, source, contract_id, gasprice, startgas, value, payload_hex): block = blocks.Block(db, util.last_block(db)['block_hash']) code = block.get_code(contract_id)
return  # todo: update </s> elif item == 'user_payment_amount':	get_user_info return  # TODO: update
# todo: check arp reply is valid </s> self.asserttrue(arp_replies)	test_arp_reply_from_host 'arp_source_ip': '10.0.0.1', 'arp_target_ip': '10.0.0.254'}) self.assertFalse(self.packet_outs_from_flows(arp_replies))
# todo stub </s> def test_filter_instances() -> none:	test_filter_instances pass
# todo: may test with codecs.open passing an encoding </s> temp = tempfile.namedtemporaryfile(delete=false, mode='wb')	test_export_to_xlsx_fobj def test_export_to_xlsx_fobj(self): file_name = temp.name + ".xlsx" fobj = open(file_name, 'wb')
# todo: argument constraints </s> ann_matches.append(r)	search_anns_for_relation if restrict_types != [] and r.type not in restrict_types: continue for r in ann_matches: matches.add_match(ann_obj, r)
# todo check </s> if self.mimetype == 'application/x-id3':	LiveDecoder @interfacedoc def format(self): self.mimetype = 'audio/mpeg' return self.mimetype
# todo doc </s> self.is_archived = true	archive def archive(self): self.save()
# todo: replace usages of strictredis (redis-py 2.x) with redis in dramatiq 2.0. </s> self.client = client = redis.strictredis(**parameters)	RedisBroker self.dead_message_ttl = dead_message_ttl self.queues = set() self.scripts = {name: client.register_script(script) for name, script in _scripts.items()} def consume(self, queue_name, prefetch=1, timeout=5000):
# todo: macos/win? </s> if platform_is_osx():	grab def grab(self, bbox=None): raise ImagemagickBackendError("osx not supported")  # TODO command = [PROGRAM, "-silent", "-window", "root"]
# todo: handle /dev/null (windows equivalent?) for new or deleted files </s> nbdiffapp.main([before, after])	show_diff def show_diff(before, after):
#todo: manage different in/out styles </s> if self.docs['in']['desc']:	_set_desc def _set_desc(self): self.docs['out']['desc'] = self.docs['in']['desc'] else:
# @todo: extend entity_types within the template </s> po_household = t("household"),	PersonEntityModel org_facility = T("Facility"), org_office = T("Office"), police_station = T("Police Station"), pr_person = T("Person"),
# todo: skip port_acl table if not configured. </s> self._configure_tables()	finalize_config (port_acl_matches, port_acl_set_fields, port_acl_exact_match, vlan_acl_matches, vlan_acl_set_fields, vlan_acl_exact_match) = resolve_acls() port_acl_table = self.tables['port_acl'] port_acl_table.match_types = port_acl_matches
# @todo: replace with plot from get /api/v2/show/{id} </s> exceptions_list = get_all_scene_exceptions(show)	sceneExceptions @staticmethod def sceneExceptions(show): if not exceptions_list: return 'No scene exceptions'
# todo: this could be done using single query, but how? </s> series = session.query(series).filter(series.name==parser.name).one()	mark_downloaded log.debug('marking series %s identifier %s as downloaded' % (parser.name, parser.identifier())) session = Session() episode = session.query(Episode).filter(Episode.series_id==series.id).\ filter(Episode.identifier==parser.identifier).one()
# todo: handle incorrect or invalid certificate connection... </s> client = soapclient(wsdl=wsdl, cert=none, cacert=none)	test_issue33 wsdl = "https://wsaahomo.afip.gob.ar/ws/services/LoginCms?wsdl"
# todo: description </s> return _check_global(global_params, result_dict, 'snooping', 'snooping', 'turn it on to prevent spoofing attack')	snooping_global def snooping_global(global_params, result_dict):
# todo: add test option fro datasets that support that </s> dataset_test = get_dataset(args, transform_test, "val")	test_main metadata = torch.load(args.val_file) root = args.valdir dataset_test.video_clips.compute_clips(args.num_frames, 1, frame_rate=15) test_sampler = UniformClipSampler(
# todo xxx graalvm change </s> raise unittest.skiptest("missing threading support")	ThreadedEchoServer self.daemon = True def __enter__(self): self.start(threading.Event()) self.flag.wait()
# todo: fix later </s> is_reset = true	decode_streaming if not is_reset: streaming._bd_offset = eout_block.size(1) - 1 if len(best_hyp_id_prefix) > 0: print('\r%s' % (idx2token(best_hyp_id_prefix)))
pass # todo(denero) implement </s> def test_missing_field(self):	test_missing_field
# todo: some regressors have extra options in their predict method, and they return a tuple of arrays. </s> if hasattr(step, 'predict'):	fit ys = [cache[o] for o in step.outputs if o in cache] step.fit(*Xs, *ys) output_data = step.predict(*Xs) elif hasattr(step, 'transform'):
# todo:  google only takes the first 180 characters, so maybe we find a logical </s> seo_summary = ''	document .only('creator') .select_related('creator')]) try: if doc_html and not doc.is_template:
# todo: remove this skip after fixing </s> if sys.version[0] == 3:	test_circle_draw @requires_application() def test_circle_draw(): raise SkipTest with TestingCanvas() as c:
# todo: consider whether this is going to cause a crash on amd cards. it's possible it should be commented out. </s> if obj.istextured and obj.texture and obj.solid:	drawMesh glEnable(GL_LIGHTING) glColorMaterial(GL_FRONT, GL_DIFFUSE) glDisable(GL_TEXTURE_2D) if obj.hasUVs:
# todo: move following logic under util.filenaming </s> if settings['windows_compatibility'] or is_win:	_format_filename new_filename = make_short_filename(new_dirname, new_filename, settings['windows_compatibility']) new_filename = new_filename.replace('./', '_/').replace('.\\', '_\\') new_filename = new_filename.replace('/.', '/_').replace('\\.', '\\_')
# todo: write me! </s> pass	NumericFilterValue def to_sql_values(self):
# todo: move this import to toplevel if possible [bruce 071029 comment] </s> if not nodes:	copied_nodes_for_DND @note: _sort is a private option for use by copy_nodes_in_order. from ops_select import Selection return None if DEBUG_ORDER:
# todo: this is a magic fudge factor... </s> size.width += 3	measure text_string = NSAttributedString.alloc().initWithString_attributes_(text, textAttributes) size = text_string.size() return size.width, size.height
# todo: tor might mark these dirs as setgid </s> assert oct(f.mode) == "0700"	test_tor_service_directores f = File(tor_service_dir) assert f.is_directory assert f.user == "debian-tor" assert f.group == "debian-tor"
# todo(sdague): enforce license in init file if it's not empty of content </s> if _project_is_apache() and not line_number > 1 and len(lines) > 10:	hacking_has_correct_license def hacking_has_correct_license(physical_line, filename, lines, line_number): H103 header does not match Apache 2.0 License notice for idx, line in enumerate(lines): if (0 < line.find('Licensed under the Apache License') < 10 and not
# todo(boto-2.49.0): remove when we pull in the next version of boto. </s> def _patchedshouldretrymethod(self, response, chunked_transfer=false):	_PatchedShouldRetryMethod provider = self.bucket.connection.provider if not chunked_transfer:
# todo sync protocol </s> time.sleep(0.01)	generate_events win2 = self.open_window() i3.command(f'[id={win1}] kill; [id={win2}] kill') i3.main_quit()
# todo: find out how to get global usernames </s> def follow(mastodon, rest):	follow @command
# todo: collect extra data </s> pass	unexpected_read def unexpected_read(data): if data:
# todo: get rid of prints, left over from refactoring </s> session = session()	forget_series_episode def forget_series_episode(self, name, identifier): series = session.query(Series).filter(Series.name == name.lower()).first() if series:
#todo: add metadata support when it is merged from develop </s> _close_conn(conn)	save_load ) )
# todo: write custom scope generator for devices (in case none, etc..). </s> if device is not none:	run_through_graph_fn_with_device_and_scope device = self.get_device(op_rec_column.component, variables=False) if get_backend() == "tf": with tf.device(device): with tf.name_scope(op_rec_column.component.global_scope +
# todo map the alias to its target </s> continue	parse_decimal_formats continue if elem.findall('./alias'): for pattern_el in elem.findall('./decimalFormat/pattern'): pattern_type = pattern_el.attrib.get('type')
# todo: figure out a way to actually log this information without </s> multiprocessing.active_children()	multiprocessing_SIGCHLD def multiprocessing_SIGCHLD(self, sig, stack):
# todo: should we do this? </s> scale_x = scale_y = scale_value	draw else: scale_value = min(scale_x, scale_y) print(translate_x) viewbox = node_format(node)[-1]
# todo: to be removed in v2.8.0 </s> return self.class_from_path_setting('section_menu_class_path')	SECTION_MENU_CLASS def SECTION_MENU_CLASS(self):
# todo: actually implement feature importance visualization for multiclass problems. </s> if isinstance(shap_values, list):	classify explainer = shap.TreeExplainer(self.clf) shap_values = explainer.shap_values(X) shap_values = np.sum(np.abs(shap_values), axis=0) top_importances = self.get_important_features(
# todo: can we make use of existing property maps for this? </s> if struct_props is none:	__init__ self.overrides.append(op) elif isinstance(base, values_struct.W_StructPropertyAccessor): struct_props = {} struct_props[base] = (op, handlers[i])
#todo - complete implementation of these apis </s> def detach_resource(self,req,tenant_id, network_id, id):	detach_resource try: self.network_manager.unplug_interface(tenant_id,
# todo: merge with config_check_pre_system_cron </s> stop_system_importer_file_csv_run = false	config_check_run def config_check_run(model): if not model.csv_import_username: mainconfigmodel = MainConfigModel.objects.get(main_config_name = 'MainConfig')
# todo: remove v1compatibility when v1 migration is complete </s> if config.scheduling.options.v1compatibility:	lookupFunction return (None, None, None) else: return (principal.record.fullName.decode("utf-8"), principal.record.guid,
# todo ... </s> testcpp(fn)	test testCpp(fn) else:
# todo setup attrs </s> self._data = directory(os.path.join(path, datapath))	_open self._meta = read_array_metadata(path)
errordialog(error[0], error[1]) # todo no-parent </s> gtk.main_quit()	__startgramps if errors: for error in errors: sys.exit(1) if argparser.errors:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# header. todo for aron: fix polib.py </s> subprocess.call(['sed', '-i', '-e 1,/^$/d', pofilename])	remove_exercise_nonmetadata os.remove(pofilename) clean_pofile.save(fpath=pofilename)
# todo: make grouper in query </s> grouper = grouper(dimension='time',	get_dataset geopolygon = geobox.extent  # intersection with query.geobox? observations = self.datacube.product_observations(geopolygon=geopolygon, **query.search_terms) group_by=lambda ds: ds.center_time, units='seconds since 1970-01-01 00:00:00')
# todo: clean up </s> for _ in range(num_hosts)	gossipsubs )
# todo: log errors to log file </s> pass	Kubeshell Kubeshell.clustername, Kubeshell.user, Kubeshell.namespace = KubeConfig.parse_kubeconfig() except  Exception as e: @registry.add_binding(Keys.F9) def _(event):
# todo: add support of tuple (row_offset, col_offset) </s> scale_size = rows + offset, cols + offset	ms_image_augment augmented_inputs = [] for offset in size_offsets: scaled_input = torch.nn.functional.interpolate(image, size=scale_size, mode=mode, align_corners=align_corners) augmented_inputs.append(scaled_input)
# todo: there should be an output for warnings and we should test we get one here </s> self.assertequal(	test_no_frequency def test_no_frequency(self): self.__parse('Name,Frequency\na,1\nb'), [{
pass  # todo </s> def create(self, name):	create
#@todo: remove in 0.4.10 </s> def error(self, reason=none, type="parse"):	error raise Fail("%s error%s | Plugin out of date" % (type.capitalize(), ':' + str(reason) if reason else "")) if self.core.debug:
# todo is pexpect thread safe, e.g. could we be blocked on this </s> if self.con.expect('%s   handle = .*? \r' % key,	_expect_async_notifications for key in ['Indication', 'Notification']: try: timeout=timeout) == 0: self._handle_notification(self.con.after)
# todo: if nucleus/symmetryconstraint bug ever fixed: </s> if not dag.node().hasfn(mfn.kdagnode):	toApiObject dag = MDagPath() sel.getDagPath( 1, dag ) obj = MObject() sel.getDependNode( 1, obj )
# todo: layer normalization </s> feed_forward_dropout_layer = keras.layers.dropout(	get_transformer name='%s-FeedForward' % name, )(multi_head_residual_layer) rate=dropout, name='%s-FeedForward-Dropout' % name,
# todo: for now we are not going to track bikes roaming around </s> if 'bike' in place_tree.attrib:	__new__ def __new__(cls, place_tree): if place_tree.attrib['bikes'] == "1": if place_tree.attrib['bike'] == "1":
# todo(stephenfin): use a helper </s> self.api.post_server_action(server['id'], {'migrate': none})	test_migrate_confirm fake_drop_move_claim, ) self._wait_for_state_change(server, 'VERIFY_RESIZE') self.assertUsage(src_host, 1)
# todo: is there a nicer way to do this? if i add a new grep plugin i won't </s> plugin.end()	test_options_for_grep_plugins plugin.get_long_desc()
# todo: not all messages have running status </s> if status_byte < 0x80:	_read_message def _read_message(self, status_byte): if self._running_status is None: return
# todo implement this </s> return notimplementederror	get_app_path if self.app_path is not None: return self.app_path from droidbot import DroidBot out_dir = DroidBot.get_instance().options.output_dir
# # fixme: # todo: remove me </s> try:	unpack_url faup.decode(url) url_unpack = faup.get() to_crawl['domain'] = url_unpack['domain'].decode() except:
# todo: more advanced logic in determining valid symbols </s> return [sym for sym in self.equation if sym.isalnum() and sym.isupper()]	extract_symbols All other characters are discarded in the processing of the equation. The first symbols in the list is the result of the equation.
# todo: remove in v1.0.0 </s> eval_results = model.validation_end(outputs)	_evaluate else: if self.is_overriden('validation_end', model=model): warnings.warn('Method `validation_end` was deprecated in 0.7.0 and will be removed 1.0.0.' ' Use `validation_epoch_end` instead.', DeprecationWarning)
# todo: make mac table updates less expensive. </s> for i, entry in enumerate(sorted(port_vlan_hosts)):	update_metrics port_vlan_hosts = port.hosts(vlans=[vlan]) assert port_vlan_hosts_learned == len(port_vlan_hosts) self.metrics.learned_macs.labels( **dict(port_vlan_labels, n=i)).set(entry.eth_src_int)
# todo: implement counters </s> return 0	getCounter def getCounter(self, name):
# todo: take this out later </s> if self.name == 'distribute' and not os.path.isdir(os.path.join(self.source_dir, 'setuptools')):	run_egg_info logger.indent += 2 try: rmtree(os.path.join(self.source_dir, 'distribute.egg-info'))
# todo: config of maps of packages </s> install_type = self.cfg['container']['install_type']	remove expect = expect or self.get_default_expect() if options is None: options = {} if install_type == 'apt': cmd = 'apt-get purge'
style="toolbutton", # todo: does this cause problems in some macs? </s> state=tk.normal	_add_toolbar_button command=handler, image=image, ) button.pack(side=tk.LEFT)
# todo test errors </s> assert_array_equal(expect, actual)	test_advanced_indexing_1d_bool actual = z[ix]
# todo(b/160294509): use tf.compat.v1 when we stop supporting tf 1.15. </s> if ops.executing_eagerly_outside_functions():	__init__ super(TransformFeaturesLayer, self).__init__(trainable=False) self._tft_output = tft_output self._check_tensorflow_version() self._saved_model_loader = saved_transform_io_v2.SavedModelLoader(
# todo: command+c for mac </s> tk.messagebox.showerror("internal error. program will close. use ctrl+c to copy",	run except: traceback.print_exc() traceback.format_exc())
self.assertfalse(greps(err, "unit zzz.service not for --user mode")) #todo </s> self.assertequal(out.strip(), "unknown")	bad_usermode_other_commands logg.info(" %s =>%s \n%s\n%s", cmd, end, err, out) self.assertEqual(end, 3) cmd = "docker exec {testname} {systemctl} is-failed zzz.service -vv" out, err, end = output3(cmd.format(**locals()))
#todo: may be this should go in a decorator for use in every command. </s> cmd_str = 'cd %s && git annex get %s %s' % (self.path, optlist, pathlist)	annex_get for key in kwargs.keys(): optlist += " --%s=%s" % (key, kwargs.get(key)) status = self.cmd_call_wrapper.run(cmd_str, log_stderr=False) if status not in [0, None]:
# todo remove this crap </s> for entity in entities:	Portal if event.get("format", None) == "org.matrix.custom.html": message, entities = formatter.matrix_to_telegram(event["formatted_body"]) if isinstance(entity, InputMessageEntityMentionName): entity.user_id = await client.get_input_entity(entity.user_id.user_id)
#@todo: remove in 0.4.10 </s> def error(self, reason=none, type="parse"):	error raise Fail("%s error%s | Plugin out of date" % (type.capitalize(), ':' + str(reason) if reason else "")) if self.core.debug:
# todo: optimize this by using send_data(array[from:to]) </s> for i in range((x_end - x_start) // 8):	display_partial for j in range(y_end - y_start): idx = (y_start + j) * width + x_start // 8 self.send_data(~self.frame_buffer[idx + i]) self.send_command(self.DISPLAY_REFRESH)   # display refresh
# todo: add detection for randomly generated macs (random 48-bit number with its eighth bit set to 1 as </s> pretty_mac = '{value:0{length}x}'.format(value=u.node, length=12)	run label="Time generated: {}".format(timestamp), parent_id=node.node_id, incoming_edge_config=uuid_edge) pretty_mac = ':'.join([pretty_mac[i]+pretty_mac[i+1] for i in range(0, 12, 2)]) unfurl.add_to_queue(
# todo: use is_accessible once two layer trie is implemented </s> if keccak(address) + code_trie_prefix not in self.read_list:	account_has_code_or_nonce def account_has_code_or_nonce(self, address): if self.is_access_restricted: raise UnannouncedStateAccess( "Attempted reading code of account outside of read list"
return 1  # todo +self.rec(expr.index)? </s> if not isinstance(array, lp.globalarg):	map_subscript array = self.knl.arg_dict[name] else: return 0  # TODO is this right? recurse on index? return 1  # TODO +self.rec(expr.index)?
# todo: support speedy mode for running the script </s> shell("make scriptconfig script=kconfiglib/examples/allyesconfig.py")	test_all_yes 'make scriptconfig' and needs to reparse the configurations, so kinda slow even in speedy mode.""" shell("mv .config ._config") if speedy_mode:
#todo: namespaces too hardwired, clean-up... </s> header = request('header' , ns=soap_namespaces.values(),)	call if k.startswith("wsse:")]) if 'wsse:Security' in self.__headers: k = 'wsse:Security' v = self.__headers[k]
#@todo: remove in 0.4.10 </s> def error(self, reason=none, type="parse"):	error raise Fail("%s error%s | Plugin out of date" % (type.capitalize(), ':' + str(reason) if reason else "")) if self.core.debug:
# todo: in 0.6.0 change this to "disabled": false </s> "enabled": true,	test_no_status_file "dev_type": "tap", "dh": "dh.pem", "key": "key.pem", "mode": "server",
# todo: still doesn't handle the case where the user wants </s> blank = r'[\w_]'	name_to_re def name_to_re(self, name): ignore = '(?:' + '|'.join(self.ignore_prefixes) + ')?' name = name.replace('&', '(?:and|&)')
# todo: check implementation </s> raise notimplementederror("this code seems not to handle several ys.")	get_active_neuron_io if len(tmp) == 1: return tmp[0]
raise notimplemented  # todo: other backends </s> ursula = ursula_config(password=password, known_nodes=teacher_nodes, lonely=lonely)	ursula click.secho(f"Added ethereum peer {enode}") else: del password  # ... under the rug
# todo: save message to history </s> if friend_num == self.get_active_number():  # add message to list	new_message :param message_type: message type - plain text or action message (/me) :param message: text of message user_name = Profile.get_instance().get_active_name() item = MessageItem(message.decode('utf-8'), curr_time(), user_name, message_type, self._messages)
# todo: maybe inefficient. use matrix operation </s> score_mat = np.empty((_batchsize, self.n_entity))	cal_scores rel_emb = self.pick_rel(rels) qs = self._composite(sub_emb, rel_emb) for i in range(_batchsize): score_mat[i] = - np.linalg.norm(qs[i] - self.pick_ent(np.arange(self.n_entity)), axis=1) ** 2
# todo: remove str() when dropping support for py37. </s> args = [self.executable_name,	runshell def runshell(self): str(self.connection.settings_dict['NAME'])] subprocess.run(args, check=True)
# todo: figure out why yask doesn't like it with dse/dle </s> operator(eqs, name='initdamp', dse='noop', dle='noop')()	initialize_damp val = -val if mask else val eqs += [Inc(damp.subs({d: dim_r}), val/d.spacing)]
# todo: check the data! </s> self.asserttrue(len(list(pipe)) > 0)	test_tail pipe_def = self._get_pipe_def(pipe_file) pipe = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def)
# todo: remove compatability hooks </s> lockfile = os.path.join(target,esky_control_dir,"lockfile.txt")	uninstall_version target = os.path.join(self.appdir,target_name) assert os.path.dirname(target) == self.appdir if not os.path.exists(lockfile): lockfile = os.path.join(target,"esky-lockfile.txt")
except exception:  # todo what exception? </s> raise validationerror(f'unable to open python code file {python_file}')	add_boilerplate_and_check_syntax try: script = open(os.path.join(filename), 'r').read() + '\n' script = module_boilerplate + script sfile = open(os.path.join(filename), 'w')
# todo: remove warning check once deprecated </s> hits = tree.intersection((1012821.80, 229228.26), objects=true)	test_merge_geo tree = merged.sindex with pytest.warns(FutureWarning, match="`objects` is deprecated"): res = [merged.loc[hit.object]["BoroName"] for hit in hits] assert res == ["Bronx", "Queens"]
# todo: sinpi </s> s = pi/math.tan(pi*x)	_digamma_real if x < 0.5: x = 1.0-x else: s = 0.0
# todo: simplest possible unicast learning. </s> valve instance or none (of edge datapath where packet received)	_edge_dp_for_host Returns:
# todo: handle marker? </s> for func in funcs['functions']:	_find_function conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) funcs = conn.list_functions() if func['FunctionName'] == name: return func
# todo: here the index is correlated to the duals, try if this can be fixed when temp duals are removed. </s> if constr.body.polynomial_degree() in (0, 1):	add_oa_cuts with time_code(solve_data.timing, 'OA cut generation'): for index, constr in enumerate(target_model.MindtPy_utils.constraint_list): continue constr_vars = list(identify_variables(constr.body))
pass # todo </s> else:	write_serialize for field in spec.parsed_fields(): if field.is_array: if field.is_builtin: write_serialize_builtin(s, field)
# todo extend to nonbinary nodes </s> if i not in self._input_indices and i in self.context.node_indices:	__init__ else: self._dimension_labels.append(None) past_tpm_on = past_tpm_on.sum(i, keepdims=True) / 2 past_tpm_off = past_tpm_off.sum(i, keepdims=True) / 2
# todo (b/151636380): remove when cl/299961405 is propagated through kokoro. </s> if metric_name == 'specificity_at_sensitivity':	testMetricsWithWeights ) def testMetricsWithWeights(self, metric_name, expected_value): fix_present = hasattr(tf.keras.metrics.SpecificityAtSensitivity, '_find_max_under_constraint')
# todo: investigate why results are sometimes 'nan' </s> return	draw_pupil_outline ) except ValueError: confidence = pupil_detection_result_2d["confidence"] * 0.7 draw_polyline(pts, 1, RGBA(1.0, 0, 0, confidence))
# todo(yamahata): eliminate dumb polling </s> attempts = 0	_await_block_device_map_created def _await_block_device_map_created(self, context, vol_id, max_tries=30, wait_between=1): start = time.time() while attempts < max_tries:
# migration todo: remove ? </s> channel = self.connection.channel()	get_store_worker def get_store_worker(self, event_store): store_worker = StoreWorker(Connection(self.backend_url), event_store) bound_store_worker_input_queue = store_worker.input_queue.bind(channel) bound_store_worker_input_queue.unbind_from(events_exchange)
#todo use backup phone </s> generated_token = totp(token.seed)	verify_computer token = user.token if token.method in ('call', 'sms'): if token.method == 'call': call(to=token.phone,
pass  # todo </s> def forcesendjob(self, print_job_uuid: str) -> none:	forceSendJob @pyqtSlot(str, name="forceSendJob")
# todo: this is untested. </s> _raise_current_error()	__init__ self._from_ssl = _lib.BIO_new(_lib.BIO_s_mem()) if self._into_ssl == _ffi.NULL or self._from_ssl == _ffi.NULL: _lib.SSL_set_bio(self._ssl, self._into_ssl, self._from_ssl) else:
# todo: use flask logger without it triggering the root </s> __name__: {	init_logging }, "loggers": { "handlers": [], "level": os.getenv("ORCHEST_LOG_LEVEL", "INFO"),
#ack = self.serialport.read() # todo: use ack </s> self.sendcommand(141) # 10001101	enable def enable(self):
# todo: need to figure out how we prevent multiple selfdestructs from </s> vm.logger.debug('deleting account: %s', encode_hex(account))	_execute_sharding_transaction with vm.state_db() as state_db: for account, beneficiary in computation.get_accounts_for_deletion(): state_db.set_balance(account, 0) state_db.delete_account(account)
except exception:  # todo: refactor this... </s> e = sys.exc_info()[0]	test_GET_InvalidData try: self.assertEqual(self.rnw.GET(url, data), 'json') self.assertEquals(e, TypeError)
# todo make this a private api </s> places = []	parse_code @staticmethod def parse_code(results): for result in results.get('RESULTS'): coordinates = result.get('COORDINATES', {})
# todo: dateparser is a little greedy, consuming the "on " as well as the date </s> self.compare_before_after()	test_DoB_4 BEFORE: My name is Mike and I was born in a land far away on 22/11/1972 AFTER:  My name is Mike and I was born in a land far away {{DATE_OF_BIRTH}}
+ ansi.ansi_normal  # todo: why does it keep it? </s> + "foo"))	test_re_inversing parser.re_inversing( "a " + ansi.ANSI_INVERSE + "red"
# todo: is there a more elegant fix? </s> return b""	_ReadOutputAtEnd return self._file_in.read() except IOError:
# todo: improve handling of s.a < s.b and s.a > s.b cases. </s> a = find_bracket_location(s.b - 1)	move_to_bracket return a if mode == MODE_VISUAL: if a is not None: a = a + 1 if a > s.b else a
# todo: test & review this part </s> sudo_command = "sudo -i -u {}".format(self._vbox_user.strip()) + " ".join(command)	_execute try: if self._vbox_user and self._vbox_user.strip(): process = yield from asyncio.create_subprocess_shell(sudo_command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE) else:
# todo: type: literal["m", "h", "x", "b"] </s> raise notimplementederror	getDefiFormat def getDefiFormat(self) -> str:
# todo make this a real request </s> messages.success(request,	update if form.is_valid(): tenant = form.clean() '%s was successfully updated.' % user['username'])
# todo: fails because of missing svg support </s> assert_pixels('inline_image_' + filename, 8, 8, image, '''	test_images )) def test_images(filename, image): <style> @page { size: 8px }
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo log here </s> return none	get_factor_id return None if response.status_code != 200: not_active = False for factor in response.json():
# todo: arrange </s> distro = self.remote.get_item_handle("distro", "testdistro0", self.token)	test_copy_distro def test_copy_distro(self): Test: copy a distro object self.assertTrue(self.remote.copy_distro(distro, "testdistrocopy", self.token)) assert 0
# todo convert to fit transform </s> def under_sampling(self, random_state=0):	under_sampling
# todo (elliot): put this in the preferences. </s> if prefs.get("stripdevelopersuffixes", false) is true:	generate_munki_recipe "manually add one to the munki recipe.") keys["Input"]["pkginfo"]["description"] = " " keys["Input"]["pkginfo"]["developer"] = strip_dev_suffix(facts.get("developer", '')) else:
# todo: 搜索和分页 </s> keyword = request.get.get('search', '')	perm_rule_edit header_title, path1, path2 = "授权规则", "规则管理", "查看规则" rules_list = PermRule.objects.all() if keyword: rules_list = rules_list.filter(Q(name=keyword))
#todo: append the rule instead of overwrite the full content </s> self.root.file( '.hgignore' ).write( file_to_ignore_regexp )	ignore separated by a\n char @returns: nothing
# todo check </s> return self.mimetype	format @interfacedoc def format(self):
# todo: figure out a better way to do things </s> gens_prod = cartesian_product([family(m.monoid_generators(),	monoid_generators ret = [lift(i, gen) for i,M in enumerate(F) for gen in M.monoid_generators()] return Family(ret) lambda g: (i, g)) for i,M in enumerate(F)])
# todo: requires explicit broadcast in future </s> if not isinstance(val, op):	AssignOp if not force and tensor.is_constant: raise ValueError("{} is not assignable.".format(tensor)) val = as_op(val) if len(val.axes) == len(tensor.axes):
# todo: check whether loaded network has the same number of classes as specified in ilastik! </s> self._tiktorchclient.train(reordered_feature_images, reordered_labels, image_ids)	update self._opReorderAxes.AxisOrder.setValue('czyx') reordered_labels.append(self._opReorderAxes.Output([]).wait())
# todo add list as an option </s> if ia.is_single_number(p):	Dropout Identical to the previous example, but the `per_channel` feature is only active for ``50`` percent of all images. p2 = iap.Binomial(1 - p) elif ia.is_iterable(p):
# todo: make the get_closest_value to return region </s> value, value_index = self.get_closest_value(	get_current_CSS_value parsed_declaration = re.search(r'^(\s*)(-[a-zA-Z]+-)?([a-zA-Z0-9-]+)(\s*(?: |\:))((?:(?!\!important).)+)', declaration) declaration_index = declaration_index + parsed_declaration.start(5) parsed_declaration.group(5), declaration_index,
pass # todo: implement </s> print url	cursor Returns None if the url was not valid or the document could not be loaded.
# todo jython </s> self._checkclosed()	selected_alpn_protocol def selected_alpn_protocol(self):
# todo: remove in v2.8 </s> if hasattr(self.choiceset, 'legacy_map') and obj in self.choiceset.legacy_map:	ChoiceField ('label', self._choices[obj]) ]) data['id'] = self.choiceset.LEGACY_MAP.get(obj) return data
# todo(lyarwood): test drivervolumeblockdevice.driver_detach in </s> volume_id = 'vol_id'	_test_detach_volume_evacuate terminate call (optional). Default is to expect the local connector to be used. instance = fake_instance.fake_instance_obj(self.context, host='evacuated-host')
# todo consider more type conversions? </s> if value.isdigit():	load_tmx name = p.attrib['name'] value = p.attrib['value'] value = int(value) tile.properties[name] = value
# todo: python-components: for now, we call each preprocessor's graph_fn directly. </s> if self_.backend == "python" or get_backend() == "python":	method def method(self_, *inputs): result = inputs for sub_component in self_.sub_components.values(): result = getattr(sub_component, "_graph_fn_"+components_api_method_name)(*force_tuple(result))
# todo: test this method </s> return self.path or "(%s)" % self.title	MetaData return self.path def __unicode__(self): def save(self, update_related=True, *args, **kwargs): self.keywords = ", ".join(self.keywords.strip().splitlines())
#reactor.stop() # for unknown reasons, reactor.stop() isn't working.  [ ] todo </s> self.log('calling os.abort()')	_die self.log('_startService() failed') log.err(failure) os.abort()
# see optimization description comments and todo for tags in matching public histories query. </s> return trans.sa_session.query(self.model_class).join("user").options(eagerload("user").load_only("username"), eagerload("annotations"), undefer("average_rating"))	build_initial_query def build_initial_query(self, trans, **kwargs):
# todo: remove </s> print("\ndisabling gnome power profiles")	gnome_power_disable_live def gnome_power_disable_live(): if(gnome_power_stats != 0): call(["systemctl", "stop", "power-profiles-daemon"]) else:
# todo: "dob.role is not none" </s> if dob.role is not none and dob.role != 'isw_raid_member' \	_update_disk_state dob.role = '{"mdraid": "' + d.fstype + '"}'  # json string else:  # We know this disk is not an mdraid raid member. and dob.role != 'linux_raid_member': known_roles = json.loads(dob.role)
#todo - once we drop support for python 2.4, instead of this </s> prefix_strings = [self._get_seq_str_and_check_alphabet(p) \	startswith True if isinstance(prefix, tuple) : for p in prefix] for prefix_str in prefix_strings :
# todo(rameshg87): need better way of asserting the routes. </s> self.assertisinstance(driver.vendor, utils.mixinvendorinterface)	test_pxe_ipminative_driver self.assertIsInstance(driver.management, ipminative.NativeIPMIManagement) self.assertIsNone(driver.inspect) self.assertIsInstance(driver.raid, agent.AgentRAID)
# todo: should this do the same cleanup as the on_message code? </s> client.request.remote_ip, error_msg)	_send_broadcast logger.debug('Broadcast of WebSocket message to %s failed: %s',
try:  # todo: fix brodcast issue if different </s> if np.ndim(x) < 2:	feed_dict_builder X = [X for _i in net_inputs] elif len(net_inputs) > 1: raise ValueError("Multiple inputs but only one data " "feeded. Please verify number of "
# todo: automate this (by lookup from nn). </s> internal_states_space=impalaagent.standard_internal_states_space,	test_impala_actor_plus_learner_agent_functionality_learner_part state_space=env.state_space, action_space=env.action_space, execution_spec=dict( mode="distributed",
# todo: log in with right permissions and request the page again </s> self.assertredirects(response, 'http://testserver/accounts/login/?next=/test_client/permission_protected_view/')	test_view_with_permissions response = self.client.get('/test_client/permission_protected_view/')
raise notimplementederror #todo </s> elif q.kw(i,"format"):	parse while i < l: if q.kw(i,"RETURN"): raise NotImplementedError #TODO elif q.kw(i,"REQUEST"):
# todo get tri-letter dimensionality from fit-transform as input shape </s> dim_triletter = self._params['input_shapes']	build Build model structure. DSSM use pair-wise arthitecture. x_in = [self._build_shared_model(dim_triletter), self._build_shared_model(dim_triletter)]
# todo: remove when unicode vlens implemented </s> if unicode_hack:	create space = h5s.create_simple(shape) htype = h5t.py_create(dtype, logical=True) htype.set_cset(h5t.CSET_UTF8) if name in self:
raise notimplementederror  # todo... </s> else:	fetch_callback plt.close() elif args.output_format == "hdf": raise NotImplementedError("output format %r" % args.output_format)
# todo: refactor accordingly when v3 websocket api is released </s> output["results"].update({	BittrexAPIOrderBookDataSource if _is_snapshot(msg): output["results"] = _decode_message(msg["R"]) "M": f"{output['results']['M'].split('-')[1]}-{output['results']['M'].split('-')[0]}" })
""" todo: documentation </s> return false	streaming @property def streaming(self):
# todo: support for 'file' type </s> raise swaggererror('unknown type {0} for value {1}'.format(obj_type, value))	marshal_schema_object if obj_type == 'object': return marshal_object(swagger_spec, schema_object_spec, value)
# todo get the default name from preferences </s> return os.path.join(self.get_scratchpad_prefix(), "autosave.ora")	get_scratchpad_autosave def get_scratchpad_autosave(self):
# todo results from p0f </s> return	_get_ipv4_os @staticmethod def _get_ipv4_os(endpoint):
# todo print out the certificates </s> server.lag = (time.time() - start) * 1000	wrap_socket cipher=cipher[0]) W.prnt(server.server_buffer, cipher_message) W.bar_item_update("lag") return ssl_socket
# todo: cleanly remove clipboard code if it is no longer needed </s> return httpresponsebadrequest('not implemented anymore')	delete_clipboard def delete_clipboard(request): if True: if request.method == 'POST': clipboard = Clipboard.objects.get(id=request.POST.get('clipboard_id'))
# todo: start here </s> for path in self._fs.ls(dir_path):	_archive_dir tar_gz_name = ( self._working_dir_mgr.name('dir', dir_path, name) + '.tar.gz') pass
# todo trace model </s> predictions = self.model(inputs, training=false)[0]	_forward with self.device_placement(): if self.framework == "tf": else: with torch.no_grad():
# todo: deprecated function. see ticket #453. </s> if not self._capabilities:	_getcapproperty def _getcapproperty(self): reader = WMTSCapabilitiesReader( self.version, url=self.url, un=self.username, pw=self.password
#todo this will change when we attempt #35, since this assumes intersection </s> query_script = """	query_indices def query_indices(name_and_query, using): Takes a list of index name/query pairs and returns the resulting nodes. neo4j = g.getRawGraph() indexManager = neo4j.index()
# todo - make this work on loop with more than two links </s> flt_parallel = lambda loop: round(loop.calc_angle(),3) == 3.142	make_railing loops = list(set(loops)) if remove_colinear: flt_mid = lambda loop: loop.link_loop_next in loops and loop.link_loop_prev in loops loops = [l for l in loops if not (flt_parallel(l) and flt_mid(l))]
# todo test </s> self.unconstrained_effect_repertoire(purview))	effect_info return utils.emd(self.effect_repertoire(mechanism, purview),
# reasons why we said no. todo: allow configurable error messages </s> raise synapseerror(	create_association ) if not self.config.is_alias_creation_allowed(user_id, room_alias.to_string()): 403, "Not allowed to create alias", )
#todo: remove convert_bare true and deprecate this in with_ </s> if self._task.loop == 'first_found':	_get_loop_items if self._task.loop: if self._task.loop in self._shared_loader_obj.lookup_loader: loop_terms = listify_lookup_plugin_terms(terms=self._task.loop_args, templar=templar, loader=self._loader, fail_on_undefined=False, convert_bare=True)
# todo check if we can avoid that </s> out = numpy.zeros(ntet)	read_buffer out = numpy.fromfile( f, count=ntet * 4, dtype=int, sep=" ").reshape(ntet,4) cells["tetra"] = out - 1 cell_data["tetra"] = {"ugrid:ref": out} if npyra > 0 :
# todo: this should be abstracted into a property/method or something </s> if region.inherited and not contents and hasattr(obj, 'parent_id') and obj.parent_id:	collect_items def collect_items(obj): contents = obj._content_for_region(region) return collect_items(obj.parent) return contents
# todo: t102735: use the page content model for <1.21 </s> for rev in pagedict['revisions']:	update_page page._protection[item['type']] = item['level'], item['expiry'] if 'revisions' in pagedict: assert 'parentid' in rev, 'parentid missing in revision %r' % rev revision = pywikibot.page.Revision(
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
# todo: (find a way to) test if the line underneath indeed correctly replaces the reimplemented values method </s> preprocess_reply=lambda r: r.replace('\x00', ''),	__init__ "Signal Recovery DSP 7265", includeSCPI=False, **kwargs
# todo: test for the _correct_ revision_id value. </s> if not activity.revision_id:	_update_resource if not activity.id: assert False, "activity object has no id value" assert False, "activity has no revision_id value" assert activity.timestamp >= before and activity.timestamp <= after, \
# todo: refactor this to be more uniform across sources </s> self.update_control_menu()	update_menu def update_menu(self):
# todo: beautify output </s> click.echo(contents)	agenda with open(todays_notes_entry_file_path(), 'r') as config_file: contents = yaml.load(config_file) else: click.echo('The configuration file for this module does not exist. Please type "dude money setup" to create a new one')
# todo: implement me </s> pass	disable_subtitles def disable_subtitles(self):
# todo: make this section of code easier to understand. </s> new_data = {}	test expected = testData['expected'] data     = testData['data'] for key, val in data.iteritems(): if isinstance(val, dict) and val.get('__tag__') == 'code':
# todo add a conditional to toggle this </s> remove_tool_output_from_mets(tree)	connect_and_index_aip conn = connect_and_create_index('aips') tree = ElementTree.parse(pathToMETS) root = tree.getroot() nsmap = { #TODO use XML namespaces from archivematicaXMLNameSpaces.py
# todo yoon </s> obtain the score of a sentence by an rnnlm	get_ppl_rnn def get_ppl_rnn(lm, sentences):
# todo: ^intel-parallel-studio can mean intel mpi, a compiler or a lib </s> else:	scalapack_libs elif self.spec.satisfies('^mpt'): libnames.append('libmkl_blacs_sgimpt') raise InstallError("No MPI found for scalapack") shared = True if '+shared' in self.spec else False
raise exceptions.mpdnotimplemented  # todo </s> underscore, dash, dot and colon.	subscribe already. The name may consist of alphanumeric ASCII characters plus
# todo: could also blame / </s> e_die('divide by zero',	ArithEvaluator elif op_id == Id.Arith_Slash: if rhs == 0: span_id=location.SpanForArithExpr(node.right)) ret = lhs / rhs
# todo ... </s> data = sorted()	userLongDescription for key in mainKeys: data[key] = data.get(key, "").strip()
#    #todo </s> else:	_handle_VENDOR if e is None or e.halt != True: con.raiseEventNoErrors(PacketIn, con, msg) _old_handler(con, msg)
# todo: (longer term) rather than abort, reject this candidate </s> assert (self._name is none or	dist self._dist = abstract_dist.get_pkg_resources_distribution() assert self._dist is not None self._name == canonicalize_name(self._dist.project_name)) assert (self._version is None or
# todo check if this always works? </s> col = bpy.data.collections.get('collection')	add_cube_based_on_bb mesh = bpy.data.meshes.new('mesh') obj = bpy.data.objects.new(name, mesh) col.objects.link(obj) new_vertices = []
"""todo: not implemented""" </s> notimplementederror("ntile not implemented")	ntile @symbolic_dispatch def ntile(x, n):
# todo: delete </s> ys = [(alpha/2.)**2 for alpha in alphas]	_d_TPD_Michelson_modified Phase Equilibrium Solver and Its Validations." Fuel 115 (January 1, 2014): 1-16 ys = normalize(Ys) z_log_fugacity_coefficients = self.fugacity_coefficients(Zz, zs)
#todo: use a getter </s> return self.dst.style['in']	get_input_style @return: the style for input docstring @rtype style: str
# todo(dcramer): we want to be less aggressive on disabling domains </s> cache.set(domain_key, error or '', 300)	fetch_file 'url': expose_url(url), } logger.warning('Disabling sources to %s for %ss', domain, 300, exc_info=True)
# todo: this is untested. </s> _raise_current_error()	Context context = _lib.SSL_CTX_new(method_obj) if context == _ffi.NULL: context = _ffi.gc(context, _lib.SSL_CTX_free) self._context = context
## todo: # fixme: remove me </s> try:	analyse url_parsed = faup.get() pprint.pprint(url_parsed) resource_path = url_parsed['resource_path'].encode() except:
# todo: we possibly need to sync so all replicas are upto date </s> get_apex_utils().sync_devices()	_test_epoch loss = self._lossfn(logits, y) self._post_step(x, y, logits, loss, steps, self._metrics) logger.popd() self._metrics.post_epoch(None)
# todo: summary hash for new current id </s> output_data_container.append(current_id, output.data_inputs, output.expected_outputs)	handle_fit context ) return self, output_data_container
# todo: how would this change with domain? </s> if value is none:	iterintervals time series, optionally only the groups where the starting value of the time series matches `value`. def value_function(x): return True
# todo: copy other properties (gcps etc). several other </s> dst.write_mask(src.read_masks()[0])	convert if MaskFlags.per_dataset in src.mask_flag_enums[0]:
# todo: change these to reflect values set in the database </s> self.ph_sensor_uart = atlasscientificuart(	__init__ self.interface = interface if self.interface == 'UART': serial_device='/dev/ttyAMA0', baudrate=38400) elif self.interface == 'I2C':
# todo: test. </s> return result	GetListMembers cursor = next_cursor
# todo: add corrections back in here, rather than at points of use </s> dpsi, deps = iau2000a(self.tt)	nutation_angles_arcseconds @reify def nutation_angles_arcseconds(self): return dpsi / 1e7, deps / 1e7
# todo: should this function go to the cache class and </s> with open(path, "w+b") as file:	replace_dummy def replace_dummy(self, path): file.write(self.api.download(path)) os.remove(self.converter.add_dummy_ending(path))
# todo: catch and report error if possible </s> self._client._loop.create_task(	_onCertificateError if not self._ignoreHTTPSErrors: return self._client.send('Security.handleCertificateError', { 'eventId': event.get('eventId'),
# todo[jigish] is this required? </s> return response(data, 200, generate_headers('{0}/{1}'.format(namespace, repository), 'read'), true)	get_user_images return api_error('images not found', 404)
# todo: refactor more better(tm) </s> p = self._linear_part(state_below)	Linear return z def fprop(self, state_below): return p def cost(self, Y, Y_hat):
sleep(5) # todo: replace this with zmq signalling </s> shutil.copyfile(available, enabled)	deploy_python echo("-----> Enabling '%s' at port %d" % (app, port), fg='green') os.unlink(enabled)
# todo: handle fancy-index copies by allocating a buffer and </s> next_index = self._subset_iterator.next()	next def next(self): return self._raw_data[next_index]
# todo(dtroyer): remove tenant_id when we clean up the sdk refactor </s> attrs['tenant_id'] = project_id	_get_attrs parsed_args.project_domain, ).id attrs['project_id'] = project_id if 'availability_zone_hints' in parsed_args and \
# todo: other "expected" error types to catch? </s> except pywikibot.error as edit_err:	handle try: func(self, *args, **kwargs) err = edit_err  # edit_err will be deleted in the end of the scope link = self.title(asLink=True)
# todo: handle data types </s> if direction == 1:	handle_optimized_r_n_n raise RuntimeError("unexpected cell type") warnings.warn("Initial weight, hidden/cell states will be ignored for now.") output, state = tf.nn.dynamic_rnn(cell, input_dict[node.inputs[1]],
# todo: figure out how to grab the creds from environment variables in travis and then release </s> c.run('python3 -m twine upload --repository-url https://test.pypi.org/legacy/ dist/*')	upload_to_pypi_test_server def upload_to_pypi_test_server(c): c.run('python3 -m pip install --upgrade twine') c.run('python3 -m pip install --index-url https://test.pypi.org/simple/ --no-deps policy_sentry')
pass # todo </s> specifies to list albums by an artist.	_music_db_list ``ARTIST`` is an optional parameter when type is ``album``, this
# todo extend to nonbinary nodes </s> augmented_child_tpms = [	get_marbl if direction == DIRECTIONS[FUTURE]: tpm_name = 'current_tpm' [child._dimension_labels[self.index], getattr(child, tpm_name)[1].squeeze()] for child in self.outputs
# todo: test that the connection gets closed if ping responses stop. </s> yield self.close(ws)	test_client_ping self.assertEqual(response, "got ping")
# todo: remove input `form` in sage 9.3 </s> from sage.misc.superseded import deprecation	set_connection_form To keep them, use the method :meth:`add_connection_form` instead. if form: msg = "the input 'form' is outdated and will be removed in a " msg += "future version of Sage"
#todo now the content must be in cache! (got to handle transfer error) </s> path = os.path.expanduser(os.path.join('~', '.config',	_get_msn_object_path self.cbs = (self._download_custom_emoticon, self._download_custom_emoticon) self.msn_object_store.request(msn_object, self.cbs) 'emesene2', self.session.account.account, msn_object._checksum_sha + ".gif"))
# todo: remove this </s> setglobalfunc(mem, 'array', objects.parameterizedarray())	Init SetGlobalFunc(mem, '_match', _Match(mem)) SetGlobalFunc(mem, 'Table', objects.Table) SetGlobalFunc(mem, 'Bool', bool) SetGlobalFunc(mem, 'Int', int)
# todo: implement </s> return 1	hook_GetStringTypeW }) def hook_GetStringTypeW(ql: Qiling, address: int, params):
except exception:  # todo - which exceptions? </s> import traceback	goto self.mark(start, stop) return traceback.print_exc() self.goto_entry.delete(0, END)
return 'ok' # todo should be a json or something </s> viewer.status = 'paused'	stop def stop():
# todo: remove "get_" from the name </s> for a sampling scheme with a list of head node types and the	get_sampling_layout def get_sampling_layout(self, head_node_types, num_samples): number of samples per hop, return the map from the actual sample index to the adjacency list index.
# todo 目前仅在 华泰子类 中实现 </s> log.info('目前仅在 佣金宝子类 中实现, 其余券商需要补充')	get_current_deal def get_current_deal(self):
# todo: how to get the api version without split & strip </s> api_version = shopifyresource._site.split('/')[-1].strip('-')	save def save(self): start_api_version = '201910' if api_version >= start_api_version:
# @todo: pheonix </s> event.skip()	changeLevel parentID = self.skillTreeListCtrl.GetItemParent(selection) parent = self.skillTreeListCtrl.GetItemData(parentID)
# todo properly use locales </s> value = value or 0	brazilian_float @register.filter() def brazilian_float(value): value = f'{value:,.2f}' return value.replace(',', 'x').replace('.', ',').replace('x', '.')
# todo(hartikainen): this should get the logdir some other way than </s> summary_dir = logger._snapshot_dir	_init_summary_ops def _init_summary_ops(self): if self._tf_summaries: self.summary_writer = tf.summary.FileWriter( summary_dir, self._sess.graph)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
except exception:  # todo: refactor this... </s> e = sys.exc_info()[0]	test_GET_InvalidData try: self.assertEqual(self.rnw.GET(url, data), 'json') self.assertEquals(e, TypeError)
x.shape[0:], # todo test reshape, dimshuffle </s> x.shape[0:1])	test_equality_shapes assert f([3, 3]) == 0 g = join(0, f = theano.function([x], T.eq(g, 0), mode=mode) assert (f([3, 3]) == 0).all()
# todo: figure out why this causes circular import </s> query = ledgeres().domain(domain).section(section_id).case(case_ids)	get_aggregated_ledger_values def get_aggregated_ledger_values(domain, case_ids, section_id, entry_ids=None): if entry_ids: query = query.entry(entry_ids)
# todo remove compatibility shims for anki 2.1.46 and lower. </s> by_name = decks.by_name if hasattr(decks, 'by_name') else decks.byname	from_collection def from_collection(collection, name, deck_metadata=None, is_child=False) -> Deck: decks = collection.decks anki_dict = by_name(name) if AnkiDeck(anki_dict).is_dynamic:
# todo(okuta): check type </s> return core.moveaxis(a, source, destination)	moveaxis Array with moved axes. This array is a view of the input array. .. seealso:: :func:`numpy.moveaxis`
# todo remove arch dependent code </s> target_oprnd = asm.operands[0]	_extract_branch_target def _extract_branch_target(self, asm): address = None if isinstance(target_oprnd, X86ImmediateOperand) or \ isinstance(target_oprnd, ArmImmediateOperand):
# todo(jd) add an exception in oslo.db to match foreign key </s> if isinstance(e.inner_exception, sqlalchemy.exc.integrityerror):	delete_archive_policy raise indexer.NoSuchArchivePolicy(name) except exception.DBError as e: raise indexer.ArchivePolicyInUse(name)
# todo: sysex messages do not arrive here. </s> def _pending(self):	_pending if self._has_callback: raise IOError('a callback is currently set for this port')
# todo(b/155804245) sanitize the names so that they're valid python names </s> output_channel_parameters[output_name] = (	create_container_component )) for output_name, channel_type in outputs.items(): component_spec.ChannelParameter(type=channel_type)) artifact = channel_type()
# todo: reevaluate how to deal with different types of errors; soft </s> del result['error']	smart_search_pool search_options ) except NipapError, e: return json.dumps({'error': 1, 'message': e.args, 'type': type(e).__name__})
#todo: validate keys </s> data = self.rh_obj.investment_profile()	test_investment_profile def test_investment_profile(self):
# todo: implement me </s> assert pytest.approx(loss.item(), 0.0)	_test_all_zeros loss = criterion(logits, labels)
# todo: inplace of df with parent (reflection) </s> def _impl(df, labels=none, axis=0, index=none, columns=none,	_impl level=None, inplace=False, errors='raise'): return hpat.hiframes.pd_dataframe_ext.drop_dummy(
#todo remove str() -- http://github.com/fifengine/fifengine/issues/701 </s> self.__emitter.setsoundclip(horizons.globals.fife.sound.soundclipmanager.load(str(soundfile)))	play_ambient else: self.__emitter.setRolloff(0) # reset to default if loop_interval == 0: self.__emitter.setLooping(True)
# todo: remove </s> if use_cache:	__init__ if cache_db is not None: logger.error('cache_db argument is depricated. Use setup_cache method.') self.setup_cache(backend='mongo', database=cache_db, use_compression=use_cache_compression)
# todo: fix this workaround </s> @asyncio.coroutine	_cursor_wrapper def _cursor_wrapper(conn): cur = yield from conn.cursor()
# todo: add cntk </s> raise notimplementederror()	gather return tensorflow.gather(x, indices, axis=axis) else:
# todo: should we also remove the keys of the delegated roles? </s> tuf.roledb.remove_delegated_roles(metadata_role)	_load_metadata_from_file self._rebuild_key_and_role_db() elif metadata_object['_type'] == 'Targets': self._import_delegations(metadata_role)
# todo: parallel processing! </s> return [_process_scene(scene, type_, augment) for scene in scenes]	_process_scenes def _process_scenes(scenes, type_, augment):
# todo: store the random state and return it to its previous value </s> random.seed(random_seed)	downsample def downsample(self, fraction_to_retain, random_seed, verbose=False): random_sequence=range(len(self._scenarios)) random.shuffle(random_sequence)
# todo: windows debug_mode? </s> return os.path.join(current_directory, "_wrapcl" + so_ext)	_get_wrapcl_so_name from distutils.sysconfig import get_config_var so_ext = get_config_var('SO')     # fall-back
# todo: https://github.com/giampaolo/psutil/issues/1035 </s> self.assertin(server_conn.raddr, ("", none))	test_unix self.check_socket(client, conn=client_conn) self.assertEqual(server_conn.laddr, name) self.assertIn(client_conn.laddr, ("", None))
# todo: @sbharadwajj implement and test </s> return true	hessian_is_zero def hessian_is_zero(self):
# todo - send file in chunks if file size > some threshold. </s> headers = {'content-type': 'application/json',	scan for fname in filelist: with open(fname, "rb") as sample: 'user_agent': conf['user agent'], 'filename': basename(fname)}
# todo : now the logger.info will error if iter > 350000, so use print haha </s> if max_iteration > 350000:	main msg = ', '.join(['iter: {it}/{max_iteration}', 'lr: {lr:4f}', 'loss: {loss:.4f}', 'eta: {eta}', 'time: {time:.4f}', ]).format(it=it, max_iteration=max_iteration, lr=lr, loss=loss_avg, time=t_intv, eta=eta) logger.info(msg) else:
</c:comp-filter>"""], "todo", items=(1, 2, 9)) </s> assert "href>/calendar.ics/todo1.ics</" in answer	test_time_range_filter_todos_rrule <C:time-range start="20130801T000000Z" end="20131001T000000Z"/> </C:comp-filter> assert "href>/calendar.ics/todo2.ics</" in answer assert "href>/calendar.ics/todo9.ics</" in answer
# todo remove in v8 </s> return output, error_level, deps	compile_string output = output.decode('utf-8')
# todo: clean up </s> for _ in range(num_hosts)	gossipsubs )
# todo: remove </s> g.user_dict = user_dict	get user_role =\ authz.users_role_for_group_or_org(id, user) or u'member' extra_vars["user_dict"] = user_dict else:
# todo: make this a hard error, instead of a silent overwrite </s> logging.warning("kvm: overriding disk_cache setting '%s' with 'none'"	_GenerateKVMBlockDevicesOptions if cfdev.dev_type in constants.DTS_EXT_MIRROR: if disk_cache != "none": " to prevent shared storage corruption on migration", disk_cache)
# todo(shilpasd): need to provide support in python - novaclient </s> with utils.record_time(self.times, args.timings,	main 'auth_url', args.os_auth_url):
# todo make atomic </s> graph = tx.graph	Cog self.__db_merge__(tx) def __db_delete__(self, tx): for related_objects in self.related.values(): related_objects.clear()
# todo: kkrampa, shouldn't we wait to save the checkpoint until after we've processed all the data? </s> save_stock_data_checkpoint(checkpoint,	sync_supply_point_status facility=facility ) 'supply_point_status', meta.get('limit') or limit,
# todo dry </s> try:	_expect except pexpect.TIMEOUT: pass pnum = self.con.expect('Indication   handle = .*? \r', timeout=.5) if pnum == 0:
# todo: assert </s> repo = self.remote.get_repo("testrepo0")	test_get_repo Test: Get a repo object
# todo remove once project goes public </s> if not current.auth.s3_has_role("org_group_admin"):	customise_org_facility_controller s3.crud_strings.org_facility.title_list = T("Test Stations for School and Child Care Staff") elif code == "TESTS-PUBLIC": r.unauthorised() s3.crud_strings.org_facility.title_list = T("Test Stations for Everybody")
# todo: detect if any database upgrading is needed and acquire the lock only in one place </s> self.acquire_lock(event=false).__enter__()	before_table_create def before_table_create(event, target, bind, tables=None, **kw): if tables:
# todo: formatting test names when non-string names are involved </s> print _formatheader("%s skipping %s, test(s) %s disabled" %	ReportTestSkip file, or a list of testnames (which will be AND-ed together) tstart = datetime.datetime.now() (tstart, desc, testnames))
# todo: change the order of these arguments. </s> path = locator.find_object(self.search_dirs, obj)	load_object search_dirs: the list of directories in which to search. locator = self._make_locator() return self.read(path)
#todo handle found multiple worksheets with name </s> raise e	get_worksheet_uuid uuid = worksheet_util.get_worksheet_uuid(self.client, None, spec) except UsageError, e: return uuid
# todo: move this scopes conversion from and to string into a utils function </s> scopes = self.oauth2_data.get('scopes', [])	get_initial def get_initial(self): initial_data = { 'redirect_uri': self.oauth2_data.get('redirect_uri', None),
# todo: remove anytime in 2016 </s> _assert(false, "status in filter wasn't set - this isn't expected to be possible")	get_prefix_and_key_for_filter_results_and_parsed_params prefix = "%s %s" % ("status", prefix) else: def _get_key(): if parsed_params.module is not None and parsed_params.get_module_int() is None:
# todo: add documentation </s> if not len(homography.shape) == 3:	inverse def inverse(homography): raise ValueError("Input size must be a three dimensional tensor. Got {}" .format(points.shape))
# todo: @sbharadwajj implement and test </s> raise notimplementederror	_weight_jac_mat_prod def _weight_jac_mat_prod(self, module, g_inp, g_out, mat):
# todo: check the data! </s> count = 0	test_submodule_loop pipe_def = self._get_pipe_def(pipe_file) pipe = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def) for i in pipe: count += 1
# todo: verify lldp message (e.g. org-specific authenticator tlv) </s> return ofmsgs	rcv_packet self.logger.info('LLDP from port %u: %s' % ( pkt_meta.port.number, pkt_meta.pkt)) ban_rules = self.host_manager.ban_rules(pkt_meta) if ban_rules:
# todo: remove in 21.08 </s> for file in glob.glob(cache_path + "/*.dialog"):	write_cache_text def write_cache_text(cache_path, f): try: with open(file, 'r') as fp:
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
# todo: remove optional argument used for debugging </s> if request:	system info_logger(csv_import_username.username, " SYSTEM_IMPORTER_FILE_CSV_CRON_STATUS " + "created:" + str(systems_created_counter) + "|" + "updated:" + str(systems_updated_counter) + "|" + "skipped:" + str(systems_skipped_counter) + "|" + "multiple:" + str(systems_multiple_counter)) info_logger(csv_import_username.username, " SYSTEM_IMPORTER_FILE_CSV_CRON_END") return redirect(reverse('system_list'))
# todo(garyk) read base mac from configuration file (conf) </s> base_mac = [0xfa, 0x16, 0x3e]	test_requested_duplicate_mac with self.port() as port: mac = port['port']['mac_address'] base_mac_address = ':'.join(map(lambda x: "%02x" % x, base_mac)) self.assertTrue(mac.startswith(base_mac_address))
pass # todo </s> def _shuffle(self, start=none, end=none):	_shuffle @register(r'^shuffle( (?P<start>\d+):(?P<end>\d+)*)*$')
# todo(mriedem): if we can parse the volume_image_metadata field from </s> self.openstack('server delete --wait ' + server_name)	test_boot_from_volume cmd_output['status'], )
# todo: start here </s> if boto3 is none:	make_s3_resource :return: a :py:class:`boto.s3.connection.S3Connection`, wrapped in a :py:class:`mrjob.retry.RetryWrapper` raise ImportError('You must install boto3 to connect to S3') endpoint_url = self._s3_endpoint or s3_endpoint_for_region(region_name)
# todo (ismailsunni): create custom exception to catch since it </s> pass	allowed_data_types result = add_to_list(result, layer_constraint['data_type']) else: return result
# todo(nnorwitz): store the function_parameters. </s> token = self._get_next_token()	_get_method function_parameters = list(self._get_matching_char('(', ')')) del function_parameters[-1]  # Remove trailing ')'. default = [] if token.name == '=':
# todo: createpropertyconditionex with propertyconditionflags_ignorecase </s> new_cond = _iuia.createpropertycondition(	_build_condition full_cond = _iuia.CreateAndCondition(new_cond, full_cond) if title: _UIA_dll.UIA_NamePropertyId, title) full_cond = _iuia.CreateAndCondition(new_cond, full_cond)
# todo: find path properly </s> if path:	changeToPlaylistIndex else: path = self.findFilenameInDirectories(filename) self._player.openFile(path) else:
#change status of todo </s> contact.status = 'replied'	test_notification_by_child_table_field }) contact.save() contact.save() email_queue = frappe.get_doc('Email Queue', {'reference_doctype': 'Contact',
# todo: change this back to a factory in the instance trait some day </s> self.tick_generator = defaulttickgenerator()	__init__ def __init__(self, component=None, **kwargs): super(PlotAxis, self).__init__(**kwargs) if component is not None:
@domain_admin_required # todo: will probably want less restrictive permission </s> return render_to_response(request, 'locations/manage/locations.html', context)	locations_list }
"""@todo add progressbar for multisite. ensure the other one is hidden first.""" </s> try:	test_progressbar_url_file_hidden_in_ennumerate_themes @patch.object(ProgressBar, 'set') def test_progressbar_url_file_hidden_in_ennumerate_themes(self, p): self.scanner.enumerate_themes(self.base_url, self.scanner.plugins_base_url, hide_progressbar=True)
assert study_id == 0  # todo </s> self.trials[trial_id].params[param_name] = value	report_param def report_param(self, study_id, trial_id, param_name, value):
#todo(mdietz): apply the rest of the instance_type attributes going </s> instance_type = self.db.instance_type_get_by_flavor_id(context,	finish_resize instance_ref = self.db.instance_get(context, migration_ref['instance_id']) migration_ref['new_flavor_id']) self.db.instance_update(context, instance_ref,
""" todo: documentation </s> return false	streaming @property def streaming(self):
# todo: move this into removedirectory maybe. doing an external </s> if os.name == "nt":	_removeCPythonTestSuiteDir os.unlink("@test") except OSError: os.system("rmdir /S /Q @test") if os.path.exists("@test"):
# todo: fix with stubber / before send event </s> with mock.patch('botocore.endpoint.endpoint._send') as _send:	test_get_export 'accepts': 'application/yaml' } _send.return_value = mock.Mock( status_code=200, headers={}, content=b'{}')
# todo is the processor the correct place to set this? </s> keras.backend.set_image_dim_ordering('th')	SevenPlaneProcessor consolidate=consolidate, use_generator=use_generator) def feature_and_label(self, color, move, go_board, num_planes): Parameters
#todo fifechan / fife 0.3.5+ compat </s> self._setting.showsettingsdialog()	show_settings self._settings_extra_inited = True if hasattr(self._setting, 'showSettingsDialog'): else: self._setting.onOptionsPress()
# through a failure or not. todo </s> crr.repair_successful = false	_repair_error def _repair_error(f): crr.repair_failure = f return f
# todo: cleanup directory/basename.* files. </s> tmp = tempfile.namedtemporaryfile(	commit def commit(self): directory, basename = os.path.split(self._tag_cache_file) prefix=basename + '.', dir=directory, delete=False) try:
# todo: improve exception handling </s> particle = particle(argument, mass_numb=mass_numb, z=z)	particle_symbol Z: int = None) -> str: r"""Returns the symbol of a particle.""" return particle.particle
# todo deal with errors! </s> backend = backend.objects.get(name=backend_name)	registration contact = Contact(name=name) contact.save() connection = Connection(backend=backend, identity=identity,\ contact=contact)
#todo: multiple replication tasks </s> break	zfs_snapshot_list if remotename in snaps: replication = 'OK' snaplist.insert(0, zfs.Snapshot(
#todo: in some cases we can have something like: a=os.path </s> defaults.append(value)	_parse_function defaults = [] for value in symbol.args.defaults: arguments = [] for arg in reversed(symbol.args.args):
# todo: add docstring </s> return sample_system(sysc, ts, method)	c2d def c2d(sysc, Ts, method):
# todo: improve the complexity of this algorithm </s> for queue in node_state.queueids_to_queues.values():	handle_processed def handle_processed(node_state, state_change): remove = [] for pos, message in enumerate(queue):
raise exceptions.mpdnotimplemented  # todo </s> underscore, dash, dot and colon.	subscribe already. The name may consist of alphanumeric ASCII characters plus
# todo: we do another lookup in cast_param, refactor to reduce number of lookups </s> action_ref = action_node.ref	_run_next_action def _run_next_action(self, action_node, parent_context, action_params, context_result, wait_for_completion=True): action_db = action_db_util.get_action_by_ref(ref=action_ref) if not action_db:
# todo: add error handling for windowserror, a builtin </s> try:	GuessOS Returns: A string indicating which OS we are dealing with. if col_obj.FindPath('/(Windows|WINNT)/System32'): return 'Windows'
# todo (1.5.1) - have another try at simplifying all this... </s> links = req.chrome.get('links')	render_template template, data = self.prepare_template(req, filename, data, text, domain) scripts = req.chrome.get('scripts') script_data = req.chrome.get('script_data')
# todo: add logging to the process </s> fobj = open_compressed(filename, mode="r", encoding=encoding)	pgimport ): Required: psql command sample = fobj.read(chunk_size) fobj.close()
# todo: should be able to specify fields </s> if isinstance(dialect, six.text_type):	sqlite2csv def sqlite2csv(input_filename, table_name, output_filename, dialect=csv.excel, batch_size=10000, encoding='utf-8', callback=None, query=None): dialect = csv.get_dialect(dialect) if query is None:
# todo add locales </s> raise yunohosterror("domain_name_unknown", domain=domain)	domain_set_settings domains = _load_domain_settings() if not domain in domains.keys(): setting_set = False if ttl is not None:
encrypting_key = bytes.fromhex(encrypting_key)  # todo: move / validate </s> new_card = card(character_flag=character_flag,	create if not encrypting_key: encrypting_key = click.prompt('Enter Encrypting Key', type=click.STRING) verifying_key=verifying_key, encrypting_key=encrypting_key,
# todo: would like to make this a name that a user can re-name. </s> ctx.out_keyword('$global%d' % (op.value))	notify_out_operand wtype = op.specval if wtype == WASM_GLOBAL: return True elif wtype == WASM_LOCAL:
#only checking length. this should check functionality as well (todo) and/or import that information from the sram </s> debug.check(len(sram.pins) == len(pin_names), "number of pins generated for characterization do match pins of sram")	inst_sram def inst_sram(self, sram, port_signal_names, port_info, abits, dbits, sram_name): pin_names = self.gen_pin_names(port_signal_names, port_info, abits, dbits) self.sf.write("Xsram ") for pin in pin_names:
#todo now the content must be in cache! (got to handle transfer error) </s> path = os.path.expanduser(os.path.join('~', '.config',	_get_msn_object_path self.cbs = (self._download_avatar, self._download_avatar) self.msn_object_store.request(msn_object, self.cbs) 'emesene2', self.session.account.account, msn_object._checksum_sha + ".png"))
# todo: follow-up actions for downtime </s> missed_periods = onchain_period - self.last_active_period	_confirm_period self.log.info("Checking for new period. Current period is {}".format(self.__current_period)) if self.__current_period != onchain_period: if missed_periods: self.log.warn(f"MISSED CONFIRMATION - {missed_periods} missed staking confirmations detected!")
# todo ... </s> self.error("preprocessor: if-evaluation not yet implemented; cannot check '" + condstr + "'")	cpreprocess_evaluate_cond def cpreprocess_evaluate_cond(state, condstr): return True # may be more often what we want :P
# todo hack! include image digest and status, needed for the downstream notifications handler </s> if curr_evaluation_result:	perform_policy_evaluation raise err curr_final_action = curr_evaluation_result.get('final_action', '').upper() curr_evaluation_result['image_digest'] = imageDigest if curr_final_action in ['GO', 'WARN']:
# todo: would need to either avoid this "decorator" approach for </s> toppath_ = os.path.join(os.path.dirname(__file__), 'testrepos') \	newfunc def newfunc(*arg, **kw): if toppath is None else toppath globs = glob.glob(os.path.join(toppath_, paths))
# todo manage tangent? </s> scale_mat = conversion.scale_to_matrix(conversion.loc_gltf_to_blender(values[idx * 3 + 1]))	parse_scale_channel for idx, key in enumerate(keys): if animation.samplers[channel.sampler].interpolation == "CUBICSPLINE": else: scale_mat = Conversion.scale_to_matrix(Conversion.loc_gltf_to_blender(values[idx]))
# todo: implement me! </s> pass	vectorize If this is a 2 level folder structure the first level is assumed to be categories to be used as labels for supervised learning.
# todo: take _buffercount into account </s> args = ql.os.utils.va_list(format, arglist)	hook___stdio_common_vswprintf format = params['_Format'] arglist = params['_ArgList'] count = ql.os.utils.sprintf(buff, format, args, wstring=True) ql.os.utils.update_ellipsis(params, args)
# todo: perhaps next loop could be made more efficient </s> c = b[l2 : l]	_solve_linear_de term2 = S(a[:L], check=False) product = (term1 * term2).list() for j in range(L2 - 1, min(L-1, len(product))): c[j - (L2-1)] = c[j - (L2-1)] + product[j]
# todo if this is going into production (hopefully it isn't) then check </s> cptp, expand to arbitrary dimensional systems, etc.	_dep_choi Returns the choi matrix corresponding to qubit depolarization for a given parameter pe. return Qobj(dims=[[2, 2], [2, 2]], inpt=array([[1. - pe / 2., 0., 0., 1. - pe],
# todo pseudo code: </s> pass	OpenUri def OpenUri(self, uri): logger.debug(u'%s.OpenUri called', PLAYER_IFACE)
# todo(b/160294509): use tf.compat.v1 when we stop supporting tf 1.15. </s> if ops.executing_eagerly_outside_functions():	__init__ super(TransformFeaturesLayer, self).__init__(trainable=False) self._tft_output = tft_output self._check_tensorflow_version() self._saved_model_loader = saved_transform_io_v2.SavedModelLoader(
# todo specify custom/caching document loader in options to speed </s> return jsonld.flatten(obj, ctx={"@context": "http://schema.org/"})	flatten_metadata_graph def flatten_metadata_graph(obj): from pyld import jsonld
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_interact_arguments "-B", "--block-size", metavar="SIZE", type=size, help="Flash block size, in pages (default: autodetect)") p_read = p_operation.add_parser( "read", help="read data and spare contents for a page range")
# todo: try to find a better way to deal with local execution </s> if self.is_method and not self.locations and self.owner == sy.hook.local_worker:	_after_running_local_method def _after_running_local_method(self): self._self.get()
# @todo: this part takes most of the time. need a better solution. </s> for idx in range(0, buffer_len - 2, 3):	get_pixels if bmp: windll.gdi32.DeleteObject(bmp) self.image[idx + 2], self.image[idx] = self.image[idx], self.image[idx + 2] return self.image
# todo handle pgp_expiration_alert and pgp_expiration_notice already included in client/app/data/txt </s> if k in ['pgp_expiration_alert', 'pgp_expiration_notice']:	initialize_node notification.source_email = notification.username for k in appdata['templates']: continue if k in appdata['templates']:
# todo: try to unify user-input collection (both for registers and this kind of </s> 'user_input': self.user_input,	parse_motion 'is_digraph_start': False, '_internal_mode': None, } if self.mode in (MODE_VISUAL, MODE_VISUAL_LINE) or (self.motion and self.action):
# todo: adjust dimension order for tf2 broadcasting </s> lambda out, zero: tf.compat.v1.where(finished, zero, out),	body if impute_finished: emit = tf.nest.map_structure( next_outputs, zero_outputs)
# todo username </s> sudo = args.pushy('ssh+sudo:{hostname}'.format(hostname=hostname))	purge for hostname in args.host: log.debug('Detecting platform for host %s ...', hostname) lsb_release_r = sudo.compile(lsb_release) (distro, codename) = lsb_release_r()
# todo(rosmaita): simplify when lower_constraints has webob >= 1.8.1 </s> try:	test_translate_exception @mock.patch.object(i18n, 'translate') def test_translate_exception(self, mock_translate): from webob.acceptparse import AcceptLanguageValidHeader  # noqa cls = webob.acceptparse.AcceptLanguageValidHeader
# todo: clean this up </s> owner = discord.utils.find(lambda m: m.id == self.config.owner_id and m.voice_channel, self.get_all_members())	MusicBot as_ok = await self._auto_summon() if self.config.auto_playlist and as_ok: await self.on_finished_playing(await self.get_player(owner.voice_channel)) async def handle_help(self):
pass  # todo </s> self._current_command = []	__init__ self._device = None
# todo: add more sell close test </s> assert len(orders) == 0	test_buy_close def handle_bar(context, bar_dict): orders = buy_close(context.f1, 1)
# todo : not sure if this is the best check here. </s> if isinstance(type(k), undefinedfunction):	parse_specifieds specified_values = np.zeros(self.num_specifieds) for k, v in r.items(): k = (k,) idx = [self.specifieds.index(symmy) for symmy in k]
# todo(b/175815580) update logic based resolution of the issue. </s> if "google" in sys.modules:	get_client_environment_name logging.info("Detected running in DLVM environment.") return ClientEnvironment.DLVM.name logging.info("Detected running in DL_CONTAINER environment.") return ClientEnvironment.DL_CONTAINER.name
# todo: logs might contain sensitive data such as contents of the </s> le_util.make_or_verify_dir(args.logs_dir, 0o700, os.geteuid())	main le_util.make_or_verify_dir( directory, constants.CONFIG_DIRS_MODE, os.geteuid()) _setup_logging(args) logging.debug("Arguments: %r", cli_args)
# todo: add here additional variants for other reprog_controls </s> count = feature_request(device, feature.reprog_controls)	get_keys def get_keys(device): if count is None: count = feature_request(device, FEATURE.REPROG_CONTROLS_V4)
buildlog = f.read() #todo: this may not return all output </s> jrf.addtest(bid, package.installed, buildlog)	testinstall installLog = join_path(package.stage.source_path, 'spack-build.out') with open(installLog, 'rb') as F: with open(args.output, 'wb') as F: jrf.writeTo(F)
# todo: don't write them? is *much* slower on re-load (~3x) </s> try:	update_lib fh.flush() os.fsync(fh.fileno())  # flush to disk os.unlink(path + 'c') except OSError:
percentiles_to_calculate = range(0, 101, 1)  # todo: get input from user </s> headers = constants.submetric_header + ',mean,std,p50,p75,p90,p95,p99,min,max\n'  # todo: this will be built from user input later on	calculate_stats def calculate_stats(self): stats_to_calculate = ['mean', 'std', 'min', 'max']  # TODO: get input from user metric_stats_csv_file = self.get_stats_csv() imp_metric_stats_csv_file = self.get_important_sub_metrics_csv()
# todo: change this to be architecture independent </s> return prev_addr	get_ret_adr prev_addr, farref = idaapi.decode_preceding_insn(nextInst)  # Get previous instruction
# todo: remove this patching when the `content` property is supported. </s> with monkeypatch_validation(validate_content):	test_annotate_document @suite.test def test_annotate_document(): document = parse_html( 'doc1.html',
# todo: rethink factorization? </s> windowcontroller = self._nsobject.window().windowcontroller()	_projectFontsChanged self.refitFontItems() self.resetSelection() windowController.loadFonts()
# todo, using publicsuffix instead of this rewrite rule </s> old_result_domainname = '.'.join(	https_url_rewrite if result['url'] == new_result_url: continue result['parsed_url'].hostname.split('.')[-2:]) new_result_domainname = '.'.join(
# todo dunno maybe use the same mypy config in repository? </s> mres = run([	config_check warning("mypy not found, can't check config with it") else: 'python3', '-m', 'mypy', '--namespace-packages',
# todo(brett.cannon) implement </s> raise importerror	_default_hook def _default_hook(self, path): If the path will not work for the default hook then raise ImportError.
self.my_sender('text/cache-manifest', bytes(manifest, 'utf-8')) # todo: cache-control/last-modified headers </s> manifest += '\n# hash: {}'.format(hasher.hexdigest().upper())	generate_manifest manifest += '*\n'
# todo: remove this </s> logger.warning((	windows_file def windows_file(*args, **kwargs): 'Use of `windows_files.windows_file` is deprecated, ' 'please use `windows_files.file` instead.'
# todo: check for stable/prerelease/current/beta if we support </s> self.lgr.info(	update_fetch "{}/jails/{}/root/dev".format(self.iocroot, uuid)] new_root = "{}/jails/{}/root".format(self.iocroot, uuid) "\n* Updating {} ({}) to the latest patch level... ".format( uuid, tag))
# todo: remove: legacy function </s> return product.objects.filter(product_category=self, draft=false).count()	published_product_count @property def published_product_count(self):
# todo: boto3 call can fail with botocore.exceptions.clienterror, </s> response = client.change_resource_record_sets(	update_dns_zone_record dns_json = get_template(template_file='infrastructure/dns_upsert.json.j2', **kwargs) HostedZoneId=zone_id, ChangeBatch=json.loads(dns_json), )
# todo 更新用户model后替换 </s> from app.lin.core import user	select_by_user_id def select_by_user_id(cls, user_id) -> list: 根据用户Id，通过User-Group关联表，获取所属用户组对象列表 from .user_group import UserGroup query = db.session.query(UserGroup.group_id).join(
# todo: remove in 21.08 </s> log.warning("this method is deprecated, use texttospeechcache.clear")	clear_cache def clear_cache(self): if not os.path.exists(mycroft.util.get_cache_directory('tts')): return
""" type setting - todo explain """ </s> return discovery['changeset']	get_changeset def get_changeset(self,obj):
pass # todo </s> def _playlistadd(self, name, uri):	_playlistadd @register(r'^playlistadd (?P<name>\S+) "(?P<uri>[^"]+)"$')
# todo: query the locations </s> pass	match_facility def match_facility(xlsx_facility_name): Given facility name taken from the spreadsheet, return the id name and id of the matching location in HQ.
# todo: test me @jmcarp </s> admins = [	manage_contributors if user not in users ] user for user in users if self.has_permission(user, 'admin')
# todo(b/163904067): mimic defun' behavior and reset the step seed to </s> resetstepseed()	Forward @tf.function(input_signature=Flatten(fwd_sig), autograph=False) def Forward(*args): with RemoveAssertContext(remove=noinline), tf.device(device): xs = Pack(fwd_sig, args)
common_path=prefix,  # todo: add key? </s> action="custom",	make_inline_attachments_decision else: md = MergeDecision( conflict=True, local_diff=[ld],
# todo: work out a way to set this based on the timespan of the data. </s> locator = mdates.autodatelocator(minticks=5, maxticks=25)	plot ax.legend(loc="upper right") axes[-1].set_xlim(self.to_dataframe().index[0], self.to_dataframe().index[-1]) formatter = mdates.ConciseDateFormatter(locator) axes[-1].xaxis.set_major_locator(locator)
# todo fix bug for not identifying unused variables in nested exceptions see issue #4391 </s> try:	function2 1 / 0 except ZeroDivisionError as error: 1 / 0 except ZeroDivisionError as error:
# todo: implement this </s> pass	__init__ pass elif field.is_many: else: self.set_books_func = (one_one_in_books if field.metadata['table']
"""todo: not implemented""" </s> notimplementederror("coalesce not implemented")	coalesce @symbolic_dispatch def coalesce(*args):
self.__parameters.update(parameters)  # todo test </s> def set_params(self, **parameters):	set_params
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
else:  # todo: localization </s> phrase = "understand the phrase " + utterance	handle_fallback query = "%s %s %s" % (utt_word, utt_verb, utt_query) phrase = "know %s %s %s" % (utt_word, utt_query, utt_verb) try: res = self.client.query(query)
# todo: should this move to case.rebuild? </s> if not case.xform_ids:	hard_rebuild_case case.domain = domain rebuild_case_from_actions(case, actions) if not found: return None
elif key_type == unicode:  # todo: change to 'str' on python3 </s> values = list(value)  # i'm not lazy, sorry	Table if key_type == int: self._rows[key] = self._make_row(value) if len(values) != len(self): raise ValueError('Values length ({}) should be the same as '
# todo implement. </s> pass	uniform_weights def uniform_weights(weights, contraints=[-0.5, 0,5]):
raise glomerror(msg)  # todo: dedicated exception type for this? </s> msg += ' (at %r)' % path(*path)	_missing_get_func if path is not None:
# todo: set fileinfo to a valid object. </s> _update_metadata('targets', none)	test_3__update_metadata self.assertRaises(tuf.RepositoryError, _update_metadata, 'targets', None) self._mock_download_url_to_tempfileobj(self.targets_filepath) list_of_targets = self.Repository.metadata['current']['targets']['targets'] if added_target_1 not in list_of_targets.keys():
#@todo: remove in 0.4.10 </s> def error(self, reason=none, type="parse"):	error raise Fail("%s error%s | Plugin out of date" % (type.capitalize(), ':' + str(reason) if reason else "")) if self.core.debug:
# todo implement this method </s> pass	start start interacting :return:
return 0.25  # todo: make configurable and/or account for as many factors as we can </s> def get_tune_delay(slf):	get_tune_delay
#todo could use original filename to verify this </s> assert_true(filename.endswith('.mp3'))  # depends on specific file	assert_download def assert_download(sid=self.songs[0].sid): filename, audio = self.mm.download_song(sid) assert_is_not_none(audio)
annot.annotation_metadata.annotator.email = "todo"  # todo </s> annot.annotation_metadata.annotator.name = "todo"	fill_annoatation_metadata annot.annotation_metadata.origin = "Centre for Digital Music"
#todo: this can probably just be removed now? </s> self.listeners[listener] = self._wnd	addWindowEventListener listener = Callback(self, cb, True) self._wnd.add_event_listener(event_name, listener, False, None)
# todo add index kwarg and move the playlist if it is specified </s> if len(name) > 255:	add_new_playlist The playlist name must not be space-only or shorter than 256 chars. Returns the new playlist. raise ValueError('Playlist name must be shorter than 256 chars') if len(name.replace(' ', '')) == 0:
# todo: where else is this a problem? </s> for rnn_type in ['grufused', 'lstmfused']:	init handle, try_caching=True, verbose=verbose) wrap.rnn_cast(torch.nn.backends.thnn.backend, 'RNN', verbose) mod = getattr(torch.nn._functions.thnn.rnnFusedPointwise, rnn_type) wrap.disable_casts(mod, 'backward', handle)
# todo: unfortunately, this feature is not yet implemented for python </s> pass	test_tls_signed def test_tls_signed(self):
#todo: read from file or generate... needs to be correlated with hive </s> return {'pop3' : {'server' : '127.0.0.1', 'port' : 2100, 'login' : 'test', 'password' : 'test'}}	get_credentials def get_credentials():
# todo debug </s> print logstring	logRule + "end=%d)") % item.element.end logging.info("[%s]: %s" % (fileName, logString)) else: raise ValueError("Rule has invalid type: '%s'."
# todo: finish this. </s> def mkdir(self, path, mode):	mkdir pass
# todo: what if service nesting depth is more than max python stack depth? </s> setforservices(childservicejob)	setForServices serviceJob.prepareForPromiseRegistration(jobStore) for childServiceJob in serviceJob.service._childServices:
# todo: query stored contract and reconstitute </s> contract = cls(alice=alice, alices_signature=alices_signature, kfrag=kfrag,	from_ursula alices_signature, policy_payload = BytestringSplitter(Signature)(cleartext, return_remainder=True) kfrag, encrypted_challenge_pack = policy_payload_splitter(policy_payload, return_remainder=True) encrypted_challenge_pack=encrypted_challenge_pack) return contract
# todo: verify lldp message (e.g. org-specific authenticator tlv) </s> return ofmsgs	rcv_packet return lacp_ofmsgs self.lldp_handler(pkt_meta) ban_rules = self.host_manager.ban_rules(pkt_meta) if ban_rules:
# todo: separate tpu case from here </s> self._clip_gradients(optimizer)	clip_gradients def clip_gradients(self, optimizer):
# todo(b/163904067): mimic defun' behavior and reset the step seed to </s> resetstepseed()	Fn @tf.function(input_signature=input_signature, autograph=False) def Fn(*args): return fn(*args)
#opusenc invokation (todo: ffmpeg?) </s> opus_convert_call = ['opusenc', fname, fbase + '.opus']	media_convert file_write(fname, file_data) if not args.no_opus: dbg("converting... : " + fname + " to opus.", 1) oc = subprocess.Popen(opus_convert_call, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
# here fixme todo </s> truestart += 1	array if entrystart != 0: basket = self.basket(basketstart, interpretation=interpretation, entrystart=entrystart, entrystop=entrystop, basketcache=basketcache) truestop = basketstop if entrystop != self.numentries:
pass # todo </s> def upload_ssk(write_capability, new_version, uploadable):	upload_ssk
# todo - probably delete </s> if hasattr(self, "admin2"):	set_admin2 def set_admin2(self): return None if not hasattr(self, "document_name"):
# todo data alignment stuff </s> stride = bufferview.byte_stride or default_stride	get_data_from_accessor_obj fmt = '<' + (fmt_char * component_nb) default_stride = struct.calcsize(fmt) unpack_from = struct.Struct(fmt).unpack_from data = [
#todo : stop properly the module </s> print "[mysqldb] module raise an exception : %s . please check the arguments!" % exp	connect_database self.db.connect_database() except _mysql_exceptions.OperationalError as exp:
# todo(stevemar): assert returned fields </s> self.openstack('container show ' + self.container_name)	test_container_show def test_container_show(self):
# todo this is a workaround since exceptions are currently not correctly stacked </s> pass	search return self._search(self.pattern, string, pos, default(endpos, -1)) except RuntimeError: return self.__compile_cpython_sre().search(string, pos, default(endpos, maxsize()))
# todo: raise a specific exception </s> assert not value or value in settings.sep_chars	sep @auto_save def sep(self, value): self._data['sep'] = value.strip()
#todo: assert that layer inputs are always >= 0 </s> def __init__(self, *args, **kwargs):	ZPlusRule for alpha=1, beta=0, which assumes inputs x >= 0 and ignores the bias. super(Alpha1Beta0IgnoreBiasRule, self).__init__(*args, **kwargs)
# todo(mfedosin): think of a way to avoid this. </s> if not hasattr(task_ex, "in_context"):	_find_next_tasks if self._is_conditional_transition(task_ex) or ctx is not None: task_ex = db_api.get_task_execution(task_ex.id) ctx_view = data_flow.ContextView(
# @todo: support for branches </s> fields.append("human_resource.organisation_id$name")	pr_person_check_duplicates show_orgs = settings.get_hrm_show_organisation() if show_orgs: rows = resource.select(fields=fields, start=0,
# todo: add this back in once we've merged back the refactored users code </s> self.assertequals(users_count, 1)	testStealCommCareUser users_count = CouchUser.view("users/by_commcare_username_domain", key=[self.domain, self.commcare_username]).total_rows
# todo: parameters to add to theme.ini: </s> themename = config.get("coffee", "themename")	main videoLayer = False if videoAvailable: vidSource = os.path.join(Version.dataPath(), 'themes', themename, \ 'menu', 'intro.avi')
# todo: change internals to use sparse throughout and delete this: </s> if issparse(l):	train self.config = recursive_merge_dicts(self.config, kwargs, misses='ignore') L = L.todense() self._set_constants(L)
# todo: re-enable for hardware </s> self.wait_for_stack_port_status(	verify_all_stack_up for switch_port_no in range(self.topo.switch_to_switch_links): port_no = port_base + switch_port_no dpid, dp_name, port_no, 3) # up
# todo: waiting for a fix: https://developer.blender.org/t53509 </s> mat = context.object	draw split.separator() if mat: layout.label("Material Nodes:") layout.template_ID(mat.luxcore, "node_tree", new="luxcore.mat_nodetree_new")
# todo: try testing this </s> if 'error' in ret and 'code' in ret['error'] and ret['error']['code'] == 'request_token_expired':	put ret = r.json() if r.status_code != requests.codes.ok and r.status_code != requests.codes.created: raise AuthError(ret) else: raise ProtocolError(ret)
# todo put this in a .extra w/a subselect </s> if not hasattr(self, '_hours_worked'):	ProjectContract num_hours = models.PositiveIntegerField() def hours_worked(self): self._hours_worked = Entry.objects.filter( project=self.project,
if path.endswith('/index'):  # todo: remove in v8 </s> utils.logger.warn("categories_index_path for language {0} is missing a .html extension. please update your configuration!".format(lang))	get_overview_path if self.site.config['CATEGORIES_INDEX_PATH'](lang): path = self.site.config['CATEGORIES_INDEX_PATH'](lang) path += '.html' return [_f for _f in [path] if _f], 'never'
# todo more_itertools? </s> return len(list(it))	ilen def ilen(it):
# todo: remove when unicode vlens implemented </s> if unicode_hack:	__getitem__ arr = numpy.ndarray(attr.shape, dtype=dt, order='C') attr.read(arr) bytestring = arr[()] return bytestring.decode('utf8')
# todo security </s> def query_suggest(request, core, query=""):	query_suggest hue_core = Core.objects.get(name=core) result = {'status': -1, 'message': 'Error'}
# note/todo: fixed (realpath) path should go. inner logic has to adapt to </s> set_db_status_from_file(realpath(filepath1))	_test_AnnexDB with open(filepath1, 'a') as f: f.write('+') assert(db.is_different('file1.txt', status1)) set_db_status_from_file('2git')
# todo 对接 </s> users, items = test_data	get_ground_truth def get_ground_truth(self, users, test_data): return users, items
# todo: handle shortcuts </s> pass	_add_ui_settings window.add_accel_group(page.action_group.shortcuts) else:
arch = 'x86_64-linux-gnu' # todo figure this out </s> configdir = os.path.join(self.installdir, 'etc', 'xdg', 'qtchooser')	build_qt_config def build_qt_config(self): try: os.makedirs(configdir)
# todo: use pybossa uploader! only for debugging: </s> open('/tmp/%d_%s_task_run_json.zip' % (app.id, name), 'wb').write(memzip.getvalue())	export_json memfile.write(str(line)) memzip = make_onefile_memzip(memfile, '%s_task_run.json' % name)
# todo: can remove this skip once cupy/cupy/#2330 is merged </s> return array	test_pad array = xp.array(self.array, dtype=dtype) if xp.dtype(dtype).kind in ['i', 'u'] and self.mode in ['mean']: def f(): if self.mode == 'constant':
# todo: could be more precise by only looking the inames' attributes </s> return (self.inames == other.inames	__eq__ KernelProxyForCodegenOperationCacheManager)): return False and self.instructions == other.instructions and self.schedule == other.schedule)
# todo: implement in all datasets. </s> idx_to_word = dataset.idx_to_word()	train def train(self, dataset): train_set   = dataset.read_data("train") sentences = [source_ids for source_ids, _ in train_set]
print(err) # todo show that message in an empty window </s> return false	_get_valid_file else: err = err + excp.message is_image, err = utilities_gfile_is_image(gfile, err) if is_image:
# todo checks for being not outside of this repository </s> return (	quick_check def quick_check(filename): filepath = opj(self.path, filename) islink(filepath) and 'annex/objects' in str(realpath(filepath))  # realpath OK
pass # todo </s> def _deleteid(self, songid):	_deleteid @register(r'^deleteid (?P<songid>.*)$')
# todo: this is untested. </s> _raise_current_error()	load_certificate_request raise ValueError("type argument must be FILETYPE_PEM or FILETYPE_ASN1") if req == _ffi.NULL: x509req = X509Req.__new__(X509Req) x509req._req = _ffi.gc(req, _lib.X509_REQ_free)
singleton=false,  # todo: re-enable </s> )	pillars tag='pillar', pack=pack, return FilterDictWrapper(ret, '.ext_pillar')
# todo(b/161529310): we flatten and convert the trainable specs to </s> return tf.constant(0), client_optimizer.initialize(	initial_state_train_reduce lambda v: tf.TensorSpec(v.shape, v.dtype), global_model_weights.trainable) _flat_tuple(trainable_tensor_specs))
# todo: remove the paired-end specific plots when all report datasets are single end </s> lineplot_data = [	adapter_removal_length_dist_plot ] } self.len_dist_plot_data['all'], self.len_dist_plot_data['mate1'],
#todo: write doc </s> newgeom = none	createGeometry def createGeometry(self, viscol, geomsrc): if viscol['geometry'] is not {}: dimensions = None
# todo: this will incorporated in the future, if needed. </s> edge_starts = []	specify_edge_starts )) elif self.net_params.additional_params["on_ramp"]: else: edge_starts = []
# todo: some kind of value escape </s> return ",".join(["%s=%s" % kv for kv in zip(self.keys, values)])	_render_key def _render_key(self, values):
# todo: this is crazy fragile, indicates we want to refactor </s> if args[0] == 'test -e "$(echo /etc/centos-release)"':	centos_exists def centos_exists(*args, **kwargs): return Result(connection=cxn, exited=0) return Result(connection=cxn, exited=1)
raise notimplementederror # the below does most probably not work anymore todo </s> create_dependencies = kwds.pop('create_dependencies', true)	gather TEST:: TODO download = kwds.pop('download', False) if len(tickets) == 0:
# todo: move to middleware </s> if 'from' not in transaction and is_checksum_address(self.default_account):	send_transaction_munger def send_transaction_munger(self, transaction: TxParams) -> Tuple[TxParams]: transaction = assoc(transaction, 'from', self.default_account) if 'gas' not in transaction:
# todo write me </s> pass	cgiHandler print >> stdout, 'Access-Control-Allow-Origin:', layer.allowed_origin if layer.max_cache_age: print >> stdout, 'Content-Length: %d' % len(content) print >> stdout, 'Content-Type: %s\n' % mimetype
# todo: use the kinetic scroller if implemented </s> view = self.parent().parent()	drag if dy < 0: dy = 0 view.startScrolling(QPoint(dx, dy))
# todo: better strategy here? </s> for k in keep_splits:	extract keep_splits = all_splits.rename(columns = new_names) if convert: try: keep_splits[k] = pd.to_numeric(keep_splits[k])
# todo: invalidate cache for former latestappinfo </s> get_all_case_properties.clear(self)	ApplicationBase if not self._rev and not domain_has_apps(self.domain): domain_has_apps.clear(self.domain) get_usercase_properties.clear(self) get_app_languages.clear(self.domain)
# todo move to glyf or gvar table proper </s> return coord, control	_GetCoordinates (0, bottomSideY)])
assert oldsize == self.size, '%s: %d, %d' % (self.type, oldsize, self.size) # todo: remove </s> return self.size	Atom else: self.size = 8 + sum([atom.calsize() for atom in self.body])
#todo: remove convert_bare true and deprecate this in with_ </s> try:	_get_delegated_vars if task.loop is not None: if task.loop in lookup_loader: loop_terms = listify_lookup_plugin_terms(terms=task.loop_args, templar=templar, loader=loader, fail_on_undefined=True, convert_bare=True) except AnsibleUndefinedVariable as e:
# todo: write out full expression for general rhs of the form </s> _, f = self.broken_rhs.split()	initialize K_local = Tensor(gammar('+') * ufl.dot(trial, n) * ufl.dS) sigma_h, u_h = self.broken_solution.split() M = B * A.inv * B.T + C u_sol = M.inv * f + M.inv * (B * A.inv *
# update new ratings kodi 17 - todo get ratingid for updates from embydb </s> if self.kodi_version > 16:	add_update total = round(float(runtime), 6) self.kodi_db.add_playstate(fileid, resume, total, playcount, dateplayed) ratingid =  self.kodi_db.create_entry_rating() self.kodi_db.add_ratings(ratingid, movieid, "movie", "default", rating, votecount)
# todo: handle this </s> @param contact: the contact whose presence changed	on_contact_infos_changed def on_contact_infos_changed(self, contact, infos): @type contact: L{Contact<papyon.profile.Contact>}""" print "[papyon]", "on_contact_infos_changed", contact, infos
# todo: shouldn't have to concretize here.  fix dag issues. </s> specs = spack.cmd.parse_specs(args.spec, concretize=true)	activate def activate(parser, args): if len(specs) != 1: tty.die("activate requires one spec.  %d given." % len(specs))
# todo: commented until i find a way to make it store only episodes really updated. </s> logging.debug('bazarr all movies synced from radarr into database.')	update_movies for added_movie in movies_to_add: store_subtitles_movie(path_replace_movie(added_movie[1])) list_missing_subtitles_movies() logging.debug('BAZARR All movie missing subtitles updated in database.')
# todo: update the tolerance after the ci moves to torch 1.10 </s> self.assertalmostequal(outputs.loss.item(), 17.7963, 2)	test_inference_speaker_verification self.assertAlmostEqual(cosine_sim(embeddings[0], embeddings[1]).numpy(), 0.7579, 3) self.assertAlmostEqual(cosine_sim(embeddings[2], embeddings[3]).numpy(), 0.7594, 3)
# todo: support botorch v0.4.0. see: https://github.com/optuna/optuna/issues/2381 </s> "botorch<0.4.0 ; python_version>'3.6'",	get_extras_require "torchaudio==0.7.2", "allennlp<2.0.0", "fastai", ],
pass # todo </s> def create(self, name):	create
# todo add assertions </s> self.assertequal(poll.objects.count(), 0)	test_delete p = Poll.objects.get() p.delete()
# todo: why does resourcefilecache return none in some cases? </s> name = self._replace_variables(name)	get_library_keywords kws.extend(self.datafile.namespace.get_library_keywords(name, args)) for name in self.get_resource_imports(): res= self.datafile.namespace.get_resource_file(self.datafile.source, name)
# todo username </s> return 'aqbwdj5qap6lhhaaskvbnukyhj7eyremko5qka=='	get_monitor_secret def get_monitor_secret():
# todo: make the buffer longer and arrange so partial updates rather than the entire buffer can be sent to clients. </s> self.__text = textstring[-100:]	get_text textstring = self.__text textstring += message.to_string().decode('us-ascii') return self.__text
#todo: orgdatetime is not implemented yet </s> 'text' contains valid dates (in the body).	test_heading_parsing_with_date_and_body def test_heading_parsing_with_date_and_body(self):
1  # todo: fill in identifier </s> )	profile_transfer amount, target, result.wait() profiling.print_all_threads()
# todo could do smarter matching of results to trials if we have extras </s> for env_id in benchmark.env_ids:	benchmark_aggregate_results end_times = [] scorer = benchmark.scorer task_list = benchmark.task_specs(env_id) num_trials = task_list[0].trials
# todo: handle cpu time differences, where "e" comes before "b" </s> root = {'name': 'root', 'value': 0, 'children': []}	trace_event_generate_flame_graph def trace_event_generate_flame_graph(filename, range_start, range_end, profile=None): open_partial_slices = {} if not profile:
# todo: allow setting a placeholder dom element, or any widget parent </s> this.node = document.createelement('button')	_js_init @js def _js_init(self): zoof.get('body').appendChild(this.node); this.node.innerHTML = 'Look, a button!'
# todo need to ditch this requirement </s> if not self.guid:	save def save(self, *args, **kwargs): raise ValueError("Profile must have a guid!") if not validate_handle(self.handle):
# todo this should be fixed in pandas 0.18.2 </s> if pd.__version__ == '0.18.0':	test_categorize_info from io import StringIO from dask.compatibility import unicode from pandas.core import format else:
# todo: update tests instead? </s> kt.erased = true	fast_dict_type if stargs and (stargs[0] != kt or stargs[1] != vt): return None vt.erased = True return self.chk.named_generic_type('builtins.dict', [kt, vt])
# todo: raise error </s> pass	handle_additional_actions return_url = payment.return_url if not return_url: request_data = { "paymentData": data["payment_data"],
# todo this is a temporary hack python-136 is the right solution for this </s> return dict	document_class def document_class(self):
# todo: kwargs </s> def _impl(df, axis=none, skipna=none, level=none, ddof=1, numeric_only=none):	std_overload @overload_method(DataFrameType, 'std') def std_overload(df, axis=None, skipna=None, level=None, ddof=1, numeric_only=None): return hpat.hiframes.pd_dataframe_ext.std_dummy(df) return _impl
#todo fifechan / fife 0.3.5+ compat </s> widget = fife_setting.optionsdlg	show fife_setting = horizons.globals.fife._setting if not hasattr(fife_setting, '_optionsDialog'): else: widget = fife_setting._optionsDialog
#ng_required="true",    # todo: validation </s> validate_username="",	NewMobileWorkerForm crispy.Field( 'username',
# todo remove this sleep to reproduce http://tracker.ceph.com/issues/8107 </s> import time	test_lifecycle self._create(cluster_id, pool_name, pg_num=64) pool_id = self._assert_visible(cluster_id, pool_name)['id'] time.sleep(10) self._update(cluster_id, pool_id, {'pg_num': 128})
# todo ... </s> self.error("preprocessor: if-evaluation not yet implemented; cannot check '" + condstr + "'")	cpreprocess_evaluate_cond def cpreprocess_evaluate_cond(state, condstr): return True # may be more often what we want :P
# todo: only devnet for now </s> geth_process = nucyphergethdevnetprocess(password=password, config_root=config_root)	deploy password = click.prompt("Enter Geth node password", hide_input=True) if geth: geth_process.start()  # TODO: Graceful shutdown geth_process.wait_for_ipc(timeout=30)
## \todo: remove nodegraph fallback when all client code has been updated </s> needsmodifiers = not gaffer.metadata.value( selection[0], "grapheditor:childrenviewable" ) or not gaffer.metadata.value( selection[0], "nodegraph:childrenviewable" )	__keyPress selection = self.scriptNode().selection() if selection.size() == 1 and selection[0].parent() == self.graphGadget().getRoot() : if ( ( needsModifiers and event.modifiers == event.modifiers.Shift | event.modifiers.Control ) or
#todo: am i supposed to be adding the namespace like this? </s> data_node.append(etree.element(ns+tag))	_ucla_form_modifier ns = "{%s}" % data_node.nsmap[None] tag = hidden_value_path.replace("/data/", "") ns = "{%s}" % xform_root.nsmap[None] itext_node = xform_root[0][1].find(ns+"itext")
# todo make the colors randomly generated from rgb values </s> color_iterator = itertools.cycle(['b', 'g', 'r', 'c', 'm', 'y', 'k'])	pr_plot_from_thresholds save (bool): False to display the image (default) or True to save it (but not display it) debug (bool): verbost output. plt.figure() plt.xlabel('Recall')
# todo check error message here </s> assert response.status_code == 400	test_nonexistent_relationship response = self.app.post('/api/person', data=dumps(data))
for obj in bpy.data.objects: #todo: this is not the best list to iterate over (there might be multiple scenes) </s> if not obj.parent and obj.marstype == "body":	getRoots def getRoots(): roots = [] roots.append(obj) if roots == []:
for c in spinner():  # todo change to yield from, when dropping python 2.7 </s> yield c	inner_play def inner_play(): while True:
# todo: fix up internal access (again)! </s> date(self._years.value, self._months.value, self._days.value)	_check_date def _check_date(self): try: self._days._is_valid = True except (TypeError, ValueError):
# todo: remove "get_" from the name </s> return the type of the node	get_node_type def get_node_type(self, node, index=False): Args: node: The node ID from the original graph
# todo: do we need to skip config.add_slack variable here? </s> var_filter = (lambda v: v[1].is_integer()) if discrete_only \	generate_norm2sq_objective_function discrete_only: Bool only optimize on distance between the discrete variables else (lambda v: v[1].name != 'MindtPy_utils.objective_value' and 'MindtPy_utils.MindtPy_feas.slack_var' not in v[1].name)
# todo implement </s> pass	get_activations def get_activations(model, inputs):
# todo assert responses, swipe down </s> tx = self.client.nem_sign_tx(self.client.expand_path("m/44'/1'/0'/0'/0'"), {	test_nem_signtx_mosaic_creation_properties self.setup_mnemonic_nopin_nopassphrase() with self.client: "timeStamp": 74649215, "fee": 2000000,
# todo: заменить на простой json # ld load_resources </s> import sublime	get_css_dict import os try: css_dict = sublime.load_settings(CSS_DICT_FILENAME).get(DICT_KEY) if css_dict is None:
# todo: make an ascii-art bar </s> ctx.fillslots("progress_hash", "%.1f%%" % (100.0 * chk))	render_row_upload self._render_common(ctx, data) (chk, ciphertext, encandpush) = data.get_progress() ctx.fillSlots("progress_ciphertext", "%.1f%%" % (100.0 * ciphertext)) ctx.fillSlots("progress_encode", "%.1f%%" % (100.0 * encandpush))
# todo extend to nonbinary nodes </s> if i not in self._input_indices and i in self.context.node_indices:	__init__ else: self._dimension_labels.append(None) past_tpm_on = past_tpm_on.sum(i, keepdims=True) / 2 past_tpm_off = past_tpm_off.sum(i, keepdims=True) / 2
# todo: this test should fail for now but should succeed once </s> x, y = data	test_clone_results_in_uninitialized_net def test_clone_results_in_uninitialized_net( self, net_fit, data): accuracy = accuracy_score(net_fit.predict(X), y) assert accuracy > ACCURACY_EXPECTED  # make sure net has learned
# todo: use pybossa uploader! only for debugging: </s> open('/tmp/%d_%s_task_run_json.zip' % (app.id, name), 'wb').write(memzip.getvalue())	export_json memfile.write(str(line)) memzip = make_onefile_memzip(memfile, '%s_task_run.json' % name)
# todo should this even include public_ip if it's always none? </s> assert logged["public_ip"] is none	test_add_event_is_logged assert logged["dst_port"] == destination_port assert logged["local"] == (destination_ip, destination_port)
## todo: raise an exception? </s> pass	_communicate if isinstance(resp, ServerMessage): if not resp.header.get('success'): return resp.data elif isinstance(resp, AckMessage):
# xxx todo: rounding </s> e = []	scvtf def scvtf(ir, instr, arg1, arg2): src = ExprOp('sint_to_fp', arg2) if arg1.size != src.size:
# todo: reflection padding </s> conv = tf.nn.conv2d(input, weights,	c7s1_k biases = tf.get_variable("biases", [k], initializer=tf.constant_initializer(0.0)) strides=[1, 1, 1, 1], padding='SAME') bn = batch_norm(conv+biases, is_training)
# todo: see https://github.com/mozilla/openwpm/issues/867 for when </s> prefs = configure_firefox.load_existing_prefs(browser_profile_path)	deploy_firefox % (browser_params.browser_id, ext_config_file) ) prefs.update(configure_firefox.DEFAULT_GECKODRIVER_PREFS) s = socket.socket()
# todo: arrange </s> profiles = self.remote.get_profiles(self.token)	test_create_subprofile def test_create_subprofile(self): Test: create/edit a subprofile object subprofile = self.remote.new_subprofile(self.token) self.assertTrue(self.remote.modify_profile(subprofile, "name", "testsubprofile0", self.token))
# todo: backend tensorflow </s> if verbose > 0:	save If `protocol` is "pickle", save using the Python pickle module. Only the protocol "backend" supports ``restore()``. print( "Epoch {}: saving model to {}-{} ...\n".format(
##? value()  --- todo fix support for tuple assignment </s> value	mapping (key, value) = item key for key, value in p.items(): key
# todo i think this is a hack. it should be an </s> c = er.wrap(self._evaluator, c.parent.parent).name.names[-1]	completions if not filter_private_variable(s, user_stmt or self._parser.user_scope(), n): if isinstance(c.parent.parent, (pr.Function, pr.Class)): new = classes.Completion(self._evaluator, c, needs_dot, len(like), s) k = (new.name, new.complete)  # key
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_transaction_wrong_type def test_fail_transaction_wrong_type(self): ``transaction`` is not a TrytesCompatible value.
#todo: the static files should handle collection of their static folder on their own </s> self._static = none	rebuild self.listener.pause() try: self.build() except Exception, e:
# todo: implement fixture after moto is ready </s> assert true	test_emr def test_emr(emr):
# todo: after https://github.com/multiagentlearning/playground/pull/40 </s> agents = [	main agent_env_vars = args.agent_env_vars game_state_file = args.game_state_file helpers._make_agent_from_string(agent_string, agent_id+1000) for agent_id, agent_string in enumerate(args.agents.split(','))
# todo: save state right here </s> [returnfromkernel(kernel_name=new_kernel_name)])	inner_mapper [CallKernel(kernel_name=new_kernel_name)] + current_chunk + current_chunk = [] else:
# todo: fix </s> 'is_paginated': t.tile_config.context_processor_class.tile_type == tiletype.paginate,	KODomainDashboardView 'url': t.tile_config.get_url(self.request), 'help_text': t.tile_config.help_text, 'paginated_items': t.context_processor.paginated_items if t.tile_config.context_processor_class.tile_type == TileType.PAGINATE else [], 'total': t.context_processor.total if t.tile_config.context_processor_class.tile_type == TileType.PAGINATE else 0,
else: self.assertequal(end, 1) # todo: simple exec should not wait_testpid!! </s> top = _recent(output(_top_list))	test_3902_start_false_exec_notify logg.info(" %s =>%s\n%s\n%s", cmd, end, out, i2(err)) if real: self.assertEqual(end, 0) logg.info("\n>>>\n%s", top) self.assertFalse(greps(top, testsleep))
#   todo:   2012-11-07 14:05:42 by brian mcfee <brm2132@columbia.edu> </s> result = scipy.signal.fftconvolve(x, x[::-1], mode='full')	autocorrelate def autocorrelate(x, max_size): result = result[len(result)/2:] if max_size is None:
assert study_id == 0  # todo </s> return copy.deepcopy(self.trials)	get_all_trials def get_all_trials(self, study_id):
#todo - files </s> curl = 'curl -l '	curl_from_request def curl_from_request(request): auth = '' if request.auth is not None:
# todo: lamp texture test.. </s> if wrd.generate_lamp_texture != '':	build_node_tree if wrd.diffuse_model == 'Oren Nayar': wrd.world_defs += '_OrenNayar' bpy.data.worlds['Arm'].world_defs += '_LampColTex' voxelgi = False
# todo: could use -xlinker here, if it's supported </s> assert "," not in dir	BaseIntelFCompiler f + '.f', '-o', f + '.o'] def runtime_library_dir_option(self, dir): return '-Wl,-rpath=%s' % dir
#todo: check cost line </s> return {}	isGroundBuildRequirementSatisfied @classmethod def isGroundBuildRequirementSatisfied(cls, x, y, island, **state):
# todo: implement this </s> pass	__init__ self.set_books_func = dummy elif self.name[0] == '#' and self.name.endswith('_index'): elif field.is_many: pass
# todo(py3.7): add required=true </s> return bytes.fromhex(arg)	hex_bytes def hex_bytes(arg):
# time.sleep(40)  # todo: should remove after polling get. </s> mpc_1_2_3 = op(mpc_1_2, tensor_pointer_3)	test_tensor_abstraction_pointer mpc_1_2 = op(tensor_pointer_1, tensor_pointer_2) mpc_1_2.block_with_timeout(secs=40) mpc_1_2_3.block_with_timeout(secs=40) exp_res = op(data_1, data_2)
# todo: align series </s> return geoseries([getattr(s[0], op)(s[1]) for s in zip(self, other)],	_geo_op def _geo_op(self, other, op): if isinstance(other, GeoSeries): index=self.index) else:
# now we can kill it. todo: on a slow machine, the node might kill </s> def _stop(res):	test_introducer d.addCallback(_started) d.addCallback(lambda res: self.poll(_node_has_started)) open(HOTLINE_FILE, "w").write("") self.failUnless(os.path.exists(TWISTD_PID_FILE))
# todo: remove in conan 2.0 </s> @pytest.mark.tool_cmake	test_cmake_find_package def test_cmake_find_package(client_weird_lib_name): c = client_weird_lib_name
# todo: write a unit test </s> return reversed(path)	get_path return None
# todo watch out because urllib.unquote will blow up on unicode text </s> text = urllib.unquote(param[1])	accept_message for param in params: if param[0] == "text": elif param[0] == "from": sender = param[1]
# @todo: deployment_setting </s> if current.auth.s3_has_role("editor"):	req_customize_req_fields ), ] filter_widgets.insert(-1, S3OptionsFilter("created_by", label=T("Logged By"),
# todo: progress +kwargs </s> rm.push(refspec=refspec, progress=progress)	push else:
# todo delete? we should search for valid parser </s> needs_dot = not dot and path	completions if not path and not isinstance(user_stmt, pr.Import): pass comps = [] comp_dct = {}
return # todo raise error </s> self.backend.playback.stop().get()	Stop if not self.get_CanControl(): logger.debug(u'%s.Stop not allowed', PLAYER_IFACE)
# todo - actually figure out types </s> return [{	infer_schema sheet = book.sheet_by_index(0) headers = sheet.row_values(0) 'column': h, 'type': 'unicode'
# @todo: deprecate </s> filterfor = hook.filterfor	_attach component.filter = None else: is_list = isinstance(filterfor, (tuple, list)) if is_list and len(filterfor) == 1:
# todo: move this into generic agentrootcomponent. </s> return step_op, (stage_op if stage_op else step_op), loss, loss_per_item, records	update_from_memory step_op = root._graph_fn_training_step(step_op)
# todo: update the tolerance after the ci moves to torch 1.10 </s> self.asserttrue(torch.allclose(outputs.logits[:, :4], expected_logits, atol=1e-2))	test_inference_diarization self.assertEqual(labels[0, :, 0].sum(), 270) self.assertEqual(labels[0, :, 1].sum(), 647)
#todo: register authorities via annotations? </s> @staticmethod	getIdP def getIdP(choice): logging.info("IdPFactory: trying dynamic loading of module : {0} ".format(choice))
self.current_width = self.current_width * 2 #todo </s> self.current_height = self.current_height * 2 #todo	layer_resize_conv layers.append(nn.ReLU())#TODO self.current_channels = channels return nn.Sequential(*layers)
# todo integrate param when g is a clustered graph </s> if true:	plot_graph def plot_graph(G): ki, kj = np.nonzero(G.A) if G.directed:
# todo: can just leave this in superclass </s> return timeout	get_default_timeout def get_default_timeout(self):
# todo: https://github.com/pycqa/pylint/issues/3139 </s> num_words = inputs.shape[0]	testViterbiDecode dtype=np.float32) sequence_lengths = np.array(3, dtype=np.int32) num_tags = inputs.shape[1] all_sequence_scores = []
# todo allow client to pass in constructor arguments/options </s> assert _manager is not none	get_distributor @return: distributor associated with the distribution type @raises PluginNotFoundError: if not importer is associated with the distribution type Distributor = _manager.lookup_distributor_class(distribution_type) if Distributor is None:
# todo nix this when fastparquet resolves </s> raise fastparquetissue361	read_header return fastparquet.ParquetFile(path) except IndexError:
# todo: why does a dataset have this? </s> x = x[:, 0]	free_energy def free_energy(self, X): y = X[:, 1] rval = T.sqr(y - T.cos(x)) / (2. * (self.std ** 2.))
# xxx todo register a failure handler that reverses the local state </s> return "ok"	archive q = action.get_queue() q.enqueue(action.get_archive_fn(account), thread_id)
#todo (dwalleck): this is a horrible stopgap. </s> temp = mgmt_url.rsplit('/')	keystone_auth return token, mgmt_url else: service_url = temp[0] + '//' + temp[2] + '/' + temp[3] + '/' management_url = service_url + tenant_id
# todo- re-implement this to make an iterator instead of returning a </s> import networkx	depth_first_search sage: D.depth_first_search(0) [0, 1, 2, 3, 4] return networkx.dfs(self._nxg, u)
# todo find better documentation </s> ql.dprint("[!] sample is checking user language!")	hook_GetUserDefaultUILanguage }) def hook_GetUserDefaultUILanguage(ql, address, params): lang = 0x445 return lang
pass    # todo </s> def test_spring_forward(self):	test_spring_forward
# todo: don't write them? is *much* slower on re-load (~3x) </s> try:	update_module fh.flush() os.fsync(fh.fileno())  # flush to disk os.unlink(self.module_path + 'c') except OSError:
# todo ... </s> assert isinstance(v.body, cstatement)	test_parse_c_cast_ptr v = state.vars["v"]
pass # todo </s> def startcdata(self):	startCDATA
#todo: hardcoded to sigma as the model </s> if getattr(self, '_mesigma', none) is none:	MeSigma @property def MeSigma(self): sigma = self.currentTransformedModel self._MeSigma = self.mesh.getEdgeInnerProduct(sigma)
final_arr = []  # todo: uncomment it </s> ftp.cwd(current)	find_files final_arr[j] = str(final_arr[j]) + '/' + item else: else: if self.__folder_exist(ftp, item):
# todo todo </s> if uri.type != "s3":	object_put def object_put(self, filename, uri, extra_headers = None): raise ValueError("Expected URI type 's3', got '%s'" % uri.type) if not os.path.isfile(filename):
# todo: retry instead? </s> pass	events_handle_logs_build_job xp_logger.handlers = [] except OSError:
# todo: avoid explicit reference to cupy </s> return "cupy" in str(type(x))	is_cupy_type def is_cupy_type(x):
# todo: assert </s> repo = self.remote.get_repo("testrepo0")	test_get_repo Test: Get a repo object
# todo: this can be optimized to one commit/write. </s> alice_pubkey_sig = self.add_key(alice_pubkey_sig)	add_policy_contract Creates a PolicyContract to the Keystore. :return: The newly added PolicyContract object alice_pubkey_enc = self.add_key(alice_pubkey_enc) bob_pubkey_sig = self.add_key(bob_pubkey_sig)
# todo: this doesn't return results as often as it should, meaning we end up marking things are more suspicious than they actually are. </s> params = {	channel_search def channel_search(self, lat, lon, gps_offset): "latitude1": lat + gps_offset, "latitude2": lat - gps_offset,
# todo: add option for attentive reader </s> print('trainable variables (only embeddings): %d' % get_total_trainable_variables())	conditional_reader_model_with_cands varscope.reuse_variables() candidates_embedded = nvocab(candidates) outputs, states = conditional_reader(support_embedded, support_lengths, question_embedded, question_lengths,
# todo: ugly n^2 </s> c.sons = [	build_tree def build_tree(comment_list): for c in comment_list: i for i in comment_list \ if i.path.startswith(c.path) and i.level == c.level+1
# todo: must be implemented </s> pass	range_from_chapters def range_from_chapters(crawler, times=0):
# todo(aivanou): t83447589 come up with the proper fix </s> res = self.run_job(node_configs)	test_run_distributed_sum_heterogenous Conf(role="sum", entrypoint=_dist_sum, local_world_size=3), ] self.assertEqual(3, len(res["sum"])) ranks = set()
# todo: skipping time.first() necessary? </s> if t != t0:	test_solve_square_dynamic block[t0].deactivate() for t in time: input_vars = [m.h[t], m.dhdt[t]] external_vars = [m.flow_out[t]]
#todo: fix the null space </s> xc = solver(a).solve(rhs)	Test1D_InhomogeneousDirichlet err = np.linalg.norm((xc-xc_anal), np.inf) elif self.myTest == 'xcJ': print np.linalg.norm(Utils.mkvc(A*xc) - rhs) j = McI*(G*xc + P*phi_bc)
# todo: do the computation without the 'sr' enforcement </s> gm = matrix( [[ gg[i, j, chart].expr(method='sr')	determinant i1 = manif.start_index() for chart in gg[[i1, i1]]._express: for j in manif.irange()] for i in manif.irange()] ) detgm = chart.simplify(gm.det(), method='SR')
# todo: progress +kwargs </s> else:	push GIT_SSH_COMMAND="ssh -S %s" % cnct.ctrl_path): rm.push(refspec=refspec, progress=progress) rm.push(refspec=refspec, progress=progress)
# todo, dot over only 1 dimension </s> b1_all = iam * np.dot(-grad_out_comp(l, m, r1, theta1, phi1), snaug)	sss_basis c_vec = Ipm - np.ones((Np, 1)) * h0_ext  # XXX h0_int in Jussi's code (mistake?) r1, theta1, phi1 = cart_to_sph(c_vec[:, 0], c_vec[:, 1], c_vec[:, 2]) b1 = np.sum(b1_all, 1)  # XXX Check that this sum is correct with Jussi's code B[:, l ** 2 + l + m] = b1
# todo: dynamically add/remove adapters </s> self._ports.clear()	_updateCallback if nb_adapters_changed: log.debug("number of adapters has changed to {}".format(self._settings["adapters"])) self._addAdapters(self._settings["adapters"]) if updated:
# todo implement test for windows. </s> if not word_count_command:	testSimpleFilterThroughShell def testSimpleFilterThroughShell(self): word_count_command = self.__class__.getWordCountCommand() return True set_text(self.view, 'One two three four\nfive six seven eight\nnine ten.')
# todo: clean up </s> for _ in range(num_hosts)	gossipsubs )
# todo: kwargs </s> def _impl(df, axis=none, skipna=none, level=none, numeric_only=none):	median_overload @overload_method(DataFrameType, 'median') def median_overload(df, axis=None, skipna=None, level=None, numeric_only=None): return hpat.hiframes.pd_dataframe_ext.median_dummy(df) return _impl
# todo: remove in 1.2 </s> def f1_score(	f1_score pred: torch.Tensor, target: torch.Tensor,
# todo: voltage dependency </s> vl = _is_elements["load_3ph"] * l_3ph["scaling"].values.t / np.float64(1000.)	_calc_pq_elements_and_add_on_ppc l_3ph = net["load_3ph"] if len(l_3ph) > 0: q = np.hstack([q, (l_3ph["q_kvar_A"].values + l_3ph["q_kvar_B"].values + l_3ph["q_kvar_C"].values) * vl]) p = np.hstack([p, (l_3ph["p_kw_A"].values + l_3ph["p_kw_B"].values + l_3ph["p_kw_C"].values) * vl])
# todo: better naming </s> return [tf.pad(input_dict[node.inputs[0]], padding, mode, none, value)]	handle_pad .astype(np.int32)) # tf requires int32 paddings
# todo(phawkins): remove when minimum jaxlib version is 0.1.48 or newer. </s> if lib.version <= (0, 1, 47) and dtype == onp.complex128:	_promote_to_complex def _promote_to_complex(arg): dtype = dtypes.result_type(arg, onp.complex64) dtype = onp.complex64 return lax.convert_element_type(arg, dtype)
# todo  batch size is changed </s> label_field = self.config['label_field']	_neg_sampling user_num_in_one_batch = self.batch_size // self.neg_sample_to self.batch_size = (user_num_in_one_batch + 1) * self.neg_sample_to self.dataset.field2type[label_field] = 'float' self.dataset.field2source[label_field] = 'inter'
# todo: port status changes should cause us to withdraw a route. </s> new_dp_bgp_speakers = {}	reset def reset(self, valves): if valves: for valve in valves.values():
# todo generator </s> lgr.debug("installed %s to fulfill request for content for "	__call__ containing_ds = install_necessary_subdatasets(ds, path, reckless) if containing_ds.path != ds.path: "path %s", containing_ds, path) if containing_ds.path == path:
# todo: this regex could change based on project req format </s> attributes[col_headers[j]] = re.findall('[a-za-z0-9]+', cell.value)	import_xlsx if j != id_col and cell.value is not None: if 'links' == col_headers[j]: else: attributes[col_headers[j]] = cell.value
# todo(lipu): fix this to show front page pictures </s> return []	getThumbnailTorrents def getThumbnailTorrents(self, limit=20): result = [] for t in self.metadata_db.getThumbnailTorrents(TUMBNAILTORRENT_REQ_COLUMNS, limit=limit):
# todo(parallel): to support nn.dataparallel, this must be changed, as it not a tensor </s> return encout(x_soft, x_hard, symbols_hard, self.l, f)	EDSRLikeEnc x = self.to_q(x) x_soft, x_hard, symbols_hard = self.q(x)
# todo: add this test </s> self.asserttrue( true )	test_no_code_disclosure_xml def test_no_code_disclosure_xml(self):
# todo: remove this log once we find out what's causing oom </s> log.info('running readthedocs.builds.tasks.sync_versions_task. locals=%s', locals())	sync_versions_task :returns: `True` or `False` if the task succeeded. project = Project.objects.get(pk=project_pk) current_stable = project.get_original_stable_version() if current_stable is not None:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: parse the field contents </s> self.index_fields.append(idx)	parse_index def parse_index(self, field, parse_func, log): idx = parse_func(field.instructions, log)
# todo: write tests </s> if relevant, add a slur to an object. both "thing" and "slurbundle" must not be ``none``, but	_addSlurToThing def _addSlurToThing(m21Start, m21End, attr, thing, slurBundle): the first three arguments may be ``None``. To trigger a slur, either "m21Start", "m21End", or "attr" must be present.
# todo: rename this to event_data.parser_chain or equivalent. </s> event_data.parser = self.getparserchain()	ProduceEventWithEventData if event.timestamp < self._INT64_MIN or event.timestamp > self._INT64_MAX: raise errors.InvalidEvent('Event timestamp value out of bounds.') try: event_data_hash = event_data.GetAttributeValuesHash()
# todo: don't create dsdp directory. add these files </s> shutil.move("col_row.nl","./dsdp/")	get_dsdp m.write("col_row.nl", io_options={"symbolic_solver_labels": True}) try: shutil.move("col_row.col","./dsdp/") shutil.move("col_row.row","./dsdp/")
# todo have no idea if is cdecl or stdcall </s> _queryinformationprocess(self, address, params)	hook_ZwQueryInformationProcess }) def hook_ZwQueryInformationProcess(self, address, params):
# todo also handle method and request </s> return self.client.post(hookurl, json.dumps({	callPreHook else: newRequestBody = result["Body"] "Type": "pre-hook", "Method": request.method,
raise notimplementederror  # todo(mattjj) </s> else:	remove_axis_from_aval return ShapedArray(aval.shape[1:], aval.dtype)
# todo: actually parse this </s> self.data = data	_DynamicRule def __init__(self, i, data): self.i = i def _data(self): return self.data
# todo: refactor common tests for all models, e.g. shape checking </s> scores = model.forward_owa(triples)	test_rescal batch_size = 16 triples = torch.zeros(batch_size, 3, dtype=torch.long) assert scores.shape == (batch_size, 1) scores = model.forward_cwa(triples[:, :2])
# todo: write </s> if len(res.keys()) != 1:	first_key def first_key(command, res): raise RedisClusterException("More then 1 result from command: {0}".format(command)) return list(res.values())[0]
# todo confirm we want floor division here </s> rowlen = self.size() // collen	reshape def reshape(self, rowlen, collen): if rowlen == -1: if collen == -1: collen = self.size() // rowlen
# todo(john-wood-w) allow until plugin validation is added. </s> self.unsupported_req = {	test_should_allow_add_new_order_unsupported_algorithm def test_should_allow_add_new_order_unsupported_algorithm(self): 'secret': { 'name': self.secret_name,
# todo: avoid dummy and generate func here when inlining is possible </s> def _impl(df, axis=none, skipna=none, level=none, numeric_only=none):	median_overload @overload_method(DataFrameType, 'median') def median_overload(df, axis=None, skipna=None, level=None, numeric_only=None): return hpat.hiframes.pd_dataframe_ext.median_dummy(df) return _impl
# todo: arrange </s> repo = self.remote.new_repo(self.token)	test_create_repo def test_create_repo(self): Test: create/edit a repo object self.assertTrue(self.remote.modify_repo(repo, "name", "testrepo0", self.token)) self.assertTrue(self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token))
# todo: plumb gravitylayout.__init__'s arguments into the config file </s> gv = gravitylayout()	_make_positions def _make_positions(): @todo: Figure out how best to put this in the config file. col_width = 1.0 / COLUMN_COUNT cycle_steps = tuple(col_width * x for x in range(1, COLUMN_COUNT))
#     todo </s> def release(self, path, fh):	Filesystem def readdir(self, path, fh): return ['.', '..'] + cache.list_effective_nodes(path) return os.close(fh)
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_interact_arguments @classmethod def add_interact_arguments(cls, parser): p_convert = p_operation.add_parser( "convert", help="convert VGM to PCM using OPL hardware")
# todo(brett.cannon) implement </s> raise importerror	_default_hook def _default_hook(self, path): If the path will not work for the default hook then raise ImportError.
pass # todo </s> def try_undo(self):	try_undo
# todo(buggay): implement num_vms_per_host functionality </s> self.num_vms_per_host = vm_spec.num_vms_per_host	AwsVirtualMachine self.firewall = aws_network.AwsFirewall.GetFirewall() self.use_dedicated_host = vm_spec.use_dedicated_host if self.num_vms_per_host: raise NotImplementedError('Num vms per host for AWS is not supported.')
# todo: why so inaccurate? </s> np.testing.assert_allclose(ret.data, out.detach().numpy(), atol=1e-5)	test_conv2d out = torch.nn.functional.conv2d(x,w) ret = Tensor.conv2d(xt, wt) out.relu().mean().backward() ret.relu().mean().backward()
return none # todo </s> def _currentsong(self):	_currentsong
# todo: currently the output code directly accesses the format string </s> self.format_string = format_string	_FormatMessages if len(short_message_string) > 80: short_message_string = u'{0:s}...'.format(short_message_string[0:77]) return message_string, short_message_string
# todo: remove when fixed in upstream gtk+ </s> stylecontext = self.props.view.get_style_context()	query_chunks else: self._chunk = None background_set, background_rgba = ( stylecontext.lookup_color('theme_bg_color'))
g.configure_new(config) # todo: test for emitted warning </s> print(g)	test_fs_neat_hidden_old self.assertEqual(gid, g.key) print("\nThis should output a warning:", file=sys.stderr) self.assertEqual(set(iterkeys(g.nodes)), {0, 1, 2}) self.assertEqual(len(g.connections), 1)
# todo support multiple backends </s> return self.backends[0].stored_playlists.rename(	rename :param new_name: the new name :type new_name: string playlist, new_name).get()
# todo: revert this. </s> from astropy.tests.helper import quantity_allclose	hgs_to_hcc def hgs_to_hcc(heliogcoord, heliocframe): Convert from Heliographic Stonyhurst to Heliocentric Cartesian. hglon = heliogcoord.lon hglat = heliogcoord.lat
#@todo: integrate this function with the one below - the pipeline() method only works with this function </s> xs["targets"] = [1.0 if xs[ans_name][i] == cand else 0.0	jtr_map_to_targets def jtr_map_to_targets(xs, cands_name, ans_name): Create cand-length vector for each training instance with 1.0s for cands which are the correct answ and 0.0s for cands which are the wrong answ for i in range(len(xs[ans_name])) for cand in xs[cands_name][i]]
# todo refactor this function </s> project_slug = request.get.get('project', none)	footer_html @decorators.renderer_classes((JSONRenderer, JSONPRenderer)) def footer_html(request): version_slug = request.GET.get('version', None) page_slug = request.GET.get('page', None)
# todo: find a better image here </s> size = (14, (55 * consumed_resources) + 17)	refresh y = (27 * consumed_resources) centered_container.position = (0, y) image = Icon(image="content/gui/icons/templates/production/production_line.png") image.position=(59, 15)
''' todo: change conditional to return on non-http responses </s> to reduce branch depth'''	main iter = TextRecordParser(**textRecordParserOptions) for entry in iter(warc): if entry.record.rec_type != 'response' or \ entry.get('mime') in ('text/dns', 'text/whois'):
# todo: should this raise ioerror? </s> mid = read_file(header_one_track + """	test_meta_messages def test_meta_messages(): 4d 54 72 6b  # MTrk 00 00 00 0c  # Chunk size
# todo: checking that hour/minute/second are not </s> _assign_hms(res, value_repr, hms)	_parse (i, hms) = _parse_hms(i, l, info, hms_idx) if hms is not None: elif i + 2 < len_l and l[i + 1] == ':': res.hour = int(value)
# todo: give a vanilla example </s> .. math::	feca def feca(predicted_power, df_appliances_ground_truth): fraction = \\sum_n min \\left (
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# env.reset()  # todo: remove once traffic bug is fixed </s> try:	run session_done = False episode = 0 while not session_done: if episode_done:
#            print "monitor[0] in if self.todo.isddc is",  monitor[0] </s> if self.todo.x.state != self.todo.monitororiginalname:	MonitorWindow self.todo.monitorHsync) self.ctree.node_set_row_data (self.originalNode, (parent, monitor)) pass else:
# todo: require rewrite </s> if start_time > end_time:	mean def mean(self, start_time, end_time): time range from `start_time` to `end_time`. msg = ( "Can't calculate mean of a Timeseries "
# todo: unfortunately, this feature is not yet implemented for python </s> pass	test_tls_require_encryption def test_tls_require_encryption(self):
# todo: provide more informative errors </s> try:	create_policy Character control endpoint for creating a policy and making arrangements with Ursulas. bob_pubkey = bytes.fromhex(request.args['bob_encrypting_key']) label = bytes.fromhex(request.args['label'])
# todo: this is lazy, we should only reconfigure the drone(s) who are actually </s> if drone_edge:	_bait_user_changed drone_edge = db_session.query(DroneEdge).filter(DroneEdge.username == username, DroneEdge.password == password).first() self._reconfigure_all_clients()
# todo(harlowja): move this code into </s> tz_file = os.path.join(self.tz_zone_dir, str(tz))	set_timezone def set_timezone(self, tz): if not os.path.isfile(tz_file): raise RuntimeError(("Invalid timezone %s,"
rec_dict._proxy._handle.close() #todo - better solution </s> del rec_dict	simple_check rec_dict = SeqIO.index(filename, format, alphabet) self.check_dict_methods(rec_dict, id_list, id_list) if not sqlite3: return
# todo(cutwater): replace `.decode('utf-8')` call with subprocess </s> return subprocess.check_output(cmd, cwd=directory).decode('utf-8').strip()	get_current_branch non-zero exit code. cmd = ['git', 'rev-parse', '--abbrev-ref', 'HEAD']
# todo: logger always requires extras['cls'] : can we fix this? </s> logger.warning(msg, extra=dict(cls=none))	_build_mesh ", consistent with the attached connectivities." ) else: quoted_topology_dimension = mesh_var.topology_dimension
# todo(jeremydw): better headers. </s> headers = {	_upload bucket_key.key = path.lstrip('/') mimetype = mimetypes.guess_type(path)[0] 'Cache-Control': 'no-cache', 'Content-Type': mimetype,
# todo: send alert </s> blacklist_whitelist_notification.delay(4) # notice_type = 4 whitelist	chk_prefix_in_whitelist break if flag: return True return False
# todo: use validation set if not-none </s> model = self.new_model()	best_model_vector_classification def best_model_vector_classification(self, train, valid): model.fit(train.x, train.y) model.trained_on = train.name
end_date=none,  # todo: expire me? </s> )	make_draft_registraton draft.approval = DraftRegistrationApproval( initiated_by=user, draft.save() node.draft_registrations.append(draft)
# todo: check that the performance measure is within some range </s> figure_eight_baseline(num_runs=1, flow_params=figureeight1,	test_figure_eight figure_eight_baseline(num_runs=1, flow_params=figureeight0, render=False) render=False) figure_eight_baseline(num_runs=1, flow_params=figureeight2,
loop=asyncio.new_event_loop(),  # todo: this doesn't work without this </s> )	test_issue_631_sharing_event_loop token=self.bot_token, run_async=False,
# todo: with only non-mandatory model attributes, it is not possible to get an invalid form </s> self.assertequal(response.status_code, 200)	test_system_exporter_spreadsheet_xls_config_post_redirect response = self.client.post('/config/system/exporter/spreadsheet/xls/', data_dict)
# todo should this reset the pts and such? </s> self._updates.insert(0, error)	set_error Can be (and is) used to pass exceptions between threads. with self._updates_lock: self._updates_available.set()
# todo(tdurakov): remove dict to object conversion once rpc api version </s> got_migrate_data_object = isinstance(dest_check_data,	check_can_live_migrate_source is_volume_backed = compute_utils.is_volume_backed_instance(ctxt, instance) migrate_data_obj.LiveMigrateData) if not got_migrate_data_object:
# todo: maybe_decay_array </s> return value.maybestrarray(self.mem.getargv())	_ApplyPrefixOp pass if val.s in ('@', '*'): i = val.s.find('[') if i >= 0 and val.s[-1] == ']':
# todo - remove by nov 1 2017 if soft assert is never sent </s> _soft_assert_dict(false, "context is type %s" % str(type(context)))	B3MultiField context_dict = context.flatten() else: context_dict = context if not (self.field_class or self.label_class):
gray_img = color.rgb2gray(state)  # todo: check image conversion doesn't cause problems </s> downsized_img = transform.resize(gray_img, (84, 84), mode='constant')  # todo: check resizing doesn't cause problems	_state_to_tensor def _state_to_tensor(state): return torch.from_numpy(downsized_img).float()  # Return 2D image tensor
# todo: clean up this event print out. we probably want something </s> if suffix == 'new':  # skip "new" events	run if not self.opts.get('quiet', False): for suffix, ret in self.get_async_returns(async_pub['tag']): continue elif suffix == 'ret':  # for "ret" just print out return
# todo: support grouping and stacking at the same time </s> if self.attributes['stack'].columns is not none:	get_group_label def get_group_label(self, group): item = group['stack'] elif self.attributes['group'].columns is not None:
#todo(qos): support the fields parameter </s> return rule_object.qosbandwidthlimitrule.get_by_id(context,	get_policy_bandwidth_limit_rule def get_policy_bandwidth_limit_rule(self, context, rule_id, policy_id, fields=None): rule_id).to_dict()
# todo(b/195364460): work around slow xla/cpu implementation of float16 matmul </s> if ctx.platform == "cpu":	_dot_general_lower aval_out, = avals_out (lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch) = dimension_numbers if lhs_aval.dtype == np.float16: f32 = mlir.dtype_to_ir_type(np.dtype(np.float32))
pass  # todo: implement </s> def predict_time(self):	predict_time
# todo: send message with login link. </s> time_left = 60	cool_your_jets def cool_your_jets(err): Sent by Flask Limiter. message = ('Cool your jets! Please wait {0} seconds before making' ' another search.').format(time_left)
# no todo item selected </s> pass	_complete_selected_item text_type(self.view.todolist.number(todo)))) except AttributeError:
# todo: this should take a vector </s> return self._mesigma	MeSigma self._MeSigma = self.mesh.getEdgeInnerProduct(self.curModel.sigma)
# todo: check rackspace file existence </s> return true	zip_existing else:
# todo: make sure we have a test case for the above point. </s> value = _get_value(value, part)	get if value is _NOT_FOUND: break if value is _NOT_FOUND: return default
# todo: add multiple $root support </s> if pkg.root != self.target_root:	_get_arg_for_pkg specific match (PackageArg type is the most specific). This will raise an InvalidDependString exception if PROVIDE is invalid. return None arg_atom = self._set_atoms.findAtomForPackage(pkg.cpv, pkg.metadata)
# todo(b/138845899): consider use span instead of id. </s> latest_model = max(previous_models, key=lambda artifact: artifact.id)	_fetch_latest_model 'ModelExportPath') if previous_models: return latest_model.uri return None
# todo: make sure package names can't be changed to look like package ids? </s> if pkg == none:	show if pkg == None: pkg = model.Package.by_name(id) response.status_int = 404 return ''
# todo - needs tests </s> return {	payments_settings def payments_settings(request): "STRIPE_PUBLIC_KEY": settings.STRIPE_PUBLIC_KEY, "PLAN_CHOICES": settings.PLAN_CHOICES,  # possibly nuke
# todo: log non-bool return values? </s> return bool(self._mixer.set_mute(bool(mute)).get())	set_mute return False with _mixer_error_handling(self._mixer): return None
# todo(stevemar): assert returned fields </s> self.openstack('container show ' + self.container_name)	test_container_show def test_container_show(self):
#todo: actually check for change </s> self._smudge('__setslice__', k, v)	__setslice__ def __setslice__ (self, k, v): list.__setslice__(self, k, v)
# todo (cyyber): move to state cache, instead of writing directly </s> for protobuf_txn in block.transactions:	update_mainstate_tx_metadata def update_mainstate_tx_metadata(self, block, batch): fee_reward = 0 txn = Transaction.from_pbdata(protobuf_txn) fee_reward += txn.fee
# todo: this is untested. </s> _raise_current_error()	_raise_ssl_error raise SysCallError(-1, "Unexpected EOF") else: elif error == _lib.SSL_ERROR_NONE: pass
# todo check the trigger_id content </s> template_name = 'delete_service.html'	delete_service def delete_service(request, trigger_id): delete a service service = get_object_or_404(TriggerService, pk=trigger_id) form = TriggerServiceForm(instance=service)
raise exception #todo </s> self.key = key[0] #todo	__init__ for k in key: if k not in columns:
# todo: catch unquoting errors, range of chars, charset </s> unq_v = urllib.unquote(esc_v)	_parse_params ) continue dec_v = unq_v.decode(enc) # ok, because we limit enc above param_dict[k_norm] = dec_v
# todo add tests </s> coords=np.concatenate([self.coords, other.coords], axis=0))	concatenate return self.deepcopy(
#  todo: test </s> ship.getmodifieditemattr("shipbonusics2"),	handler "maxGroupOnline",
# todo: return errors in a universal way </s> print("shivyc: error: cannot read file '{}'"	main c_source = c_file.read() except IOError: .format(arguments.file_name)) c_file.close()
# todo: then we can pull the descriptor out of the tile_spec </s> su_descriptor = index_netcdfs([filename[7:]])[filename[7:]]	create_storage_unit except OSError: pass return StorageUnit([dataset.id for dataset in datasets], mapping,
"""todo: doesn't remove unused nodes/renumber elements""" </s> x = self.xyz[:, 0]	slice_xyz def slice_xyz(self, xslice, yslice, zslice): y = self.xyz[:, 1] z = self.xyz[:, 2]
# todo (#567): bucket the node as suspicious </s> return response(message, status=httpstatus.bad_request)	reencrypt message = f'{bob_identity_message} Invalid EncryptedKeyFrag: {e}.' log.info(message) try: verified_kfrag = authorized_kfrag.verify(hrac=hrac,
# todo: find a value for % width </s> raise typeerror('width %s is unknown' % box.style['width'])	block_preferred_minimum_width return box.margin_width() else:
# todo: implement this </s> def _parse_raw_args(parser, args):	_parse_raw_args (dest, switch, args)""" pass
# todo fix. </s> self.assertequals(reil_ctx_out["rax"] & 0xffffffff, res)	test_pmovmskb res = 0x29 # 00101001b x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init)
# todo: delete _check_on_hpc_hooks in v1.8 </s> _check_on_hpc_hooks(model)	verify_loop_configurations _check_dl_idx_in_on_train_batch_hooks(trainer, model) _check_on_init_start_end(trainer)
# todo: support dynamic conversion </s> if on is not none:	get_rolling_setup_args if on is None: raise ValueError("'on' argument to rolling() should be constant string") window = guard(find_const, func_ir, window) if not isinstance(window, str):
# todo(b/172668718) enable tests after b/172668718 is resolved. </s> "auto_tpu_strategy": self.auto_tpu_strategy(),	test_run_on_script def test_run_on_script(self): track_status = { "auto_multi_worker_strategy": self.auto_multi_worker_strategy(), "docker_config_parent_img": self.docker_config_parent_img(),
# todo: consider passing search_dirs in the constructor. </s> return self.read(path)	load_name path = locator.find_name(search_dirs, name)
#todo: maybe i should still update specials? </s> logger.log(u"not updating episodes for show "+self.show.name+" because it's marked as ended.", logger.debug)	run curQueueItem = sickbeard.showQueueScheduler.action.updateShow(curShow, True) else: curQueueItem = sickbeard.showQueueScheduler.action.refreshShow(curShow, True) piList.append(curQueueItem)
# todo: translate </s> context["title"] = "posts for year %s" % year	task_render_archive context["lang"] = lang context["items"] = [("[%s] %s" % (post.date, post.title(lang)), post.permalink(lang)) for post in post_list] yield generic_post_list_renderer( lang,
# todo: remove </s> g.q = q	read data_dict = {u'id': id, u'type': group_type} q = request.params.get(u'q', u'') extra_vars[u"q"] = q try:
# todo: handle escape (0x1b) </s> for d in data:	decode_in def decode_in(self, data): d = ord(d) if not self.in_parsing and d != Decoder.REQUEST_MAGIC:
).consume()  # todo see issue 170 </s> if instance.get("keyname"):	load_ec2_instances Region=region, aws_update_tag=aws_update_tag, key_name = instance["KeyName"] key_pair_arn = f'arn:aws:ec2:{region}:{current_aws_account_id}:key-pair/{key_name}'
exported.js_support = current.js_support  # todo: check articles have js_support </s> create_gallery_for_article(exported)	migrate_articles exported.image = current.image exported.description = current.description shutil.copytree(current.get_path(False), exported.get_repo_path(False)) exported.sha_draft = current.sha_draft
# @todo: is this always true? </s> sg.attrs["staggering"] = 0	write_to_gdf sg.attrs["field_units"] = "None" sg.attrs["field_to_cgs"] = 1.0 g = f.create_group("particle_types") sg = g.create_group(particle_type_name)
# todo this is also doing a mongo query for each owner </s> django_obj.user_id = modm_to_django[format_lookup_key(modm_obj.owner._id, model=osfuser)]	save_page_of_fk_relationships if isinstance(modm_obj, MODMNotificationSubscription): if isinstance(modm_obj.owner, MODMUser): django_obj.node_id = None elif isinstance(modm_obj.owner, MODMNode):
# todo: when timers are introduced, just make this wait </s> if not self._event.is_set():	custom_payload Otherwise it may throw if the response has not been received. :return: :ref:`custom_payload`. raise Exception("custom_payload cannot be retrieved before ResponseFuture is finalized") return self._custom_payload
# todo it might still leave unused database entries referring to the community id </s> database.execute(u"delete from community where id = ?", (community_database_id,))	create_community community.create_dispersy_authorize(permission_triplets, sign_with_master=True, forward=False) except: raise else:
# todo make this configurable </s> def get_prefix_color(prefix):	get_prefix_color if prefix == "&": return "lightgreen"
# todo: grab values from blender lamps </s> 'constantattenuation': 1.0,	export_light 'spot': { 'color': (light.color * light.energy)[:], 'fallOffAngle': 3.14159265, 'fallOffExponent': 0.0,
# todo: why do we have an __init__? we should be able to set up the class inside the </s> self.state = state	__init__ def __init__(self, state=None):
#todo: add support for root position v7 chords with missing fifth </s> raise resolutionexception("pitches do not form a correctly spelled dominant seventh chord.")	dominantSeventhToTonic c = chord.Chord(copy.deepcopy(pitches)) if not c.isDominantSeventh(): if not len(pitches) == 4: raise ResolutionException("Not a four part chord. Can't resolve.")
# todo: tighten this filter to match on the string of the error </s> s = customserializable()	test_forgotten_protobuf_type_flag raise NotImplementedError with pytest.raises(TypeError) as e: with pytest.raises(AttributeError) as e: s = CustomSerializable(as_wrapper=False)
# todo: should be able to pass in an optional function for </s> node_type = 'cdao:terminalnode' if clade.is_terminal() else 'cdao:ancestralnode'	process_clade (nUri(tu_uri), qUri('rdf:label'), clade.name), ] statements += [ (nUri(clade.uri), qUri('rdf:type'), qUri(node_type)),
# todo iterate with schematizer as to exact interface </s> self.schema_cache[table] = schemacacheentry(	_populate_schema_cache def _populate_schema_cache(self, table, resp): avro_obj=avro.schema.parse(resp['schema']), kafka_topic=resp['kafka_topic'],
# todo docstring </s> see :class:`lists3keys`.	_perform_upload_s3_key_recursively @sync_performer def _perform_upload_s3_key_recursively(self, dispatcher, intent): bucket = self.s3_buckets[intent.source_bucket] for f in intent.source_path.walk():
# todo assert cls.__tablename__ == '' </s> with dbcleanup(session, cls):	test_ToolTagAssociation def test_ToolTagAssociation(model, session, tag, user): cls = model.ToolTagAssociation user_tname, value, user_value, tool_id = 'a', 'b', 'c', 'd' obj = cls(user=user, tag_id=tag.id, user_tname=user_tname, value=value)
# todo: remove in django 1.10 </s> connections['slave'].cursor().close()	test_db_agnostic_disabled def test_db_agnostic_disabled(self): list(DbBinded.objects.cache()) with self.assertNumQueries(1, using='slave'): list(DbBinded.objects.cache().using('slave'))
# todo: abort cleanly here, as the resolution has been </s> assert (self._name is none or	dist self._dist = abstract_dist.get_pkg_resources_distribution() assert self._dist is not None self._name == canonicalize_name(self._dist.project_name)) assert (self._version is None or
# todo: add cn to domains? </s> domains = crypto_util.get_sans_from_csr(csr.data, openssl.crypto.filetype_asn1)	handle_csr webroot plugin can know about the calls to _process_domain csr = le_util.CSR(file=parsed_args.csr[0], data=parsed_args.csr[1], form="der") for d in domains: _process_domain(parsed_args, d)
# todo(john-wood-w) allow until plugin validation is added. </s> self.unsupported_req = {	test_should_allow_add_new_order_unsupported_algorithm def test_should_allow_add_new_order_unsupported_algorithm(self): 'secret': { 'name': self.secret_name,
# todo(bichen): move this color dict to configuration file </s> cls2clr = {	video_demo t_filter = time.time() times['filter']= t_filter - t_detect 'car': (255, 191, 0), 'cyclist': (0, 191, 255),
# todo: use different flag than .reentrant </s> pos = colorsorter._transform_point(pos)	schedule_sphere color, pos, radius, ColorSorter._debug_transforms() ##### if ColorSorter._parent_csdl and ColorSorter._parent_csdl.reentrant: if drawing_globals.use_c_renderer and ColorSorter.sorting: if len(color) == 3:
# todo: raise an exception </s> return	enable_extension return if extension_id not in self._extension_classes: ext_class = self._extension_classes[extension_id] ext_class.registration.enabled = True
# todo: this code need to be revised </s> for _table_elem in elems:	install_tool_data_tables for elem in tree.getroot(): if elem.tag == 'tables': elems.append(elem) else:
# todo: these two probably shouldn't reach back to main.. </s> def _save_playlist_cb(widget, name, page, context):	_save_playlist_cb main.exaile().playlists.save_playlist(page.playlist, overwrite=True)
# todo: cmake imported target shouldn't be namespaced (waiting https://github.com/conan-io/conan/issues/7615 to be implemented) </s> self.cpp_info.names["cmake_find_package"] = "mpark_variant"	package_info def package_info(self): self.cpp_info.names["cmake_find_package_multi"] = "mpark_variant"
# todo(dustin): we used to take non when the vcs was unknown. now we'll only </s> ver = none	get_versions ver = versions_from_keywords_f(vcs_keywords, tag_prefix) except AttributeError: if ver: if verbose: print("got version from expanded keyword %s" % ver)
# todo: update the tolerance after the ci moves to torch 1.10 </s> self.asserttrue(torch.allclose(outputs.logits[:, :4], expected_logits, atol=1e-2))	test_inference_diarization self.assertEqual(labels[0, :, 0].sum(), 555) self.assertEqual(labels[0, :, 1].sum(), 299)
# xxx todo </s> raise notimplementederror()	set_postmortem_debugger elif bits not in (32, 64): raise NotImplementedError("Unknown architecture (%r bits)" % bits)
commonname = "goagent xx-net - goagent" #todo: here should be goagent - xx-net </s> if sys.platform.startswith('win'):	import_ca @staticmethod def import_ca(certfile): CertUtil.import_windows_ca(commonname, certfile) elif sys.platform == 'darwin':
#todo use backup phone </s> generated_token = totp(self.get_token().seed)	render_next_step if self.steps.current in ['call-verify', 'sms-verify']: method = self.get_form_data('method', 'method') if method == 'call': phone = self.get_form_data('call', 'phone')
# todo(yamahata): ephemeraln where n > 0 </s> if num > 0:	_volume_size elif block_device.is_ephemeral(virtual_name): num = block_device.ephemeral_num(virtual_name) return 0 size = instance_type.get('local_gb')
# todo: make it possible to plot a plated variable using _subplots function. </s> if axes is none and fig is none:	contour y : array Grid points on y axis axes = plt.gca() else:
# todo(emfree): remove after status overhaul. </s> if account.sync_state != 'running':	create_account account.link = response.get('link') account.locale = response.get('locale') account.sync_state = None return account
# todo(ib-steffen): allow custom ca bundles </s> r = requests.get(url,headers=headers, timeout=120, verify=false, stream=true)	ArchiveDownload headers = {'Authorization': 'bearer ' + token} logger.info('get archive %s from %s', [filename, url]) f = r.raw if not f:
# todo raise exception </s> return {}	journals_display "logs": logs, }
# todo. create readme file in <output_dir> </s> step = arguments['--step']	main output_dir = Path(arguments['<output_dir>']) output_dir = output_dir.expanduser().resolve(strict=False) if step is not None: step = float(step)
# todo -- make sure more stringent and parse each kext in-memory so we only allow whitelist from .text </s> kmods = [(kmod.address.v(), kmod.address.v() + kmod.m('size'), kmod.name) for kmod in lsmod.mac_lsmod(obj_ref._config).calculate() if str(kmod.name) != "com.apple.kpi.unsupported"]	get_kernel_addrs_start_end start = obj.Object("unsigned long", offset = s, vm = obj_ref.addr_space) end   = obj.Object("unsigned long", offset = e, vm = obj_ref.addr_space) return (start, end, kmods)
# todo: remove this once sphinx is gone. </s> start = time.time()	search @mobile_template('search/{mobile/}results.html') def search(request, template=None): is_json = (request.GET.get('format') == 'json') callback = request.GET.get('callback', '').strip()
# todo: use zfs plugin </s> proc = await popen([	SystemDatasetService os.unlink(SYSDATASET_PATH) os.mkdir(SYSDATASET_PATH) 'zfs', 'get', '-H', '-o', 'value', 'aclmode', dataset, ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
recording_name = none  # todo </s> system_info = none #todo	_recording_update_legacy_from_v1_15_to_pprf_2_0 recording_software_name = None  # TODO recording_software_version = None  # TODO new_info_file = RecordingInfoFile.create_empty_file(rec_dir) new_info_file.recording_uuid = recording_uuid
# todo: support default period argument </s> shift_const = rhs.args[0]	_run_call_series return self._replace_func(func, [series_var, val]) if func_name in ('shift', 'pct_change'): func = series_replace_funcs[func_name] return self._replace_func(func, [series_var, shift_const])
# todo: wobble </s> position_au = einsum('ij...,j...->i...', r, position_au)	_to_altaz ' observe from a specific Earth location that' ' you specify using a Topos instance') r_au, alt, az = to_polar(position_au) if temperature_C is None:
# todo: optimize db call </s> dataset_instance = dataset_collection.collection.dataset_instances[ 0 ]	__summarize self.__append_dataset( content ) for dataset_collection in implicit_outputs: if not self.__check_state( dataset_instance ): continue
# todo test that these are the right attributes </s> new_metabolites.append(new_metabolite)	load_json_model for k, v in metabolite.iteritems(): setattr(new_metabolite, k, v) model.add_metabolites(new_metabolites) new_reactions = []
# todo implement. </s> return none	Mass return MassParameterServer(self.master_model, self.master_port) def allocate_worker(self):
# todo: this is untested. </s> _raise_current_error()	set_default_verify_paths set_result = _lib.SSL_CTX_set_default_verify_paths(self._context) if not set_result:
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
# todo check output </s> obj_name = os.path.basename(obj_file)	_test_obj self.assertTrue(len(listing) >= 1) self.openio('container show ' + self.CONTAINER_NAME) opts = self.get_opts(OBJ_HEADERS) output = self.openio('object create ' + self.CONTAINER_NAME +
# test for uwsgi -- todo save this somewhere so we only have to do it once. </s> is_uwsgi = false	app_factory if kwargs.get( 'middleware', True ): webapp = wrap_in_middleware( webapp, global_conf, **kwargs ) try: import uwsgi
# todo add read unlock </s> pass	discard_current_batch self.storage_reader.discard_batch() finally:
# todo fetch a real object </s> return loc_code	location_from_code def location_from_code(loc_code):
# todo compare file contents? </s> self.sim.targets.version += 1	test_refresh updater = self._new_updater() updater.refresh() self.sim.update_snapshot() updater = self._new_updater()
pass # todo </s> def _moveid(self, songid, to):	_moveid @register(r'^moveid (?P<songid>\S+) (?P<to>\d+)$')
# todo: create a log message this error? </s> result_dict['isopen'] = false	current_package_list_with_resources result_dict['isopen'] = isopen except KeyError: else: result_dict['isopen'] = False
# todo: make it more stringent? </s> from humanize import naturalsize	_annex_get_archive_by_key def _annex_get_archive_by_key(self, akey): akey_size = self.repo.get_size_from_key(akey) self.info(
# todo(danms) once libvirt has support for lxc hotplug, </s> domxml = virt_dom.xmldesc(libvirt.vir_domain_xml_secure)	attach_volume raise exception.DeviceIsBusy(device=mount_device) raise self._conn.defineXML(domxml)
# todo: what decorator should we put here? in scipy.fftpack there is no planning, </s> def test_ifft2_plan(self, xp, scp, dtype):	test_ifft2_plan @testing.for_complex_dtypes() x = testing.shaped_random(self.shape, xp, dtype) x_orig = x.copy()
# todo: refactor common tests for all models, e.g. shape checking </s> scores = model.forward_owa(triples)	test_simple batch_size = 16 triples = torch.zeros(batch_size, 3, dtype=torch.long) assert scores.shape == (batch_size, 1) scores = model.forward_cwa(triples[:, :2])
# todo: more arguments possible: objectdb etc. </s> except gitcommanderror as e:	clone lgr.debug("Git clone completed") break e_str = str(e) if re.search("Request for .*aborted.*Unable to find", str(e),
# todo: non-numeric columns should be ignored automatically </s> def test_impl(n):	test_prod1 def test_prod1(self): df = pd.DataFrame({'A': np.arange(n)+1.0, 'B': np.arange(n)+1}) return df.prod()
# todo: upsert is too much. insert is fine as all keys are deleted. </s> upsert_keys(self.session, key_table, key_map)	_after_apply key_table = LabelKey if table == Label else GoldLabelKey self.session.query(key_table).delete(synchronize_session="fetch")
# todo: make sure limits are deterministic then update this </s> assert not self.viewer.state.x_log	test_basic assert self.viewer.state.y_att_world is self.image1.id['World 0'] assert self.viewer.state.y_att is self.image1.pixel_component_ids[0] assert not self.viewer.state.y_log assert len(self.viewer.state.layers) == 1
# todo: check arp reply is valid </s> self.asserttrue(self.packet_outs_from_flows(arp_replies))	arp_for_controller 'arp_source_ip': '10.0.0.1', 'arp_target_ip': '10.0.0.254'})
# todo: more? </s> yield entry	gen_entries entry['y:title'] = entry.get('title') entry['y:id'] = entry.get('id')
return s #todo return partial result instead of giving up </s> last_line = current_block.pop(len(current_block) - 2)	bad_empty_lines_removed current_block = complete_blocks.pop() if len(current_block) < 2: assert not last_line, last_line new_finished, new_valid = code_finished_will_parse('\n'.join(current_block))
# wait until the chunks have added, todo change this to a qtbot.waitsignal </s> qtbot.wait(10 * long_loading_delay)	test_tiled_single_scale visual = viewer.window.qt_viewer.layer_to_visual[layer] assert isinstance(visual, VispyTiledImageLayer) screenshot = viewer.screenshot(canvas_only=True) center_coord = np.round(np.array(screenshot.shape[:2]) / 2).astype(np.int)
f = self.uri('#%s' % f[2:]) # todo: can we make formula identifiers urirefs? </s> if self.paths:	obrace def obrace(prod, tok): f = self.bnode('formula') self.paths[-1].append(f) elif self.mode and self.mode[-1] == 'list':
# todo: write </s> return dict.fromkeys(key_strings, callback)	string_keys_to_dict def string_keys_to_dict(key_strings, callback):
## todo: # fixme: remove me </s> try:	analyse except: resource_path = url_parsed['resource_path'] query_string = url_parsed['query_string'].encode() except:
# todo: remove in v1.5 </s> self.setup_training_type_plugin(plugin, model)	connect_training_type_plugin ' It will be removed in v1.5.')
# todo: get a buffer to test against </s> obj = receive_n_pdu_number_list_value.clone()	test_rcvnpdunumlist def test_rcvnpdunumlist():
# todo: also save as json </s> coll = self.document.entity(uuid.uuid4().urn,	declare_artefact return coll else: [ (provM.PROV_TYPE, WFPROV["Artifact"]), (provM.PROV_TYPE, PROV["Collection"]),
# todo: add docstring </s> for valid_value in valid_values:	_validate_estimator def _validate_estimator(estimator: EstTypes, *valid_values: EstTypes) -> Optional[bool]: if isinstance(valid_value, str) and estimator == valid_value.upper(): return True
# todo: make mac table updates less expensive. </s> for i, host in enumerate(sorted(port.hosts(vlans=[vlan]))):	update_metrics port_labels = dict(self.base_prom_labels, port=port.number) port_vlan_labels = dict(self.base_prom_labels, vlan=vlan.vid, port=port.number) mac_int = int(host.replace(':', ''), 16) self.metrics.learned_macs.labels(
# todo: check the data! </s> self.asserttrue(len(list(pipe)) > 0)	test_tail pipe_def = self._get_pipe_def(pipe_file) pipe = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def)
# todo: move this to pymatgen </s> if not spacegroup_symbol:	unicodeify_spacegroup def unicodeify_spacegroup(spacegroup_symbol): return "" subscript_unicode_map = {
# todo there is a cleaner way to do this in python 2.6, once </s> warnings.filterwarnings(	test_inplace_set_value assert may_share_memory(old_data, x_shared.container.storage[0]) == self.set_value_inplace finally: action='default', message='The .value property of shared variables is deprecated.'
# todo: update after date time test function code clean up. </s> self.assertequals(event_object.timestamp, 1331846259807996)	testParse23MultiVolume self.assertEquals(event_container.version, 23) event_object = event_container.events[5] self.assertEquals( event_object.timestamp_desc, eventdata.EventTimestamp.LAST_RUNTIME)
# todo: create unsupportedproviderexception. (?) </s> raise exception("this provider is not supported: {p}".format(p=cli_context.obj['provider']))	stop stop_ec2(cluster_name=cluster_name, region=ec2_region, assume_yes=assume_yes) else:
# todo use deepcopy() here </s> return polygonsonimage(polys_clean, shape=self.shape)	remove_out_of_image if not poly.is_out_of_image(self.shape, fully=fully, partly=partly) ]
# todo: funcbody </s> self.assertequal(7, p._pos)	testExpValueFunction self.assertIsNotNone(node)
# end todo </s> sqrt_ggn = torch.nn.functional.conv_transpose2d(	backpropagate_sqrt_ggn sqrt_ggn = sqrt_ggn.view(num_classes * batch, out_features) sqrt_ggn = sqrt_ggn.view(num_classes * batch, out_channels, out_x, out_y) sqrt_ggn, module.weight.data,
# todo: switch to: </s> self._trigger_track_playback_resumed()	resume self.state = PlaybackState.PLAYING
# todo we could reload the message </s> return self.input_sender	Forward async def get_input_sender(self): Returns `input_sender` but will make an API call if necessary. @property def chat(self):
# todo: add test and check this more throughroughly. </s> if hasattr(layer, "bias"):	contains_bias def contains_bias(layer): Check whether the layer contains a bias. return True else:
# todo replace with pec byte check </s> if res[0] == 0xaa and res[1] == data_len + 1:	_exec_read self._write(msg) res = self._read() data = res break
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
# todo: push all this code down to vigotoline? </s> self.view.window().run_command('vi_add_to_jump_list')	ExGoto state = VintageState(self.view) if state.mode == MODE_NORMAL: self.view.run_command('vi_go_to_line', {'line': b, 'mode': MODE_NORMAL}) self.view.window().run_command('vi_add_to_jump_list')
# todo enforce uniqueness on arrange panel? </s> models.siparrange.objects.create(	copy_to_arrange for entry in to_add: try: original_path=entry['original_path'], arrange_path=entry['arrange_path'],
# todo: replace with np.isnat </s> return lambda arr,i: arr[i] == nat	isna_overload if isinstance(dtype, (types.NPDatetime, types.NPTimedelta)): nat = dtype('NaT') return lambda arr,i: False
# todo: compare to plain for loop through the labels </s> sel = n.array([], dtype=n.int16)	getSampleIdsByLabels if not operator.isSequenceType(labels): labels = [ labels ] for label in labels: sel = N.concatenate((sel, N.where(self.__labels==label)[0]))
# todo(b/142684737): the multi-processing api might change. </s> beam_pipeline_args=['--direct_num_workers=%d' % direct_num_workers])	_create_pipeline metadata_connection_config=metadata.sqlite_metadata_connection_config( metadata_path),
# todo: check that the birth date is not in the future </s> if len(number) == 10:	validate raise InvalidLength() birth_date = get_birth_date(number) check = int(number[:-1]) % 11 if birth_date < datetime.date(1985, 1, 1):
# todo support for absolute episode scrobbling </s> log.info('absolute season mappings are not supported yet')	_build_episode_request if isinstance(match, EpisodeMatch): if match.absolute_num is not None: return None if match.season_num is None or match.episode_num is None:
# todo(nnorwitz): this doesn't handle namespaces properly. </s> for node in ast_list:	_IsSymbolUsed def _IsSymbolUsed(self, symbol): if node.Requires(symbol): return True
federated_only=self.federated_only)  # todo: 466 </s> if seed_node is false:	__attempt_seednode_learning certificates_directory=self.known_certificates_dir, timeout=timeout, self.unresponsive_seed_nodes.add(seednode_metadata) else:
# todo: error detection </s> __connect()	serialize_item def serialize_item(obj, item): collection = mongodb[obj.collection_type()] data = collection.find_one({'name':item.name})
# todo: find another way </s> for i in range(n):	scroll_down self.cursor_y = 3 else: self.dump_update_bottom(self.win_y, h) if self.win_y >= len(self.token_lines) - h:
# todo: handle multiple skip stacks </s> (skip, skip_stack), = skip_stack.items()	block_container_layout first_letter_style = getattr(box, 'first_letter_style', None) else: first_letter_style = None for index, child in enumerate(box.children[skip:], start=(skip or 0)):
# todo: currently this test breaks the bleu implementation (13.03.2016) </s> references = ['the candidate has no alignment to any of the references'.split()]	test_empty_hypothesis def test_empty_hypothesis(self): hypothesis = [] assert(sentence_bleu(references, hypothesis) == 0)
# todo: update the review with a message </s> gitcontext, review_branch, working_branch)	processUpdatedRepo abdt_workingbranch.pushBadInReview(
assert isinstance(obj_type, instance)  # todo more flexible </s> typeinfo = (cast(instance, obj_type)).type	visit_assignment_stmt obj = self.accept(member.expr) obj_type = self.types[member.expr] source = self.accept(s.rvalue) if member.direct:
# todo: redundancy between all gaze mappers -> might be moved to parent class </s> audio.say("stopping calibration")	stop def stop(self): logger.info("Stopping Calibration") self.active = False
# todo: this will need to be more complicated to support sparse features </s> assert self.max_num_actions is not none, "missing max_num_actions"	TrainingFeatureExtractor not_terminal = fetch(input_record.not_terminal).reshape(-1, 1) if self.include_possible_actions: possible_actions_mask = fetch(extract_record.possible_actions_mask).reshape( -1, self.max_num_actions
# todo: feed.abort() should be done by using exception? not a flag that has to be checked everywhere </s> self._abort = false	_reset self.failed = []   # failed entries self.disabled_phases = [] self.current_phase = None self.current_plugin = None
# todo: handle unsigned </s> maxage = str2time(re_t.group(1), thread.timestamp)	should_archive_thread if not thread.timestamp: return None if self.now - thread.timestamp > maxage: duration = str2localized_duration(self.site, re_t.group(1))
# time.sleep(40)  # todo: should remove after polling get. </s> mpc_1_2_3 = op(mpc_1_2, tensor_pointer_3)	test_tensor_abstraction_pointer mpc_1_2 = op(tensor_pointer_1, tensor_pointer_2) mpc_1_2.block_with_timeout(secs=40) mpc_1_2_3.block_with_timeout(secs=40) exp_res = op(data_1, data_2)
from .mpc import quadcost, lindx # todo: this is messy. </s> if isinstance(dynamics, lindx):	get_traj def get_traj(T, u, x_init, dynamics): F = get_data_maybe(dynamics.F) f = get_data_maybe(dynamics.f)
# todo: move to 80 bits </s> a = m2_expr.exprmem(a.arg, size=64)	fstp if isinstance(a, m2_expr.ExprMem): if a.size > 64: src = m2_expr.ExprOp('double_to_mem_%.2d' % a.size, float_st0) e.append(m2_expr.ExprAff(a, src))
# todo parameter order? (for clobj_list) </s> _handle_error(_lib.create_program_with_binary(	_init_binary for binary in binaries] binary_sizes = [len(b) for b in binaries] ptr_program, context.ptr, num_devices, ptr_devices, ptr_binaries, binary_sizes))
# todo(b/134377706): remove the wrapper once the bug is fixed. </s> @tf.function	testNumStepsPolymorphism initial_state = unrolled_lstm.initial_state(self.batch_size) if use_tf_function: def unrolled_lstm_fn(*args, **kwargs): with tf.device("/device:%s:0" % self.primary_device):
# todo: support all tzinfo subclasses by calling utcoffset() </s> if date_time.tzinfo is not none and\	datetime_obj_to_dtstruct dt.time.offset = 0 dt.time.ok = '\1' date_time.tzinfo.__class__ is TZFixedOffset: dt.time.offset = date_time.tzinfo.offset
# todo(ytknzw): add more specific assertion with the test case. </s> study = create_study()	test_plot_intermediate_values trial.report(2.0, step=1) return 0.0 study.optimize(lambda t: objective(t, True), n_trials=1) figure = plot_intermediate_values(study)
col_width = 30  # todo: use screen size </s> iprint(	header return f"{hbytes.rjust(10)}/s - {count:6}/s" if system_info is not None: row( ("Mem.", render_meminfo(system_info.memory), col_width),
# user perm is created on todo but for doctype assignment rule only </s> self.assertequals(new_doc.doc, "todo")	test_user_perm_on_new_doc_with_field_default frappe.set_user("new_doc_test@example.com") new_doc = frappe.new_doc("Doc A") frappe.set_user('Administrator') remove_applicable(["Assignment Rule"], "new_doc_test@example.com", "DocType", "ToDo")
# todo: implement positional encoding as described in the paper. </s> with tf.variable_scope("position_embedding"):	encode def encode(self, inputs, sequence_length=None, mode=tf.estimator.ModeKeys.TRAIN): input_dim = inputs.get_shape().as_list()[-1] position_embedding = create_position_embedding(
# todo: fix/disambiguate. </s> for timespan in node.payload:	elementsStoppingAt if node is not None: # could happen in an empty TimespanTree if node.endTimeLow <= offset <= node.endTimeHigh: if timespan.endTime == offset: result.append(timespan)
@pytest.mark.filterwarnings('ignore:missing metadata')  # todo: fix bug for hgs maps </s> def test_reproject_to_hgs_wcs(aia171_test_map, hgs_header):	test_reproject_to_hgs_wcs @figure_test aia171_test_map.reproject_to(WCS(hgs_header)).plot()
# todo: test require restart </s> tasks.restart_named(self.master, self.replicas[0])	test_disable_reenable_signing_replica ] self.master.run_command(args) assert wait_until_record_is_signed( self.master.ip, test_zone_repl, timeout=100
# todo add locales </s> raise yunohosterror("domain_name_unknown", domain=domain)	domain_set_settings domains = _load_domain_settings() if not domain in domains.keys(): setting_set = False if ttl is not None:
# todo: it has not been decided by the css working group how </s> total = (percent1 + percent2)	evaluate elif percent2 is not None and percent1 is None: percent1 = 100 - min(percent2, 100) if total != 100: factor = 100.0 / total
# todo add locales </s> raise yunohosterror("domain_name_unknown", domain=domain)	domain_set_settings domains = _load_domain_settings() if not domain in domains.keys(): setting_set = False if ttl is not None:
# todo should print message but carry on as if 0 </s> ui.stderr('osh error: printf: %s: invalid number', val)	Printf num = int(val) except ValueError: return 1 parts.append(str(num))
# todo: logging </s> contract_sizes = dict()	deploy_contract deploy_transaction = {'from': self.deployer_address, 'gasPrice': self.w3.eth.gasPrice} deploy_bytecode = contract_factory.constructor(*args, **kwargs).buildTransaction(deploy_transaction) if len(deploy_bytecode['data']) > 1000: contract_sizes[contract_name] = str(len(deploy_bytecode['data']))
# todo: move to base class </s> return self.getlodvaluefromscale(numlods, self.currentviewscale())	getLodValueFromCurrentScale def getLodValueFromCurrentScale(self, numLods=5):
# todo: assert </s> repo = self.remote.get_repo("testrepo0")	test_get_repo Test: Get a repo object
# todo: simplest possible unicast learning. </s> valve instance or none (of edge datapath where packet received)	_edge_dp_for_host Returns:
# todo: can we use a real variablemanager? </s> mock_variable_manager = magicmock(name='mockvariablemanager')	mock_variable_manager @pytest.fixture def mock_variable_manager(): mock_variable_manager.get_vars.return_value = dict() return mock_variable_manager
# todo implement for all channels </s> return none	_handle_toggle def _handle_toggle(self, message):
self.setup()  # todo: perhaps, remove this to pass path in context </s> current_ids = self.hash(	transform :param data_inputs: the data input to transform :return: transformed data inputs current_ids=None, hyperparameters=self.hyperparams,
# todo: remove in 1.2 </s> @pytest.mark.parametrize("attribute", ["fit_", "partial_fit_"])	test_birch_fit_attributes_deprecated def test_birch_fit_attributes_deprecated(attribute): msg = f"{attribute} is deprecated in 1.0 and will be removed in 1.2"
r = redirect("../../..") # todo: no longer correct </s> d = defer.deferred()	stop "'%s': %s\n" % (name, comments)) c.stopBuild(reason) reactor.callLater(1, d.callback, r) return DeferredResource(d)
#todo: implement xml support </s> return "whatever, we don't have xml yet"	create_tenant con.close() elif content == 'application/xml': accept_header = request.header.get('Accept') if accept_header in content_types:
### todo: need to improve </s> calculate f1 score for dev and test sets	calc_score def calc_score(self, ner_model, dataset_loader): args: ner_model: ner model
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: check </s> y = np.repeat([1], repeats=pos_scores.shape[0])	_compute_loss :param neg_scores: :return: y = torch.tensor(y, dtype=torch.float, device=self.device) pos_scores = torch.tensor(pos_scores, dtype=torch.float, device=self.device)
# todo: save as yaml file </s> directory = '~/.ros/handeye_calibration'	to_file def to_file(self, namespace, calibration): if not os.path.exists(directory): os.makedirs(directory)
# todo: confirm if gpu is used in hpo (probably not) </s> pass	_fit if num_gpus != 0: if 'device' not in params: logger.log(15, f'Training Gradient Boosting Model for {num_boost_round} rounds...') logger.log(15, "with the following hyperparameter settings:")
# todo: enable custom config </s> result.sort(key=lambda e: len(e['word']))	process_matches continue result.append(e) return result
# todo: some other way? this seems like a slippery slope of convenience functions </s> def fire_progress(data, outputter='pprint'):	fire_progress progress_event = {'data': data, 'outputter': outputter}
report_config = {}  # todo port to fooddata.from_request </s> report_config.update(	report_config @property def report_config(self): gap_type=self.request.GET.get('gap_type') or '', recall_status=self.request.GET.get('recall_status') or '',
# todo: document params </s> super(checkandrepairresultsrenderer, self).__init__()	CheckAndRepairResultsRenderer formatArgument = "output" def __init__(self, client, results): self.client = client self.r = None
# todo: should be named get_fields() ? </s> meta = self._get_extension_by_related_name(related_name)	get_translated_fields Return the translated fields of this model. By default, the top-level translation is required, unless ``related_name`` is provided. return meta.get_translated_fields()
return 0 # todo </s> def get_required_upgrade_resources(self, resource_id, upgrade_limit):	get_required_upgrade_resources
# todo:  implement this using in-memory locking </s> pass	UnlockFile @handle_fs_errors def UnlockFile(self, path, byteOffset, length, info):
# todo: exp_block_pairs </s> self.assertequal(9, p._pos)	testStatIf self.assertIsNotNone(node)
# todo: fix this </s> items = list(collection.list())	discover yield collection if depth != "0": if items: for item in items:
# todo: does this import need to be delayed because </s> from pyomo.solvers.plugins.solvers.persistent_solver import \	solve exception_on_failure=True, io_options=None): PersistentSolver if self.instance is None:
""" todo. """ </s> return true	should_poll @property def should_poll(self):
#todo: add csrf here? or make ids uuid so they can't be guessed? </s> def _index_delete(request):	_index_DELETE try: relay_address = RelayAddress.objects.get(
# todo: weather data source changed, temporarily disable, will enable it later when new data source is available. </s> for station in self._stations:	_update_station_extra_features weather = 0 temperature = 0 station.weekday = weekday station.holiday = holiday
pass # todo </s> def handle_request(self, input):	handle_request
#todo investigate 3.5/3.6 slowness </s> print('sum = {:f}'.format(sum(rewards)))	test_trpo_agent_continuous return
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo verify </s> kw_param = quote(keyword)	suggestions def suggestions(self, keyword): req = self.ses.get("https://www.google.com/trends/api/autocomplete/" + kw_param) try:
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> sampleparameters = {	test_acquire_token_with_user_pass def test_acquire_token_with_user_pass(self): "tenant" : "common", "authorityHostUrl" : "https://login.windows.net",
# todo(bcipolli): bulk saving of logs </s> log.save()	generate_fake_exercise_logs completion_counter=(date_completed-start_date).total_seconds()) log.full_clean() exercise_logs.append(log) return exercise_logs
# todo: this is a hack to make a rule know </s> if slot.value == "none" and slot.as_feature():	get_parsing_states for key, slot in tracker.slots.items(): if slot is not None: slot_id = f"slot_{key}_None" state_dict[slot_id] = 1
# todo: let this exception propagate </s> return false, none, bytes_transferred	_stream_data response.connection.connection.send(chunk) except Exception: bytes_transferred += len(chunk) if calculate_hash:
# todo: fix this to properly record the last stream id we've seen. </s> if self.streams:	receive_frame except ProtocolError as e: f = GoAwayFrame(0) f.last_stream_id = sorted(self.streams.keys())[-1] else:
pass # todo </s> def preview_drag_to(self, event_x, event_y):	preview_drag_to
#     todo </s> def release(self, path, fh):	Filesystem def readdir(self, path, fh): return ['.', '..'] + cache.list_effective_nodes(path) return os.close(fh)
# todo: we probably don't want to say this for re-formulation ones. </s> return result, "new_statements", "reduced scope of tried block."	computeStatement source_ref = self.getSourceReference() ) return self, None, None
# todo: capture stdout for both the test assert and docs embedding </s> prob.run_model()	test_feature_iprint_neg1 ln_scipy.options['iprint'] = -1
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_sandbox_command def test_sandbox_command(self): Sending a sandbox command to the node.
# todo(kan-bayashi): need to be fixed in pytorch v4 </s> if torch_is_old:	decode spemb = kaldi_io_py.read_vec_flt(js[utt_id]['input'][1]['feat']) spemb = torch.from_numpy(spemb) spemb = Variable(spemb, volatile=True) if args.ngpu > 0:
# todo: compare col/row widths before/after - not implemented yet </s> range('sheet1', 'a1:d4').value = 'test_string'	test_autofit_col def test_autofit_col(self): Range('Sheet1', 'A:D').autofit() Range('Sheet1', 'A:D').autofit(0)
# todo(b/186451541): reduce the number of calls to model_fn. </s> self.assertequal(mock_model_fn.call_count, 4)	test_construction_calls_model_fn model_to_client_delta_fn=DummyClientDeltaFn, server_optimizer_fn=tf.keras.optimizers.SGD)
# todo: show deprecation message in the future. </s> translator_services = copy.deepcopy(translator_config['services'])	get_translator self.root, ) if service is not utils.SENTINEL: valid_service_kinds = [each['service']
# todo 当我不使用下面的语句时，project和host貌似在线程里面会没有值，也许我要把lazy值设置成select或者其他 </s> a = deploy.project, deploy.host	build def build(self, deploy): t = threading.Thread(target=build_thread, args=(deploy,)) t.start()
# todo: improve the unicode checking </s> try:	__setitem__ % (item, self._index_for.capitalize(), self._index_for)) value = urllib.quote_plus(value) except (KeyError, UnicodeEncodeError, UnicodeError):
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	pause def pause(self, instance, callback):
# todo _cphttptools.applyfilterlist('afterrequestheader') </s> _cphttptools.dorequest(self.wfile)	do_GET _cphttptools.parseFirstLine(self.raw_requestline) self.cook_headers()
# todo: maybe[int] and maybe[simple_sum] are invalid </s> return 'optional[%s]' % _mypytype(typ.children[0])	_MyPyType return 'List[%s]' % _MyPyType(typ.children[0]) if type_name == 'maybe': if typ.resolved: if isinstance(typ.resolved, asdl_.Sum):  # includes SimpleSum
# todo(efried): change this to an inner join when we are sure all </s> join_chain = sa.outerjoin(	_anchors_for_sharing_provider join_chain, shr_with_sps, shr_with_sps_aggs.c.resource_provider_id == shr_with_sps.c.id) join_chain, rps, shr_with_sps.c.root_provider_id == rps.c.id) sel = sa.select([func.coalesce(rps.c.uuid, shr_with_sps.c.uuid)])
pass # todo </s> def test_delete(self):	test_delete
# todo: check what other filetpyes supported </s> if file.endswith('.png') or file.endswith('.png'):	video_create img_list = [] for file in os.listdir(input_path): img_list.append(input_path  + file) print('adding image ', input_path  + file)
# todo: [config] modify for new importer </s> stop_system_importer_file_csv = false	pre_check_config_attributes def pre_check_config_attributes(request, model): if not 1<= model.csv_column_system <= 256:
#todo - use a context manager here once we drop python 2.6 </s> self.assertraises(valueerror, kcluster, data,	test_kcluster [ 1, 1, 1, 1, 1], [ 1, 1, 1, 1, 1]], int) **{"nclusters": nclusters, "mask": mask, "weight": weight, "transpose": 0, "npass": 100,
# todo: remove anytime in 2016 </s> _assert(false, "status in filter wasn't set - this isn't expected to be possible")	get_prefix_and_key_for_filter_results_and_parsed_params prefix = "%s %s" % ("status", prefix) else: def _get_key(): if parsed_params.module is not None and parsed_params.get_module_int() is None:
# todo test </s> if self.network.connectivity_matrix != none:	__init__ self.index = index self.label = label self._input_indices = np.array( [index for index in range(self.network.size) if
# todo: test this </s> if elem.get('grace') is not none:	noteFromElement if dotElements > 0: post.duration = makeDuration(_qlDurationFromAttr(elem.get('dur')), dotElements) post.duration = duration.GraceDuration(post.duration.quarterLength) if elem.get('m21TupletNum') is not None:
# todo not an api for now </s> nodecontrolutil._get_curr_info(pkg_name)	test_generated_cmd_get_curr_info def test_generated_cmd_get_curr_info(catch_generated_command): pkg_name = 'some_package' assert generated_command == "dpkg -s {}".format(pkg_name)
# todo: reflection padding </s> conv = tf.nn.conv2d(input, weights,	c7s1_k biases = tf.get_variable("biases", [k], initializer=tf.constant_initializer(0.0)) strides=[1, 1, 1, 1], padding='SAME') bn = batch_norm(conv+biases, is_training)
# todo (shea): extract method(s) to get_source_processor() </s> elif "sourceforge_id" in facts:	generate_download_recipe github_repo="%GITHUB_REPO%") recipe.append_processor(gh_release_info_provider) if "developer" in facts and not prefs.get( "FollowOfficialJSSRecipesFormat"):
# todo: also run container and make sure that the env var is set inside the container </s> buildah("rmi", [target_image])	test_build_basic_image_with_env_vars assert a_b in out["OCIv1"]["config"]["Env"] assert x_y in out["OCIv1"]["config"]["Env"]
# todo stdin </s> )	__call__ expect_stderr=True, expect_fail=True, except CommandError as e: cmd_exitcode = e.code
# todo: put this into timeframegroup. #316 </s> return [timeframe.to_dict() for timeframe in timeframes]	list_of_timeframe_dicts ------- list of dicts
# todo: renderer.media_type isn't the right thing to do here... </s> if response.has_content_body:	render renderer = self.default_renderer response = exc.response content = renderer(self).render(response.cleaned_content, renderer.media_type) else:
# todo generator </s> containing_ds = install_necessary_subdatasets(ds, path, reckless)	__call__ ds = Dataset(dspath) assert ds.is_installed() if containing_ds.path != ds.path: lgr.debug("Installed %s to fulfill request for content for "
# todo : pytest.mark.parametrise once nose is gone. </s> def test_collections_defaultdict():	test_collections_defaultdict a = defaultdict() a.default_factory = a
# todo: gdal doesn't support signed 8-bit values, so we coerce to uint8, </s> if dst.dtype.name == 'int8':	rio_reproject resampling = resampling_s2rio(resampling) dst0 = dst dst = dst.view('uint8') if dst_nodata is not None:
# todo: this should show up in events </s> yield self.stop(true)	_make_create_pod_request raise self.log.info(f'Found existing pod {pod_name}, attempting to kill') self.log.info(f'Killed pod {pod_name}, will try starting singleuser pod again') return False
# todo: verify </s> manager.content.update(self.consumer_id, units, options)	test_content_update manager = factory.consumer_agent_manager()
# todo: implement </s> return patches	blast_radius_upgrade :rtype: list patches = []
# todo: the following skipped suite and fixtures should be enabled </s> return	test_provider_when_calling_delete_record_by_identifier_should_remove_record def test_provider_when_calling_delete_record_by_identifier_should_remove_record(self):
# todo sync protocol </s> time.sleep(0.01)	generate_events win2 = self.open_window() i3.command(f'[id={win1}] kill; [id={win2}] kill') i3.main_quit()
self.data = fp.read(1_048_576)  # todo: recheck in 0.6.x </s> self.check_errors()	check_download except UnicodeDecodeError: with open(os.fsdecode(self.last_download), mode="r", encoding='latin1') as fp: else: self.log_info(self._("No errors found"))
else: self.assertequal(end, 1) # todo: simple exec should not wait_testpid!! </s> top = _recent(output(_top_list))	test_3903_start_false_exec_oneshot logg.info(" %s =>%s\n%s\n%s", cmd, end, out, i2(err)) if real: self.assertEqual(end, 0) logg.info("\n>>>\n%s", top) self.assertFalse(greps(top, testsleep))
##todo(ziad):we need to figure out how to auth to keystone </s> conn = http_connect(self.auth_host, self.auth_port, 'get',	_expound_claims "Accept": "text/json", "X-Auth-Token": self.admin_token} '/v1.0/token/%s' % self.claims, headers=headers) resp = conn.getresponse()
# todo(necula): fix gather bug on tpu </s> if jtu.device_under_test() == "tpu":	test_gather_rank_change def test_gather_rank_change(self): raise unittest.SkipTest("TODO: fix bug: not compilable") params = jnp.array([[1.0, 1.5, 2.0], [2.0, 2.5, 3.0], [3.0, 3.5, 4.0]])
# todo: refactor the mockrequesthandler </s> class mockrequesthandler(object):	MockRequestHandler def __init__(self): self.n_written = 0
# todo unordered float </s> if a is none and b is none:	fcomi def fcomi(ir, instr, a=None, b=None): a, b = float_st0, float_st1 elif b is None:
# todo - temporary until flash algo is rebuilt with 4k page program size </s> super(flash_lpc1768, self).__init__(target, flash_algo)	__init__ def __init__(self, target):
# todo handle errors </s> vbox = vb_get_box()	vb_start_vm Blocking function! @return dict of started VM, contains IP addresses and what not log.info("Starting machine %s" % name) machine = vbox.findMachine(name)
# todo: make sure this openssl command works everywhere, maybe we should use a text_base64_decode? </s> result = run("echo '%s' | openssl base64 -a -d -out %s" % (base64.b64encode(content), shell_safe(location)))	file_write **{MODE_SUDO: use_sudo} ): if "openssl:Error" in result: fabric.api.abort('cuisine.file_write("%s",...) failed because openssl does not support base64 command.' % (location))
# todo: use mapper </s> params = {"event":[self.cube.name]}	facts "Selecting all fields provided byt the Mixpanel API.") cell = cell or Cell(self.cube) params.update(self.condition_for_cell(cell)) response = self.store.request(["export"], params, is_data=True)
# todo: specific exception handling </s> term_hierarchy = default	__read_term_hierarchy_file f.close() except: return term_hierarchy
# todo: improve error handling </s> return make_response("error", status_code=500)	handle_ws ) if status_code != 200:  # pragma: no cover return make_response("OK", status_code=200) elif event_type == "MESSAGE":
# todo: automate this (by lookup from nn). </s> internal_states_space=impalaagent.standard_internal_states_space,	test_isolated_impala_actor_agent_functionality state_space=env.state_space, action_space=env.action_space, ) agent.call_api_method("reset")
# todo: replace with assertion test </s> log.debug(obj.lookup('krnic'))	test__NIRWhoisLookup net = Net('115.1.2.3') obj = NIRWhois(net)
raise notimplementederror #todo </s> else:	__call__ raise NotImplementedError #TODO
# todo: empty env? </s> sendrc=false, timeout=120, usepty=false, environ={})	test_simple Obfuscated('hushnow', 'XXXXXXXX'), '-c', 'buildbot_test_10', 'sync', '-f'], self.basedir, + 0, Expect(['p4', '-p', 'p4dserv:1666', '-u', 'jimmy', '-P',
# todo verbosity was mistakenly removed from task_parameters on release 0.11.0, need to bring it back </s> if self.ap.is_a_highest_level_agent:	reset_evaluation_state self.num_successes_across_evaluation_episodes = 0 self.num_evaluation_episodes_completed = 0 screen.log_title("{}: Starting evaluation phase".format(self.name)) elif ending_evaluation:
# todo(b/124466113): remove tf.compat.v2 once tf 2.0 is the default. </s> if hasattr(tf, 'compat.v2'):	testAdaptToRemoveMetrics metric_values = eval_saved_model.get_metric_values() self.assertNotIn('average_loss', metric_values) imported = tf.compat.v2.saved_model.load( eval_export_dir, tags=tf.saved_model.SERVING)
# todo: avoid using default index? </s> with option_context("compute.default_index_type", "distributed-sequence"):	compare_values def compare_values(): return ( self.to_series().rename("self").to_frame().reset_index()['self'] ==
# todo: once we allow filtering, unit.store.units has to be a qs </s> profile = get_profile(request.user)	process_submit suggester=get_profile(request.user), state='pending', unit=unit.id) unit_rows = profile.get_unit_rows() preceding = unit.store.units.filter(index__lt=unit.index).count()
## \todo: remove nodegraph fallback when all client code has been updated </s> return gaffer.metadata.value( node, "nodegraph:childrenviewable" )	__childrenViewable if viewable is not None : return viewable
# todo: replace with "yield from" when dropping python 2. </s> for __ in value:	__call__ else: try: yield __ except TypeError:
# todo: could refactor this, probably </s> new_sents = []	augment_arabic_padt random.seed(1234) sents = read_sentences_from_conllu(input_conllu) for sentence in sents: if len(sentence) < 4:
# todo(pachristopher): remove this once tfdv 0.14 is released. </s> (major, minor, _) = tfdv.__version__.split('.')	_CsvToExample raise RuntimeError( 'Files in same split {} have different header.'.format(csv_pattern)) if int(major) > 0 or int(minor) >= 14: decoder = csv_decoder.DecodeCSVToDict
# todo: implement auto-dtype method in general parameters </s> zlp.data = zlp.data.astype('float32')	splice_zero_loss_peak_flog s.data[ith:2*ith] = s.data[ith-1] * np.hanning((ith)*4)[-ith:] pbar.update(i) s = self.deepcopy() Eaxis = s.axes_manager.signal_axes[0]
# todo should be outside the 'with' block </s> assert sio.getvalue() == '@name\nccata\n+\n!#!#!\n@name2\nhello\n+\n&&&!&&\n'	test_write_to_file_like_object fq.writeseq("name", "CCATA", "!#!#!") fq.writeseq("name2", "HELLO", "&&&!&&") assert fq._file.closed
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: if none, then get from the filename </s> sequences = seqio.parse(fname, file_type, alphabet)	encode_sequence_with_biopython if alphabet is None: alphabet = IUPACUnambiguousDNAWithN() letter_encoder = {l: i for i, l in enumerate(alphabet.letters)} alphabet_length = len(letter_encoder)
# todo: proper partitioning of unittests </s> break	do_sweep debug('TEST', 'Failed #%d' % len(failed_tests_str)) if environ.has_key('MVPA_QUICKTEST'): if exception is not None: exception.__init__('\n'.join(failed_tests_str))
await self._stream.reset()  # todo: specify error code </s> async def __aenter__(self):	Stream await self._stream.end() async def reset(self): return self async def __aexit__(self, exc_type, exc_val, exc_tb):
# todo(kan-bayashi): need to make more smart way </s> ys = [y[y != self.ignore_id] for y in ys_pad]  # parse padded ys	CTC :param ys: :return: self.loss = None ilens = torch.from_numpy(np.fromiter(ilens, dtype=np.int32))
# todo: fix </s> response = self.client.get('/reports/')	testBasicViews def testBasicViews(self): domain = Domain.objects.get(name='mockdomain') self.assertNotContains(response,"Error", status_code=200) self.assertNotContains(response,"Exception", status_code=200)
# todo: document! </s> return map(qobj, array(u * s).reshape((d, d, dk)).transpose((2, 0, 1)))	_svd_u_to_kraus def _svd_u_to_kraus(U, S, d, dK):
if optimizer in ['adam', 'adadelta', 'rmsprop', 'sgd', 'sgdmomentum']: #todo: this could also be done for other optimizers </s> cost = f_update(lrate, x, x_mask, y, y_mask, loss)	train scorer.set_reference(y_s) loss = mean_loss - numpy.array(scorer.score_matrix(samples), dtype='float32') else: cost = f_grad_shared(x, x_mask, y, y_mask, loss)
# todo assert increasing timestamp? </s> with sqlite3.connect(f'file:{f}?immutable=1', uri=true) as db:	measurements tot = 0 new = 0 try: datas = db.execute('SELECT Time, Temperature FROM data ORDER BY log_index')
if self._ndim == 3: # todo: use hasz </s> lgeos.geoscoordseq_getz(self._cseq, i, byref(dz))	__getitem__ lgeos.GEOSCoordSeq_getX(self._cseq, i, byref(dx)) lgeos.GEOSCoordSeq_getY(self._cseq, i, byref(dy)) return (dx.value, dy.value, dz.value) else:
recording_name = none  # todo </s> system_info = none #todo	recording_update_pupil_invisible_to_pprf_2_0 recording_software_name = None  # TODO recording_software_version = None  # TODO new_info_file = RecordingInfoFile.create_empty_file(rec_dir) new_info_file.recording_uuid = recording_uuid
# todo: refactor accordingly when v3 websocket api is released </s> temp_trading_pair = f"{trading_pair.split('-')[1]}-{trading_pair.split('-')[0]}"	BittrexAPIOrderBookDataSource number_of_pairs: int = len(trading_pairs) for index, trading_pair in enumerate(trading_pairs): try: snapshot: Dict[str, any] = await self.get_snapshot(temp_trading_pair)
# todo change to native framework call, when plex allows token in header </s> opener = urllib2.build_opener(urllib2.httphandler)	getPlayListItems try: users = plexTV().getUserList() request = urllib2.Request(sizeURL) request.add_header(
# todo: make test method </s> readline	test_readline except: return False return True
# todo: must be implemented </s> pass	range_from_chapters def range_from_chapters(crawler, times=0):
rm.fetch(refspec=refspec, progress=progress, **kwargs)  # todo: progress +kwargs </s> else:	fetch rm.fetch(refspec=refspec, progress=progress, **kwargs)  # TODO: progress +kwargs
# todo: implement this </s> if manifest:	_setup_wrapper_script_content cannot run simultaneously on the same machine (this helps for running :command:`make` on a shared source code archive, for example). raise NotImplementedError out = []
# todo: with git <= 2.3 keep old mechanism: </s> with remote.repo.git.custom_environment(	pull cnct = ssh_manager.get_connection(fetch_url) cnct.open() GIT_SSH_COMMAND="ssh -S %s" % cnct.ctrl_path): remote.pull(refspec=refspec, progress=progress)
# todo: given trial, a repo, and an observer </s> repo: hyperparamsjsonrepository = hyperparamsjsonrepository()	test_hyperparams_json_repository_should_be_observable_with_file_system_changes def test_hyperparams_json_repository_should_be_observable_with_file_system_changes(): pass
#@todo: re-enable test when exception handling is in place </s> self.assertraises(notimplementederror, pyro.naming.resolve, "pyroname:objectname" )	testResolve self.assertNotEqual(Pyro.constants.NAMESERVER_NAME,uri.object) self.assertEqual(uri, ns._pyroUri) self.assertRaises(TypeError, Pyro.naming.resolve, 999)  #wrong arg type
# todo: change this to use assertsetequal: </s> self.assertequal(true, all(updateddoc[k] == originaldoc[k] for k in updateddoc.keys()	test_field_update self.assertEqual(len(updatedDoc.keys()), len(originalDoc.keys())) self.assertEqual(updatedDoc['word_ss'], originalDoc['word_ss'] + ['epsilon', 'gamma']) if k not in ['_version_', 'word_ss']))
# todo(ayoung): support the ability for a project acting as a domain </s> project = token_data['project']	_populate_is_admin_project def _populate_is_admin_project(self, token_data): r = CONF.resource if (project['name'] == r.admin_project_name and
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo for windows: </s> print(open_cmd)	main "--tpslimit 3 --transfers 3 --drive-chunk-size 32M --fast-list " \ "--log-file=%s %s %s" % (screen_name, logfile, src_label+src_folder, dst_label+dst_folder) try: subprocess.check_call(open_cmd, shell=True)
# todo(gibi): remove this when live migration is fully supported and </s> self._turn_off_api_check()	test_live_migrate_with_qos_port_reschedule_success def test_live_migrate_with_qos_port_reschedule_success(self): self._start_compute('host3') compute3_rp_uuid = self._get_provider_uuid_by_host('host3')
# todo(albert): windows machines don't have a readline module. </s> readline.clear_history()	run log   -- list; a list of lines of output, as captured by the OutputLogger. self._activate_logger() outputs = iter(case.outputs)
# todo: may test file contents </s> temp = tempfile.namedtemporaryfile(delete=false)	test_export_to_xlsx_filename def test_export_to_xlsx_filename(self): self.files_to_delete.append(temp.name + ".xlsx") rows.export_to_xlsx(utils.table, temp.name + ".xlsx")
# todo: refactor codebase to avoid needing this kind of special case check by being more </s> if self.id == 'python' and self.is_installed():	install return True else: print("Skipped installing " + self.name + ", already installed.") return True
# todo(chen3feng): reuse cclibrary </s> self.data['generate_dynamic'] = (getattr(options, 'generate_dynamic') or	__init__ self.data['generate_python'] = 'python' in target_languages or getattr(options, 'generate_python') self.data['generate_go'] = 'go' in target_languages or getattr(options, 'generate_go') config.get_item('cc_library_config', 'generate_dynamic')) full_cpp_headers = []
# todo: the eigenvector associated with the smallest eigenvalue </s> max_ori = eig_vecs[:, idx_middle]	dics_epochs if i != eig_vals.argmax() and i != eig_vals.argmin(): idx_middle = i Wk[:] = np.dot(max_ori, Wk) Ck = np.dot(max_ori, np.dot(Ck, max_ori))
# todo: integrate this into emrjobrunner </s> step_num_to_id = runner._step_num_to_id()	list_relevant def list_relevant(runner, step_nums): logs = {} for log_type in _RELEVANT_LOG_TYPES:
# todo: use ledgerprocessor </s> stockreport.objects.filter(form_id=xform.form_id).delete()	remove_data @receiver(xform_archived) def remove_data(sender, xform, *args, **kwargs):
# todo(mlavalle) this notification should be updated to publish when </s> registry.notify(address_group, events.after_delete, self,	delete_address_group address_group = self._get_address_group(context, id) address_group.delete() context=context, address_group_id=id)
# todo: warn if not used with -t roles </s> exclusive.add_argument("-e", "--entry-point", dest="entry_point",	init_parser help='The path to the directory containing your roles.') exclusive = self.parser.add_mutually_exclusive_group() help="Select the entry point for role(s).") exclusive.add_argument("-s", "--snippet", action="store_true", default=False, dest='show_snippet',
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pshufd x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo: assert </s> self.asserttrue(self.remote.copy_distro(distro, "testdistrocopy", self.token))	test_copy_distro Test: copy a distro object distro = self.remote.get_item_handle("distro", "testdistro0", self.token) assert 0
# todo debug </s> print ("rule element evaluates "	_evaluateRuleElementsRecursively + "to triggered. Set 'or' rule " + "also to triggered.") + "to triggered. Set 'or' rule " + "also to triggered.")
pass  # todo </s> def get_xl_sheet(xl_workbook, sheet_name_or_index):	get_xl_sheet
# todo add verbose output </s> return self._domain	LocalizationModel @property def domain(self): @domain.setter def domain(self, new_domain):
# todo untested </s> 1/0	_handle_bio_errors raise WantReadError() elif _api.BIO_should_write(bio): raise WantWriteError() elif _api.BIO_should_io_special(bio):
# todo: timeline is global, get rid of it </s> for post in timeline:	gen_task_render_posts default_lang for lang in kw["translations"]: source = post.source_path dest = post.base_path
#todo: proper implementation </s> return {"term": {field: value}}	match_of def match_of(self, field, value):
# todo debug </s> print logstring	logRule + "end=%d)") % ruleElement.element.end logging.info("[%s]: %s" % (fileName, logString)) else: raise ValueError("Rule has invalid type: '%s'." % ruleElement.type)
# todo: don't let this crash the robot </s> assert abs(cos_param) < 1	leg_explicit_inverse_kinematics R_hip_foot = (R_hip_foot_yz ** 2 + x ** 2) ** 0.5 cos_param = (config.UPPER_LEG ** 2 + R_hip_foot ** 2 - config.LOWER_LEG ** 2) / (2.0*config.UPPER_LEG*R_hip_foot) gamma = np.arccos(cos_param) return np.array([abduction_angle, theta + gamma, theta - gamma])
# todo(stubexecutor): (optional) use stubbed_component_map to insert custom stub </s> self.stubbed_component_map = {}	__init__ 'SchemaGen', 'ExampleValidator', 'Trainer', 'Transform', 'Evaluator', 'Pusher'] for c_id in self.stubbed_component_ids: self.stubbed_component_map[c_id] = base_stub_executor.BaseStubExecutor
# todo: get rid of this </s> return true	_editing_allowed def _editing_allowed(self):
# todo: refactor this into a django form </s> from datetime import datetime	post @method_decorator(login_and_domain_required) def post(self, request, domain, *args, **kwargs): user_id = request.couch_user._id POST = json.loads(request.body)
# todo: make this more portable with shutil etc. </s> self.run = phlsys_subprocess.run	setUp def setUp(self): self.runCommands = phlsys_subprocess.runCommands self.path = "phlgit_diff_TestDiff"
# todo: test for last revision minus 50 on second page. </s> offset = url_for(controller='revision', action='list')	test_list_format_atom revision1 = revisions[0] try: res = self.app.get(offset + '?format=atom') print res
# todo(b/1613650790: move this logic to ppoklpenaltyagent. </s> if self._initial_adaptive_kl_beta == 0:	get_loss else: entropy_regularization_loss = tf.zeros_like(policy_gradient_loss) kl_penalty_loss = tf.zeros_like(policy_gradient_loss) else:
# todo: implement test </s> pass	test_linux def test_linux(self):
# todo: write tests </s> convert a @tie attribute to the required :class:`tie` object.	_tieFromAttr def _tieFromAttr(attr): :param str attr: The MEI @tie attribute to convert. :return: The relevant :class:`Tie` object.
# todo xxx graalvm change </s> raise unittest.skiptest("not supported")	test_sni_callback_alert @needs_sni def test_sni_callback_alert(self): server_context, other_context, client_context = self.sni_contexts() def cb_returning_alert(ssl_sock, server_name, initial_context):
pass # todo doing nothing is also a possibility </s> else:	_set_thumbnail_color pass # TODO doing nothing is also a possibility elif op_enum == cairo.Operator.DEST_IN: # blur cairo_context.set_source_rgba(red, green, blue, alpha) cairo_context.paint()
# todo: this might need a better test </s> xy_projected = warper._compute_projection(0.0, 0.0, 1.0)	test_depth_warper inv_depth_ref, patch_i) assert utils.check_equal_torch(patch_ref, patch_ref_functional) assert xy_projected.shape == (1, 2) subpixel_step = warper.compute_subpixel_step()
#todo raise error or others </s> return	on_recover_button_clicked if stderr: log.error(stderr) InfoDialog(_('Recover successfully!\nYou may need to restart your desktop to take effect')).launch()
# todo: arrange </s> repo = self.remote.new_repo(self.token)	test_create_repo def test_create_repo(self): Test: create/edit a repo object self.assertTrue(self.remote.modify_repo(repo, "name", "testrepo0", self.token)) self.assertTrue(self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token))
# todo consider adding the individual tiles to the resource? </s> for c in tileset_tag.findall("tile"):	capture_tileset else: return None gid = tileset.firstgid + int(c.attrib['id']) tile = tileset[gid]
# todo(iceboy): rate limit base on ip. </s> tdoc, pdoc = await asyncio.gather(contest.get(self.domain_id, tid),	ContestDetailProblemSubmitHandler async def post(self, *, tid: objectid.ObjectId, pid: document.convert_doc_id, lang: str, code: str): problem.get(self.domain_id, pid)) tsdoc = await contest.get_status(self.domain_id, tdoc['doc_id'], self.user['_id'])
# todo map relationship backreferences using the django names </s> base.prepare()	prepare_base metadata.reflect(engine) Base = automap_base(metadata=metadata) set_all_class_defaults(Base) return Base
# todo this is not tested yet. </s> from scipy.special import gamma	mean def mean(t, a, b): from scipy.special import gammainc
else: # todo support ssl </s> contextfactory = none	send contextFactory = ssl.ClientContextFactory() contextFactory.method = SSL.SSLv3_METHOD factory = ESMTPSenderFactory(self.username, self.password, message.from_addr,
# todo: implement this for mesh tallies </s> elif domain_filter.type == 'mesh':	get_subdomain_avg_xs domain_filter.type = 'cell' domain_filter.num_bins = 1 raise NotImplementedError('Average mesh xs are not yet implemented') new_shape = \
# todo: exp_block_pairs </s> self.assertequal(9, p._pos)	testStatIf self.assertIsNotNone(node)
#there's nothing todo </s> self.__remove_node(tid)	FilteredTree if curdis:
# todo: change </s> return self.slices[self.dataset.keys()[0]].size(0) - 1	__len__ def __len__(self):
# todo: lamp texture test.. </s> if n.inputs[0].is_linked:	export_lamp elif o['type'] == 'sun': o['strength'] *= 0.325 color_node = n.inputs[0].links[0].from_node if color_node.type == 'TEX_IMAGE':
# todo cache? </s> def test_disappearing():	test_disappearing saves = get_events(all_=True) favs = [s.kind for s in saves if s.text == 'favorited']
# todo implement. </s> self.is_running = false	PunchcardJob self.thread.join() def run(self):
""" a completed todo must start with an x followed by a date. """ </s> todo = todobase.todobase("x not complete")	test_completion3 def test_completion3(self): self.assertFalse(todo.is_completed())
# todo: log.warning("tried to release connection that did not exist any longer : {0}".format(connection)) </s> self._available_connections.setdefault(connection._node["name"], []).append(connection)	release else: pass
# todo, make connection _cache attr public </s> reader = objectreader(conn, conn._cache, self._factory)	load_multi_persistent def load_multi_persistent(self, database_name, oid, klass): conn = self._conn.get_connection(database_name) return reader.load_persistent(oid, klass)
# @todo: support for branches </s> fields.append("human_resource.organisation_id$name")	pr_search_ac show_orgs = settings.get_hrm_show_organisation() if show_orgs: name_format = settings.get_pr_name_format() import re
# todo verify the stream </s> finally:	test_record_to_file finally: camera.stop_recording() os.unlink(filename)
# todo: consider using eafp here instead. </s> if value is _not_found:	resolve value = self.get(parts[0], _NOT_FOUND) for part in parts[1:]: break value = _get_value(value, part)
# todo xxx graalvm change </s> raise unittest.skiptest("missing threading support")	ThreadedEchoServer self.daemon = True def __enter__(self): self.start(threading.Event()) self.flag.wait()
##? value()  --- todo fix support for tuple assignment </s> value	mapping (key, value) = item key for key, value in p.items(): key
# todo: delet this when all preprintproviders have a mapping </s> from osf.models.subject import subject	all_subjects return self.subjects.all() else: q = [] for rule in self.subjects_acceptable:
@unittest.skip('not written')  # todo: finish! </s> raise notimplementederror	test_empty def test_empty(self):
# todo use abi </s> output_value = self.symb.symbols[self.symb.ir_arch.arch.regs.rax]	build_references memory_in = {} memory_out = {} for expr in self.memories_write: value = self.symb.eval_expr(expr)
# todo: only consider the right ones for sources and targets </s> plural_unit = store.addsourceunit(sources)	_collapse sources = [u.source for u in units] targets = [u.target for u in units] plural_unit.target = targets
# todo: is this used? </s> if encode:	MongoDocumentField if group: self.group = copy.deepcopy(group) self.encode = eval(compile(encode, '__encode__', 'eval'), copy.copy(MONGO_EVAL_NS)) else:
# todo: currently stubs internal method, should provide stub </s> pairing._get_private_ip_addresses = \	setUp pass asyncio.sleep = fake_sleep lambda: [ipaddress.ip_address('10.0.0.1')]
# todo: make treecount configurable via an inputslot </s> self._tree_count = 10	OpTrainRandomForest Operator.__init__(self, parent) self._forest_count = 10 def setupOutputs(self): if self.inputs["fixClassifier"].value == False:
# todo: must be implemented </s> pass	range_from_chapters def range_from_chapters(crawler, times=0):
# todo: supporting fat binaries will be annoying. </s> self._warnunimplemented("archs")	GetCflags archs = self.GetActiveArchs(self.configname) if len(archs) != 1: archs = ["i386"] cflags.append("-arch " + archs[0])
# todo test </s> return self._item.aliases.keys()	keys_with_aliases def keys_with_aliases(self):
# todo: clean up this event print out. we probably want something </s> if suffix == 'new':  # skip "new" events	run if not self.opts.get('quiet', False): for suffix, ret in self.get_async_returns(async_pub['tag']): continue elif suffix == 'ret':  # for "ret" just print out return
# todo order this!!!!!! </s> cells = mesh.cells.copy()	write if write_binary: raise NameError('Only ASCII mdpa supported') if "tetra10" in cells: cells["tetra10"] = cells["tetra10"][:, [0, 1, 2, 3, 4, 5, 6, 7, 9, 8]]
self.button_align_test = wx.button(self, label="align test")    # todo maybe align left? </s> self.button_apply = wx.button(self, label="apply")	create_sizer_bottom_buttonrow def create_sizer_bottom_buttonrow(self): self.button_help = wx.Button(self, label="Help")                # TODO maybe align left? self.button_close = wx.Button(self, label="Close") self.button_apply.Bind(wx.EVT_BUTTON, self.onApply)
# todo: where to store config? </s> db = mysqldb.connect(	ingest_upload_atk_get_resource_children def ingest_upload_atk_get_resource_children(resource_id): resource_id = 31; host="localhost", user="root",
# todo: make handlers for modelfactoryevent from within the gui obj </s> model = main_window.tree_model	_load self.filename = filename view.expand_root_nodes() try: iter = model.get_iter((0,))
# todo optimize: special case where there is only one dynamic </s> if child_abi_t.is_dynamic():	lazy_abi_decode child_abi_t = abi_type_of(t) loc = _add_ofst(src, ofst) dyn_ofst = unwrap_location(ofst) loc = _add_ofst(src, dyn_ofst)
# todo integrate param when g is a clustered graph </s> if true:	plot_graph def plot_graph(G): ki, kj = np.nonzero(G.A) if G.directed:
#todo: add csrf here? or make ids uuid so they can't be guessed? </s> def _index_delete(request):	_index_DELETE try: relay_address = RelayAddress.objects.get(
#todo set the correct varname here </s> writefile.writesafe(self.jsontree, f, self.levelname )	exportJS def exportJS(self, charsn):
# todo: get rid of this copout and find a way to deal </s> elif type(range[0]) == pd.timestamp:	expand_range erange = (range[0] - zero_width/2, range[0] + zero_width/2) erange = range else:
# todo: this should be made more flexibly to handle differeing params for xform submission </s> try:	extract_instance_from_request attachments = {} if request.META['CONTENT_TYPE'].startswith('multipart/form-data'): instance = request.FILES[MAGIC_PROPERTY].read() except MultiValueDictKeyError:
# todo(pfnet): implement crop function </s> h = score_pool3	__call__ h = self.upscore_pool4(fuse_pool4) upscore_pool4 = h  # 1/8 for axis in [2, 3]: start = 9
# todo make this configurable </s> def get_prefix_color(prefix):	get_prefix_color if prefix == "&": return "lightgreen"
# todo(yanase): check values </s> assert len(weights) == 2	test_calculate_with_prior consider_endpoints=consider_endpoints, weights_func=default_weights) assert len(mus) == 2 assert len(sigma) == 2
# todo: update graph references </s> elif isinstance(src, dict):	MetaCollection lgr.error(e_msg) raise TypeError(e_msg) for key in src: if isinstance(src[key], Collection):
# todo: figure out a way to not trigger the "action_auth_started" when </s> http_referer = request.meta.get("http_referer")	dispatch def dispatch(self, request): if http_referer:
self.prob.cleanup()  # closes recorders todo_recorder: need to implement a cleanup </s> cr = casereader(self.filename)	test_basic_sellar2 self.prob.driver.add_recorder(self.recorder) self.prob.run_driver() last_case = cr.get_case(-1) print('last_case.desvars', last_case.desvars['pz.z'])
# todo: test logging messages. </s> self.manager.startup()	test_startup def test_startup(self): self.assertEquals(self.states, [])
# todo fix the fold to allow any number of dispositions </s> return (	new_admissions_chart else: x_kwargs = {"shorthand": "day", "title": "Days from today"} alt.Chart(projection_admits.head(plot_projection_days)) .transform_fold(fold=["hospitalized", "icu", "ventilated"])
# todo: make the get_closest_value to return region </s> value, value_index = self.get_closest_value(	get_current_CSS_value parsed_declaration = re.search(r'^(\s*)(-[a-zA-Z]+-)?([a-zA-Z0-9-]+)(\s*(?: |\:))((?:(?!\!important).)+)', declaration) declaration_index = declaration_index + parsed_declaration.start(5) parsed_declaration.group(5), declaration_index,
# todo: will work only if table.fields is ordereddict </s> csv_writer.writerow([type_.serialize(getattr(row, field))	export_to_csv csv_writer.writerow([field.encode(encoding) for field, _ in fields]) for row in table: for field, type_ in fields])
1  # todo: fill in identifier </s> )	profile_transfer amount, target, result.wait() profiling.print_all_threads()
# todo: make daemon? </s> self._listener = threading.thread(target=self._listen)	start self._port = port self._sock = self._connect(self._host, self._port) self._listener.start() return _Started(self)
# todo: refactor into one function that just takes nodes </s> for i in range(count):	add_scanner_count count = int(root.getChildCount()) print "length: " + str(len(scanner_issues)) node = model.getChild(root, i) tree_issue_name = node.toString()
# todo ? </s> return "method2"	method2 def method2(self, other):
# todo: check whether loaded network has the same number of classes as specified in ilastik! </s> self._loaded_pytorch_net = tiktorch.unserialize(self._filename)	create_and_train_pixelwise def create_and_train_pixelwise(self, feature_images, label_images, axistags=None, feature_names=None): logger.debug('Loading pytorch network from {}'.format(self._filename)) return PyTorchLazyflowClassifier(self._loaded_pytorch_net, self._filename)
assert isinstance(obj_type, instance)  # todo more flexible </s> typeinfo = (cast(instance, obj_type)).type	visit_assignment_stmt obj = self.accept(member.expr) obj_type = self.types[member.expr] source = self.accept(s.rvalue) if member.direct:
# todo(stevemar): remove the line below when we support multitenancy </s> project._info.pop('parent_id', none)	CreateProject raise e project._info.pop('links') return zip(*sorted(six.iteritems(project._info)))
# todo find out what's going on </s> return _read(this_dir / "maeztu_sainz.json", source)	maeztu_sainz def maeztu_sainz():
# todo(b/141131288): enable complex-valued sorts on tpu. </s> if (onp.issubdtype(dtype, onp.complexfloating) and (	LaxTest for axis in [-1, len(shape) - 1])) def testSort(self, shape, dtype, axis): (jtu.device_under_test() == "cpu" and jax.lib.version <= (0, 1, 47)) or jtu.device_under_test() == "tpu")):
# todo(blk-u): this doesn't look like it works as expected. </s> self.assertequal(	test_no_version_base_solidus def test_no_version_base_solidus(self): 'http://localhost:35357/', auth.replace_version('http://localhost:35357/', 'v2.0'))
# todo: content-type </s> encoder = http.httpjsonconverter()	_HandleRequest logging.exception("Unknown exception") raise http.HttpInternalServerError(message="Unknown error") self.response_msg.start_line.code = http.HTTP_OK self.response_msg.body = encoder.Encode(result)
# todo(dougwig) - remove this disable when fixing bug #1816874 </s> floating_ip_status = self.add_centralized_floatingip(fip, fip_cidr)	floating_ip_added_dist def floating_ip_added_dist(self, fip, fip_cidr): if fip.get(lib_constants.DVR_SNAT_BOUND): return floating_ip_status if not self._check_if_floatingip_bound_to_host(fip):
# # todo: should be able to just access the api from qml. </s> @pyqtslot()	DiscoverUM3Action else: return [] def refreshConnections(self) -> None: if self._network_plugin:
# todo: reimplement this. </s> def run(self, count=1, mode=none):	_vi_big_b class _vi_big_b(ViMotionCommand): def move(): if mode == modes.INTERNAL_NORMAL:
# todo: add ssl verification </s> configuration = client.configuration()	get_api_object def get_api_object(cluster_name, cluster_endpoint, challenge, evalai): aws_eks_api = evalai.get_aws_eks_bearer_token(challenge.get("id")) configuration.host = cluster_endpoint
# todo test for final </s> if not link_text:	hyperlink def hyperlink(self, link_text = None): link_text = self.canonical_basename() return """<a href="%s">%s</a>""" % (self.url(), link_text)
pass  # todo </s> def test_pipe_mock(popen):	test_pipe_mock
# todo: maybe[int] and maybe[simple_sum] are invalid </s> return c_type	_GetCppType elif type_name == 'maybe': c_type = _GetCppType(typ.children[0]) elif typ.resolved: if isinstance(typ.resolved, asdl_.SimpleSum):
# todo: check for existing record first. </s> existing = session.query(cls).filter_by(app=app).first()	Apps @classmethod def register(cls, app, session): if existing: self = existing
# todo(stephenfin): remove the hardcoded limit, possibly overridding </s> self.compute = self.start_service('compute', host='compute1')	test_create_server_with_pinning_quota_fails flavor_id = self._create_flavor(vcpu=2, extra_spec=extra_spec) self.api.update_quota({'cores': 1}) post = {'server': self._build_server(flavor_id)} ex = self.assertRaises(client.OpenStackApiException,
# todo: abstract this away into a function. </s> read_only = false	ViEnterInsertMode state.enter_insert_mode() self.view.run_command('collapse_to_direction') if self.view.file_name(): mode = os.stat(self.view.file_name())
# todo: this is untested. </s> _raise_current_error()	load_certificate_request raise ValueError("type argument must be FILETYPE_PEM or FILETYPE_ASN1") if req == _ffi.NULL: x509req = X509Req.__new__(X509Req) x509req._req = _ffi.gc(req, _lib.X509_REQ_free)
# todo: cache properly </s> points = get_user_energy_and_review_points(user)	get_energy_points @frappe.whitelist() def get_energy_points(user): return frappe._dict(points.get(user, {}))
# todo: cannot be loaded with plugins; improve this solution </s> d = helpers.import_from_plugin("googleapiclient.discovery", plugin="bigquery")	create_parser def create_parser(self, resource): try: if isinstance(resource.source, d.Resource): return BigqueryParser(resource)
#todo : what if multiple projects are selected ? </s> p = selected_proj[0]	on_add_task def on_add_task(self,widget) : selected_proj,sel_task = self.get_selected_task() task = self.projects[p][1].new_task() self.open_task(task)
# todo extend to nonbinary nodes </s> cyphi_tpm = np.zeros([2 ** number_of_nodes, number_of_nodes])	matlab_tpm2cyphi_tpm number_of_states = matlab_tpm.shape[0] number_of_nodes = matlab_tpm.shape[1] for i in range(number_of_states): cyphi_tpm[i] = matlab_tpm[cyphi_index2matlab_index(i, number_of_nodes)]
# todo: enhance and use textlib.multitemplatematchbuilder </s> template_regex = re.compile(	run with open(self.datafile + '.bak', 'wb') as f: pickle.dump(self.record, f, protocol=config.pickle_protocol) r"""{{\s*(?:%(prefix)s\s*:\s*)?  # optional "template:" (?:%(template)s)\s*\|   # catredir template name
# todo: speed up by initalising devicemanager() once - param maybe? </s> devman = rclient.devicemanager()	get_device_serial See: middleman.get_device_serial(...) try: devman.sync_effects = False rdevice = devman.devices[uid]
# todo replace with collections.sequence subclass </s> spotify.error.maybe_raise(self.error)	tracks def tracks(self): Will always return :class:`None` if the search isn't loaded. if not self.is_loaded: return None
# todo how to handle this when parent class has no embedders? </s> self._base_model._entity_embedder.penalty(**kwargs) +	penalty def penalty(self, **kwargs): return ( self._base_model._relation_embedder.penalty(**kwargs)
# todo(dcramer): it'd be nice to support more than this, but its </s> cursor_result, _ = self._search(request, project, {	delete else: try: 'limit': 1000, 'paginator_options': {'max_limit': 1000},
# todo: actually apply </s> self.back_to_parent()	__actions_read pass elif self.cur_action == 2: #  OK
# todo: write tests </s> text = textinfo.text	getGlyphRun def getGlyphRun(font, textInfo, **kwargs): runLengths = textInfo.runLengths direction = textInfo.directionForShaper
# todo(slaweq): remove that is_admin check and always perform rules checks </s> if not cfg.conf.oslo_policy.enforce_new_defaults and context.is_admin:	check e.g. firewall_policy -> pluralized = "firewall_policies" :return: Returns True if access is permitted else False. return True if might_not_exist and not (_ENFORCER.rules and action in _ENFORCER.rules):
# todo: maybe introduce a "closest_wall_or_door_given_dir" function to decide between right and left </s> return actions.left	_iterate if not fwd_cell: return actions.forward if np.array_equal(adj_pos, fwd_pos): self.stack.pop()
# todo: need to cleanup the named argument mess before it is possible. </s> pass	OptimizeFunctionCallArgsVisitor if star_dict_arg is not None: if star_dict_arg.isExpressionMakeDict(): elif star_dict_arg.isExpressionConstantRef(): pass
# enforce no minor ticks labels. todo: document? </s> axis.set_minor_formatter(mticker.nullformatter())	XYAxes if level > 2: axis.set_major_formatter(mticker.NullFormatter()) def _sharex_setup(self, sharex, level): this one will draw its properties."""
# todo: checking for sys.stdout.isatty() prevents shell redirections and pipes (useful for list commands). can we remove this check? </s> if not quiet and sys.stdout.isatty():	println def println(msg, console_prefix=None, end="\n", flush=False, logger=None, overline=None, underline=None): complete_msg = "%s %s" % (console_prefix, msg) if console_prefix else msg if overline:
# todo tags: ipdb </s> for device in self.by_index:	update_slaves index = msg['index'] if msg['event'] == 'RTM_NEWLINK': if index in self[device]['ports']: self[device]['ports'].remove(index)
#todo change to native framework call, when plex allows token in header </s> request = urllib2.request(self.getlistsurl, headers=myheader)	LIST myHeader = {} myHeader['X-Plex-Token'] = users[user]['accessToken'] playlists = XML.ElementFromString(urllib2.urlopen(request).read()) result = {}
# todo: need to add counter </s> return true	send_message delay.small_delay(self) if super(self.__class__, self).sendDirectItem('message', user_ids, text=text, thread=thread_id): self.logger.info("Message to {user_ids} wasn't sended".format(user_ids=user_ids)) return False
none, fifo_mode=false,  # todo python3: fifo mode fails in py3, needs fix </s> result_store=none	test_noref_moment_fextractor_frames self.fextractor = MomentNorefFeatureExtractor( [asset], ) self.fextractor.run()
# todo: detect if this untracked pending transaction is a commitment transaction at all. </s> message = f"we've an untracked pending transaction. issuing a replacement transaction."	__handle_replacement_commitment if wait_time_in_seconds > self.max_confirmation_time: if txhash is UNTRACKED_PENDING_TRANSACTION: else: message = f"We've waited for {wait_time_in_seconds}, but max time is {self.max_confirmation_time}" \
# todo: finish me </s> return "extra_pulmonary"	convert_disease_site if xlsx_value.split()[0] in ("EP", "Extrapulmonary"):
# todo: find these references and ensure they are closed </s> if is_win:	_del_git self._git.clear_cache() self._git = None gc.collect() gitdb.util.mman.collect()
# todo return empty list if not loaded </s> spotify.error.maybe_raise(self.error)	tracks def tracks(self): Will always return :class:`None` if the search isn't loaded. if not self.is_loaded: return None
# todo(rbharath): this should be modified to contain a cluster split so </s> splitters = {	load_pdbbind_from_dir if split == None: return pdbbind_tasks, (dataset, None, None), transformers 'index': deepchem.splits.IndexSplitter(), 'random': deepchem.splits.RandomSplitter(),
# todo discont: use offsets instead (note need for int conversion) </s> spans = [(start, end)]	__create_span text = txt_file.read()[start:end] if '\n' not in text: ann = TextBoundAnnotationWithText(spans, new_id, type, text) ann_obj.add_annotation(ann)
# todo: hints </s> while not ok:	challenge elif challenge_type == 'golf': ok = False self.pause_point(shutit_util.colour('31',task_desc),colour='31') # TODO: message check_command = follow_on_context.get('check_command')
# todo: consider paging for large result sets </s> query = urllib.urlencode({'q':yql,})	pipe_yql item = DotDict(item) yql = util.get_value(query, item, **kwargs) req = urllib2.Request(url, query) response = urllib2.urlopen(req)
# todo: this probably needs app_id too </s> .sort("received_on"))	_get_form_export_base_query .xmlns(export_instance.xmlns)
# todo: https://github.com/microsoft/ptvsd/issues/137 </s> self.assert_received(self.debugger, [])	test_pydevd_name self.assert_vsc_received(received, [])
# todo: fix this, this is one of the few cases where using the config </s> try:	__start_job_logging fmt = '%(asctime)s %(levelname)-5.5s| %(message)s' formatter = logging.Formatter(fmt=fmt, datefmt='%H:%M:%S') store_logging_stream = self.config.get('run.store_logging_stream', []) except AttributeError:
# todo: should not apply if the exception is from _run_command </s> self._transaction_failed = true	sendall except redis.ResponseError as exc: if self._transaction is not None: result = exc result = self._decode_result(result)
# todo: only return distribute if setuptools < 0.7 </s> req = requirement.parse('distribute')	resolve req = requirements.pop(0)   # process dependencies breadth-first if req.project_name == 'setuptools': if req in processed: continue
# todo: https://github.com/horovod/horovod/issues/2438 </s> all_envs = ray.get([h.env_vars.remote() for h in worker_handles])	test_colocator_gpu assert resources.get("CPU", 0) == 0, resources assert resources.get("GPU", 0) == 0, resources assert len({ev["CUDA_VISIBLE_DEVICES"] for ev in all_envs}) == 1 assert len(all_envs[0]["CUDA_VISIBLE_DEVICES"].split(",")) == 4
# todo generator </s> if containing_ds.path == path:	__call__ lgr.debug("Installed %s to fulfill request for content for " "path %s", containing_ds, path) content_by_ds[path] = [curdir] else:
# todo: wrap backend call in error handling. </s> if backend and backend.playback.resume().get():	resume return backend = self._get_backend(self.get_current_tl_track()) self.set_state(PlaybackState.PLAYING) self._trigger_track_playback_resumed()
# todo: review </s> raise notimplementederror	GBlock self.conv = spectral_norm(self.conv) if activation == 'glu': self.glu_conv = nn.Conv1d(ninputs, fmaps, kwidth, stride=pooling,
# todo: deal with the event in which create_circuit fails after the first_hop has been selected </s> self.create_circuit(2, circuit_type_intro, self._create_introduction_point)	create_introduction_point def create_introduction_point(self):
# todo(asalkeld) support versions </s> pkg_str = ''	_handle_apt_packages def _handle_apt_packages(self, packages): very basic support for apt for pkg_name, versions in packages.iteritems(): pkg_str.append(' %s ' % pkg_name)
# todo: there's a race with the initial "output" event. </s> with self.vsc.wait_for_event('output'):	test_launch addr = (None, 8888) with self.fake.start(addr): pass with self.vsc.wait_for_event('initialized'):
# todo: convert non uris to file uris. </s> cp = configparser.rawconfigparser()	parse_pls def parse_pls(data): cp.readfp(data) for i in xrange(1, cp.getint('playlist', 'numberofentries')):
# todo semantic validation </s> create_response = self.client.create(fsid, crush_node, serializer.get_data())	CrushNodeViewSet serializer = self.serializer_class(data=request.DATA) if serializer.is_valid(request.method): assert 'request_id' in create_response return Response(create_response, status=status.HTTP_202_ACCEPTED)
#todo: search text as well as title, figure out best way to sort results </s> return run_query(sqlquery)	search_notes def search_notes(query): sqlQuery = "SELECT ZUNIQUEIDENTIFIER, ZTITLE FROM ZSFNOTE WHERE ZARCHIVED=0 AND ZTRASHED=0 AND lower(ZTITLE) LIKE lower('%{0}%')".format(query)
raise notimplementederror  # todo </s> @asyncio.coroutine	XBoardProtocol @asyncio.coroutine def analysis(self): def configure(self): raise NotImplementedError  # TODO
# todo: this doesn't clean up the index </s> for model in models:	delete_objects def delete_objects(models, relation, limit=1000, logger=None): if logger is not None: logger.info('Removing %r objects where %r', model, relation)
# todo: this is slow, we should have a better accessor </s> for ann in ann_obj.get_relations():	_delete_arc_nonequiv_rel def _delete_arc_nonequiv_rel(origin, target, type_, mods, ann_obj): if ann.type == type_ and ann.arg1 == origin and ann.arg2 == target: ann_obj.del_annotation(ann)
# todo move the file obj into the decrypt to do streaming </s> file_data = await s3_response['body'].read()	S3CSE body = await self._decrypt_v1(file_data, metadata, actual_range_start) else: body = await self._decrypt_v2(file_data, metadata, whole_file_length, actual_range_start, desired_range_start,
# todo: include args in description </s> description = "relations"	search_anns_for_relation restrict_types = [] if restrict_types is None else restrict_types ignore_types   = [] if ignore_types is None else ignore_types if restrict_types != []: description = description + ' (of type %s)' % (",".join(restrict_types))
# todo: it can be also: </s> return return_value(values.w_void, env, cont)	do_extract_struct_info return v.call([], env, cont) else:
# todo featureparams nameids </s> t.table.featurelist.maplookups(lookupmap)	_preMerge lookupMap = dict(enumerate(t.table.LookupList.Lookup))
# todo: implement '-view ' </s> group2 = parser.add_argument_group('advanced options')	parse_command_line help="Enable verbose output from scan-build. A second and third '-v'\ increases verbosity.") group2.add_argument( '--no-failure-reports',
#todo: redo this with html parser instead of regex </s> def remote(pages, category, sort, mode, terms, mirror):	remote res_l = [] pages = int(pages)
# todo: check </s> return arm_is_branch_taken(insn)	aarch64_is_branch_taken def aarch64_is_branch_taken(insn):
### todo: method to register this system as brand new </s> logging.error("error finding %s: %s", host.hostname, detail)	get_system_handle system = self.server.find_system({"name" : host.hostname})[0] except IndexError, detail: raise ValueError("No system %s registered on install server" % host.hostname)
pass  # todo: finish implementation </s> def hold_browser_open(self, value: bool):	hold_browser_open @hold_browser_open.setter
# todo handle when something happens during writing of data. </s> _log.error('some things did not work')	publish_to_historian self.report_all_handled() else:
# todo: and results </s> return asynclist(tasks)	_init tasks = [mounter.remove(path, **strategy) for path in options['<device>']] else: return mounter.remove_all(**strategy)
# @todo: add more comprehensive show validation </s> try:	snatchSelection perform_search=0, down_cur_quality=0, show_all_results=0): indexer_tvdb = 1 show = int(show)  # fails if show id ends in a period SickRage/sickrage-issues#65 show_obj = Show.find(app.showList, show)
replace = re.sub('\$(\d+)', r'\\\1', replace)   #map $1 to \1 etc.   #todo: also need to escape any existing \1 etc. </s> rules.append((match, replace))	pipe_strregex match = util.get_value(rule['match'], kwargs) #todo use subkey? replace = util.get_value(rule['replace'], kwargs) #todo use subkey? for item in _INPUT: for rule in rules:
# todo: decide which one to use </s> options.update(cube.get("options", {}))	cube_options if cube_name in self.cubes_metadata: cube = self.cubes_metadata[cube_name] options.update(cube.get("browser_options", {})) return options
# todo(inf) de-wx!, needed for wx3, check if needed in phoenix </s> self.combobox.unbind(wx.evt_size)	_unbind_combobox def _unbind_combobox(self):
# todo complete this method </s> partition, kptlist, dtype)	eeccsd return ipccsd(eom, nroots, koopmans, guess, left, eris, imds,
# todo disconnect dealer/router </s> pass	HelloWorldMessage print() finally: async def lang_changer_router(self): rout = self.ctx.socket(zmq.ROUTER)
options['taskid'] = none # todo </s> agent = pulpagent(consumer)	install if consumer is None: raise MissingResource(id) agent.install_units(units, options)
# todo: test this </s> style = element.style	handle_computed_display_float Computed values of the display and float properties according to http://www.w3.org/TR/CSS21/visuren.html#dis-pos-flo if get_value(style, 'display') == 'none': return # position and float do not apply, but leave them
if self.is_direct_mode() or not allow_quick:  # todo: thin mode </s> info = self.annex_info(files, normalize_paths=false, batch=batch)	is_under_annex list of bool Per each input file states either file is under annex return [bool(info[f]) for f in files] else:  # ad-hoc check which should be faster than call into annex
# todo: move update_spec to worker. agent should not hold these execution details. </s> if time_percentage is none:	update def update(self, batch=None, time_percentage=None): time_percentage = self.timesteps / self.update_spec.get("max_timesteps", 1e6) self.steps_since_target_net_sync += self.update_spec["update_interval"]
# todo(pts): unify /encoding dicts globally, not only for </s> obj_nums = copy_encoding_dict.get(fd_obj_num)	UnifyType1CFonts
# todo: handle string array reflection </s> def str_fillna_impl(a, fill):	str_fillna_impl hpat.hiframes_api.fillna_str_alloc(A, fill)
# todo: it seems that yahoo! converts relative links to </s> content = unicode(f.read(), 'utf-8')	pipe_fetchpage url = util.get_abspath(url) f = urlopen(url) if context and context.verbose: print '............Content .................'
# todo: pandas formula is better or welford? </s> delta2 = val - mean_x	_column_var_impl_linear delta = val - mean_x mean_x += delta / nobs ssqdm_x += delta * delta2 return hpat.hiframes_rolling.calc_var(2, nobs, mean_x, ssqdm_x)
# todo: g+ has a multiply-valued 'urls' field. ignoring for now because </s> domain = none	_url_and_domain Returns: (string url, string domain, boolean ok) tuple actor = self.as_source.user_to_actor(json.loads(auth_entity.user_json)) ok = False url = actor.get('url')
## \todo adjust for #1438. </s> constant["format"].setvalue( gafferimage.format( iecore.box2i( iecore.v2i( 0 ), iecore.v2i( 199, 149 ) ), 1 ) )	testDataWindowRounding def testDataWindowRounding( self ) : constant = GafferImage.Constant() resize = GafferImage.Resize() resize["in"].setInput( constant["out"] )
# todo: use valid_episodes.mask for mean </s> kl = torch.mean(kl_divergence(pi, old_pi))	surrogate_loss loss = -torch.mean(ratio * advantages) losses.append(loss) kls.append(kl) return (torch.mean(torch.cat(losses, dim=0)),
raise deprecatedtest # this test is now broken. todo: fix it. </s> ap = make_test_ap()	test_simple_view_request def test_simple_view_request(): req = normalize_request({'id': None, 'name': None}, {'id':3, 'name':'stuff'})
# todov06 updatetablerow instead </s> for i in self.arrayofmessages[messageindex+1:]:	deleteMessage self.deletedMessageCount += 1 if len(self.arrayOfMessages) > messageIndex+1: i._tableRow -= 1 self.lock.release()
# todo(solitude): remove this. </s> data.update({'pattern': 'account.payment'})	preapproval key = result['key'] else: try: result = paypal.get_preapproval_key(data)
invoice.objects.filter(subscription=sub).latest('date_due').date_due # todo - check query </s> if len(invoice.objects.filter(subscription=sub)) != 0 else 'none on record',	ManageBillingAccountView parent_link='<a href="%s">%s<a>' % (AccountingInterface.get_url(), AccountingInterface.name), subscription_list=[(sub, 'ADD LINE ITEMS') for sub in Subscription.objects.filter(account=account)],
# todo add negative sampling? </s> l1 += dot(ga, l2a)  # learn input -> hidden	train_sentences ga = (1 - word.code - fa) * alpha  # vector of error gradients multiplied by the learning rate model.syn1[word.point] += outer(ga, l1)  # learn hidden -> output result += len([word for word in sentence if word is not None]) return result
# todo: this is untested. </s> _raise_current_error()	dump_publickey result_code = write_bio(bio, pkey._pkey) if result_code != 1: return _bio_to_string(bio)
limit = 20  # todo: change to setting </s> opts = '\n'.join('%s. opt' % x for x in range(limit))	test_markdown_poll_choice_limit_ok def test_markdown_poll_choice_limit_ok(self): Should not exceed the limit comment = "[poll name=foo]\n" + opts + "\n[/poll]" md = Markdown(escape=True, hard_wrap=True)
# todo: add more field checks here </s> self.assertequal(self.sounds[i].user.username, sound.username)	test_bulk_query_id_field_contents for i, sound in enumerate(Sound.objects.bulk_query_id(sound_ids=self.sound_ids)):
# todo(sdague): remove in juno </s> disk_names.extend(	_lvm_disks logical_volumes = libvirt_utils.list_logical_volumes(vg) disk_names = filter(belongs_to_instance, logical_volumes) filter(belongs_to_instance_legacy, logical_volumes) )
# todo, pass complete checkpoint as state dictionary </s> mp_queue.put(best_model_path)	transfer_distrib_spawn_state_on_fit_end if self.trainer.global_rank == 0 and mp_queue is not None: rank_zero_warn('cleaning up ddp environment...') mp_queue.put(results) last_path = None
# todo(akshayka): remove the in_graph_mode check once caching devices are </s> if in_graph_mode:	foldl in_graph_mode = context.in_graph_mode() with ops.name_scope(name, "foldl", [elems]): varscope = vs.get_variable_scope() varscope_caching_device_was_none = False
# todo make this smarter about more complex configurations (backup original values, etc) </s> obj_copy = deepcopy(obj_doc)	clear_mirrors 'Failed to load config because: {0}'.format(str(e))) return False for switch in obj_copy['dps']: for port in obj_copy['dps'][switch]:
# todo xxx graalvm change </s> raise unittest.skiptest("missing threading support")	ThreadedEchoServer self.daemon = True def __enter__(self): self.start(threading.Event()) self.flag.wait()
#todo load this from somewhere </s> pad_target = 189	assign_dev_data 0.0268245, -0.0277465, 0.258805, -0.187777, -2.3835, -1.42065] device.data[l:, q] = pad_data device.targets[l:, q] = pad_target chunking_active = dataset.chunk_size > 0
# todo(b/114938612): eventually remove this override. </s> validate=false)	test_preprocessing_fn os.path.join(testdata_path, 'example_gen_output/train/*'), coder=beam.coders.BytesCoder(), | 'DecodeTrainData' >> beam.Map(decoder.decode)) (transformed_examples, transformed_metadata), transform_fn = (
# todo implement some tolerance </s> self.assertequal(a, b)	test_create_time assert b >= 0 else:
# todo(sharadmv): don't drop the custom derivative rule </s> del primitive, jvp  # unused.	process_custom_jvp_call def process_custom_jvp_call(self, primitive, fun, jvp, tracers): return fun.call_wrapped(*tracers)
# todo(mattjj): revise this approach </s> is_float = lambda c: dtypes.issubdtype(dtypes.dtype(c), jnp.inexact)	closure_convert wrapped_fun, in_pvals, instantiate=True, stage_out=False) out_tree = out_tree() (closure_consts, hoisted_consts), merge = partition_list(is_float, consts) num_consts = len(hoisted_consts)
# todo: remove in 1.2 </s> return self._deprecated_fit	fit_ def fit_(self):
# todo: if a logographic word list is added to electrum2, might need to revisit this (no spaces) </s> return btcr.pbkdf2_hmac("sha512", " ".join(mnemonic_words), "electrum", 2048)	_derive_seed @staticmethod def _derive_seed(mnemonic_words):
# todo: remove when we stop using buildbot. </s> self.execute(	cancel_all_resultset_jobs job_guids = list(self.get_incomplete_job_guids(resultset_id)) jobs = self.get_job_ids_by_guid(job_guids).values() proc='jobs.updates.cancel_all', placeholders=[resultset_id],
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_all_parameters_empty def test_fail_all_parameters_empty(self):
# todo(metzman): change the strategy to record which plugin was used, and </s> extra_env['ld_preload'] = mutator_plugin_path	use_mutator_plugin return False logs.log('Using mutator plugin: %s' % mutator_plugin_path) if chroot: mutator_plugin_dir = os.path.dirname(mutator_plugin_path)
# todo: more error handling </s> self.play        = play	__init__ def __init__(self, play, ds): self.name        = ds.get('name', None) self.action      = ds.get('action', '')
# todo: insert or make chord </s> gmeasure.voices[0].insert(rn.measureoffset, n)	_createReduction if oneVoice: n, te = rn.getNoteAndTextExpression() if te is not None: gMeasure.voices[0].insert(rn.measureOffset, te)
#todo - oauth </s> curl_cmd = 'curl -l '	curl @property def curl(self): header = '' if self.headers:
# todo: replace this with proper storage reporting </s> if has_lvm:	_ComputeDynamicNodeData if iinfo.admin_state == constants.ADMINST_UP: i_p_up_mem += beinfo[constants.BE_MAXMEM] total_disk = get_attr("vg_size") free_disk = get_attr("vg_free")
# todo: register a custom error handling function to replace </s> regex = regex.encode('utf8', 'replace')	_compile_regex def _compile_regex(cls, regex, byte_regex=False): if byte_regex and not isinstance(regex, bytes): try: compiled = re.compile(regex, re.M)
# todo: this needs refactoring </s> if args.metadata_table:  # pragma: no cover	args_download assert args.refseq_category in EDefaults.REFSEQ_CATEGORIES.choices, \ "Unsupported refseq_category: {}".format(args.refseq_category) logging.info('Creating metadata file: %r', args.metadata_table) with open(args.metadata_table, 'wt') as metadata_table:
# todo: hydrate directly instead of via http hydration </s> if isinstance(obj, structure):	hydrate_ def hydrate_(obj, inst=None): signature, args = obj if signature == b"N":
# todo username </s> sudo = args.pushy('ssh+sudo:{hostname}'.format(	activate ) for hostname, disk, journal in args.disk: hostname=hostname, ))
# todo: migrate to new tilegrid format via library. </s> grid = {	run os.getenv("XRAY_DATABASE")), "r") as f: new_grid = json.load(f) 'tiles': new_grid, 'segments': {}
# todo: handle fixed and % widths </s> raise typeerror('width %s is unknown' % box.width)	block_preferred_width return 0 else:
# todo: give a vanilla example </s> .. math::	tpr_fpr def tpr_fpr(predicted_states, ground_truth_states): TPR^{(n)} = \\frac{TP}{\\left ( TP + FN \\right )} FPR^{(n)} = \\frac{FP}{\\left ( FP + TN \\right )}
# todo: this case is not fused! </s> (theano.tensor.mul(fx,ftanx,ftanx),(fx,),(fxv,),2,fxv*numpy.tan(fxv)*numpy.tan(fxv),'float32'),	do (fv-fy+tanh(fz),(fv,fy,fz),(fvv,fyv,fzv),2,fvv-fyv+numpy.tanh(fzv),'float32'),#fused with a dimshuffle (theano.tensor.mul(fx,fx,fx,fx),(fx,),(fxv,),1,fxv*fxv*fxv*fxv,'float32'), (theano.tensor.mul(fx,ftanx,ftanx,fx),(fx,),(fxv,),2,fxv*numpy.tan(fxv)*numpy.tan(fxv)*fxv,'float32'), (theano.tensor.mul(ftanx,ftanx,fx+fy),(fx,fy),(fxv,fyv),2,numpy.tan(fxv)*numpy.tan(fxv)*(fxv+fyv),'float32'),
# todo postremora replace the above with this line when remora goes away </s> user.confirmationcode = ''	confirm amo.utils.clear_messages(request) return http.HttpResponseRedirect(reverse('users.login') + '?m=5') user.save() messages.success(request, _('Successfully verified!'))
# todo(efried): due to bug #1782386, swap is not being reported. </s> 30,	test_ephemeral_has_disk_allocation self.assertEqual(expected_usage, resources['DISK_GB']) self.assertEqual( self.admin_api.get_hypervisor_stats()['local_gb_used'])
# tracks a suggested todo, which will reduce the 3 rpc calls here to only </s> translator = contracttranslator(netting_channel_abi)	get_channel_event def get_channel_event(self, channel_address, event_id, from_block, to_block=''): channel = self.raiden.api.get_channel(channel_address) filter_ = channel.external_state.netting_channel.events_filter(
# todo should we pass? </s> pass	WeatherCache except sqlite3.Error as err: _log.debug("Unable to access database tables: ?", err) self.max_size = max_size self.trim_percent = trim_percent
# todo check error message </s> assert response.status_code == 400	test_serialization_exception print(response.data)
# todo: div by zero error if all data exists at a single point. </s> pixels_per_degree = float(options.width - padding) / bounding_box_xy.sizex() * scale_factor	AutoSetScale pixels_per_degree = pixels_per_lat = float(options.height - padding) / bounding_box_xy.SizeY() * SCALE_FACTOR if options.width: if options.height: pixels_per_degree = min(pixels_per_degree, pixels_per_lat)
# todo private access </s> if hasattr(self, 'arguments'):	create_instance_context bound_method = BoundMethod(self, func) if scope.name.value == '__init__' and parent_context == class_context: return bound_method.as_context(self.arguments) else:
# todo(hartikainen): once tfp.bijectors.chain supports conditioning, </s> ildj = tf.cast(0., dtype=y.dtype.base_dtype)	_inverse_log_det_jacobian self._maybe_assert_valid_y(y) conditions = self._get_flow_conditions(**condition_kwargs) event_ndims = self._maybe_get_static_event_ndims( self.inverse_min_event_ndims)
# todo(b/160795287): deprecate estimator based executor. </s> eval_source = executor._eval_model_path(fn_args.model_run_dir)	run_fn io_utils.copy_dir(serving_source, serving_dest) absl.logging.info('Serving model copied to: %s.', serving_dest) eval_dest = fn_args.eval_model_dir io_utils.copy_dir(eval_source, eval_dest)
# todo: use shlex.quote as soon as a newer python version is available. </s> quoted_file_path = pipes.quote(file_path)	check_file def check_file(self, file_path): rvm_cmd = os.path.expanduser('~/.rvm/bin/rvm-auto-ruby') rubocop_cmd = rvm_cmd + ' -S rubocop ' + quoted_file_path self.run_shell_command(rubocop_cmd)
# todo: support other deployment situations. </s> if os.environ.get('server_software', '').startswith('google app engine'):	get_deployment_timestamp def get_deployment_timestamp(): Used for busting caches. version_id = os.environ.get('CURRENT_VERSION_ID') major_version, timestamp = version_id.split('.', 1)
assert self.restart_seq is none #todo: better handling of this situation </s> l.debug("initializing start sequence")	start_program assert self.start_seq is None #TODO: Better handling of this situation assert self.stop_seq is None #TODO: Better handling of this situation self.start_seq = sequence_controller() for p in self.roaster:
# todo: this test requires manifold access, see: t88318502 </s> self._test_retinanet_model("coco-detection/retinanet_r_50_fpn_3x.yaml")	TestScripting @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available") def testRetinaNet(self): def _test_rcnn_model(self, config_path): model = model_zoo.get(config_path, trained=True)
# todo remove backwards compatability fix in a future version </s> if build_version < 11100:	plugin_loaded preferences.set('vintageous_use_super_keys', preferences.get('vintageous_use_super_keys')) sublime.save_settings('Preferences.sublime-settings') def _migrate_rcfile(): old_file = os.path.join(sublime.packages_path(), 'User', '.vintageousrc')
# todo: use cli.output.write </s> print(result)	process_python_input result = eval(text, _globals, _locals) if result: except SyntaxError: exec(text, _globals, _locals)
# todo: check if output is spent (still neccessary?) </s> o.spent = none	gettransaction i.value = tx_api['inputs'][n]['amount'] for n, o in enumerate(t.outputs): if tx['confirmations']: t.status = 'confirmed'
#todo same issue with batch_size </s> if len(self.inputs) == 0:	width def width(self): raise ValidationException("gan.width() requested but no inputs provided") return self.ops.shape(self.inputs[0])[2]
# todo: requires special treatment? </s> current_unit = unit_line.variants[0].line[0]	_unit_line_to_game_entity :type unit_line: ..dataformat.converter_object.ConverterObjectGroup if isinstance(unit_line, GenieVillagerGroup): else: current_unit = unit_line.line[0]
# todo: make test method </s> termios	test_termios except: return False return True
# todo: only write this when the checksums file changed </s> with open(os.path.join(full_output_dir, 'md5sums'), 'w') as fh:	download_entry full_output_dir = create_dir(entry, section, domain, output) checksums = grab_checksums_file(entry) fh.write(checksums) parsed_checksums = parse_checksums(checksums)
# todo: check that the birth date is not in the future </s> except valueerror, e:	is_valid try: birth_date = _get_birth_date(number) return False if len(number) == 10:
#todo: http://www.6502.buss.hk/6502-instruction-set/jmp says that there is a indirect </s> self.assertequals(code, [0x20, 0x34, 0x12])	test_jsr_abs code = semantic(ast)
# todo: do not store message if the ajax client stats that it will not redirect </s> data.update({	_return_celery_result if smes: messages.success(self.request, smes) 'redirect': self.get_success_url(res.info), 'message': str(self.get_success_message(res.info))
# todo: check output </s> def test_hierarchy_iprint2(self):	TestSolverPrint prob.setup(check=False) output = run_model(prob) prob = Problem() model = prob.model
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: is wavelet centred properly? </s> y_0 = self.wavelet	C_d dt = self.dt C_d = 1 W_d = self.wavelet_transform_delta s = np.expand_dims(self.scales, 1)
# todo(b/186439691): remove when placer is fixed. </s> _add_skip_zero_before_finalize_dataset(graph_def)	function_to_wrap num_gpu_devices = len(tf.config.list_logical_devices('GPU')) if num_gpu_devices > 0: if num_gpu_devices > 1: _check_dataset_reduce_for_multi_gpu(graph_def)
# todo: remove this when domain decomposition is merged </s> warnings.warn('this feature is not yet implemented in a release ' \	set_dd_mesh_dimension def set_dd_mesh_dimension(self, dimension): 'version of openmc') if not isinstance(dimension, tuple) and \
# todo: return a list of chapters to download </s> raise not_implemented	get_range_using_urls def get_range_using_urls(self, crawler):
#todo: implement xml support </s> return "whatever, we don't have xml yet"	create_tenant con.close() elif content == 'application/xml': accept_header = request.header.get('Accept') if accept_header in content_types:
# todo:: fix typing in later version of numpy </s> x.flat[locs] = np.nan  # type: ignore	generate_panel_data y.flat[locs] = np.nan  # type: ignore locs = rng.choice(n * t * k, int(n * t * k * missing)) entities = [f"firm{i}" for i in range(n)] time = date_range("1-1-1900", periods=t, freq="A-DEC")
# todo handling when g is a list of graphs </s> global window_list	pg_plot_graph >>> sen = graphs.Logo() >>> plotting.plot_graph(sen) if 'window_list' not in globals(): window_list = {}
# todo: make sure this openssl command works everywhere, maybe we should use a text_base64_decode? </s> sudo("echo %s | openssl base64 -a -d | chpasswd" % (shell_safe(encoded_password)))	user_passwd_linux sudo("usermod -p '%s' %s" % (passwd,name)) else:
# todo get this from config </s> storage_service = "http://localhost:8000"	storagesetup return HttpResponseRedirect(reverse('main.views.home')) else: return render(request, 'installer/storagesetup.html', locals())
# @todo: test </s> wx.postevent(self.mainframe, ge.fitchanged(fitid=self.activefitid))	removeModule self.slotsChanged()
# todo/perf: users_id iterates over all items of all collections </s> for linked_datablock in library_datablock.users_id:	register library_datablock.mixer_uuid = self.mixer_uuid proxy_state.add_datablock(self.mixer_uuid, library_datablock) identifier = repr(linked_datablock) uuid = self._unregistered_datablocks.get(identifier)
# todo(yanase): change `task` in storages to `direction`. </s> self.storage.set_study_task(self.study_id, direction_obj)	_set_direction 'Otherwise, the direction is set to \'minimize\'.') if self.direction == structs.StudyTask.NOT_SET: return if direction is None:
# todo(jay-lau-513) translate the contents to a json stdin </s> out, err = utils.trycmd('echo contents | kubectl', 'update',	pod_update contents.pod_definition_url) else: '-f', '-') if err:
# todo: try mlp rather than bilinear </s> self.logits_second = tf.transpose(bilinear(emb_first, emb_node, name='logits_second'), [0, 2, 1])	_init emb_first_real = tf.boolean_mask(emb_node, mask_real) emb_first_real = tf.expand_dims(emb_first_real, axis=1) self.logits_second = tf.squeeze(self.logits_second, axis=-1) ac_first_mask = tf.one_hot(ac_first, depth=tf.shape(emb_node)[1], dtype=tf.bool, on_value=False, off_value=True)
# todo: another way to provide a (set of) potential defined object(s) </s> obj = none	_from_oer_ws Obj = const_obj[0] else: val_bytes, GEN = ASN1CodecOER.decode_open_type_ws(char) if Obj is None:
# todo: handle situations where share is password protected </s> path = string.replace(path,'/', '\\')	list_path def list_path(self, shareName, path, password = None): path = ntpath.normpath(path) if len(path) > 0 and path[0] == '\\':
# todo have one global properties object so this is no longer necessary </s> exporter.node_cache.clear()	export def export(self, exporter, props, luxcore_name): prefix = "scene.materials." + luxcore_name + "." interior_volume_name = None exterior_volume_name = None
# todo: weather data source changed, temporarily disable, will enable it later when new data source is available. </s> if not os.path.exists(trip_data_path):	_init if trip_data_path.startswith("~"): trip_data_path = os.path.expanduser(trip_data_path) self._build_temp_data() self._trip_reader = BinaryReader(self._conf["trip_data"])
# todo: detect if this untracked pending transaction is a commitment transaction at all. </s> message = f"we've an untracked pending transaction. issuing a replacement transaction."	__handle_replacement_commitment if wait_time_in_seconds > self.max_confirmation_time: if txhash is UNTRACKED_PENDING_TRANSACTION: else: message = f"We've waited for {wait_time_in_seconds}, but max time is {self.max_confirmation_time}" \
# todo is_compiled_with_cuda() has not been moved </s> compiled_with_cuda = paddle.fluid.is_compiled_with_cuda()	get_environ_info pass env_info['Python'] = sys.version.replace('\n', '') env_info['Paddle compiled with cuda'] = compiled_with_cuda if compiled_with_cuda:
# todo: test re-authentication when the token expires </s> test_data.alice.certificate)	test_raises_on_non_auth "dontcreateausercalledthis",
# todo(b/171088214): remove it after the control dependency in </s> with tf.control_dependencies([grad]):	_resource_apply_dense def _resource_apply_dense(self, grad, var, apply_state=None): lr_t, kwargs = self._get_lr(var.device, var.dtype.base_dtype, apply_state) decay = self._decay_weights_op(var, lr_t, apply_state)
# todo username </s> return 'aqbwdj5qap6lhhaaskvbnukyhj7eyremko5qka=='	get_monitor_secret def get_monitor_secret():
# todo: make truly async </s> self._resourcemanager_client = discovery.build('cloudresourcemanager', 'v1', cache_discovery=false, cache=memorycache())	__init__ def __init__(self):
# todo: fails because of missing svg support </s> url = "data:image/svg+xml,<svg width='2.54cm' height='0.5in'></svg>"	test_images_2 @assert_no_logs def test_images_2(): body, img = get_img('<img src="%s">' % url) assert img.width == 96
# todo: determine between create and/or start? </s> self.docker_client.start(container_id)	start if not container_id: continue device = self._init_container(state.blockade_id, container_id, container) updated_containers = state.containers
# todo(jeremydw): thread pool. </s> threads = []	apply_diffs @classmethod def apply_diffs(cls, diffs, paths_to_content, write_func, delete_func): for path in diffs.adds: logging.info('Writing new file: {}'.format(path))
# todo log/warn </s> return none	_try_load_impl_from_mod 'get_opentelemetry_implementation') # type: _UntrustedImplFactory except AttributeError: return _try_load_impl_from_callback(implementation_fn, api_type)
# todo: handle exceptions </s> raise notimplementederror	handle_cli raise NotImplementedError except Exception as e:
# todo document - how to deal with web media vs. normal? </s> if isinstance(self.result, types.botinlineresult):	photo @property def photo(self): return self.result.thumb elif isinstance(self.result, types.BotInlineMediaResult):
# todo: is this the right str? </s> excstr = "runtimeerror('something went wrong',)"	test_active_exception ) received = self.vsc.received self.assert_vsc_received(received, [ self.expected_response(
# todo!! add more assertions for the smaller subsystems </s> def test_complexes(standard, flushdb):	test_complexes complexes = list(compute.complexes(standard)) assert standard_example_is_correct(complexes[7])
'type': 'string', #todo: resolve </s> 'comment': ''	execute 'meta': [{ 'name': col[0] if type(col) is dict or type(col) is tuple else col, } for col in result.cursor.description] }
# todo(jaypipes): port nova's fault infrastructure </s> self.assertraises(exception.invalid, req.get_response, controllers.api())	test_create_image_with_bad_status req.method = 'POST' req.body = json.dumps(fixture)
# todo: fix 'extract_primitives' so that the value of 'material' is none and not empty string </s> return none	__gather_materials def __gather_materials(blender_primitive, blender_mesh, modifiers, export_settings): if not blender_primitive['material']: mesh_double_sided = blender_mesh.show_double_sided material = bpy.data.materials[blender_primitive['material']]
# @todo: return only packages for the current architecture </s> pkg_name = package.name	print_package_search_results reverse=True ): if args.quiet: print(pkg_name)
# todo: will need some code for the tabs active terms to work </s> self.get_toplevel().last_active_term = none	on_vte_focus_in self.terminator.last_focused_term = self if self.get_toplevel().is_child_notebook(): else: self.get_toplevel().last_active_term = self.uuid
# todo legacy method to be removed/refactored </s> from corehq.apps.locations.models import location	locations @property def locations(self): from corehq.apps.commtrack.models import SupplyPointCase def _get_linked_supply_point_ids():
federated_only=self.federated_only,  # todo: 466 </s> treasure_map_tracker=self.treasure_maps,	__init__ db_filepath=db_filepath, network_middleware=self.network_middleware, node_tracker=self.known_nodes, node_bytes_caster=self.__bytes__,
# todo error on missing levels </s> pass	parse_style markers = dict(zip(style_levels, self.default_markers)) elif markers and isinstance(markers, dict): elif markers: markers = dict(zip(style_levels, markers))
#        #todo: refresh all k-buckets further away than this node's closest neighbour </s> self.next_refresh_call = reactor.calllater(constants.checkrefreshinterval,	startNetwork raise ValueError("%s lbrynet may already be running." % str(e)) self.change_token_lc.start(constants.tokenSecretChangeInterval) self._refreshNode) self.hash_watcher.tick()
# todo: remove .nocache() when iterator() is dropped </s> self._result_cache = list(self.nocache().iterator())	_fetch_all self._result_cache = pickle.loads(cache_data) else: self._cache_results(cache_key, self._result_cache) self._no_monkey._fetch_all(self)
#todo use logging </s> print e	check_update return appcenter_check_update() except Exception, e:
# todo: validate and mask this before it's selected </s> show_invalid_depstring_notice(pkg, dep_string, str(e))	_add_pkg_deps except portage.exception.InvalidDependString as e: if not pkg.installed: return 0 del e
# todo generator </s> lgr.debug("found no annex at %s. skipped.", cur_ds)	_get found_an_annex = isinstance(cur_ds.repo, AnnexRepo) if not found_an_annex: if results: yield results
# todo: for training data (but not validation), do data augmentation here. </s> print "\tsaving prepared data..."	prepare_data (train_paths, validation_paths, train_targets, validation_targets) = _split_data_sets(details) validation_paths = _move_validation_images(validation_paths, output_images) training_file = os.path.join(output_leveldb, "train_leveldb") validation_file = os.path.join(output_leveldb, "validation_leveldb")
# todo: remove in 0.7.0 </s> self._create_transition(model.state, state_name).execute(event)	to_state model, args=args, kwargs=kwargs)
# todo - override the 'create' method for this view, </s> return self.serializer_class(*args, **kwargs)	get_serializer kwargs['context'] = self.get_serializer_context()
# todo should this reset the pts and such? </s> for _ in range(self._workers):	_stop_workers if self._workers: with self._updates_lock: self._updates.appendleft(StopIteration()) self._updates_available.set()
# todo: is this recalculation really necessary? (see below as well) </s> trterm = np.sum(np.sum(	dhdx_local dVdy = dVdy[np.triu(np.ones((N,N))).astype(bool)] dMM = dMdy.dot(dMdy.T) np.multiply(ddlogPdMdM, np.reshape(dMM, (1,dMM.shape[0],dMM.shape[1]))), 2), 1)
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
# todo: move to base class </s> (xfo, invres) = self.transform().inverted()	wheelEvent def wheelEvent(self, event): topLeft = xfo.map(self.rect().topLeft()) bottomRight = xfo.map(self.rect().bottomRight())
self.parent().hide() # todo: make configurable </s> self.parent().mainwindow().currentview().setfocus()	slotReturnPressed def slotReturnPressed(self):
# todo placeholder; implement </s> return roleinfo(name=role, role_world_size=0, local_world_size=0, worker_infos=[])	get_role_info def get_role_info(role: str) -> RoleInfo: Returns the role information.
# steps = 0 # todo </s> print("dupli export took %.3fs" % (time() - start))	convert transformations = array("f", duplis.matrices) luxcore_scene.DuplicateObject(src_name, dst_name, count, transformations)
# # todo: mrcc energy is way off expected </s> import pyscf.cc	test_iter_s def test_iter_s(self): e1, t1 = cc.kernel_ground_state_s(self.ccsd) cc1 = pyscf.cc.UCCSD(self.mf, frozen=1) old_update_amps = cc1.update_amps
# todo do something with temp </s> self._remove_substates_from_subhmms()	HSMMSubHMMStates super(HSMMSubHMMStates,self).clear_caches() def resample(self,temp=None): super(HSMMSubHMMStates,self).resample() # resamples superstates self._resample_substates()
if not file_name:  # todo: file completion </s> self.window.run_command('show_overlay', {	TabControlCommand (group_index, view_index) = self.window.get_view_index(self._view) if command == 'open': 'overlay': 'goto', 'show_files': True,
# todo (#567): bucket the node as suspicious </s> return	learn_from_teacher_node self.known_nodes.mark_as(current_teacher.InvalidNode, current_teacher) self.log.warn(f"Teacher {str(current_teacher)} is invalid (hex={bytes(current_teacher.metadata()).hex()}):{e}.") except RuntimeError as e: if canceller and canceller.stop_now:
# todo: error checking </s> body = json.dumps(request),	add_card headers = headers,
# todo: should we enable auto-retry, </s> headers = self.get_message_headers(worker_ctx)	__call__ exchange = get_rpc_exchange(container) with producers[conn].acquire(block=True) as producer: correlation_id = str(uuid.uuid4()) reply_listener = self.reply_listener
# todo: handle if there is a selection </s> send(vt100_left)	MicroPythonREPLPane super().keyPressEvent(data) else: elif key == Qt.Key_Home: send(VT100_HOME)
else: self.assertequal(end, 1) # todo: simple exec should not wait_testpid!! </s> top = _recent(output(_top_list))	test_3904_start_false_exec_forking logg.info(" %s =>%s\n%s\n%s", cmd, end, out, i2(err)) if real: self.assertEqual(end, 0) logg.info("\n>>>\n%s", top) self.assertFalse(greps(top, testsleep))
# todo: make this status more clear </s> item.status_code = 400	FetchThread item.data = conn.read() except socket.timeout, e: return item finally:
# todo: add run on all columns functionality </s> xl_woorkbook = xlrd.open_workbook(file_path)	xls_file_to_indicator_list def xls_file_to_indicator_list(file_path, sheet_name, col_num, starting_row, auto_detect, default_type, type_col): indicator_list = [] if sheet_name and sheet_name != 'None': xl_sheet = xl_woorkbook.sheet_by_name(sheet_name)
# todo: handle other hosts </s> raise notimplementederror	get_ipv4_addresses return addresses else:
logger.warn('zzz')  # todo: buffer .. </s> else:	read_ipc if o_client != client: if not o_lock.acquire(False): o_pipe.send((event, data,)) o_lock.release()
# todo ??? other type of cases </s> def test_check_version_fail():	test_check_version_fail for version in [ '1',
# todo: error handling like numba callwrappers.py </s> return unbox_array(types.array(dtype, 1, 'c'), arr_obj, c).value	_unbox_series_data elif dtype == datetime_date_type: return unbox_datetime_date_array(data_typ, val, c).value
# todo(john-wood-w) allow until plugin validation is added. </s> self.secret_req['bit_length'] = 129	test_should_allow_non_multiple_eight_bit_length def test_should_allow_non_multiple_eight_bit_length(self): result = self.validator.validate(self.order_req) self.assertTrue('secret' in result)
# todo, can we avoid the copy? </s> with self._xbc.dat.vec as v:	mult if self.on_diag: if len(self.row_bcs) > 0: X.copy(v) for bc in self.row_bcs:
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
# todo: implement </s> pass	_registration def _registration(self, arg):
# todo(jk0): this will eventually need to take ssl into consideration </s> return "http://%s:%d" % (flags.glance_host, flags.glance_port)	_construct_glance_url def _construct_glance_url():
# todo: use sqlalchemy to build this query! </s> sql = "select r.cid, r.ease, r.id from revlog as r inner join (select cid, max(id) as id from revlog group by cid) as q on r.cid = q.cid and r.id = q.id"	cards_recent_ease def cards_recent_ease(self, col, req): where = [] if req.data.has_key('ids'):
# todo: setup/teardown must be methods of a class so we can reuse them </s> workbook_kwargs = workbook_kwargs or {}	sheet_names def sheet_names(filename_or_fobj, workbook_kwargs=None): workbook_kwargs["read_only"] = workbook_kwargs.get("read_only", True) workbook = load_workbook(filename_or_fobj, **workbook_kwargs)
# todo(dcramer): re-enable test when find_green_parent_sha is turned </s> assert source.revision_sha == 'a' * 40	test_with_full_params assert job.label == self.plan.label assert source.repository_id == self.project.repository_id assert source.data == {'foo': 'bar'} patch = source.patch
# todo: reenable in the features branch </s> return cls	_getcustomclass else: cls = getattr(self, name)
# todo add docstring </s> query_identifier = self.query_types[query_type]	parse_datafile def parse_datafile(self, datafile, query_type, coll_partner_index=0): if query_type == 'coll_rates': i = coll_partner_index
# todo update this code once keras > 2.0.4 is released </s> try:	restart weights_h5 = LoggingCallback.WEIGHTS_H5.format(log_dir=log_dir, epoch=epoch) embedding = keras.models.load_model( weights_h5, custom_objects=CUSTOM_OBJECTS,
# todo results from p0f </s> return	_get_ipv4_os @staticmethod def _get_ipv4_os(endpoint):
# todo(iceboy): rate limit base on ip. </s> tdoc, pdoc = await asyncio.gather(contest.get(self.domain_id, tid),	ContestDetailProblemSubmitHandler async def post(self, *, tid: objectid.ObjectId, pid: document.convert_doc_id, lang: str, code: str): problem.get(self.domain_id, pid)) tsdoc = await contest.get_status(self.domain_id, tdoc['doc_id'], self.user['_id'])
# todo: change logging </s> root_logger = logging.getlogger()	init_logging written into this file. Does nothing if logging has already been initialized
# todo this requires fleshing out some more.. </s> class foo(object):	test_string_output def test_string_output(): name = 'value' field = String(name='name', required=True)
# todo append masters as named-instances as well; needs .designspace change. </s> gx = ttfont(master_ttfs[base_idx])	main style = instance[4] instance_list.append((style, loc)) _add_fvar(gx, axes, instance_list) print("Setting up glyph variations")
# todo(laigd): remove this check when 313682500 is in the release. </s> if use_tf_function and tf.compat.v1.__version__ < '2.3.0':	IfTest ) def testSimple(self, use_tf_function): return FLAGS.if_use_tf_function = use_tf_function
# @todo - link changes with colud </s> @property	defaultformat def defaultformat(self): return self._defaultFormat
#time = "todo" </s> annot.annotation_metadata.annotator.email = "todo"  # todo	fill_annoatation_metadata annot.annotation_metadata.annotator.name = name
# todo: fix self.cursor_x >= w </s> line = self.win_y + self.cursor_y	main_cmd_next_bracket def main_cmd_next_bracket(self, h, w): x = self.cursor_x char = self.output.lines[line][x]
# todo may want to pass iv and dv instead of expr (especially since iv/dv may not be variables) </s> stat_test = find_test(dataset, expr, data_props, design, sample_size=sample_size, effect_size=effect_size, alpha=alpha)	execute_test effect_size = design['effect size'] if ('effect size' in design) else [.2, .5, .8] # default range unless user defines alpha = design['alpha'] if ('alpha' in design) else .05 results = stat_test() stat_test_name = results.__class__.__name__
# todo: use is_accessible once two layer trie is implemented </s> if keccak(address) + code_trie_prefix not in self.read_list:	delete_account 'Attempted deleting account without full storage access' ) raise UnannouncedStateAccess( "Attempted deleting code of account outside of write list"
# todo generator </s> get_results = get.__call__(	__call__ if to_get: lgr.debug("Install passes into get %d items", len(to_get)) to_get,
# tracks a suggested todo, which will reduce the 3 rpc calls here to only </s> translator = contracttranslator(channel_manager_abi)	get_channel_new_events def get_channel_new_events(self, token_address, from_block, to_block=''): token_address_bin = address_decoder(token_address) channel_manager = self.raiden.chain.manager_by_token(token_address_bin)
# todo: fix with stubber / before send event </s> with mock.patch('botocore.endpoint.endpoint._send') as mock_send:	test_int_values_with_sigv4 s3 = self.session.create_client( 's3', config=Config(signature_version='s3v4')) mock_send.return_value = mock.Mock(status_code=200, content=b'',
#todo: this should never happen, dep on equiv </s> print 'content-type: application/json\n'	delete_arc mods.deletion(eq_ann) except DependingAnnotationDeleteError, e: display_message(e.json_error_response(), type='error', duration=3) print dumps(add_messages_to_json({}), sort_keys=True, indent=2)
# todo - add hubbub package when available. </s> raise conaninvalidconfiguration("library hubbub not implemented (yet) in cci")	requirements if self.options.with_hubbub:
# todo use display_message and add_messages_to_json </s> print dumps({ 'error': error }, sort_keys=true, indent=2)	save_span print 'Content-Type: application/json\n' error = 'unable to change the span of an existing annotation' assert False, error except AttributeError:
# todo refactor and remove </s> raise notimplementederror	make_residual_mat_prod zero and not diagonal (for instance, `BatchNorm`).
pass # todo </s> def handle_request(self, input):	handle_request
# todo(ytknzw): add more specific assertion with the test case. </s> study = create_study()	test_plot_intermediate_values trial.report(2.0, step=1) return 0.0 study.optimize(lambda t: objective(t, True), n_trials=1) figure = plot_intermediate_values(study)
#@todo: remove in 0.4.10 </s> def error(self, reason=none, type="parse"):	error raise Fail("%s error%s | Plugin out of date" % (type.capitalize(), ':' + str(reason) if reason else "")) if self.core.debug:
# todo: timeline is global, get rid of it </s> for post in timeline:	gen_task_render_posts default_lang for lang in kw["translations"]: source = post.source_path dest = post.base_path
# todo: sort the functionality by name and by vuln class </s> for functionality_name in functionality:	createMenuItems functionality = data["functionality"] bugcatcher_menu = JMenu("Send to Bug Catcher") vulns = functionality[functionality_name]["vulns"] menu_vuln = JMenu(functionality_name)
# todo(#6071): our executeprocessrequest expects a specific string type for arguments, </s> str_jvm_options = [text_type(opt) for opt in self.get_options().jvm_options]	_runtool_hermetic initial_args = [native_image_path] else: additional_snapshots = [] initial_args = [
# todo tmp return, to unify with monitor auto-fetch later </s> return loss, explore_var	Agent loss = self.algorithm.train() explore_var = self.algorithm.update() def close(self): model_for_loading_next_trial = 'not implemented'
1  # todo: fill in identifier </s> )	profile_transfer amount, target, result.wait() profiling.print_all_threads()
# todo: numba or cython to improve performance of this kernel </s> def _der_local_values_kernel(	_der_local_values_kernel log_vals, log_val_p, mels, der_log, der_log_p, sections, out ):
# todo(necula): produces mismatched outputs on gpu. </s> jax2tflimitation("mismatched outputs on gpu",	igammac rtol=tol) return [ devices=("gpu",), skip_comparison=True), missing_tf_kernel(
# todo add shape check </s> raise notimplementederror	_jac_t_mat_prod def _jac_t_mat_prod(self, module, g_inp, g_out, mat):
# todo(crcrpar): annotate this correctly. </s> @functools.wraps(func)	new_func def new_func(*args: Any, **kwargs: Any) -> Any: warnings.simplefilter('always', UserWarning)
# todo: look at args for remotedata </s> workers = frequencies(w for dep in deps	decide_worker 'bob' deps = dependencies[key] for w in who_has[dep]) if not workers:
# todo: remove once typeshed supports literal types </s> assert isinstance(ov, _winapi.overlapped)	read while True: ov, err = _winapi.ReadFile(self.connection, self.READ_SIZE, overlapped=True) assert isinstance(err, int) try:
# todo switch to transform </s> return window(self.affine, left, bottom, right, top,	window def window(self, left, bottom, right, top, boundless=False): height=self.height, width=self.width, boundless=boundless)
# todo test </s> self.unconstrained_effect_repertoire(purview))	effect_info return utils.emd(self.effect_repertoire(mechanism, purview),
logg.error("too long") #todo </s> self.end(266)	test_5137_usermode_notify_service_functions_with_reload_user self.rm_testdir() self.coverage()
# todo: unittest before committing </s> raise valueerror, "%r in %s contains %s not a dataset.  " \	from_hdf5 res = hdf2obj(dsgrp) if not isinstance(res, AttrDataset): "File contains groups: %s." \ % (name, source, type(res), hdf.keys())
# todo: remove this compatibility layer with rally 1.1.1 </s> index_info = self._client.get_index(self._index)	open self._client.create_index(index=self._index) else: if "metrics" in index_info[self._index]["mappings"]: old_index_name = self._index
raise exceptions.mpdnotimplemented  # todo </s> underscore, dash, dot and colon.	subscribe already. The name may consist of alphanumeric ASCII characters plus
# todo: does updating sigma here (as opposed to after regret) miss out </s> vo = 0.0	cfr I = state.info_set calculate_strategy(regret, sigma, I, state) voa = {} for a in state.legal_actions:
# todo: kwargs </s> def _impl(df, axis=none, skipna=none, level=none, numeric_only=none,	prod_overload def prod_overload(df, axis=None, skipna=None, level=None, numeric_only=None, min_count=0): min_count=0): return hpat.hiframes.pd_dataframe_ext.prod_dummy(df)
# todo check default function methods and return them </s> result = []	follow_path else: if isinstance(scope, parsing.Function): else: result = strip_imports(get_scopes_for_name(scope, current))
pass # todo </s> def test_selectmultiplefield():	test_SelectMultipleField
# todo not implemented yet </s> return	move_current_view_to_far_left Currently only supports 2 row or 2 column layouts. if self.window.num_groups() > 2: if self.window.num_groups() < 2: return
# todo: report </s> msg.set_personality(self.personality)	handle_io_in logger.debug("Got {} bytes, Used {} bytes".format(len(data), len_used))
# todo: deprecate `pages` in favor of `page_limit` since it is less confusing </s> if 'pages' in kwargs:	get_posts raise ValueError("You need to specify either account or group") _scraper.requests_kwargs['timeout'] = kwargs.pop('timeout', DEFAULT_REQUESTS_TIMEOUT) kwargs['page_limit'] = kwargs.pop('pages') if credentials is not None:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: wrap backend call in error handling. </s> return backend.playback.get_time_position().get()	get_time_position backend = self._get_backend(self.get_current_tl_track()) if backend: else: return 0
# todo: replace with stream-changed </s> logger.debug('triggering track playback started event')	_trigger_track_playback_started def _trigger_track_playback_started(self): if self.current_tl_track is None: return
# todo: unescape values </s> filename, _ = get_filename_and_fobj(filename_or_fobj)	import_from_ods def import_from_ods(filename_or_fobj, index=0, *args, **kwargs): ods_file = zipfile.ZipFile(filename) content_fobj = ods_file.open('content.xml')
'''todo: add docs''' </s> d = {}	scatter_args @property def scatter_args(self): d['tools'] = 'pan,wheel_zoom' d['data'] = self.model.df
# todo: implement </s> def adddelta(self, diff):	addDelta
# todo: currently can't initialize hadooprunner without setting these </s> runner = hadoopmrjobrunner(	test_prefer_own_methods def test_prefer_own_methods(self): hadoop_bin='hadoop', hadoop_home='kansas',
# todo: if the main thread receives a signal and we have no timeout, we </s> io_events = poller.poll(timeout)	SingleWaitForFdCondition poller.register(fdobj, event) try: except select.error, err: if err[0] != errno.EINTR:
# todo - molecule type - see issue 363 / pull request #1005 </s> d = consumer.data.annotations.get('data_file_division', none)	test_topology_genbank self.assertEqual(t, topo, "Wrong topology %r not %r from %r" % (t, topo, line)) self.assertEqual(d, div, "Wrong division %r not %r from %r" % (d, div, line))
# todo(ahundt) softmax is pre-applied, so need different train, inference, evaluate. </s> return densenet.densenetfcn(input_shape=input_shape,	DenseNet_FCN include_top=False): if include_top is True: weights=None, classes=classes, nb_layers_per_block=[4, 5, 7, 10, 12, 15],
# todo: add this back in once we've merged back the refactored users code </s> self.assertequals(commcare_users_count, 1)	testUnlinkCommCareUser commcare_users_count = CouchUser.view("users/commcare_users_by_domain_username", key=[domain, commcare_username]).total_rows
# todo: move this to a common test module (tests/common.py?) </s> def popen_python(command_arg_list):	popen_python Run subprocess.Popen() to produce a process running a Python interpreter. Uses the same Python interpreter that the current process is using, via
# todo pydocs </s> def __init__(self, service, project_id):	BigQueryBaseCursor class BigQueryBaseCursor(object): self.service = service self.project_id = project_id
# todo: perhaps something more self-documenting for variables names? </s> a = 133	test_trade_feed_protocol def test_trade_feed_protocol(self): b = [10] * 4 c = [100] * 4
# todo: add tcp-like buffering </s> pass	write self.protocol.connectionRefused() elif no == EAGAIN: else: raise
# todo: if py3k, override unpickler.find_class(). </s> pass	_aux_cache_init mypickle.find_global = None except AttributeError: aux_cache = mypickle.load() f.close()
def __init__(self, p_args, p_todolist, #pragma: no branch </s> p_out=lambda a: none,	__init__ p_err=lambda a: None, p_prompt=lambda a: None):
# todo ... </s> if isinstance(token, cclosingbracket):	cpre3_parse curCObj = _CBaseWithOptBody(parent=parent) elif state == 60: # func if token.brackets == curCObj._bracketlevel: state = 0
# todo: update graph references </s> elif src is none:	MetaCollection lgr.error(e_msg) raise TypeError(e_msg) pass else:
# todo we need to work on the unit from the exposure. </s> exposure = exposure.keywords['exposure']	generate_classified_legend else: hazard_unit = None exposure_definitions = definition(exposure) exposure_unit = exposure_definitions['units'][0]['abbreviation']
# todo: sync with link.smart_link() to choose a linker </s> linkers = { 'cxx': ['g++'], 'cc': ['gcc'] }	exists def exists(env): alltools = [] for langvar, linktools in linkers.items():
# todo: support speedy mode for running the script </s> shell("make scriptconfig script=kconfiglib/examples/allnoconfig.py")	test_all_no 'make scriptconfig' and needs to reparse the configurations, so kinda slow even in speedy mode.""" shell("mv .config ._config") if speedy_mode:
# todo: check </s> projections = entity_embs - torch.sum(torch.mul(normal_vec_embs, entity_embs), dim=1)	project_to_hyperplane def project_to_hyperplane(self, entity_embs, normal_vec_embs): return projections
# todo(huanxuan): remove this if condition once the fixed </s> if not hasattr(_quota.quotadefault, 'project'):	test_quota_show_with_default ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) self.cmd.take_action(parsed_args) self.quotas_mock.defaults.assert_called_once_with(
# todo: edge dps could use a different forwarding algorithm </s> eth_src = pkt_meta.eth_src	_edge_dp_for_host Returns: Valve instance or None (of edge datapath where packet received) vlan_vid = pkt_meta.vlan.vid for other_valve in other_valves:
# todo add method if no aws creds </s> list_bucket_perm_allowed = true	check_perm_list_bucket if not self.aws_creds_configured: raise NotImplementedError("check_perm_list_bucket not implemented for no aws creds") try: self.s3_client.list_objects_v2(Bucket=bucket.name, MaxKeys=0)
# todo: check output </s> def test_hierarchy_iprint2(self):	TestSolverPrint prob.setup(check=False) output = run_model(prob) prob = Problem() model = prob.model
# todo(nakago): check why tolerance is high </s> def test_backward_cpu(model, data):	test_backward_cpu atom_data, adj_data, y_grad = data gradient_check.check_backward(model, (atom_data, adj_data), y_grad,
# todo: conflict detection/resolution </s> for key in d:	_collect_analysis if not d: continue analysis.setdefault(key, {}).update(d[key]) return analysis
# todo: cannot be loaded with plugins; improve this solution </s> d = helpers.import_from_plugin("googleapiclient.discovery", plugin="bigquery")	create_dialect def create_dialect(self, resource, *, descriptor): try: if isinstance(resource.source, d.Resource): return BigqueryDialect(descriptor)
# todo: deprecated - remove in version 0.10 </s> if isinstance(training_trackers, string_types):	train "Pass appropriate featurizer " "directly to the policy instead.") logger.warn("Passing a file name to `agent.train(...)` is " "deprecated. Rather load the data with "
# todo: handle marker? </s> for func in funcs['functions']:	_find_function conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) funcs = conn.list_functions() if func['FunctionName'] == name: return func
#todo : manage more than just quit </s> def manage_signal(self, sig, frame):	Satellite print "Allocate : ", w.id self.workers[w.id].start() print "\nExiting with signal", sig for w in self.workers.values():
# todo: come up with a proper detect() routine...and enable it. </s> return 1	exists def exists(env):
# todo: we need to insert a linebreak here, but there is no </s> t0 = t0 + "\n"	copy_chunk if chunk[2] >= b0.get_line_count() and \ chunk[3] < b1.get_line_count(): insert_with_tags_by_name(b1, chunk[3], t0, "edited line") else: # copy down
# todo: support preferences file in backup under linux (is in different directory). </s> version_data_dir = resources.getdatastoragepath()	restore Logger.log("w", "Tried to restore a Cura backup without having proper data or meta data.") return False archive = ZipFile(io.BytesIO(self.zip_file), "r") extracted = self._extractArchive(archive, version_data_dir)
# todo: not sure if this is pg only or standard </s> "alter table t alter column c drop not null"	test_alter_column_nullable op.alter_column("t", "c", nullable=True) context.assert_(
# todo crossplataform?: </s> serverpath = os.path.join(path, str(options['job_id']))	run_blender_in_thread for fname in filenames: filepath = os.path.join(dirpath, fname) file_ = open(filepath, 'rb') ftp.storbinary('STOR '+str(fname), file_)
# todo setup mahout, must be checked out from repo atm: </s> pass	_setup_hadoop http://archive.cloudera.com/docs/ec2.html http://archive.cloudera.com/cdh/3/
# todo: "annotator" is a very confusing term for a web service </s> return projectconfiguration(directory).get_annotator_config()	get_annotator_config def get_annotator_config(directory):
# todo(amaranth-0.4): remove </s> if ports is none:	convert def convert(elaboratable, name="top", platform=None, ports=None, *, emit_src=True, **kwargs): warnings.warn("Implicit port determination is deprecated, specify ports explictly", DeprecationWarning, stacklevel=2)
# todo: temporary thing (there is no uia based wrappers tree yet) </s> return uiawrapper	find_wrapper def find_wrapper(element): "Find the correct wrapper for this UI element"
# todo: avoid flush all caches </s> pwndbg.memoize.reset()	add_custom_page def add_custom_page(page): bisect.insort(custom_pages, page)
# todo(toshihikoyanase): remove the constraints when tensorflow==2.7.0 is released. </s> "tensorflow-estimator<2.7.0",	get_extras_require "scikit-optimize", "xgboost", "tensorflow<2.7.0", "tensorflow-datasets",
# todo: implement persistence for configuredname </s> input_source.configure_char('configuredname', value=source_name)	__init__ input_source.configure_char('Name', value=source_name) input_source.configure_char('Identifier', value=idx + 1) input_source.configure_char('InputSourceType', value=source_type) input_source.configure_char('IsConfigured', value=1)
# todo: should not assume presence of any kind of parameter </s> if 'chunksizes' not in param:	write_dataset_to_storage for band, param in variable_params.items(): output = {} raise DatacubeException('Missing `chunksizes` parameter, cannot write to storage.') output['chunk_size'] = self._get_chunksizes(param['chunksizes'])
# todo: include a validation step for x </s> self._x = reduce(iconcat, x, [])	fit ------- self : object self._X = [(node_info[0], *node_info[1]) for node_info in zip(range(len(self._X)), self._X)]
# todo(nmakhotkin): simplify calculating task output. </s> e_data = raw_result.error	get_for_each_output out_key = (task_spec.get_publish().keys()[0] if task_spec.get_publish() else None) output = expr.evaluate_recursively( task_spec.get_publish(),
return  # todo return placeholder "[loading]" album? </s> if sp_album.artist is not none:	to_album def to_album(sp_album): if not sp_album.is_loaded: artists = [to_artist(sp_album.artist)] else:
# todo: try to figure out the right lexer for these files </s> markup_a = apply_pygments(old or '', filediff.source_file)	get_chunks if enable_syntax_highlighting: try: markup_b = apply_pygments(new or '', filediff.dest_file) except ValueError:
# todo: rebalance if output distributions are 1d instead of 1d_var </s> df_vars = filter_node.df_vars	filter_distributed_run def filter_distributed_run(filter_node, typemap, calltypes): df_in_vars = df_vars[filter_node.df_in] df_out_vars = df_vars[filter_node.df_out]
# todo: this comparison should happens only if users </s> message = wrong_reason_template.format(	check if reason and self.failures: if isinstance(self.failures, EnumMeta) and not isinstance(reason, Enum): reason=reason, available=self.available,
# todo: run filters here. </s> print(">", data)	_proxy_out_request else: data = [] self.libusb_device.ctrl_transfer(req.request_type, req.request, req.value, req.index, data)
# todo integrate into keypoints </s> def project_coords(coords, from_shape, to_shape):	project_coords Project coordinates from one image shape to another. This performs a relative projection, e.g. a point at 60% of the old
self.assertequals(status, 200) # todo: should be 202 </s> status, body = self.post(path, body)	test_install options=options,)
# todo: parse human-friendly logmaxsize ... e.g. 10mb </s> logmaxsize = config.getint('general','logmaxsize')	get_logconfig else: try: x = config.get('general','logbackups') if x == '<inf>':
# todo make atomic </s> graph = tx.graph	Cog self.__db_merge__(tx) def __db_delete__(self, tx): for related_objects in self.related.values(): related_objects.clear()
# todo: deprecate </s> self.init_jwt_config(app.config)	init_app self.config.setdefault('error_uris', app.config.get('OAUTH2_ERROR_URIS')) if app.config.get('OAUTH2_JWT_ENABLED'):
# todo(b/186451541): reduce the number of calls to model_fn. </s> self.assertequal(mock_model_fn.call_count, 4)	test_construction_calls_model_fn mock_model_fn = mock.Mock(side_effect=model_examples.LinearRegression) federated_sgd.build_federated_sgd_process(model_fn=mock_model_fn)
# todo transition this once pushy is out </s> rconn = connection(hostname, logger, sudo=true)	create common.mon_create(distro, logger, args, monitor_keyring, hostname) service = common.which_service(distro.sudo_conn, logger) process.run( rconn,
# todo remove in next major version 5.0.0 see serializers.reservedfieldnamesmixin </s> if serializer_class is not none:	parse serializer_class = getattr(view, "serializer_class", None) parsed_data = {"id": data.get("id")} if "id" in data else {} if issubclass(serializer_class, serializers.PolymorphicModelSerializer): parsed_data["type"] = data.get("type")
#todo - complete implementation of these apis </s> return faults.fault(faults.portnotfound(e))	get_resource except exception.PortNotFound as e:
# todo: enable admin tests </s> test_api(ctx)	test_travis_else jshint(ctx)
# todo: use get_cstr_and_len instead of getitem </s> _str = str_arr[ind_arr[i]]	str_arr_arr_impl n_chars = 0 for i in range(n): n_strs += 1 n_chars += len(_str)
# todo: refactor this to avoid additional lookup in cast_params </s> action_ref = self.rule.action['ref']	_do_enforce def _do_enforce(self): action_db = action_utils.get_action_by_ref(action_ref) if not action_db:
# todo: will need some code for the tabs active terms to work </s> self.get_toplevel().last_active_term = none	on_vte_focus_in self.terminator.last_focused_term = self if self.get_toplevel().is_child_notebook(): else: self.get_toplevel().last_active_term = self.uuid
# todo(albert): implement stub. </s> pass	serialize RETURNS: Test
# todo: assert </s> self.asserttrue(result)	test_remove_repo Test: remove a repo object result = self.remote.remove_repo("testrepo0", self.token)
# todo: use different flag than .reentrant </s> pos = colorsorter._transform_point(pos)	schedule_sphere color, pos, radius, ColorSorter._debug_transforms() ##### if ColorSorter._parent_csdl and ColorSorter._parent_csdl.reentrant: if drawing_globals.use_c_renderer and ColorSorter.sorting: if len(color) == 3:
# fixme todo replace with proper url section joining taking unicode inputs </s> url = base_url + \	parse_test val = lowercase_keys(configvalue)[u'template'] assert isinstance(val, basestring) or isinstance(val, int) unicode(val, 'UTF-8').encode('ascii', 'ignore') mytest.set_url(url, isTemplate=True)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_pass_optional_parameters_excluded def test_pass_optional_parameters_excluded(self): Request omits optional parameters.
# todo add shape check </s> def sqrt_hessian(self, module, g_inp, g_out):	sqrt_hessian return self._sqrt_hessian(module, g_inp, g_out)
#todo: not integrated with cbar yet... </s> is_failed = true	get_axes_by_nodes :func:`pyNastran.bdf.cards.elements.bars._rotate_v_wa_wb` for a description of the OFFT flag. eid = self.eid elem = self
# todo: handle non numpy alloc types </s> def f(a):	_handle_map and func.op == 'make_function'): raise ValueError("lambda for map not found") S = np.empty_like(A) for i in numba.parfor.internal_prange(len(A)):
# todo: make previous blocking instead of sleep </s> time.sleep(0.1)	shareConstant while all(key in elements[scoop.worker] for key in kwargs.keys()) is not True: _control.execQueue.socket.pumpInfoSocket()
# todo: cat, slice, array, clocksignal, resetsignal, memory </s> raise notimplementederror	_eval return str2op[node.op](*operands) else:
# todo: find out how to get global usernames </s> def block(mastodon, rest):	block @command
#todo the tolerance needed to pass is very high for float32(0.16). is this acceptable? expected? </s> return out	testf unroll_kern=un_k)
# todo: is there a better way to check this? </s> return isinstance(val, int) or isinstance(val, float) or isinstance(val, long)	isnum def isnum(val):
# todo -- we might need to expanduser taking .user into account </s> out = createsibling._run_on_ds_ssh_remote(	_get_ds_remote_shared_setting shared = None try: ds, name, ssh, 'git -C {path} config --get core.sharedrepository'
# todo. percentile </s> if self.sampling == 'all':	fit distances_ = distances.data.numpy() distances_avg += np.mean(distances_) anchors, positives, negatives = self.batch_all( batch['y'], distances)
# todo: add .data and .grad to syft tensors </s> for p in nn_self.parameters():	module_float_precision_ def module_float_precision_(nn_self): parameters to normal float parameters""" p.float_precision_() return nn_self
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: only handle it if the trigger refers to the current sensor </s> trigger = self._sanitize_trigger(trigger=trigger)	_handle_update_trigger def _handle_update_trigger(self, trigger): self._sensor_instance.update_trigger(trigger=trigger) pass
name='unknown',  # todo currently not storing requester </s> summary=' '.join(summary),	expected_notification_message ) return const.notification_template(req.get_next_action().action).format( loc=self.sp.location.site_code, keyword=req.get_next_action().keyword
# todo: let's see if we can find sane versioning for `latest` from upstream </s> if image_config['tag'] == 'latest':	ChartReleaseService image_config = release_data['config'].get('image') or {} if all(k in image_config for k in ('tag', 'repository')): app_version = f'{image_config["repository"]}_{image_config["tag"]}' else:
pass # todo </s> def _listall(self, uri):	_listall @register(r'^listall "(?P<uri>[^"]+)"')
# todo: index arg? </s> func_text += "  arr = hpat.pio_api.h5_read_dummy(dset, {}, '{}')\n".format(tp.ndim, dtype_str)	_run_assign dtype_str = str(tp.dtype) func_text = "def _h5_read_impl(dset, index):\n" loc_vars = {} exec(func_text, {}, loc_vars)
# todo subject.cn from cert? </s> shutil.rmtree(app_path)	test_simple_app lib_hashes = self.assert_common_signed_hashes(lib_info, -2, -1) assert '-3' not in lib_hashes return app_info
#todo only apply this filter if set by the user </s> if sum(none in g for g in gts) >= args.min_call_rate * len(vcf_samples): continue	main if variant.stop - variant.start > args.max_mb * 1000000: continue gts = [s.get("GT", (None, None)) for s in variant.samples.values()] if sum(sum(x) >= 1 for x in gts if not None in x) > args.max_hets: continue if not any(sum(x) > 0 for x in gts if not None in x): continue
# todo: remove in v.0.6 </s> x = np.array([[0, 0], [0, 1], [2, 0], [2, 1]])	TestLSML self.assertLess(csep, 0.8)  # it's pretty terrible def test_deprecation_num_labeled(self): y = np.array([1, 0, 1, 0]) lsml_supervised = LSML_Supervised(num_labeled=np.inf)
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_METADATA_font_filename_canonical def test_result_METADATA_font_filename_canonical(self): r = run_set(p, 'result') success_tests = r['success']
# todo(t2r_contributors): switch to using gin config for all saver params. </s> return optimizer.swapping_saver(	create_swapping_saver save_relative_paths = False, ): keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours, save_relative_paths=save_relative_paths)
# todo(cmaloney): switch to a sane http server </s> def wsgi_app(env, start_response):	run_server try: marathon.add_subscriber(callback_url) length = int(env['CONTENT_LENGTH']) data = env['wsgi.input'].read(length)
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> sampleparameters = {	test_acquire_token_with_user_pass def test_acquire_token_with_user_pass(self): "tenant" : "common", "authorityHostUrl" : "https://login.windows.net",
# todo: test that valueerror is raised </s> validate.direction(direction)	reducible _from, to = mechanism, purview else: return utils.block_reducible(cm, _from, to)
evaluation.message('setstreamposition', 'todo2', name)   #todo </s> return symbol('$failed')	apply_input seekpos = m.to_python() if not ((isinstance(seekpos, int) and seekpos >= 0) or seekpos == 'Infinity'): if seekpos == 'Infinity': tmp = stream.seek(0, 2)
# todo(b/129148632): the current apache-beam 2.11.0 do not work with py3 </s> skip_beam_test = bool(six.py3)	_download_and_prepare_as_dataset ): if isinstance(builder, dataset_builder.BeamBasedBuilder): if skip_beam_test: return
# todo: hand derive these results. </s> expected_sharpe = {	test_daily_buy_and_hold third_date: -0.050018 } first_date: np.nan, second_date: -1.630920,
# todo: refactor </s> if mode is none:	_init_iterator raise ValueError('data_specs not provided and no default data ' 'spec set for %s' % str(self)) if hasattr(self, '_iter_subset_class'): mode = self._iter_subset_class
# todo: ensure it's up to date </s> return self._model_change_counter	model_change_counter def model_change_counter(self): #bruce 080731
# todo: have the transform on this label be less hacky </s> label_group = object("g").append('"text"') \	build_js draw += yaxis_group if self.label: .add_attribute("text", '"%s"'%self.label) \ .attr('"transform"', '"translate(-" + (margin.left-20) + ", " + (height/2+20) + "), rotate(-90,0,0)"')
# todo(jamalex): burn it all down! </s> if lang_code == "pt-br":	get_dubbed_video_map logging.debug("Adding dubbed video map entry for %s (name=%s)" % (get_langcode_map(lang_name), lang_name)) DUBBED_VIDEO_MAP[get_langcode_map(lang_name)] = video_map lang_code = "pt" return DUBBED_VIDEO_MAP.get(lang_code, {}) if lang_code else DUBBED_VIDEO_MAP
# todo: pace ourselves (send through the uploader...) </s> for buf in changed_bufs:	_initial_upload for buf in missing_bufs: self.send({'name': 'delete_buf', 'id': buf['id']}) self.send({ 'name': 'set_buf',
# todo filter chamber in case person went between chambers same day? </s> role = person.get_role_at_date(joined_date, congress=bill.congress)	get_role_for @staticmethod def get_role_for(person, bill, joined_date): if role is not None: return role for role in person.roles.all():
# todo: handle `stream.close` and `stream.reset` </s> peer_id = stream.mplex_conn.peer_id	Node await stream.close() async def _handle_beacon_blocks(self, stream: INetStream) -> None: if peer_id not in self.handshaked_peers: self.logger.info(
# todo: perhaps something more self-documenting for variables names? </s> a = 133	test_trade_feed_protocol def test_trade_feed_protocol(self): b = [10] * 4 c = [100] * 4
# todo: support minp arg end_range etc. </s> minp = win	roll_sum_fixed N = len(in_arr) output = np.empty(N, dtype=np.float64) range_endpoint = max(minp, 1) - 1 for i in range(0, range_endpoint):
# todo: should this raise an exception if the script didn't finish? </s> if current_mem[has_finished_address] == 0:	inject_asm_into_rom another_mem[has_finished_address] = cached_wram_value self.vba.memory = another_mem return False elif current_mem[has_finished_address] == 1:
# todo: get file name from user. </s> try:	_on_export_open def _on_export_open(self, menu_item): self.controller.export_project_file(filename=None, openafter=True) except Exception, exc:
# todo: must be implemented </s> pass	range_from_chapters def range_from_chapters(crawler, times=0):
# todo: replace and deprecate? </s> mr = self.mr_from_search(cls, query)	Manager return the results as instances of cls (or as keys if return_keys is set to True).""" if not return_keys: mr = mr.map(function="""
# todo: check lens </s> ma = s1.mean()	_column_cov_impl S1 = hpat.hiframes_api.to_series_type(A) S2 = hpat.hiframes_api.to_series_type(B) mb = S2.mean() return ((S1-ma)*(S2-mb)).sum()/(S1.count()-1.0)
# todo: remove after implementing django-constance </s> if not dfirtrack_config.csv_skip_existing_system:	system return redirect(reverse('system_list')) else: messages.warning(request, 'WARNING: Existing systems will be updated!') form = SystemImporterFileCsvConfigbasedForm()
# todo i think this is a hack. it should be an </s> c = er.wrap(self._evaluator, c.parent.parent).name.names[-1]	completions if not filter_private_variable(s, user_stmt or self._parser.user_scope(), n): if isinstance(c.parent.parent, (pr.Function, pr.Class)): new = classes.Completion(self._evaluator, c, needs_dot, len(like), s) k = (new.name, new.complete)  # key
# todo(obondarev): use neutron_lib constant </s> @resource_extend.extends(['ports_bulk'])	_extend_port_resource_request_bulk @staticmethod def _extend_port_resource_request_bulk(ports_res, noop): min_bw_rules = dict()
# todo find better documentation </s> ql.dprint("[!] sample is checking system language!")	hook_GetSystemDefaultUILanguage }) def hook_GetSystemDefaultUILanguage(ql, address, params): lang = 0x445 return lang
# todo: remove this - cura-4482 </s> if self._add_global:	__updateExtruders global_container_stack = Application.getInstance().getGlobalContainerStack() if global_container_stack: material = global_container_stack.material color = material.getMetaDataEntry("color_code", default = self.defaultColors[0]) if material else self.defaultColors[0]
# todo check </s> return self.mimetype	format @interfacedoc def format(self):
raise notimplementederror #todo </s> else:	__call__ root = doc.data[0] #will select the text element for e in self.targets(root): raise NotImplementedError #TODO
# todo: provide a kernel which will describe how coordinates are extruded. </s> mesh = firedrake.extrudedmesh(m, layers, layer_height=0.1)	integrate_p0 m = UnitSquareMesh(2 ** power, 2 ** power) layers = 11 fs = firedrake.FunctionSpace(mesh, family, degree, name="fs") f = firedrake.Function(fs)
# todo: python-2442 use _asdict() instead </s> return {	_options_dict def _options_dict(self): 'document_class': self.document_class, 'tz_aware': self.tz_aware,
# todo: maybe we should open this one to the users, as it lets them </s> self.filter.r *= 4.	setup_kf self.filter.F[p, p + dim_z] = dt self.filter.H = np.eye(dim_z, dim_x,) self.filter.x[:dim_z] = np.expand_dims(initial_detection.flatten(), 0).T
# todo(mordred) when this changes to rest, force interface=admin </s> with _utils.shade_exceptions(	create_endpoint if region is not None: kwargs['region'] = region "Failed to create endpoint for service" " {service}".format(service=service['name'])
# todo: remove compatability hooks </s> lockfile = os.path.join(target,esky_control_dir,"lockfile.txt")	uninstall_version target = os.path.join(self.appdir,target_name) assert os.path.dirname(target) == self.appdir if not os.path.exists(lockfile): lockfile = os.path.join(target,"esky-lockfile.txt")
# todo: test this! </s> gpt_bios_grub_part_size = 1	run key_files = ["/tmp/.keyfile-root", "/tmp/.keyfile-home"] if self.GPT: efisys_part_size = 512 empty_space_size = 2
# todo(dolph): remove this check after default-domain migration </s> if tenant_ref.get('domain_id') is not none:	validate_auth_info LOG.warning(msg) raise exception.Unauthorized(msg) project_domain_ref = self.identity_api.get_domain( context,
# todo: assert metrics. </s> assert not error	test_get_metrics error = repository.get_metrics(commit, metrics["spaces"])
# todo: make truly async </s> self._resourcemanager_client = discovery.build('cloudresourcemanager', 'v1', cache_discovery=false, cache=memorycache())	__init__ def __init__(self):
# todo add checks for source space </s> if stc1.data.shape != stc2.data.shape:	_check_stc def _check_stc(stc1, stc2): raise ValueError('Data in stcs must have the same size') if np.all(stc1.times != stc2.times):
# todo(porg) not sure if created should ever throw... maybe warning/log? </s> created = x.created	_iterate for x in o.iterate(): try: except Exception as e: created = None
# todo: move install_time away from app_setting </s> app_setting(app_id, 'update_time', now)	app_upgrade else: now = int(time.time()) status['upgraded_at'] = now with open(app_setting_path + '/status.json', 'w+') as f:
#todo - check for or silence the expected warning? </s> continue	test_the_translate continue if len(example1) % 3 != 0: try : tran = example1.translate()
# todo: 'ignore_status' could/should be removed when globalres.log is </s> ret = runcmd(['du', '-s', path], ignore_status=true)	measure_disk_usage def measure_disk_usage(self, path, name, legend): if ret.status: log.error("du failed, disk usage will be reported as 0")
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
# todo: it would be nice to be async about this. set 1 second timeout. </s> try:	version_check 'version': bayeslite.version.__version__ } r = requests.post(SERVICE, data=json.dumps(payload), timeout=1) if FAIL_VERSION_CHECK or r.status_code == 200 and r.json.result != "current":
# todo setup mahout, must be checked out from repo atm: </s> pass	_setup_hadoop http://archive.cloudera.com/docs/ec2.html http://archive.cloudera.com/cdh/3/
# todo change when v4 web3.py will released </s> assert miner_id == escrow.call().getminerid(miner, 1).encode('latin-1')	test_miner_id chain.wait.for_receipt(tx) assert 2 == escrow.call().getMinerIdsCount(miner)
# todo send the key to the master for approval </s> sh_ = '/bin/sh'	seed bs_ = __salt__['config.gather_bootstrap_script']() salt.crypt.gen_keys(mpt_tmp, 'minion', 2048) if os.path.isfile(os.path.join(mpt, 'bin/bash')): sh_ = '/bin/bash'
# todo: remove this when fixed in: https://github.com/seleniumhq/selenium/issues/767 </s> self.browser.service.process.send_signal(signal.sigterm)	teardown_browser def teardown_browser(self): self.browser.close() self.browser.quit()
# todo: support multi-index columns </s> kdf["a"].plot(kind="pie"), express.pie(pdf, values=pdf.columns[0], names=pdf.index),	test_pie_plot self.assertEqual(
# todo private access.. </s> argument_iterator = field_tree_instance._arguments.unpack()	_get_foreign_key_values def _get_foreign_key_values(cls, field_tree_instance): if isinstance(field_tree_instance, TreeInstance): key, lazy_values = next(argument_iterator, (None, None)) if key is None and lazy_values is not None:
return 'ok' # todo should be a json or something </s> viewer.status = 'running'	play def play():
# todo: is write=true a reasonable way to do it? </s> except oserror, e:	git_add try: self.repo.index.add(files, write=True) lgr.error("git_add: %s" % e) raise
# todo investigate different results between mac and linux/win platforms </s> return fixoutput(retinfo)	test_low_il_ssa retinfo.append("Function: {:x} Instruction: {:x} LLIL_SSA->MLILS: {}".format(func.source_function.start, ins.address, str(sorted(list(map(str, ins.mlils)))))) retinfo.append("Function: {:x} Instruction: {:x} LLIL_SSA->HLIL: {}".format(func.source_function.start, ins.address, str(ins.hlil)))
# todo debug </s> print logstring	logRule logString = "%s %s" % (spaceString, ruleElement.element.type) logging.info("[%s]: %s" % (fileName, logString)) spaceString += "   " for item in ruleElement.element.elements:
# todo: invalidate cache for former latestappinfo </s> super().save(	GlobalAppConfig return cls.by_app(app) def save(self, force_insert=False, force_update=False, using=DEFAULT_DB_ALIAS, update_fields=None): force_insert=force_insert, force_update=force_update, using=using, update_fields=update_fields )
# todo -- can we do this without a subscription? </s> if not api.use_store:	play_similar_song_radio @ask.intent("GeeMusicPlaySimilarSongsRadioIntent") def play_similar_song_radio(): return statement(render_template("not_supported_without_store")) if len(queue.song_ids) == 0:
## todo delete me? </s> :type int	createPreview
# todo replace with collections.sequence subclass </s> spotify.error.maybe_raise(self.error)	tracks def tracks(self): Will always return :class:`None` if the search isn't loaded. if not self.is_loaded: return None
# todo: reproduce and submit traceback to issue 41 </s> ipaddr = str(ipaddr)	_update return Response('nohost') try: kind = check_ip(ipaddr, ('ipv4', 'ipv6')) except (ValueError, UnicodeError):
# todo: refactor accordingly when v3 websocket api is released </s> trading_pair = f"{trading_pair.split('-')[1]}-{trading_pair.split('-')[0]}"	BittrexAPIOrderBookDataSource trading_pairs = await self.get_trading_pairs()  # Symbols of trading pair in V3 format i.e. 'Base-Quote' for trading_pair in trading_pairs: self.logger().info(f"Subscribed to {trading_pair}") hub.server.invoke("queryExchangeState", trading_pair)
# todo: remove requestor and integrate with wallet from gateway </s> async def evaluate_transaction(self, name, args, requestor):	Contract wait_for_event=True) return response Evaluate a transaction function and return its results. The transaction function will be evaluated on
# todo is there a way to actually test that the creds work? </s> pass	verify_google_credentials @collect_credential(token="GOOGLE_API_KEY") def verify_google_credentials(config: Config, system: System, *, token: str) -> None:
# todo: write me </s> raise notimplementederror('limiteddisk cache does not yet implement the .remove() method.')	remove def remove(self, layer, coord, format):
"""todo: not implemented""" </s> notimplementederror("near not implemented")	near @symbolic_dispatch def near(x):
# todo: scalar, min is a workaround </s> return vals.min()	kurtosis else: if vals.ndim == 0: return vals
# todo: replace this with a report api </s> print "[downloaded] - %s"%imgurl	handle_imageurls print "[FAILURE] - "+imgurl raise gen.Return() img = PIL.Image.open(response.buffer) w, h = img.size
# todo use deepcopy() here </s> return polygonsonimage(polygons, shape)	on else: polygons = [poly.project(self.shape, shape) for poly in self.polygons]
# todo: parse text </s> link, title = def_links.get(key)	parse_ref_link if not def_links or key not in def_links: return self.renderer.text(line) if line[0] == '!': return self.renderer.image(link, text, title)
# todo: remove when we stop supporting python < 3.5 </s> if sys.version_info.major < 3 or sys.version_info.minor < 5:	get_metric def get_metric(self): check_is_fitted(self, 'components_') else:
# todo: test this </s> style = element.style	handle_computed_display_float Computed values of the display and float properties according to http://www.w3.org/TR/CSS21/visuren.html#dis-pos-flo if get_value(style, 'display') == 'none': return # position and float do not apply, but leave them
# todo: remove this (ssh_user is a legacy arg) </s> user = _user_or_ssh_user(user, ssh_user)	command user='vagrant', ) command = shlex_quote(command) connection_target = hostname
# todo: process bind. </s> return binds	bind myid = bundle.getid() binds = bindings.bind.find_by_id(myid, repoid)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: write this method if possible </s> pass	address_transactions def address_transactions(self, addresslist):
# xxx todo </s> raise notimplementederror()	add_to_postmortem_exclusion_list elif bits not in (32, 64): raise NotImplementedError("Unknown architecture (%r bits)" % bits)
# todo: add logger here </s> print("save_raw_output")	save_raw_output sessions.destroy_session(session) except Exception as e: print(e) raise e
# todo: remove "get_" from the name </s> get the nodes of the graph with the specified node types.	get_nodes_of_type def get_nodes_of_type(self, node_type=None): Args: node_type:
#todo: assuming constant mu </s> if getattr(self, '_memu', none) is none:	MeMu @property def MeMu(self): self._MeMu = self.mesh.getEdgeInnerProduct(self.mu) return self._MeMu
# todo: need to update this so that it flushes bulk queue </s> print "need to implement this!"	get_indices return self.es_connection.indices.stats()['indices'].keys() else: raise NotImplementedError
# todo: remove deprected flags in 1.2 </s> self._deprecated_partial_fit, self._deprecated_fit = true, false	partial_fit self Fitted estimator. if X is None: self._global_clustering()
# todo: add `coerce_float`, `params`, and 'parse_dates' parameters </s> def read_sql_query(sql, con, index_col=none, **options):	read_sql_query Returns a DataFrame corresponding to the result set of the query string. Optionally provide an `index_col` parameter to use one of the
nullcontext = contextlib.exitstack()  # todo: use contextlib.nullcontext after python 3.7 </s> with lock or nullcontext:	check_randomness_of_generator return None input_digest = hashlib.sha1(input_data).digest() if len(generated_input_hashes) < limit: if input_digest in generated_input_hashes:
# todo: rename this test </s> self.any_func_return = any_func_return	setUp self.any_func_args = any_func_args
# todo: is there a benefit from differing bid and ask? </s> ticker = await self.exchange.fetch_ticker(market)	AsyncBellmanGraphInitializer return self.graph async def process_market(self, market): ticker_exchange_rate = (ticker['ask'] + ticker['bid']) / 2 if ticker_exchange_rate == 0:
raise  # todo: what if our seed node fails verification? </s> return potential_seed_node	learn_from_seednode potential_seed_node.verify_node(self, accept_federated_only=accept_federated_only) except potential_seed_node.InvalidNode:
# todo - do these in a single transaction? </s> self._set_register_bit(intcon, 1)	interrupt_when def interrupt_when(self, value): self._set_register_bit(DEFVAL, not value) self._set_register_bit(GPINTEN, 1)
# todo: explain these prior terms </s> ll_prior_a = -log_sigma_alpha**2 \	generalized_gamma_LL W * B * LL_observed + W * (1 - B) * LL_censored, 0) - dot(alpha, alpha) / (2*exp(log_sigma_alpha)**2) \ - n_features*log_sigma_alpha
pass # todo </s> else:	write_serialize for field in spec.parsed_fields(): if field.is_array: if field.is_builtin: write_serialize_builtin(s, field)
# todo error on too many levels </s> dashes = dict(zip(style_levels, self.default_dashes))	determine_attributes style_levels = categorical_order(data["style"]) if dashes is True: elif isinstance(dashes, dict): pass
# todo: "wildcards" other than <any> </s> if type == "<any>" or type == term:	get_relations_by_arg1 continue for dummy, type in arg1s: rels.append(r) cache[directory] = rels
# todo implement </s> ret = 1	hook_GetStringTypeW }) def hook_GetStringTypeW(ql, address, params): return ret
#todo todo todo todo todo todo todo todo todo </s> pub_key.generate_key(crypto.type_rsa, 1024)	gen_RSA_key :rtype: An RSA key as an `pyopenssl.OpenSSL.crypto.PKey` pub_key = crypto.PKey() return pub_key
entry['meta']['type'] = 'padding'  # todo handle padding, summarize and transfer </s> entry['flag']       = posting.flag	_journal_for_postings if isinstance(posting, Transaction): if posting.flag == 'P': entry['payee']      = posting.payee entry['narration']  = posting.narration
# todo: use madmom.utils.open </s> f = open(filename, 'r')	load Load the configuration from file. :param filename: name of the configuration file for line in f.readlines(): if line.startswith('trainFile'):
# todo: this check may hide a bug a should be removed. </s> if date is none:	datetime_to_pretty_str def datetime_to_pretty_str(date): print a datetime in pretty formatted str format date = datetime_null() return date.strftime("%A %d %B %Y %H:%M (UTC)")
# todo: improve the unicode checking </s> try:	Base tx = tx or value value = value.get_value() url_key = urllib.quote_plus(key) except (KeyError, UnicodeEncodeError, UnicodeError):
# todo: fix </s> d.array = true	process_element fn.array = True else: else: fn = [fn]
# todo: series support is not implemented yet. </s> only_numeric=false)	any lambda col: F.max(F.coalesce(col.cast('boolean'), F.lit(False))),
# todo maybe a better function would do here </s> return sum(first_occ) / len(first_occ)	get_first_phrase_occurrence terms = phrase.split() first_occ = [self._get_first_term_occurrence(term) for term in terms]
# todo: remove temporary hack giving special status to "*" </s> if text != none and text != "" and text != "*" and text not in t.text:	search_textbound if restrict_types != [] and t.type not in restrict_types: continue continue if nested_types != []:
# todo: bob crashes if he hasn't learned about this ursula #999 </s> ursula = self.known_nodes[node_id]	work_orders_for_capsules except KeyError: capsules_to_include.append(capsule) if capsules_to_include: work_order = WorkOrder.construct_by_bob(arrangement_id=arrangement_id,
# todo: automate detection of max string length to set up numpy array accordingly </s> self._rangednames = np.zeros(shape = (int(self.app.activeworkbook.names.count),1), dtype=[('id', 'int_'), ('name', 's200'), ('formula', 's200')])	ExcelComWrapper self.app.DisplayAlerts = 0 self.app.Workbooks.Open(self.filename) for i in range(0, self.app.ActiveWorkbook.Names.Count): self._rangednames[i]['id'] = int(i+1)
# todo: span_id= </s> raise args.usageerror('export: invalid variable name %r' % name)	Export val = value.Str(s) if not match.IsValidVarName(name): mem.SetVar( lvalue.LhsName(name), val, (var_flags_e.Exported,), scope_e.Dynamic)
# todo pass this to json schema validation </s> for id_name in data.get('movie_identifiers'):	post 'message': 'list_id %d does not exist' % list_id}, 404 data = request.json if set(id_name.keys()) & set(allowed_ids) == set([]): return {'status': 'error',
# todo: name, exp_init, exp_end, exp_step, block </s> def teststatforstep(self):	TestParser self.assertIsNotNone(node) self.assertEqual(13, p._pos) p = get_parser('for foo=1,3,10 do break end') node = p._stat()
# todo generator </s> results = []	_get cur_ds = Dataset(ds_path) content = content_by_ds[ds_path] if len(content) >= 1 and content[0] == curdir: results.append(cur_ds)
# todo: with only non-mandatory model attributes, it is not possible to get an invalid form </s> else:   # coverage: ignore branch	system_exporter_spreadsheet_xls_config_view info_logger(str(request.user), " SYSTEM_EXPORTER_SPREADSHEET_XLS_CONFIG_CHANGED") return HttpResponse('<script type="text/javascript">window.close();</script>') return render( request,
# todo: assert </s> assert 0	test_get_distro Test: get a distro object""" distro = self.remote.get_distro("testdistro0")
# todo: verify some output </s> nested_container_id = {'value': 'debug-%s' % str(uuid.uuid4())}	test_if_marathon_app_can_be_debugged attach_out_data['attach_container_output']['container_id'] = container_id_data logging.info('Making POST call to %s with: %s', agent_v1_url, attach_out_data) nested_container_id['parent'] = container_id_data logging.info('Creating nested container session: %s', nested_container_id)
# todo: uncomment when adding support for literal hex bytes </s> print(b'hello world   '.islower())	test_islower print(b'hello world'.islower())
# todo: act </s> systems = self.remote.get_systems(self.token)	test_create_system def test_create_system(self): Test: create/edit a system object system = self.remote.new_system(self.token) self.assertTrue(self.remote.modify_system(system, "name", "testsystem0", self.token))
# todo log? </s> return	make is_empty = set(raw.keys()) == {Keys.EDITED, Keys.EDIT_EMAIL, Keys.TITLE} if is_empty: yield Node(raw)
# todo: look this up in one query </s> for user_id in obj['collaborator_ids']:	get_by_mbid obj['collaborator_ids'] = playlist_collaborator_ids.get(obj['id'], []) collaborators = [] user = db_user.get(user_id) if user:
# todo: remove when botfactory can force everything to be unthreaded </s> time.sleep(0.1)	test_isup_command_ok irc.pm(user, '.isup example.com') while bot.running_triggers: assert len(bot.backend.message_sent) == 1, ( '.isup command should output exactly one line')
# todo: explicitly exploit symmetry and set onesided=true </s> a_fft = torch.rfft(a, signal_ndim=1, onesided=false)	circular_correlation :param b: torch.tensor, shape: (batch_size, dim) :return: torch.tensor, shape: (batch_size, dim) b_fft = torch.rfft(b, signal_ndim=1, onesided=False) a_fft[:, :, 1] *= -1
# todo: and netcdf writer will be more generic </s> return storage.in_memory_storage_unit_from_file(filename, datasets, storage_type)	_create_storage_unit filename = storage.generate_filename(tile_index, datasets, storage_type) storage.create_storage_unit_from_datasets(tile_index, datasets, storage_type, filename)
# todo: the delay was a dirty hack. </s> if blue is none:	LedCtrlString def LedCtrlString( self, string, red, green, blue = None, direction = 0, waitms = 150 ): red   *= 21 green *= 21
# todo this dosn't work yet </s> self.assertisinstance(e, myexception)	testWithTraceback e = MyException().with_traceback(tb)
# todo: add assertions </s> plt.title("%s (n=%d)" % (solver.__name__, n_neurons))	test_regularization plt.ylabel("reg")
# todo: just access the original event position, rather </s> p1 = np.array(event.last_event.pos)[:2]	PanZoomCamera event.handled = True elif 2 in event.buttons: p2 = np.array(event.pos)[:2] p1c = event.map_to_canvas(p1)[:2]
# todo: remove this skip after fixing </s> if sys.version[0] == 3:	test_circle_draw @requires_application() def test_circle_draw(): raise SkipTest with TestingCanvas() as c:
sil_ph = ["sil", "end"]  # todo fix hardcoded values </s> text = raw_text.split(" ")	ph_based_trim duration_path = config.get("duration_path", f"{config['rootdir']}/durations") duration_fixed_path = config.get("duration_fixed_path", f"{config['rootdir']}/trimmed-durations") trim_start, trim_end = False, False if text[0] in sil_ph:
# todo(ahundt) for multi-label try per class sigmoid top as follows: </s> return densenet.densenet(depth=none, nb_dense_block=3, growth_rate=32,	Atrous_DenseNet include_top=False): if include_top is True: nb_filter=-1, nb_layers_per_block=[6, 12, 24, 16], bottleneck=True, reduction=0.5, dropout_rate=0.2,
# todo: ensure it's up to date </s> return self._model_change_counter	model_change_counter def model_change_counter(self): #bruce 080731
# todo extend to nonbinary nodes </s> if i not in self._input_indices and i in self.context.node_indices:	__init__ else: self._dimension_labels.append(None) past_tpm_on = past_tpm_on.sum(i, keepdims=True) / 2 past_tpm_off = past_tpm_off.sum(i, keepdims=True) / 2
# todo: we're in need of more meta schema tests </s> def test_minitems_invalid_string(self):	test_minItems_invalid_string with self.assertRaises(SchemaError): validate([1], {"minItems" : "1"})  # needs to be an integer
# todo uncomment the actual test below after we have implemented the l1 attack </s> **self.attack_param)	test_adv_example_success_rate_l1 NotImplementedError, self.help_adv_examples_success_rate, ord=1,
'type': 'string', #todo: resolve </s> 'comment': ''	SqlAlchemyApi response['full_headers'] = [{ 'name': col[0] if type(col) is dict or type(col) is tuple else col, } for col in metadata] if has_result_set else [] return response
# todo: implement @plist </s> return _accidentalfromattr(elem.get('accid'))	accidFromElement ============ None.
#todo(jogo) make admin=false work </s> creds = ('--os-username %s --os-tenant-name %s --os-password %s '	cmd_with_auth def cmd_with_auth(self, cmd, action, flags='', params='', admin=True, fail_ok=False): '--os-auth-url %s ' % (self.identity.admin_username, self.identity.admin_tenant_name, self.identity.admin_password,
pass # todo: explain </s> pass # todo: explain	status503 def status503(self):        # Service Unavailable
# todo what should the swissnum _actually_ be? </s> self._http_server = httpserver(self.storage_server, swissnum_for_test)	setup_http_test raise SkipTest("Not going to bother supporting Python 2") self.storage_server = StorageServer(self.mktemp(), b"\x00" * 20) self.client = StorageClient( DecodedURL.from_text("http://127.0.0.1"),
# todo(brian): s/_container/container once other changes propogate </s> self._delete(_container.container, value, ignore_missing)	delete_container attempting to delete a nonexistent server. :returns: ``None``
# todo: use regexps </s> if name[0] == '_' and not name[1] == '_':	__getattr__ def __getattr__(self, name): return self.session_proxy.get(name, None) else:
# todo(lucasagomes): backward compatibility with :hexraw, </s> if conf.pxe.ipxe_enabled:	_link_mac_pxe_configs for mac in driver_utils.get_node_mac_addresses(task): create_link(_get_pxe_mac_path(mac)) create_link(_get_pxe_mac_path(mac, delimiter=''))
# todo -- can we do this without a subscription? </s> if not api.use_store:	play_similar_song_radio @ask.intent("GeeMusicPlaySimilarSongsRadioIntent") def play_similar_song_radio(): return statement(render_template("not_supported_without_store")) if len(queue.song_ids) == 0:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_transaction_wrong_type def test_fail_transaction_wrong_type(self): ``transaction`` is not a TrytesCompatible value.
# todo: put an explanation here </s> raise valueerror()	__add__ self, other = other, self else: selfp = getattr(self, 'positives', None) or (self,) selfn = getattr(self, 'negatives', ())
# todo: implement this </s> pass	_showOrUpdateUploadProgressMessage def _showOrUpdateUploadProgressMessage(self, new_progress = 0):
# todo : docker download </s> parser_destroy_docker = subparsers.add_parser('destroy/docker',	setup parser_setup.set_defaults(func=composite_action(docker_setup_create, docker_setup_build)) parents=[options]) parser_destroy_docker.set_defaults(func=docker_destroy_docker)
# todo: put the formula in terms of conventions and give a vanilla example </s> explaining the same	re def re(predicted_power, df_appliances_ground_truth): Parameters ----------
# todo(brian): s/_container/container once other changes propogate </s> return self._create(_container.container, **attrs)	create_container :returns: The results of container creation :rtype: :class:`~openstack.compute.v2.container.Container`
# todo private access! </s> for c in apply_py__get__(result_value, self._instance, self.class_context._value):	infer def infer(self): for result_value in self._class_member_name.infer(): yield c
# todo: depreciated </s> self.members(*args, **kwargs)	values def values(self, *args, **kwargs):
# todo: workaround the fact that skiptest is not defined by unittest2.testcase </s> splver = service.splunk_version	setUp import splunklib.client as client service = client.Service(**self.opts.kwargs) if splver[:2] < (6, 2): self.skipTest("Skipping cookie-auth tests, running in %d.%d.%d, this feature was added in 6.2+" % splver)
# todo test that this is called on next/prev/end-of-track </s> if self.current_track is none:	_trigger_stopped_playing_event def _trigger_stopped_playing_event(self): return for listener_ref in ActorRegistry.get_by_class(BackendListener):
# todo need to cache this </s> return module.cipher	symmetric_from_algorithm def symmetric_from_algorithm(algorithm): module = importlib('nkms.crypto.block.' + algorithm['symmetric']['cipher'])
# todo exceptions don't seem to be using parent constructors at all. </s> self.url = url	__init__ def __init__(self, url, status, resource_name, content): self.status = status self.resource_name = resource_name
# todo: avoid dummy and generate func here when inlining is possible </s> def _impl(df, n=5):	_impl return hpat.hiframes.pd_dataframe_ext.head_dummy(df, n)
# todo also test these! </s> continue	test_classifiers continue if Clf in [MultinomialNB, BernoulliNB]: clf = Clf() clf.fit(X, y)
raise exceptions.mpdnotimplemented  # todo </s> underscore, dash, dot and colon.	subscribe already. The name may consist of alphanumeric ASCII characters plus
# todo: if py3k, override unpickler.find_class(). </s> pass	dir_get_metadata mypickle.find_global = None except AttributeError: metadata = mypickle.load() out.write("Loaded metadata pickle.\n")
# @todo: check permissions & avoid db updates in gets </s> db(sendtable.id == r.id).update(status = ship_status_received)	prep if r.id and request.get_vars.get("received", None): del request.get_vars["received"] db(tracktable.send_id == r.id).update(status = TRACK_STATUS_ARRIVED) req_ref = record.req_ref
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_transactions_wrong_type def test_fail_transactions_wrong_type(self):
# todo: @sbharadwajj implement and test </s> raise notimplementederror	_weight_jac_mat_prod def _weight_jac_mat_prod(self, module, g_inp, g_out, mat):
#todo assert old_r.pubmed_id == new_r.pubmed_id </s> assert old_r.medline_id == new_r.medline_id	compare_records assert old_r.journal == new_r.journal
# todo: en passant. </s> moves = shift_up_right(movers) & self.occupied_co[black]	generate_pseudo_legal_moves if True: movers = self.pawns & self.occupied_co[WHITE] while moves: to_square, moves = next_bit(moves)
# todo: fix with stubber / before send event </s> with mock.patch('botocore.endpoint.endpoint._send') as _send:	test_get_export 'accepts': 'application/yaml' } _send.return_value = mock.Mock( status_code=200, headers={}, content=b'{}')
# todo implement. </s> yaml = self.master_model.to_yaml()	_train def _train(self, rdd, nb_epoch, batch_size, verbose, validation_split):
# todo (#2743, see also #2556): make a portable constant or remove completely </s> sig_header = signature_is_on_ciphertext	author signature_in_kit = None else: capsule, ciphertext = umbral.encrypt(recipient_key, sig_header + plaintext) signature = signer(ciphertext)
elif room_hosts:  # todo: shouldn't this be remote_room_host? </s> should_do_dance = true	send_membership_event if is_host_in_room: should_do_dance = False else: inviter = yield self.get_inviter(event)
# todo: implement me </s> pass	_test_1 def _test_1(self):
#todo this is wrong - need to add to biases only on no reuse </s> self.add_weights(alphas)	_trelu dtype=tf.float32) net = activation(_x - alphas) + alphas return tf.reshape(net, orig_shape)
# todo: calculate and write fingerprint </s> if curve_name == 'x25519':	generate_ec_key pubkey_enc = data[0x86] self._put_data(key_slot.gen_time, struct.pack('>I', timestamp)) from cryptography.hazmat.primitives.asymmetric import x25519 return x25519.X25519PublicKey.from_public_bytes(pubkey_enc)
# todo: consider alternatives to using pillow here. </s> image_pil = image.open(image_path)	image_to_example annotation = read_xml(annotation_path) image = read_image(image_path) width = image_pil.width height = image_pil.height
# todo: exception for coalesces that represents all sub_specs tried </s> class coalesce(object):	Coalesce def __init__(self, *sub_specs, **kwargs): self.sub_specs = sub_specs
# todo: store this data </s> p, h, g, d, comment = match.groups()	_parse_data_extension match = re.match(r'^PHG(\d)(\d)(\d)(\d)(.*)$', data) if match: errors.append('PHG parsing not implemented') return comment
# todo: investigate why this fails </s> return	test_field_renaming def test_field_renaming(self): value = self.field_values[0] Model = self.model_def.model_class()
# todo: allow configurable headers. </s> headers = {	write_file file_key = key.Key(self.bucket) file_key.key = path 'Cache-Control': 'no-cache', 'Content-Type': mimetype,
# todo(yanase): check values </s> assert len(weights) == 3	test_calculate_with_prior consider_endpoints=consider_endpoints, weights_func=default_weights) assert len(mus) == 3 assert len(sigma) == 3
# todo skip this in the future </s> print("skip pdf")	process_row if r.status_code == 200: if row["mime_type"] == "application/pdf": break filename = "/tmp/release-colors-%s.img" % get_ident()
# todo(hirofumi0810) fix this for after supporting transformer </s> args.ctc_weight = 0.0	trans set_deterministic_pytorch(args) model, train_args = load_trained_model(args.model) model.trans_args = args if args.rnnlm:
# todo: add logging to indicate the failure </s> await secured_conn.close()	Swarm ) except MuxerUpgradeFailure as error: raise SwarmException( f"fail to upgrade the connection to a muxed connection from {peer_id}"
# todo: add location info for invalid backslash </s> s = ''.join(word_compile.evalcstringtoken(t.id, t.val)	_EvalWordPart s = ''.join(t.val for t in part.tokens) elif part.left.id == Id.Left_DollarSingleQuote: for t in part.tokens) else:
# todo action required that updates the endpoint </s> endpoints = []	collect_on def collect_on(self, args): eps = self._get_endpoints(args, -1) for endpoint in eps:
#if (have_kratos is true): # todo: implement natively </s> with open(filename, "rb") as f:	read def read(filename): mesh = read_buffer(f) return mesh
# todo track_set_seen() </s> return playlisttrack(track, create_time, creator, seen, message)	_build_playlist_track message = utils.to_unicode(message)
# todo: support multi-index here </s> if len(index_scols) != 1:	filter raise ValueError("items should be a list-like object.") if axis in ('index', 0): raise ValueError("Single index must be specified.") col = None
# todo: larger gains expected with scipy.signal.signaltools.fftconvolve(). </s> psi = segment_axis(y, k, 1, axis=-1)[:, :t - delay - k + 1, ::-1]	get_correlations_narrow_v5 def get_correlations_narrow_v5(Y, inverse_power, K, delay): D, T = Y.shape Psi_conj_norm = inverse_power[None, delay + K - 1:, None] * Psi.conj() correlation_matrix = np.einsum('dtk,etl->kdle', Psi_conj_norm, Psi)
# todo(aditya): temporarily we are filtering out todos </s> num_non_template_todos = (	tasks_assigned_to_worker 'due_datetime': due_str } task_assignment.task.todos .filter(template=None,
# todo: make locking work for mssql </s> pass	create_global_lock conn.execute(text("SELECT GET_LOCK(:id, :timeout)"), id=str(lock), timeout=lock_timeout) elif dialect.name == 'mssql': yield finally:
# todo: move this into the operations code for its caller </s> count = 0	Dehydrogenate Remove hydrogen atoms from this chunk. @return: number of atoms removed. for a in self.atoms.values(): count += a.Dehydrogenate()
# todo(lbragstad): sleeping after the response status has been checked </s> time.sleep(1)	test_user_update_own_password password=old_pass, original_password=new_pass) self.non_admin_client.update_user_password(
# todo: this is a case we should deal with, but there are probably </s> anchor = none	_handle_previous_segments ) elif len(segments_since_code) > 1: pass else:
# todo: fix this </s> if not self._global_container_stack:	_onMaterialIdChanged def _onMaterialIdChanged(self, index, material_id): return if self._global_container_stack.getMetaDataEntry("has_machine_materials", False):
# todo implement this </s> pass	matches @param device_state: DeviceState @return:
# todo - this should be moved to the `finalize` method of the base resource, as it's not cross-service </s> self._map_all_subnets()	preprocessing :return: None ip_ranges = [] if ip_ranges is None else ip_ranges if 'ec2' in self.service_list: self._map_all_sgs()
# todo: tf and jax sort [inf, nan] differently. </s> elif harness.name.startswith("nan_"):	test_top_k with self.assertRaisesRegex(RuntimeError, "Unimplemented: complex comparison"): harness.dyn_fun(*harness.dyn_args_maker(self.rng())) raise unittest.SkipTest("inconsistent [nan, inf] sorting") else:
## fixme: # todo: remove me </s> paste= paste.replace(self.paste_directory, '')[1:]	get_domain_random_screenshot l_screenshot_paste = [] for paste in l_crawled_pastes: paste = paste.replace(self.paste_crawled_directory_name, '') if os.path.isfile( '{}{}.png'.format(self.screenshot_directory, paste) ):
# todo: add and store preprocessing errors. </s> logging.error('unable to decode group identifier.')	_ParseFileData group_identifier = row[3].decode('utf-8') except UnicodeDecodeError: full_name = None if row[4]:
# todo: change docstring or remove unsqueeze(-1) </s> model = cosinecutoff()	x_test_shape_cosine_cutoff def x_test_shape_cosine_cutoff(distances): inputs = [distances] out_shape = list(distances.shape)
# todo col.type.python_type contains the type that </s> py_type = _type_map[type(col.type)]	set_attrs setattr(self, prp.key, val) else: if not isinstance(val, py_type): raise TypeError(
# xxx todo </s> return 'unknown'	get_arch return 'ia64'
# todo(rosmaita): bug #1745003 </s> self.assertequal('private_id_2', rows[1]['id'])	TestOcataMigrate01Mixin self.assertEqual(4, len(rows)) self.assertEqual('private_id_1', rows[0]['id']) self.assertEqual('public_id', rows[2]['id'])
# todo: remove in v.0.6 </s> x = np.array([[0, 0], [0, 1], [2, 0], [2, 1]])	TestLSML self.assertLess(csep, 0.8)  # it's pretty terrible def test_deprecation_num_labeled(self): y = np.array([1, 0, 1, 0]) lsml_supervised = LSML_Supervised(num_labeled=np.inf)
except exception:  # todo - which exceptions? </s> return ''	get_selection try: return w.selection_get()
# todo: consider filtering by location type </s> try:	match_district def match_district(domain, xlsx_district_name): Given district name taken from the spreadsheet, return the name and id of the matching location in HQ. location = SQLLocation.active_objects.get(domain=domain, name__iexact=xlsx_district_name) except SQLLocation.DoesNotExist:
#todo: make more general (if possible) </s> return datetime.fromtimestamp(record_dict['slice_index_value'] + (record_dict['x_min'] + record_dict['x_max']) * 120).date()	solar_date Function which takes a record_dict containing all values from a query in the get_db_slices function and returns the solar date of the observation
# todo: let the globe return the semimajor axis always. </s> return coords	ellipse coords += ([easting], [northing])
pass # todo: explain </s> pass	status408 def status408(self):        # Request Timeout
# todo: fix as soon as we have preprocessvector (different parallel dict preprocessor for different spaces in a dict) </s> if len(self.preprocessor) > 1 or "" not in self.preprocessor:	get_preprocessed_state_and_action - DataOp: The chosen action. max_likelihood = self.max_likelihood if self.max_likelihood is not None else self.policy.max_likelihood preprocessed_states = self.call(self.vector_preprocess, states) else:
# temporary for testing. todo: remove </s> print("-------- templist start----------")	process_history self.calculate_asset_details() self.csvexporter.create_files() print(self.temp_list) print("-------- TEMPLIST END----------")
# todo:  we might need additional logic comparing the state of git-annex </s> if not force:	_publish_data lgr.debug("Invoking copy --auto") annex_copy_options_ += ' --auto' annex_copy_options_ += ' --fast' for r in ds.repo.copy_to(
# todo: test that valueerror is raised </s> validate.direction(direction)	reducible _from, to = mechanism, purview else: return utils.block_reducible(cm, _from, to)
# todo: remove </s> g.error = error	history error = \ _(u'Select two revisions before doing the comparison.') else: params[u'diff_entity'] = u'group'
# todo(yanase): remove number from system_attrs after adding trialmodel.number. </s> assert len(df.columns) == 10	test_trials_dataframe_with_failure df.set_index(('number', ''), inplace=True, drop=False) assert len(df) == 3 for i in range(3): assert df.number[i] == i
#ng_maxlength="30",     # todo: validation </s> ),	NewMobileWorkerForm 'first_name', data_bind='value: first_name', crispy.Field( 'last_name',
#todo: add way to check if alt is pressed </s> self.currenttool.flip(blocksonly=true)	key_down getattr(self.currentTool, name)() elif keyname == config.config.get('Keys', 'Flip'): elif keyname == config.config.get('Keys', 'Roll'): self.currentTool.roll(blocksOnly=True)
# todo signals instead of direct dialog creation? </s> dialog = changemodeldialog(collection, [self.anki_object.id], self.anki_object.model())	handle_model_update else: new_model.make_current(collection) def on_accepted(): model_map_cache[old_model_uuid][self.note_model_uuid] = \
#todo: handle ipv6 </s> with socket.socket(socket.af_inet, socket.sock_dgram) as sock:	add_nio rport = request["nio"]["rport"] try: sock.connect((rhost, rport)) except OSError as e:
# todo(jaypipes): port nova's fault infrastructure </s> self.assertraises(exception.invalid, req.get_response, controllers.api())	test_create_image_with_bad_status req.method = 'POST' req.body = json.dumps(fixture)
# todo: detect if any database upgrading is needed and acquire the lock only in one place </s> with self.acquire_lock(event=false):	__init__ if db_schema.upgrade_required(): log.info('Database upgrade is required. Attempting now.') fire_event('manager.upgrade', self) if manager.db_upgraded:
pass  # todo </s> def get_xl_sheet(xl_workbook, sheet_name_or_index):	get_xl_sheet
# todo: fill the blank space at the bottom of the page </s> result = find_earlier_page_break(	block_container_layout if new_child is None: if page_break in ('avoid', 'avoid-page'): new_children, absolute_boxes, fixed_boxes) if result:
# todo: send `goodbye` req then disconnect </s> return	Node hello_other_side = await read_req(stream, HelloRequest) except ReadMessageFailure as error: self.logger.debug("Received the hello message %s", hello_other_side) try:
# todo add support </s> for color in ['ambient', 'diffuse', 'specular', 'emissive']:	parseSDFMaterial materialdict = {} materialdict['name'] = 'mat_' + visualname if color in material.attrib: materialdict[color] = gUtils.parse_text(material.attrib[color])
# todo consolidate </s> if color_scheme_name == 'default':	loadini 'prompt_more': 'g', } struct.color_scheme = default_colors else:
@unittest.skip('not written')  # todo: finish! </s> self.assert_shortened([value], "[b'aaaaaaaaaaaaaaaaaa...aaaaaaaaa']")	test_bytes_large "b'" + 'A' * 43688 + "..." + 'A' * 21844 + "'")
# todo: let the globe return the semimajor axis always. </s> return coords	ellipse coords += ([easting], [northing])
#todo: decide if this method really adds anything of value ... </s> def new_target(self, basestr=none, **kwargs):	new_target if 'dep' in kwargs: path = self.get_path(kwargs['dep'])
# todo: figure out why this fails </s> x = [300, 600, 10, 0]	test_brle_split_merge if True: return split = rl.split_long_brle_lengths(x, np.uint8) merged = rl.merge_brle_lengths(split)
# todo(guillermooo): add regexes </s> cmd = [sdk.path_to_dart2js,	DartRunCommand sdk = SDK() if action == 'primary': '--minify', '-o', file_name + '.js', file_name]
# todo: look this up in one query </s> for user_id in obj['collaborator_ids']:	get_by_mbid obj['collaborator_ids'] = playlist_collaborator_ids.get(obj['id'], []) collaborators = [] user = db_user.get(user_id) if user:
# todo raise exception if source_group is none? </s> if source_group:	authorize_security_group_ingress for source_group_name in source_group_names: source_group = self.get_security_group_from_name(source_group_name, vpc_id) source_groups.append(source_group) for source_group_id in source_group_ids:
# todo: add and store preprocessing errors. </s> logging.error('unable to decode full name.')	_ParseFileData full_name = row[4].decode('utf-8') except UnicodeDecodeError: user_directory = None if row[5]:
# todo: inform cluster </s> self.pending_clusters.clear()	_shutdown except: logger.debug(traceback.format_exc()) self.unsched_clusters = [] while (any(cluster.pending_jobs for cluster in self._clusters.values())):
# todo: non standard, uses convs </s> raise notimplementederror()	get_classifier_module clf = f"classifier.{i}" elif isinstance(model, torchvision.models.SqueezeNet): elif isinstance(model, torchvision.models.Inception3): raise NotImplementedError()
# todo: support minp arg end_range etc. </s> minp = win	roll_sum_fixed N = len(in_arr) output = np.empty(N, dtype=np.float64) range_endpoint = max(minp, 1) - 1 for i in range(0, range_endpoint):
# todo: switch to: </s> self.change_track(	previous will continue. If it was paused, it will still be paused, etc. tl_track = self.current_tl_track self.core.tracklist.previous_track(tl_track), on_error_step=-1)
# self.assertisnotnone(result_set.service_processing_time_in_millis)  # todo flaky test </s> self.assertisnotnone(result_set.output_location)	test_fetchone self.assertIsNotNone(result_set.total_execution_time_in_millis) self.assertIsNotNone(result_set.query_planning_time_in_millis) self.assertIsNone(result_set.data_manifest_location)
# todo: cleanup directory/basename.* files. </s> tmp = tempfile.namedtemporaryfile(	write_library data['version'] = mopidy.__version__ directory, basename = os.path.split(json_file) prefix=basename + '.', dir=directory, delete=False) try:
# todo: restore... needed for complete compatibility with tomxobjects... </s> and (n.notehead != 'normal'	dealWithNotehead foundANotehead = False if (hasattr(n, 'notehead') or n.noteheadParenthesis or n.noteheadFill is not None
# todo: check if "class" in current line, add class name </s> txt = txt.replace("protected ", "")	java2pythonlinebyline txt = count * ' ' + txt[count+1:] if txt[count:].startswith("protected ") >= 0: if txt[count:].startswith("public ") >= 0: txt = txt.replace("public ", "")
# todo: remove this skip after fixing </s> if sys.version[0] == '3':	test_reactive_draw @requires_application() def test_reactive_draw(): raise SkipTest with TestingCanvas() as c:
# todo(b/183565702): support integer convolutions on cpu/gpu. </s> if jtu.device_under_test() == "gpu":	testConvGeneralDilated if jtu.device_under_test() == "cpu" and jax.lib.version < (0, 1, 65): raise SkipTest("Integer convolution requires jaxlib 0.1.65 or newer on CPU") raise SkipTest("Integer convolution not yet supported on GPU") rng = jtu.rand_small(self.rng())
# todo: remove this and figure out why queue thread does not properly exit </s> logging.info('forcefully exiting optimization')	handler logging.info('caught CTRL+C, exiting...') self.term_event.set() self.forceful_exit = True self.sess.close()
# todo: this will incorporated in the future, if needed. </s> edge_starts = []	specify_edge_starts )) elif self.net_params.additional_params["on_ramp"]: else: edge_starts = []
# todo check if varnos are exactly the crosscat variables. </s> return varnos, seen_optimized	retrieve_analyze_variables if seen_variables: raise BQLError(bdb, 'OPTIMIZED incompatible with VARIABLES')
# todo: check </s> corrupted_subject_based_hashed = np.apply_along_axis(self._hash_triples, 1, corrupted_subject_based)	_filter_corrupted_triples all_pos_triples_hashed, ): mask = np.in1d(corrupted_subject_based_hashed, all_pos_triples_hashed, invert=True) mask = np.where(mask)[0]
# todo: this stuff should be generated by a template of some sort </s> data += '<hr /><div class="footer">\n'	body data += '</tr>\n' data += '</table>\n' welcomeurl = self.path_to_root(request) + "index.html" data += '[<a href="%s">welcome</a>]\n' % welcomeurl
# todo: make it pass </s> self.assertequals('sample_function', word_finder.get_name_at(10))	test_function_calls word_finder = WordRangeFinder('sample_function()')
# todo: add for morph targets data. </s> if found:	extract_primitives found = False break indices.append(current_new_index) create = False
# todo(twd2): do more visibility check eg. contest </s> path_components = self.build_path(	DiscussionCreateHandler if vnode['doc_type'] == document.TYPE_PROBLEM and vnode.get('hidden', False): self.check_perm(builtin.PERM_VIEW_PROBLEM_HIDDEN) (self.translate('discussion_main'), self.reverse_url('discussion_main')), (vnode['title'], node_url(self, 'discussion_node', node_or_dtuple)),
# end todo </s> result = torch.zeros(	backpropagate_sqrt_ggn sqrt_ggn = sqrt_ggn.view(num_classes * batch, out_features) sqrt_ggn = sqrt_ggn.view(num_classes * batch, channels, out_x * out_y) num_classes * batch, channels, in_x * in_y, device=sqrt_ggn.device) pool_idx = pool_idx.view(batch, channels, out_x * out_y)
# todo: figure out way to paramaterize this test </s> for x in range(0, node["num_devices"] * 2):	test_osds_listen_on_public_network def test_osds_listen_on_public_network(self, node, Socket): port = "680{}".format(x) assert Socket("tcp://{address}:{port}".format(
# todo message </s> return symbol('$failed')	get_results Expression('Close', stream).evaluate(evaluation) else: tmp = tmp.get_leaves() if not all(expr.has_form('Rule', None) for expr in tmp):
#todo: increase timeout based on number of plugins </s> response = opener.open(url, none, 5).read()	get_plugin_status )) ] json = simplejson.loads(response) except Exception, e:
# time.sleep(40)  # todo: should remove after polling get. </s> mpc_1_2_3 = op(mpc_1_2, mpc_2_3)	test_tensor_abstraction_subsets mpc_2_3 = op(tensor_pointer_2, tensor_pointer_3) mpc_2_3.block_with_timeout(40) mpc_1_2_3.block_with_timeout(secs=40) exp_res_1 = op(data_1, data_2)
# todo(rbharath): how does distance need to be modified here to </s> if np.linalg.norm(coords[atom] - coords[neighbor_atom]) < self.neighbor_cutoff:	_featurize if neighbor_atom == atom: continue neighbor_list[atom].add(neighbor_atom) all_nbrs.add(neighbor_atom)
# todo refactor to .session </s> def set_session_view_value(view, name: str, value) -> none:	set_session_view_value _views[view.id()][name] = value
# todo refactor set position cursor after operation into reusable api. </s> line = self.view.line(self.view.sel()[0].b)	_vi_left_square_bracket_c def run(self, mode=None, count=1): self.view.run_command('git_gutter_prev_change', {'count': count, 'wrap': False}) if line.size() > 0: pt = self.view.find('^\\s*', line.begin()).end()
# todo: insert more test values here </s> self.assertequal(reflection_coefficient(75), 0.2)	test_reflection_coefficient self.assertEqual(reflection_coefficient(50), 0)
self.setup()  # todo: perhaps, remove this to pass path in context </s> current_ids = self.hash(	fit :param expected_outputs: the expected data output to fit on :return: the pipeline itself current_ids=None, hyperparameters=self.hyperparams,
# todo[lauren]: add proper authorizers to draftregistrationapproval </s> user = osf_user.load('dsmpw')	reject_draft :return: DraftRegistrationApproval obj draft = get_draft_obj(draft_pk) draftRegistrationApproval = draft[0].approval draftRegistrationApproval.add_authorizer(user)
# todo: use validation set if not-none </s> model = self.new_model()	best_model_indexed_image_classification def best_model_indexed_image_classification(self, train, valid): X = train.all_images[train.idxs] y = train.all_labels[train.idxs]
# todo(twd2): check permission for visibility. (e.g. test). </s> rdocs = await record.get_multi().sort([('_id', -1)]).to_list(50)	RecordMainView class RecordMainView(base.Handler): async def get(self): await asyncio.gather(user.attach_udocs(rdocs, 'uid'), problem.attach_pdocs(rdocs, 'domain_id', 'pid'))
csv_reader = caches[caches_celery_query_result_key].get(_result_key(notebook)) # todo check if expired </s> headers = csv_reader[0] # todo check size	_get_data def _get_data(notebook): if TASK_SERVER.RESULT_CACHE.get(): csv_reader = csv_reader[1:] else:
# todo dry </s> try:	run except pexpect.TIMEOUT: pass pnum = self.con.expect('Indication   handle = .*? \r', timeout=.5) if pnum == 0:
# todo(python3): modify pool to context manager (with statement) </s> results = self.pool.map(	execute_queries_w_backup print('Preparing to execute %d queries.'%len(queries)) tic = time.clock() partial(execute_query_w_backup, conn_args=self.conn_args, verbose=self.verbose, timeout=self.timeout), [(idx, q) for idx, q in enumerate(queries)])
# todo: run this with mock file </s> from zim.fs import file	testSingleFile def testSingleFile(self): folder = self.setUpFolder('single', mock=tests.MOCK_ALWAYS_REAL) file = folder.file('test.html')
1  # todo: fill in identifier </s> )	profile_transfer amount, target, result.wait() profiling.print_all_threads()
# todo: test for external ring source values is missing as it needs </s> lvalues = [[2., 5., 3.], [4., 1., 4.], [6., 4., 3.]]	TestDonut assert_array_equal(_chart.data['end'], end) assert_array_equal(_chart.data['colors'], colors) lvalues_int = [[2, 5, 3], [4, 1, 4], [6, 4, 3]] for lvalues in [lvalues, lvalues_int]:
# todo ?? </s> if isinstance(ns, str):	number def number(self) -> Optional[float]: ns = self.row['number'] ns = ns.strip() return None if len(ns) == 0 else float(ns)
# todo defensive? </s> tags = tuple(t['label'] for t in tags_dict.values())	load tags_dict = di['tags'] pm = j['pageMetas'] path = Path(config.polar_dir) / 'stash' / filename yield Book(
# todo: deal with error </s> assert 0, error	TTXFont fontData, error = await compileTTXToBytes(self._fontPath, outputWriter) if error: f = io.BytesIO(fontData) self.ttFont = TTFont(f, fontNumber=self._fontNumber, lazy=True)
# todo: handle % widths </s> raise typeerror('width %s is unknown' % box.style['width'])	block_preferred_width return box.style['width'] else:
# todo: finish this. </s> pass	truncate def truncate(self, path, length, fh=None):
# todo(b/178123173) enable tests after b/193022465 is resolved. </s> "auto_one_device_strategy": self.auto_one_device_strategy(),	test_run_on_notebook track_status = { "auto_mirrored_strategy": self.auto_mirrored_strategy(), "auto_multi_worker_strategy": self.auto_multi_worker_strategy(), "docker_config_cloud_build": self.docker_config_cloud_build(),
# todo: sync the doc. </s> return self._reduce_for_stat_function(f.sum, only_numeric=true)	sum databricks.koalas.DataFrame.groupby
#  todo: test </s> skill="industrial command ships"	handler src.getModifiedItemAttr("shipBonusICS5"),
# todo: move space inference here. </s> component.constant_op_records.add(op_rec)	DataOpRecordColumn constant_op = np.array(value) op_rec.op = constant_op self.op_records.append(op_rec) else:
# todo: out to file </s> pass	write_up def write_up(self):
# todo: deprecate this?  properties are generally preferred over "set*()" </s> return self.ub not in _no_upper_bound	has_ub :const:`None` or positive infinity"""
# todo: the following skipped suite and fixtures should be enabled </s> return ['api-key']	_filter_headers def _filter_headers(self):
# todo(elliot): include the rest of the necessary keys </s> )	handle_download_recipe_input "destination_path": "%RECIPE_CACHE_DIR%/%NAME%/Applications", "purge_destination": True}} else: pass
# @todo: widget? </s> ),	DataCollectionTemplateModel Field("model", "json", requires = IS_EMPTY_OR(IS_JSON()), s3_comments(), *s3_meta_fields())
# todo: renable when t34648262 is fixed </s> outputs.append(new_output)	Preprocessor ) new_output *= not_missing_input[:, begin_index:end_index] if len(outputs) == 1: return torch.clamp(outputs[0], MIN_FEATURE_VALUE, MAX_FEATURE_VALUE)
# todo: this should support multi-db </s> cursor = connection.cursor()	_table_columns_iterator def _table_columns_iterator(self, table_name): description = connection.introspection.get_table_description(cursor, table_name) return (row[0] for row in description)
# todo(dcramer): this should respect rate limits/etc and use the normal </s> from sentry.app import tsdb	send def send(self, **kwargs): from sentry.coreapi import insert_data_to_database from sentry.event_manager import EventManager
# todo ideally this happens a layer higher, but this is a bad </s> return sanitize_html(open(filename).read()).encode('utf-8')	_yield_user_file_content if not from_dataset.creating_job.imported and from_dataset.creating_job.tool_id in trans.app.config.sanitize_whitelist: return open(filename) return open(filename)
# todo align api and test after - now tip_id is ignored </s> receiver_status = yield get_receiver_settings(self.current_user['username'])	ReceiverInstance Response: receiverReceiverDesc Errors: TipGusNotFound, InvalidInputFormat, InvalidTipAuthToken self.set_status(200) self.finish(receiver_status)
#todo: dont unfold all, but allow enum_all() to work </s> tree_proc(self.tree, tree_item_unfold_deep, 0)	goto_main msg_status('Project not opened') return fn = self.project.get('mainfile', '') if not fn:
# todo: try/catch </s> m = importlib.import_module(module_name)	class_for_name def class_for_name(module_name, class_name): c = getattr(m, class_name) return c
# todo: change this to be architecture independent </s> return getregvalue('rip')	get_cur_ea if nativeSize is 64:
# todo: commented out so we don't prompt for installing vc or vcp until they </s> _update_config_file()	wizard do_web_enabled_zmq(volttron_home)
# todo: refactor common tests for all models, e.g. shape checking </s> scores = model.forward_owa(triples)	test_distmult batch_size = 16 triples = torch.zeros(batch_size, 3, dtype=torch.long) assert scores.shape == (batch_size, 1) scores = model.forward_cwa(triples[:, :2])
pass # todo </s> def _playlistinfo(self, position=none, start=none, end=none):	_playlistinfo @register(r'^playlistinfo( ((?P<position>\d+)|(?P<start>\d+):(?P<end>\d+)*))*$')
raise skiptest() # todo fixme </s> dexy.commands.run()	test_filters_text_single_alias_source @patch('sys.stdout', new_callable=StringIO) def test_filters_text_single_alias_source(stdout): text = stdout.getvalue() sys.stderr.write("TEXT IS:\n%s" % text)
# todo: warn on failure to delete? </s> mods.deletion(ann)	_delete_arc_nonequiv_rel ann_obj.del_annotation(ann)
# todo -- validate other options </s> die("[%s] has no 'username'" % target)	_validate_github if not config.has_option(target, 'username'):
# todo: use utils.normalize when rebased onto develop </s> macro_tpm = np.array([list(row) if sum(row) == 0 else list(row / sum(row))	macro_tpm mapping[current_state_index]] += \ micro_tpm[past_state_index, current_state_index] for row in macro_tpm]) if (check_independence and
# todo: seems to be doing < rather than <= ??? </s> xpath.add_post_condition("@x0 <= %s" % x1)	_xpath_overlaps_bbox def _xpath_overlaps_bbox(self, xpath, expr): x0,y0,x1,y1 = map(float, expr.split(",")) xpath.add_post_condition("@y0 <= %s" % y1) xpath.add_post_condition("@x1 >= %s" % x0)
# use the mean of the previous noise values (todo: be smarter here). </s> noise = self.likelihood.noise.mean().expand(x.shape[:-1])	BatchedMultiOutputGPyTorchModel if observation_noise: if isinstance(self.likelihood, FixedNoiseGaussianLikelihood): mvn = self.likelihood(mvn, X, noise=noise) else:
# todo: clean up </s> yield _pubsubs_gsub	pubsubs_gsub _pubsubs_gsub = _make_pubsubs(hosts, gossipsubs)
# todo check behavior when not loaded </s> return playlistofflinestatus(lib.sp_playlist_get_offline_status(	offline_status @property def offline_status(self): spotify.session_instance._sp_session, self._sp_playlist))
#@todo: remove in 0.4.10 </s> def error(self, reason=none, type="parse"):	error raise Fail("%s error%s | Plugin out of date" % (type.capitalize(), ':' + str(reason) if reason else "")) if self.core.debug:
# todo: ignore models which were killed early by scheduler (eg. in hyperband). how to id these? </s> file_id = "trial_"+str(trial) # unique identifier to files from this trial	hyperparameter_tune hpo_model_performances = {} for trial in sorted(hpo_results['trial_info'].keys()): file_prefix = file_id + "_" trial_model_name = self.name+"_"+file_id
# todo remove! </s> stmt = element._array.parent	get_execution_parent def get_execution_parent(element, *stop_classes): if isinstance(element, Array): else: stmt = element.var_args[0].var_args.parent
raise skiptest  # todo: figure out why this randomly started failing. </s> qs = {'a': 1, 'w': 4, 'format': 'json'}	test_discussion_filter_forum def test_discussion_filter_forum(self): forum_vals = ( (1, 4),
# todo: distributed search messages need to implemented </s> self.logmessage("%s %s" %(msg.__class__, vars(msg)), 4)	ChildDepth def ChildDepth(self, msg):
# todo: optimize queries </s> choices.append(	get_team_choices choices = [] for team in sorted_team_list: (team.id, get_team_label(team)) )
# todo: remove in galaxy 20.xx, for running jobs at gx upgrade </s> return os.path.abspath(os.path.join(self.working_directory, command_version_filename))	get_version_string_path def get_version_string_path(self):
# todo: speedup by allocating the denominator directly instead of constructing it by sum </s> return tf.slice(tens, [0, 0, second * single_batch_size], [m, n, single_batch_size])	half n = int(n)
reorder_attributes(root_element)  # todo: remove when support is python 3.8+ </s> tree = et.elementtree(root_element)	export_to_xml if p.is_dir(): p /= 'tallies.xml' tree.write(str(p), xml_declaration=True, encoding='utf-8')
# todo: maps of packages </s> install_type = self.cfg['container']['install_type']	remove expect = expect or self.get_default_expect() if options is None: options = {} if install_type == 'apt': cmd = 'apt-get purge'
# todo: uncomment this test case when #1217 is fixed. </s> 'http:/yo/pytest2-1.0.tar.gz',	est_link_package_versions_substring_fails 'url', [ 'http:/yo/pytest_xdist-1.0-py2.py3-none-any.whl', ],
# todo - fix meta.submission to point to real submission </s> self._remove_handled(meta.submission.submission)	remove_instance_matching_schema try: meta = Metadata.objects.get(raw_data=instance_id, formdefmodel=formdef_id) meta.delete() except Metadata.DoesNotExist:
# todo: test </s> return self.get_final_response(base_response, "dag [{}] is now fresh as a daisy".format(dag_id))	refresh_dag logging.info(html)
# todo(b/160795287): deprecate estimator based executor. </s> fn_args_dict = fn_args.get_dict()	run_fn fn_args: Holds args used to train the model as name/value pairs. schema = io_utils.parse_pbtxt_file(fn_args.schema_file, schema_pb2.Schema()) fn_args_dict['serving_model_dir'] = os.path.join(fn_args.model_run_dir, path_utils.SERVING_MODEL_DIR)
# todo(bnemec): this should be documented as an ivar, but can't be due </s> self.state = none	FlowDetail self._name = name self._atomdetails_by_id = {} self.meta = {} def update(self, fd):
# todo: https://github.com/fonttools/fonttools/issues/842 </s> pass	test_nameid_windows_utf16_surroates def test_nameid_windows_utf16_surroates(self):
# todo(b/160795287): deprecate estimator based executor. </s> fn_args_dict = fn_args.get_dict()	run_fn fn_args: Holds args used to train the model as name/value pairs. schema = io_utils.parse_pbtxt_file(fn_args.schema_file, schema_pb2.Schema()) fn_args_dict['serving_model_dir'] = os.path.join(fn_args.model_run_dir, path_utils.SERVING_MODEL_DIR)
# todo: update this when we have replaced elements that do not </s> assert intrinsic_width is not none	replaced_box_height Compute and set the used height for replaced boxes (inline- or block-level) _surface, intrinsic_width, intrinsic_height = box.replacement assert intrinsic_height is not None if intrinsic_height == 0:
# todo todo todo </s> self.set_progression(src, ((float)(page_idx+1) / self.doc.nb_pages),	__on_page_thumbnailing_page_done_cb line_iter = self.lists['pages']['model'].get_iter(page_idx) self.lists['pages']['model'].set_value(line_iter, 0, thumbnail) _("Thumbnailing ..."))
# todo permissions scoping </s> def unarchive_product(request, domain, prod_id, archive=true):	unarchive_product Unarchive product product = Product.get(prod_id)
# todo subject.cn from cert? </s> shutil.rmtree(app_path)	test_simple_app lib_hashes = self.assert_common_signed_hashes(lib_info, -2, -1) assert '-3' not in lib_hashes return app_info
# todo check executions for dict contents </s> is_array_assignment = true	_process is_exe |= is_execution(assignee) if is_exe: else: details = par.assignment_details
if self.is_direct_mode() or batch or not allow_quick:  # todo: thin mode </s> find = self.find(files, normalize_paths=false, batch=batch)	file_has_content list of bool For each input file states either file has content locally return [bool(filename) for filename in find] else:  # ad-hoc check which should be faster than call into annex
# todo: why does pipeline not look like a regular result? </s> with self.assertraises(typeerror):	test_can_run_simple_statement for record in pipeline.pull(): assert record[0] == 1 _ = record[object()] assert repr(record)
# todo: what's the right status code here?  202?  different if we already knew about the node(s)? </s> return all_known_nodes()	node_metadata_exchange for node in sprouts: this_node.remember_node(node)
# todo: wells if display config has more than one column </s> "put_loners_in_wells": display	render_form "cases": cases, "form_table_options": { }, "form_meta_data": form_meta_data,
# todo: eliminate asap, for backwards compatibility only </s> return self.get_location(self)	find_location def find_location(self):
# todo: we might want to handle still_alive, e.g. to allow for </s> stdout = assure_unicode(self.output_proc(process.stdout)) \	proc1 lgr.log(5, "Done sending.") still_alive, stderr = self._check_process(restart=False) if not process.stdout.closed else None if stderr:
# todo: share code with the loader attribute here. </s> locator = locator(extension=self.loader.extension)	get_template_path Return the path to the view's associated template. dir_path, file_name = self.get_relative_template_location(view) if dir_path is None: path = locator.find_path_by_object(self.search_dirs, view, file_name=file_name)
# todo: cache the list of components that were deactivated </s> for (name, data) in sub.component_map(active=true).items():	_apply_to self._unfix_all() sub = getattr(instance,self._submodel) if not isinstance(data,Var) and not isinstance(data, Set): data.deactivate()
# todo: how to compare images? </s> def test_plot_many(self):	TestOps r = self._execute_plot_op(plot_op, print_image=True) self.assertEquals(hash_image(r), 'e49475e0452fd744864b118c0464bdf1e12d678d') batch_size = 3 image_tensor = tf.constant(skimage.data.chelsea())
raise  # todo </s> else:	deploy_contract cached_contract = self.__contract_cache[contract_name] except KeyError: self.__registrar.enroll(contract_name=contract_name, contract_address=address,
# todo(pkilambi): process the output as needed </s> return out	pod_get try: out = utils.execute('kubectl', 'get', 'pod', uuid) except Exception as e: LOG.error("Couldn't get service  %s due to error %s" % (uuid, e))
# llamo al método remoto: </s> try:	test_timbrado WSDL = "federico.wsdl" client = SoapClient(wsdl=WSDL, ns="ns0", soap_ns="soapenv") retval = client.TimbrarTest(comprobanteBytesZipped="1234") except SoapFault as sf:
# todo ... </s> self._index_list = tf.range(0, self.min_after_dequeue)	_init_index_list self._free_mask = constant_with_shape(True, shape=(self.min_after_dequeue,))
#todo: check if/where this is used; if not used externally - remove </s> return self.marker_detector.marker_min_perimeter	Surface_Tracker @property def marker_min_perimeter(self) -> int: @marker_min_perimeter.setter def marker_min_perimeter(self, value: int):
pytest.config.argon_skip_now("argon transformer error")  # todo triage </s> axes = ng.make_axes([ng.make_axis(length=4),	test_reduction def test_reduction(reduction, sub_axes): if reduction == 'prod': ng.make_axis(length=4), ng.make_axis(length=4)])
# todo: use mode, state </s> ret = []	availableSizes def availableSizes(self, mode, state): size = self._engine.size() if size.isValid():
pass # todo </s> if '%s' in args:	parse_exec pass # TODO generate tmp page if '%d' in args: if hasattr(page, 'source') and isinstance(page.source, File): args[args.index('%s')] = page.source.path
# todo: targets are always updated if destination directory is new, right? </s> updated_targets = self.updater.updated_targets(	download_target filename = os.path.join( destination_directory, target_filepath ) targets = [ self.updater.target( filepath ) ] targets, destination_directory )
# todo: remove compatability hook </s> shutil.copyfile(os.path.join(self.freeze_dir,esky_control_dir,"lockfile.txt"),os.path.join(self.freeze_dir,"esky-lockfile.txt"))	_run with open(lockfile,"w") as lf: lf.write("this file is used by esky to lock the version dir\n") shutil.copyfile(os.path.join(self.freeze_dir,ESKY_CONTROL_DIR,"bootstrap-manifest.txt"),os.path.join(self.freeze_dir,"esky-bootstrap.txt")) print "zipping up the esky"
if self._ndim == 3: # todo: use hasz </s> array = c_double * 3	ctypes @property def ctypes(self): return array(self.x, self.y, self.z) else:
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
#todo fixme: we need to check that we aren't adding a duplicate </s> item.addclaim(claim)	main pywikibot.output('Adding %s --> %s' % (claim.getID(), claim.getTarget().getID()))
# todo(b/147499373): if none-arguments were uniformly represented as empty </s> after_param_type = computation_types.structtype([	force_align_and_split_by_intrinsics after_param_name = next(name_generator) if comp.parameter_type is not None: ('original_arg', comp.parameter_type), ('intrinsic_results',
# todo: to be removed in v2.8.0 </s> return self.class_from_path_setting('section_menu_class_path')	SECTION_MENU_CLASS def SECTION_MENU_CLASS(self):
'location_type': loc.location_type.name,  # todo: remove when types aren't optional </s> 'uuid': loc.location_id,	loc_to_json ret = { 'name': loc.name, 'is_archived': loc.is_archived, 'can_edit': True
# todo: implement this </s> return 'calibre'	prefix_for_location def prefix_for_location(self, on_card):
# todo : uncomment these out to set default to densejacobian, once we have resolved further </s> new_jacvec_prod != self._inst_functs['compute_jacvec_product']))	_configure (new_jacvec_prod is not None and
# todo(tr3buchet) - remove comment in multi-nic </s> ips = db.fixed_ip_get_all_by_instance(admin_context, instance['id'])	spawn VMHelper.create_vbd(self._session, vm_ref, vdi_ref, 0, True) admin_context = context.get_admin_context() for network in db.network_get_all_by_instance(admin_context, instance['id']):
# todo: deprecated - remove in version 0.10 </s> return [states]	_preprocess_states E.g., to a create list of states with deleted history for augmented Memoization"""
# todo: reduce_mean? </s> action = tf.argmax(	get_action state = np.expand_dims(state, axis=0).astype(np.float64) action_probs = self._get_action_body(tf.constant(state)) tf.reduce_sum(action_probs * self.z_list_broadcasted, axis=2), axis=1).numpy()[0] return action
pass  # todo </s> self._current_command = []	__init__ self._device = None
# todo: finish </s> pass	loadUuid def loadUuid(self):
# todo: remove this - cura-4482 </s> material = self._global_container_stack.material	_onGlobalContainerChanged except TypeError: pass material.nameChanged.disconnect(self._onMaterialNameChanged) quality = self._global_container_stack.quality
# todo(user): keepalive is not enabled on the netperf control socket. </s> vm.applysysctlpersistent({	PrepareNetperf def PrepareNetperf(vm): vm.Install('netperf') 'net.ipv4.tcp_keepalive_time': 60, 'net.ipv4.tcp_keepalive_intvl': 60,
# todo: this is not handling decoding errors all that well. </s> if str is not unicode and type(module_name) is unicode:	resolveModuleName def resolveModuleName(module_name): module_name = str(module_name) module_name = ModuleName(module_name)
kwargs['application'] = application.objects.get(client_id=credentials['client_id'])  # todo: this should be cached one day </s> kwargs.update(credentials)	get scopes, credentials = self.validate_authorization_request(request) kwargs['scopes'] = scopes self.oauth2_data = kwargs form = self.get_form(self.get_form_class())
# todo: add these lines back </s> inspired by the forget/input gates in lstm.	write According to the paper, the erase/add decomposition was
# todo: move this to sublime_lib; make it accept a point or a region. </s> def eol(line):	EOL return line.end()
x_vec = np.matrix(xs).t  # todo python3: fix np.matrix </s> a_b = lstsq(y_mtx, x_vec, rcond=-1)[0]	sigmoid_adjust_raw zs = -np.log(1.0 / ys_.T - 1.0) Y_mtx = np.matrix((np.ones(len(ys_)), zs)).T  # TODO python3: fix np.matrix a = a_b.item(0) b = a_b.item(1)
# todo: add docs </s> def init_fn(prox_center=0.):	init_fn x_t = 0. x_avg = 0.  # average of primal sequence
"""todo doc me""" </s> table = [['foo', 'bar', 'baz'],	test_profile_default def test_profile_default(): ['A', 1, 2], ['B', '2', '3.4'],
# todo:eliminate this </s> logger.debug("testinga: " + str(results) )	query_dataset metric = DistanceFunctionFactory.create('euclidean', layout, {'descriptorNames': feature_names}) results = view.nnSearch(q,str(filter),metric).get(int(number_of_results)) p = Point() DB = DataSet()
# todo find out what is best used here! </s> 'preferred_dtype': np.float32}	get_meta_information 'is_deterministic': True, 'handles_sparse': False,
# todo: load state into here </s> current_chunk +	inner_mapper new_schedule.extend( [CallKernel(kernel_name=new_kernel_name)] + [ReturnFromKernel(kernel_name=new_kernel_name)]) new_schedule.extend(
# todo instead of 3*t, use log_sf </s> self.durations[-1] += sample_discrete(dur_distn.pmf(np.arange(dur+1,3*self.t))) + 1	generate_states dur = self.durations[-1] dur_distn = self.dur_distns[self.stateseq_norep[-1]] assert np.any(self.durations[-1] >= dur_distn.r_support) return ret
# todo: enhance this method to set a flag and alert an admin to review content since </s> shutil.move( uploaded_file_name, full_path )	upload else: full_path = os.path.abspath( os.path.join( repo_dir, uploaded_file_filename ) ) commands.add( repo.ui, repo, full_path ) try:
# todo: implement link-local handling in networkmanager backend and move this test into commontests() </s> self.assert_iface_up(self.dev_e_client, [], ['inet6 2600:'])	test_eth_dhcp6_off_no_accept_ra self.generate_and_settle()
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_METADATA_postScriptName_matches_font_filename_success def test_result_METADATA_postScriptName_matches_font_filename_success(self): r = run_set(p, 'result') success_tests = r['success']
# todo: we should consider the probabilities of `task1 failure -> task2 failure` and </s> count_runs = collections.counter()	generate_failing_together_probabilities def generate_failing_together_probabilities(push_data): count_single_failures = collections.Counter() count_both_failures = collections.Counter()
# todo: does this import need to be delayed because </s> from pyomo.solvers.plugins.solvers.persistent_solver import \	_populate_bundle_dual_master_model def _populate_bundle_dual_master_model(self, ph): PersistentSolver current_iteration = ph._current_iteration
for joint in annos:  # todo : speed up with affine transform </s> adjust_joint = []	keypoint_random_rotate img = ret[newy:newy + newh, newx:newx + neww] adjust_joint_list = [] for point in joint: if point[0] < -100 or point[1] < -100:
# (todo) chagne the dgl link </s> if osp.isdir(self.folder) and (not osp.exists(osp.join(self.folder, f'release_v{self.version}.txt'))):	__init__ self.version = 1 self.url = f'http://ogb-data.stanford.edu/data/lsc/pcqm4m-v2.zip' print('PCQM4Mv2 dataset has been updated.') if input('Will you update the dataset now? (y/N)\n').lower() == 'y':
# todo check </s> if self.mimetype == 'application/x-id3':	format @interfacedoc def format(self): self.mimetype = 'audio/mpeg' return self.mimetype
# todo: find better way to avoid entering "__hh_previous_frame" to avoid traceback added by `tracers.locationtracer` </s> return default_enter(path, key, value)	_enter if isinstance(value, Traceback): return dict(), []
#todo: this variable plan_id is never used </s> if "plan_id" in request.post:	daily_billing_report to_date = request.POST['to_date'] end_date = ceil_strdate(to_date, 'end') plan_id = request.POST['plan_id'] if "switch" in request.POST:
# todo: get these values from the same place as setup.py </s> return appdirs.user_data_dir("ice","scott rice")	app_data_directory def app_data_directory():
#todo: define tests which check db contents </s> test_gdf = gdf() # test default configuration	test_GDF_get_ndarrays def test_GDF_get_ndarrays(self): "Test GDF get_ndarrays function" ndarray_dict = test_gdf.get_ndarrays(self.TEST_2D_DIMENSION_RANGE_DICT) ndarray_dict = test_gdf.get_ndarrays(self.TEST_2D_DIMENSION_RANGE_DICT, ndarray_type_tags=['LS5TM'])
# todo: remove in 1.3 </s> with pytest.warns(futurewarning, match="the 'verbose' parameter"):	test_imputation_deletion_warning_feature_names ) imputer = SimpleImputer(strategy=strategy, verbose=1) imputer.fit(X) assert_array_equal(imputer.feature_names_in_, feature_names)
return  # todo return placeholder "[error]" track? </s> if sp_track.availability != spotify.trackavailability.available:	to_track_ref return  # TODO Return placeholder "[loading]" track? if sp_track.error != spotify.ErrorType.OK: return  # TODO Return placeholder "[unavailable]" track? return models.Ref.track(uri=sp_track.link.uri, name=sp_track.name)
# todo: self.assertfalse(prop.is_valid(np.bool8(false))) </s> self.asserttrue(prop.is_valid(np.int8(0)))	test_Int try: import numpy as np self.assertTrue(prop.is_valid(np.int8(1))) self.assertTrue(prop.is_valid(np.int16(0)))
# todo is this serious enough to raise a canerror exception? </s> if res != 0:	set_filters filter_struct, len(filter_struct) ) log.error('Setting filters failed: ' + str(res))
# @todo: deprecate </s> popup_fields = self.popup_fields	LayerFeature output["popup_format"] = popup_format else: if popup_fields: popup_label = self.popup_label
# todo new message here </s> routing_key = 'sms.ack.%s' % (self.transport_name)	submit_sm_resp log.msg("Mapping transport_msg_id=%s to sent_sms_id=%s" % ( transport_msg_id, sent_sms_id)) message = Message(**{ 'id': sent_sms_id,
# todo: remove return </s> if true:	test_brle_split_merge def test_brle_split_merge(self): return x = [300, 600, 10, 0]
# todo: if you try to fix this first read issue #958 and 1018 </s> hbox = gtk.hbox()	get_message_content def get_message_content (heading_text, message_text, image_stock_id): vbox = Gtk.VBox()
# todo(nzw0301) support intloguniform </s> if isinstance(distribution, intloguniformdistribution):	_initialize_x0 x0 = {} for name, distribution in search_space.items(): raise NotImplementedError if isinstance(distribution, UniformDistribution):
# todo: add and store preprocessing errors. </s> logging.error('unable to decode shell.')	_ParseFileData shell = row[6].decode('utf-8') except UnicodeDecodeError: user_account = artifacts.UserAccountArtifact( identifier=identifier, username=username)
#todo check max_amount on conversion?? </s> def create_new_offer(self):	create_new_offer total_amount = self.total_amount(self.our_offers()) if total_amount < self.min_amount:
# todo(jflesch): check last_mod ! </s> print ("%s has been modified. reindexing ..." % doc.docid)	__update_index progress_cb(progress*3, len(docdirs)*4, self.INDEX_STEP_READING, doc) last_mod = datetime.datetime.fromtimestamp(doc.last_mod) docid = unicode(doc.docid) txt = u""
# todo: to be implemented </s> pass	handle_cli_command self.print_module_description(details, mod_name, mod_path) elif cmd[:2] in (['modules', 'enable'], ['modules', 'disable']): else: print_stderr('Error: unknown command')
# todo test that citext.sql gets loaded with 9.0.x </s> version_info = [('postgresql 9.1.1 blah blah blah',)]	test_setupdb_app_main with mock.patch(self.psycopg2_module_path) as psycopg2: app = setupdb_app.SocorroDB(config) psycopg2.connect().cursor().fetchall.return_value = version_info result = app.main()
# todo: use bezier curve instead of polygon. perhaps aggdraw module. </s> draw.polygon(path, fill=(255 - color))	draw_vector_mask int(knot.anchor[0] * height), ) for knot in subpath] del draw return mask.crop(layer.bbox)
#todo_jay: put seed in job </s> qasm_circuit = qasmsimulator(job["compiled_circuit"], random.random()).run()	run_local_qasm_simulator one_result = {'result': None, 'status': "FAIL"} if job["shots"] == 1: one_result["result"] = qasm_circuit["result"] one_result["status"] = 'COMPLETED'
# todo: remove after py2.5 deprecation </s> if sys.version_info[:2] > (2, 5):	test_blasttab_2226_tblastn_011 def test_blasttab_2226_tblastn_011(self): filename = 'Blast/tab_2226_tblastn_011.txt' with warnings.catch_warnings(record=True) as w: warnings.simplefilter('always')
# todo: replace sdolenc with tducret after merge </s> url = "https://raw.githack.com/sdolenc/\	test_amazonscraper_not_satisfied_url def test_amazonscraper_not_satisfied_url(): amazon-scraper-python/urltests/test/not_satisfied.html" products = amazonscraper.search(
# todo if the other logging that is happening is less frontpage </s> pbar = ui.get_progressbar(	emit update = getattr(record, 'dlm_progress_update', None) if pid not in self.pbars: label=getattr(record, 'dlm_progress_label', ''), unit=getattr(record, 'dlm_progress_unit', ''),
self.group_self(self._grouped_on)  # todo: think about removing </s> groups = []	apply_on_groups def apply_on_groups(self, delayedFcn, otherDf): for group_vals, group_inds in self._group_dict.iteritems(): subsetDf = otherDf[group_inds]
if isinstance(err, asyncio.cancellederror):  # todo: remove when updated to 3.8 </s> raise	Ledger account.change.gap, channel_count, len(account.channel_keys), claim_count) except Exception as err: log.exception( 'Failed to display wallet state, please file issue '
# todo: signal the user </s> self.finish()	TaskStatementViewHandler if service not in self.application.service.remote_services or \ not self.application.service.remote_services[service].connected: return self.application.service.remote_services[service].get_file(\
# pycryptodome does not expose the mode attribute </s> plaintext = "a" * 32	test_cleartext_message_matches_decrypted_message_with_block_cipher client_enc_cipher = sec_params.get_client_enc_cipher() client_dec_cipher = sec_params.get_client_dec_cipher() self.assertEqual(client_dec_cipher.decrypt(client_enc_cipher.encrypt(plaintext)), plaintext)
# todo: what actually raises valueerror in the following code? </s> try:	read def read(self, *args, **kwargs): result = super(GzipStreamFile, self).read(*args, **kwargs) if result is None:
# todo: test that valueerror is raised </s> validate.direction(direction)	directional_emd func = effect_emd else: return round(func(d1, d2), config.PRECISION)
# todo docs </s> return self.run_with_configuration(configuration)	run }
# todo: command+c for mac </s> tk.messagebox.showerror("internal error. use ctrl+c to copy",	on_tk_exception sys.last_traceback = tb traceback.print_exception(exc, val, tb) traceback.format_exc())
# todo support intloguniformdistribution </s> else:	_initialize_x0 log_low = math.log(distribution.low) x0[name] = math.exp(np.mean([log_high, log_low])) raise NotImplementedError( "The distribution {} is not implemented.".format(distribution)
# todo complete this method </s> partition, kptlist, dtype)	eeccsd return ipccsd(eom, nroots, koopmans, guess, left, eris, imds,
# todo: move into toolevaluator test(s) </s> with self._prepared_wrapper() as wrapper:	test_prepare_sets_version_command def test_prepare_sets_version_command(self): assert TEST_VERSION_COMMAND in wrapper.write_version_cmd, wrapper.write_version_cmd
# todo(jamalex): could do md5 checks here instead, to be ultra-safe </s> if os.path.isfile(path) and os.path.getsize(path) == f.file_size:	handle_network_download url = paths.get_content_storage_file_url(filename) path = paths.get_content_storage_file_path(filename) overall_progress_update(f.file_size) continue
# todo(rbharath): can this be removed? </s> integer_type = np.int8 if sys.version_info[0] == 2 else np.int32	seq_one_hot_encode Array of genetic sequences sequence_length = len(sequences[0]) print("sequences") print(sequences)
# todo(cmaloney): good exception catching, etc </s> def wsgi_app(env, start_response):	wsgi_app subscriber.handle_event(json.load(env['wsgi.input'])) start_response('200 OK', [('Content-Type', 'text/html')])
except (testtransactionfailed, validationerror, valueerror):  # todo: 1950 </s> self.log.debug(f"batch of {len(test_batch)} is too big. let's stick to {len(test_batch)-1} then")	deposit_next_batch dry_run=True, gas_limit=gas_limit) break else:
# todo! get destination path from user save dialog. </s> path = '/tmp/{name}'.format(name=name)	export_sprite pdb.gimp_edit_copy(layer) imgtmp = pdb.gimp_edit_paste_as_new() self.save_png(imgtmp, path) pdb.gimp_image_delete(imgtmp)
#todo: add support for network load balancer </s> if pdim.albhours:	calculate query = ((priceQuery['usageType'] == consts.REGION_PREFIX_MAP[pdim.region]+'DataProcessing-Bytes') & (priceQuery['operation'] == 'LoadBalancing')) pricing_records, cost = phelper.calculate_price(consts.SERVICE_ELB, elbDb, query, pdim.elbDataProcessedGb, pricing_records, cost) albDb = dbs[phelper.create_file_key((consts.REGION_MAP[pdim.region], consts.TERM_TYPE_MAP[pdim.termType], consts.PRODUCT_FAMILY_APPLICATION_LOAD_BALANCER))] query = ((priceQuery['usageType'] == consts.REGION_PREFIX_MAP[pdim.region]+'LoadBalancerUsage') & (priceQuery['operation'] == 'LoadBalancing:Application'))
el_movieposter.set('onselect', "atv.loadurl('"+el_path+"')")  # todo: 'select' - show metadata </s> el_movieposter.set('onplay', "atv.loadurl('"+el_path+"')")	XML_ActorView el_moviePoster.set('id', 'shelf_item_'+str(aTV_shelf_item)) aTV_shelf_item += 1 el = etree.SubElement(el_moviePoster, 'label') el.text = i.get('title')
#todo: check for continous or discrete, only continuous supported right now </s> dico = 'c'	gram Wc = gram(sys,'c') Wo = gram(sys,'o') D,V = np.linalg.eig(sys.A)
#todo(#212): use a map construct instead of unrolling. </s> lhs = batching.move_dim_to_front(lhs, lhs_bdim)	conv_general_dilated_batch_rule lhs_dim, rhs_dim, out_dim = dimension_numbers if lhs_bdim is not None and rhs_bdim is not None: rhs = batching.move_dim_to_front(rhs, rhs_bdim) outputs = [
#todo: namespaces too hardwired, clean-up... </s> header = request('header' , ns=soap_namespaces.values(),)	call if k.startswith("wsse:")]) if 'wsse:Security' in self.__headers: k = 'wsse:Security' v = self.__headers[k]
'"ed25519_secret_seed", "pre_auth_tx", "sha256_hash"'.format(version_byte_name))  # todo </s> payload = version_byte + data	encode_check except KeyError: raise KeyError('{} is not a valid version byte name. expected one of "ed25519_public_key", ' crc = _calculate_checksum(payload) return base64.b32encode(payload + crc).decode('utf-8')
# todo: replace </s> corrupted = concatenate_fct(candidate_entities=all_entities, tuples=tuples)	_compute_metrics newshape=(1, 2)) tuples = np.repeat(a=tuple, repeats=all_entities.shape[0], axis=0) corrupted = torch.tensor(corrupted, dtype=torch.long, device=device) scores_of_corrupted = kg_embedding_model.predict(corrupted)
# todo: uncomment when adding support for literal hex bytes </s> print(bytearray(b'hello world   ').upper())	test_upper print(bytearray(b'hello world').upper())
# todo: this is not using any cache... </s> data = pod.read_yaml(path, locale=locale)	construct_string doc._locale_kwarg or doc.collection.default_locale) if reference: return YamlLoader.deep_reference(reference, data) return None
# todo: limit/check colorcode </s> if colorcode is none:	LaunchpadPro if number < 0 or number > 99: return colorcode = LaunchpadPro.COLORS['white'] self.midi.RawWriteSysEx( [ 0, 32, 41, 2, 16, 40, number, colorcode ] )
# todo move to a proper logging framework </s> global debug	set_flags def set_flags(debug: bool, quiet: bool) -> None: global QUIET if debug:
# todo optimize: special case where there is only one dynamic </s> child_loc = ["add", "src", unwrap_location(src_loc)]	abi_decode if parent_abi_t.is_complex_type(): if abi_t.is_dynamic(): child_loc = LLLnode.from_list(child_loc, typ=o.typ, location=src.location) else:
# todo: remove anytime in 2016 </s> _assert(false, "status in filter wasn't set - this isn't expected to be possible")	get_prefix_and_key_for_filter_results_and_parsed_params prefix = "%s %s" % ("status", prefix) else: def _get_key(): if parsed_params.module is not None and parsed_params.get_module_int() is None:
# todo return, catch exception in main() </s> raise e	create_r2_script except Exception as e:
# todo: implement me </s> pass	test_flags_a_version_for_human_review def test_flags_a_version_for_human_review(self):
mock = create_mock_json('tests/resources/list_race_details.json')  # todo </s> mock_response.return_value = mock	test_list_scores @mock.patch('betfairlightweight.endpoints.scores.Scores.request') def test_list_scores(self, mock_response): mock_update_keys = mock.Mock() response = self.scores.list_scores(mock_update_keys)
# todo 目前仅在 华泰子类 中实现 </s> log.info('目前仅在 华泰子类 中实现, 其余券商需要补充')	get_exchangebill :param end_date: 20160211 :return:
codecs.decode('5050827d08000000d00745b2cf451b00', 'hex'),  # tcp random cmd_ack_ok todo: generate proper sequenced response </s> codecs.decode('5050827d10000000dc053b59d0983500f401ae4301000000f19449000000120c07130906', 'hex'), # tcp prepare_data 1011	test_tcp_live_connect_small codecs.decode('5050827d04020000dd05942c96631500f801000001000e0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003830380000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003832310000000000000000000000000000000000000000000300000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003833350000000000000000000000000000000000000000000400000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003833310000000000000000000000000000000000000000000500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003833320000000000000000000000000000000000000000000600000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003836000000000000000000000000000000000000000000000c0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000383432000000000000000000000000000000000000000000','hex'), #DATA directly(not ok) codecs.decode('5050827d08000000d00745b2cf451b00', 'hex'), # tcp random CMD_ACK_OK TODO: generate proper sequenced response codecs.decode('5050827df8030000f401ae43010000003131343030363400000000000000000000000000000000000f00120b1d0c3703', 'hex'), # reg_event! codecs.decode('5050827d08000000d007fcf701003200', 'hex'),  # tcp CMD_ACK_OK
# todo document </s> def _evaluate_cut(subsystem, partition, unpartitioned_constellation):	_evaluate_cut forward_cut = Cut(partition[0], partition[1]) forward_constellation = subsystem.constellation(forward_cut)
#todo the tooltip should actually hide on its own. ticket #1096 </s> cb = callback.chainedcallbacks(cb1, cb2)	ShipOverviewTab None ) # hides the resource status widget cb2 = Callback( self.widget.child_finder('foundSettlement').hide_tooltip) events['foundSettlement/mouseExited'] = cb self.widget.mapEvents(events)
# todo: check the values from the related manager </s> pass	edit_object for i in self.data_update: if getattr(entry_after, i).__class__.__name__ == 'ManyRelatedManager': else: self.assertEqual(getattr(entry_after, i), self.data_update[i])
#todo(bcwaldon): use the schema to actually validate something </s> json.loads(response.text)	test_resource response = requests.get(path) self.assertEqual(response.status_code, 200)
pass # todo </s> def _listplaylistinfo(self, name):	_listplaylistinfo @register(r'^listplaylistinfo (?P<name>.+)$')
# todo figure out something useful to do with the newbranch param </s> @util.transform_notgit	exchangepush def exchangepush(orig, repo, remote, force=False, revs=None, newbranch=False, bookmarks=()):
# todo: move to pytest.mark.parametrize once nose gone </s> @dec.skip_if_not_win32	test_arg_split_win32 def test_arg_split_win32(): tests = [['hi', ['hi']],
# todo - get rid of these </s> self.value_boundaries = none #x_min, x_max, y_min, y_max	DayLine self.in_motion = False self.days = [] self.x_factor, self.y_factor = None, None self.graph_x, self.graph_y = 0, 0
#todo implement transformation for corner nodes </s> if 'quad8' in vecname:	data_in_material_coord thetadeg_new = thetadeg_to_principal(Sxx_theta, Syy_theta, Sxy_theta) new_vector.data[:, :, 4][:, check] = thetadeg_new for i in [2, 3, 4, 5, 6, 7, 8, 9]: new_vector.data[:, i, :] = 0
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_depth_too_small def test_fail_depth_too_small(self): ``depth`` is < 1.
# todo: check output </s> output = run_model(prob)	test_hierarchy_iprint3 prob.setup(check=False)
# todo(b/138845899): consider use span instead of id. </s> last_blessed_model = max(	_fetch_last_blessed_model previous_blessed_models.append(a) if previous_blessed_models: previous_blessed_models, key=lambda artifact: artifact.id) return (
# todo: check whether it's already installed?. see yum notes  yum list installed "$@" >/dev/null 2>&1 </s> cmd += 'yum install'	install opts += ' --reinstall' elif install_type == 'yum': if 'yum' in options: opts = options['yum']
# todo: @sbharadwajj implement and test </s> def _weight_jac_mat_prod(self, module, g_inp, g_out, mat):	_weight_jac_mat_prod raise NotImplementedError
# todo: must be implemented </s> pass	range_from_chapters def range_from_chapters(crawler, times=0):
# todo debug </s> print ("rule element evaluates "	_evaluateRuleElementsRecursively + "to not triggered. Set 'and' rule " + "also to not triggered.") + "to not triggered. Set 'and' rule " + "also to not triggered.")
# todo: ignoring repeat letters </s> targetstart = targetnumbers[0]	_copyMultipleMeasures if len(targetNumbers) == 1: # this is an encoding error raise TranslateRomanTextException('a multiple measure range cannot copy a single measure') targetEnd = targetNumbers[1] measures = []
# todo(dcramer): remove in 7.6.x </s> project_id = filters.get('project_id')	record_project_tag_count if not created: return if not project_id: project_id = filters['project'].id
# todo: this is all just debugging stuff and can be removed </s> if debug:	translate self.phi = phih self.u = uh uh = [ui.copy() for ui in uh] gh = self.g.copy()
# todo: improve this. </s> self.view.erase_regions('vi_inc_search')	on_change def on_change(self, s): next_hit = self.view.find(s, self.view.sel()[0].b) if next_hit:
# todo in python 2.7 or later, this should be </s> only = set(get_column_name(column) for column in only)	__init__ ' `additional_attributes` keyword argument') if only is not None: only |= {'type', 'id'} if exclude is not None:
pass                # todo(nnorwitz): impl </s> elif isinstance(token, ast.union):	_ProcessTypedef classes_used[token.name] = True elif isinstance(token, ast.Struct): pass                # TODO(nnorwitz): impl
# todo(dspasovski): fix this. </s> raise skiptest	TestIndexLanding @mock_es def test_good_cat(self): r = self.client.get(self.url) eq_(r.status_code, 200)
# todo users aren't going to match </s> if django_model is not osfuser:	validate_page_of_model_data django_ids = [get_pk(modm_obj, django_model, modm_to_django) for modm_obj in page_of_modm_objects] django_objects = django_model.objects.filter(id__in=django_ids) try: assert len(django_ids) == len(django_objects) == len(page_of_modm_objects), 'Lost some keys along the way for {}.{}\n' \
transparent = false  # todo </s> if transparent:	__init__ self._height = filmsize[1] self._border = utils.calc_blender_border(scene) bufferdepth = 4 self._output_type = pyluxcore.FilmOutputType.RGBA_IMAGEPIPELINE
# todo: check the data! </s> self.asserttrue(len(list(pipe)) > 0)	test_tail pipe_def = self._get_pipe_def(pipe_file) pipe = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def)
# todo: test me </s> self.cache_errors = extensions_api_settings.default_cache_errors	__init__ self.key_func = key_func if cache_errors is None: else: self.cache_errors = cache_errors
pass # todo: explain </s> pass # todo: explain	status403 def status403(self):        # Forbidden
# todo: the following should be handled within inputspecs ? </s> self.inputspecs.load_all_specs()	updateAll if self.DEBUG: print("input_widgets.updateAll:\n",self.sender().objectName()) self.inputSpecs.color_design_button("ok") self.inputInfo.showInfo() self.inputCoeffs.show_coeffs()
# todo(jblespiau): we can simply use buf.xla_shape() when version 0.1.58 is </s> xla_shape = getattr(buf, "xla_shape", buf.shape)()	from_dlpack client = getattr(backend, "client", backend) buf = xla_client._xla.dlpack_managed_tensor_to_buffer(dlpack, client) assert not xla_shape.is_tuple() aval = core.ShapedArray(xla_shape.dimensions(), xla_shape.numpy_dtype())
# todo check </s> return self.mimetype	format @interfacedoc def format(self):
pass # todo </s> field = field.lower()	_music_db_list - capitalizes the field argument.
_imdb_queue['quality'] = 'dvd' # todo: get defaul from config somehow? </s> else:	optik_imdb_queue _imdb_queue['quality'] = parser.rargs[2]
# todo: manage other values </s> if node.get("meetorslice") == "slice":	draw translate_y = -size(node.get("refY")) if node.get("preserveAspectRatio", "xMidYMid"): scale_value = max(scale_x, scale_y) else:
# todo: order of attributes is not assured; allow for any order. </s> match1 = '<words default-y="45.0" font-weight="bold" justify="left">super fast</words>'	testExportMetronomeMarksD p.repeatAppend(note.Note('g#3'), 8) p.insert(0, tempo.MetronomeMark('super fast', number=222.2)) match2 = '<per-minute>222.2</per-minute>' raw = fromMusic21Object(p)
# todo differentiate integer and float? using custom renderers and </s> return {	get_cell_type def get_cell_type(dt): 'b': 'checkbox', 'i': 'numeric',
self.assertequal(out.strip(), "inactive") # todo real "failed" </s> logg.info("== 'start' will have a later exiting service as remaining active")	test_4090_simple_service_RemainAfterExit logg.info(" %s =>%s \n%s", cmd, end, out) self.assertEqual(end, 3) cmd = "{systemctl} start zzr.service {vv}" out, end = output2(cmd.format(**locals()))
return none # xxx todo return accordingly generated command for initiating this proxy </s> proxyclientudpport = proxy["ports"]["client"]	getProxyConfig proxyServerUDPPort = proxy["ports"]["server"]
# todo: handle 2d and 3d data </s> pupil_datum = self._pupil_positions_of_eye.by_ts(frame.timestamp)	__call__ eye_image = frame.img try: self.renderer.render_pupil(eye_image, pupil_datum) except ValueError:
# todo(ls): revert this loop to "yield from" </s> for __x in self.format_exception_only(): yield __x	format yield 'Traceback (most recent call last):\n' for __x in self.stack.format(): yield __x
pass # todo </s> def add_indexed(self, mode, state, indices, *data):	add_indexed
# todo remove me </s> import yaml	parseSDFGeometry else: geometrydict['scale'] = [1.0, 1.0, 1.0] print(yaml.dump(geometrydict)) return geometrydict
#todo: check if/where this is used; if not used externally - remove </s> return self.marker_detector.marker_min_confidence	Surface_Tracker @property def marker_min_confidence(self) -> float: @marker_min_confidence.setter def marker_min_confidence(self, value: float):
# todo(vish): move this into the driver layer </s> yield process.simple_execute(	detach_volume volume_ref = db.volume_get(context, volume_id) target = volume_ref['mountpoint'].rpartition('/dev/')[2] "sudo virsh detach-disk %s %s " % (instance_id, target)) db.volume_detached(context, volume_id)
# todo: constants file for "broadcast" </s> self.socket.setsockopt(zmq.subscribe, 'broadcast')	_setsockopts def _setsockopts(self): if self.opts['zmq_filtering']: self.socket.setsockopt(zmq.SUBSCRIBE, self.hexid) else:
# todo change to native framework call, when plex allows token in header </s> except exception, e:	getPlayListItems users = plexTV().getUserList() userToken = users[user]['accessToken'] Log.Exception('Exception getting the token for a user was: %s' %str(e)) try:
# todo remove after unified backends </s> if get_backend() == "pytorch":	test_learning_2x2_grid_world ) agent_config = config_from_path("configs/apex_agent_gridworld_for_2x2_grid.json") agent_config["memory_spec"]["type"] = "mem_prioritized_replay" executor = ApexExecutor(
# test_qnetwork_weight_quantization: todo </s> all_weights = np.array(all_weights)	test_qrnn all_weights.append(w)
# todo: make grouper in query </s> grouper = grouper(dimension='time',	get_data_array geopolygon = query.geopolygon or get_bounds(observations, crs=(query.output_crs or get_crs(observations))) geobox = GeoBox.from_geopolygon(geopolygon, resolution=(query.resolution or get_resolution(observations))) group_by=lambda ds: ds.center_time, units='seconds since 1970-01-01 00:00:00')
# todo: replace with "yield from" when dropping python 2. </s> for stream in streams.items():	_get_live_streams try: streams = self._get_desktop_streams(channel_id) has_desktop_streams = True yield stream
# todo: it would be nice to be async about this. set 1 second timeout. </s> try:	version_check 'version': bayeslite.version.__version__ } r = requests.post(SERVICE, data=payload, timeout=1) if r.status_code == 200 and r.json.result != "current":
# todo(dspasovski): fix this. </s> raise skiptest	TestIndexLanding @mock_es def test_good_cat(self): r = self.client.get(self.url) eq_(r.status_code, 200)
# todo: make this a hard error, instead of a silent overwrite </s> logging.warning("kvm: overriding disk_cache setting '%s' with 'none'"	_GenerateKVMBlockDevicesOptions cache_val = ",cache=none" elif aio_mode == constants.HT_KVM_AIO_NATIVE and disk_cache != "none": " to prevent QEMU failures in version 2.6+", disk_cache)
# todo: don't rely on the touch command </s> phlsys_subprocess.run("touch", args.ok_touch_path)	run_once if args.ok_touch_path: try: except Exception: pass  # XXX: we don't care atm, later log this
raise notimplementederror  # todo </s> def compute_generator(self, generator, **kwargs):	compute_generator
# todo: look this up in one query </s> for user_id in obj['collaborator_ids']:	get_by_mbid obj['collaborator_ids'] = playlist_collaborator_ids.get(obj['id'], []) collaborators = [] user = db_user.get(user_id) if user:
# todo: change template so it iterates through form and not formfields </s> context['form_fields'] = context['form']	WgerFormMixin context.update(csrf(self.request)) context['sidebar'] = self.sidebar context['custom_js'] = self.custom_js if self.form_action_urlname:
# todo complete this method </s> partition, kptlist, dtype)	eeccsd return ipccsd(eom, nroots, koopmans, guess, left, eris, imds,
# todo(sbauza): the current placement noauthmiddleware returns a 401 </s> return self._client.post(	_fake_post def _fake_post(self, *args): (url, data) = args[1:] url, json=data, endpoint_override="http://127.0.0.1:%s" % self.service.port,
# todo: chunk transmissions into more managable lenths </s> chunk = transmissions.filter(q).values_list('pk', flat=true)	send for backend_id in backends.distinct(): q = Q(connection__backend_id=backend_id) send_transmissions.delay(backend_id=backend_id, message_id=message_id,
# todo: verify exception type once those exists </s> dotfile(name, target).link()	test_link for x in range(2, times): with pytest.raises(Exception): assert target.check(file=1, link=0) assert name.check(file=1, link=1)
# todo assert cls.__tablename__ == '' </s> with dbcleanup(session, cls):	test_LibraryDatasetCollectionTagAssociation def test_LibraryDatasetCollectionTagAssociation(model, session, library_dataset_collection_association, tag, user): cls = model.LibraryDatasetCollectionTagAssociation user_tname, value, user_value = 'a', 'b', 'c' obj = cls(user=user, tag_id=tag.id, user_tname=user_tname, value=value)
# todo executor, max_workers </s> to_parquet(chunk, conn, bucket,	to_sql for key, val in zip(partitions, list(keys))]) for chunk in get_chunks(group, chunksize): '{0}/{1}'.format(key_prefix, partition_prefix), compression=compression,
# todo - use new error message api! ts </s> self.showerrormessage(m.message(str(mymessage)))	spawnError myMessage = getExceptionWithStacktrace( theException, theHtml=True, theContext=theMessage)
# todo: fix comment for this and simplifier </s> obj_id = tensor_tuple[0]	_detail_log_tensor Examples: ptr = _detail_pointer_tensor(data) return LogTensor(owner=worker, id=obj_id)
# todo: do we need to do this? </s> self._parser.push_symbol(unionend())	read_index label, data = self._current.popitem() self._current = data else: if self._current[self._key] is None:
# todo: when hytra is supported on windows, we shouldn't skip the test and throw an assert instead </s> try:	testCSVExport @timeLogged(logger) def testCSVExport(self): import hytra except ImportError as e:
# todo xxx graalvm change </s> raise unittest.skiptest("missing threading support")	ThreadedEchoServer self.daemon = True def __enter__(self): self.start(threading.Event()) self.flag.wait()
# todo(ytknzw): add more specific assertion with the test case. </s> figure = plot_param_importances(study)	test_plot_param_importances assert figure.has_data() is False study = prepare_study_with_trials(with_c_d=True) assert figure.has_data() is True plot_param_importances(study, evaluator=MeanDecreaseImpurityImportanceEvaluator())
# todo handle valueerror </s> if type != none:	sharedproperty value = self.extra_props[name] del self.extra_props[name] value = type(value) self.__dict__[name] = value
#todo - use a context manager here once we drop python 2.6 </s> self.assertraises(valueerror, somcluster,	test_somcluster [ 1, 1, 1, 1, 1], [ 1, 1, 1, 1, 1]], int) **{"data": data, "mask": mask, "weight": weight, "transpose": 0, "nxgrid": 10, "nygrid": 10,
# todo don't break, exhaust the iterator, otherwise </s> return self._input_chat	EventCommon self._chat = d.entity self._input_chat = d.input_entity @property def client(self):
# todo: configurable rsync options? </s> output = run_command("rsync -avtz --delete %s/ %s" %	YumRepoMockReleaser debug(output) print("Syncing yum repository back to: %s" % rsync_location) (yum_temp_dir, rsync_location)) debug(output)
# todo: needs further implementation </s> return datatablesheader(	headers @property def headers(self): DataTablesColumn(self.selected_location_type), DataTablesColumn(
# todo: make sort function that sorts events by mechanism so that </s> if true_mechanisms:	extrinsic_events true_mechanisms = set([c.mechanism for c in true_causes]).\ intersection(c.mechanism for c in true_effects) true_causes = tuple(filter(lambda t: t.mechanism in true_mechanisms, true_causes))
# todo: down to empty </s> assert x.left == 4	test_popleft assert x.right == 4
# todo: avoid dummy and generate func here when inlining is possible </s> def _impl(df, axis=none, skipna=none, level=none, numeric_only=none,	_impl min_count=0): return hpat.hiframes.pd_dataframe_ext.sum_dummy(df)
# todo: assert </s> repo = self.remote.get_repo("testrepo0")	test_get_repo Test: Get a repo object
# todo(b/160786085): move this logic into overriding vars logic itself, </s> def _resolveckptpath(ckpt_rules):	__init__ self._uninitialized_vars = tf.report_uninitialized_variables( tf.global_variables()) return {GetSpecificCheckpoint(k): v for k, v in ckpt_rules.items()} self._restore_fns = []
#todo : make a real log mangment </s> print "stalking", self.output	manage_stalking need_stalk = False if need_stalk:
#todo: check login_required? </s> if( ( trans.user == none )	display hda_dict = {} try: and ( history_id == trans.security.encode_id( trans.history.id ) ) ): history = trans.history
# todo: revisit this when we have a users app </s> return '/tiki-user_information.php?locale=en-us&userid=%s' % user.id	profile_url @register.function def profile_url(user):
# todo(shardy): may be able to remove when the bug above is fixed </s> nova = client.client(username=con.username,	authenticate if con.password is not None: try: api_key=con.password, project_id=con.tenant,
# todo(rakhmerov): here we can define more informative messages </s> state_info = {	on_action_complete if self.is_with_items_completed(): state = self._get_final_state() states.SUCCESS: None, states.ERROR: 'One or more actions had failed.',
# todo: manage errors </s> self._uuid = params["uuid"]	project_created def project_created(params): log.info("Project {} created".format(self._uuid))
# todo: handle multiple skip stacks </s> child_skip_stack, = skip_stack.values()	first_letter_to_box character_found = False if skip_stack: if child_skip_stack: index, = child_skip_stack
# todo: next major version, remove cam id (unique_id is already in path) </s> filename = 'timelapse-{cam_id}-{cam}-{st}-img-{cn:05d}.jpg'.format(	camera_record start = datetime.datetime.fromtimestamp( settings.timelapse_start_time).strftime("%Y-%m-%d_%H-%M-%S") cam_id=settings.id, cam=settings.name,
assert oldsize == self.size, '%s: %d, %d' % (self.type, oldsize, self.size) # todo: remove </s> return self.size	Atom else: self.size = 8 + sum([atom.calsize() for atom in self.body])
# todo: replace with stream-changed </s> self._trigger_track_playback_started()	play self.core.tracklist.mark_playing(tl_track) self.core.history.add(tl_track.track) else: self.core.tracklist.mark_unplayable(tl_track)
# todo: merge scopes and claims </s> profile = self.generate_user_claims(request.user, scopes)	process_token def process_token(self, token, request): scopes = scope_to_list(self.request.scope) id_token = generate_id_token( token, profile,
raise notimplementederror #todo, implement! </s> def json(self):	json
common_path=prefix,  # todo: add key? </s> action="remote",	make_inline_attachments_decision elif ld.op == DiffOp.REMOVE: md = MergeDecision( conflict=True, local_diff=[ld],
# todo(developer): uncomment and set to a path to your audio file. </s> with io.open(speech_file, 'rb') as audio_file:	transcribe_file_with_metadata from google.cloud import speech_v1p1beta1 as speech client = speech.SpeechClient() content = audio_file.read() metadata = speech.types.RecognitionMetadata()
# todo: identify the specific structure we're finding and document this a bit better </s> pointer = context.object("unsigned long long", offset = (result - 16 - 8),	determine_valid_kernels seen = set() for result in results: layer_name = physical_layer_name) address = pointer & vlayer.address_mask
writetimeout=10000, # todo 1.3.8 rename to write_timeout for pyserial >= 3.x </s> parity=serial.parity_odd)	default baudrate, timeout=read_timeout, serial_obj.close() serial_obj.parity = serial.PARITY_NONE
# todo: determine proper template to use. </s> app_name + "." + recipe_format + ".recipe"] = "template tbd"	main if app_name + "." + recipe_format + ".recipe" not in existing_recipes: buildable_recipes[ print "\nExisting recipes: %s" % existing_recipes print "\nAvailable recipe formats: %s" % avail_recipe_formats
# todo: fix these repos finally! </s> repo = annexrepo(path, create=false, init=true)	test_AnnexRepo_add_to_git @with_testrepos('.*annex.*', flavors=['clone']) def test_AnnexRepo_add_to_git(path): if repo.is_direct_mode(): ok_clean_git_annex_proxy(path)
# todo: check that the performance measure is within some range </s> figure_eight_baseline(num_runs=1, flow_params=figureeight2,	test_figure_eight figure_eight_baseline(num_runs=1, flow_params=figureeight1, render=False) render=False)
# todo: ensure that if multiple flags are provided, the *last* one overrides </s> if arg.p:	Pwd arg, i = PWD_SPEC.Parse(argv) pwd = mem.GetVar('PWD').s pwd = os.path.realpath(pwd) print(pwd)
# todo there's probably a better way besides np.where, something from </s> for name, tag in zip(names, tags):	int_data_to_sets if len(names) != len(tags): names = [f"set{tag}" for tag in tags] self.point_sets[name] = np.where(data == tag)[0] for key in keys:
# todo(mattjj): not passing forece_broadcast recursively... intentional? </s> return _moveaxis(sz, dst, src, get_aval(x), x, force_broadcast)	moveaxis def moveaxis(sz, dst, src, x, force_broadcast=True):
# todo(justinsb): mock doesn't yet do this... </s> self.assertequal('active', found_server['status'])	test_deferred_delete_restore created_server_id = created_server['id'] found_server = self._wait_for_state_change(created_server, 'BUILD') self.api.delete_server(created_server_id) found_server = self._wait_for_state_change(found_server, 'ACTIVE')
# todo: fix this! </s> pacman_options["needed"] = true	install_packages self.queue_event('local_percent', 0) pacman_options = {} for package_type in downloaded_ok: logging.debug(_("Installing packages from '%s' group...") % package_type)
# todo: should probably replace with input handler to remain consistent </s> if view:	to_raster apply_mask : bool If True, write the "masked" view of the dataset. data = self.view(data_name, apply_mask=apply_mask, nodata=nodata, interpolation=interpolation, as_crs=as_crs, kx=kx, ky=ky, s=s,
# todo: make sure pub is always correct </s> fname = 'pub/%s/%s/%s.%s.%s' % (	copy_data_from fi = default_storage.open(s.value[7:], 'rb') nonce = get_random_string(length=8) self.organizer.slug, self.slug, s.key, nonce, s.value.split('.')[-1] )
# todo(b/130724878): these conversions should not be needed. </s> obj_1 = obj.from_anon_tuple(server_state, 1)	test_load_latest_state models.model_fn, server_optimizer_fn=server_optimizer_fn) server_state = iterative_process.initialize() export_dir = os.path.join(self.get_temp_dir(), 'ckpt_1') checkpoint_utils.save(obj_1, export_dir)
# todo: test me @jmcarp </s> admins = [	manage_contributors if user not in users ] user for user in users if self.has_permission(user, 'admin')
recording_software_name = none  # todo </s> recording_software_version = none  # todo	recording_update_pupil_invisible_to_pprf_2_0 start_time_synced_ns = None  # TODO duration_ns = None  # TODO recording_name = None  # TODO system_info = None #TODO
# todo: verify </s> manager.bind(bind)	test_unbind manager = factory.consumer_agent_manager()
# todo(guillermooo): we cannot access the ouput panel used by exec. </s> self.window.run_command('exec', {	execute def execute(self, shell_cmd, working_dir): 'shell_cmd': shell_cmd, 'working_dir': working_dir
# todo: remove when 36lts is discontinued </s> with open(recorded_args, 'r') as args_file:	retrieve_args if recorded_args is None: return None return pickle.load(args_file)
# todo: use color instead of [ ] </s> if case(source_e.interactive):	_PrintWithSpanId UP_src = src with tagswitch(src) as case: src = cast(source__Interactive, UP_src) source_str = '[ interactive ]'  # This might need some changes
# todo: rip that out </s> self.text = self._starlette.body	Request async def content(self): return await self._starlette.body() @property async def text(self):
#todo force redraw rather than queue? (like before) </s> self.p_da_cur.queue_draw()	page_preview pr = page_cur.get_aspect_ratio(self.notes_mode) self.p_frame_cur.set_property("ratio", pr) if page_next is not None: pr = page_next.get_aspect_ratio(self.notes_mode)
# todo: clean up </s> chunk_path = os.path.join(render_path,  "render_chunk_")	gen_render_process_args def gen_render_process_args(args, start_frame, end_frame, render_path): Generates params = [ 'blender', '-b', args.blendfile, '-s',
# todo: make sure the image is present or pull it </s> base_image = "registry.fedoraproject.org/fedora:28"	test_build_basic_image def test_build_basic_image(): basic_playbook_path = os.path.join(data_dir, "basic_playbook.yaml") target_image = "registry.example.com/ab-test-" + random_word(12) + ":oldest" cmd = ["build", basic_playbook_path, base_image, target_image]
# todo: more advanced widget for this </s> if not (self.ui and hasattr(self.ui, 'notebook')):	_set_page_completion def _set_page_completion(self, entry): logger.warn('Could not set page completion, no ui object') return
# todo remove sorted? completions should be sorted? </s> comp_str = str(sorted(set([str(c) for c in completions])))	run_completion_test return 1 else: if comp_str != correct: print('Solution @%s not right, received %s, wanted %s'\
# todo: replace with stream-changed </s> self._trigger_track_playback_started()	previous self.core.tracklist.mark_playing(prev_tl_track) self.core.history.add(prev_tl_track.track) elif not result: self.core.tracklist.mark_unplayable(prev_tl_track)
# todo: this completion may not be good, since it resets to 0 later. </s> history.append([note_hot, beat_input, completion_input, style])	main beat_input = compute_beat(i, NOTES_PER_BAR) completion_input = np.array([i / (len(inspiration) - 1)]) composition = [] N = NOTES_PER_BAR * BARS
time_behind = time.time() - cblock.ntime   # todo: block times are not very reliable. </s> if time_behind > 60 * 60 * 2:   # two hours.	check_backend_state block_hash = backend.getblockhash(block_count) cblock = backend.getblock(block_hash) raise BackendError('Bitcoind is running about {} hours behind.'.format(round(time_behind / 3600))) logger.debug('Backend state check passed.')
# todo: verify/modify these lists </s> version_changing_attrs = ["xpath", "version", "xmlns"]	testBasicAttributes def testBasicAttributes(self): filled = self._get_basic_formdef() nonversion_changing_attrs = [ "name", "short_name", "uiversion"] for attr in version_changing_attrs:
# todo: this actually looks like a bug, but this code </s> d.addcallback(lambda s: s.unsubscribe(receiver))	buildStarted s.subscribe(receiver) d = s.waitUntilFinished() except Exception: log.msg(
# todo: handle ta seeds </s> status, cost, runtime, additional_info = executor.run(	run rand_inst_id = random.randint(0,len(self.scenario.train_insts)) rand_inst = self.scenario.train_insts[rand_inst_id][0] default_conf, instance=rand_inst, cutoff=self.scenario.cutoff) self.runhistory.add(config=default_conf, cost=1, time=1,
device.tags[q] = self.data.tags[ids] #todo </s> device.index[:l, q] = numpy.ones((l,), dtype = 'int8')	allocate_devices if self.data.ctc_targets is not None: device.ctc_targets[q] = self.data.ctc_targets[ids] offset += batch.shape[1] else:
# todo deprecated, remove in 1.4.0 </s> "file": entry.path})	_analyze "origin": entry.location, "type": entry.type, try: result = self._do_analysis(high_priority=high_priority)
# todo add options to maodify the sorted by key and the header options </s> matrix = sorted(matrix, key=lambda endpoint: endpoint[2])	do_show endpoint.p_next_state]) if len(matrix) > 0: matrix.insert(0, ['Name', 'State', 'MAC Address', 'Segment', 'Port', 'VLAN', 'IPv4', 'IPv6', 'Next State'])
# todo: get this from ourselves. </s> version      = key[1],	ConstraintCollectionBase self.variable_traces[ key ] = VariableMergeTrace( variable     = yes_variable, trace_yes    = self.variable_traces[ yes_variable, yes_version ], trace_no     = self.variable_traces[ no_variable, no_version ]
# todo: arrange </s> repo = self.remote.new_repo(self.token)	test_create_repo def test_create_repo(self): Test: create/edit a repo object self.assertTrue(self.remote.modify_repo(repo, "name", "testrepo0", self.token)) self.assertTrue(self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token))
logg.error("too long") # todo </s> self.end(200)	test_5034_notify_service_functions_system self.rm_testdir() self.coverage()
# todo(ib-steffen): allow custom ca bundles </s> r = requests.post(url, files=files, headers=headers, timeout=120, verify=false)	post headers = {'Authorization': 'bearer ' + token} logger.info('Also uploading to %s', url) if r.status_code != 200: abort(500, "Failed to upload data")
# todo implement </s> volume_name = ("aaaabbbb"+"\x00").encode("utf-16le")	hook_GetVolumeInformationW pt_volume_name = params["lpVolumeNameBuffer"] if pt_volume_name != 0: ql.uc.mem_write(pt_volume_name, volume_name) pt_serial_number = params["lpVolumeSerialNumber"]
#todo: geli detach -l </s> if to_swap:	zfs_replace_disk self.__system('/sbin/geli detach %s' % (devname, )) raise MiddlewareError('Disk replacement failed: "%s"' % error) self.__system('/sbin/geli onetime /dev/%s' % (to_swap)) self.__system('/sbin/swapon /dev/%s.eli' % (to_swap))
# todo: remove update flicker. for win32console we could set the cursor </s> screen_buffer.setconsolecursorposition(top_left_coordinate)	_ClearScreen screen_buffer_attributes, console_size, top_left_coordinate)
# todo: move this function somewhere else </s> def get_quality_check_failures(path_obj, dir_stats):	get_quality_check_failures checks = {} try:
# todo: handle escape (0x1b) </s> for d in data:	decode_out def decode_out(self, data): d = ord(d) if not self.out_parsing and d != Decoder.RESPONSE_MAGIC:
# todo: the logic here for ion concentration setting is in two </s> if len(ion_elts) == 1:	PourbaixDiagram for entry in ion_entries: ion_elts = list(set(entry.composition.elements) - elements_HO) entry.concentration = conc_dict[ion_elts[0].symbol] \ * entry.normalization_factor
# todo(tr3buchet) - remove comment in multi-nic </s> ips = db.fixed_ip_get_all_by_instance(admin_context, instance['id'])	spawn VMHelper.create_vbd(self._session, vm_ref, vdi_ref, 0, True) admin_context = context.get_admin_context() for network in db.network_get_all_by_instance(admin_context, instance['id']):
# todo(nakago): check why tolerance is high </s> def test_backward_cpu(model, data):	test_backward_cpu atom_data, adj_data, y_grad = data gradient_check.check_backward(model, (atom_data, adj_data), y_grad,
# todo: cleanup </s> def test_find_repo(self, createrepo):	TestRepo Test: Get a repo object repo = self.remote.get_repo("testrepo0") Test: find a repo object result = self.remote.find_repo({"name": "testrepo0"}, self.token)
except httperror as e:  # @todo ask for server instead </s> print("cannot get")	updateCountryCodes try: response = requests.get(url, headers=headers) try: bsObj = BeautifulSoup(response.text, "html.parser")
# todo: raise proper error </s> raise assertionerror("ifs shouldn't be an array")	_GetSplitter ifs = val.s else: try: sp = self.splitters[ifs]
# todo: replace with is_finite() instead of checking cardinality? </s> if all(m.monoid_generators().cardinality() != float('inf') for m in f):	monoid_generators return self._cartesian_product_of_elements(cur) from sage.sets.family import Family ret = [lift(i, gen) for i,M in enumerate(F) for gen in M.monoid_generators()] return Family(ret)
# @todo: return only packages for the current architecture </s> package_names = list(package_names)[:]	find_aur_packages def find_aur_packages(package_names): json_results = [] for package_name in package_names[:]:
# todo: write validate method </s> return true	validate def validate(self):
# todo: determine correct computation for panning. http://en.wikipedia.org/wiki/pan_law seems relevant but was short on actual formulas. may depend on headphones vs speakers? this may be correct already for headphones -- it sounds nearly-flat to me. </s> self.__audio_gain_blocks[0].set_k(gain_lin * (1 - pan))	__update_audio_gain if self.__audio_channels == 2: pan = self.audio_pan self.__audio_gain_blocks[1].set_k(gain_lin * (1 + pan)) else:
# todo: ensure template has resources </s> from zim.fs import dir	testMultiFile def testMultiFile(self): folder = self.setUpFolder('multi', mock=tests.MOCK_ALWAYS_REAL) exporter = build_notebook_exporter(Dir(folder.path), 'html', 'Default.html')
# todo: make options for "text included" vs. "text matches" </s> if text in t.text:	search_textbound if restrict_types != [] and t.type not in restrict_types: continue matches.add_match(ann_obj, t) return matches
# todo: fetch spoolup option </s> defaultspoolvalue = 1	exportEfs resonance = {"hull": hullResonance, "armor": armorResonance, "shield": shieldResonance} shipSize = EfsPort.getShipSize(fit.ship.item.groupID) spoolOptions = SpoolOptions(SpoolType.SCALE, defaultSpoolValue, False) try:
# todo(mriedem): call select_destinations() with a </s> source_node, dest_node = self._check_requested_destination()	_execute
# todo: check return value of attachthreadinput properly </s> if isinstance(keys, six.text_type):	TypeKeys window_thread_id = win32functions.GetWindowThreadProcessId(self, 0) win32functions.AttachThreadInput(win32functions.GetCurrentThreadId(), window_thread_id, win32defines.TRUE) aligned_keys = keys.encode(locale.getpreferredencoding(), 'ignore') elif isinstance(keys, six.binary_type):
pass # todo </s> def try_undo(self, *args):	try_undo
# todo: process </s> result = content.uninstall(units, options)	uninstall_units content = Agent.Content()
# todo(ihrachys): replace with port.create() once we get an object </s> self._port = db_api.create_object(self.context, models_v2.port,	_create_test_port def _create_test_port(self, network): {'name': 'test-port1', 'network_id': network['id'],
# despite copystat mtime is not copied. todo </s> return stats	stats else:
log_importance_weight = none  # todo: check the reason/behavior for this </s> variable = variable(distribution=distribution, value=value, address_base=address_base, address=address, instance=instance, log_prob=log_prob, log_importance_weight=log_importance_weight, observed=observed, name=name)	observe log_importance_weight = float(log_prob) else: _current_trace.add(variable)
# todo(jeremydw): read manifest and takedown old content here. </s> pod.dump(out_dir=self.out_dir)	deploy def deploy(self, pod):
# todo: write this method if possible </s> pass	address_transactions def address_transactions(self, addresslist):
raise notimplementederror #todo </s> elif q.kw(i,"format"):	parse while i < l: if q.kw(i,"RETURN"): raise NotImplementedError #TODO elif q.kw(i,"REQUEST"):
# todo: make an ascii-art bar </s> ctx.fillslots("progress_hash", "%.1f%%" % (100.0 * chk))	render_row_upload self._render_common(ctx, data) (chk, ciphertext, encandpush) = data.get_progress() ctx.fillSlots("progress_ciphertext", "%.1f%%" % (100.0 * ciphertext)) ctx.fillSlots("progress_encode", "%.1f%%" % (100.0 * encandpush))
pass  # todo </s> self._current_command = []	__init__ self._device = None
# todo: add strict mode and expose warnings. </s> pass	MakeReplacer regex, warnings = glob_.GlobToERE(pat) if warnings: if regex is None: return _ConstStringReplacer(pat, replace_str)
# todo add locales </s> raise yunohosterror("domain_name_unknown", domain=domain)	domain_set_settings domains = _load_domain_settings() if not domain in domains.keys(): setting_set = False if ttl is not None:
raise exceptions.mpdnotimplemented  # todo </s> underscore, dash, dot and colon.	subscribe already. The name may consist of alphanumeric ASCII characters plus
# todo: test me @jmcarp </s> def get_download_count(nodes):	get_download_count from website.addons.osffiles.model import NodeFile count = 0
# @todo: extend entity_types within the template </s> need_response = t("activity group"),	DocumentLibrary inv_send = T("Sent Shipment"), inv_warehouse = T("Warehouse"), police_station = T("Police Station"), pr_group = T("Team"),
# bezel correction todo don't show if span mode is multi image or simple span. </s> self.sizer_setting_paths = wx.staticboxsizer(wx.vertical, self, "wallpaper paths")	WallpaperSettingsPanel self.create_sizer_profiles() self.create_sizer_settins_left() self.sizer_settings_right.Add(self.sizer_setting_paths, 0, wx.CENTER|wx.EXPAND)
# todo: sinpi </s> return pi / (math.sin(pi*x)*_gamma_real(1-x))	_gamma_real return _exact_gamma[_intx] if x < 0.5: else: x -= 1.0
# todo: dynamically define all endpoints </s> routes.extend([	_create_routes list((l[0], l[1]) for l in routes) ) (r"/mopidy/ws/?", handlers.WebSocketHandler, {'actor': self}), (r"/mopidy/rpc", handlers.JsonRpcHandler, {'actor': self}),
#todo discont: actually use offsets instead of (start, end)! </s> messager.warning('_edit_span(): using (start, end)')	_edit_span def _edit_span(ann_obj, mods, id, offsets, projectconf, attributes, type, undo_resp={}): start, end = offsets[0] ann = ann_obj.get_ann_by_id(id)
# todo: do we change this to something like "threshold" </s> m, n = request_data['m'], request_data['n']	create_policy bob_pubkey = bytes.fromhex(request_data['bob_encrypting_key']) label = b64decode(request_data['label']) federated_only = True  # const for now bob = Bob.from_public_keys({DecryptingPower: bob_pubkey,
# todo(piyush): current api-site doesn't contain this api description. </s> uri = '/agents/%s/l3-routers' % agent_id	add_router_to_l3_agent def add_router_to_l3_agent(self, agent_id, **kwargs): return self.create_resource(uri, kwargs)
return  # todo: return a 200, with whatever policy metadata. </s> raise	set_policy except IntegrityError:
# todo: action value doesn't exist for beta </s> self.unittest(	test_early_horizon_estimate baseline_objective = 'policy_gradient' baseline_optimizer = 'adam' exclude_bounded_action=True, reward_estimation=reward_estimation, baseline_objective=baseline_objective, baseline_optimizer=baseline_optimizer
#todo: python2 specific, remove </s> escaped_fn = filename	main ).decode() except UnicodeDecodeError: file_start = timeit.default_timer() try:
#todo: overly broad exception needs fixing </s> except exception:	get_counts count_index, count_type = index, _type count = elastic.count(count_query, index=count_index, doc_type=count_type)['count'] count = 0 counts[ALIASES.get(_type, _type)] = count
# todo(b/161332815): make jax actor work with batched or unbatched inputs. </s> observation = utils.add_batch_dim(observation)	FeedForwardActor def select_action(self, observation: types.NestedArray) -> types.NestedArray: key = next(self._rng) action = self._policy(self._client.params, key, observation) return utils.to_numpy_squeeze(action)
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pcmpeqb res = 0x000000ff0000ff00ff00000000ffff00 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo: arrange </s> repo = self.remote.new_repo(self.token)	test_create_repo def test_create_repo(self): Test: create/edit a repo object self.assertTrue(self.remote.modify_repo(repo, "name", "testrepo0", self.token)) self.assertTrue(self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token))
#todo this is not valid in multiprocessing! </s> yield pulsar.not_done	testSignal self.assertTrue(arbiter.signal_queue.qsize() >= 1)
pass # todo </s> def handle_request(self, input):	handle_request
# todo find out if this is good because of sparcity... </s> 'prefers_data_normalized': false,	get_meta_information 'handles_numerical_features': True, 'prefers_data_scaled': False, 'handles_multiclass': True, 'handles_multilabel': True,
# todo: change the frontend to pass seconds instead. </s> expires_at = (now_in_seconds + one_hour_in_seconds) * 1000	test_auth_login_and_logout assert "sessionid" not in client.cookies client_id = "mozilla-ldap/user@foo.com" resp = client.get( reverse("auth-login"),
# todo: remove after pylint 1.4+ </s> ext = tldextract.extract(url)._asdict()	Extract if not url: return web.webapi.badrequest() web.header('Content-Type', 'application/json') return json.dumps(ext) + '\n'
# todo(ytknzw): add more specific assertion with the test case. </s> study = create_study()	test_plot_intermediate_values trial.report(2.0, step=1) return 0.0 study.optimize(lambda t: objective(t, True), n_trials=1) figure = plot_intermediate_values(study)
# fixme: todo </s> return results	execute_query result.append(value) results.append(ResultRow(*result))
#todo: assuming constant mu </s> self._mfmui = self.mesh.getfaceinnerproduct(1/mu_0)	makeMassMatrices self._MeSigma = self.mesh.getEdgeInnerProduct(m) self._MeSigmaI = sdiag(1/self.MeSigma.diagonal())
# todo test cases </s> return self._promise	execute self._thread.start()
# todo(pkilambi): process the output as needed </s> return out	service_get try: out = utils.execute('kubectl', 'get', 'service', uuid) except Exception as e: LOG.error("Couldn't get service  %s due to error %s" % (uuid, e))
# todo(developer): uncomment these lines and replace with your values. </s> queue_path = client.queue_path(project, location, queue)	purge_queue def purge_queue(project, location, queue): client = tasks.CloudTasksClient() response = client.purge_queue(queue_path) return response
#todo: actually check for change </s> self._smudge('__setitem__slice',k,v)	DirtyList def __setitem__ (self, k, v): if isinstance(k, slice): elif self[k] != v: self._smudge('__setitem__',k,v)
# todo: password is a required field for a galaxy user record. however, it should not be required </s> kwargs['password'] = ''.join(	_new_instance @classmethod def _new_instance(cls, model, *args, **kwargs): random.SystemRandom().choice(string.ascii_letters + string.digits) for _ in range(16)) return cls._save_instance(model(*args, **kwargs))
# todo: remove snap when updating this image </s> plt.pcolormesh(xbnds, ybnds, data, transform=rp, cmap='spectral',	test_pcolormesh_single_column_wrap plt.figure(figsize=(10, 6)) ax = plt.subplot(111, projection=ccrs.PlateCarree(180)) snap=False) ax.coastlines()
# todo: decorate with abstractmethod after torchhook is extended </s> pass	__init__ def __init__(self):
# todo: extend it for estimator from other frameworks - mllib, h20, vw </s> coef_list = list(np.squeeze(estimator.coef_[class_label_index]))	understand_estimator if ('coef_' in estimator.__dict__) is False: raise KeyError('the estimator does not support coef, try using LIME for local interpretation') feature_coef_df = pd.DataFrame(np.column_stack([feature_names, coef_list]), columns=['features', 'coef_wts'])
# todo : </s> self.draw()	setdata self.compute_peaks(y)
# todo: must be implemented </s> pass	range_from_chapters def range_from_chapters(crawler, times=0):
# todo setup mahout, must be checked out from repo atm: </s> pass	_setup_hadoop http://archive.cloudera.com/docs/ec2.html http://archive.cloudera.com/cdh/3/
# todo: add proper checks (e.g. check if input stuff is pandas full of objects) </s> return check_array(x, dtype=none, ensure_2d=false)	check_ts_array def check_ts_array(X): use preexisting ones with bypass (temporarily)
# todo: instead of raising, we should do something </s> if reencryption_signature.verify(bytes(cfrag), ursula_verifying_key):	complete if not metadata_as_signature.verify(metadata_input, ursula_verifying_key): raise InvalidSignature(f"Invalid metadata for {cfrag}.") good_cfrags.append(cfrag) else:
# todo: change the frontend to pass seconds instead. </s> expires_at = (now_in_seconds + one_hour_in_seconds) * 1000	test_login_email_user_doesnt_exist return {'sub': 'email', 'email': test_user.email, 'exp': id_token_expiration_timestamp} monkeypatch.setattr(AuthBackend, '_get_user_info', userinfo_mock) resp = client.get( reverse("auth-login"),
# todo placeholder; implement </s> pass	init_app backend_options: torch_rpc.RpcBackendOptions, ):
# todo remove input dropout, just here for testing </s> self.input_dropout = torch.nn.dropout(0.2)	ConvEScorer self.stride = config.get("conve.stride") self.padding = config.get("conve.padding") self.bn0 = torch.nn.BatchNorm2d(1, affine=False) self.feature_map_dropout = torch.nn.Dropout2d(config.get("conve.feature_map_dropout"))
# todo: ... </s> pass	pull_child_entities This fulfills the third requirement of `DHIS2 Integration`_. .. _DHIS2 Integration: https://www.dropbox.com/s/8djk1vh797t6cmt/WV Sri Lanka Detailed Requirements.docx
raise  # todo </s> else:	deploy_contract cached_contract = self.__contract_cache[contract_name] except KeyError: self.__registrar.enroll(contract_name=contract_name, contract_address=address,
#todo: add metadata support when it is merged from develop </s> result["jid"] = data[0]	_build_dict Rebuild dict result = {} result["tgt_type"] = data[1] result["cmd"] = data[2]
timeout = 0.1  # todo: receive as a parameter </s> start_time = time.time()	handle database_uri = os.environ['DATABASE_URL'] encoding = 'utf-8'  # TODO: receive as a parameter try: rows_imported = pgimport(
# todo: move this file counting into the `productgraph`. </s> return sum(t.sources_count() for t in self.targets())	target_file_count def target_file_count(self):
# todo: make sure reply_email is unique </s> reply_email = f"reply+{random_words()}@{email_domain}"	MailHandler website_email, ) forward_email = ForwardEmail.create( gen_email_id=gen_email.id,
# todo: logging, or warning </s> print (	_save pass if str(e) == '' and str(e2) == '': 'neither cPickle nor pickle could write to %s' % filepath )
# todo: convert [n, c, v] to  new convention [v, n, c] </s> return mseloss	get_module def get_module(self):
# todo xxx graalvm change </s> raise unittest.skiptest("missing threading support")	ThreadedEchoServer self.daemon = True def __enter__(self): self.start(threading.Event()) self.flag.wait()
# todo still need to remove the file </s> try:	render_POST print('Debugging: shutil.copy({0}, {1}'.format(uploaded_file.name, newpath)) shutil.copy(uploaded_file.name, newpath) os.remove(uploaded_file.name) except WindowsError as e:
# todo(rbg): remove duplicated code </s> self._perm = np.random.permutation(np.arange(len(self._roidb)))	_shuffle_roidb_inds def _shuffle_roidb_inds(self): self._cur = 0
# todo: remove when pre-csrf token templatetags are no longer supported </s> if not is_old_django:	get_render 'toggle_fields': final_toggle_fields } if use_csrf_protection and context.has_key('csrf_token'): response_dict['csrf_token'] = context['csrf_token']
# todo: remove in 21.08 </s> if cache_audio_dir is not none:	generate_cache_text cache_audio_dir (path): DEPRECATED path to store .wav files cache_text_file (file): file containing the sentences LOG.warning( "the cache_audio_dir argument is deprecated. ensure the directory "
assert study_id == 0  # todo(akiba) </s> self.trials[trial_id].intermediate_values[step] = intermediate_value	set_trial_intermediate_value def set_trial_intermediate_value(self, study_id, trial_id, step, intermediate_value):
# todo: this check was too simple, and broke a few things: </s> if (self.source is not none) and (self.include is not none):	sanity_check def sanity_check(self): return raise CoverageException("--include and --source are mutually exclusive")
# todo it would be good to support different kind of shells </s> if not _get_user_for_ssh(auth, username) :	user_disallow_ssh Keyword argument: username -- User username raise MoulinetteError(errno.EINVAL, m18n.n('user_unknown', user=username)) auth.update('uid=%s,ou=users' % username, {'loginShell': '/bin/false'})
raise notimplementederror # todo </s> def em_step(self):	EM_step
# todo generator </s> if len(content) >= 1 and content[0] == curdir:	_get content = content_by_ds[ds_path] results = [] results.append(cur_ds) if not get_data:
# todo find out if this is good because of sparcity... </s> 'prefers_data_normalized': false,	get_meta_information 'handles_numerical_features': True, 'prefers_data_scaled': False, 'handles_multiclass': True, 'handles_multilabel': True,
# todo: fixed by using realpath, but there should be a cleaner </s> assert_equal(	_test_AnnexDB {opj(realpath(path), p) for p in ['file1.txt', '2git']}) db2.get(curdir + sep + 'file1.txt') set(db2.get_obsolete()), {opj(realpath(path), p) for p in ['2git']})
# '-rs',  # @todo: manually remove dependencies of conflicting packages, </s> '-r',	_remove_packages 'sudo', 'pacman', '--noconfirm', ] + packages_to_be_removed,
# todo : error on unmatched alias </s> log.exception('validation failed for action alias data=%s.', action_alias)	match return match except (ActionAliasAmbiguityException) as e: pecan.abort(http_client.BAD_REQUEST, str(e)) return
self.mechanism_bin = serialize(value, to_bytes=true)  # todo: techdebt fix </s> def obj(self, value: any) -> none:	obj @obj.setter
# todo(okuta): check type </s> return core.moveaxis(a, source, destination)	moveaxis Array with moved axes. This array is a view of the input array. .. seealso:: :func:`numpy.moveaxis`
# todo enable ssh on the nodes by changing the image </s> d = self.client.add(node_1_name, image)	setUp node_2_name = random_name() image = u"openshift/busybox-http-app" d.addCallback(lambda _: self.client.add(node_2_name, image)) d.addCallback(lambda _: self.client.list())
# todo: change this to use a getter </s> if self.options.verbose > 2:	check_yaml die(self.invalid_yaml_msg_single_quotes) else: yaml.load(content) die(self.invalid_yaml_msg)
#todo: add_land arg </s> if(len(args) < 1):	cmd_rally def cmd_rally(args): print("Usage: rally <list|load|save|add|clear>") return
raise notimplementederror # todo </s> def register_broadcasted_data(data):	register_broadcasted_data
prob_dist = results[0]#[-1] # todo: used for old model architecture </s> note = np.random.choice(len(prob_dist), p=prob_dist)	main for i in range(N): results = model.predict([np.array([x]) for x in zip(*history)]) note_hot = one_hot(note, NUM_CLASSES) beat_input = compute_beat(i, NOTES_PER_BAR)
# todo: delete me </s> try:	resume except exceptions.NotFound: raise NoSuchJobStoreException(self.locator) self.files = self.uri.get_bucket(headers=self.headerValues, validate=True) except GSResponseError:
raise notimplementederror # todo </s> list every item in entry_point that match request	search def search(self, entry_point, request):
# todo? don't consider the empty set here </s> return all_coarse_grains_for_blackbox(blackbox)	coarse_grains return all_coarse_grains(system)
# todo: add a check for similarly valid value back in time. maybe if it the </s> future_timestamp = (	_VerifyRecord Returns: bool: True if this is a valid PLS Recall record, False otherwise. timelib.Timestamp.GetNow() + self._SIX_YEARS_IN_MICRO_SECONDS) if pls_record.last_written_time > future_timestamp:
# todo(jakevdp): remove when minimum jaxlib is has extension version 4 </s> if self._thread_local_state.enable_x64 is none:	x64_enabled return lib.jax_jit.get_enable_x64() else: self._thread_local_state.enable_x64 = bool(self.read('jax_enable_x64')) return self._thread_local_state.enable_x64
# todo: add a better throttling mechanism </s> if 'sleep' in kwargs:	get_posts _scraper.requests_kwargs['timeout'] = kwargs.pop('timeout', DEFAULT_REQUESTS_TIMEOUT) options = kwargs.setdefault('options', set()) warnings.warn( "The sleep parameter has been removed, it won't have any effect.", stacklevel=2
# todo move to common? </s> def dir_hash(path: path):	dir_hash mtimes = tuple(p.stat().st_mtime for p in sorted(path.glob('*.json'))) return mtimes
# todo(dcramer): this would make more sense as part of the xunit handler </s> teststat = itemstat(	sync_job if not is_finished: raise sync_job.NotFinished item_id=job.id, name='test_count',
# todo_recorders - need to pass in parent info instead of none </s> metadata = create_local_meta(none, self.pathname)	record_iteration Record an iteration of the current System. self.iter_count += 1 update_local_meta(metadata, (self.iter_count,)) self._rec_mgr.record_iteration(self, metadata, method=inspect.stack()[1][3])
# todo: fill these in </s> command = none	ShellFuncAction state.SetGlobalString(self.ex.mem, 'COMP_CWORD', str(index)) self.log('Running completion function %r', self.func.name) prev = ''  # TODO: Fill in argv = [command, to_complete, prev]
except (asyncio.cancellederror, asyncio.timeouterror) as err:  # todo: is this needed? </s> log.error("%s downloading blob from %s:%i", str(err), self.peer_address, self.peer_port)	data_received if self._response_fut and not self._response_fut.done(): self._response_fut.set_exception(err) if self._response_fut and not self._response_fut.done(): self._response_fut.set_exception(err)
# todo: refactor me, please! </s> self.__image_steps = [self.__input_path] + [	_setup ] else: self.__input_path for p in self.__phases[0:(self.__starting_step - 1)]
# todo: this needs serious refactoring </s> await self._did_init(seed, taa_text, taa_version)	LoadClient await self._init_pool(genesis_path) await self._wallet_init(w_key) await self._post_init() self._logger.info("call _req_generator.on_pool_create")
# todo for pytorch 2.0.4, need to set dim=1 for log_softmax or use softmax then take log </s> loss = self.logsoftmax(output.view(batch, self.num_labels * max_len, max_len))	BiRecurrentConvBiAffine log_mask = torch.log(mask) output = output + log_mask.view(batch, 1, max_len, 1) + log_mask.view(batch, 1, 1, max_len) loss = loss.view(batch, self.num_labels, max_len, max_len) if mask is not None:
# todo: i bet this interferes with views whose column names can </s> return object.__setattr__(self, attr, val)	ThingBase raise ValueError('cannot change _id on a committed %r' % (self.__class__)) if isinstance(attr, basestring) and attr.startswith('_'): try: self._deletes.remove(attr)
# todo: use tor proxy session </s> def apigethosts(shopurl):	apiGetHosts print("# apiGetHosts") hosts=[]
# todo document after plugin data is refactored </s> def execute_python_plugin(plugin, command):	execute_python_plugin python_plugin_name = checkdicts(plugin, 'file') python_plugin_path = os.path.abspath("plugins/{0}/{1}".format(
# todo: migrate to sql </s> raise commanderror("this doesn't work since the synclogs are now migrated to sql")	handle def handle(self, filename, *args, **kwargs): database = SyncLog.get_db() all_sync_log_ids = [
# todo: remove when transition to python3 complete </s> return other / self	__rtruediv__ implemented only for SISO systems.")
self.__top.start()  # todo overriding internals </s> def close(self):	DemodulatorTester self.__top = Top(devices={'s1': SimulatedDevice()}) self.__top.add_receiver(mode, key='a') if self.__top is not None: self.__top.stop()
# todo: check if this logic is sufficient </s> for cindex in set(bottom.kinfo.coefficient_map	compile_expression ) index = top.indices + top.kinfo.coefficient_map): c = exp.coefficients()[cindex]
# todo: investigate django how this can be avoided </s> return super(polymorphicmodelbase, self).__getattribute__(name)	__getattribute__ return self.base_objects
# todo: determine language based on preprocessing information. </s> preferred_language = self._preferred_language or 'en-us'	ParseOptions helpers_manager.ArgumentHelperManager.ParseOptions( options, self, names=['data_location']) output_mediator = self._CreateOutputMediator(preferred_language) self._ReadMessageFormatters(output_mediator)
# todo: if not standalone, call ipc directly rather than </s> import zim.ipc	spawn def spawn(self, *args): if not zim.ipc.in_child_process(): args = args + ('--standalone',)
# todo: allow units to be added/removed </s> elif units < ps.units:	execute_signal if units == ps.units: self.close_position(currency_pair) return elif units > ps.units:
# todo(tsileo): handle tombstone </s> if not is_api_request():	outbox_activity_shares @app.route('/outbox/<item_id>/shares') def outbox_activity_shares(item_id): abort(404) data = DB.outbox.find_one({'id': item_id, 'meta.deleted': False})
# todo put this in a .extra w/a subselect </s> entries = self.post_launch_entries.filter(activity__billable=true)	post_launch_hours_worked def post_launch_hours_worked(self): if not hasattr(self, '_worked_post_launch'): self._worked_post_launch = entries.aggregate(s=Sum('hours'))['s'] or 0 return self._worked_post_launch or 0
# todo: implement this. </s> pass	test_get_key def test_get_key(self):
# [[shape], kernel, stride] #todo: add padding </s> param_list = [[[1, 3, 10, 10], 2, 2],	test_avgpool def test_avgpool(): workspace.ResetWorkspace() [[2, 3, 5, 5], 1, 1], [[2, 2, 7, 7], 3, 2],
# todo: validate </s> self.send_line("variant {}", variant)	_variant def _variant(self, variant):
" # todo: i18n", </s> '-print("hello world!")',	test_process_hunks_patch_called_correctly " ", " ", '+print(tr("Hello world!"))', " ",
# todo: should be a method on shape. </s> self.texcoords.extend([tx, ty])	add_vertex self.norms.extend([nx, ny, nz])
# todo: check syntax </s> return values	allow @GenericHeaderSyntax def allow(self, name, values):
# todo: return errors in a universal way </s> print("shivyc: error: cannot open output file '{}'"	main s_file = open("out.s", "w") except IOError: .format("out.s")) return
# todo(john sirois): clean this up when build parse refactoring is tackled. </s> unused_resolved_java_sources = self.java_sources	resolve def resolve(self): for resolved in super(ScalaLibrary, self).resolve(): yield resolved
# todo: verify behavior </s> self.assert_received(self.debugger, [])	test_active_exception ])
if python_version < 340 or true: # todo: temporarily reverted: </s> node.getvariableforreference(	onLeaveNode def onLeaveNode(self, node): if node.isExpressionFunctionBody() and node.isClassClosureTaker(): variable_name = "__class__" )
# todo put an index.html in front of this bucket </s> if setuptools_version != '3.6':	upload_python_packages :param bytes version: Version to upload packages as. :param FilePath top_level: The top-level of the flocker repository. raise ValueError("setuptools version is not 3.6") check_call([
pass  # todo: raise error to force subclasses to implement it </s> def _deactivate(self):	_deactivate
# todo(rkukura): filter on extended provider attributes. </s> nets = self._filter_nets_l3(context, nets, filters)	get_networks self._extend_network_dict_provider(context, net) self._extend_network_dict_l3(context, net) return [self._fields(net, fields) for net in nets]
for char in ('\\', '\n', '\t'):  # todo: more escapes? </s> value = value.replace(char, char.encode('unicode-escape'))	encode if not isinstance(value, unicode): return value return value.encode('utf-8')
# todo: out to file </s> pass	write_up def write_up(self):
# todo: paginate </s> users = user.objects.all()	user_list @administrator_required def user_list(request): context = {'users': users, } return render(request, 'spirit/admin/user/user_list.html', context)
# todo: reconsider logging level when we have consistent practice. </s> logger.warning(msg, extra=dict(cls=none))	_build_mesh ", consistent with the attached connectivities." ) else: quoted_topology_dimension = mesh_var.topology_dimension
# todo: add test and check this more throughroughly. </s> if hasattr(layer, "kernel"):	contains_kernel def contains_kernel(layer): Check whether the layer contains a kernel. return True else:
# todo: must be implemented </s> pass	range_from_chapters def range_from_chapters(crawler, times=0):
# todo: cleanup and deprecate worker_address in config files, leaving only checksum_address </s> if config_class == ursulaconfiguration:	extract_checksum_address_from_filepath if filename == default_name: checksum_address = config_class.peek(filepath=filepath, field='checksum_address') federated = bool(config_class.peek(filepath=filepath, field='federated_only')) if not federated:
# directory exists, but no todo.txt file - create an empty one </s> open(file_path, 'a').close()	get_real_path directory = os.path.dirname(file_path) if os.path.isdir(directory): else: exit_with_error("ERROR: The directory: '{0}' does not exist\n\nPlease create the directory or specify a different\n{0} file on the command line.".format(directory, description))
# todo: set cchq_case_id in dhis2 </s> pass	pull_child_entities def pull_child_entities(): Create new child cases for nutrition tracking in CommCare.
# todo: error handling like numba callwrappers.py </s> native_val = unbox_array(types.array(dtype=dtype, ndim=1, layout='c'), arr_obj, c)	lower_unbox_df_column else: dtype = sig.args[2].dtype c.pyapi.decref(series_obj) c.pyapi.decref(arr_obj)
if field.unique or field.primary_key:  # todo: multi-fields. </s> query.append(field == value)	create_or_get for field_name, value in kwargs.items(): field = cls._meta.fields[field_name] return cls.get(*query)
# todo -1 here added because that is done in rot90, but will have to be fixed </s> assert np.allclose(bbsoi_aug.bounding_boxes[0].x1, 5 - 5 - 1)	test_Augmenter_augment_bounding_boxes aug = iaa.Rot90(1, keep_size=False) bbsoi_aug = aug.augment_bounding_boxes(bbsoi) assert np.allclose(bbsoi_aug.bounding_boxes[0].x2, 5 - 4 - 1) assert np.allclose(bbsoi_aug.bounding_boxes[0].y1, 1)
'xception'      : [testmodels.coremlemit, testmodels.cntkemit, testmodels.mxnetemit, testmodels.pytorchemit, testmodels.tensorflowemit], #  todo: caffe(crash) testmodels.kerasemit(too slow) </s> 'squeezenet'    : [testmodels.caffeemit, testmodels.cntkemit, testmodels.coremlemit, testmodels.kerasemit, testmodels.mxnetemit, testmodels.pytorchemit, testmodels.tensorflowemit],	get_test_table 'resnet152'     : [TestModels.CaffeEmit, TestModels.CntkEmit, TestModels.CoreMLEmit, TestModels.KerasEmit, TestModels.MXNetEmit, TestModels.PytorchEmit, TestModels.TensorflowEmit],
# todo: refactor, move to utils </s> from django.template.loader import get_template	compile_message def compile_message(self, form_data, template=None): from django.template import Context, Template if template:
# todo - send file in chunks if file size > some threshold. </s> headers = {'content-type': 'application/json',	_submit_sample } with open(fname, "rb") as sample: 'user_agent': user_agent, 'filename': basename(fname)}
# todo: should we ignore and use 0.0.0.0, or try using what the user asked and fail? </s> log.warning('%s is not a valid interface. ignoring.', opts[u'source_interface'])	resolve_dns log.warning('The interface %s is down. Ignoring', opts[u'source_interface']) else: elif opts[u'source_address']: ret[u'source_ip'] = salt.utils.network.dns_check(
# todo: add prompt to specify user and pass manually. </s> sess = requests.session()	anubis print_error("You need to specify a user/pass") return sess.auth = (ANUBIS_USER, ANUBIS_PASS) res = sess.post(
return # todo raise error </s> self.backend.playback.stop().get()	Stop if not self.get_CanControl(): logger.debug(u'%s.Stop not allowed', PLAYER_IFACE)
# self.assertisnotnone(result_set.query_planning_time_in_millis)  # todo flaky test </s> self.assertisnotnone(result_set.output_location)	test_fetchone self.assertIsNotNone(result_set.query_queue_time_in_millis) self.assertIsNotNone(result_set.total_execution_time_in_millis) self.assertIsNone(result_set.data_manifest_location)
# todo: test without file </s> def test_impl():	test_unique_parallel def test_unique_parallel(self): df = pq.read_table('example.parquet').to_pandas() return (df.four.unique() == 3.0).sum()
# todo: move this into onnx main library </s> pass	get_type def get_type(onnx_type):
# todo: handle multiple skip stacks </s> (skip, skip_stack), = skip_stack.items()	block_container_layout first_letter_style = getattr(box, 'first_letter_style', None) else: first_letter_style = None for index, child in enumerate(box.children[skip:], start=(skip or 0)):
# todo this is not right, but does it need to be? </s> return ['tags']	value_columns return ['tags'] elif 'call' in val: else: assert 0, 'Unknown dict value: %r' % val
# todo change this and other yml names to match the tutorial </s> deployment_moved_config = temp.child(b"deployment.yml")	test_moving })) flocker_deploy(deployment_config, application_config) deployment_moved_config.setContent(safe_dump({ u"version": 1,
# todo do a proper mro resolution. currently we are just listing </s> from jedi.inference.gradual.typing import typeddictbase	is_typeddict def is_typeddict(self): for lazy_cls in self.py__bases__(): if not isinstance(lazy_cls, LazyTreeValue):
# todo handle 4 types of transition exceptions </s> pass	create_next pass except Exception: new_transaction.status = next_status return new_transaction
# todo uncomment the actual test below after we have implemented the l1 attack </s> notimplementederror, self.clip_eta, eta=self.rand_eta, ord=1, eps=.5)	test_clip_eta_l1 self.assertRaises(
# todo(solitude): remove this. </s> data.update({'pattern': 'account.payment'})	preapproval key = result['key'] else: try: result = paypal.get_preapproval_key(data)
# todo: not close enough </s> np.testing.assert_allclose(cc, result, atol=0.000001)	test_template_matching_time shift = 20 cc = correlate(data, template, shift, normalize="full", domain="time") shift, corr = xcorr_max(cc) self.assertAlmostEqual(corr, 1.0, 7)
# todo: better error reporting </s> if req.error:	GetResults results[name] = serializer.LoadJson(req.resp_body) continue msg = req.error else:
# todo support startblock, endblock </s> def registertxevent(self, tx_id, unregister=true,	registerTxEvent disconnect=False, onEvent=None): self._tx_ids[tx_id] = EventRegistration(onEvent,
# todo(jin feng) always output the unhandled exception details into a log file. </s> self.exit_and_fail(msg)	unhandled_exception_hook else: msg += '\nNo specific exception message.\n'
# todo: translate </s> context["title"] = "posts for year %s" % year	task_render_archive context["lang"] = lang context["items"] = [("[%s] %s" % (post.date, post.title(lang)), post.permalink(lang)) for post in post_list] yield generic_post_list_renderer( lang,
# todo: check proactive neighbor resolution </s> self.asserttrue(self.packet_outs_from_flows(echo_replies))	icmp_ping_unknown_neighbor 'ipv4_dst': '10.0.0.99', 'echo_request_data': bytes('A'*8, encoding='UTF-8')})
# todo: call into expression language. </s> self._next(lex_mode)  # get )	_ReadCompoundWord self._Next(lex_mode)  # Skip ( self._Peek() self._Peek() if self.token_type != Id.Op_RParen:
# todo: consider using eafp here instead. </s> if result is _not_found:	get result = self._get_simple(parts[0]) for part in parts[1:]: break
# todo return empty list if not loaded </s> spotify.error.maybe_raise(self.error)	artists def artists(self): Will always return :class:`None` if the search isn't loaded. if not self.is_loaded: return None
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> response = util.create_response();	test_happy_path @httpretty.activate def test_happy_path(self): self.setup_expected_auth_code_token_request_response(200, response['wireResponse']) def callback(err, tokenResponse):
# todo(termie): this stuff should probably be moved to middleware </s> self.assert_admin(context)	delete_token def delete_token(self, context, token_id): token_ref = self.token_api.delete_token(context=context, token_id=token_id)
# todo have one global properties object so this is no longer necessary </s> exporter.node_cache.clear()	export def export(self, exporter, props, luxcore_name): prefix = "scene.materials." + luxcore_name + "." interior_volume_name = None exterior_volume_name = None
# todo: wrap exception </s> return contracts_decorate(function, **kwargs)	wrap def wrap(function):
# todo: _busy_wait should timeout after n seconds </s> gpio.input.return_value = gpio.low	test_init_7colour_setup Verify our expectations for GPIO setup in order to catch regressions. from inky.inky_uc8159 import Inky inky = Inky() inky.setup()
if isinstance(err, asyncio.cancellederror):  # todo: remove when updated to 3.8 </s> raise	StreamManager raise ResolveTimeoutError(uri) except Exception as err: log.exception("Unexpected error resolving stream:") raise ResolveError(f"Unexpected error resolving stream: {str(err)}")
# todo(slaweq): change this to neutron floating ips and turn neutron </s> mock_nova.floating_ips.list.return_value = [fake_floating_ip]	test_create_server_with_admin_pass_no_wait mock_nova.servers.create.return_value = fake_create_server mock_nova.servers.get.return_value = fake_server self.assertEqual( self.cloud._normalize_server(
# todo: make the get_closest_value to return region </s> value, value_index = self.get_closest_value(	get_current_CSS_value parsed_declaration = re.search(r'^(\s*)(-[a-zA-Z]+-)?([a-zA-Z0-9-]+)(\s*(?: |\:))((?:(?!\!important).)+)', declaration) declaration_index = declaration_index + parsed_declaration.start(5) parsed_declaration.group(5), declaration_index,
# todo: py3 typeerror: a bytes-like object is required, not 'str' </s> outpasswd.write("%s:%s:%s:%s:%s:%s:%s\n" %	add_user return False else: (user, passw, uid, gid, gecos, home, shell)) outpasswd.close()
#@todo: recheck in 0.4.10 </s> if self.account:	process elif e.code in (401, 403): self.log_debug("Auth required", "Received HTTP status code: %d" % e.code) servers = [x['login'] for x in self.account.getAllAccounts()] else:
pass  # todo: implement this </s> def close(self) -> none:	close
# todo(dcramer): we should default offset to previous entry in logsource </s> kwargs.setdefault('offset', 0)	logchunk def logchunk(source, **kwargs): text = kwargs.pop('text', None) or '\n'.join(get_sentences(4)) logchunk = LogChunk(
# todo: if !blocking... </s> self.__wait(self.__turn_shr)	__shared_acquire while self.__exc is not None: wait = True self.__shr.add(threading.currentThread()) if wait:
# todo: rate limiting </s> p_ctx.active = true	handle_rpc contexts = self.get_contexts(request) p_ctx, others = contexts[0], contexts[1:] if p_ctx.in_error: return self.handle_error(p_ctx, others, p_ctx.in_error)
# todo(rakhmerov): it's not stable, need to avoid race condition. </s> eventlet.sleep(delay * 2)	test_from_no_retry_to_retry_task {'output': 'result'}) tasks = db_api.tasks_get(WB_NAME, execution['id']) self.engine.convey_task_result(WB_NAME, execution['id'], tasks[1]['id'], states.ERROR,
# todo: theme me </s> drawings = []	draw for txt in self.key['label']: labels.append(TextArea(txt, textprops=dict(color='k'))) for g in self.geoms: g.data.rename(columns=g.geom._aes_renames, inplace=True)
# todo: clean up </s> yield gossipsubfactory.create_batch(num_hosts, **gossipsub_params._asdict())	gossipsubs def gossipsubs(num_hosts, gossipsub_params):
step = 0.1  # todo </s> jd = arange(jd0, jd1, step)	find raise ValueError('your start_time {} is later than your end_time {}' .format(start_time, end_time)) while True: t = ts.tt_jd(jd)
# todo remove </s> args = parsing.array(parsing.array.tuple, none, values=[])	handle_iterators else: try: generators += \ it.execute_subscope_by_name('__iter__', args)
# todo: add also video files? </s> for item in dirs:	on_feed_filter dirs = [x.decode('utf-8', 'ignore') for x in dirs] files = [x.decode('utf-8', 'ignore') for x in files] if item.lower() in self.skip: continue
#assert false, "todo: implement" </s> pass	init_table_wdl self.precomp[chess.BLACK].data = data_ptr else:
# todo: prettify output </s> root_check()	main gnome_power_enable() elif disable: print("Disabling") gnome_power_disable()
# todo: implement this method </s> return self._ref	Handle @property def ref(self): @ref.setter def ref(self, value):
# todo repition of normalization may be wasteful on large phase diagrams </s> if entry.normalize() not in [e.normalize() for e in self.stable_entries]:	get_quasi_e_to_hull energies <= 0, Stable elemental entries should have energies = 0 and unstable entries should have energies > 0. return self.get_decomp_and_e_above_hull(entry, allow_negative=True)[1] if entry.is_element:
# todo: should we restore the user-registered handler? </s> signal.signal(signal.sigint, signal.sig_ign)	EndReadline def EndReadline(self):
# todo: hack </s> network_authentication_id = container_stacks[0].getmetadataentry("network_authentication_id")	read stack = container_stacks[0] if self._resolve_strategies["machine"] == "override": network_authentication_key = container_stacks[0].getMetaDataEntry("network_authentication_key") container_stacks[0].deserialize(archive.open(global_stack_file).read().decode("utf-8"))
# todo: improve logic to handle simple types like list of strings? </s> try:	parse_config_overrides else: value = args.pop(0) result[opt] = srsly.json_loads(value) except ValueError:
# todo: support multi-index here </s> if len(index_scols) != 1:	filter raise ValueError("items should be a list-like object.") if axis in ('index', 0): raise ValueError("Single index must be specified.") col = None
# todo: obviously incrementing the rows individually is bad. how </s> for i, word_piece_slice in enumerate(wp_rows):	set_tensors doc.tensor = xp.zeros((len(doc), outputs.width), dtype="f") wp_weighted = wp_tensor / xp.array(align_sizes, dtype="f").reshape((-1, 1)) doc.tensor[i] = wp_weighted[word_piece_slice,].sum(0)
# todo: not implemented yet </s> messager.warning('event search not implemented yet, sorry!')	search_event def search_event(directory, type, trigger): return format_results(SearchMatchSet('empty'))
# todo extend to nonbinary nodes </s> augmented_child_tpms = [	raw_marbl return self._raw_marbl else: [child._dimension_labels[self.index], child.tpm[1].squeeze()] for child in self.outputs
# todo. optionally sort on birthdate </s> for child_handle in childlist:	display_ind_parents of.write('\t\t\t\t\t<ol>\n') childlist = [child_ref.ref for child_ref in child_ref_list] sibling.add(child_handle)   # remember that we've already "seen" this child if child_handle != self.person.handle:
# todo: remove when we stop supporting python < 3.5 </s> if sys.version_info.major < 3 or sys.version_info.minor < 5:	predict prediction : `numpy.ndarray` of floats, shape=(n_constraints,) Predictions of the ordering of pairs, for each quadruplet. check_is_fitted(self, 'preprocessor_') else:
# todo: this is a debug level log </s> logger.info(io.open(config_file, "rt", encoding="utf-8").read())	main if config_file_exists: logger.info("Reading config file {}:".format(config_file)) try: config.read_file(io.open(config_file, "rt", encoding="utf-8"))
#todo: mock the socket! </s> try:	test_sessionkeys accounts = {} sut = pop3.pop3(sessions, accounts) sut.handle(None, ['192.168.1.200', 51000]) except:
# todo replace with clone </s> self.drift_detector = copy.deepcopy(base_drift_detector)  # actual detector used	__init__ if base_drift_detector is not None: self.disable_drift_detector = False else: self.disable_drift_detector = True
# todo: check aligned nans, (s1.notna() != s2.notna()).any() </s> a = n * ((s1*s2).sum()) - ma * mb	_column_corr_impl ma = S1.sum() mb = S2.sum() b1 = n * (S1**2).sum() - ma**2 b2 = n * (S2**2).sum() - mb**2
# todo: capture makedirs invocation here </s> expect([ 'path/to/git', 'init'],	test_simple self.basedir) + 0, os.path.join(self.basedir, 'source'), sendRC=False, timeout=120, usePTY=False)
#todo: dataset/hda by id (from history) or check_ownership for anon user </s> hda = self.get_history_dataset_association( trans, history, id,	show and ( history_id == trans.security.encode_id( trans.history.id ) ) ): history = trans.history check_ownership=False, check_accessible=True ) else:
# todo(cp16net): need to set the return code correctly </s> return wsgi.result(views.instanceview(server).data(), 201)	show tenant=tenant_id) server = models.Instance(context=context, uuid=id).data()
pass # todo: pass link problem upstream? </s> def link_to(matchobj):	format_body_sample link = urljoin(resource.response.base_uri, link) except ValueError as why: return r"%s<a href='?%s' class='nocode'>%s</a>%s" % ( matchobj.group(1),
# todo(stephenfin): the mock of 'migrate_disk_and_power_off' should </s> with mock.patch('nova.virt.libvirt.driver.libvirtdriver'	test_cold_migrate_server_with_neutron def test_cold_migrate_server_with_neutron(self): def move_operation(source_server): '.migrate_disk_and_power_off', return_value='{}'): self._migrate_server(source_server)
# todo: update this to the correct kaggle.gcp path once we no longer inject modules </s> from kaggle_gcp import publicbigqueryclient, kagglekernelcredentials	init from google.cloud import bigquery from google.cloud.bigquery._http import Connection if get_integrations().has_bigquery(): from google.cloud.bigquery import magics
# todo extract that to a method in otc(...) ? </s> offers = [self.otc.get_offer(offer_id + 1) for offer_id in range(self.otc.get_last_offer_id())]	otc_offers def otc_offers(self, sell_token: Address, buy_token: Address): offers = [offer for offer in offers if offer is not None] return list(filter(lambda offer: offer.owner == self.our_address and
# rbarlow_todo: convert this callrequest into a celery task call </s> call_request = callrequest(manager.install_content, args, kwargs, weight=weight, tags=tags, archive=true, asynchronous=true)	consumer_content_install_itinerary tags = [resource_tag(dispatch_constants.RESOURCE_CONSUMER_TYPE, consumer_id), action_tag('unit_install')] call_request.add_control_hook(dispatch_constants.CALL_CANCEL_CONTROL_HOOK, cancel_agent_request) call_request.reads_resource(dispatch_constants.RESOURCE_CONSUMER_TYPE, consumer_id)
# todo: this should be handled in run_script </s> os.unlink(pid_file)	run logfile.write('----------------------------------------\n\n') logfile.close() os._exit(1) if not isinstance(delay, int):
# todo: normalise by s**-.5? </s> return output	ricker A = np.pi ** -0.25 * np.sqrt(4 / 3) output = A * (1 - x ** 2) * np.exp(-x ** 2 / 2)
#todo get this working </s> self._test_cant('list', self.mrloggedin, ['deleted'], test=['wui'])	test_admin_list_deleted self._test_can('list', self.admin, ['--', 'r-', 'w-', 'rr', 'wr', 'ww', 'deleted'], test=['wui']) self._test_can('list', self.mrloggedin, ['--', 'r-', 'w-', 'rr', 'wr', 'ww'], test=['wui'])
# todo: figure out what to do with this list </s> self.fd = fd[0]	__init__ def __init__(self, fd):
# todo: this test is covered by the previous class, do we need a dedicated one? </s> pass	test_munsell_colour_to_xyY def test_munsell_colour_to_xyY(self): Tests :func:`colour.computation.colourspaces.munsell.munsell_colour_to_xyY` definition.
# todo: this might be too slow because of the addition </s> data = reduce(lambda res, (key,val): res + int(val)*[key], data.iteritems(), [] )	series data = int(data) if data else 0 elif config.get('compress',False): if config.get('read_cast'): data = map(config.get('read_cast'), data)
# todo: check against plural_rules[lang]['nplurals'] </s> repl = variants.split('|')[plural_func(num)]	twntranslate except KeyError: plural_func = plural_rules['_default']['plural'] trans = re.sub(PATTERN, repl, trans) if param:
# todo include efficientnet </s> else:	replace_head elif isinstance(model, torchvision.models.Inception3): raise NotImplementedError() raise ValueError("Model {model} not recognized")
from vyper.old_codegen.expr import expr  # todo rethink this circular import </s> value, gas = none, none	get_gas_and_value def get_gas_and_value(stmt_expr, context): for kw in stmt_expr.keywords: if kw.arg == "gas":
# todo: figure out how to best show this kind of warning to the </s> return self.to_bytes(obj.__name__)	_to_bytes return self._code_to_bytes(obj, context) elif inspect.ismodule(obj): elif inspect.isclass(obj): st.warning(('Streamlit does not support hashing classes. '
# @todo: possible optimisation: create a re.compile list </s> return any(j for j in [re.match(i, value) for i in self.get_conf_value('hide', header=header)])	is_hide Example for diskio: hide=sda2,sda5,loop.*
# todo: there’s a vertical 0.5px shift on the second page </s> assert_pixels('colspan_last_row', 22, 36, '''	test_tables_15 @assert_no_logs def test_tables_15(): ______________________ __RRRRRRRRRRRRRRRRRR__
# todo: sound feedback to signal that this is an invalid action </s> return	speed_down def speed_down(self): if self.speed_is_paused(): if self.timer.ticks_per_second in GAME_SPEED.TICK_RATES: i = GAME_SPEED.TICK_RATES.index(self.timer.ticks_per_second)
except exception as error:  # todo: be specific </s> exit_code = none	call try: exit_code = func(sopel, trigger) self.error(trigger, exception=error) if exit_code != NOLIMIT:
# todo: ... </s> return '## comments'	get_comments_md def get_comments_md(comments):
# todo: finish this. </s> def mkdir(self, path, mode):	mkdir pass
assert self.stop_seq is none #todo: better handling of this situation </s> l.debug("initializing start sequence")	start_program def start_program(self): assert self.start_seq is None #TODO: Better handling of this situation self.start_seq = sequence_controller() for p in self.roaster:
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> util.setup_expected_user_realm_response_common(false)	test_invalid_id_token @httpretty.activate def test_invalid_id_token(self): response = util.create_response() wireResponse = response['wireResponse']
return -1  # todo: followup after decision around returning none </s> "cannot access _eprocess.objecttable.handlecount at {0:#x}".format(self.vol.offset))	get_handle_count vollog.log(constants.LOGLEVEL_VVV,
# todo: more variables on the same line? </s> match = variable_use_rx.match(line)	parse imported_files += [match['file']] continue if match and match['key'] in variables: out.write(match['before'])
# todo(luotao): use clone() method to flush the program.desc in force, </s> program = program.clone()	_fuse_conv_eltwise_mkldnn self._adjust_input() self._remove_unused_var()
# todo: move to base class </s> return self._manipulationmode	Canvas @property def manipulationMode(self): @manipulationMode.setter def manipulationMode(self, value):
# todo: the following skipped suite and fixtures should be enabled </s> return ['_key', '_user']	_filter_query_parameters def _filter_query_parameters(self):
# todo do something with temp </s> self.init_state_distn.resample([s.stateseq[:1] for s in self.states_list])	resample_init_state_distn def resample_init_state_distn(self,temp=None): self._clear_caches()
f.writable = false # @todo: currently this hides the widget from update forms instead of just rendering read-only! </s> crud_fields.append(s3sqlinlinelink("need",	customise_project_activity_resource f = natable.need_id f.represent = req_NeedRepresent() field = "need_id", label = T("Need"),
# todo policyuniverse can't expand resource wildcards so further thought is needed here </s> session.run(	load_group_policies role_arns = [role_arns] for role_arn in role_arns: ingest_policies_assume_role, GroupName=group_name,
# todo tests for this </s> return the number of a pre-release.	get_pre_release def get_pre_release(version): :param bytes version: A pre-release version of Flocker. :return int: The number of the pre-release.
# todo: header fields might vary across file types, thus prior sensing would be needed </s> header_fields = file_headers[0].keys()	diff file_headers = [nib.load(f).header for f in files] if header_fields == 'all': else: header_fields = header_fields.split(',')
# todo: check the data! </s> self.asserttrue(len(list(pipe)) > 0)	test_tail pipe_def = self._get_pipe_def(pipe_file) pipe = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def)
# todo verbosity was mistakenly removed from task_parameters on release 0.11.0, need to bring it back </s> if self.ap.is_a_highest_level_agent:	reset_evaluation_state "Success Rate", success_rate) screen.log_title("{}: Finished evaluation phase. Success rate = {}, Avg Total Reward = {}" .format(self.name, np.round(success_rate, 2), np.round(evaluation_reward, 2)))
# todo check if lus can be more than one token </s> if annotations['lu'] in lines[i]:	produce_training_data lines[i].insert(1, str(i)) lines[i].append(annotations['frame']) lines[i].append('B-LU') else: lines[i].append('O')
# todo: error detection </s> __connect()	serialize_item def serialize_item(obj, item): collection = mongodb[obj.collection_type()] data = collection.find_one({'name':item.name})
# todo: i should make sure to escape single quotes here </s> self._cursor.execute("select value from %s where key='%s'" % (self._name, key))	__getitem__ if not isinstance(key, basestring): raise ValueError("key must be a string") results = self._cursor.fetchall() if len(results) == 0:
# todo: check that the performance measure is within some range </s> bottleneck2_baseline(num_runs=1, sumo_binary="sumo")	test_bottleneck2 Tests flow/benchmark/baselines/bottleneck2.py
# todo assert cls.__tablename__ == '' </s> with dbcleanup(session, cls):	test_PageTagAssociation def test_PageTagAssociation(model, session, page, tag, user): cls = model.PageTagAssociation user_tname, value, user_value = 'a', 'b', 'c' obj = cls(user=user, tag_id=tag.id, user_tname=user_tname, value=value)
except:  # todo: do not use bare except </s> attemptcount = '{0}/{1}'.format(retrycount + 1, ipfsretrycount)	pushToIPFS print('Run "ipfs daemon" in another terminal session.') sys.exit() logError('IPFS failed to add, ' + 'retrying attempt {0}'.format(attemptCount))
# todo(brett.cannon) implement </s> raise importerror	_default_hook def _default_hook(self, path): If the path will not work for the default hook then raise ImportError.
# todo(b/157460932): migrate to glaziererror </s> terminator.log_and_exit('unable to remove task list', self._build_info,	_SetupTaskList os.remove(location) except OSError as e: 4303, e) return location
# todo: test permissions, non-writable fields </s> assert_equal(self.node.title, new_title2)	test_update_title self.node.update({'title': new_title2}, auth=Auth(write_contrib))
# todo check .item() migration </s> for o in outputs:	PtrExtractorRL out = torch.distributions.Categorical(prob) else: score[0, o[0, 0].item()][0] = -1e18 out = score.max(dim=1, keepdim=True)[1]
# todo: implement "n-v.a" form and fix this test </s> self.assertequal(len(result), 0)	test_nsap result = list(subj.get_nsvap_possibilities(forms=hawkey.FORM_NEVRA))
# todo: we really need a "builtin" name ref node to avoid creating a </s> result = nodes.cpythonexpressionvariableref(	type1_extractor type_name = value.__class__.__name__ assert (type_name in _builtin_names), (type_name, _builtin_names) variable_name = type_name, source_ref    = node.getSourceReference()
# todo: write tests </s> <beam> a container for a series of explicitly beamed events that begins and ends entirely	beamFromElement def beamFromElement(elem, slurBundle=None): within a measure. In MEI 2013: pg.264 (278 in PDF) (MEI.cmn module)
# todo test </s> return true	state "according to the given TPM.")
# todo: the status should be infeasible here, i think </s> status = constants.lpstatusnotsolved	GUROBI_CMD raise PulpSolverError("PuLP: Error while trying to execute "+self.path) if not os.path.exists(tmpSol): values = reducedCosts = shadowPrices = slacks = None else:
# todo: support more than just lists </s> self._unify(tlist(node.target.type), node.iter.type,	visit_For def visit_For(self, node): node = self.generic_visit(node) node.target.loc, node.iter.loc) return node
# todo: handle case where entity has never been assigned components </s> entity1 = world.create_entity()	test_delete_entity def test_delete_entity(world): world.add_component(entity1, ComponentC()) entity2 = world.create_entity()
# todo: look this up in one query </s> for user_id in obj['collaborator_ids']:	get_by_mbid obj['collaborator_ids'] = playlist_collaborator_ids.get(obj['id'], []) collaborators = [] user = db_user.get(user_id) if user:
# todo: add input option for sample_size </s> input_encoding = input_encoding or default_input_encoding	csv_clean - Output dialect: excel - Output encoding: UTF-8 sample_size = 1024 * 1024 fobj = open_compressed(source, mode="rb")
# todo(nakago): check why tolerance is high </s> def test_backward_cpu(model, data):	test_backward_cpu atom_data, adj_data, y_grad = data gradient_check.check_backward(model, (atom_data, adj_data), y_grad,
# todo: activate this code if we have limits at webmail level </s> if not self.limiter.test(self.rate,"client-ip",clientip):	check def check(self,clientip): raise RateLimitExceeded()
pass  # todo </s> def test_print_list_with_selector_success(self):	outTestsWithHackedStdout self.assertEquals(sys.stdout.read(), notes_list) def test_print_list_with_selector_success(self): pass  # todo def test_search_result_success(self):
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo uncomment the actual test below after we have implemented the l1 attack </s> **self.attack_param)	test_adv_example_success_rate_l1 NotImplementedError, self.help_adv_examples_success_rate, norm=1,
# todo: inputs and outputs could be pretty long. these may be worth </s> inputs = dry_run_info["inputs"]	_display_basic dry_run_info = res["dry_run_info"] lines = [fmt_line("location", dry_run_info["pwd_full"])] if inputs: lines.append(fmt_line("expanded inputs", inputs,
# todo: warn </s> return none	get_image_surface_from_uri return handler(file_like) except (IOError, MemoryError): finally: file_like.close()
# todo(jd) move into prepare_service gettextutils and eventlet? </s> eventlet.monkey_patch()	agent_central def agent_central(): gettextutils.install('ceilometer') service.prepare_service(sys.argv)
# todo: can we not reestablish the connection earlier? </s> with pytest.raises(rpcconnectionerror) as exc_info:	test_recover_from_timeout assert "Socket closed" in str(exc_info.value) toxiproxy.reset_timeout() service_rpc.echo(3) assert "Disconnected while waiting for reply" in str(exc_info.value)
ioloop.current().close()  # never reached. todo: clean shutdown of ioloop </s> ioloop.current().start()	run_center center.listen(port)
# hack to support saving/loading pytorch models. todo: improve </s> if hasattr(layer, '_model') and not isinstance(layer._model, model):	from_bytes i = 0 for layer in queue: layer.from_bytes(weights[i]) i += 1
#todo: mock the socket! </s> try:	test_initial_session Session.authenticator = authenticator sut = ftp.ftp(sessions, 21) sut.handle_session(None, ['192.168.1.200', 12000]) except AttributeError:
# todo: handle boolean overrides </s> try:	alice rest_port=rest_port, provider_uri=provider_uri) click.secho("Decrypting keyring...", fg='blue') alice_config.keyring.unlock(password=click_config.get_password())
# todo: hand derive. current value is just a canary to detect changes. </s> np.testing.assert_almost_equal(	test_minute_buy_and_hold "Number of positions should stay the same.") self.assertIsNotNone(third_msg, "There should be a message emitted.") -0.047639464532418657, crm.algorithm_returns[-1],
# todo: kickoff syncing process with this peer </s> pass	Node (peer_has_equal_finalized_epoch and peer_has_higher_head_slot) ): await stream.close()
# todo: consider a case where len(url_info_list) > 1. </s> url = port.url_info_list[0].url	connect_device else: port = self.connecting_device.remote_port location, others = url.split(':', 1) file_name, address, size = others.split(';')
# store the reason into entry, todo: plugin in the future? </s> if reason:	reject self.rejected.append(entry) self.verbose_details('Rejected %s' % entry['title'], reason) entry['reason'] = reason entry['rejected_by'] = self.current_plugin
# todo(dcramer): we want to be less aggressive on disabling domains </s> if domain_lock_enabled:	fetch_file logger.exception(six.text_type(exc)) error = {"type": EventError.UNKNOWN_ERROR, "url": expose_url(url)} cache.set(domain_key, error or "", 300) logger.warning("source.disabled", extra=error)
# todo(py27): python versions < 3.3 do not support this syntax. </s> yield from batch_gen(file_stream(), batch_size, self.overfit, self.mute_spurious_targets)	iterate_forever log.info('queueing data from %s for iterate forever', file_name) yield file_name
# todo: discuss api </s> @abc.abstractmethod	create_new_study_id def create_new_study_id(self): raise NotImplementedError
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered value as 'not' rule. " + "Toggle triggered value of 'not' rule.") + "remote id '%d' and username '%s' has same " % (element.element.remoteSensorId,
# todo(ultrotter): import/export still to be converted to os api 10 </s> logging.error("import/export still to be converted to os api 10")	ExportSnapshot Returns: True if successful, False otherwise. return False inst_os = OSFromDisk(instance.os)
# todo extend to nonbinary nodes </s> def uniform_distribution(number_of_nodes):	uniform_distribution :param nodes: a set of indices of binary nodes :type nodes: ``np.ndarray``
# todo: update the entity with program_data? </s> event = dhis2_api.form_to_event(nutrition_id, form, nutrition_assessment_event_fields,	get_payload if form['xmlns'] == 'http://openrosa.org/formdesigner/b6a45e8c03a6167acefcdb225ee671cbeb332a40': nutrition_id = dhis2_api.get_program_id('Paediatric Nutrition Assessment') case['external_id']) return json.dumps(event, default=json_serializer) if event else None
raise skiptest  #todo: figure out why this randomly started failing. </s> qs = {'a': 1, 'w': 4, 'format': 'json', 'thread_type': 1, 'forum': 1}	test_discussion_filter_sticky def test_discussion_filter_sticky(self): response = self.client.get(reverse('search'), qs) result = json.loads(response.content)['results'][0]
# todo add arch arm/aarch/mips/mips64/sparc/sparc64 </s> if arch is none and mode is none and endian is none:	get_arch_mode else: arch, mode, endian = unicorn.UC_ARCH_X86, unicorn.UC_MODE_64, unicorn.UC_MODE_LITTLE_ENDIAN raise Exception("Failed to get architecture parameter from mode") return arch, mode, endian
#todo - return self? </s> raise valueerror("feature references another sequence.")	FeatureLocation def _shift(self, offset): if self.ref or self.ref_db: return FeatureLocation(start = self._start._shift(offset), end = self._end._shift(offset),
# todo all this for now, until someone fixes the codegen. </s> continue	find_issues analyse_each_line = False if filename.endswith('.gen.h') or filename.endswith('.gen.cpp'): if filename.startswith('openage/'  ) and filename.endswith('.cpp'): continue
# todo: refactor to remove all references to "set_detection_mapping_mode" from codebase </s> new_mode = "3d" if is_on else "disabled"	detection_enabled_setter def detection_enabled_setter(is_on: bool): n = {"subject": "set_detection_mapping_mode", "mode": new_mode} ipc_pub.notify(n)
# todo: align series </s> return geoseries([s[0].union(s[1]) for s in zip(self, other)],	union Operates on either a GeoSeries or a Shapely geometry if isinstance(other, GeoSeries): index=self.index) else:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_transactions_null def test_fail_transactions_null(self):
# todo generator </s> if not get_data:	_get if len(content) >= 1 and content[0] == curdir: results.append(cur_ds) lgr.debug( "Will not get any content in %s, as instructed.",
# todo: give a vanilla example </s> .. math::	feca def feca(predicted_power, df_appliances_ground_truth): fraction = \\sum_n min \\left (
# todo: instead of discarding pending jobs, maintain them </s> for njob in node.pending_jobs:	run_job _job.uid, node.ip_addr, cluster._compute.name) if node.pending_jobs: if cluster.status_callback and dispy_node: dispy_node.update_time = time.time()
# todo: then observer.events[0] == trial </s> pass	test_hyperparams_json_repository_should_be_observable_in_memory def test_hyperparams_json_repository_should_be_observable_in_memory(): repo: HyperparamsJSONRepository = HyperparamsJSONRepository()
f.write(self.pastie_content.encode('utf8'))  # todo error checking </s> f.close()	savePastie if self.site.archive_compress: f = gzip.open(full_path, 'w') else: f = open(full_path, 'w')
# todo: allow partial prase complete </s> if shouldeval and prase_input_complete(data):	_ data = data.replace('\r', '\n') shouldeval = data[-1] == "\n" and len(event.current_buffer.document.text_after_cursor) == 0 data = data.rstrip("\n") event.current_buffer.insert_text(data)
# todo add locales </s> raise yunohosterror("domain_name_unknown", domain=domain)	domain_set_settings domains = _load_domain_settings() if not domain in domains.keys(): setting_set = False if ttl is not None:
# todo: allow multiple callbacks for each hotkey without overwriting the </s> del _hotkeys[callback]	remove_ del _hotkeys[remove_]
# todo: use pabot options here </s> _copy_screenshots(options)	_report_results outputs += [_merge_one_run(os.path.join(outs_dir, index), options, tests_root_name, stats, outputfile=os.path.join('pabot_results', 'output%s.xml' % index))] if 'output' not in options: options['output'] = 'output.xml'
# todo(b/123883319) convert to tf.function. </s> if tf.executing_eagerly():	testTrainWithRNN optimizer=tf.train.AdamOptimizer(learning_rate=0.01)) traj = traj.replace(policy_info=()) train_and_loss = lambda: agent.train(traj) else:
# todo: this is a hack to make a rule know </s> if f.endswith("_none"):	_features_in_state f_slots = defaultdict(set) for f in features: if any(f[: f.rfind("_")] in key for key in state.keys()): return False
return deserialize(self.entity_bin, from_bytes=true)  # todo: techdebt fix </s> @obj.setter	Entity @property def obj(self) -> Any: def obj(self, value: Any) -> None: self.entity_bin = serialize(value, to_bytes=True)  # TODO: techdebt fix
# todo: not implemented yet </s> float_frmt = np.float16	frmt2float float_frmt = np.float32 elif frmt == 'float16': if frmt == 'float': try:
# todo: exp_block_pairs </s> self.assertequal(9, p._pos)	testStatIf self.assertIsNotNone(node)
# @todo: call onaccept if this starts doing anything other than just setting 'master' </s> else:	dc_question_onaccept if other_id: db(current.s3db.s3_field.id == other_id).update(label = question.name) ttable = db.dc_template template = db(ttable.id == question.template_id).select(ttable.table_id,
# todo: create an asynchronous celery task for this </s> start_export_tasks(event)	after_create_object if event.state == 'published' and event.schedule_published_on:
if ursula_accepts:  # todo: read the negotiation results from rest </s> accepted.add(blockchain_arrangement)	__consider_arrangements arrangement=blockchain_arrangement, network_middleware=network_middleware) else: rejected.add(blockchain_arrangement)
# forward all other methods. todo(l.zou): could use a proxy to automate these </s> return modified_grad	compute_gradients modified_grad.append((grad, var))
# todo: fix string formatting to match python/pandas </s> s = "count    " + str(a_count) + "\n"\	_gen_col_describe q50 = hpat.hiframes_api.quantile(A, .5) q75 = hpat.hiframes_api.quantile(A, .75) "mean     " + str(a_mean) + "\n"\ "std      " + str(a_std) + "\n"\
# todo: we don't support assigning permissions on key value pairs yet </s> return true	user_has_resource_permission def user_has_resource_permission(self, user_db, resource_db, permission_type):
#todo: make this actually read all </s> self._r.recv(1)	pong def pong (self):
# todo explicit inputs to nodes (right now each node is implicitly </s> mechanism_inputs = set(mechanism)	effect_repertoire [2 if node in purview else 1 for node in self.network.nodes])) external_inputs = set(self.network.nodes) - set(self.nodes)
# todo: remove anytime in 2016 </s> _assert(false, "status in filter wasn't set - this isn't expected to be possible")	get_prefix_and_key_for_filter_results_and_parsed_params prefix = "%s %s" % ("status", prefix) else: def _get_key(): if parsed_params.module is not None and parsed_params.get_module_int() is None:
# todo: what other exceptions might be returned? </s> self._text = u""	_textgetter self._text = self.get() except pywikibot.NoPage: return self._text
self.binary = serialize(value, to_bytes=true)  # todo: techdebt fix </s> def object(self, value: any) -> none:	object @object.setter
# todo(unno): too large and too small axis is deprecated in numpy 1.13 </s> warnings.warn(	expand_dims axis = axis + len(shape) + 1 if axis > a.shape or axis < 0: 'Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and ' 'will raise an AxisError in the future.',
# todo this should be a return and printed elsewhere </s> print("ideal cutoff is %0.2f, yielding tpr of %0.2f and fpr of %0.2f"	pr_curve best_recall = recall[ind] cutoff = thresh[ind] % (cutoff, best_precision, best_recall)) if show_all_cutoffs is True:
# todo: does not find everything it should when contentproxy content </s> pages = self.get_descendants().order_by('lft')	save if self._cached_url == self._original_cached_url: return for page in pages: if page.override_url:
# todo(rosmaita): bug #1745003 </s> self.assertequal('private_id_2', rows[1]['id'])	TestOcataMigrate01Mixin self.assertEqual(4, len(rows)) self.assertEqual('private_id_1', rows[0]['id']) self.assertEqual('public_id', rows[2]['id'])
# todo fetch a real object </s> return prod_code	product_from_code def product_from_code(prod_code):
# todo: skipped due to gh-4436 </s> yield skip_if_on_windows(skip_ssh(_test_remote_layout)), 'datalad-test'	test_remote_layout def test_remote_layout(): yield _test_remote_layout, None
# todo: may be check whether it fits to tracking branch </s> raise valueerror("refspec specified without a remote. (%s)" %	pull if remote is None: if refspec is not None: refspec) tb = self.repo.active_branch.tracking_branch().name
# drl_todo: this is the opposite of what i would expect </s> reverse = true	search sort_by_list.append(order_by) if len(sort_by_list) == 1: sort_by = sort_by_list[0] if facets is not None:
## todo : log error </s> error_code = delete_functionparameters_doctype_submission(doctype=doctype, action=action)	_delete_submission_from_doctype user_msg.append("""When deleting Submission "%s" from Document Type "%s", it wasn't possible to delete all Submission Fields""" \ % (action, doctype)) if error_code != 0: user_msg.append("""When deleting Submission "%s" from Document Type "%s", it wasn't possible to delete all Function Parameters""" \
# todo: exc_info. </s> future.set_exception(error)	_scan_callback def _scan_callback(self, future, command_cursors, error): if error: else: command_cursor_class = create_class_with_framework(
# todo: check error location </s> assert result.data == {'nest': none}	test_non_nullable_list_of_nullables assert len(result.errors) == 1 assert result.errors[0].message == 'Cannot return null for non-nullable type.'
# todo: handle marker? </s> for func in funcs['functions']:	_find_function conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) funcs = conn.list_functions() if func['FunctionName'] == name: return func
# todo(kenta oono) </s> unbiased_var = var * self.ny / (self.ny - 1)	check_statistics2 var = (self.x.var(axis=0) * self.nx + self.y.var(axis=0) * self.ny) / (self.nx + self.ny) gradient_check.assert_allclose(mean, self.link.avg_mean) gradient_check.assert_allclose(unbiased_var, self.link.avg_var)
#   todo: doing this for the last len(x) terms should be enough </s> self.xi = np.concatenate((self.xi, x))	fxfilter_zi logger.warning("len(zi) > len(b) - 1, zi was truncated") y_q = xb_q = np.zeros(len(x)) for k in range(len(x)): xb_q = self.Q_mul.fixp(
# todo remove? </s> yield key_stmt[0].name, value_stmt	iterate yield call.name, value_stmt else: else: if stmt.assignment_details:
# todo: compute the average cost of these feature relative to hash feature </s> return 2	_get_node_cost elif isinstance(node, (capa.features.common.Substring, capa.features.common.Regex)): elif isinstance(node, (ceng.Not, ceng.Range)): return RuleSet._get_node_cost(node.child)
if self._ndim == 3: # todo: use hasz </s> array = c_double * 3	ctypes @property def ctypes(self): return array(self.x, self.y, self.z) else:
# todo switch to transform </s> transform = self.affine	window_bounds def window_bounds(self, window): return window_bounds(transform, window)
# todo: direct use of json['error'] is deprecated; use display_message('...', 'error') instead. </s> response['error'] = 'error selecting arc types!'	arc_types_html possible = possible_arc_types_from_to(projectconfig, origin_type, target_type) if possible is None: elif possible == []: response['html'] = "<fieldset><legend>Type</legend>(No valid arc types)</fieldset>"
# todo: make this cache preparation configurable </s> return carefully_strip_whitespace(nodelist.render(context))	cached @cacheops.cached(timeout=timeout, extra=(fragment_name,) + extra) def _handle_tag(): return _handle_tag()
# todo: httpok is only handled by the httpexceptions </s> response.redirect(exc)	_publish_wsgi response._unauthorized() except HTTPRedirection as exc: status, headers = response.finalize() start_response(status, headers)
# todo: instead of arrays, vbos should be used here, as a large part of </s> gl.gldrawarrays(gl.gl_quads, 0, 4*self.n)	drawDataQuads def drawDataQuads(self):
#todo: this is just for backwards compatibility. it should be removed in v0.98 with p2.6 </s> try:	configFromFile logging.config._install_loggers(cp, handlers) except: logging.config._install_loggers(cp, handlers, False) except:
# todo change to test models separated from ralph </s> permission_admin.model = datacenterasset()	test_get_empty_list_display request.user = self.user permission_admin = PermissionAdminMixin() permission_admin.list_display = [] self.assertListEqual(
# todo(b/182316162): unify publisher handing so that post-execution artifact </s> outputs_utils.tag_executor_output_with_version(result.executor_output)	_publish_execution_results message=result.executor_output.execution_result.result_message)) return publish_params['executor_output'] = result.executor_output execution_publish_utils.publish_succeeded_execution(mlmd_handle,
'scoped': false,  # todo </s> 'type': 'open' if group.is_public else 'private',  # todo	_group_model 'id': group.pubid, 'public': group.is_public, 'urls': {} }
# todo: serialize properly </s> return self.ok(distributor)	GET try: distributor = distributor_manager.get_distributor(repo_id, distributor_id) except errors.MissingDistributor: serialized = http_error_obj(404)
# todo: for now we dependend on the timestamp to be </s> evt.timestamp = 0	Parse for evt in evt_gen: if evt.timestamp < 0: evt.query = query if not hasattr(evt, 'offset'):
# todo: read the email sender and recipient from configs. </s> self.logger = logutil.setup_logging(__name__)	__init__ def __init__(self): api_key = 'my_secret_key' self.sendgrid = sendgrid.SendGridAPIClient(apikey=api_key)
# todo: implement textures properly </s> mat.use_fake_user = true	createMaterial mat.use_transparency = True mat.ambient = 1 for prop in material: if prop.startswith('$'):
""" todo: documentation </s> return false	streaming @property def streaming(self):
# todo: change to appropriate 'clone' method </s> leaf_model = self.leaf_model.__class__(**self.leaf_model._get_params())	_new_learning_node if self.leaf_prediction in {self._MODEL, self._ADAPTIVE}: if parent is None: else: leaf_model = deepcopy(parent._leaf_model)
# todo: message depending on narrowing/float-conversion </s> msg = 'implicit conversion from {} to {} (narrowing)'	__setitem__ val = np.asarray(val) if val.dtype != dtype: warnings.warn(msg.format(val.dtype, dtype), RuntimeWarning) xs = np.asarray(val, order = 'C', dtype = dtype)
# todo remove these in enaml version 0.8.0 </s> self.closed()	_handle_close self.visible = False
# todo: if py3k, override unpickler.find_class(). </s> pass	_load mypickle.find_global = None except AttributeError: d = mypickle.load() f.close()
# todo: update once lakshmi's pr is merged </s> packs_base_path = '/opt/stackstorm'	run_sensors trigger_type_refs.append(trigger_type_ref) file_path = os.path.abspath(filename) virtualenv_path = os.path.join(packs_base_path, 'virtualenvs/',
# todo: sinpi </s> return pi / (cmath.sin(pi*x)*_gamma_complex(1-x))	_gamma_complex return complex(_gamma_real(x.real)) if x.real < 0.5: else: x -= 1.0
# todo: add at least reflection tests before adding notimplemented version </s> def prioid(context, *args):	prioid *musicpd.org, current playlist section:* ``prioid {PRIORITY} {ID...}``
@unittest.skip('not written')  # todo: finish! </s> raise notimplementederror	test_range def test_range(self):
# todo: progress +kwargs </s> rm.push(refspec=refspec, progress=progress, **kwargs)	push else:
# todo: different codec to be used </s> raise(asn1notsupperr('{0}: specific containing encoder unhandled' \	_to_oer Cont = self._get_val_obj(self._val[0]) if Cont == self._const_cont and self._const_cont_enc is not None: .format(self.fullname()))) Cont._val = self._val[1]
# todo: the stuff </s> pass	update self.updated_at = parse_datetime(trakt_movie.get('updated_at'))  # TODO: Real date parsing for genre in trakt_movie.get('genres', ()): for actor in trakt_movie.get('actors', ()): pass
# todo: implement </s> pass	add_errors def add_errors(self, res, errors):
pass # todo </s> def try_undo(self, *args):	try_undo
#todo: test size=var, with shape that change from call to call </s> if mode in ['debug_mode','fast_compile']:	test_binomial def test_binomial(): sample_size = (10,50) steps = int(1e2)
# todo - this isn't actually the correct way to set the vary header, </s> response.headers['allow'] = ', '.join(self.allowed_methods)	final def final(self, request, response, *args, **kargs): As initial, final can be overriden to add code that must be set after the render response.headers['Vary'] = 'Authenticate, Accept' response.headers.update(self.headers)
# todo: use unshare() here </s> quiet_call(	BtrFS mpoint = st.enter_context( tempfile.TemporaryDirectory(suffix='.privmnt')) 'mount -t btrfs -o noatime,noexec,nodev -n --'.split() + [self.device, mpoint])
# todo(unno): numpy.matrix is used for scipy.sparse though </s> out = xp.asmatrix(out)	test_sum_with_out out = xp.empty(shape, dtype=self.ret_dtype) if xp is numpy: return m.sum(axis=self.axis, dtype=self.ret_dtype, out=out)
# todo: check if this logic is sufficient </s> for cindex in set(bottom.kinfo.coefficient_map	compile_expression ) index = top.indices + top.kinfo.coefficient_map): c = exp.coefficients()[cindex]
#todo: implement check_equals </s> index_ic = indexcorrespondence.from_correspondence(self._index, index)	reindex index = index_from_optional_constructor(index, default_constructor=Index) else: index = self._index
# todo(rbharath, enf): figure out why pi_stack is slow and cation_pi </s> voxel_feature_types=["ecfp", "splif", "hbond",	load_core_pdbbind_grid grid_featurizer = GridFeaturizer( voxel_width=16.0, feature_types="voxel_combined", "salt_bridge"], ecfp_power=9, splif_power=9, parallel=True, flatten=True,
pass # todo </s> def rename(self, playlist, new_name):	rename
# todo action required that updates the endpoint </s> endpoints = []	collect_on def collect_on(self, args): eps = self._get_endpoints(args, -1) for endpoint in eps:
# todo username </s> return 'aqbwdj5qap6lhhaaskvbnukyhj7eyremko5qka=='	get_monitor_secret def get_monitor_secret():
# todo: remove this function </s> logger.warning((	__getitem__ def __getitem__(self, key): DEPRECATED: please use ``Inventory.get_host`` instead. 'Use of Inventory[<host_name>] is deprecated, ' 'please use `Inventory.get_host` instead.'
#todo: can't initialize these values in the __init__? </s> self.done       = false	run_unsafe def run_unsafe(self): self.sockets    = [] if self.gevent_needed:
# todo: this check is to maintain backwards compatibility with the old way of creating </s> if hasattr(self, 'package_form'):	new vars = {'data': data, 'errors': errors, 'error_summary': error_summary} self._setup_template_variables(context, {'id': id}) c.form = render(self.package_form, extra_vars=vars) else:
# todo: this doesn't seem necessary; test passes without it </s> time.sleep(1)	test_watch_file assert watcher.is_changed(filepath) assert watcher.is_changed(filepath) is False with open(filepath, 'w') as f: f.write('')
# todo(pvp): this and other input_ids i tried for generation give pretty bad results. not sure why. model might just not be made for auto-regressive inference </s> output_ids = model.generate(input_ids, do_sample=false)	test_lm_generate_xlm_mlm_en_2048 447, ]  # the president the president the president the president the president the president the president the president the president the president self.assertListEqual(output_ids[0].numpy().tolist(), expected_output_ids)
# todo: better error reporting </s> if req.error:	GetResults results[name] = serializer.LoadJson(req.resp_body) continue msg = req.error else:
# todo: replace with "yield from" when dropping python 2. </s> for stream in streams.items():	_get_smil_streams url = "{0}/{1}{2}".format(smil["base"], video, HDCORE_PARAMETER) streams = HDSStream.parse_manifest(self.session, url, pvswf=SWF_URL) yield stream
# todo: use flask logger without it triggering the root </s> __name__: {	init_logging }, "loggers": { "handlers": [], "level": os.getenv("ORCHEST_LOG_LEVEL", "INFO"),
# todo: refactor </s> aggregrate = {}	_executor_internal return self._executor_internal_inner(host) else: all_comm_ok = True all_changed = False
# todo: refactor </s> if 10 > micro:	automatically major, minor, micro = [ int(i) for i in version.split('.')] micro += 1 elif 10 == micro:
# todo: remove "get_" from the name </s> creates a bfs sampling tree as an adjacency list from head node types.	get_type_adjacency_list def get_type_adjacency_list(self, head_node_types, n_hops): Each list element is a tuple of: (node_type, [child_1, child_2, ...])
# domain=domain,  # todo: why isn't domain being saved on commcarecaseindexsql objects? </s> case_id__in=case_ids	get_indexed_case_ids Given a base list of case ids, gets all ids of cases they reference (parent and host cases) query = CommCareCaseIndexSQL.objects.filter( ) return list(set(query.values_list('referenced_id', flat=True)))
raise exceptions.mpdnotimplemented  # todo </s> underscore, dash, dot and colon.	subscribe already. The name may consist of alphanumeric ASCII characters plus
# todo include results from results.albums(), etc. too </s> tracks = [	callback def callback(results, userdata=None): translator.to_mopidy_track(t) for t in results.tracks()] future.set(tracks)
# todo: this is incomplete. </s> elif page_is_empty and new_position_y > max_position_y:	_linebox_layout stop = True break new_position_y -= box.margin_top line.translate(0, -box.margin_top)
# todo implement. </s> yaml = self.master_model.to_yaml()	_train def _train(self, rdd, nb_epoch, batch_size, verbose, validation_split):
# todo come up with a better error reporting mechanism so that we don't need this as a special case. </s> if isinstance(state.exc, resolveerror):	_index for node, state in roots.items(): if type(state) is not Return: raise AddressLookupError(str(state.exc)) else:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo only ubuntu 12.04 is supported right now </s> if (distro != 'ubuntu'	purge lsb_release_r = sudo.compile(lsb_release) (distro, codename) = lsb_release_r() or codename != 'precise'): raise exc.UnsupportedPlatform(distro=distro, codename=codename)
# todo(amotoki): due to neutron bug 1378525, neutron disables </s> api.neutron.router_update(isa(http.httprequest), router.id,	test_router_update_post_dvr_ha_enabled "dvr", "update")\ .AndReturn(True) name=router.name, admin_state_up=router.admin_state_up,
# todo:check the note in docstring. change that behavior to return the joint map </s> final_distribution = self._variable_elimination(variables, 'marginalize',	VariableElimination >>> inference = VariableElimination(model) >>> phi_query = inference.map_query(['A', 'B']) evidence=evidence, elimination_order=elimination_order)
# todo: remove when support for django 1.4 is dropped </s> from django.db.models.fields import field	patch_db_field_compare object it's compared to isn't a Field instance. Let's monkey patch it! see https://code.djangoproject.com/ticket/17851 try: assert Field() != None
#todo handle partial datetime values </s> if datetime.datetime(*item[field][:7]) < datetime.datetime.strptime(value, date_format):	_rulepass return True if op == "before": return True return False
#todo - once we drop support for python 2.3, this helper function can be </s> seqio.write(_multiple_alignment_record_iterator(alignments), handle, format)	write writer_class(handle).write_file(alignments) elif format in SeqIO._FormatToWriter : elif format in _FormatToIterator or format in SeqIO._FormatToIterator : raise ValueError("Reading format '%s' is supported, but not writing" % format)
# todo: support domain-specific settings </s> backend_mapping = sorted(settings.sms_backends.iteritems(),	get_outbound_sms_backend Get the appropriate outbound SMS backend to send to a particular phone_number key=lambda (prefix, backend): len(prefix), reverse=True)
### todo: code this! </s> pass	_handle_option_tag_outside_form def _handle_option_tag_outside_form(self, tag, attrs):
# todo: speed up </s> pos = exportvertexarray[vertexindex].position	DeindexMesh faceIndex = 0 for face in mesh.tessfaces: tc = exportVertexArray[vertexIndex].texcoord0 v0 = Vector((pos[0], pos[1], pos[2]))
# todo: clean up </s> for _ in range(num_hosts)	gossipsubs )
# todo(yanase): remove number from system_attrs after adding trialmodel.number. </s> assert system_attrs == {'baseline_score': 0.001, 'number': 1}	test_set_and_get_tiral_system_attr check_set_and_get(trial_id_2, 'baseline_score', 0.001) system_attrs = storage.get_trial(trial_id_2).system_attrs
# todo(nateh): fix for arch's multi-file net config </s> return false	_bring_up_interface util.logexc(LOG, "Running interface command %s failed", cmd)
# todo check error message </s> assert app_is_not_installed("whatever.nope", "legacy_app")	test_legacy_app_install_unknown_domain with pytest.raises(YunohostError): install_legacy_app("whatever.nope", "/legacy")
# todo: handle multiple credentials (username/password pairs). </s> for user in local_users:	authenticate_using_local_database except Exception: local_groups = [] if credentials['user'][0] == user['username']: if credentials['password'][0] == user['password']:
# todo: check what kind of exception raising if no location </s> self.birthday = html.find('.profileheadercard-birthdatetext')[0].text	__parse_profile pass self.location = html.find('.ProfileHeaderCard-locationText')[0].text if self.birthday: self.birthday = self.birthday.replace('Born ', '')
# todo: shouldn't this be none? </s> return tilelayer()	_default_right_layer @default('right_layer') def _default_right_layer(self):
# todo: this scrolling is lame and centers text :/ </s> view.show(size)	chat view.insert(ed, size, str(envelope)) view.set_read_only(True)
# todo consolidate </s> if color_scheme_name == 'default':	loadini 'prompt_more': 'g', } struct.color_scheme = default_colors else:
return none # todo </s> def _currentsong(self):	_currentsong
# todo: improve this. </s> self.view.sel().clear()	_vi_redo for i in range(state.count): self.view.run_command('redo') for s in old_sels: self.view.sel().add(s)
# todo: verify the choice between failover and migration </s> qa_utils.runinstancecheck(instance, true)	TestInstanceMigrate instance["name"]]) AssertCommand(cmd) if toggle_always_failover: AssertCommand(["gnt-instance", "modify", "-B",
# todo: respond with "user has been added to the list?" </s> async def handle_blacklist(self, message, username):	MusicBot self.whitelist.add(str(user_id)) write_file('whitelist.txt', self.whitelist) Usage: {command_prefix}blacklist @UserName Adds the user to the blacklist, forbidding them from using bot commands.
# todo: remove this compatibility layer with rally 1.1.1 </s> idx = self.index_name()	_store def _store(self, doc): self.client.put_template("rally-races", self.index_template_provider.races_template()) if self.client.exists(idx): index_info = self.client.get_index(idx)
#todo: check if all selected objects are on visible layers (option bpy.ops.object.select_all()?) </s> if bpy.data.worlds[0].relativepath:	execute def execute(self, context): path = securepath(os.path.expanduser(os.path.join(bpy.path.abspath("//"), bpy.context.scene.world.path))) else:
# todo: this here always returns empty response. if/when we want to </s> args = []	mock_etherscan_balances_query output_types = get_abi_output_types(fn_abi) decoded_input = web3.codec.decode_abi(input_types, bytes.fromhex(data[10:])) result = '0x' + web3.codec.encode_abi(output_types, [args]).hex() response = f'{{"jsonrpc":"2.0","id":1,"result":"{result}"}}'
#todo: this is a horrible thing to do, we consume lots of memory </s> for fr in set(self._fuzzable_request_set):	_audit self._w3af_core.status.set_running_plugin( plugin.getName() ) self._auth_login() try: self._w3af_core.status.set_current_fuzzable_request( fr )
raise exception('lol') #todo fixme </s> project_dir = project_dir[:-1] if project_dir.endswith('/') else project_dir # strip trailing slash	_get_project_name def _get_project_name(project_dir): if not project_dir: project_name = os.path.split(project_dir)[-1] return project_name
# todo: add possible tags </s> )	get_arguments help="path where to write the downloaded track to, special tags " "are to be surrounded by curly braces. Possible tags: " parser.add_argument( "--trim-silence",
# todo: consolidate these trivial group by dispatched funcs </s> groupings = __data.grouper.groupings	_extract_gdf @extract.register(DataFrameGroupBy) def _extract_gdf(__data, *args, **kwargs): df = __data.obj f_extract = extract.registry[pd.DataFrame]
# todo: this is a very flaky assumption. find a better one. </s> current_timestamp = timelib.timestamp.getnow()	VerifyFile if timestamp <= 0: return False if timestamp > current_timestamp + self._SIX_YEARS_IN_MICRO_SECONDS: return False
# todo: fails because of missing svg support </s> assert_pixels('inline_image_' + filename, 8, 8, image, '''	test_images )) def test_images(filename, image): <style> @page { size: 8px }
# todo: handle overwrite case </s> sftp.get(source.path, target.path)	append_sshcsv_to_csv ssh = source.connect() sftp = ssh.open_sftp() return target
# todo(vish): move this into the driver layer </s> yield process.simple_execute(	attach_volume volume_ref = db.volume_get(context, volume_id) yield self._init_aoe() "sudo virsh attach-disk %s /dev/etherd/%s %s" % (instance_id,
# todo: remove once multiple meanings in vocabulary are </s> cards = card_type.create_sister_cards(fact)	create_new_cards if answer == 2:  # Don't add. return db.add_fact(fact) for card in cards:
# todo: this class is incorrect: buildbot.slave.bot.slavebuilder </s> i am called by the worker's l{buildbot.slave.bot.slavebuilder} so	remote_update def remote_update(self, updates): I can receive updates from the running remote command. @type  updates: list of [object, int]
# todo test for final </s> args = {	iframe def iframe(self, link_text = None, width = "600px", height = "300px"): 'url' : self.url(), 'hyperlink' : self.hyperlink(link_text),
# todo why not just expect *only* the attribute value response, </s> if matched_packet_type != eventpackettype.attclient_attribute_value:	char_read_handle EventPacketType.attclient_procedure_completed], timeout=timeout) raise BGAPIError("Unable to read characteristic") if response['atthandle'] == handle:
# todo: need to add counter </s> return true	send_message delay.small_delay(self) if super(self.__class__, self).sendDirectItem('message', user_ids, text=text, thread=thread_id): self.logger.info("Message to {user_ids} wasn't sended".format(user_ids=user_ids)) return False
# todo - use new error message api! ts </s> self.showerrormessage(m.message(str(mymessage)))	accept except (KeywordDbError, Exception), e: myMessage = getExceptionWithStacktrace(e, theHtml=True) self.hideBusy() return
# todo(solitude): remove this. </s> try:	issue_refund results = response['response'] else: results = paypal.refund(contribution.paykey) except PaypalError, e:
raise skiptest("buggy")  # todo(mattjj): fix </s> n = 4 * xla_bridge.device_count()	testGradOfSoftPmap def testGradOfSoftPmap(self): @partial(soft_pmap, axis_name='i') def f(x):
# todo(guillermooo): generalize class so it can run any polymer command. </s> def run(self):	GeneratePolymerElementCommand class GeneratePolymerElementCommand(sublime_plugin.WindowCommand): pub run polymer:new_element v = self.window.active_view() project = DartProject.from_path(v.file_name())
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: this remembers every file user ever saw in nautilus. </s> self.files.add(path)	update_file_info else: file.add_emblem("syncthing-offline")
# todo: we should store a storage version number in later releases. </s> if isinstance(state, heteropicklestates):	HeteroGraphIndex def __setstate__(self, state): self._cache = {} self.__init_handle_by_constructor__(_CAPI_DGLHeteroUnpickle, state) elif isinstance(state, tuple) and len(state) == 3:
# todo: check that path combined with uri does not go above </s> return '..' not in self.token and '/' not in self.token	good_token characters", but it should also warrant that it doesn't contain ".." or "/"...
# see optimization description comments and todo for tags in matching public histories query. </s> return trans.sa_session.query(self.model_class).join("user").options(eagerload("user").load_only("username"), eagerload("annotations"), undefer("average_rating"))	build_initial_query def build_initial_query(self, trans, **kwargs):
#todo: overly broad exception needs fixing </s> except exception as exc:	get_tags results = elastic.search(query, index=index, doc_type='_all') tags = results['aggregations']['tag_cloud']['buckets'] tags = [] return tags
# todo: what are these doing here? </s> from corehq.apps.hqwebapp.templatetags.proptable_tags import get_display_data	render_case_hierarchy @register.simple_tag def render_case_hierarchy(case, options): case = wrapped_case(case) get_case_url = options.get('get_case_url')
# todo: check if this is a windows symbol requirement, otherwise ignore it </s> if not requirement.validate(context, config_path):	recurse_symbol_requirements results = [] if isinstance(requirement, interfaces.configuration.SymbolRequirement): results.append((config_path, sub_config_path, requirement)) else:
singleton=false,  # todo: re-enable </s> )	pillars tag='pillar', pack=pack, return FilterDictWrapper(ret, '.ext_pillar')
# todo: does it get closed properly after process gets killed? </s> self._client.connect(hostname=host, username=user, password=password)	__init__ self._client = SSHClient() self._client.load_system_host_keys() self._cwd = cwd super().__init__(mp_executable, api_stubs_path, cwd=cwd)
# todo: this should be abstracted into a property/method or something </s> if region.inherited and not contents and hasattr(obj, 'parent_id') and obj.parent_id:	collect_items def collect_items(obj, region): contents = obj._content_for_region(region) return collect_items(obj.parent) return contents
# @todo: catch warnings </s> self.httplib_object.verify = false	test_setup_ca_cert def test_setup_ca_cert(self): self.httplib_object._setup_ca_cert() self.assertEqual(self.httplib_object.ca_cert, None)
# todo: fix </s> 5,	optimization request_json.start_date, request_json.finish_date, 0, request_json.export_csv,
# todo use a proper category instead </s> "search": "python_en",	_wordpress_get_pages "status": "publish", "number": number, "offset": offset}, headers=_wordpress_headers())
# todo(b/157460932): migrate to glaziererror </s> terminator.log_and_exit('unable to remove task list', self._build_info,	_SetupTaskList os.remove(location) except OSError as e: 4303, e) return location
pass # todo </s> def _previous(self):	_previous @register(r'^previous$')
#todo: should raise an exception or warning </s> self._read_only = true	Annotations input_files = [sugg_path] if not access(sugg_path, W_OK): else: input_files = [sugg_path for sugg_path in
# todo does not work after multiprocessing branch merge </s> 'uptime': 0, # self.session.stats_uptime(),	_status_stats 'albums': 0, # TODO 'songs': 0, # TODO 'db_playtime': 0, # TODO 'db_update': 0, # TODO
# todo(b/182316162): unify publisher handing so that post-execution artifact </s> outputs_utils.tag_output_artifacts_with_version(result.output_artifacts)	_publish_execution_results publish_params = dict(output_artifacts=task.output_artifacts) if result.output_artifacts is not None: publish_params['output_artifacts'] = result.output_artifacts elif result.executor_output is not None:
# todo: optimise this to move some of the work to the workers. </s> room_id = receipt["room_id"]	_push_remotes poked and pokes them. try: receipt_type = receipt["receipt_type"] user_id = receipt["user_id"]
# todo implement this </s> return notimplementederror	get_app_path if self.app_path is not None: return self.app_path from droidbot import DroidBot out_dir = DroidBot.get_instance().options.output_dir
# todo is_compiled_with_cuda() has not been moved </s> compiled_with_cuda = paddle.fluid.is_compiled_with_cuda()	get_environ_info pass env_info['Python'] = sys.version.replace('\n', '') env_info['Paddle compiled with cuda'] = compiled_with_cuda if compiled_with_cuda:
# todo: look at this </s> resp = n.name_query_request(self.servername, self.machine)	test_name_query_request def test_name_query_request(self): n = nmb.NetBIOS() print resp.entries
# (todo) chagne the dgl link </s> if osp.isdir(self.folder) and (not osp.exists(osp.join(self.folder, f'release_v{self.version}.txt'))):	__init__ self.version = 1 self.url = f'http://ogb-data.stanford.edu/data/lsc/pcqm4m-v2.zip' print('PCQM4Mv2 dataset has been updated.') if input('Will you update the dataset now? (y/N)\n').lower() == 'y':
# todo: create a hard bounce receipt rule in ses </s> except deletedaddresses.doesnotexist:	_sns_message address_hash=local_portion_hash ) logger.error( 'Received email for unknown address.',
# todo: finish this </s> return dct.to_xml()	msnplus_to_list dct = Plus.msnplus(txt)
#! todo: this is currently limited to siso systems </s> nout, nin = self.d.shape	freqresp def freqresp(self, omega=None): den = sp.poly1d(sp.poly(self.A)) num = sp.poly1d(\
# resume normal sphinx.ext.autodoc operation </s> return super(functiondocumenter, self).format_name()	format_name Format the function name if not hasattr(self.module, '__func_alias__'): if not self.objpath: return super(FunctionDocumenter, self).format_name()
# todo: rewrite this simpler, we're using less than written </s> perform first and second level decoding of name as specified in rfc 1001 (section 4)	decode_name def decode_name(name): :param binary name: the name to dencode :return string: the decoded name.
# todo add help_text and label when they are available </s> 'read_only': false,	test_options_root_view expected['actions']['POST'] = { 'text': { 'required': True, 'type': 'Single Character',
# todo get fileid from event </s> filemodel = file()	downloadFileCompleteEvent def downloadFileCompleteEvent(event): file = fileModel.load(event.info['id'], force=True) downloadFileComplete(file['_id'])
# todo: decide what we consider to be a trending pack, for now we just take the last 9 that were updated </s> cache.set("trending_pack_ids", trending_pack_ids, cache_time)	handle .order_by('-last_updated').values_list('id', flat=True)[:9])
"""todo: to be implemented""" </s> def onsubmitsmresp(self, sequence_number, message_id, command_status):	onSubmitSMResp
# todo: consider revising to use new disk.target_name property. </s> try:	_role_filter_disk_name :param request: :return: by-id disk name (without path) post role filter processing disk_name = disk.name if disk.role is not None:
''' todo: change conditional to return on non-http responses </s> to reduce branch depth'''	getCDXJLinesFromFile showProgress(msg, recordsProcessed, recordCount) recordsProcessed += 1 if entry.record.rec_type != 'response' or \ entry.get('mime') in ('text/dns', 'text/whois'):
# todo(yanase): remove number from system_attrs after adding trialmodel.number. </s> assert trials[0].system_attrs == {'number': 0}	test_create_new_trial_id assert trials[0].state == TrialState.RUNNING assert trials[0].user_attrs == {}
#todo: the static files should not run everything on __init__ </s> self._static = none	_rebuild self.listener.pause() try: self.build() except Exception, e:
# todo project_id = 'your google cloud project id' </s> client = asset_v1beta1.assetserviceclient()	export_assets from google.cloud import asset_v1beta1 from google.cloud.asset_v1beta1.proto import asset_service_pb2 parent = client.project_path(project_id) output_config = asset_service_pb2.OutputConfig()
pass # todo </s> def _search(self, type, what):	_search @register(r'^search (?P<type>(album|artist|filename|title)) (?P<what>.+)$')
pass  # todo </s> def create(self, name):	create
# todo: only decrypt metadata if header is present </s> meta_raw = b64decode(bkey.get_metadata('meta'))	lookup_key if bkey is None: raise KeyError('Key does not exist: %s' % key) encrypted = bkey.get_metadata('encrypted') == 'True' if encrypted and not self.passphrase:
# todo: solution is not really elegant. should find </s> if update_var.name.startswith(('weight', 'bias')):	init_layer_update zero_weight_square = self.zero_weight ** 2 for update_var, update_func in updates: update_func -= decay_koef * ( (2 * update_var / zero_weight_square) / (
extruder_stack.userchanges.setproperty(key, "value", new_value)  # todo: nested property access, should be improved </s> if extruder_stack != self._active_container_stack and extruder_stack.getproperty(key, "value") != new_value:	copyValueToExtruders for extruder_stack in extruder_stacks:
# todo hard-disable tests for now, since rapidcheck not in spack </s> "-dlazyten_enable_tests=off",	cmake_args "-DBUILD_SHARED_LIBS=" + str("+shared" in spec), "-DDRB_MACHINE_SPECIFIC_OPTIM_Release=ON",  # Adds -march=native "-DLAZYTEN_ENABLE_EXAMPLES=" + str("+examples" in spec), ]
# todo implement callback </s> if callback is not none:	write except IOError as e: log.error("Failed to create a new file!", e) pass if len(self.files.get_data_files()) == self.settings.get_max_files_count():
# todo: this only works on local files !!! </s> return os.path.isfile(safe_join(filepath, filename))	zip_existing filepath = self._download_path(app) filename=self.download_name(app, ty)
# todo: duplicate checking </s> self._associate.append((association_type, value))	add def add(self, association_type: LicenseAssociationType, value: str):
# todo check if we can avoid that </s> out = numpy.zeros(nhex)	read_buffer out = numpy.fromfile( f, count=nhex * 8, dtype=int, sep=" ").reshape(nhex,8) cells["hexahedron"] = out - 1 cell_data["hexahedron"] = {"ugrid:ref": out} return Mesh(points, cells, cell_data=cell_data)
# @todo: the values must be in cgs already right? </s> sg.attrs["field_to_cgs"] = 1.0	write_to_gdf else: sg.attrs["field_units"] = "None" sg.attrs["staggering"] = 0 g = f.create_group("particle_types")
output_zero_point = none # todo non-zero zero point </s> output = quanttensor(	__truediv__ output_zero_point = self.zero_point / other.zero_point else: value=output_tensor, scale=output_scale,
# todo: before merge should discuss with syft core team on max time. </s> ctr = 3000	process func = _MAP_ACTION_TO_FUNCTION[msg.name_action] try: while True: store_object_self = node.store.get_object(key=msg.self_id)
# todo: turn this into a runtime error </s> logger.warning("attempted to updated plugins when registry is initialized")	enable_new_default_plugins from kolibri.plugins.registry import is_initialized if is_initialized(): changed = False for module_path in DEFAULT_PLUGINS:
# todo: we currently lack a reference family that passes this test! </s> ttfont = ttfont("data/test/mada/mada-medium.ttf")	test_id_064 from fontbakery.specifications.googlefonts import (com_google_fonts_test_064 as test, ligatures) lig = ligatures(ttFont) print ("Test WARN with a bad font...")
file_object = open(file_path, mode='r') #todo add  encoding='utf8' for version python3 </s> lines = file_object.readlines()	load_label_dict_accu :param file_path: :return: a dict, named:label2index_dict label2index_dict = {} for i, label in enumerate(lines):
# todo: read this parameter from the command line by implementing make_cmdline_parser and parse_known_cmdline_args! </s> if self._gui is not none:	getSelectedExportPluginName def getSelectedExportPluginName(self): return self._gui.selectedPlugin
#@todo: move this and other methods out of this file, into a general </s> from utilities.prefs_constants import dnabaseindicatorsangle_prefs_key	get_dna_base_orientation_indicator_dict Returns two  dictionaries for DNA bases perpendicular and anti-perpendicular to a plane specified by the plane normal vector. from utilities.prefs_constants import dnaBaseIndicatorsDistance_prefs_key indicators_angle = env.prefs[dnaBaseIndicatorsAngle_prefs_key]
# todo ensure that if you try to filter on an invalid field, it returns a useful error. </s> return_val = [item for item in default_queryset if value in item.get(field_name, none)]	get_serializer_field_value return_val = [item for item in default_queryset if item.get(field_name, None) == value] else: return return_val
# todo: return errors in a universal way </s> print("shivyc: error: no such file or directory: '{}'"	main c_file = open(arguments.file_name) except IOError: .format(arguments.file_name)) return
if lang is none:  # todo: remove in v8 </s> logger.warn("slugify() called without language!")	slugify >>> print(slugify('foo bar', lang='en')) foo-bar if not isinstance(value, unicode_str): raise ValueError("Not a unicode object: {0}".format(value))
# todo: force this somehow so that this isn't just a warning but </s> url_object_list.extend( new_list )	_update_URLs_in_KB new_list = [ fr.getURL() for fr in fuzzable_request_list \ if fr.getURL() not in url_object_list ] kb.kb.save( 'urls', 'url_objects' ,  url_object_list )
# todo: not all values have exact matches in flexget, need to update flexget qualities </s> sources = {'br-disk': 'remux',  # not a perfect match, but as close as currently possible	quality_requirement_builder def quality_requirement_builder(self, quality_profile): Converts CP's quality profile to a format that can be converted to FlexGet QualityRequirement 'brrip': 'bluray', 'dvdr': 'dvdrip',  # Not a perfect match, but as close as currently possible
# todo: only handle events that are new. </s> for e in events:	_watch_slack_rtm events = self.client.rtm_read() if len(events) > 0: self.handle_incoming_event(e) current_poll_count += 1
# todo: skipped due to gh-4436 </s> yield skip_if_on_windows(skip_ssh(_test_bare_git_version_2)), 'datalad-test'	test_bare_git_version_2 def test_bare_git_version_2(): yield _test_bare_git_version_2, None
# todo: if table_name is "2019" the final name will be "field_2019" - must </s> inputs = [pathlib.path(filename) for filename in sources]	command_csv_to_sqlite batch_size, samples, input_encoding, dialect, schemas, sources, output ): output = pathlib.Path(output) table_names = make_header([filename.name.split(".")[0] for filename in inputs])
raise notimplementederror # todo </s> def generate(self):	generate
# todo: delete </s> test_filetype_to_instance(".asc", can.ascreader)	testPlayerTypeResolution with can.LogReader(filename) as reader: self.assertIsInstance(reader, klass) test_filetype_to_instance(".blf", can.BLFReader) test_filetype_to_instance(".csv", can.CSVReader)
# todo: does this import need to be delayed because </s> from pyomo.solvers.plugins.solvers.persistent_solver import \	solve exception_on_failure=True, io_options=None): PersistentSolver if self.instance is None:
pass  # todo... </s> beam_out[0] = beam	perform beam = beam_trans.transpose(*map(array_trans_dims_order.index, range(array.ndim))) if self.wrap_mode == "pad_zero":
# todo this seems not to be very convenient... </s> for key, classdef in inspect.getmembers(sys.modules[__name__], inspect.isclass):	register print("Registering operators.editing...") bpy.utils.register_class(DynamicProperty) bpy.utils.register_class(classdef)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
pass # todo </s> def _make_entries(self):	_make_entries
# todo(harlowja): should we be a little more cautious about </s> details = flow_details[name][0]	_task_result_fetcher name = task_and_state(task, s) if name in flow_details: if details.metadata and 'result' in details.metadata: return (True, s == states.FAILURE,
# todo: assert </s> repo = self.remote.get_repo("testrepo0")	test_get_repo Test: Get a repo object
#todo need gutter of scrollbar - how do we get that? </s> if event.button == 1:	on_diffmap_button_press_event def on_diffmap_button_press_event(self, area, event): size_of_arrow = 14 diffmapindex = self.diffmap.index(area)
# todo: this should be made more flexibly to handle differeing params for xform submission </s> try:	get_instance_and_attachment attachments = {} if request.META['CONTENT_TYPE'].startswith('multipart/form-data'): instance = request.FILES[MAGIC_PROPERTY].read() except MultiValueDictKeyError:
# todo: this currently looks only in current table; </s> spans = [c] if isinstance(c, temporaryspan) else c.get_arguments()	get_horz_aligned_ngrams def get_horz_aligned_ngrams(c, attrib='words', n_min=1, n_max=1, lower=True): for span in spans: if span.sentence.table is None: continue
# todo(wentingli): create manifest from dependency jars later if needed </s> contents = 'manifest-version: 1.0\ncreated-by: python.zipfile (blade)\n'	generate_fat_jar print >>sys.stdout, log print >>sys.stderr, '\n'.join(zip_path_logs) contents += '\n' target_fat_jar.writestr(_JAR_MANIFEST, contents)
# todo(b/142684737): the multi-processing api might change. </s> beam_pipeline_args=['--direct_num_workers=%d' % direct_num_workers])	_create_pipeline metadata_connection_config=metadata.sqlite_metadata_connection_config( metadata_path),
# todo: for now, circumnavigate the detached head issue. </s> for subds in source.get_dataset_handles(recursive=true):	test_publish_simple def test_publish_simple(origin, src_path, dst_path): source = install(path=src_path, source=origin, recursive=True) AnnexRepo(opj(src_path, subds), init=True, create=False).git_checkout("master") target = GitRepo(dst_path, create=True)
# todo: --bytype </s> return res + tr_hline	tabulate 100 * files / max(1, stats_tot["files"])).replace('100.0', ' 100')) \ + '\n'
node.test = gast.call(gast.attribute( # todo any over dim 0 </s> test, gast.name('any', gast.load(), none), none), [], [])	visit_While raise NotImplementedError("cannot process while-else") test = node.test return self.visit_loop(node, test)
# todo: slow </s> v0 = vector((posa[i0 * 3 + 0], posa[i0 * 3 + 1], posa[i0 * 3 + 2]))	ExportGeometry i1 = ia[i * 3 + 1] i2 = ia[i * 3 + 2] v1 = Vector((posa[i1 * 3 + 0], posa[i1 * 3 + 1], posa[i1 * 3 + 2])) v2 = Vector((posa[i2 * 3 + 0], posa[i2 * 3 + 1], posa[i2 * 3 + 2]))
# todo: raise a specific exception for invalid separator characters </s> assert not value or value in settings.sep_chars	sep @auto_save def sep(self, value): self._data['sep'] = value.strip()
# todo: smart profiling for parameter back-time (if it set up to 'auto') </s> def connect(self):	GraphiteClient self.config = config self._result_fields = config.get('metrics', ValueError("Metrics list required")) pass def start(self):
# todo obtain this from entitlements </s> team_id = "jwkxd469l2"	resign_cons signer_key_file, cert_file): make_entitlements_data(codesig_cons, entitlements_file) make_requirements_data(codesig_cons, signer_key_file)
# todo -- can we do this without a subscription? </s> if not api.use_store:	play_similar_song_radio @ask.intent("GeeMusicPlaySimilarSongsRadioIntent") def play_similar_song_radio(): return statement(render_template("not_supported_without_store")) if len(queue.song_ids) == 0:
# todo: fails for rsa256_key </s> csr_pem, _ = self._call(	test_must_staple def test_must_staple(self): RSA512_KEY, ['example.com', 'www.example.com'], must_staple=True) csr = OpenSSL.crypto.load_certificate_request(
# python3-todo: use yield from </s> for tmp in supergreedy_rec(h, linext+[e]):	supergreedy_rec k -= 1 for e in S: yield tmp
# todo: is this a public attribute? </s> self._registeroldworkerattr("localworkerfactory", pattern="buildworker")	checkConfig Worker.checkConfig(self, name, None, **kwargs) self.LocalWorkerFactory = None try: from buildslave.bot import LocalBuildSlave as RemoteLocalWorker
# todo add list as an option </s> if ia.is_single_number(p_drop):	_convert_p_drop_to_inverted_mask_param @classmethod def _convert_p_drop_to_inverted_mask_param(cls, p_drop): p_drop = iap.Binomial(1 - p_drop) elif ia.is_iterable(p_drop):
# todo (a8): add user to models </s> setattr(result, 'user', none)	obj_get result.branch = result.revision.branch setattr(result, 'result', result) return result
f.write(self.pastie_content.encode('utf8'))  # todo error checking </s> f = open(directory + os.sep + self.id, 'w')	savePastie raise SystemExit('BUG: Content not set, sannot save')
# todo: cache this result so multiple failing calls don't keep hitting the db </s> return none	sql_location self._sql_location = SQLLocation.objects.get(domain=self.domain, location_id=self.location_id) except ObjectDoesNotExist: return self._sql_location
# todo: prefer to enumerate specific </s> "s3:getobject*",	grant_read_permissions_to "Effect": "Allow", "Action": [ "s3:GetBucket*", "s3:List*",
raise notimplementederror #todo </s> elif q.kw(i,"format"):	parse while i < l: if q.kw(i,"RETURN"): raise NotImplementedError #TODO elif q.kw(i,"REQUEST"):
# todo: switch _ignore_connection_reset for _ignore_transmission_error, or provide retry mechanism </s> if self._ignore_connection_reset:	transmit self.last_send = data except sex.BoofuzzTargetConnectionReset: self._fuzz_data_logger.log_info("Target connection reset.") else:
# todo: strip leading whitespace for ''' and r''' </s> if 0:	EvalSingleQuoted if part.left.id in (Id.Left_SingleQuote, Id.Left_RSingleQuote, Id.Left_TSingleQuote, Id.Left_RTSingleQuote): for t in part.tokens: log('sq tok %s', t)
# todo: support steps and times (motion blur) </s> print("dupli export took %.3fs" % (time() - start))	create_session transformations = array("f", duplis.matrices) luxcore_scene.DuplicateObject(src_name, dst_name, count, transformations) engine.update_progress(index / len_objs)
# todo(bowen): check </s> sys.stdout = old_stdout	nostdout finally:
# todo: purge expired tokens </s> log.audit('token "%s" has expired.' % env['http_x_auth_token'])	_validate_token token = Token.get(env['HTTP_X_AUTH_TOKEN']) if token.expiry <= isotime.add_utc_tz(datetime.datetime.utcnow()): raise exceptions.TokenExpiredError('Token has expired.') LOG.audit('Token "%s" is validated.' % env['HTTP_X_AUTH_TOKEN'])
# @todo: move this to a popup behind an action button, to make it clearer that this isn't a maintained link </s> stable = current.s3db.event_scenario	event_rheader editable = current.auth.s3_has_permission("UPDATE", "event_incident", record_id) if editable and r.method == "plan": query = (stable.incident_type_id == incident_type_id) & \ (stable.deleted == False)
# todo more through test </s> x = np.ones((2, 3))	test_iterator tests wether SparseDataset can be loaded and initialize iterator ds = SparseDataset(from_scipy_sparse_dataset = x) it = ds.iterator(mode = 'sequential', batch_size = 1)
# todo: implement this! </s> python = self._exec('which python')	_selectClient This method selects the w3afAgent client to use based on the remote OS and some other factors like having a working python installation. if python.startswith('/'): fileContent = file( 'core' + os.path.sep + 'controllers' + os.path.sep + 'w3afAgent' + os.path.sep +\
# todo: add at least reflection tests before adding notimplemented version </s> def mixrampdelay(context, seconds):	mixrampdelay *musicpd.org, playback section:* ``mixrampdelay {SECONDS}``
# todo(b/161952382): replace with keras premade models and </s> deep = tf.keras.layers.densefeatures(deep_columns)(input_layers)	_wide_and_deep_classifier colname in features.transformed_names(features.CATEGORICAL_FEATURE_KEYS) }) for numnodes in dnn_hidden_units: deep = tf.keras.layers.Dense(numnodes)(deep)
# todo(phawkins): remove this after a jaxlib release. </s> if not hasattr(lapack, "jax_getrf"):	testLu @jtu.skip_on_devices("gpu", "tpu") def testLu(self, shape, dtype, rng): self.skipTest("No LU implementation available") args_maker = lambda: [rng(shape, dtype)]
# todo: implement </s> return false, r_checkpoints, r_sum_checkpoints	_is_iterative_turning def _is_iterative_turning(num_proposals, inverse_mass_matrix, r, r_sum, r_checkpoints, r_sum_checkpoints):
# todo use deepcopy() here </s> return polygonsonimage(polys_cut_flat, shape=self.shape)	clip_out_of_image ] polys_cut_flat = [poly for poly_lst in polys_cut for poly in poly_lst]
# todo: explicitly commit files by name </s> youngest_ancestor = os.path.commonprefix(files)	add raise IOError("[BZR] add in '%s' failed: %s" \ % (self.location_abs, error)) return output + type(self)(youngest_ancestor).commit(message, author)
# todo add options to modify the columns </s> matrix.append([endpoint.machine.name, endpoint.state,	do_show if vlan.startswith('VLAN'): vlan.split('VLAN')[1] endpoint.endpoint_data['mac'], endpoint.endpoint_data['segment'],
# todo: make sure loop index is not used for calculations in </s> prepend = []	_run_parfor_1D_Var def _run_parfor_1D_Var(self, parfor, namevar_table): for l in parfor.loop_nests: if l.stop.name in self.oneDVar_len_vars:
pass ## fixme: todo </s> def test_date_ticker_one_month(self):	test_date_ticker_one_month
# todo merged db? </s> fill_influxdb()	main setup_logzero(logger)
# todo: --csr could have a priority, when --domains is </s> return "--domains and --csr are mutually exclusive"	obtaincert def obtaincert(args, config, plugins): if args.domains is not None and args.csr is not None: try: installer, authenticator = choose_configurator_plugins(args, config, plugins, "certonly")
# todo(chunla) move this to engine specific module </s> def _getcommonsysbenchoptions(benchmark_spec):	_GetCommonSysbenchOptions db = benchmark_spec.relational_db engine = sql_engine_utils.GetDbEngineType(FLAGS.managed_db_engine)
self.key_vb   = f.tanh(self.hid_2_key(hidden_vb)).view(-1, self.num_heads, self.mem_wid)    # todo: relu to bias the memory to store positive values ??? check again </s> self.beta_vb  = (1. + f.softplus(self.hid_2_beta(hidden_vb))).view(-1, self.num_heads, 1)   # beta >=1: https://github.com/deepmind/dnc/issues/9	forward def forward(self, hidden_vb, memory_vb): self.gate_vb  = F.sigmoid(self.hid_2_gate(hidden_vb)).view(-1, self.num_heads, 1)           # gate /in (0, 1): interpolation gate, blend wl_{t-1} & wc self.shift_vb = F.softmax(self.hid_2_shift(hidden_vb).view(-1, self.num_heads, self.num_allowed_shifts).transpose(0, 2)).transpose(0, 2)    # shift: /sum=1
# todo / fixme : here we are ignoring error messages ... </s> output, err = call.communicate()	is_vulnerable_to_meltdown stdout=subprocess.PIPE, stderr=subprocess.PIPE) assert call.returncode in (0, 2, 3), "Return code: %s" % call.returncode
# todo isoformat? </s> return pytz.utc.localize(datetime.strptime(s, '%y-%m-%dt%h:%m:%sz'))	_parse_dt def _parse_dt(s: str) -> datetime:
# todo: need to test this logic </s> field_renumbering = dict([f, i] for i, f in enumerate(field))	reconstruct index = Vbc.index cmpt = Vbc.component if index in field: if len(field) == 1:
logging.info(f'running command\n`{cmd}`') #todo: consider with ilya </s> return run(*args, **kwargs)	run_with_log cmd = args[0] if _is_verbose_flag_set():
# todo: arrange </s> repo = self.remote.new_repo(self.token)	test_create_repo def test_create_repo(self): Test: create/edit a repo object self.assertTrue(self.remote.modify_repo(repo, "name", "testrepo0", self.token)) self.assertTrue(self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token))
# todo: typing for pb </s> async def _write_pb(writer: asyncio.streamwriter, data_pb: any) -> none:	Client await handler(stream_info, reader, writer) @staticmethod data_bytes = serialize(data_pb) writer.write(data_bytes)
# todo speed! </s> ct = contenttype.objects.get_for_model(model)	has_model_list_permission def has_model_list_permission(user, model): qs = CategoryUserRole.objects.filter(user=user, group__permissions__content_type=ct) if qs.count():
# todo: see get_scale_factor() to choose 72 px on hidpi </s> if icon and icon.bind(interface.factory):	pixbuf def pixbuf(clazz, interface, icon): pixbuf = None pixbuf = getattr(icon.bind(interface.factory), 'native_%i' % clazz.icon_size()).get_pixbuf() return pixbuf
# todo: refactor accordingly when v3 websocket api is released </s> output["results"].update({	BittrexAPIOrderBookDataSource if _is_snapshot(msg): output["results"] = _decode_message(msg["R"]) "M": f"{output['results']['M'].split('-')[1]}-{output['results']['M'].split('-')[0]}" })
# todo: use stream_with_context instead of list </s> streamed_response = response(list(audit_iterator),	search delimiter = self.request_params.get('delimiter', ',') or ',' audit_iterator = CSVAuditIterator(audit_query, delimiter) content_type="text/csv") filename = "linotp-audit.csv"
# todo: if needed allow other handling (like adding values) </s> filt_pos = band[start:stop]	_put_filter fltr = fltr[:stop - len(bank)] stop = len(bank) np.maximum(fltr, filt_pos, out=filt_pos)
# todo. tune this "10" hyperparameter </s> loss = f.sigmoid(10 * (delta + self.margin_))	centroid_loss loss = torch.log1p(torch.exp(delta)) elif self.clamp == 'sigmoid': return loss
# todo(mgius): tests for views using this api call </s> tenants = auth_api().tenants.for_token(request.session['token'])	token_get_tenant def token_get_tenant(request, tenant_id): for t in tenants: if str(t.id) == str(tenant_id):
"""todo: doesn't remove unused nodes/renumber elements""" </s> x = self.xyz[:, 0]	slice_x def slice_x(self, xslice): self._slice_plane(x, xslice)
# todo device strategy in pytorch? </s> self.graph_builder.build_graph(	build for component in root_components: meta_graph = self.meta_graph_builder.build(component, input_spaces) meta_graph=meta_graph, input_spaces=input_spaces, available_devices=self.available_devices
# todo: rename dest to cron, since this does more than just quiet </s> namespace.loglevel = 'info'	CronAction setattr(namespace, self.dest, True)
# todo: replace use of request.forms with json </s> if is_admin(request.forms.get("sessionid", "")):	session_list def session_list(): List all active sessions return jsonize({"sessions": session_ids}) else:
# todo: improve logic to handle simple types like list of strings? </s> try:	parse_config_overrides else: value = args.pop(0) result[opt] = srsly.json_loads(value) except ValueError:
# todo: test for last revision on first page. </s> offset = url_for(controller='revision', action='list')	test_list_long self.create_100_revisions() try: res = self.app.get(offset) self.assert_click(res, '2', 'Revision 2')
# todo: remove them when the old workflow system will be </s> if (get_last_daemon_timestamp('workflow',when='stop')	daemon_stop dead = True print "AiiDA Daemon shut down correctly." -get_last_daemon_timestamp('workflow',when='start'))<timedelta(0): logger.info("Workflow stop timestamp was {}; re-initializing"
# todo: remove when we stop supporting python < 3.5 </s> if sys.version_info.major < 3 or sys.version_info.minor < 5:	get_mahalanobis_matrix M : `numpy.ndarray`, shape=(n_features, n_features) The copy of the learned Mahalanobis matrix. check_is_fitted(self, 'components_') else:
# todo: warn/error: check if this var has units: assigning </s> pass	_GeneralVarData % (type(val),)) if type(val) in native_numeric_types or val is None: else: if self.parent_component()._units is not None:
# todo: [phil] i think we could avoid this and use a bytes buffer in memory instead, zipfile supports it </s> f = tempfile.namedtemporaryfile(delete=false)	pull_embedded_pe_files Directly pull out any PE files embedded in the given data. if filetype.is_office2007_file(data, is_data=True): fname = f.name f.write(data)
# todo add arch arm/aarch/mips/mips64/sparc/sparc64 </s> raise exception("cannot find register '%s' for arch '%s'" % (reg, self.mode))	unicorn_register Architecture.X86_64_INTEL, Architecture.X86_64_ATT): return getattr(unicorn.x86_const, "UC_X86_REG_%s"%reg.upper())
for line in json_generator:     # todo: save file here as zip on cdn </s> print line	export_tasks print line json_generator = respond_json("task_run", app.id) def format_csv_properly(row, ty=None): tmp = row.keys()
"rulesactivated": false,  # todo for testing to be compatible with protocol 0.6 </s> }	_buildSensorAlertMessage "dataType": sensorAlert.dataType, "data": sensorAlert.sensorData, utcTimestamp = int(time.time()) message = {"serverTime": utcTimestamp,
# todo lauren: make shorter by pulling out lamda and then filtering </s> self.new_mentions = [m for m in self.new_mentions if m not in self.old_mentions and validate_contributor(m, self.node.contributors)]	edit self.date_modified = datetime.datetime.utcnow() if (self.new_mentions): if len(self.new_mentions) > 0: if save:
# todo: we really shouldn't be using socket.request.db at all, but </s> socket.request.db.close()	handle_message if not socket.terminated: socket.send(json.dumps(reply))
# todo: update this code to return statistics on deleted objects once we </s> try:	purge_executions if action_ref: liveaction_filters['action'] = action_ref ActionExecution.delete_by_query(**exec_filters) except InvalidQueryError as e:
raise deprecatedtest # this test is now broken. todo: fix it. </s> site = make_test_site()	test_simplest_view def test_simplest_view(): aliases = {'id_select': 'id', 'name_select': 'label', 'remote_select': 'remote.name'} req = normalize_request({'remote.id': None}, {'remote.id':10})
# todo: st api does not allow us to say "do not focus this new view" </s> filename = uri_to_filename(uri)	m_window_showDocument success(open_externally(uri, bool(params.get("takeFocus")))) else: selection = params.get("selection") open_file_and_center_async(self.window, filename, selection).then(lambda _: success(True))
# todo: confirmation in "r" mode </s> return none	result_from_prompt except Exception as e: if isinstance(e, EOFError): else: print(e)
# todo: non-numeric columns should be ignored automatically </s> def test_impl(n):	test_count1 def test_count1(self): df = pd.DataFrame({'A': np.arange(n)+1.0, 'B': np.arange(n)+1}) return df.count()
# todo: remove </s> print("\ndisabling gnome power profiles")	gnome_power_disable_live def gnome_power_disable_live(): if(gnome_power_stats != 0): call(["systemctl", "stop", "power-profiles-daemon"]) else:
# todo proper unit testing </s> expected_tte = expected_tte_d	test_censoring_funs def test_censoring_funs(): expected_is_censored = expected_is_censored_d times_to_event = padded_events_to_tte(discrete_events, discrete_time=True)
# todo: make idempotent </s> click.echo(f"creating dgraph cluster of {instance_type} instances")	create @pass_graplctl_state def create(graplctl_state: State, instance_type: InstanceTypeType) -> None: if not dgraph_ops.create_dgraph( graplctl_state=graplctl_state, instance_type=instance_type
# todo support domain delegation, which will allow us to set a sub-account to execute as. we can then </s> http = httplib2.http()	get_conn key, scope=self.scope) http_authorized = credentials.authorize(http) service = build('storage', 'v1', http=http_authorized)
# todo: migrate to glaziererror </s> _logfatal('failed to execute the task list', self._build_info, 4303, e)	RunBuild r.Start(task_list=task_list) except runner.ConfigRunnerError as e: except KeyboardInterrupt: logging.info('KeyboardInterrupt detected, exiting.')
# todo(mordred) add this back wnen ksa releases </s> self.assertnotin('links', host['flavor'])	_test_host_content self.assertEqual(host['image']['id'], self.image.id) self.assertNotIn('links', host['image']) self.assertNotIn('links', host) self.assertIsInstance(host['volumes'], list)
raise pathaccesserror()  # todo: path </s> if ret is _missing:	_eval ret = getattr(target, arg, _MISSING) except AttributeError: raise PathAccessError()  # TODO: path elif self.operation == '[':
# todo: make sure this works </s> @mention_added.connect	send_mention_added_notification def send_mention_added_notification(comment, auth): node = comment.node
##todo(ziad):we need to figure out how to auth to keystone </s> conn = http_connect(self.auth_host, self.auth_port, 'get',	_expound_claims "Accept": "application/json", "X-Auth-Token": self.admin_token} '/v2.0/tokens/%s' % claims, headers=headers, ssl=(self.auth_protocol == 'https'),
# todo: move to validator or helper </s> admins = [	manage_contributors users.append(user) self.contributors = users user for user in users if self.has_permission(user, 'admin')
# todo(developer): uncomment and set to a path to your audio file. </s> with open(speech_file, 'rb') as audio_file:	transcribe_file_with_word_level_confidence from google.cloud import speech_v1p1beta1 as speech client = speech.SpeechClient() content = audio_file.read() audio = speech.types.RecognitionAudio(content=content)
# todo: send alert </s> blacklist_whitelist_notification.delay(4) # notice_type = 4 whitelist	chk_prefix_in_whitelist break if flag: return True return False
# todo check for types that are not classes and add it to </s> exceptions = evaluator.eval_element(name.prev_sibling().prev_sibling())	_names_to_types types += imports.ImportWrapper(self._evaluator, name).follow() elif isinstance(typ, pr.TryStmt): types = list(chain.from_iterable( evaluator.execute(t) for t in exceptions))
observable_pars = np.abs(np.random.randn(observable_class.n_params)) #todo: some operations fails when parameters are negative (e.g. thermal state) but par_domain is not fine grained enough to capture this </s> gate_class(*gate_pars, list(range(gate_class.n_wires)))	circuit gate_class = getattr(qm, gate) gate_pars = np.abs(np.random.randn(gate_class.n_params)) return observable_class(*observable_pars, list(range(observable_class.n_wires)))
# todo: sumo has a single es_url and that's the zlb and does </s> es_deets = requests.get(settings.es_urls[0]).json()	es_status_cmd def es_status_cmd(checkindex=False, log=log): try: except requests.exceptions.RequestException: pass
# np.datetime64[ns] which invalid, todo: fix pa </s> if (rhs.attr == 'dtype' and (is_series_type(rhs_type)	_run_assign if (rhs.op == 'getattr'): rhs_type = self.typemap[rhs.value.name]  # get type of rhs value "S" or isinstance(rhs_type, types.Array)) and isinstance( rhs_type.dtype,
#todo(wwolf) get correct value for these </s> "updated": "2010-10-09t11:30:00z",	_versions_multi_choice "id": "v1.0", "status": "DEPRECATED", }, ]
raise notimplementederror # todo </s> a = self.trans_matrix	HSMMStatesPython def sample_forwards(self,betal,betastarl): if self.left_censoring: apmf = self.aD T, state_dim = betal.shape
raise exception('lol') #todo fixme </s> project_dir = project_dir[:-1] if project_dir.endswith('/') else project_dir # strip trailing slash	_get_project_name def _get_project_name(project_dir): if not project_dir: project_name = os.path.split(project_dir)[-1] return project_name
#todo _rule_for_parents needs to made into a generator </s> index for index in range(len(tail))]	add_edges self.node[head_node]['_rule_for_parents'] = [
# todo: fix this somehow? better use a helper func which goes over the structure. </s> ctypes.pointer(self.vars[name])[0] = value	getVar valueAst = getAstNode_newTypeInstance(self.interpreter, decl_type, bodyAst, bodyType) value = evalValueAst(self, valueAst, "<PyCParser_globalvar_" + name + "_init_value>") return self.vars[name]
# todo find out what is best used here! </s> 'preferred_dtype' : none}	get_meta_information 'is_deterministic': True, 'handles_sparse': True,
# todo(laigd): remove this check when 312743821 is in the release. </s> if use_tf_function and tf.compat.v1.__version__ < '2.3.0':	CallDefunTest ) def testSimple(self, use_tf_function): return FLAGS.call_defun_use_tf_function = use_tf_function
time.sleep(40)  # todo: should remove after polling get. </s> exp_res_1 = op(data_1, data_2)	test_tensor_abstraction_subsets time.sleep(40)  # TODO: should remove after polling get. mpc_1_2_3 = op(mpc_1_2, mpc_2_3) assert (mpc_1_2.reconstruct() == exp_res_1.child).all() exp_res_2 = op(data_2, data_3)
# todo: we want to create a state group for this set of events, to </s> prev_group = none	resolve_state_groups state_group = sg break delta_ids = None for old_group, old_ids in state_groups_ids.iteritems():
# todo: pandas returns dataframe, maybe return namedtuple instread of </s> s = "count    " + str(a_count) + "\n"\	_gen_col_describe q50 = hpat.hiframes_api.quantile(A, .5) q75 = hpat.hiframes_api.quantile(A, .75) "mean     " + str(a_mean) + "\n"\ "std      " + str(a_std) + "\n"\
# todo: uncomment once outstanding issues with this feature are addressed </s> handlers.celery_teardown_request()	test_unsubscribe_mailchimp_not_called_if_user_not_subscribed assert_equal(mock_client.lists.subscribe.call_count, 0)
# todo: write this </s> pass	test_bert def test_bert(self):
# todo: logging </s> return none	_get_cert cert_url = self._data.get('SigningCertURL') if not cert_url: try: import requests
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
# todo find commit hash </s> prefixes = {	index_repo path_str, line = position.rsplit(":", 1) path = Path(path_str) "nixpkgs": "https://github.com/nixos/nixpkgs/tree/master/", "nur": "https://github.com/nix-community/nur-combined/tree/master/",
# todo(twilson) we can remove this when we require ovs>=2.12.0 </s> super(metadataagentovnsbidl, self).__init__(	__init__ None, connection_string, helper, leader_only=False) except TypeError: None, connection_string, helper) if chassis:
# todo put this in a .extra w/a subselect </s> if not hasattr(self, '_hours_worked'):	hours_worked @property def hours_worked(self): self._hours_worked = Entry.objects.filter( user=self.contact.user,
#todo also check type!! </s> if all(matches_condition(node, c) for c in itertools.chain(indexed, unindexed)):	execute try: node = connections[using].nodes[int(id_val)] yield node except:
# todo: parse flags, error checking, etc. </s> dest_dir = argv[1]	_Cd def _Cd(self, argv): if dest_dir == '-': old = self.mem.Get('OLDPWD')
# todo: either fix this or remove this code </s> return false	_all_ratings_equal def _all_ratings_equal(self, tracks = None): Returns True if the rating of the tracks for this widget is equal if not tracks: tracks = self._get_tracks()
# todo: add tests for `in_rebase` true </s> self.assertequal(actual, expected)	test_format_branch_status_for_status_dashboard actual = git.get_branch_status(delim="\n")
# todo(stephenfin): the mock of 'migrate_disk_and_power_off' should </s> with mock.patch(	test_resize_bug_1879878 flavor_b_id = self._create_flavor( vcpu=2, extra_spec={'hw:cpu_policy': 'dedicated'}) 'nova.virt.libvirt.driver.LibvirtDriver' '.migrate_disk_and_power_off', return_value='{}',
# todo: fix once issue #306 addressed in dfvfs </s> if file_object._is_open:  # pylint: disable=protected-access	ParseDataStream file_object=file_object) finally: file_object.close()
assert study_id == 0  # todo </s> self.trials[trial_id].value = value	set_trial_value def set_trial_value(self, study_id, trial_id, value):
# todo: figure out way to paramaterize node['osd_ids'] for this test </s> for osd_id in node["osd_ids"]:	test_osd_services_are_running def test_osd_services_are_running(self, node, Service): assert Service("ceph-osd@%s" % osd_id).is_running
system_info = none #todo </s> new_info_file = recordinginfofile.create_empty_file(rec_dir)	_recording_update_legacy_from_v1_15_to_pprf_2_0 recording_software_version = None  # TODO recording_name = None  # TODO new_info_file.recording_uuid = recording_uuid new_info_file.start_time_system_s = start_time_system_s
# todo link subscribers_changed in docstring to callback docs </s> return lib.sp_playlist_num_subscribers(self._sp_playlist)	num_subscribers May be zero until you call :meth:`update_subscribers` and the ``subscribers_changed`` callback is called.
# todo: use proper file name </s> filename = "/tmp/release-colors-%s.img" % get_ident()	process_row print("skip PDF") break with open(filename, 'wb') as f: for chunk in r:
# todo: replicate complete behaviour of urllib.urlopener.retrieve </s> def __tuf_retrieve(	__tuf_retrieve self, parsed_url,
# todo if not found, getmessages to find the sender and chat </s> return self._client.get_input_entity(self.message.from_id)	input_sender @property def input_sender(self):
"history_id": self.history_id,  # todo: shouldn't be needed :( </s> "targets": json.dumps(targets),	test_fetch_history_compressed_type }] payload = { } self.dataset_populator.fetch(payload)
# todo: unit test! </s> get the feature sizes for the specified node types.	node_feature_sizes def node_feature_sizes(self, node_types=None): Args: node_types: A list of node types. If None all current node types
# todo: remove pragma when we drop 2.7 </s> except re.error as e:  # pragma: no cover	DateTimeParser try: fmt_tokens, fmt_pattern_re = self._generate_pattern_re(fmt) raise ParserMatchError( "Failed to generate regular expression pattern: {}".format(e)
#todo fixme: we should provide an option to create the page </s> return false	addClaims if not item.exists(): pywikibot.output('%s doesn\'t have a wikidata item :(' % page) for claim in claims: pywikibot.output('Adding %s --> %s' % (claim.getID(), claim.getTarget().getID()))
# todo implement with unfold </s> raise notimplementederror	weight_jac_mat_prod def weight_jac_mat_prod(self, module, g_inp, g_out, mat):
# todo: op should be initialized with op = be.constant(np_val, name=node.name) </s> if len(shape) == 2:	create_neon_graph shape = [d.size for d in const_tensor.tensor_shape.dim] print(shape) if 'weights' in node.name: assert (in_axis is not None)
return # todo raise error </s> if value == 0:	set_Rate if not self.get_CanControl(): logger.debug(u'Setting %s.Rate not allowed', PLAYER_IFACE) self.Pause()
print("error, how does this happen?") #todo </s> 1/0	get_regs_per_thread3_2 reg_counters.append(RegisterUsageEstimator(knl)) else: elif isinstance(sched_item, LeaveLoop): if sched_item.iname:  # (if not empty)
# todo implement this method </s> :return:	on_state_update :param new_state: new state of App
# todo: verify logic for create -- we shouldn't 'annexify' non-annexified </s> annex = annexrepo(repotop, create=true)  # if got there -- must be a git repo	_handle_auto_get filedir = dirname(filepath) repotop = GitRepo.get_toppath(filedir) if not annex.file_has_content(filepath): lgr.info("File %s has no content -- retrieving", filepath)
# todo: collapse identical parameter values in a single one </s> for instance in instances:	_GetAllHypervisorParameters full_params = cluster.GetHVDefaults(hv_name, os_name=os_name) hvp_data.append(("os %s" % os_name, hv_name, full_params)) if instance.hvparams: hvp_data.append(("instance %s" % instance.name, instance.hypervisor,
'units': '1',  # todo: where does this come from??? </s> 'dtype': stat['dtype'],	do_stats for stat in config['stats']: measurements = [{'name': measurement, 'nodata': stat['nodata']}] results[stat['name']] = nco_from_sources(task['data']['sources'],
# todo increase precision </s> eps = numpy.finfo(float).eps	check_degree ) exact_val = exact(k) alpha = abs(exact_val) * tol + (1e4+tol+exact_val)*eps if abs(exact_val - val) > alpha:
# todo: check md5sum if available </s> piece_length = info['piece length']	verify_bt_multiple def verify_bt_multiple(folder, info): assert piece_length <= 1024*1024 files = [{'path':os.path.join(folder, apply(os.path.join, x['path'])), 'length':x['length']} for x in info['files']]
# todo: does this import need to be delayed because </s> from pyomo.solvers.plugins.solvers.persistent_solver import \	preprocess_scenario_instance preprocess_fixed_variables, solver): PersistentSolver persistent_solver_in_use = isinstance(solver, PersistentSolver)
# @todo: pheonix </s> tree.setitemtext(childid, 1, "level %d" % int(level) if isinstance(level, float) else level)	expandLookup level, dirty = sChar.getSkillLevel(char.ID, id)
# todo: unit tests </s> user = auth.user	get_all_registrations_smart_folder def get_all_registrations_smart_folder(auth, **kwargs): contributed = user.node__contributed nodes = contributed.find(
# todo: fixme-  assumes only one topic (next two lines) </s> self.context.currentnode = self.topology.sources[0]	process return False record = self.queue.get() self.topology.sources[0].process(record.key(), record.value())
# todo: refactor common tests for all models, e.g. shape checking </s> scores = model.forward_owa(triples)	test_simple batch_size = 16 triples = torch.zeros(batch_size, 3, dtype=torch.long) assert scores.shape == (batch_size, 1) scores = model.forward_cwa(triples[:, :2])
# todo: manage errors </s> self._uuid = params["uuid"]	project_created def project_created(params): log.info("Project {} created".format(self._uuid))
# todo: when tool prints input bam filename, use that instead </s> s_name = self.clean_s_name(os.path.basename(f['root']), f['root'])	parse_clipandmerge_log log.debug('Could not calculate "not_removed"') if len(parsed_data) > 0: self.clipandmerge_data[s_name] = parsed_data
# todo(nnorwitz): enable test. </s> self.assertequal(['const', 'volatile'], modifiers)	testSimpleModifiers self.assertEqual([], templated_types)
# todo: save as yaml file </s> directory = '~/.ros/handeye_calibration'	_write_to_file def _write_to_file(self, calibration): if not os.path.exists(directory): os.makedirs(directory)
except oserror:  # todo: use filenotfounderror once drop python 2 </s> if dssp == "mkdssp":	dssp_dict_from_pdb_file p = subprocess.Popen([DSSP, in_file], universal_newlines=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE) raise p = subprocess.Popen(["mkdssp", in_file], universal_newlines=True,
# todo: fix later </s> input_names = self.df.index.values.tolist()	make_batch ys = [list(map(int, transcripts[b].split(' '))) for b in range(len(data_indices))] return {'ys': ys, 'input_names': input_names}
except exception:  # todo - which exceptions? </s> handle.close()	check_EMBOSS_to_AlignIO try: new_aligns = list(AlignIO.parse(handle, new_format)) raise ValueError("Can't parse %s file %s in %s format." % (old_format, filename, new_format))
# todo: also dispatch server-sent event </s> store_execution_stdout_line(execution_id=execution_id, line=line)	read_and_store_stdout if not line: break buff.write(line) except RuntimeError:
with prepare_file(["#todo this is todo"], none) as (lines, filename): </s> bear = "keywordbear"	test_find_issues def test_find_issues(self): retval, output = execute_coala(coala_ci.main, "coala-ci", "-c", os.devnull, "-S",
# @todo: "smart" & ssh keys for non-localhost </s> "connection": "local",	setup_setting_apply appname = r.application playbook = [{"hosts": host, "remote_user": remote_user, "become_method": "sudo",
# todo: this scrolling is lame and centers text :/ </s> view.show(size)	chat view.insert(ed, size, str(envelope)) view.set_read_only(True)
else:  # todo(@rasooli) t44144867: remove this logic after a while. </s> source_dict = pytorch_translate_dictionary.dictionary.load(	PytorchTranslateTask source_dict = kwargs["extra_state"]["src_dict"] target_dict = kwargs["extra_state"]["tgt_dict"] args.source_vocab_file )
raise notimplementederror # todo </s> list every item in entry_point that match request	search def search(self, entry_point, request):
# todo: position independend compare </s> if encrypted_pin == enc_pin.encode('utf-8'):	check_encrypted_pin enc_pin = utils.encryptPin( pin.encode('utf-8'), iv=binascii.unhexlify(iv)) return True return False
# todo: update npt.assert_almost_equal calls to use distancematrix </s> dm = beta_diversity('unweighted_unifrac', self.table1, self.sids1,	test_unweighted_unifrac_partial def test_unweighted_unifrac_partial(self): otu_ids=self.oids1, tree=self.tree1, id_pairs=[('B', 'C'), ])
# todo: remove this skip after fixing </s> if sys.version[0] == 3:	test_circle_draw @requires_application() def test_circle_draw(): raise SkipTest with TestingCanvas() as c:
# todo: make it irrelevent whether we test a python or a tf component (api and handling should be 100% identical) </s> self.assertequal(index_value, 1)	test_sequence_preprocessor index_value, buffer_value = test.get_variable_values(index, buffer)
# todo: write me </s> pass	test_lineage def test_lineage(self):
### todo: etc </s> repo.git.merge(upstream.name)	_update_branch else:
# todo: rf to use --batch where possible instead of splitting </s> if not files:	_run_annex_command_json if opts: annex_options += opts file_chunks = [[]] else:
# todo: check against plural_rules[lang]['nplurals'] </s> repl = variants.split('|')[plural_func(num)]	twntranslate except KeyError: plural_func = plural_rules['_default']['plural'] trans = re.sub(PATTERN, repl, trans) if param:
# todo - exponential backoff </s> req = offsetfetchrequest(consumer_group, partition_requests=preqs)	fetch_consumer_group_offsets :param preqs: a sequence of <protocol.PartitionOffsetFetchRequest> :type preqs: sequence return self.handler.request(req).get(OffsetFetchResponse)
# todo: can this be replaced by action_channel()? </s> if module_name == 'channel':	set self[module_name] = value log.info("%s = %s" % (module_name, value)) self['shell_php']['status'] = Status.IDLE
n)  # todo: access alice's private key inside this method. </s> policy = policy.from_alice(	create_policy_group alice_priv_enc = self.owner._crypto_power._power_ups[EncryptingPower].priv_key kfrags, pfrag = self.owner.generate_rekey_frags(alice_priv_enc, bob, m, alice=self.owner, bob=bob,
# todo delete me </s> import yaml	parseSDFLink sensors = parseSDFSensors(link.findall('sensor')) newlink['annotations'] = {'sdf': sdfannos} print(yaml.dump(newlink)) if newlink == {}:
# todo(leofang): test newer rocm versions </s> if (self.axes == (0, 1) and self.shape == (2, 3, 4)):	setUp def setUp(self): if cupy.cuda.runtime.is_hip: raise unittest.SkipTest("hipFFT's PlanNd for this case "
#todo rewrite this part of pdfkit.py </s> r = pdfkit.pdfkit('<html><body>hai!</body></html>', 'string')	test_stylesheet_adding_without_head_tag def test_stylesheet_adding_without_head_tag(self): css = open('testfiles/example.css') r.stylesheets.append(css)
# todo: fix with stubber / before send event </s> with mock.patch('botocore.endpoint.endpoint._send') as _send:	test_get_export 'accepts': 'application/yaml' } _send.return_value = mock.Mock( status_code=200, headers={}, content=b'{}')
pass # todo </s> def check(self):	check
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pcmpeqb res = 0x000000ff0000ff00ff00000000ffff00 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo: handle non-numerical (e.g. string, datetime) columns </s> nodes = []	_handle_concat_df def _handle_concat_df(self, lhs, df_list, label): done_cols = {} i = 0
# @todo: refactor: split to smaller routines </s> def _get_deps_and_version_matchers(result):	find_aur_deps def find_aur_deps(package_names): deps = {} for dep in (result.Depends or []) + (result.MakeDepends or []):
# todo: assert </s> repo = self.remote.get_repo("testrepo0")	test_get_repo Test: Get a repo object
# todo : use starttls when mailnag has been migrated to python 3 </s> conn.getwelcome()	_get_POP3_connection else: conn = poplib.POP3(self.server, int(self.port)) conn.user(self.user) conn.pass_(self.password)
# todo use case-xml case creation workflow </s> return p	make_product p.save()
# todo(eric_k): unicorn@1.0.2rc1 doesn't like writing to </s> if reg in {"fs"}:	write_back_register self._emu.reg_write(self._to_unicorn_id("EFLAGS"), self._cpu.read_register("EFLAGS")) return logger.warning(f"Skipping {reg} write. Unicorn unsupported register write.") return
# todo: properly resolve samaccounname in gc </s> domain = self.domain	process_computer sessions = [] for ses in sessions: user = ('%s@%s' % (ses['user'], domain)).upper() target = str(ses['target']).upper()
# todo add rising vs top </s> searchtopics_payload = dict()	search_topics def search_topics(self): req_url = "https://www.google.com/trends/api/widgetdata/relatedsearches" searchtopics_payload['req'] = json.dumps(self.related_queries_widget['request']) searchtopics_payload['token'] = self.related_queries_widget['token']
# todo: how to check it? meybe we can omit this test </s> pass	test_gausianfill def test_gausianfill():
# todo: add logger here </s> print(''.join('!! ' + line for line in lines))	make_screenshot exc_type, exc_value, exc_traceback = sys.exc_info() lines = traceback.format_exception(exc_type, exc_value, exc_traceback) browser.quit() return {"success": None, "error": True}
# todo: fixme </s> wa = node1.cd_ref.transform_node_to_global_assuming_rectangular(n1 - wa)	_fill_bar_yz pass elif offt_end_a == 'O': else: msg = 'offt_end_a=%r is not supported; offt=%s' % (offt_end_a, elem.offt)
# todo(junxian): transform to decoder state size </s> self._add_internal_trainable_variables()	_build connector_inputs = [tf.cast(connector, tf.float32) for connector in connector_inputs] output = tf.concat(connector_inputs, axis=1) self._built = True
# todo(b/80125832): enable nccl in tests </s> all_reduce_spec='',	_testVariables staged_vars=False, optimizer='momentum', use_fp16=False, fp16_vars=False,
# todo logging </s> pass	_generate_cover_img f.write(r.content) except:
''' # todo filter in the database? </s> cursor = bdb.sql_execute(params_sql, (generator_id, modelno))	simulate_joint WHERE generator_id = :generator_id AND modelno = :modelno mus = {} sigmas = {}
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_punpcklqdq x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo: slugification should be abstracted out somewhere reusable </s> self.url_name = self.name.lower().replace(" ", "-")	__init__ def __init__(self, name): self.name = unicode(name) self.path = bf.util.site_path_helper( bf.config.controllers.blog.path,
# todo: log errors to log file </s> return resources	get_resources config.load_kube_config() except  Exception, e: v1 = client.CoreV1Api() v1Beta1 = client.AppsV1beta1Api()
# todo: should be able to just access the api from qml. </s> return bool(containers)	existsKey containers = CuraContainerRegistry.getInstance().findContainerStacks(type="machine", **metadata_filter)
time.sleep(1)  # todo: avoid race conditions in other way </s> self.server.start()	setUp self.server = threading.Thread(target=self._run_server, args=(certs, sock))
# todo confirm we want floor division here </s> utilities.reshape(numalts, utilities.size() // numalts)	mnl_probs if numalts == 0: raise Exception("Number of alternatives is zero") exponentiated_utility = utilities.exp(inplace=True) if clamp:
#todo take image as a factor </s> img_len = 0	calc_best_node def calc_best_node(self, cur_node, depth=0.1): text_len = self.text_len(cur_node) impact_factor = self.semantic_effect(cur_node) score = (text_len + img_len) * impact_factor * (depth**1.5) # yes 1.5 is a big number
# todo: checks for being not outside of this repository </s> out.append(exists(target_path) and '.git/annex/objects' in target_path)	file_has_content if islink(filepath):                    # if symlink target_path = realpath(filepath)    # find abspath of node pointed to by symlink else: out.append(False)
# todo: no-op component? </s> stage_op = none	define_api_methods_single initial_internal_states = self_.call(staging_area.unstage)  # preprocessed_last_s_prime else: state_values_pi, logits_pi, probs_pi, log_probabilities_pi, current_internal_states = \ self_.call(policy.get_state_values_logits_parameters_log_probs, preprocessed_s_all,
"""the event which triggered the message."""#todo elaborate </s> s.bytes = bytes	__new__ s.nick = origin.nick s.event = event s.match = match The regular expression ``MatchObject_`` for the triggering line.
# todo: could use trim_silence() here or a better vad. </s> audio_voice_only = audio[offsets[0]:offsets[-1]]	read_mfcc silence_threshold = np.percentile(energy, 95) offsets = np.where(energy > silence_threshold)[0] mfcc = mfcc_fbank(audio_voice_only, sample_rate) return mfcc
# todo: allow prerelease for now </s> prerelease = release.get('prerelease', false) and false	_find_required_version tag = tag_name.replace('%V', req_v) if tag == release.get('tag_name'): if prerelease and not self.force_install: click.secho(
# todo: add recovery test </s> self._phabupdatewithexpectations(total=1, bad=1)	test_badBaseWorkflow self._devPushNewFile("NEWFILE", has_plan=False)
# todo: remove in 21.08 </s> self.result.clear()	reset def reset(self):
# todo(developer): uncomment these lines and replace with your values. </s> queue_path = client.queue_path(project, location, queue)	pause_queue def pause_queue(project, location, queue): client = tasks.CloudTasksClient() response = client.pause_queue(queue_path) return response
# todo: truffle change end </s> ss[4], ss[0] = rnd(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],63,0xc67178f2);	do_part2 ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],62,0xbef9a3f7);
# todo: make sure reply_email is unique </s> reply_email = f"reply+{random_words()}@{email_domain}"	MailHandler website_email, ) forward_email = ForwardEmail.create( gen_email_id=gen_email.id,
# todo! make this whole operation one undo </s> sprites = {}	export_to_coatools def export_to_coatools(img, drw): for layer in img.layers: if layer.visible:
# todo: passing config is wrong, but changing this revealed more complicated issues </s> self._lang_exporters[lang_name] = exporter(config=self.config, parent=self)	_get_language_exporter self._lang_exporters[lang_name] = None else: return self._lang_exporters[lang_name]
# todo: replace with "yield from" when dropping python 2. </s> for stream in streams.items():	_get_live_streams streams = self._get_hls_streams(channel_id, wait_for_transcode=not has_desktop_streams) yield stream except PluginError as err:
# todo: test </s> data.extend(obj.data)	ConcatDummyTyper col_no += 1 else:  # DataFrameType names.extend(obj.columns) ret_typ = DataFrameType(tuple(data), None, tuple(names))
# populated by coredb._solve(). todo: find a better solution for that. </s> self.direct_deps = []	__init__ basename = os.path.basename(self.core_file) self.core_root = os.path.dirname(self.core_file) try: _root = Root(utils.yaml_fread(self.core_file))
# todo(b/147242148): introduce principled artifact structure (directory </s> if isinstance(artifact, types.valueartifact):	_prepare_output_paths absl.logging.warning(msg) return artifact_dir = os.path.dirname(artifact.uri) else:
# todo: remove this log statement when invoking this method on each iteration of the goal state loop (currently it is invoked only on a new goal state) </s> logger.info("fetching extensions goal state [correlation id: {0} etag: {1}]", correlation_id, etag)	update_extensions_goal_state if etag is not None: headers['if-none-match'] = etag vm_settings, response_headers = self.fetch(url, headers, ok_codes=[httpclient.OK, httpclient.NOT_MODIFIED]) if vm_settings is None:
# compare filesizes todo print analysis of this :) </s> cmd = "ls -l '%s.ttf'*" % filename	ttx_process cmd = "ttx -i '%s.ttx'" % filename run(cmd, cwd=_out, log=log) run(cmd, cwd=_out, log=log) cmd = "rm  '%s.ttf.orig'" % filename
#todo - use the following more helpful error, but update unit tests </s> raise valueerror("sequences must all be the same length")	append if _private_expected_length is None: if self._records and len(record) != self.get_alignment_length(): elif len(record) != _private_expected_length: raise ValueError("Sequences must all be the same length")
# todo(shardy): remove when we no longer support essex </s> nova = client.client(**args)	authenticate nova = client.Client(no_cache=True, **args) except TypeError: nova.authenticate() return nova
# todo 找出数据重复的原因 </s> spread_others = list(set(spread_others))	_get_current_reposts else: so.upper_user_id = user_id spread_other_dao.save(spread_others) print('一共获取了{num}条转发信息'.format(num=len(spread_others)))
# todo: remove this </s> if config.require_pyinfra_version is none:	init config = Config() if config.MIN_PYINFRA_VERSION is not None: config.REQUIRE_PYINFRA_VERSION = '>={0}'.format(config.MIN_PYINFRA_VERSION) logger.warning(
# todo(ja): reclaiming space should be done lazy and low priority </s> if is_snapshot:	_clear_volume def _clear_volume(self, volume, is_snapshot=False): dev_path = self.local_path(volume) + "-cow" else:
# todo find a better way of checking for no pregenerated thresholds </s> self.code_gen_dict["$globals$"] += '#include "thresh.h" \n'	global_includes self.code_gen_dict["$GLOBALS$"] += '#include "params.h" \n' if self.TMEM != 0:
# todo: the test currently demonstrates broken behavior </s> check that buildbot does not allow extra claims on a claimed lock after	test_changing_max_lock_count_does_not_break_step_locks self, builder_count, lock_cls, mode, max_count_before, max_count_after, allowed_steps_before, allowed_steps_after): a reconfig that changed the maxCount of that lock. Some Buildbot versions created a completely separate real lock after each maxCount
# @todo: replace with consent tracking </s> if deployment_settings.get_auth_terms_of_service():	register field_id + SQLFORM.ID_ROW_SUFFIX, ) field_id = "%s_tos" % utablename label = T("I agree to the %(terms_of_service)s") % \
# todo: remove need for --no-strict-optional </s> driver.add_mypy_modules('stdlibsamples (3.2)', modules,	add_stdlibsamples modules.append(module) if modules: cwd=stdlibsamples_dir, extra_args=['--no-strict-optional'])
# name. it should be handled by making a simple class to hold todo </s> if sum(todo_position) > 1:	_get_next_state todo_position = [1 if current_state in todo_set else 0 for todo_set in cleaned_todos] echom("There are 2 todo's with the same name, currently this is not supported. ") next_dir = -1 if direction == Direction.BACKWARD else 1
# todo: kodi 17 compat removal cleanup </s> if kodi_version() < 18:	_add_editcontrol noFocusTexture="-" ) kwargs['isPassword'] = password control = xbmcgui.ControlEdit(0, 0, 0, 0, **kwargs)
# todo: only remove excess partitions if new data has fewer </s> self._delete_item(key)	_consume_save_queue try: key, val = self.save_queue.popleft() self[key] = val errors = 0
# no todo item selected </s> pass	_complete_selected_item text_type(self.view.todolist.number(todo)))) except AttributeError:
# todo: complex numbers return complex </s> return signature(types.float64, *args)	resolve_corr assert isinstance(ary.dtype, types.Number) assert isinstance(args[0].dtype, types.Number)
# todo: add other types to this table (e.g., functional.all_types) </s> mytable = table("mytable",	test_sqlalchemy_impala4_compilation engine = create_test_engine() metadata = MetaData(engine) metadata, Column('col1', STRING),
# todo: fill some sane numbers here </s> return -1	fracUpload def fracUpload(self):
# todo: explicitly exploit symmetry and set onesided=true </s> a_fft = torch.rfft(a, signal_ndim=1, onesided=false)	circular_correlation :param b: torch.tensor, shape: (batch_size, dim) :return: torch.tensor, shape: (batch_size, dim) b_fft = torch.rfft(b, signal_ndim=1, onesided=False) a_fft[:, :, 1] *= -1
# todo: optimize this implementation </s> dx[:] = dy * (y >= 0.) + dy * (y + 1.) * (y < 0.)	el_deriv def el_deriv(self, x, y, dy, dx):
# todo: this will currently still fail, since it has children </s> assert steps['step-2'].parents == [steps['step-1']]	test_pipeline_incoming assert steps['step-1']._children == [steps['step-2']] assert steps['step-1'].parents == [] incoming_inclusive = pipeline.incoming(['uuid-4', 'uuid-6'], inclusive=True) steps = {step.properties['name']: step for step in incoming_inclusive.steps}
# todo complete this method </s> partition, kptlist, dtype)	eeccsd return ipccsd(eom, nroots, koopmans, guess, left, eris, imds,
# todo: proper test </s> tf_siso = tf(ss_siso)	test_bode_basic def test_bode_basic(ss_siso): bode(ss_siso) bode(tf_siso)
# todo: replace this by bulk update if we can </s> for obj in self.all():	delete def delete(self): assert self.query.can_filter(), "Cannot use 'limit' or 'offset' with delete." obj.delete() self._result_cache = None
# todo: add checks for broken paddings/encrypted values and malformed enc_data </s> not_valid_password = b"\x01\x02\x03\x04\xff"	test_01_encrypt_decrypt_pass pw3 = '7a4d5e2f26978394e33715bc3e8188a3:90b2782112ad7bbc5b48bd10e5c7c096cfe4ef7d9d11272595dc5b6c7f21d98a' self.assertEquals(decryptPassword(pw3, ), u'passwörd') r = encryptPassword(not_valid_password) self.assertEqual(decryptPassword(r), 'FAILED TO DECRYPT PASSWORD!')
# todo: make truly async </s> self._resourcemanager_client = discovery.build('cloudresourcemanager', 'v1', cache_discovery=false, cache=memorycache())	__init__ def __init__(self):
# todo: work out a way to set this based on the timespan of the data. </s> locator = mdates.autodatelocator(minticks=5, maxticks=25)	plot ax.legend(loc="upper right") axes[-1].set_xlim(self.to_dataframe().index[0], self.to_dataframe().index[-1]) formatter = mdates.ConciseDateFormatter(locator) axes[-1].xaxis.set_major_locator(locator)
# todo: index </s> ret_typ = seriestype(data_typ.dtype, none, true)	GetItemDataFrameLoc col_no = df.df_type.columns.index(col_name) data_typ = df.df_type.data[col_no] return signature(ret_typ, *args)
except(wx._core.pyassertionerror): #todo: error win64 </s> path = dlg.getpath()	ShowImportDirDialog else: path = dlg.GetPath().encode('utf-8') dlg.Destroy() os.chdir(current_dir)
# todo: test 2: quando para a data no estado tem a planilha de total e outras </s> if date in cases:	get_state_data for spreadsheet in spreadsheets: date = spreadsheet.date continue
# xxx todo: rounding </s> e = []	fcvt def fcvt(ir, instr, arg1, arg2): src = ExprOp('fpconvert_fp%d' % arg1.size, arg2) e.append(ExprAssign(arg1, src))
# todo: parlist, dots, block </s> self.assertequal(10, p._pos)	testFuncBodyParList self.assertIsNotNone(node)
# - todo default from user if citizen </s> field = table.contact_phone	customise_br_assistance_offer_controller show_map = False, ) from s3 import IS_PHONE_NUMBER_MULTI field.requires = [IS_NOT_EMPTY(), IS_PHONE_NUMBER_MULTI()]
# todo: remove / replace? </s> def sequence_timelock_blocks(self, blocks):	sequence_timelock_blocks if blocks > SEQUENCE_LOCKTIME_MASK: raise TransactionError("Number of nSequence timelock blocks exceeds %d" % SEQUENCE_LOCKTIME_MASK)
# todo: remove </s> g.fields = fields	_read else: search_extras[param] = value g.fields_grouped = fields_grouped facets = OrderedDict()
# todo(b/131719250): add option to output a sample of anomalous examples for </s> pipeline_options = none,	validate_examples_in_tfrecord stats_options, output_path = None, ): Runs a Beam pipeline to detect anomalies on a per-example basis. If this
# todo: remove method in 0.24 </s> msg = ("the n_classes_ attribute is to be deprecated from version "	n_classes_ @property def n_classes_(self): "0.22 and will be removed in 0.24.") warnings.warn(msg, DeprecationWarning)
# todo: give a vanilla example </s> .. math::	feca def feca(predicted_power, df_appliances_ground_truth): fraction = \\sum_n min \\left (
# todo: scale and translation could be merged into a single network </s> with tf.variable_scope("scale", reuse=tf.auto_reuse):	forward_and_jacobian mask = self.get_mask(inputs, dtype=inputs.dtype) masked_inputs = inputs * mask scale = mask * self.scale_fn(masked_inputs) with tf.variable_scope("translation", reuse=tf.AUTO_REUSE):
# todo error on too many levels </s> markers = dict(zip(style_levels, self.default_markers))	determine_attributes dashes = dict(zip(style_levels, dashes)) if markers is True: elif isinstance(markers, dict): pass
# todo: this sometimes segfaults. i must fix this! </s> err = pm.lib.pm_write(self.stream, event, 1)	send_event event.timestamp = pm.lib.Pt_Time() event.message = value _check_err(err)
#todo: use ffi function </s> return vec2d(self._start).get_distance(self._end) * self.t	get_hit_distance def get_hit_distance(self):
#todo: support broadcast case: (x,) (x, y) </s> margins = mp.transpose(mp.maximum(0, mp.transpose(x) - mp.transpose(correct_class_scores) + 1.0))	svm_loss N = x.shape[0] correct_class_scores = x[mp.arange(N), y] loss = (mp.sum(margins) - mp.sum(margins[mp.arange(N), y])) / N margins[mp.arange(N), y] = 0
#todo : parse unit  25, 25m, 25 ft, etc. </s> except:	seed try: offset, unit = float(htag[:i]), htag[i:].strip() offset = None elif "building:levels" in tags:
#todo: update this to try fsfindfolder and fallback to this </s> basepath = os.path.expanduser('/library/application support')	user_data_dir elif sys.platform == 'darwin': if os.uname()[-1] == 'i386': else: from Carbon import Folder, Folders
# todo(yamahata): creating volume simultaneously </s> attempts = 0	_await_block_device_map_created def _await_block_device_map_created(self, context, vol_id, max_tries=30, wait_between=1): start = time.time() while attempts < max_tries:
# todo: put this in network definitions: </s> denominator = pow(10, 8)	create_transaction if fee is None: fee = 1000000 if input_arr is None: input_arr = []
# todo do something with reccomendation </s> return ret_val	process_rabbit_message value = my_obj[1] self.logger.debug('value:{0}'.format(value))
# todo: split lines here? </s> d = diff_notebooks(a, b)	main_diff a = nbformat.read(afn, as_version=4) b = nbformat.read(bfn, as_version=4) verbose = True if verbose:
# todo: log discarded bytes? </s> return 'response discarded due to invalid crc.'	decode_out self.out_parsing = False self.out_data = [] comm_address = self.out_data[1] if self.out_data[2] in self.response_map:
nullcontext = contextlib.exitstack()  # todo: use contextlib.nullcontext after python 3.7 </s> with lock or nullcontext:	try_hack_once if len(generated_input_hashes) < generated_input_hashes_length_limit: input_digest = hashlib.sha1(input_data).digest() if len(generated_input_hashes) < generated_input_hashes_length_limit: if input_digest in generated_input_hashes:
# todo: improve performance </s> return jac_mat.view(batch, -1, num_cols)	bias_jac_mat_prod jac_mat = jac_mat.expand(batch, -1, out_x, out_y, -1).contiguous()
# todo: change the frontend to pass seconds instead. </s> expires_at = (now_in_seconds + one_hour_in_seconds) * 1000	test_existing_email_create_user return {'sub': 'email', 'email': email, 'exp': id_token_expiration_timestamp} monkeypatch.setattr(AuthBackend, '_get_user_info', userinfo_mock) existing_user = User.objects.create(username="email/foo@bar.net", email=email) resp = client.get(
# todo: add error parameter </s> return result	symmetric_difference result.names = result_name
pass  # todo </s> elif ch == 'op_m':  # one or more opcodes	output_script_type data.append(script[cur+1:]) elif ch == 'multisig':  # one or more signature if script[cur] in OP_N_CODES: number_of_sigs = script[cur] - opcodes['OP_1'] + 1
# todo: make input channels configurable, not hard-coded to three channels for rgb </s> self.resnet = resnet50(pretrained=pretrained)	__init__ pretrained: use ImageNet pre-trained backbone feature extractor super().__init__() self.enc0 = nn.Sequential(self.resnet.conv1, self.resnet.bn1, self.resnet.relu, self.resnet.maxpool) self.enc1 = self.resnet.layer1  # 256
# todo assert cls.__tablename__ == '' </s> with dbcleanup(session, cls):	test_StoredWorkflowTagAssociation def test_StoredWorkflowTagAssociation(model, session, stored_workflow, tag, user): cls = model.StoredWorkflowTagAssociation user_tname, value, user_value = 'a', 'b', 'c' obj = cls(user=user, tag_id=tag.id, user_tname=user_tname, value=value)
# todo: it's better for us to have checked this a while ago so that this situation is impossible.  #443 </s> if host and (host != common_name_on_certificate):	_write_tls_certificate if not is_checksum_address(checksum_address):  # TODO: more? raise RuntimeError("Invalid certificate checksum address encountered: {}".format(checksum_address)) raise ValueError('You passed a hostname ("{}") that does not match the certificat\'s common name.'.format(host)) certificate_filepath = self.generate_certificate_filepath(checksum_address=checksum_address)
# todo: support steps and times (motion blur) </s> print("dupli export took %.3fs" % (time() - start))	create_session transformations = array("f", duplis.matrices) luxcore_scene.DuplicateObject(src_name, dst_name, count, transformations) engine.update_progress(index / len_objs)
# todo compare file contents? </s> updater.refresh()	test_refresh updater = self._new_updater()
# todo: get current encoding </s> return s.encode('utf-8', 'ignore') if isinstance(s, string_types) else "error"	pretty_str except UnicodeEncodeError: lgr.warning("Failed to encode value correctly. Ignoring errors in encoding") else: return str(s)
# todo make this a private api </s> location = resource['display_name']	parse_resource @classmethod def parse_resource(cls, resource): latitude = resource['lat'] or None longitude = resource['lon'] or None
# todo: this is wrong. globe.semiminor_axis does </s> [-19970526.37931940, 20104926.35931940], decimal=6)	test_ellipsoid_micronesia_transform [-20009068.84931940, 20066383.88931940], decimal=6) assert_almost_equal(np.array(aeqd.y_limits), pt_lat = 15 + (14 + 47.4930 / 60) / 60 pt_lon = 145 + (47 + 34.9080 / 60) / 60
'target': 20000, # todo target max size </s> }	build_tasks 'word': args.max_word_v_size, 'char': args.max_char_v_size, word2freq, char2freq, target2freq = get_words(tasks) vocab = get_vocab(word2freq, char2freq, target2freq, max_v_sizes)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_pass_empty def test_pass_empty(self): Request is empty, so default values are used for all parameters.
# todo: use widgets.dialog </s> wx.messagedialog(self._editor,	save self._data.update_from(self._editor.utf8_text) except AssertionError: 'ERROR: Data sanity check failed!', 'Can not apply changes from Txt Editor',
# todo: test! </s> if not settings.st_unique_emails:	EmailUniqueMixin def clean_email(self): email = self.cleaned_data["email"] return email is_taken = User._default_manager\
# todo: convert 'pre' to 'code' ? </s> self._editmode_tags = list(filter(_is_indent_tag, self._editmode_tags))	_insert_bullet_at_cursor end.forward_to_line_end() self.smart_remove_tags(_is_heading_tag, insert, end) if not self._editmode_tags: dir = self._find_base_dir(insert.get_line())
# todo: remove all elements of the list and remove the allowlist </s> allowlist = [	test_no_private_tf_api def test_no_private_tf_api(): "tensorflow_addons/optimizers/novograd.py", "tensorflow_addons/optimizers/moving_average.py",
# todo: if there are resources, combine them into a zip file </s> assert not has_resource_files(resources)	get self.set_header('Last-Modified', tz.utcfromtimestamp(info.st_mtime)) output, resources = exporter.from_filename(os_path) self.finish(output)
# todo: fire error </s> self._wait_event_yield = false	tick event.value.value = value if self._wait_event_yield: except StopIteration: self.unregisterTask((event, task))
# todo: non-numeric columns should be ignored automatically </s> def test_impl(n):	test_std1 def test_std1(self): df = pd.DataFrame({'A': np.arange(n)+1.0, 'B': np.arange(n)+1}) return df.std()
# todo - log details about the test </s> return result	check_mvip_connection test = self.sfe.test_connect_mvip(mvip=self.mvip) result = test.details.connected except: err = get_exception()
# todo: add more complicated testcases </s> assert_equal(score.shape, (4,))	test_aom_static_repeat random_state=42)
# todo(ohta): convert `study` and `trial` to single objective versions before passing. </s> return self._sampler.infer_relative_search_space(study, trial)	infer_relative_search_space self, study: "mo.study.MoStudy", trial: "mo.trial.FrozenMoTrial" ):
#todo: combine all slp frames to one single png, much more efficient loading and editing possible.. </s> player_id = 4 #yellow	save_pngs
# todo: add type and value checkings </s> self.width = width	__init__ def __init__(self, pinholes, width=None, height=None): super(DepthWarper, self).__init__() self.height = height self._pinholes = pinholes
# todo: abstract this away into a function. </s> read_only = false	eval self.reset() elif self.action: if self.view.file_name(): mode = os.stat(self.view.file_name())
# todo: accept external hrefs </s> text_path_href = url(	text ascent, descent, _, max_x_advance, max_y_advance = ( surface.context.font_extents()) node.get('{http://www.w3.org/1999/xlink}href', '') or node.parent.get('{http://www.w3.org/1999/xlink}href', ''))
# todo: fetchable files, boot files, etc. </s> ["name",["testprofile0",],[]],	setUp ] self.profile_fields = [ ["distro",["testdistro0",],["baddistro",]], ["enable_gpxe",["yes","YES","1","0","no"],[]],
# todo: check windows does not modify \n to \r\n here </s> sha1 = os.path.join(self.ro.folder, "manifest-sha1.txt")	test_data self.assertTrue(f1.writable()) f1.write(u"Hello\n") self.assertTrue(os.path.isfile(sha1)) with open(sha1, "r", encoding="UTF-8") as f2:
#todo(wwolf) get correct value for these </s> "updated": "2011-07-18t11:30:00z",	_versions_multi_choice "id": "v1.1", "status": "CURRENT", }, {
# todo: we can't wait for this, since the loop is running with .loop_forever() </s> self._chat_on = value	set_chat_on info = self._mqtt.publish("/set_client_settings", payload=payload, qos=1)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_seed_not_trytes def test_fail_seed_not_trytes(self): ``seed`` contains invalid characters.
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_hashes_null def test_fail_hashes_null(self):
#time = "todo" </s> annot.annotation_metadata.annotator.email = "todo" #todo	fill_annoatation_metadata annot.annotation_metadata.annotator.name = "TODO"
# todo sync protocol </s> await asyncio.sleep(0.01)	TestWindow win2 = self.open_window() await i3.command(f'[id={win1}] kill; [id={win2}] kill') i3.main_quit() @i3.on(Event.WINDOW_NEW)
# todo: refactoring </s> def push(**kwargs):	push Dict to json file. :param kwargs: Kwargs
# todo use all awcs, not just ones with data </s> case_objects = self.row_objects + self.extra_row_objects	awc_data def awc_data(self): awcs = {} for case_object in case_objects:
# @todo: multisystem... </s> try:	get_info def get_info(self): if self.timer_cpu_info.finished(): self.cpu_info['cpu_name'] = open('/proc/cpuinfo', 'r').readlines()[4].split(':')[1][1:-2] except:
# todo: fixme! this throws an index out of range exception </s> http_troute = http_troute[0].get_trace().values()[0]	discover last_https_ip = https_ip_tuples[-1] http_troute = traceroute(domain, dport=http_port) http_ip_tuples = http_troute.values() last_http_ip = http_ip_tuples[-1]
# todo: add auto_detect_types=true parameter </s> table_rows = list(data)	create_table def create_table(data, force_headers=None, fields=None, encoding=None, *args, **kwargs): if fields is None: if force_headers is None:
# todo: implement this method </s> pass	tear_down_sources_with_task def tear_down_sources_with_task(self):
pass # todo </s> def test_set_position(self):	test_set_position @SkipTest
# todo fix. </s> self.assertequals(reil_ctx_out["rax"], res)	test_movd_4 res = 0x0000000012345678 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm0"], ctx_init["xmm0"])
# todo debug </s> print logstring	logRule + "monthday=%d)" % item.element.monthday) logging.info("[%s]: %s" % (fileName, logString)) elif item.type == "hour": logString = ("%s hour " % spaceString
# todo(b/158462888): use aggregete losses that works with replicas. </s> tf.reduce_sum(input_tensor=tf.square(v)) * self._policy_l2_reg	l2_regularization_loss policy_vars_to_regularize, vf_vars_to_regularize) policy_l2_losses = [ for v in unshared_policy_vars_to_regularize ]
# todo private access </s> param_names, _ = func_context._arguments.get_executed_param_names_and_issues(self.function_value)	SimpleParamName def infer(self): func_context = self.function_value.as_context() return param_names[self._get_param_node().position_index].infer()
raise exceptions.mpdnotimplemented  # todo </s> .. versionadded:: mpd protocol 0.19	mount mount foo nfs://192.168.1.4/export/mp3
# todo : component handling </s> return pynode(oname, otype)	_MObjectPyNode if _isValidMObject (comp) : clist = None else : return PyNode(oname, otype)
pass ## fixme: todo </s> def test_date_ticker_three_months(self):	test_date_ticker_three_months
# xxx todo </s> event.debug.system.enable_step_on_branch_mode()	__branch def __branch(self, event): if self.options.mode == "branch":
# todo find the correct hbin </s> d = hbincell(self._buf, offset, self.parent())	classname classname_length = self.unpack_word(0x4A) offset = self.abs_offset_from_hbin_offset(classname_offset) return struct.unpack_from("<%ds" % (classname_length), self._buf, d.data_offset())[0]
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo change format of formatted_preds in qa (list of dicts) </s> preds_all.extend(self._get_predictions(dataset, tensor_names, baskets, rest_api_schema, disable_tqdm=true))	inference_from_dicts with tqdm(total=len(dicts), desc=f"Inferencing Dicts", unit=" Dicts") as pbar: for dataset, tensor_names, baskets in results: pbar.update(multiprocessing_chunk_size) p.close()
#todo check our balance </s> if self.total_sell_amount(our_offers) < self.min_amount:	update_otc_orders print(f"Failed to cancel offer #{offer.offer_id}!") our_offers = self.otc_offers(self.sell_token, self.buy_token) rate = self.apply_gap(conversion.rate, self.avg_gap) have_amount = self.max_amount - self.total_sell_amount(our_offers)
# todo(nnorwitz): enable test. </s> self.assertequal(['const', 'volatile'], modifiers)	testSimpleModifiers self.assertEqual([], templated_types)
# todo: integrate this into emrjobrunner </s> step_num_to_id = runner._step_num_to_id()	list_relevant def list_relevant(runner, step_nums): logs = {} for log_type in _RELEVANT_LOG_TYPES:
pass  # todo </s> def stop_threads(self):	stop_threads
# todo: fix arrows </s> pass	animate_layout self._nodes.set_data(pos=node_vertices) if self._directed: self._edges.set_data(pos=line_vertices) return True
# todo: migrate new article ids to oldrecipearticleredirect </s> super(migration, self).move_self_foreignkeys(orm)	move_self_foreignkeys def move_self_foreignkeys(self, orm):
# todo extend to nonbinary nodes </s> if i not in self._input_indices and i in self.context.node_indices:	__init__ else: self._dimension_labels.append(None) past_tpm_on = past_tpm_on.sum(i, keepdims=True) / 2 past_tpm_off = past_tpm_off.sum(i, keepdims=True) / 2
# todo: only works for ratios </s> res.metadata['reference_kpi'][dk['name']] = re.sub(pattern+'/', '', dk['formula'])	mock_results_object kpis_to_analyse.update([dk['name']]) data.kpis.loc[:,dk['name']] = eval(re.sub(pattern, r'data.kpis.\1.astype(float)', dk['formula'])) if kpi_subset is not None: kpis_to_analyse.intersection_update(kpi_subset)
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
# todo: fix with stubber / before send event </s> with mock.patch('botocore.endpoint.endpoint._send') as _send:	test_get_export 'accepts': 'application/yaml' } _send.return_value = mock.Mock( status_code=200, headers={}, content=b'{}')
# todo refactor when recurse_differ supports list_differ </s> for policy in policies:	storage_policies_configured service_instance=si) log.trace('current_policies = {0}'.format(current_policies)) policy_copy = copy.deepcopy(policy) filtered_policies = [p for p in current_policies
# todo: add random string. </s> new_rar_path = rar_path + ".old"	extract_rar if self.is_overwritten(rar_path): log.debug("RAR file contains a file with the same name, original is going to be overwrite") shutil.move(rar_path, new_rar_path) rar_path = new_rar_path
# todo curves </s> if new_op and len(args1) + len(args2) <= maxstack:	specializeCommands elif {op1, op2} == {'vlineto', 'hlineto'}: new_op = op1 commands[i-1] = (new_op, args1+args2) del commands[i]
# todo(henry-nash): add implementation here. </s> pass	expand_schema This is run manually by the keystone-manage command before the first keystone node is migrated to the latest release.
pass # todo </s> def add_data(data):	add_data
pass #todo: show multi select menu </s> elif len(selectable) == 1 and hasattr(selectable[0], 'show_menu'):	mouseReleased game.main.session.ingame_gui.hide_menu() if len(selectable) > 1: selectable[0].show_menu() game.main.session.selected_instances = selectable
# todo rename "search_string" argument "pattern" </s> pattern = search_string	_vi_slash_impl class _vi_slash_impl(ViMotionCommand, BufferSearchBase): def run(self, search_string, mode=None, count=1): if not pattern: return
# todo: remove in v2.7 </s> @property	InterfaceTemplate def __str__(self): return self.name def form_factor(self): Backward-compatibility for form_factor
# todo: use transe embeddings for initialization.. </s> torch.nn.init.xavier_uniform_(self.linear.weight)	_initialize def _initialize(self): torch.nn.init.zeros_(self.linear.bias)
#todo this should get extracted somewhere </s> conversion.source_amount = wad.min(amount_in_sai, conversion.max_source_amount)	order_take_handler amount_in_sai = log_take.give_amount conversion = copy.deepcopy(self.get_conversion(self.sell_token, self.buy_token)) conversion.target_amount = Wad(Ray(conversion.source_amount) * conversion.rate) print(f"Someone exchanged {amount_in_gem} {ERC20Token.token_name_by_address(self.sell_token)} to {amount_in_sai} {ERC20Token.token_name_by_address(self.buy_token)}")
# todo: real error handling </s> log.error("failed to store encryption key: %s", e)	write_escrow_packets backup_passphrase) except (IOError, RuntimeError) as e: log.debug("escrow: write_escrow_packets done")
# todo: i should make sure to escape single quotes here </s> self._cursor.execute("select value from %s where key='%s'" % (self._name, key))	__getitem__ if not isinstance(key, basestring): raise ValueError("key must be a string") results = self._cursor.fetchall() if len(results) == 0:
# todo link parameters </s> if self.extract_stem:	extract if self.extract_eomi: self._extract_eomi() self.__extract_stem() return self._extract_predicator(candidates, min_count, reset_lrgraph)
# todo: this type conversion seems to be bottle neck </s> features = tf.divide(tf.cast(inputs, tf.float32),	call def call(self, inputs): tf.constant(255.)) features = self.conv1(features)
# todo print out profile changes </s> return	handle_room_membership_events joined = True else: else: join(event, date, room, room_buffer, is_state_event)
# todo: will implement along with test_fuzzy_find </s> receipt = none	test_parse_market def test_parse_market(self): with open(self.dir_path + "/data/receipts/sample_text_receipt.txt") as receipt_file: receipt = Receipt(self.config, receipt_file.readlines())
# todo: the following skipped suite and fixtures should be enabled </s> return ['login_token']	_filter_post_data_parameters def _filter_post_data_parameters(self):
# todo ... </s> pass	makeAstNodeCall def makeAstNodeCall(func, *args): if not isinstance(func, ast.AST): return ast.Call(func=func, args=args)
# todo: this is untested. </s> _raise_current_error()	load_certificate_request raise ValueError("type argument must be FILETYPE_PEM or FILETYPE_ASN1") if req == _ffi.NULL: x509req = X509Req.__new__(X509Req) x509req._req = _ffi.gc(req, _lib.X509_REQ_free)
# todo move away </s> self.lengths_front[match.rstop] += 1	Adapter return self._trimmed_back(match) def _trimmed_front(self, match): return match.read[match.rstop:] def _trimmed_back(self, match):
# todo: refactor exceptioncache to be usable by multiple indexers. </s> if indexer_id not in exceptionscache:	get_scene_exceptions if exceptions: exceptions_list = list({cur_exception[b'show_name'] for cur_exception in exceptions}) exceptionsCache[indexer_id] = {} exceptionsCache[indexer_id][season] = exceptions_list
# todo: sort </s> for line in self.db.open(url):	distinct if skip is not None: url += "%20skip:%d" % skip yield json.loads(line)
# todo: do this automatically when using the `+` operator on dataoprecords. </s> nn_inputs = self.tuple_merger.merge(nn_inputs, other_nn_inputs)	get_preprocessed_state_action_and_action_probs nn_inputs = preprocessed_states if other_nn_inputs is not None:
# todo: add broadcasting to get_rotation_matrix2d for center </s> center = center.expand(tensor.shape[0], -1)	scale if center is None: center: torch.Tensor = _compute_tensor_center(tensor) scale_factor = scale_factor.expand(tensor.shape[0]) scaling_matrix: torch.Tensor = _compute_scaling_matrix(scale_factor, center)
self._data[arraytype.mxnet] = mxnet.ndarray.array(nparray, ctx=mxnet.gpu(0)) # todo on which device ? </s> else:	create_data _logger.info('Copy from numpy array to mxnet array Node#{}'.format(id(self))) nparray = self.get_data(ArrayType.NUMPY) raise UnknownArrayTypeError( 'Array data of type {} unknown.'.format(t))
# todo improve tests - read the output more thoroughly </s> self.assertin(	test_cloud_list_full List all machines with full information ret = self.run_cloud('-f list_nodes_full virtualbox-config') BASE_BOX_NAME + ":", [i.strip() for i in ret]
#todo: check the data! </s> count = 0	test_twitter_caption_search pipe_def = self._get_pipe_def("pipe_eb3e27f8f1841835fdfd279cd96ff9d8.json") p = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def) for i in p: count += 1
# todo: move to base class </s> return self._manipulationmode	Canvas @property def manipulationMode(self): @manipulationMode.setter def manipulationMode(self, value):
# todo: error logging </s> print 'invalid endpoint: %s' % msg_endpoint	raw_message conn = self.get_connection(msg_endpoint) if conn is None: return if msg_type == proto.HEARTBEAT:
# todo: expose from marshal </s> def _r_long(int_bytes):	_r_long XXX Temporary until marshal's long function are exposed. x = int_bytes[0]
raise notimplementederror # the below does most probably not work anymore todo </s> raw_branches = self.git.read_output("branch").split()	local_tickets TESTS:: TODO raw_branches.remove('*') branch_info = [(b, self._ticket[b]) for b in raw_branches
# todo(joshblum): workflow_step slugs may not be unique across </s> for cert_details in workflow_details.get('certifications', []):	_setup_workflows ) test_case.workflows[workflow_details['slug']] = workflow certification = CertificationFactory( slug=cert_details['slug'],
# todo: make this pretty </s> return httpresponse('error retrieving source directories: is the storage server running? please contact administrator.')	grid source_directories = storage_service.get_location(purpose="TS") except: logging.debug("Source directories found: {}".format(source_directories)) polling_interval = django_settings.POLLING_INTERVAL
# todo: not all messages have running status </s> if status_byte < 0x80:	_read_message def _read_message(self, status_byte): dbg('+') dbg('    --- {}'.format('running status')) if self._running_status is None:
#todo : parse unit  25, 25m, 25 ft, etc. </s> except:	seed try: offset, unit = float(htag[:i]), htag[i:].strip() offset = None elif "building:levels" in tags:
# todo: replace with "yield from" when dropping python 2. </s> for stream in parser(stream):	_get_streams parser_name = "HTTP" try: yield stream except IOError as err:
# todo: assign a suitable letter </s> new_id = ann_obj.get_new_id('r')	create_arc mods.change(before, found) else: ann = BinaryRelationAnnotation(new_id, type, origin.id, target.id, '\t') mods.addition(ann)
# todo -- this block is repeated in lots of places, refactor </s> dark_hosts = results.get('dark',{})	_async_poll runner.background = async_seconds results = runner.run() contacted_hosts = results.get('contacted',{}) for (host, error) in dark_hosts.iteritems():
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_transactions_empty def test_fail_transactions_empty(self):
# todo: make targetadaptor return a 'sources' field with an empty snapshot instead of </s> if union_membership.is_member(targetwithsources, target.adaptor) and hasattr(target.adaptor, "sources")	fmt Get(FmtResult, TargetWithSources, target.adaptor) for target in targets ] for result in results:
# todo(datapipe-1509|abrar): currently we have </s> raise importerror	get_base_model try: if env_config.force_avoid_internal_packages: from yelp_conn.session import declarative_base return declarative_base()
# todo: implement </s> can be either registered (and thus, logged in), or only session-based guests	get_shipping_address_from_request Get the shipping address from the request. This abstracts the fact that users
# todo: may be in the future, add a check here to ensure that we are not over-writing any existing file. </s> dump_cmd = 'screenrecord %s --time-limit 10 ' % filepath_on_device	dump_screenrecord def dump_screenrecord(adb_prefix, filepath): filepath_on_device = "/sdcard/screenrecord-%d.mp4" % random.randint(1, 1000 * 1000 * 1000) execute_adb_shell_command(adb_prefix, dump_cmd) pull_cmd = 'pull %s %s' % (filepath_on_device, filepath)
#todo: assuming constant mu </s> self._mfmui = self.mesh.getfaceinnerproduct(1/mu_0)	makeMassMatrices self._MeSigma = self.mesh.getEdgeInnerProduct(m) self._MeSigmaI = sdiag(1/self.MeSigma.diagonal())
transform = self.affine  # todo </s> return window_bounds(transform, window)	window_bounds def window_bounds(self, window):
# todo: rewrite variables with env. variables ( current implementation not final ) </s> if os.getenv('intelmq_is_docker', none):	load_defaults_configuration for option, value in config.items(): setattr(self.parameters, option, value) pipeline_host = os.getenv('INTELMQ_PIPELINE_HOST') setattr(self.parameters, 'destination_pipeline_host', pipeline_host)
max_rates, intercepts = none, none  # todo: determine from gain & bias </s> elif ens.gain is not none or ens.bias is not none:	build_ensemble gain = sample(ens.gain, ens.n_neurons, rng=rng) bias = sample(ens.bias, ens.n_neurons, rng=rng) raise NotImplementedError("gain or bias set for %s, but not both. " "Solving for one given the other is not "
# todo: return errors in a universal way </s> subprocess.run(["nasm", "-f", "elf64", "-o", "out.o", "out.s"]).check_returncode()	main return s_file.close() subprocess.run(["ld", "out.o", "-o", "out"]).check_returncode()
# todo: need to update this so that it flushes bulk queue </s> print "need to implement this!"	get_cluster_health return escluster.health() else: raise NotImplementedError
# todo: update the review with a message </s> gitcontext, review_branch, working_branch)	processUpdatedRepo abdt_workingbranch.pushBadInReview(
# todo support going from customer encryption to other forms </s> if kms_key_id:	encrypt else: copy_source = {'Bucket': bucket.name, 'Key': obj.key} obj.copy_from( CopySource=copy_source,
bufferview = gltf.data.buffer_views[accessor.buffer_view] # todo initialize with 0 when not present! </s> if bufferview.buffer in gltf.buffers.keys():	get_binary_from_accessor def get_binary_from_accessor(gltf, accessor_idx): accessor   = gltf.data.accessors[accessor_idx] buffer = gltf.buffers[bufferView.buffer] else:
#todo: this isn't actually most_recently_used (as defined in histories) </s> if( ( trans.user == none )	show hda_dict = {} try: and ( history_id == trans.security.encode_id( trans.history.id ) ) ): history = trans.history
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_interact_arguments "-g", "--gain", metavar="GAIN", type=int, choices=(32, 64, 128), default=128, help="amplify GAIN times (64 or 128 for channel A, 32 for channel B)") p_measure = p_operation.add_parser( "measure", help="read measured values")
# todo results from ml </s> return	_get_prev_role_confidences def _get_prev_role_confidences(endpoint):
# todo improve tests - read the output more thoroughly </s> self.assertin(	test_cloud_list List all machines in virtualbox ret = self.run_cloud('-f list_nodes virtualbox-config') BASE_BOX_NAME + ":", [i.strip() for i in ret]
'units': '1',  # todo: where does this come from??? </s> 'dtype': stat['dtype'],	do_stats for stat in config['stats']: measurements = [{'name': measurement, 'nodata': stat['nodata']}] results[stat['name']] = nco_from_sources(task['data']['sources'],
#todo: check the data! </s> count = 0	test_filtered_multiple_sources pipe_def = self._get_pipe_def("pipe_c1cfa58f96243cea6ff50a12fc50c984.json") p = pipe2py.compile.parse_and_build_pipe(pipe_def, verbose=True) for i in p: count += 1
# todo: copy icons into directory </s> return path	_make_native_kernel_dir }, f)
pass # todo </s> def test_getitem_projection(self):	test_getitem_projection
# todo(dcramer): deal with case when the user cannot create orgs </s> organization = self.get_active_organization(request)	get def get(self, request): if organization is None: url = reverse('sentry-create-organization')
# todo(b/142684737): the multi-processing api might change. </s> beam_pipeline_args=['--direct_num_workers=%d' % direct_num_workers],	_create_pipeline metadata_connection_config=metadata.sqlite_metadata_connection_config( metadata_path),
# todo: why is this late imported </s> import grp	chownbyname uid = pwd.getpwnam(user).pw_uid if group: gid = grp.getgrnam(group).gr_gid os.chown(fname, uid, gid)
pass # todo </s> def _password(self, password):	_password @register(r'^password "(?P<password>[^"]+)"$')
# todo: test logging messages. </s> self.manager.startup()	test_startup def test_startup(self): self.assertEquals(self.states, [])
a = 0 #todo wrong </s> nn.init.kaiming_uniform_(layer, mode="fan_in", nonlinearity=options["gain"])	layer_initializer nn.init.xavier_uniform_(layer, gain=gain) elif args[0] == "kaiming_uniform": elif args[0] == "kaiming_normal": a = 0 #TODO wrong
# todo: remove when #980 has been merged </s> info.update(formats[-1])	_real_extract 'thumbnail': video_thumbnail, } return info
# todo: handle agg_columns. </s> with self.assertraisesregex(typeerror, "<class 'int'> object is not callable"):	test_apply pdf.groupby(pdf.b // 5)["a"].apply(lambda x: x + x.min()).sort_index(), ) kdf.groupby("b").apply(1) columns = pd.MultiIndex.from_tuples([("x", "a"), ("x", "b"), ("y", "c")])
# todo: build url with python </s> return redirect('/admin/django_q/schedule/add/?name=system_importer_file_csv_cron_based&func=dfirtrack_main.importer.file.csv_cron_based.system')	config_check_cron return redirect(reverse('system_list')) else:
# assume ethernet, todo: infiniband, wifi, vlan </s> slave_name = slave	add_connection_for_ksdata values.append(['team', 'config', networkdata.teamconfig, 's']) for i, (slave, cfg) in enumerate(networkdata.teamslaves): svalues = [] suuid =  str(uuid.uuid4())
return 0  # todo: aria2 doesn't provide this information </s> def creationtime(self):	creationTime @pyqtProperty(int, notify = initialized)
# todo check transformation to the reference element </s> x_1 = mesh.coors[:-1] + (mesh.coors[1:] - mesh.coors[:-1]) * (-nm.sqrt(1 / 2) + 1) / 2	intGauss2 @staticmethod def intGauss2(mesh, f): x_2 = mesh.coors[:-1] + (mesh.coors[1:] - mesh.coors[:-1]) * (nm.sqrt(1 / 3) + 1) / 2 w = (mesh.coors[1:] - mesh.coors[:-1]) / 2
# todo: need to cleanup the named argument mess before it is possible. </s> pass	OptimizeFunctionCallArgsVisitor if star_dict_arg is not None: if star_dict_arg.isExpressionMakeDict(): elif star_dict_arg.isExpressionConstantRef(): pass
# @todo: save the results for the onaccept </s> for record in rows:	inv_kit_onvalidate quantity = vars.quantity max_kits = 0 one_kit = record.quantity * ptable[record.item_pack_id].quantity stock_amount = 0
#                assert false # todo </s> else:	execute hdf5File.close() opH5Writer.cleanUp() assert False, "Unknown export format" result[0] = not self.Dirty.value
# todo: fix with stubber / before send event </s> with mock.patch('botocore.endpoint.endpoint._send') as _send:	test_get_export 'accepts': 'application/yaml' } _send.return_value = mock.Mock( status_code=200, headers={}, content=b'{}')
# todo: move 'vote-%d.%d.%s' to settings or something </s> cookie_name = 'vote-%d.%d.%s' % (kwargs['content_type'].pk, kwargs['object_id'], kwargs['key'][:6],) # -> md5_hexdigest?	get_rating_for_user use_cookies = (self.field.allow_anonymous and self.field.use_cookies) if use_cookies: cookie = cookies.get(cookie_name) if cookie:
# todo: check that the birth date is not in the future </s> except valueerror, e:	is_valid try: birth_date = _get_birth_date(number) return False if len(number) == 10:
return -1  # todo: followup after decision around returning none </s> "cannot access _eprocess.objecttable.handlecount at {0:#x}".format(self.vol.offset))	get_handle_count vollog.log(constants.LOGLEVEL_VVV,
# todo: check error location </s> assert result.data == {'nest': none}	test_non_nullable_list_of_non_nullables assert len(result.errors) == 1 assert result.errors[0].message == 'Cannot return null for non-nullable type.' result = run(type, None) assert len(result.errors) == 1
# todo: test for the _correct_ revision_id value. </s> if not activity.revision_id:	_update_resource if not activity.id: assert False, "activity object has no id value" assert False, "activity has no revision_id value" assert activity.timestamp >= before and activity.timestamp <= after, \
pass # todo </s> def search(self, field, what):	LibspotifyLibraryController return LibspotifyTranslator.to_mopidy_track(spotify_track) def refresh(self, uri=None): if field == u'any': query = what
# todo(b/145514490): this is a bit heavy handed, there maybe caches where </s> self._cache = {}	CachingExecutor await cached_value.target_future except Exception as e: raise e return cached_value
# @todo: test </s> res = dropsource.dodragdrop()	startDrag dropSource.SetData(data)
# todo(soren): we need this until we can stop polling in the rpc code </s> greenthread.sleep(0.3)	test_ajax_console self.assertEquals(b64decode(output['output']), 'http://fakeajaxconsole.com/?token=FAKETOKEN') rv = yield self.cloud.terminate_instances(self.context, [instance_id])
# todo: can't do this until we find a way to link with the </s> return self.dialect_impl(dialect).result_processor(dialect, coltype)	_cached_result_processor def _cached_result_processor(self, dialect, coltype):
# todo: should also assert that the task is in the expected state once that's hooked up </s> assert mock_log_event_info.called	test_handle_event_log_event_info_exception mock_event_factory(task_id=mock_kubernetes_task.get_kubernetes_id(), platform_type="running") )
# todo(neuberg): this will need to be adapted against an already trained weight </s> trained_weight_file = get_trained_weight_file()	train generate_parsed_logs() (training_details, validation_details) = parse_logs()
# todo: pytest.warns is not supported until pytest >= 2.8.0, whose </s> assert warnings.warn.call_args_list == [call(any, deprecationwarning)]	test_spawn_managed_thread_backwards_compat_warning Event().wait() container.spawn_managed_thread(wait, *args, **kwargs) container.kill()
# todo: determine if this puts the case properties in the expected order. </s> case_properties={	handle case_name=hidden_value_path, case_type='task', 'task_responsible': '/data/task_responsible', 'task_due': '/data/task_due',
# todo a more reliable way of getting the windows location </s> cudnn_checkfile = os.path.join(self._cuda_path, "include", "cudnn.h")	cudnn_checkfiles_windows def cudnn_checkfiles_windows(self): return [cudnn_checkfile]
# todo: add option for attentive reader </s> print('trainable variables (only embeddings): %d' % get_total_trainable_variables())	boe_nosupport_cands_reader_model varscope.reuse_variables() candidates_embedded = nvocab(candidates) question_encoding = tf.reduce_sum(question_embedded, 1) scores = logits = tf.reduce_sum(tf.expand_dims(question_encoding, 1) * candidates_embedded, 2)
# todo: do the computation without the 'sr' enforcement </s> mat_expr = matrix([[mat[i,j].expr(method='sr') for i in range(self._nc)]	jacobian_det raise ValueError("the Jacobian matrix is not a square matrix") mat = self.jacobian() for j in range(self._nc)]) det = mat_expr.det() # the unsimplified determinant
# todo(jheek): consider re-introducing the tracer check </s> return ()	_masters def _masters():
# todo implement through browser </s> step_message = 'verify an alert is not present'	verify_alert_is_not_present def verify_alert_is_not_present(): execution.logger.info(step_message) _capture_or_add_step(step_message, execution.settings['screenshot_on_step'])
assert value == '' or value.isdigit(), 'bad call'  # todo remove assertion </s> set_session_view_value(view, 'action_count', value)	set_action_count def set_action_count(view, value: str) -> None:
# todo: add the rest of the api actions here and call them directly from the api controller </s> self.shareable_service = sharable.shareableservice(self.manager, self.serializer)	__init__ self.serializer = serializer
# todo: test on linux, assuming same as macos right now </s> pass	showWindow pass elif sys.platform == 'linux': else: print("Warning: Unhandled sys.platform: ", sys.platform)
# (@todo: add a js i18n formatter for the tooltips) </s> represent = row["_row"][fieldname]	get_location_data represent = s3_unicode(represent) else: elif ftype in ("double", "float"): represent = row["_row"][fieldname]
@jtu.skip_on_devices("tpu")  # todo(phawkins): re-enable </s> def testt(self, df, dtype):	testT for df in [0.1, 1., 10.] for dtype in [onp.float32, onp.float64])) key = random.PRNGKey(0) rand = lambda key, df: random.t(key, df, (10000,), dtype)
#! todo: this needs to be made more intelligent </s> if (omega == none):	gangof4 raise NotImplementedError("Gang of four is currently only implemented for SISO systems.") else: omega = default_frequency_range((P,C)) L = P*C;
annot.annotation_metadata.validation_and_reliability = "todo" #todo </s> annot.annotation_metadata.origin = metadata[1]	fill_annotation annot.annotation_metadata.annotation_tools = "Sonic Visualizer" annot.annotation_metadata.annotation_rules = "TODO" #TODO annot.annotation_metadata.annotator.name = metadata[annotation_id + 2] annot.annotation_metadata.annotator.email = "TODO" #TODO
# todo: common crud method </s> user = await self.middleware.call('datastore.query', 'account.bsdusers', [('id', '=', pk)], {'prefix': 'bsdusr_'})	UserService ) async def do_update(self, pk, data): if not user: raise ValidationError(None, f'User {pk} does not exist', errno.ENOENT)
# todo(b/110096942): more efficient gather </s> return _xla_split(c.getshape(x), x)	xla_split None, dims[1:])
# todo(jflesch): i18n/l10n </s> msg = "this may take a very long time\nare you sure ?"	_redo_ocr_on_all def _redo_ocr_on_all(self, src = None): confirm = gtk.MessageDialog(parent = self.mainWindow, flags = gtk.DIALOG_MODAL | gtk.DIALOG_DESTROY_WITH_PARENT,
# @todo: threadpool().map() </s> results = packagedb.get_print_format_output(pacman_args + [pkg_name])	_get_repo_pkgs_info pkg_install_infos = [] for pkg_name in pkg_names[:]: if not results: self.not_found_repo_pkgs_names.append(pkg_name)
# todoc: i have no clue what this is doing </s> addr = ''.join((addr[x*3:x*3+2] for x in xrange(0,6)))	EthAddr if addr[2::3] != ':::::' and addr[2::3] != '-----': raise RuntimeError("Bad format for ethernet address") elif len(addr) == 12: pass
# todo curves </s> for i in range(len(commands)):	specializeCommands op,args = commands[i] if op in {'rmoveto', 'rlineto'}:
# todo @chris ... </s> weight is a float, in -log space	ctc_fsa_for_label_seq label_idx >= 0 and label_idx < num_labels  --or-- label_idx == num_labels for blank symbol
# todo: posts_per_year is global, kill it </s> for year, posts in self.site.posts_per_year.items():	gen_tasks } template_name = "list.tmpl" for lang in kw["translations"]: output_name = os.path.join(
# todo: update user icon on button to user avatar </s> pass	setPavloviaUser def setPavloviaUser(self, user):
# todo ??? other type of cases </s> def test_check_version_pass():	test_check_version_pass for version in [ (1, 2, 3, 'dev', 5),
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
# todo: should export bytesio as stringio in libcloud.utils.py3 </s> if py3:	decompress_data return zlib.decompress(data) elif compression_type == 'gzip': from io import BytesIO cls = BytesIO
# todo delay </s> async_create_case(upload._id)	sonosite_upload name, ) response_data['result'] = 'uploaded' response_data['message'] = 'uploaded'
"size": 50  # todo: support pagination. </s> }	elastic_project_search ] }, kwargs['routing'] = project_slug results = PageIndex().search(body, **kwargs)
# todo remove backwards compatability patch in version 2.0 </s> if build_version < 11100:	_init_backwards_compat_patches preferences.set('vintageous_use_super_keys', preferences.get('vintageous_use_super_keys')) preferences.set('neovintageous_build_version', 11000) old_file = os.path.join(sublime.packages_path(), 'User', '.vintageousrc') new_file = os.path.join(sublime.packages_path(), 'User', '.neovintageousrc')
# todo: use ctx.send_help() once pr #519 is merged. </s> help_command = await self.get_help_command(ctx.command)	ErrorHandler return async def handle_user_input_error(self, ctx: Context, e: UserInputError) -> None: if isinstance(e, BadArgument): await ctx.send(f"Bad argument: {e}\n")
# todo: if an attachment is filtered, the score is not complete </s> attachments = mailattachments(attachments)	check_phishing subject = email.get('subject') from_ = email.get('from') urls = ( (urls_body, 'urls_body'),
# todo simplify </s> if self._dsm.model_type is 'classification':	ensemble def ensemble(self): self._dsm.ensemble_classification(scoring_metric='roc_auc') elif self._dsm.model_type is 'regression':
# todo: add 3ph sgens </s> stor = net["storage"]	_calc_pq_elements_and_add_on_ppc p = np.hstack([p, sgen["p_kw"].values * vl]) b = np.hstack([b, sgen["bus"].values]) if not stor.empty: stor["controllable"] = _controllable_to_bool(stor["controllable"])
# todo ensure that if you try to filter on an invalid field, it returns a useful error. </s> return_val = [item for item in default_queryset if value in item.get('key', none)]	param_queryset return_val = [item for item in default_queryset if item.get('key', None) == value] else: return return_val
# todo: move the inside snippets to the corresponding snippets dict </s> if value and '()' in value:	generate_snippet if template == 'full': data = generate_snippet_parts(expanded, options) if value.replace('()', '') in ['rotate','rotateX','rotateY','rotateZ','skew','skewX','skewY']: value = value.replace('()', '($1${1/^((?!0$)-?(\d*.)?\d+)?.*$/(?1:deg)/m})')
# todo: this is temporary until the function argument list is refactored to work with kwargs only. </s> self.shapes.append((zip(lats, lngs), {	polygon kwargs.setdefault("color", color) kwargs.setdefault("c", c) 'edge_color': _get_value(kwargs, ['color', 'c', 'edge_color', 'ec'], '#000000'), 'edge_alpha': _get_value(kwargs, ['alpha', 'edge_alpha', 'ea'], 1.0),
## slightly different api - todo test if this works </s> self[...].setdamping( {x:x, y:y, z:z}, none )	setLinearDamping with javascript:
# todo(dcramer): ideally we could just send the signal to the subprocess </s> self.task.status = taskstatus.failed	_read_timeout self._logreporter.terminate() self._logreporter.save_chunk('>> Process did not receive updates in %ds\n' % self.read_timeout) self.task.date_finished = datetime.utcnow() db.session.add(self.task)
# todo: for now, circumnavigate the detached head issue. </s> for subds in source.get_dataset_handles(recursive=true):	test_publish_simple def test_publish_simple(origin, src_path, dst_path): source = install(path=src_path, source=origin, recursive=True) AnnexRepo(opj(src_path, subds), init=True, create=False).git_checkout("master") target = GitRepo(dst_path, create=True)
# todo(b/184055743): once tensorflow is released with </s> }	test3dSparseWithTFXIO name: "x$sparse_indices_0" type: INT feature { name: "x$sparse_indices_1"
raise notimplementederror # todo </s> list every item in entry_point that match request	search def search(self, entry_point, request):
# todo: adjust edges </s> xx, yy = self._get_meshgrid(xmin, xmax, ymin, ymax)	_plot_proba ymin = y.min() ymax = y.max() Xfull = np.c_[xx.ravel(), yy.ravel()] Xfull = self._maybe_inverse(Xfull)
if not config.testnet:  # todo </s> return	compose def compose (db, source, contract_id, gasprice, startgas, value, payload_hex): block = blocks.Block(db, util.last_block(db)['block_hash']) code = block.get_code(contract_id)
pass  # todo </s> self._current_command = []	__init__ self._device = None
# todo add something like this in the future, its cleaner than the </s> return names	_sub_modules name.parent = imp names.append(name)
# todo support multiple backends </s> self.backends[0].stored_playlists.playlists = playlists	playlists @playlists.setter  # noqa def playlists(self, playlists):
### todo put memozation here </s> with file_transaction(out_dir) as tx_out_dir:	salmon_index gtf_fa = sailfish._create_combined_fasta(data, out_dir) tmpdir = dd.get_tmp_dir(data) cmd = "{salmon} index -k 31 -p {num_cores} -i {tx_out_dir} -t {gtf_fa}" message = "Creating Salmon index for {gtf_fa}."
#set limit to 25 --> currently hardcoded --> todo: add a setting for this ? </s> subelement(root, "limit").text = "25"	addVideoNodesForTag SubElement(Rule, "value").text = tagname SubElement(root, "order", {"direction":"descending"}).text = "dateadded" Rule2 = SubElement(root, "rule", {"field":"playcount","operator":"is"}) SubElement(Rule2, "value").text = "0"
# todo: the defs should be util or elsewhere... </s> def vector(atm1, atm2):	vector return atm1 - atm2
# todo: context manager </s> h = csv.writer(open(filename, 'w'))	export_status :param data: The data to generate the file from :param filename: The target file h.writerow(['Category', 'Value']) for thing in status(data):
# todo: disconnect </s> return	Node except asyncio.TimeoutError: await stream.reset() self.logger.debug(f"Received the hello message {hello_other_side}") if not (await self._validate_hello_req(hello_other_side)):
walk_related=false,  # todo i'm not sure what this should be </s> )],	get_voucher_case_structure related_type='prescription',
# todo: i am not at all sure why we need to </s> comp._autodisjuncts.construct()	_set_value_impl unique_component_name(b, comp.local_name), comp._autodisjuncts ) disjunct = comp._autodisjuncts[len(comp._autodisjuncts)] disjunct.constraint = Constraint(expr=e)
# todo: only do this if needed (depending on the storage backend the whole file will be downloaded) </s> try:	save self._move_file() self._old_is_public = self.is_public self.generate_sha1() except Exception, e:
# todo: remove this logging statement when all other todos </s> logger.error("in check_valid, join is not fully implemented")	check_valid LOGGER.error("in check_valid, CREATE is not fully implemented") elif self._action == 'JOIN': elif self._action == 'FIRE': if self._name not in store:
#@todo: remove in 0.4.10 </s> def error(self, reason=none, type="parse"):	error raise Fail("%s error%s | Plugin out of date" % (type.capitalize(), ':' + str(reason) if reason else "")) if self.core.debug:
# todo consolidate this and pr plotter into 1 function </s> colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']	roc_plot_from_predictions def roc_plot_from_predictions(y_test, y_predictions_by_model, save=False, debug=False): plt.figure() plt.xlabel('False Positive Rate')
# todo: need to add counter </s> return true	send_message delay.small_delay(self) if super(self.__class__, self).sendDirectItem('message', user_ids, text=text, thread=thread_id): self.logger.info("Message to {user_ids} wasn't sended".format(user_ids=user_ids)) return False
# todo: add highlighting line </s> else:	mouseDoubleClickEvent if new_line >= 0: self.verticalScrollBar().setValue(new_line) self.read_memory(new_pos) else:
return  # todo return placeholder "[unavailable]" track? </s> return models.track(	to_track return  # TODO Return placeholder "[error]" track? if sp_track.availability != spotify.TrackAvailability.AVAILABLE: uri=sp_track.link.uri, name=sp_track.name,
# todo use libssl if available </s> result.bytes = cdn_aes.encrypt(result.bytes)	download_file return getattr(result, 'type', '') if cdn_redirect: f.write(result.bytes) if progress_callback:
# for ~otheruser/src.  todo: should this be cached? </s> try:	_EvalTildeSub assert val.tag == value_e.Str, val return val.s e = pwd.getpwnam(prefix) except KeyError:
# todo model? </s> cut_result_json = os.path.join(data_home, 'cut_result.json')	classify compress_rate: float = 0.2, limit: int = None): res = None stable = None
# logger ..." todo: this should be done before plugins discovery </s> for directory in config.config_dir, config.work_dir:	main args = create_parser(plugins, cli_args).parse_args(cli_args) config = configuration.NamespaceConfig(args) le_util.make_or_verify_dir( directory, constants.CONFIG_DIRS_MODE, os.geteuid())
# todo ... </s> return []	get_depfiles def get_depfiles(outfile):
# todo actions may contain wildcards, e.g. sts:* -- this can be solved by using policyuniverse to </s> if action == "sts:assumerole":	_find_roles_assumable_in_policy actions = [actions] for action in actions: if statement["Effect"] == "Allow": role_arns = statement["Resource"]
# todo: this is a jump. </s> vi_cmd_data['motion']['command'] = '_vi_quote'	vi_quote def vi_quote(vi_cmd_data): vi_cmd_data['motion']['args'] = {'character': vi_cmd_data['user_motion_input'], 'mode': vi_cmd_data['mode']} vi_cmd_data['count'] = 1
# todo: set file creation mask to 640 </s> shutil.move(npath, upssched_conf)	update_upssched_early_shutdown if not stop_timer_found: outo.write(timer_stop_pattern) run_command([CHOWN, 'root.nut', UPSSCHED_CONF]) run_command([CHMOD, '640', UPSSCHED_CONF])
# todo: add logging </s> pass	parse_intent def parse_intent(self, token='ri', params='n'):
#todo(#212): use a map construct instead of unrolling. </s> lhs = batching.move_dim_to_front(lhs, lhs_bdim)	conv_general_dilated_batch_rule lhs_dim, rhs_dim, out_dim = dimension_numbers if lhs_bdim is not None and rhs_bdim is not None: rhs = batching.move_dim_to_front(rhs, rhs_bdim) outputs = [
# todo: why is this failing? </s> expected_json_output = file.read()	FlaskRoutesTest with open(filepath, "r") as file:
raise skiptest("broken test")  # todo(mattjj): fix </s> if not config.omnistaging_enabled:	test_random_split_doesnt_device_put_during_tracing def test_random_split_doesnt_device_put_during_tracing(self): raise SkipTest("test is omnistaging-specific") key = random.PRNGKey(1)
# todo(andym): delete once personas migration is live. </s> class testprepersonadetailpage(testpersonadetailpage):	TestPrePersonaDetailPage def setUp(self): self.addon = Addon.objects.get(id=15663)
return skiptest("test doesn't pass yet")  # todo(frostig) </s> def fun(x):	testLogSoftmax def testLogSoftmax(self): return x - np.log(np.sum(np.exp(x))) pfun, axis_name = papply(fun, 5)
# todo add switch to make tarball/zip </s> type='directory',	__call__ status='ok', path=output, action='bids2scidata', logger=lgr)
# todo: i think this should use '$ fileregions' </s> return self.id1.get_segment(ea).bounds.start	SegStart def SegStart(self, ea):
# todo: test bytearray </s> if not isinstance(prefix, (bytes, bytearray)):	address versionbyte = self.network.prefix_address else: versionbyte = binascii.unhexlify(prefix) else:
# todo: same as above, what if there's no splitfrac </s> gofx.append(value(mem * sf))	generate_gofx for name, mem in src.iter_vars(names=True): if src.is_extensive(name) and sf is not None: else: gofx.append(value(mem))
# todo: remove when #980 has been merged </s> info['formats'][-1]['ext'] = determine_ext(info['formats'][-1]['url'])	_real_extract 'formats': formats, } info.update(info['formats'][-1]) return info
# todo: refactor linodeexception, args[0] should be error_id </s> if e.args[0] == 5:	delete_zone data = self.connection.request(API_ROOT, params=params).objects[0] except LinodeException, e: raise ZoneDoesNotExistError(value='', driver=self, zone_id=zone.id)
blockchain.connect()  # todo: leave this here? </s> if not funding_address:	stake registry=registry, poa=poa) funding_address = select_client_account(blockchain=blockchain) new_stakeholder = StakeHolder(config_root=config_root,
# todo support for keccack </s> key = _standardize_path(path)	compile_from_input_dict contract_sources: ContractCodes = {} for path, value in input_dict['sources'].items(): contract_sources[key] = value['content'] interface_sources: ContractCodes = {}
# todo: parallelize this loop </s> for blockstart in blockstarts:	execute def execute(self, slot, subindex, roi, result): blockstarts = self.getIntersectingBlocks( (roi.start, roi.stop) ) blockroi = self.getBlockIntersection( blockstart, (roi.start, roi.stop) ) resultSlice = roiToSlice(blockstart + blockroi[0], blockstart + blockroi[1])
# todo: handle "other" </s> return cls(elem.attrib,	Sequence @classmethod def from_element(cls, elem): symbol=check_str(get_child_text(elem, 'symbol'), r'\S{1,10}'), accession=get_child_as(elem, 'accession', Accession),
# todo: remove this logging statement when all other todos </s> logger.error("in check_valid, join is not fully implemented")	check_valid LOGGER.error("in check_valid, CREATE is not fully implemented") elif self._action == 'JOIN': elif self._action == 'FIRE': if self._name not in store:
# todo _cphttptools.applyfilterlist('afterrequestbody') </s> _cphttptools.dorequest(self.wfile)	do_GET _cphttptools.parseFirstLine(self.raw_requestline) self.cook_headers()
#ack = self.serialport.read() # todo: use ack </s> self.sendcommand(141) # 10001101	enable def enable(self):
# todo: connect to agent code </s> print('set mission: ' + self.missionbox.toplaintext())	setMission def setMission(self):
# todo: may test file contents </s> temp = tempfile.namedtemporaryfile(delete=false)	test_export_to_json_fobj def test_export_to_json_fobj(self): self.files_to_delete.append(temp.name) rows.export_to_json(utils.table, temp.file)
percentiles_to_calculate = range(0, 100, 1)  # todo: get input from user </s> headers = constants.submetric_header + ',mean,std,p50,p75,p90,p95,p99,min,max\n'  # todo: this will be built from user input later on	calculate_stats def calculate_stats(self): stats_to_calculate = ['mean', 'std', 'min', 'max']  # TODO: get input from user metric_stats_csv_file = self.get_stats_csv() imp_metric_stats_csv_file = self.get_important_sub_metrics_csv()
# todo: earlier warning in conf check ? </s> logger.warning('inventory source slug %s exists already', key)	serialize_machine key = slugify(source.name) if key in ms_d: machine_d[key] = ms_d for tag in self.machine.tags:
# todo: move this wrapping logic into a common templatetag. </s> for i in range(0, len(base64), 64):	ssh_settings fingerprint = sshutils.humanize_key(key) base64 = key.get_base64() public_key += base64[i:i + 64] + '\n' else:
# todo: if clang will be extended with an extra analyzer option in </s> flags_with_path = ['-i', '-idirafter', '-imacros', '-imultilib',	__collect_compile_opts next(flag_iterator) param = flag_iterator.item '-include', '-iquote', '-isysroot', '-isystem', '-iwithprefix', '-iwithprefixbefore', '-sysroot',
# todo -- can we do this without a subscription? </s> if not api.use_store:	play_similar_song_radio @ask.intent("GeeMusicPlaySimilarSongsRadioIntent") def play_similar_song_radio(): return statement(render_template("not_supported_without_store")) if len(queue.song_ids) == 0:
# todo make sure this works </s> log("creating sensors...", 'info')	buildModelFromDictionary rootlink['modelname'] = model['name'] rootlink.location = (0, 0, 0) if 'sensors' in model and model['sensors']: for sen in model['sensors']:
# todo: add some unit tests for this. </s> def resolve(context, name):	resolve Resolve the given name against the given context stack. This function follows the rules outlined in the section of the spec
# todo: add support for composite primary keys. </s> self.id_attribute = meta.get(	__init__ super(PeeweeManager, self).__init__(resource, model) meta = resource.meta 'id_attribute', model._meta.primary_key.name) if 'id_field' in resource.meta:
# todo: what about alpha (rgba)? </s> output_name = "denoised"	draw refresh_denoised = (time() - self.last_denoiser_refresh) > scene.luxcore.denoiser.refresh_interval if "DENOISED" in engine.aov_imagepipelines and refresh_denoised: output_type = pyluxcore.FilmOutputType.RGB_IMAGEPIPELINE was_paused = session.IsInPause()
# todo: skipped due to gh-4436 </s> yield skip_if_on_windows(skip_ssh(_test_initremote_basic)), 'datalad-test'	test_initremote_basic def test_initremote_basic(): yield _test_initremote_basic, None
# todo(anjalisridhar): you can pass test name and sample type when creating </s> def __init__(self):	Cifar10CnnBenchmark class Cifar10CnnBenchmark(BenchmarkModel): self._test_name = "cifar10_cnn" self._sample_type="images"
# todo: decouple code generator. perhaps allow  pydy and/or pyodesys </s> self.output_equation_function = lambdify_with_vector_args( \	output_equations if self.n_states: assert find_dynamicsymbols(output_equations) <= set(self.states) [dynamicsymbols._t] + sp.flatten(self.states), \ output_equations.subs(self.constants_values), modules="numpy")
pass # todo </s> @register(r'^addid "(?p<uri>[^"]*)"( (?p<position>\d+))*$')	MpdHandler @register(r'^add "(?P<uri>[^"]*)"$') def _add(self, uri): def _add(self, uri, position=None): pass # TODO
setattr(model, "require_backward_grad_sync", false)  # todo: needed? </s> return [model], optimizers	_setup_models_and_optimizers optimizers = self._wrap_optimizers(optimizers) model = ShardedDataParallel(models[0], sharded_optimizer=optimizers, **self._ddp_kwargs)
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_movlpd reil_instrs = self.__asm_to_reil(asm, address) reil_ctx_out, _ = self.reil_emulator.execute(reil_instrs, start=address << 8, registers=ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo remove? </s> children = self.children	get_description def get_description(self): if children[-1] == ',': children = children[:-1]
# todo: if empty, add 'pass' </s> except:	uncompyle_find if ast[-1] == walker.RETURN_NONE: ast.pop() # remove last node pass walk.mod_globs = walker.find_globals(ast, set())
# todo: switch _ignore_connection_reset for _ignore_transmission_error, or provide retry mechanism </s> if self._ignore_connection_reset:	_fuzz_current_case callback_data = self._callback_current_node(node=self.fuzz_node, edge=path[-1]) except sex.BoofuzzTargetConnectionReset: self._fuzz_data_logger.log_info("Target connection reset.") else:
match_mode = datawalker.repeat  # todo should be determined by modes of input walkers </s> max_value_len = max(w.next_values_number for w in walkers)	flat_walk_data def flat_walk_data(*walkers): [w.step_down_matching(max_value_len, match_mode) for w in walkers] while any(not w.is_exhausted for w in walkers):
# todo: do we require graphs with no nodes/edges to have the same schema?  currently </s> ret_feat = _batch_feat_dicts(frames, ndata, 'nodes["{}"].data'.format(ntype))	batch g._node_frames[ntype_id] for g in graphs if g._graph.number_of_nodes(ntype_id) > 0] retg.nodes[ntype].data.update(ret_feat) if edata is not None:
# todo: support aa </s> aa = []	join po: Array = xp.vstack([x.po for x in sub_acts]) ah = list(map(xp.vstack, zip(*[x.ah for x in sub_acts]))) return cls(lh, po, ah, aa)
# todo : documentation pending </s> if sess:	assign_tf_variable def assign_tf_variable(variable, value, sess=None): assign_op = variable.assign(value) sess.run(assign_op)
# todo: implement </s> record0.write(pack('>i', 0xffffffff))	_generate_record0 0xffffffff, 0xffffffff, 0, 0)) record0.write('\0'*12) record0.write('\0'*4) record0.write(pack('>I', 0xffffffff))
# todo ... </s> if isinstance(token, cclosingbracket):	cpre3_parse stateStruct.error("cpre3 parse: unexpected token " + str(token) + " in base state") elif state == 1: # "(" bracket if token.brackets == curCObj._bracketlevel: state = 0
#todo classes broken </s> sample = self.sess.run(generator, feed_dict={ z_t: z})	sample_zeros z = np.ones(z_t.get_shape())*2 print("generator is ", generator) stacks = [np.hstack(sample[x*8:x*8+8]) for x in range(4)] plot(self.config, np.vstack(stacks), sample_file)
#todo: add way to check if alt is pressed </s> self.currenttool.flip(blocksonly=true)	key_down getattr(self.currentTool, name)() elif keyname == config.config.get('Keys', 'Flip'): elif keyname == config.config.get('Keys', 'Roll'): self.currentTool.roll(blocksOnly=True)
# todo: shouldn't it take place only on paste? </s> if self._propose_remove_line_numbers and isinstance(args[1], str):	TextWrapper if text >= "\uf704" and text <= "\uf70d": # Function keys F1..F10 in Mac cause these return args = tuple((args[0],) + (try_remove_linenumbers(args[1], self.text),) + args[2:]) self._original_user_text_insert(*args, **kw)
# todo: test the size when the field is new </s> self.assertequal(self.rt2.get_header_size(),32)	test_09_rate_field self.assertEqual(self.rt2.get_size(),len(self.frame_orig_2))
raise notimplementederror #todo, implement! </s> def testclass(self,cls):	testclass
# todo this is not tested yet. </s> print 'not tested'	quantile def quantile(t, a, b, p): L = np.power((t+.0)/ a,b) quantile = a * np.power(-np.log(1. - p) - L,1. / b)
#todo: call _update_node_parents and _update_node_rule_for_parents </s> nx.digraph.add_edge(self, u, v)	add_edge raise TypeError("Name of nodes must be strings")
# todo(b/130724878): these conversions should not be needed. </s> obj_2 = obj.from_anon_tuple(server_state, 2)	test_load_latest_state export_dir = os.path.join(self.get_temp_dir(), 'ckpt_1') checkpoint_utils.save(obj_1, export_dir) export_dir = os.path.join(self.get_temp_dir(), 'ckpt_2') checkpoint_utils.save(obj_2, export_dir)
pass # todo </s> def invert_selection(self):	invert_selection
# todo generator </s> if unavailable_paths:  # and likely other error flags	__call__ _get(content_by_ds, refpath=dataset_path, source=source, jobs=jobs, get_data=get_data))) if _return_datasets: results = sorted(set(content_by_ds).difference(unavailable_paths))
# todo(skeen): think of a better solution </s> if not addon.eula:	eula def eula(request, addon_id, file_id): addon = get_object_or_404(Addon.objects.valid(), id=addon_id) return http.HttpResponsePermanentRedirect(reverse( 'addons.detail', args=[addon.id]))
assert oldsize == self.size, '%s: %d, %d' % (self.type, oldsize, self.size) # todo: remove </s> return self.size	stss_atom oldsize = self.size # TODO: remove self.size = 8 + 4 + 4 + len(self.body[1]) * 4
# todo: disconnect </s> return	Node except asyncio.TimeoutError: await stream.reset() self.logger.debug(f"Received the hello message {hello_other_side}") if not (await self._validate_hello_req(hello_other_side)):
# todo: if empty, add 'pass' </s> except:	uncompyle_find if ast[-1] == walker.RETURN_NONE: ast.pop() # remove last node pass walk.mod_globs = walker.find_globals(ast, set())
# @todo: handle lama correctly </s> if not memory_type:	create_from_params state_shape = (state_shape, ) if len(state_shape) in [1, 2]: state_size = reduce(lambda x, y: x * y, state_shape) else:
ndpi = (96, 96) # todo: read real dpi </s> debug_out("input dpi = %d x %d"%ndpi)	main debug_out("input dpi (forced) = %d x %d"%ndpi) else: if colorspace: color = colorspace
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_error_no_digests digests. I mean, why even bother, right?
# todo: this should not be hard coded </s> if len(path) == 2 and path[0] in ['tasks', 'presets']:	with_path except LookupError as e: return unicode(e), 404 result.headers[b'Content-Type'] += '; profile=/schema/plugins?context=task' return result
# todo: write tests </s> use :func:`_attrtranslator` to convert the value of a "left" or "right" attribute to a	_barlineFromAttr def _barlineFromAttr(attr): list of :class:`Barline` or :class:`Repeat`. Note this must be a list because an end-repeat and start-repeat at the same time must be two distinct objects in music21.
return deserialize(self.binary, from_bytes=true)  # todo: techdebt fix </s> @object.setter	BinObject @property def object(self): def object(self, value): self.binary = serialize(value, to_bytes=True)  # TODO: techdebt fix
# todo: cache this value. same value should be used for future tests in this policy or other policies when handling this request </s> request_value = self.pip.get_attribute_value(attribute_type, attribute)	target_matches except KeyError: try: except KeyError: _log.debug("PolicyDecisionPoint: Attribute not found: %s %s" % (attribute_type, attribute))
# todo: test 1: quando para a data no estado só tem a planilha de total, </s> if date in cases:	get_state_data for spreadsheet in spreadsheets: date = spreadsheet.date continue
# todo: is this right? </s> if self.endian == ">":	checksum elif self.algorithm == "md5": digest = hashlib.md5(data).digest() (a, b, c, d) = struct.unpack("<LLLL", digest) digest       = struct.pack(">LLLL", a, b, c, d)
# todo allow multiple barriers to be executed </s> self._store_proxy.wait_until('barrier', 0)	cpu_barrier def cpu_barrier(self):
# todo: retry our one ping. we should not have to retry. </s> for retry in range(2):	one_ipv6_ping def one_ipv6_ping(self, host, dst): ping_result = host.cmd('ping6 -c1 %s' % dst) if re.search(self.ONE_GOOD_PING, ping_result):
# todo you should put some extra protection on this, so a user can only </s> return jsonify(get_stored_tokens()), 200	list_tokens @app.route('/auth/tokens', methods=['GET']) def list_tokens():
# todo (a8): add user to models </s> setattr(self.obj, 'user', user.objects.get(pk=1))	_populate_by_obj self.obj.branch = self.obj.revision.branch setattr(self.obj, 'result', self.obj) setattr(self.obj, 'notify', None)
# todo(b/207464757): tf compilation is disabled </s> self.assertfalse(has_xla_must_compile(jax2tf.convert(f_jit), x))	test_nested_jit_is_compiled self.assertFalse(has_xla_must_compile(jax2tf.convert(f_no_jit), x)) f_jit = lambda x: jax.jit(jnp.sin)(x)
# todo(b/141131288): enable complex-valued sorts on tpu. </s> if (onp.issubdtype(dtype, onp.complexfloating) and (	LaxTest for axis in [-1, len(shape) - 1])) def testSort(self, shape, dtype, axis): (jtu.device_under_test() == "cpu" and jax.lib.version <= (0, 1, 47)) or jtu.device_under_test() == "tpu")):
# todo: check utf-8 </s> return read_bytes_exactly_stream(stream, str_len)	read_fasl_string_stream str_len = read_fasl_integer_stream(stream)
# todo: rename style -> parser </s> editor.setparser(pyzo.config.settings.defaultstyle)	createEditor editor.setParser(parser) else: return editor
#todo: make more straightforward (somehow) </s> display_apps = []	get_display_types def get_display_types( trans, hda ): for display_app in hda.datatype.get_display_types(): app_links = []
# todo: remove linear scale to exponential scale </s> if progress > 1:	_progress_to_color color_range = COLOR16_RANGE if p_safe else COLOR256_RANGE progress = get_progress() return color_range[-1] else:
# todo: support this instead of failing. </s> keyboard.parse_hotkey_combinations('')	test_parse_hotkey_combinations_fail_empty with self.assertRaises(ValueError):
raise notimplementederror # todo </s> def latest_offsets(self):	latest_offsets
# todo: drop non-callable keys in dramatiq v2. </s> key_list = keys() if callable(keys) else keys	incr_and_sum while True: try: pipe.watch(key, *key_list) value = int(pipe.get(key) or b"0")
# todo: make not hardcoded </s> log_filename = os.path.join(	error report.append('source unknown') signature = '%s (%s)' % (report[0], report[1]) self.config.logdir, 'exceptions.log') with codecs.open(log_filename, 'a', encoding='utf-8') as logfile:
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
# todo move to common? </s> def dir_hash(path: path):	dir_hash mtimes = tuple(p.stat().st_mtime for p in sorted(path.glob('*.json'))) return mtimes
# todo add ruby personality. </s> return yaml2po(inputfile, outputfile, templatefile, blank_msgstr=pot,	run_converter def run_converter(inputfile, outputfile, templatefile, pot=False, duplicatestyle="msgctxt"): duplicate_style=duplicatestyle).run()
#todo same issue with batch_size </s> if len(self.inputs) == 0:	height def height(self): raise ValidationException("gan.height() requested but no inputs provided") return self.ops.shape(self.inputs[0])[1]
# todo: this is just for compatibility with </s> 'items': activities,	make_activities_base_response 'itemsPerPage': len(activities), 'totalResults': None if kwargs.get('activity_id') else len(activities), 'filtered': False, 'sorted': False,
# todo: don't resize image but change create_tiles. </s> image = np.reshape(image, (image.shape[0], image.shape[1], 1))	create_bitmap print("Load water bitmap from cache.") _, image = read_geotiff(cache_file) image[image == 255] = 1 return image
# todo: needs input cleansing and validation </s> try:	grant def grant(): Character control endpoint for policy granting. bob_pubkey = bytes.fromhex(request.args['bob_encrypting_key']) label = bytes.fromhex(request.args['label'])
# todo: i don't think we need these here because likes of to_parent </s> handler = self._to_unicode_handlers[cls]	to_unicode if value is None: return None retval = handler(cls, value, *args, **kwargs)
# todo: no unit tests cover any of this. </s> @staticmethod	pre_recursion def pre_recursion(desc): if 'animations' in desc:
# todo: breaklines should be displayed correctly </s> content = "  " + re.sub('<[^<]+?>', '', toot['content'])	public clean = re.sub('<[^<]+?>', '', toot['reblog']['content']) content = username + display_name + clean print(content + "\n")
# todo: maybe we should do this in a thread </s> backup_database()	delete from freenasUI.common.system import backup_database try: except: pass
# todo return somethign useful ;) </s> return kgeembedder(config, dataset)	create def create(config, dataset, for_entities): pass
# todo: use more broadly numpy </s> full_data = list(np.array(self.lastchanneldata) - np.array(delta))	parse19bit deltas = decompressDeltas19Bit(packet) for delta in deltas: sample = OpenBCISample(sample_id, full_data, []) self.samples.append(sample)
# todo: maybe check if we are inside a classic snap and don't do </s> busctl = shutil.which("busctl")	command_generate def command_generate(self): if "SNAP" in os.environ: if busctl is None: raise RuntimeError("missing busctl utility")  # pragma: nocover
return  # todo: update </s> elif item == 'user_payment_amount':	get_user_info return  # TODO: update elif item == 'user_payment_id': return  # TODO: update
# todo: should wait for the end of the ongoing test case, and stop gracefully netmon and procmon </s> self.export_file()	fuzz num_cases_actually_fuzzed += 1 except KeyboardInterrupt: self._fuzz_data_logger.log_error("SIGINT received ... exiting") raise
# todo: fix this, this is one of the few cases where using the config </s> runner_name = self.config.get('test_runner', 'runner')	Job raise exceptions.OptionValidationError("Unable to parse " "variant: %s" % details) try: runner_extension = dispatcher.RunnerDispatcher()[runner_name]
# todo remove </s> return reverse('blog:detail', kwargs={	Article 'day': self.created_time.day }) 'article_id': self.id, 'year': self.created_time.year,
# todo: replace suite with testcases </s> test_folder_path = test_folder_path or os.path.join(os.getcwd(), "suite")	load_test_folder } test_definition_mapping = {} test_items_mapping = load_folder_content(test_folder_path) for test_file_path, items in test_items_mapping.items():
# todo: implement this </s> return self._get_usages()[0]	get_tle def get_tle(self):
# todo: investigate why this happens </s> args.grid = '--'	fill_global_args args.ylabel_fontsize = (args.ylabel_fontsize, True) if type(args.grid) is list: args.grid = (args.grid, True)
# todo: remove this when domain decomposition is merged </s> warnings.warn('this feature is not yet implemented in a release ' \	set_dd_mesh_dimension def set_dd_mesh_dimension(self, dimension): 'version of openmc') if not isinstance(dimension, tuple) and \
# todo: wobble </s> return einsum('i...,ij...->j...', vector, rotation)	spin ])
# todo: this check is to maintain backwards compatibility with the old way of creating </s> if hasattr(self, 'package_form'):	edit vars = {'data': data, 'errors': errors, 'error_summary': error_summary} self._setup_template_variables(context, {'id': id}, package_type=package_type) c.form = render(self.package_form, extra_vars=vars) else:
# todo(blk-u): shouldn't need to clear the registry here, but some </s> dependency.reset()	setUp def setUp(self): super(BackendLoader, self).setUp() kvs_core.KEY_VALUE_STORE_REGISTRY.clear()
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_inputs_explicit_insufficient def test_fail_inputs_explicit_insufficient(self): Specified inputs are not sufficient to cover spend amount.
# todo: process </s> result = content.install(units, options)	install_units content = Agent.Content()
# todo: handle a possible deadlock more gracefully. </s> thread_a.join()	test_hl_pos thread_c.sleeptime = 1 thread_a.start() thread_b.join() thread_c.join()
"""todo: not implemented""" </s> notimplementederror("first not implemented")	first @symbolic_dispatch def first(x):
# todo add locales </s> raise yunohosterror("domain_name_unknown", domain=domain)	domain_set_settings domains = _load_domain_settings() if not domain in domains.keys(): setting_set = False if ttl is not None:
# todo: this never checks in case one of the operations fails </s> def download_if_missing(dirname="compact", filename="compact.tar.z",	download_if_missing url="http://sound.media.mit.edu/resources/KEMAR/compact.tar.Z",tar=True): if not os.path.isdir(dirname):
# todo: check rootfs fs against parameter injection </s> with open("/tmp/arm_now/save", "w") as f:	add_local_files def add_local_files(rootfs, dest): F.write("cd /root;tar cf /root.tar *") subprocess.check_call("e2cp -G 0 -O 0 -P 555 /tmp/arm_now/save".split(' ') + [rootfs + ":/sbin/"])
# todo: probabilistic with each route </s> self.kernel_api.vehicle.addfull(	add if type(self.master_kernel.scenario.rts[edge][0]) == 'str': route_num = 0 veh_id, 'route{}_{}'.format(edge, route_num),
# todo: must be the same if we merged/pushed before, if not -- skip </s> if not force:	_publish_data lgr.debug("Invoking copy --auto") annex_copy_options_ += ' --auto' annex_copy_options_ += ' --fast' for r in ds.repo.copy_to(
#todo verify input </s> path = request.args.get('paste')	confirm_tag @Tags.route("/Tags/confirm_tag") def confirm_tag(): tag = request.args.get('tag') if(tag[9:28] == 'automatic-detection'):
# todo: give a vanilla example </s> attributes	confusion_matrices def confusion_matrices(predicted_states, ground_truth_states): ---------- predicted_state: Pandas DataFrame of type {appliance :
# todo operators </s> properties.register()	register texture_nodes.register() volume_nodes.register() ui.register() from .utils.log import LuxCoreLog
common_path=prefix,  # todo: add key? </s> action="either",	make_inline_attachments_decision if ld.op == rd.op == DiffOp.REMOVE: md = MergeDecision( conflict=False, local_diff=[ld],
# todo: check rootfs fs against parameter injection </s> with open("/tmp/arm_now/save", "w") as f:	add_local_files def add_local_files(rootfs, dest): F.write("cd /root;tar cf /root.tar *") subprocess.check_call("e2cp -G 0 -O 0 -P 555 /tmp/arm_now/save".split(' ') + [rootfs + ":/sbin/"])
# todo: add logging </s> return	Bot codeblock_tag = await self.bot.get_cog("Tags").get_tag_data("codeblock") if codeblock_tag == {}: howto = (f"Hey {msg.author.mention}!\n\n" "I noticed you were trying to paste code into this channel.\n\n"
# todo this makes self variables non-breakable. wanted? </s> r = [n for n in par.get_set_vars()	process if details and details[0][0] != '=': no_break_scope = True if len(n) > 1 and str(n.names[-1] == name)] if isinstance(name, InstanceElement) and r:
# todo: for now, circumnavigate the detached head issue. </s> for subds in source.get_dataset_handles(recursive=true):	test_publish_simple def test_publish_simple(origin, src_path, dst_path): source = install(path=src_path, source=origin, recursive=True) AnnexRepo(opj(src_path, subds), init=True, create=False).git_checkout("master") target = GitRepo(dst_path, create=True)
# todo make this cancellable with is_cancellable_behavior </s> @connection.on_connection_thread()	BehaviorComponent find_faces_request = protocol.FindFacesRequest() return await self.grpc_interface.FindFaces(find_faces_request) async def look_around_in_place(self) -> protocol.LookAroundInPlaceResponse: Turn in place and move head to see what's around Vector
# todo: make treecount configurable via an inputslot </s> rf=vigra.learning.randomforest(100)	OpTrainRandomForest featMatrix=numpy.concatenate(featMatrix,axis=0) labelsMatrix=numpy.concatenate(labelsMatrix,axis=0) try: RF.learnRF(featMatrix.astype(numpy.float32),labelsMatrix.astype(numpy.uint32))
# todo: error handling and such </s> res = response.json()	create_virtual_facility response = requests.post(self.create_virtual_facility_url, data=json.dumps(facility_data)) if res['Success']: return True
# todo: conflict detection/resolution </s> for key in d:	_collect_analysis if not d: continue analysis.setdefault(key, {}).update(d[key]) return analysis
# todo - add tinfo when available </s> raise conaninvalidconfiguration("tinfo is not (yet) available on cci")	requirements self.requires("ncurses/6.2") elif self.options.terminal_db == "tinfo":
# todo(boris-42): make it work through assertisinstance </s> self.assertequal(str(type(engine_inst)), str(e))	test_get_engine engine_inst = deploy_engine.EngineFactory.get_engine(e.__name__, None)
# todo: gae: support parents via gaekeyfield </s> assert entity.key().parent() is none, "parents are not yet supported!"	_make_result def _make_result(self, entity): entity[self.query.get_meta().pk.column] = entity.key().id_or_name() result = []
# todo: is there a check we can do to ensure that we've matched the </s> opens.values[indexer] = special_opens.values	_overwrite_special_opens return indexer = closes.searchsorted(special_opens)
# todo: checking that hour/minute/second are not </s> _assign_hms(res, value_repr, hms)	_parse (i, hms) = _parse_hms(i, l, info, hms_idx) if hms is not None: elif (len(ymd) == 3 and len_li in (2, 4) and res.hour is None and
# todo html </s> from_name = self.get_display_name(message.from_id) or "(???)"	generate_message_html Return HTML for a message, showing reply message, forward headers, view count, post author, and media (if applicable). return "{}: {}".format(from_name, message.text)
# todo(mvandijk): enable cluster security once trove features are in </s> self.store_key(key_value)	_configure_cluster_security def _configure_cluster_security(self, key_value):
pass  # todo </s> elif ch == 'op_m':  # one or more opcodes	output_script_type data.append(script[cur+1:]) elif ch == 'multisig':  # one or more signature if script[cur] in OP_N_CODES: number_of_sigs = script[cur] - opcodes['OP_1'] + 1
# todo: async </s> self.session.raw_message(data.decode('utf-8', 'replace'))	post print 'Unauthorized' raise HTTPError(401, 'unauthorized') self.write('ok') self.finish()
# todo: devise a way so we don't need to "always trust". </s> return super(gpgwrapper, self).encrypt(data, recipient, sign=sign,	encrypt passphrase=None, symmetric=False): Encrypt data using GPG. always_trust=always_trust, passphrase=passphrase,
# todo: expand to full set of info </s> def create_workflow_table(run_id, meta):	create_workflow_table table_name = run_id return Table(
# todo log. </s> return	_visit content = open(pathname).read() except IOError: if hashlib.md5(content).hexdigest() == _md5(pathname): return
# todo: provide a kernel which will describe how coordinates are extruded. </s> mesh = firedrake.extrudedmesh(m, layers, layer_height=0.1)	integrate_rhs m = UnitSquareMesh(2 ** power, 2 ** power) layers = 11 horiz = ufl.FiniteElement(family, None, degree) vert = ufl.FiniteElement(family, None, degree)
date_format = "aman"  ## todo: fix this </s> date_format = str(date_format).replace(	get_results_json json_object = json.dumps(data) date_from_db = scan_id_temp[0].date "-", "_").replace(":", "_").replace(" ", "_") filename = "report-" + date_format + "".join(
# todo: it would be good to be able to notify an external </s> ofmsgs.extend(self.host_manager.learn_host_on_vlan_port(	rcv_packet vlan.max_hosts, vlan.vid, eth_src) else: port, vlan, eth_src)) self.logger.info(
## todo: impala attempt to speed up final pass after lstm. </s> out = self.action_adapter.get_logits_probabilities_log_probs(nn_output)	get_state_values_logits_probabilities_log_probs last_internal_states = nn_output.get("last_internal_states") nn_output = nn_output["output"]
# todo - remove the line below and use repo_url as your foundation </s> package.repo_url = _get_sourceforge_repo_url(sf_package_data)	pull package.repo_watchers = len(sf_package_data.get('maintainers', [])) + len(sf_package_data.get('developers', [])) package.repo_description = sf_package_data.get('description', '') package.repo_forks = None package.participants = _get_sourceforge_participants(sf_package_data)
# todo: this is untested. </s> _raise_current_error()	load_certificate_request raise ValueError("type argument must be FILETYPE_PEM or FILETYPE_ASN1") if req == _ffi.NULL: x509req = X509Req.__new__(X509Req) x509req._req = _ffi.gc(req, _lib.X509_REQ_free)
# todo: test filter functionality more </s> f = multistagechannelfilter(input_rate=32000000, output_rate=16000, cutoff_freq=3000, transition_width=1200)	test_basic def test_basic(self): self.__run(f, 400000, 16000/32000000)
'license': none, # todo </s> 'card': card,	subscription_put response = utils.request.put(SUBSCRIPTION_SERVER, json_data={ 'email': email, },
# todo lib </s> renamed_uuids = {	BpyDataCollectionDiff proxy_uuids = set(proxies.keys()) blender_uuids = set(blender_items.keys()) uuid for uuid in blender_uuids & proxy_uuids if proxies[uuid].data("name") != blender_items[uuid][0].name }
pass # todo: explain </s> pass	status426 def status426(self):        # Upgrade Required
# todo: remove when #980 has been merged </s> info['url'] = formats[-1]['url']	_real_extract 'upload_date': upload_date, } info['ext'] = determine_ext(formats[-1]['url']) return self.video_result(info)
# todo: remove parametrized workaround once collection structure contains parametrization. </s> if x.name == names[0] or x.name.split("[")[0] == names[0]:	matchnodes submatching = [] for x in rep.result: submatching.append(x) if submatching:
# todo: remove this line when issue #2062 is fixed </s> return x + y	my_plan y = (x - 2) * 10
# todo: figure out why not working </s> utils.skip_under_xvfb()             # skip late so we smoke test the code	test_rating_scale rs.draw() if self.win.winType=='pyglet': utils.compareScreenshot('ratingscale1_%s.png' %(self.contextName), win, crit=30.0) win.flip()#AFTER compare screenshot
# todo check </s> return self.tags	metadata @interfacedoc def metadata(self):
# todo: remove when 3.8 with https://github.com/getpelican/pelican/pull/2256 </s> reader = rstreader(pelicanobj.settings)	configure_pelican pelicanobj.settings['JINJA_FILTERS']['hyphenate'] = hyphenate pelicanobj.settings['JINJA_FILTERS']['dehyphenate'] = dehyphenate pub = reader._get_publisher(os.devnull) if pub.settings.language_code != pelicanobj.settings['DEFAULT_LANG']:
#todo: make sure that the body is actually json. </s> return '{"token": "abcdefghijklmnopqrstuvwxyz"}'	create_token return "whatever, we don't have XML yet" else: return 'it did NOT work\n'
# todo: use error page with message and redirect </s> return httpresponse("we didn't redirect you to twitter...")	twitter_login_done request_token = request.session.get('request_token', None) if not request_token: token = oauth.OAuthToken.from_string(request_token) if token.key != request.GET.get('oauth_token', 'no-token'):
# todo unordered float </s> if a is none and b is none:	fcomi def fcomi(ir, instr, a=None, b=None): a, b = float_st0, float_st1 elif b is None:
"meta.deleted": false,  # todo(tsileo): retrieve deleted and expose tombstone </s> 'type': {'$in': [activitytype.create.value, activitytype.announce.value]},	outbox q = { "box": Box.OUTBOX.value, } return jsonify(
pass # todo </s> def save(self, playlist):	save
# todo xxx postremora: uncomment when remora goes away </s> u = user.objects.get(email='john.connor@sky.net').get_profile()	test_success r = self.client.post('/en-US/firefox/users/register', data, follow=True) assert u.confirmationcode eq_(len(mail.outbox), 1)
# todo (aron): add i18n by varying the language of the topic tree here </s> topictree = get_flat_topic_tree()	video_dict_by_video_id def video_dict_by_video_id(): video_title_dict = {} video_id_regex = re.compile('.*/v/(?P<entity_id>.*)/')
# todo: fix circular import </s> from website.utils import waterbutler_url_for	_get_fileobj_child_metadata def _get_fileobj_child_metadata(self, filenode, user, cookie=None, version=None): kwargs = dict( provider=self.config.short_name,
# todo test errors </s> assert_array_equal(expect, actual)	test_advanced_indexing_1d_int actual = z[ix]
# todo: find a better random value </s> return datetime.datetime.now()	_auto_value def _auto_value(self, prop): if prop.type == datetime.datetime: elif prop.type == datetime.date: return datetime.date.today()
# todo deprecate </s> this method creates and assesses the accuracy of a logistic regression	linear def linear(self, cores=4, debug=False): model. Parameters
# todo: fixed by using realpath, but there should be a cleaner </s> assert_equal(	_test_AnnexDB status2_ = db2.get(filep2) assert_equal(status2, status2_) set(db2.get_obsolete()), {opj(realpath(path), p) for p in ['file1.txt', '2git']})
# todo: deal with any overwrite issues </s> if len(self.request_cache) == 0:	_save_requests def _save_requests(self, filepath=None): crypten.log("Request cache not saved - cache is empty") return
# todo: this logic below was borrowed from `dataframe.pandas_df` to set the index </s> import pandas as pd	rename_output def rename_output(pdf): if len(index_columns) > 0: append = False
# todo: check if contact is cached (same contact as </s> form_data = none	lookup if len(val) > 0: for contact in val: if NIR_WHOIS[nir]['form_data_ip_field']: form_data = {NIR_WHOIS[nir]['form_data_ip_field']:
# todo: do we change this to something like "threshold" </s> m, n = request_data['m'], request_data['n']	grant bob_pubkey = bytes.fromhex(request_data['bob_encrypting_key']) label = b64decode(request_data['label']) expiration_time = maya.MayaDT.from_iso8601( request_data['expiration_time'])
# todo: common crud method </s> group = await self.middleware.call('datastore.query', 'account.bsdgroups', [('id', '=', pk)], {'prefix': 'bsdgrp_'})	GroupService return pk async def do_delete(self, pk): if not group: raise ValidationError(None, f'Group {pk} does not exist', errno.ENOENT)
# todo: parse output and check if succeeded </s> for line in output.stdout.splitlines():	_CallHotplugCommands for c in cmds: output = self._CallMonitorCommand(name, c) logging.info("%s", line) time.sleep(1)
# todo: replace with remoteunixcommand </s> exists = cmd("test -e %s" % backup_label)	SshBackupExecutor backup_label = os.path.join(last_backup.pgdata, 'backup_label') if exists == 0: hint = 'Check that the PostgreSQL server is up ' \
# todo do this abstractly </s> if f.name == 'player_relative':	preprocess_obs if f.type == features.FeatureType.CATEGORICAL: cat.append(one_hot(obs.observation[type][f.index], f.scale)) cat[-1] = cat[-1][:, :, 1:] else:
self.req.setoption("timeout", 60)  #@todo: remove in 0.4.10 </s> self.last_html   = ""	_init self.info = {} self.req = self.pyload.requestFactory.getRequest(self.classname) self.last_header = {}
# todo: if mainchare constructor is *always* going to be threaded, we can remove the second condition </s> if self.buildingmainchare and threads.get_ident() != self.threadmgr.main_thread_id:	recvArrayMsg self.runningEntryMethod.stopMeasuringTime() em.startMeasuringTime() em.run_non_threaded(obj, header, args)  # now call the user's array element __init__ else:
# todo: deprecate </s> model = self.get_model()	run_evaluation def run_evaluation(self, test_mode: bool = False): self.evaluation_loop.testing = test_mode model.on_pre_performance_check() if test_mode:
# todo(jd) move into prepare_service gettextutils and eventlet? </s> eventlet.monkey_patch()	collector def collector(): gettextutils.install('ceilometer') prepare_service(sys.argv)
# todo: handler_node.name and handler_node.type </s> handler_exits |= self.add_body_arcs(handler_node.body, from_line=handler_start)	handle_Try for handler_node in node.handlers: handler_start = self.line_for_node(handler_node) if node.finalbody: exits = self.add_body_arcs(node.finalbody, prev_lines=exits|handler_exits)
# todo: renable when tagging is removed from the analysis report. </s> return []	GenerateLabels if hash_information: return [u'nsrl_present']
# todo: move the inside snippets to the corresponding snippets dict </s> if value and '()' in value:	generate_snippet def generate_snippet(data): value = data.get('value') if value.replace('()', '') in ['rotate','rotateX','rotateY','rotateZ','skew','skewX','skewY']: value = value.replace('()', '($1${1/^((?!0$)-?(\d*.)?\d+)?.*$/(?1:deg)/m})')
#todo: let's not use the solitude transaction id if we can help it. </s> signed_jwt = request.read()	chargeback Verify signature from and create a refund contribution tied to the original transaction. try: data = parse_from_webpay(signed_jwt, request.META.get('REMOTE_ADDR'))
# todo: give a vanilla example </s> .. math::	feca def feca(predicted_power, df_appliances_ground_truth): fraction = \\sum_n min \\left (
# todo: this is wrong. globe.semiminor_axis does </s> [-19987726.36931940, 20087726.36931940], decimal=6)	test_ellipsoid_guam_transform [-19987726.36931940, 20087726.36931940], decimal=6) assert_almost_equal(np.array(aeqd.y_limits), pt_lat = 13 + (20 + 20.53846 / 60) / 60 pt_lon = 144 + (38 + 7.19265 / 60) / 60
# todo: update consumer </s> collection.remove(bind, safe=true)	unbind return
# todo: explicitly exploit symmetry and set onesided=true </s> a_fft = torch.rfft(h, signal_ndim=1, onesided=false)	forward_owa r = self.relation_embeddings(batch[:, 1]) t = self.entity_embeddings(batch[:, 2]) b_fft = torch.rfft(t, signal_ndim=1, onesided=False) a_fft[:, :, 1] *= -1
# todo: use the dbobject instance instead of it's qualified_name </s> write_schemas = {s.qualified_name for s, _ in write_schemas_and_owners}	determine_schema_privileges write_schemas_and_owners = dbcontext.get_role_current_nondefaults(role, 'schemas', 'write') read_schemas_and_owners = dbcontext.get_role_current_nondefaults(role,  'schemas', 'read') read_schemas = {s.qualified_name for s, _ in read_schemas_and_owners} all_owned_schemas = dbcontext.get_all_schemas_and_owners()
# todo implement .!{cmd} (ex shell out) test for windows and osx </s> self.assertcontentisequalto("2\nbbb\nccc")	test_simple_filter_through_shell })
# todo(sirp): should this be a dict, or a list of dicts? </s> properties = dict((p['key'], p['value'])	make_image_dict if a in d.keys()]) files = [_fetch_attrs(f, db.IMAGE_FILE_ATTRS) for f in image['files']] for p in image['properties'] if 'deleted' in p.keys() and not p['deleted'])
svg = apply_template(svg, {"maxpoints":maxpoints, "trdn": trdn, "msg":"", "valuemid":"0.5", "timemid":"10s", "datapoints":"","init_max_y": "false", "max_y": 0, "seconds_scale":0, "y_shift": 0, "zero": 0, "l_y":""}) # todo templating engine </s> if width and height: svg = svg.replace('height="210" width="610"', 'height="%s" width="%s"' % (height, width)) # todo: switch to templating	plotwh svg = apply_template(svg, {"MAXPOINTS":MAXPOINTS, "TRDN": trdn, "MSG":"", "VALUEMID":"0.5", "TIMEMID":"10s", "DATAPOINTS":"","INIT_MAX_Y": "false", "MAX_Y": 0, "SECONDS_SCALE":0, "Y_SHIFT": 0, "ZERO": 0, "L_Y":""}) # TODO templating engine else: image_views += 1 return flask.Response(svg,  mimetype= 'image/svg+xml', headers={'Cache-Control': 'no-cache, no-store, must-revalidate', 'Pragma': 'no-cache'})
# todo(pfnet): implement crop function </s> h = score_pool4	__call__ h = self.upscore2(score_fr) upscore2 = h  # 1/16 for axis in [2, 3]: start = 5
# todo(b/80125832): enable nccl in tests </s> all_reduce_spec='',	_testVariables staged_vars=False, optimizer='momentum', use_fp16=False, fp16_vars=False,
# todo - add some tests to this response </s> response = self.do_list(	test_list response = self.do_list() self.assertEqual(len(response), 0) { 'item': 10,
# todo(erikbern): should compute jacobian of this one </s> def f(x):	f a, b, lambd, k = x neg_LL = 0
# todo add description field to the model </s> group_dict = self.group_manager.create( trans, name=name ).to_dict( view='element', value_mapper=self.__get_value_mapper( trans ) )	create description = '' else: else: raise RequestParameterMissingException( 'Missing required parameter "name".' )
# todo multi-level import non-breakable </s> if isinstance(par, pr.import) and len(par.namespace) > 1:	process result.append(par) else: no_break_scope = True result.append(par)
# todo(ytknzw): add more specific assertion with the test case. </s> study = create_study()	test_plot_intermediate_values trial.report(2.0, step=1) return 0.0 study.optimize(lambda t: objective(t, True), n_trials=1) figure = plot_intermediate_values(study)
# todo there are more rules for adjusting rx, ry </s> self._start_path()	_parse_rect rx = _prefer_non_zero(rx, ry) ry = _prefer_non_zero(ry, rx) self.M(x + rx, y) self.H(x + w -rx)
# todo check output </s> obj_name = os.path.basename(obj_file)	_test_obj self.assertTrue(len(listing) >= 1) self.openio('container show ' + self.CONTAINER_NAME) opts = self.get_opts(OBJ_HEADERS) output = self.openio('object create ' + self.CONTAINER_NAME +
# todo: support media types </s> yield tinycss2.parse_stylesheet(	find_stylesheets element.get('type', default_type) == 'text/css' and element.text): element.text, skip_comments=True, skip_whitespace=True)
# todo xxx graalvm change </s> raise unittest.skiptest("missing threading support")	ThreadedEchoServer self.daemon = True def __enter__(self): self.start(threading.Event()) self.flag.wait()
# todo: create unsupportedproviderexception. (?) </s> raise exception("this provider is not supported: {p}".format(p=cli_context.obj['provider']))	run_command user=ec2_user) else:
# todo: implement this. </s> pass	process_delivery 'id': 'transport message id if this was a reply, else internal id' }
# todo(b/163904067): mimic defun' behavior and reset the step seed to </s> resetstepseed()	Forward @tf.function(input_signature=Flatten(fwd_sig), autograph=False) def Forward(*args): with RemoveAssertContext(remove=noinline), tf.device(device): xs = Pack(fwd_sig, args)
# todo(mottodora): find better way to ignore non connected </s> e = functions.where(cond, e,	__call__ cond = xp.reshape(cond, (mb, self.n_edge_types, 1, atom, atom)) cond = xp.broadcast_to(cond, e.array.shape) xp.broadcast_to(xp.array(-10000), e.array.shape) .astype(xp.float32))
# todo: fix maximum size limit handling to create new stream. </s> data_size = construct.ulint32(u'size').build(data_size)	WriteEntry raise IOError(u'Unable to write to closed serialized data stream.') data_size = len(data) self._file_object.write(data_size) self._file_object.write(data)
# todo: remove "get_" from the name </s> get a list of all node types in the graph.	get_node_types def get_node_types(self): Returns: set of types
raise notimplementederror  # todo ... </s> print("finishdiscard()")	finishDiscard def finishDiscard():
# todo: how do i make the __iter__ thread safe? </s> cursor = self._conn.execute('select information from data order by index_ desc')	__reversed__ def __reversed__(self): for r in cursor: obj = cPickle.loads(r[0])
system_info = none #todo </s> new_info_file = recordinginfofile.create_empty_file(rec_dir)	recording_update_pupil_invisible_to_pprf_2_0 recording_software_version = None  # TODO recording_name = None  # TODO new_info_file.recording_uuid = recording_uuid new_info_file.start_time_system_ns = start_time_system_ns
# todo: expose more of the connection create parameters (instead of </s> self.sendhcicommand(0x0405, bt_addr + '\x00\x00\x00\x00\x00\x00\x01')	connectToRemoteDevice e.g. for 'f8:95:c7:83:f8:11' you would pass b'\x11\xf8\x83\xc7\x95\xf8'."""
# todo: 3.5 does not maintain order but it should be deprecated soon </s> assert r[1]['steps'][0]['message'] == 'failure'	test_first_test_passes_second_test_fails assert len(r[1]['steps']) == 1
# todo: raise warning if computed output is already in cache. </s> cache.update(zip(step.outputs, listify(output_data)))	fit else: raise TypeError('{} does not implement predict or transform!'.format(step.name))
# todo: remove warning check once deprecated </s> hits = tree.intersection((1012821.80, 229228.26), objects=true)	test_merge_geo tree = self.boros.sindex with pytest.warns(FutureWarning, match="`objects` is deprecated"): res = [self.boros.loc[hit.object]["BoroName"] for hit in hits] assert res == ["Bronx", "Queens"]
# todo: process form submission </s> return render_template("admin_edit_user.html", user=user)	admin_edit_user def admin_edit_user(user_id): user = Journalist.query.get(user_id)
## todo: # fixme: remove me </s> try:	create_spider_splash except: domain = unpack_url['domain'] tld = unpack_url['tld'].decode() except:
#todo - uncomment if read/write and zero init sections can be moved into a separate flash algo section </s> error = false	callFunction final_sp = self.target.readCoreRegister('sp') final_pc = self.target.readCoreRegister('pc') if final_fp != expected_fp: logging.error("Frame pointer should be 0x%x but is 0x%x" % (expected_fp, final_fp))
# todo debug </s> print "rule has not triggered (yet)"	run sensorAlertsToHandleWithRules.remove(sensorAlertToHandle) else: if not self._checkRulesCanTrigger(sensorAlertList, alertLevel):
# todo: figure out why </s> l.warning("got an unexpected none for variable offset when variable %s is not none.",	_init_widgets self._variable_ident = "<%s>" % variable.ident if offset is None: variable )
'username': 'fakeuser@dimagi.com',  #todo </s> 'doc_id': uuid.uuid4().hex,	render_xform 'modified_date': datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ'), 'user_id': 'f72265c0-362a-11e0-9e24-005056aa7fb5',  #TODO 'case_id': uuid.uuid4().hex, 'patient_case_id': patient_case_id,
# todo ... </s> if isinstance(token, cclosingbracket):	cpre3_parse state = 10 elif state == 30: # struct if token.brackets == curCObj._bracketlevel: state = 0
# todo - need to parameterize this into generate_match_filters, </s> exact_match_filter = "(&(objectclass=posixgroup)%s)" % exact_match_filter	find_groups (exact_match_filter, partial_match_filter) = self.__generate_match_filters( search_fields, criteria_words) partial_match_filter = "(&(objectClass=posixGroup)%s)" % partial_match_filter conn = self.getConnection(opts)
pass # todo </s> @register(r'^playlistfind (?p<tag>\s+) (?p<needle>\s+)$')	MpdHandler @register(r'^playlistdelete (?P<name>\S+) (?P<songpos>\d+)$') def _playlistdelete(self, name, songpos): def _playlistfind(self, tag, needle): pass # TODO
# todo(rlrossit): these look like dicts, but they're actually versioned </s> try:	_translate_floating_ip_view result['instance_id'] = None return {'floating_ip': result} if 'address' in floating_ip['fixed_ip']: result['fixed_ip'] = floating_ip['fixed_ip']['address']
# todo: uncomment report abuse gets ported to mkt. </s> raise skiptest	test_submit def test_submit(self): self.client.login(username='regular@mozilla.com', password='password') r = self.client.post(self.url, {'text': 'this is some rauncy ish'})
# todo: refactor this to be more uniform across sources </s> self.update_control_menu()	update_menu def update_menu(self):
# todo xxx graalvm change </s> raise unittest.skiptest("missing threading support")	test_handshake_timeout sock.close() t = threading.Thread(target=serve) t.start() started.wait()
# todo(nnorwitz): enable test. </s> self.assertequal(['const', 'volatile'], modifiers)	testSimpleModifiers self.assertEqual([], templated_types)
raise exceptions.mpdnotimplemented  # todo </s> underscore, dash, dot and colon.	subscribe already. The name may consist of alphanumeric ASCII characters plus
# todo: let the globe return the semimajor axis always. </s> return coords	ellipse coords += ([easting], [northing])
# todo see issue 1935 </s> pass	quicksave def quicksave(self):
# todo - perhaps time_created could go here too? </s> return (self.client, self.mid)	__key def __key(self):
# todo: let the globe return the semimajor axis always. </s> a = np.float(globe.semimajor_axis or wgs84_semimajor_axis)	Miller if globe is None: globe = Globe(semimajor_axis=math.degrees(1), ellipse=None) b = np.float(globe.semiminor_axis or a) if b != a or globe.ellipse is not None:
# todo(sbauza): the current placement noauthmiddleware returns a 401 </s> return self._client.get(	_fake_get def _fake_get(self, *args): (url,) = args[1:] url, endpoint_override="http://127.0.0.1:%s" % self.service.port,
# todo: pass fail_silently or whatever. </s> connection.send_messages(event._mails(event._users_watching()))	_fire_task connection = mail.get_connection()
# todo: assert metrics. </s> metrics = code_analysis_server.metrics(	test_get_metrics error = repository.get_metrics(commit, metrics["spaces"]) assert not error "file.cpp", int i = 1;
# todo find a better way of checking for no pregenerated thresholds </s> self.code_gen_dict["$globals$"] += '#include "thresh.h" \n'	global_includes self.code_gen_dict["$GLOBALS$"] += '#include "params.h" \n' if self.TMEM != 0:
# todo: show in-app notification? </s> print("device connected")	_on_device_added welcome_perspective.add_device(device) else:
# todo: should we enable auto-retry, </s> producer.publish(msg, exchange=exchange, **kwargs)	publish exchange = queue.exchange with self.get_producer(worker_ctx.srv_ctx) as producer:
# todo(mordred) remove this, it's a waste of a call. it's here for </s> group = self._compute_client.get(	update_security_group '/os-security-groups/{id}'.format(id=group['id']), json={'security-group': kwargs}) '/os-security-groups/{id}'.format(id=group['id'])) return self._normalize_secgroup(group)
ndpi = (96, 96) # todo: read real dpi </s> debug_out("input dpi = %d x %d"%ndpi, verbose)	convert debug_out("input dpi (forced) = %d x %d"%ndpi, verbose) else: if colorspace: color = colorspace
# todo: out to file </s> pass	write_up def write_up(self):
# now we can kill it. todo: on a slow machine, the node might kill </s> def _stop(res):	test_client d.addCallback(_started) d.addCallback(lambda res: self.poll(_node_has_started)) open(HOTLINE_FILE, "w").write("") self.failUnless(os.path.exists(TWISTD_PID_FILE))
# todo: progress +kwargs </s> remote.pull(refspec=refspec, progress=progress)	pull else:
"""todo: very inneficient code tag""" </s> root = hierarchicalnode.get_root_object(page)	show_sub_menu @register.inclusion_tag('sub_menu.html', takes_context=True) def show_sub_menu(context, page, url='/'): children = HierarchicalNode.get_children_objects(root) request = context['request']
# todo: should this raise ioerror? </s> with raises(ioerror):	test_invalid_data_byte_no_clipping def test_invalid_data_byte_no_clipping(): read_file(HEADER_ONE_TRACK + """ 4d 54 72 6b  # MTrk
# todo: add for morph targets data. </s> min_index = min(indices)	extract_primitive_floor process_bone = False bone_max = bone_index max_index = max(indices) for old_index in indices:
#todo: check the data! e.g. pubdate etc. </s> count = 0	test_twitter pipe_def = self._get_pipe_def("pipe_ac45e9eb9b0174a4e53f23c4c9903c3f.json") p = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def) for i in p: count += 1
# todo pydocs </s> self.service = service	BigQueryBaseCursor class BigQueryBaseCursor(object): def __init__(self, service, project_id): self.project_id = project_id def run_query(self, bql, destination_dataset_table = False, write_disposition = 'WRITE_EMPTY'):
# todo: test! </s> if not settings.st_unique_emails:	EmailUniqueMixin def clean_email(self): email = self.cleaned_data["email"] return email is_taken = User._default_manager\
# todo: replace with specific error when exceptions are refactored </s> raise xlwings.xlwingserror('local is not supported on macos')	open add_to_mru=None, local=None, corrupt_load=None): if local is not None: if corrupt_load is not None: raise xlwings.XlwingsError('corrupt_load is not supported on macOS')
annot.annotation_metadata.annotation_rules = "todo" #todo </s> annot.annotation_metadata.validation_and_reliability = "todo" #todo	fill_annotation annot.annotation_metadata.version = "1.2" annot.annotation_metadata.annotation_tools = "Sonic Visualizer" annot.annotation_metadata.origin = metadata[1] annot.annotation_metadata.annotator.name = metadata[annotation_id + 2]
# todo: this regex could change based on project req format </s> attributes[col_headers[j]] = re.findall('[a-za-z0-9]+', cell.value)	import_xlsx if j != id_col and cell.value is not None: if 'links' == col_headers[j]: else: attributes[col_headers[j]] = cell.value
# todo equip `unique()` with a tolerance </s> points, idx = \	read_buffer line = f.readline().decode('utf-8') assert line.strip() == 'endfacet' numpy.unique(numpy.concatenate(facets), axis=0, return_inverse=True) cells = {'triangle': idx.reshape(-1, 3)}
# todo(sloria): test me </s> def metadata_to_hgrid(item, node, permissions):	metadata_to_hgrid filename = get_file_name(item['path']) serialized = {
#todo - use a context manager here once we drop python 2.6 </s> self.assertraises(valueerror, kcluster, data,	test_kcluster [ 1, 1, 1, 1, 1], [ 1, 1, 1, 1, 1]], int) **{"nclusters": nclusters, "mask": mask, "weight": weight, "transpose": 0, "npass": 100,
# @todo need a py2/3 way to compare xml easily. </s> <duedate>2015-07-06 16:25:02.711136</duedate>	test_serializer <Type>ACCREC</Type>
# todo make comments clearer, see _viterbi_decode </s> broadcast_emissions = emissions[i].unsqueeze(1)	_compute_normalizer for i in range(1, seq_length): broadcast_score = score.unsqueeze(2)
# todo: use invalidation time </s> params: userfull to dump, mediaid of the profile photo in the db	dump_user def dump_user(self, user_full, photo_id): Returns -, or False if not added""" timestamp = round(time.time())
# todo-me move sorting and add more sorting options </s> aired_lst = sorted(child_lst, key=operator.itemgetter(1))	get_content else: pass play_lst = [x[0] for x in aired_lst] return play_lst
# todo: the problem was fixed in vobject 0.9.5 </s> raise vobjectbugexception(	getrruleset None) except TypeError as e: "failed to call getrruleset: %s" % e) from e if first_dtstart is None:
# todo: "wildcards" other than <any> </s> if type == "<any>" or type == to_ann:	relation_types_from_to continue for dummy, type in arg2s: types.append(r.storage_form()) return types
# todo: consider using eafp here instead. </s> if value is _not_found:	resolve value = self.get(parts[0], _NOT_FOUND) for part in parts[1:]: break value = _get_value(value, part)
# todo: may want to validate its grouper </s> return grouped_res.obj	grouped_eval return grouped_res._broadcast_agg_result() elif isinstance(grouped_res, SeriesGroupBy) and not require_agg: else: raise ValueError("Result must be subclass of SeriesGroupBy")
# todo: re-enable for hardware </s> for portno, config in list(interfaces_config.items()):	add_dp name, dp_config, port, interfaces_config, i, dpid_count, stack, n_tagged, tagged_vid, n_untagged, untagged_vid) stack = config.get('stack', None) if stack:
# todo: is this behavior desired? </s> self.playlists.get_items.return_value.get.side_effect = exception	test_get_items_backend_exception_gets_through def test_get_items_backend_exception_gets_through(self, logger): with self.assertRaises(Exception): self.core.playlists.get_items('dummy:/1')
# todo: test filter functionality more </s> f = multistagechannelfilter(input_rate=32000000, output_rate=16000, cutoff_freq=3000, transition_width=1200)	test_basic def test_basic(self): self.__run(f, 400000, 16000/32000000)
# todo notify orgadmin </s> elif any(tags[k] == "review" for k in review):	facility_approval_workflow update["PUBLIC"] = "N" update["STATUS"] = "REVISE" update["PUBLIC"] = "N" update["STATUS"] = "REVIEW"
# todo(laigd): remove this check when 312743821 is in the release. </s> if use_tf_function and tf.compat.v1.__version__ < '2.3.0':	CallDefunTest ) def testSimple(self, use_tf_function): return FLAGS.call_defun_use_tf_function = use_tf_function
return  # todo: handle images </s> if display == 'block':	process_block display = style.get('display', 'inline') if tag == 'img': b = Block() self.blocks.append(b)
# todo: implement me </s> def _test_all_zeros(self):	_test_all_zeros num_classes = 3 logits = torch.zeros(2, num_classes, 1, 2)
# todo: explicitly commit files by name </s> youngest_ancestor = os.path.commonprefix(files)	add raise IOError("[Darcs] Error running darcs command '%s': %s" \ % (command, error)) return output + type(self)(youngest_ancestor).commit(message, author)
# todo improve precision </s> warnings.warn("the cohen-gismalla schemes are only given in single-precision.")	__init__ def __init__(self, index, symbolic=False): frac = sympy.Rational if symbolic else lambda x, y: x / y self.name = "CohenGismalla({})".format(index) if index == 1:
# todo: overwrite databus values with function in this class </s> self.voltage[0] = databus.get_value("register_1054")	initialize self.energy_in = databus.get_value("register_13") self.energy_out = databus.get_value("register_13") self.voltage[1] = databus.get_value("register_1055") self.voltage[2] = databus.get_value("register_1056")
#todo todo todo todo todo todo todo todo todo </s> pub_key.generate_key(crypto.type_rsa, 1024)	gen_RSA_key :rtype: An RSA key as an `pyopenssl.OpenSSL.crypto.PKey` pub_key = crypto.PKey() return pub_key
return cursor_offset, line #todo not implemented </s> def uppercase_next_word(cursor_offset, line):	uppercase_next_word @on('\x1bu')
# todo: account for point size </s> data = []	point_3d_box def point_3d_box(self): face_color = self.face_color if list(face_color.to_rgba()[:3]) == [1, 1, 1]:
# todo remove hardcoded path </s> languages = {'en', 'de'}	_read_wikidata def _read_wikidata(): with bz2.open('C:/Users/Sofie/Documents/data/wikidata/wikidata-20190304-all.json.bz2', mode='rb') as file: line = file.readline()
raise notimplementederror # todo </s> list every item in entry_point that match request	search def search(self, entry_point, request):
# todo(mordred) add this back wnen ksa releases </s> self.assertin('interface_ip', host)	_test_host_content self.assertIsInstance(host['metadata'], dict)
# todo: follow `references` to add reference information here </s> snl = structurenl(	_get_snls_from_resource ) namespaced_data = {k: v for k, v in data["attributes"].items() if k.startswith("_")} structure, authors={},
# todo: could chain of 'source' with the spid </s> source_str = src.path	_PrintWithSpanId elif case(source_e.SourcedFile): src = cast(source__SourcedFile, UP_src) elif case(source_e.Alias): src = cast(source__Alias, UP_src)
# todo(b/160795287): deprecate estimator based executor. </s> fn_args_dict = fn_args.get_dict()	Do fn_args = self._GetFnArgs(input_dict, output_dict, exec_properties) trainer_fn = udf_utils.get_fn(exec_properties, 'trainer_fn') fn_args_dict['serving_model_dir'] = os.path.join(fn_args_dict['model_run'], path_utils.SERVING_MODEL_DIR)
# todo: add more complicated testcases </s> assert_equal(score.shape, (4,))	test_aom_static_repeat random_state=42)
# todo: test! </s> comment = self.topic.comment_set.last()	clean if notification.exists(): raise forms.ValidationError(_("This notification already exists")) if comment is None: raise forms.ValidationError(_("You can't subscribe to a topic with no comments"))
gle = [-1, -1, -1]  # todo calculate based on mesh </s> gre = [1, 1, 1]  # todo calculate based on mesh	_parse_index 'constant', constant_values=(0, 1)) num_grids = min(shape[0], int(np.ceil(functools.reduce(mul, shape) * self.vpg ** -1))) grid_dim_offset = np.linspace(0, domain_dimension[0], num_grids + 1, dtype=np.int32) grid_edge_offset = grid_dim_offset * np.float(domain_dimension[0]) ** -1 * (gre[0] - gle[0]) + gle[0]
# todo docstring </s> logging.info('begin extract')	extract def extract():
# (which seems a bit odd - todo - check with ncbi?) </s> extra = extra.difference(["-gapextend", "-gapopen",	check extra.remove("-off_diagonal_range") if exe_name == "tblastx": "-xdrop_gap", "-xdrop_gap_final"]) if exe_name in ["rpsblast", "rpstblastn"]:
# todo: possibly emit an onscenechanged event </s> setscenelighting(scene)	setScene from glmodule import setSceneLighting
# todo: refactor to compose a list and join with ';', would be more clean. </s> commands = commands.rstrip(";")	build_command if not commands: return None if job_wrapper.version_string_cmd: commands = "%s &> %s; " % ( job_wrapper.version_string_cmd, job_wrapper.get_version_string_path() ) + commands
# todo: once/if we have gpu and language labels then we might be </s> filters = {	update else: stop(skip_names=["nginx-proxy", "update-server"]) "label": [ "_orchest_project_uuid",
# todo: fix test </s> if (sys.version_info.major, sys.version_info.minor) in [(3, 4), (3, 5), (3, 6)]:	test_update_rec_update_all_bookmark assert caplog.records[0].levelname == 'DEBUG' except IndexError as e: print('caplog records: {}'.format(caplog.records)) for idx, record in enumerate(caplog.records):
# todo: need to close computations on this node? </s> node.clusters.clear()	run_job (DispyJob.Cancelled, dispy_node, njob.job))) node.pending_jobs = [] self._nodes.pop(node.ip_addr, None) if self._sched_jobs.pop(_job.uid, None) == _job:
# todo: really need crs specified properly in agdc-metadata.yaml </s> if datum_ == 'gda94':	crs if zone_ and datum_: try: return geometry.CRS('EPSG:283' + str(abs(zone_))) if datum_ == 'WGS84':
@unittest.skip('not written')  # todo: finish! </s> raise notimplementederror	test_float def test_float(self):
# todo: deprecate `extra_info` in favor of `options` </s> options['reactions'] = kwargs.pop('extra_info', false)	get_photos if 'pages' in kwargs: kwargs['page_limit'] = kwargs.pop('pages') options['youtube_dl'] = kwargs.pop('youtube_dl', False) if credentials is not None:
# todo: remove when #980 has been merged </s> info.update(info['formats'][-1])	_get_video_info 'description': description, } return info
# todo: test </s> @overload(gather_scalar)	gather_scalar_overload def gather_scalar_overload(data_t): assert isinstance(data_t, (types.Integer, types.Float))
# todo(aarontp): remove hard-coded sudo in commands: </s> umount_cmd = ['sudo', 'umount', evidence.mount_path]	PostprocessUnmountDisk Args: evidence: A turbinia.evidence.RawDisk or subclass object. log.info('Running: {0:s}'.format(' '.join(umount_cmd))) try:
# todo(dspasovski): fix this. </s> raise skiptest	TestIndexLanding @mock_es def test_good_cat(self): r = self.client.get(self.url) eq_(r.status_code, 200)
# todo: infer kernel arguments </s> [callkernel(kernel_name=new_kernel_name)] +	inner_mapper new_kernel_name = kernel_name_gen() new_schedule.extend( current_chunk + [ReturnFromKernel(kernel_name=new_kernel_name)])
# todo: remove this skip after fixing </s> if sys.version[0] == 3:	test_regular_polygon_draw1 @requires_application() def test_regular_polygon_draw1(): raise SkipTest with TestingCanvas() as c:
# todo: test </s> @overload(gather_scalar)	gather_scalar_overload def gather_scalar_overload(data_t): assert isinstance(data_t, (types.Integer, types.Float))
# @todo: make this configurable </s> location_level = "l3"	vulnerability_update_location_aggregate @param start_date: the start date of the time period (as string) @param end_date: the end date of the time period (as string) db = current.db dtable = current.s3db.vulnerability_data
# todo(piyush): current api-site doesn't contain this api description. </s> uri = '/agents/%s' % agent_id	update_agent :param agent_info: Agent update information. E.g {"admin_state_up": True} return self.update_resource(uri, kwargs)
#todo: popen2("dot -tpng | display") and actually make the graph window pop up </s> print "digraph unix { size = '6,6'; node [color = lightblue2; style = filled];"	print_for_dot def print_for_dot(self): for op in self.order: for input in op.inputs:
# todo(remove this when 11.6.1 is no longer supported) </s> if 'name' in kwargs:	_fixup_name def _fixup_name(self, kwargs): kwargs['name'] = fqdn_name('Common', kwargs['name']) else:
# todo: uncomment when adding support for literal hex bytes </s> print(bytearray(b'hello world   ').isalpha())	test_isalpha print(bytearray(b'hello world').isalpha())
# todo: we can't do this; this shells out for each selection change... </s> actions["vcpush"] = true	update_actions_for_paths _vc.STATE_NONE, _vc.STATE_IGNORED) for s in states) actions["VcUpdate"] = True actions["VcAdd"] = all(s not in ( _vc.STATE_NORMAL, _vc.STATE_REMOVED) for s in states)
#todo: check system tables instead of using cql thrifteries </s> if not any([name == k.name for k in con.con.client.describe_keyspaces()]):	create_keyspace :param **replication_values: 1.2 only, additional values to ad to the replication data map with connection_manager() as con: try: con.execute("""CREATE KEYSPACE {}
# todo, pass also best score </s> if last_path is not none and not self.trainer.testing:	__recover_child_process_weights if self.trainer.checkpoint_callback: self.trainer.checkpoint_callback.best_model_path = best_path ckpt = torch.load(last_path, map_location=lambda storage, loc: storage) model.load_state_dict(ckpt)
# todo: handle parser errors. </s> for event, node in xml.dom.pulldom.parsestring(data):	detect_xspf_header if not data or b'xml' not in data or b'playlist' not in data: return False if event == xml.dom.pulldom.START_ELEMENT: return (node.tagName == 'playlist' and
raise notimplementederror # todo </s> tblock = self.tblock	generate_states def generate_states(self): if self.left_censoring: assert Tblock == len(self.changepoints) blockstateseq = np.zeros(Tblock,dtype=np.int32)
node_list = ursula.batch_from_bytes(nodes, federated_only=self.federated_only)  # todo: 466 </s> for node in node_list:	get_bootnodes raise RuntimeError("Bad response from bootnode {}".format(bootnode.rest_url)) signature, nodes = signature_splitter(response.content, return_remainder=True) self.known_nodes.add(node) self.log.debug("Connected to Bootnode {}|{}".format(bootnode.checksum_address, parsed_url.geturl()))
# todo remove after v0.19 </s> if lock is not none:	open_dataset lock=None, ): warnings.warn( "The kwarg 'lock' has been deprecated for this backend, and is now "
# todo: avoid dummy and generate func here when inlining is possible </s> def _impl(df, n=5):	_impl return hpat.hiframes.pd_dataframe_ext.head_dummy(df, n)
# todo: totally not in vim </s> region = sublime.region(offset, offset + length)	on_patch length = patch[1] patch_text = patch[2] regions.append(region) self.MODIFIED_EVENTS.put(1)
# todo: check error location </s> assert result.data == {'nest': none}	test_non_nullable_list_of_nullables assert len(result.errors) == 1 assert result.errors[0].message == 'Cannot return null for non-nullable type.'
# todo: this can be formulated more efficiently </s> sqrt_ggn = einsum('boc->cbo', (sqrt_ggn_out, )).contiguous()	weight_diag_ggn num_classes = sqrt_ggn_out.size(2) assert tuple(sqrt_ggn_out.size())[:2] == (batch, out_features) sqrt_ggn = sqrt_ggn.view(num_classes * batch, out_features) sqrt_ggn = sqrt_ggn.view(num_classes * batch, out_channels, out_x * out_y)
# todo: toots with only html do not display (images, links) </s> content = "  " + re.sub('<[^<]+?>', '', toot['content'])	home clean = re.sub('<[^<]+?>', '', toot['reblog']['content']) content = username + display_name + clean print(content + "\n")
self.whitelist.add(user_id) # todo: fix passing ints to file.write() </s> write_file('whitelist.txt', self.whitelist)	MusicBot if not user_id: raise CommandError('Invalid user specified') async def handle_blacklist(self, message, username): Usage: {command_prefix}blacklist @UserName
# compare filesizes todo print analysis of this :) </s> comment = "# look at the size savings of that subset process"	ttfautohint_process 'processed': op.getsize(name + '.autohint.ttf') }) cmd = "ls -l %s.*ttf %s" % (op.basename(name), comment) run(cmd, cwd=self.builddir, log=self.stdout_pipe)
# todo: backwards compatibility; remove in favor of class method </s> return str(self.__calibration_directory_from_recording(self._rec_dir))	_calibration_folder @property def _calibration_folder(self):
# todo: for now the memory-server will be booted when jupyter </s> mounts['memory_server_sock'] = mount(	_get_mounts type='bind' ) target='/tmp', source=pipeline_dir,
# todo: reinstate </s> assert not 'error' in res, res	_test_new_all_fields res = fv.submit('preview')
# todo: implement </s> pass	_info def _info(self, arg):
#todo: check the data! </s> count = 0	test_filtered_multiple_sources pipe_def = self._get_pipe_def("pipe_c1cfa58f96243cea6ff50a12fc50c984.json") p = pipe2py.compile.parse_and_build_pipe(pipe_def, verbose=True) for i in p: count += 1
# todo: need to test this logic </s> if index in field:	split index = bc.function_space().index cmpt = bc.function_space().component if len(field) == 1: W = V
'user_id': 'f72265c0-362a-11e0-9e24-005056aa7fb5',  #todo </s> 'username': 'fakeuser@dimagi.com',  #todo	render_vscan_xform 'time_end': datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ'), 'modified_date': datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ'), 'doc_id': uuid.uuid4().hex, 'case_id': case_id,
# todo: also check for already-existing path. </s> click.echo("importing checkpoint... ", nl=false)	download_remote_checkpoint shutil.copyfileobj(response.raw, f) click.echo('done.') output = os.path.join(LUMINOTH_PATH, CHECKPOINT_PATH, checkpoint['id']) with tarfile.open(path) as f:
# todo: see issue #944 </s> context = contexts.fetch(self.dimensions, self.dtype)	wrapper if self._data is None: log("Allocating memory for %s%s" % (self.name, self.shape_allocated)) var = context.make_var(self)
# todo: actually tests mismatchs, this only ensures the code-path is run </s> self._reader(self.traj)	test_persistent_offsets_size_mismatch np.savez(f, **saved_offsets)
# todo(pts): move reused /encoding dicts to separate objects. </s> obj_nums = copy_encoding_dict.get(fd_obj_num)	UnifyType1CFonts if obj_nums is None: obj_nums = copy_encoding_dict[fd_obj_num] = [None, []]
# todo remove this eventually </s> self.is_album = true	ImportTask self.sentinel = False self.remove_duplicates = False self.choice_flag = None @classmethod
# todo figure out something useful to do with the newbranch param </s> @util.transform_notgit	push def push(self, remote, force=False, revs=None, newbranch=False): if isinstance(remote, gitrepo):
return # ::todo:: </s> db2=apsw.connection(":memory:")	testWith except ZeroDivisionError: self.assertRaises(ValueError, blob.read) run(""" with db2.backup("main", self.db, "main") as b:
gc.collect()  # todo: see first comment above </s> ok_(ds2 is not none)	test_Dataset_flyweight ok_(ds4.repo is ds1.repo) del ds1 ok_(ds2.repo is ds3.repo) if not on_windows:
# todo ditto </s> message.respond("oops. cannot find a report called %s for %s. available reports for %s are %s" % \	form continue if not self.handled: (type.upper(), code.upper(), code.upper(), ", ".join([f.keys().pop().upper() for f in forms]))) self.handled = True
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pcmpeqb res = 0x000000ff0000ff00ff00000000ffff00 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo: proper java error? </s> raise runtimeerror('could not find class \'%s\' for jnienv.' % name)	find_class clazz = self._class_loader.find_class_by_name(name) if clazz is None: return clazz.jvm_id
# todo - actually figure out types </s> return [{	infer_schema sheet = book.get_active_sheet() headers = sheet.iter_rows().next() 'column': h.internal_value, 'type': 'unicode'
# todo: check if we can use orm to do that </s> sub_query = """	__get_list queryset = queryset.distinct() if with_comments_count: SELECT COUNT(*) FROM tutorialv2_contentreaction
# todo: renaming function paramters </s> self.assertequals("def f(new_param):\n    print new_param\n", refactored)	xxx_test_renaming_function_parameters1 refactored = self.refactoring.rename("def f(a_param):\n    print a_param\n", 8, 'new_param')
pass  ## fixme: todo </s> else:	_book pass  ## FIXME: TODO
# todo: add this to simulator_objects </s> pop.input_signal.name = name + '[%i].input' % ii	make self.ensembles[name] = rval for ii, pop in enumerate(rval.neurons): pop.bias_signal.name = name + '[%i].bias' % ii pop.output_signal.name = name + '[%i].output' % ii
# todo: fix to avoid the implementation depended on the order of dict implicitly </s> 'metrics': [{'name': k, 'value': float(v)} for k, v in zip(list(metrics_ops_dict.keys()), metrics_values)],	evaluate metrics_dict = { 'last_step': int(last_step), } save_json(output_dir, json.dumps(metrics_dict, indent=4,), metrics_dict["last_step"])
# todo: support the <base> html element, but do not use </s> if url_is_absolute(attr_value):	get_url_attribute attr_value = element.get(key, '').strip() if attr_value: return attr_value elif element.base_url:
# todo: something a bit less heavy than eval </s> terms = safe_eval(terms)	run def run(self, terms, **kwargs): if '{' or '[' in terms: if not isinstance(terms, list): raise errors.AnsibleError("a list is required for with_nested")
# todo: move this to sublime_lib; make it accept a point or a region. </s> def bol(line):	BOL return line.begin()
# todo assert input entity is not none to avoid weird errors </s> self._sender = self._client.get_entity(self.input_sender)	sender def sender(self): if self._sender is None: return self._sender
# todo this should be more modular </s> if 'bindings' in response:	__fetch_service if 'items' in response: targets += response['items'] targets += response['bindings'] if 'accounts' in response:
el_movieposter.set('onselect', "atv.loadurl('"+el_path+"')")  # todo: 'select' - show metadata </s> el_movieposter.set('onplay', "atv.loadurl('"+el_path+"')")	XML_TVSeason el_moviePoster.set('id', 'shelf_item_'+str(aTV_shelf_item)) aTV_shelf_item += 1 el = etree.SubElement(el_moviePoster, 'label') el.text = i.get('title')
# todo: raise description error if 2 values not provided </s> name, value = value.split('|')	render form_class = ' '+ value if key in ['button','submit','hidden']: inputs.append({'name':name,'value':value,'type':key}) response_dict = {
raise skiptest("buggy")  # todo(mattjj): fix </s> device_count = xla_bridge.device_count()	testAxisIndex def testAxisIndex(self): f = pmap(lambda x: x + pxla.axis_index('i'), 'i') x = np.ones(device_count)
# todo: think of something more sensible to do than sum(). on one </s> return sum([((inp - rec)**2).sum(axis=1).mean() for inp, rec in pairs])	mse denoised reconstruction. pairs = izip(inputs, self.reconstruction(inputs))
#@todo: move to utils in 0.4.10 </s> def timestamp():	timestamp return int(time.time() * 1000)
# todo should we pass? </s> pass	WeatherCache else: _log.debug("In WeatherCache: Invalid request type: ?", request_type) self.trim_period = cache_period try:
# todo: date problem here </s> city_on_date = values_for_date.get(city, {})	merge_state_data for date, values_for_date in new_cases.items(): date_str = f"{date.day:02d}_{date.month:02d}" row[f"confirmados_{date_str}"] = city_on_date.get("confirmed", None) row[f"mortes_{date_str}"] = city_on_date.get("deaths", None)
# todo: we need a clean consistent way to get the type of a cap string </s> if cap:	_cap_to_link :param cap: the capability-string :returns: tags.a instance if cap.startswith("URI:CHK") or cap.startswith("URI:SSK"): nameurl = urllib.quote(path[-1].encode("utf-8"))
pass # todo </s> subsystems.	_status_idle notifications when something changed in one of the specified
# todo complete this method </s> return none	eeccsd_matvec def eeccsd_matvec(eom, vector, kshift, imds=None, diag=None):
# todo: implement this. </s> pass	test_del_key def test_del_key(self):
# todo: if the procpool has been exhausted this will block. </s> self.procpool.spawn(process_message, consumer_config,	on_consume_message def on_consume_message(self, consumer_config, consumer_method, body, message): with self.messagesem: consumer_method, body, message)
# todo: remove in v8 </s> self._global_context['blog_desc'] = self.config.get('blog_description')	__init__ self._GLOBAL_CONTEXT['blog_title'] = self.config.get('BLOG_TITLE') self._GLOBAL_CONTEXT['blog_description'] = self.config.get('BLOG_DESCRIPTION') self._GLOBAL_CONTEXT['blog_url'] = self.config.get('SITE_URL', self.config.get('BLOG_URL')) self._GLOBAL_CONTEXT['body_end'] = self.config.get('BODY_END')
# todo deal with other types </s> self.attrs[key] = dynamotype({"s": value})	update elif action == 'SET': key, value = value.split("=:")
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_interact_arguments @classmethod def add_interact_arguments(cls, parser): p_convert = p_operation.add_parser( "convert", help="convert VGM to PCM using OPL hardware")
# todo test </s> self.unconstrained_effect_repertoire(purview))	effect_info return utils.emd(self.effect_repertoire(mechanism, purview),
# todo: untested </s> torch.save(self._model.state_dict(), str(path))	to_disk def to_disk(self, path):
# todo: clarify language here. </s> s = ("the supplied identity is %(cid)s, the server is at %(serv)s,"	__str__ def __str__(self): " identity at the server is %(sid)s" % { 'cid': self.identity_info.consumer_id,
@unittest.skip('not written')  # todo: finish! </s> self.assert_shortened([value], "[b'aaaaaaaaaaaaaaaaaa...aaaaaaaaa']")	test_bytes_large "b'" + 'A' * 43688 + "..." + 'A' * 21844 + "'")
# todo timeout? </s> pass	xcp_command_discovery can_wrap.send_single_message_with_callback(cmd_msg, callback=callback_handler) while not command_reply: can_wrap.clear_listeners() print("\nDone!")
# todo: temporary! remove me once profiling is supported for v2 </s> def test_get_transforms_raises(klio_pipeline, mocker, monkeypatch):	test_get_transforms_raises transforms_module = mocker.Mock() mock_load_source = mocker.Mock()
# todo; not sure what's wrong here. possible bug? </s> assert len(matcher(doc)) == 1	test_matcher_match_zero_plus doc = get_doc(matcher.vocab, words)
# todo: return a list of chapters to download </s> raise not_implemented	get_range_using_index def get_range_using_index(chapter_count):
# todo implement through browser </s> step_message = 'dismiss alert'	dismiss_alert def dismiss_alert(): execution.logger.info(step_message) _capture_or_add_step(step_message, execution.settings['screenshot_on_step'])
# todo private access </s> param_names, _ = value.parent_context._arguments.get_executed_param_names_and_issues(	_check_name_for_execution elif isinstance(value.parent_context, FunctionExecutionContext) and \ compare_node.type == 'funcdef': value.parent_context._value) if len(param_names) != 1:
# todo: for now we use fake eth1 monitor. </s> fake_eth1_data_provider = fakeeth1dataprovider(	Eth1MonitorComponent async def run(self) -> None: trinity_config = self.boot_info.trinity_config start_block_number=START_BLOCK_NUMBER, start_block_timestamp=START_BLOCK_TIMESTAMP,
# todo: handle fancy-index copies by allocating a buffer and </s> next_index = self._subset_iterator.next()	next def next(self): return self._raw_data[next_index]
# todo you should put some extra protection on this, so a user can only </s> return jsonify(get_stored_tokens()), 200	list_tokens @app.route('/auth/tokens', methods=['GET']) def list_tokens():
# todo: some way of indicating progress. </s> click.echo(	download tempdir = tempfile.mkdtemp() path = os.path.join(tempdir, '{}.tar'.format(checkpoint['id'])) "Downloading checkpoint '{}'... ".format(checkpoint['id']), nl=False
# todo: cleanup directory/basename.* files. </s> tmp = tempfile.namedtemporaryfile(	commit def commit(self): directory, basename = os.path.split(self._tag_cache_file) prefix=basename + '.', dir=directory, delete=False) try:
# todo: support steps and times (motion blur) </s> print("dupli export took %.3fs" % (time() - start))	create_session transformations = array("f", duplis.matrices) luxcore_scene.DuplicateObject(src_name, dst_name, count, transformations) engine.update_progress(index / len_objs)
# todo: add option to only show error runs </s> return {	index @nav.active_section('runs') def index(self): 'runs': request.db.get_runs(sort_order='DESC'),
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pcmpeqb res = 0x000000ff0000ff00ff00000000ffff00 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
feature_dim,  # todo define proper size </s> "a",	combine_sparse_dense_features train_utils.tf_dense_layer( batch[key_sparse], self.C2, feature_dim=feature_dim,
# todo how to get number of cells from mesh? </s> a = nm.zeros((len(self.mesh.coors), len(self.mesh.coors)), dtype=nm.float64)	solve def solve(self, t0, tend, tsteps=10): print("Running testing solver: it only calculates matrix A and vector b") b = nm.zeros((len(self.mesh.coors), 1), dtype=nm.float64) A = self.equation.evaluate(dw_mode="matrix", asm_obj=A, diff_var="v")
#todo: make service manager configurable </s> get_service_manager().unregister(self.resolved_name, self)	shutdown logdebug('[%s].shutdown: reason [%s]'%(self.resolved_name, reason)) try: except Exception, e: logerr("Unable to unregister with master: "+traceback.format_exc())
# todo: currently this test breaks the bleu implementation (13.03.2016) </s> references = [[]]	test_empty_references def test_empty_references(self): hypothesis = 'John loves Mary'.split() assert(sentence_bleu(references, hypothesis) == 0)
# @todo: enclosures </s> )	poll_rss location_id = location_id, tags = tags, record = dict(id=id) update_super(mtable, record)
# todo: discriminate between worksheet & workbook ranged names </s> self.rangednames = np.zeros(shape = (int(self.app.activeworkbook.names.count),1), dtype=[('id', 'int_'), ('name', 's200'), ('formula', 's200')])	get_rangednames def get_rangednames(self): for i in range(0, self.app.ActiveWorkbook.Names.Count): self.rangednames[i]['id'] = int(i+1)
# todo: assert </s> profiles = self.remote.get_profiles(self.token)	test_create_profile def test_create_profile(self): Test: create/edit a profile object profile = self.remote.new_profile(self.token) for field in self.profile_fields:
pass # todo </s> def delete(self, playlist):	delete
# todo: may use sys.stdout.encoding if output_file = '-' </s> output_encoding = output_encoding or sys.stdout.encoding or \	query_ def query_(input_encoding, output_encoding, input_locale, output_locale, verify_ssl, fields, source, query, destination): DEFAULT_OUTPUT_ENCODING if not query.lower().startswith('select'):
# todo: handle situations where share is password protected </s> path = string.replace(path,'/', '\\')	list_path def list_path(self, shareName, path, password = None): path = ntpath.normpath(path) if len(path) > 0 and path[0] == '\\':
# todo: find a better way to handle indentation? </s> new_message = ['executed %s times'] + ['\t\t\t%s' % line for line in message.split('\n')]	executemany if self.logger: message = sqlparse.format(sql, reindent=True, keyword_case='upper') self.logger.debug('\n'.join(new_message), duration=duration, id='query') self.logger.debug('Found %s matching rows', self.cursor.rowcount, duration=duration, id='query')
# todo: output_path is an integer, who knows why? </s> del keyfile_descriptor	_save_public_keyfile keyfile.write(key_data) output_path = keyfile.name return output_path
# todo look at media_item_json_status["status"] for individual errors </s> media_item_json = media_item_json_status.get("mediaitem")	download_batch r_json = response.json() for media_item_json_status in r_json["mediaItemResults"]: if not media_item_json: log.warning('Null response in mediaItems.batchGet %s', batch.keys())
# todo: parallelize this </s> for policy in policies:	IAMFacade policies = await AWSFacadeUtils.get_all_pages('iam', None, self.session, 'list_policies', 'Policies', OnlyAttached=True) client = AWSFacadeUtils.get_client('iam', self.session) policy_version = client.get_policy_version(PolicyArn=policy['Arn'], VersionId=policy['DefaultVersionId']) policy['PolicyDocument'] = policy_version['PolicyVersion']['Document']
logfile = open(os.path.join(self.config.logdir, 'exceptions.log'), 'a') #todo: make not hardcoded </s> logfile.write('from %s at %s:\n' % (origin.sender, str(datetime.now())))	error except: pass logfile.write('Message was: <%s> %s\n' % (trigger.nick, trigger.group(0))) logfile.write(trace)
# todo clean up how this is passed around? </s> self.remote_permanent_pubkey = b""	__init__ self.remote_peer_id = peer_id
pass  # todo - should this do something </s> press "join domain" on users tab	on_btchangerole_clicked def on_btchangerole_clicked(self, widget, data=None):
# todo check initial residual with original weights </s> r0 = f(x0)	_optimize splits = numpy.cumsum(sizes)[:-1] x0 = numpy.concatenate([numpy.array(val).flat for val in values_without_weights]) out = minimize(f, x0, method="Nelder-Mead", tol=1.0e-17)
## todo: check if idle is supported </s> return connection	_connection_ssl def _connection_ssl(self): connection = imaplib.IMAP4_SSL(self.server, self.port)
# todo: check how to be writeable only from same group </s> if os.path.exists(pipe_jtop_ctrl):	__init__ self.stats = {} super(JtopServer, self).__init__() print("Remove old pipe {pipe}".format(pipe=PIPE_JTOP_CTRL)) os.remove(PIPE_JTOP_CTRL)
except exception:  # todo: refactor this... </s> e = sys.exc_info()[0]	test_GET_InvalidData try: self.assertEqual(self.rnw.GET(url, data), 'json') self.assertEquals(e, TypeError)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_pass_all_parameters def test_pass_all_parameters(self):
# todo: add a parameter to condition the derivative being returned </s> dim = x.shape[1]	EI self.alpha = np.linalg.solve(self.model.cK, np.linalg.solve(self.model.cK.transpose(), self.model.Y)) def __call__(self, x, Z=None, **kwargs): f_est = self.model.predict(x) eta = self.model.getCurrentBest()
# todo(mcgallaspy): get rid of old integration tests and refactor the mixin methods </s> class contextwithmixin(createadminmixin):	login_as_admin def login_as_admin(context, admin_name="admin", admin_pass="abc123"): if not User.objects.filter(username=admin_name): def __init__(self): self.browser = context.browser
# @todo do not generate __complete if type has no url attribute </s> self.__complete()	__completeIfNeeded if not self.__completed and testedAttribute is None:
# todo: handle cancellation robustly </s> mpf._prec += 10	atan if isinstance(x, mpf): return _make_mpf(fatan(x.val, mpf._prec, mpf._rounding)) t = (0.5j)*(log(1-1j*x) - log(1+1j*x)) mpf._prec -= 10
# todo: test with_polls! </s> user_comments = comment.objects\	likes def likes(request, pk, slug): .filter(comment_likes__user_id=pk)\ .visible()\
# todo tell it to some human operator </s> pass	__init__ self._store[ent[:-5]] = entity(data) except OSError: except IOError: pass
# todo: add only public info </s> update_feed(obj)	add_task_event obj['name'] = r.name obj['short_name'] = r.short_name
pass  # todo: replace this </s> except exception:	__eq__ return False except Exception: return False else:
# todo allow (1, none) and set to identity matrix if k == 1 </s> k_param = iap.handle_discrete_param(k, "k", value_range=(3, none), tuple_to_uniform=true, list_to_choice=true,	MotionBlur Create a motion blur augmenter with kernel size of 15x15 and a blur angle of either -45 or 45 degrees (randomly picked per image). allow_floats=False) angle_param = iap.handle_continuous_param(angle, "angle", value_range=None, tuple_to_uniform=True,
# todo: put this into timeframegroup. #316 </s> return [timeframe_from_dict(d) for d in dicts]	list_of_timeframes_from_list_of_dicts def list_of_timeframes_from_list_of_dicts(dicts):
# todo implement </s> ret = 1	hook_GetStringTypeW }) def hook_GetStringTypeW(ql, address, params): return ret
"""todo: fixed code to test passed """ </s> code = 'def a_func():\n'\	test_extract_function_with_for_else_statemant_more def test_extract_function_with_for_else_statemant_more(self): '    for i in range(10):\n'\ '        a = i\n'\
# todo make test_var `nonlocal` once we drop py2 -- it can just be a </s> test_var[dict_key] += 1	set_global def set_global(dict_key, old_value, new_value):
# todo: support minp arg end_range etc. </s> minp = win	roll_sum_fixed N = len(in_arr) output = np.empty(N, dtype=np.float64) range_endpoint = max(minp, 1) - 1 for i in range(0, range_endpoint):
# todo: test me. </s> def plugin_unloaded():	plugin_unloaded view = sublime.active_window().active_view() view.settings().set('command_mode', False)
#todo (this should be done even when add_trust hasn't been </s> n.attr['color'] = colors['insecure_non_existent']	status_for_node n.attr['color'] = COLORS['bogus_non_existent'] elif 'dashed' in style: status = Status.RRSET_STATUS_NON_EXISTENT else:
# todo: think of better way doing this. </s> if len(samples) == 1 and isinstance(samples[0], list):	_cached_as extra = kwargs.get('extra') _get_key =  kwargs.get('_get_key') return lambda func: func def _get_queryset(sample):
#todo: rally_land points </s> for i in range(mpstate.status.rallyloader.rally_count()):	list_rally_points return mpstate.status.rallyloader.append_rally_point(p) p = mpstate.status.rallyloader.rally_point(i) mpstate.console.writeln("lat=%f lng=%f alt=%f break_alt=%f land_dir=%f" % (p.lat * 1e-7, p.lng * 1e-7, p.alt / 100.0, p.break_alt / 100.0, p.land_dir / 100.0))
# todo(sfinucan): remove this warning when the named config options </s> if max_instances_per_host < 1:	_get_max_instances_per_host def _get_max_instances_per_host(self, host_state, spec_obj): max_instances_per_host = CONF.filter_scheduler.max_instances_per_host LOG.warning(_LW('Future versions of nova will restrict the ' '"filter_scheduler.max_instances_per_host" config option to '
# todo generator </s> handle_dirty_datasets(	__call__ raise ValueError( 'inappropriate arguments, see previous error message(s)') content_by_ds, mode=if_dirty, base=dataset) for ds_path in sorted(content_by_ds, reverse=True):
# todo: should this test support cff as well? </s> logging.debug("is there any unused data at the end of the glyf table?")	main else: logging.error('Encoding mismatch between NAMEID_FONT_FAMILY_NAME and NAMEID_FULL_FONT_NAME entries.') if 'CFF ' not in font: logging.info("Skipping test. Not a CFF font.")
# todo: for backward compatibility only, remove if not used anymore </s> self.cs = cloudstack(**read_config())	_connect else:
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	get_diagnostics def get_diagnostics(self, instance):
# todo: truffle revertme once doctest is supported (once io can read text files) (gr-9151) </s> return tests	load_tests def load_tests(loader, tests, pattern):
# todo: add kwargs in command </s> cmd, _, args = command	handle_func_command <no self>, arguments[, kwargs]) :return: the response of the function command print("Logtensor logging function", cmd) new_args, new_type = syft.frameworks.torch.hook_args.hook_function_args(cmd, args)
# todo(qos): figure out why it passes locally but fails in gate </s> self.skiptest("gate is voodoo failing")	test_create_network_bulk_rpc_outside_transaction def test_create_network_bulk_rpc_outside_transaction(self): with mock.patch.object(ml2_plugin.Ml2Plugin, '__init__') as init,\ mock.patch.object(base_plugin.NeutronDbPluginV2,
if event.keysym in ("control_l", "control_r", "command"):  # todo: check in mac </s> self.text.tag_configure("value", foreground="darkblue", underline=1)	_text_key_press def _text_key_press(self, event):
# todo: timeline is global, get rid of it </s> for post in timeline:	gen_task_render_posts default_lang for lang in kw["translations"]: source = post.source_path dest = post.base_path
# todo(b/142684737): the multi-processing api might change. </s> beam_pipeline_args=['--direct_num_workers=%d' % direct_num_workers])	_create_pipeline metadata_connection_config=metadata.sqlite_metadata_connection_config( metadata_path),
# todo: drape topography? </s> m = np.c_[mx, np.zeros_like(mx)]	genDCSurvey_2D Ax = self.SrcLoc[iSrc, 0] Bx = self.SrcLoc[iSrc, 1] N = np.c_[Nx, np.zeros_like(Nx)] A = np.r_[Ax, 0.].reshape([1, -1])
# todo(user): remove version when we update aws-cli </s> f"sudo pip3 install 'awscli=={_version}' "	Install vm.RemoteCommand(
# todo: verify these values and formula </s> block reward calculation	calc_coeff @staticmethod def calc_coeff(N_tot, block_tot): decay curve: 200 years (until 2217AD, 420480000 blocks at 15s block-times) N_tot is less the initial coin supply.
session.add(job)  # todo review this after remapping job (required to lazy-load attr) </s> obj.job = job	TestImplicitCollectionJobsJobAssociation obj = cls_() obj.implicit_collection_jobs = implicit_collection_jobs obj.order_index = order_index with dbcleanup(session, obj) as obj_id:
# todo remove after pytorch 1.0 </s> with warnings.catch_warnings():	wrap_batch for batch_num, _batch in enumerate(batch): if thu.is_tensor(_batch): warnings.simplefilter('ignore') variable_batch.append(Variable(_batch, requires_grad=requires_grad,
# todo: download bwta </s> if not args.headless:	run_game if "sscai" in args.map and not exists(f"{args.map_dir}/sscai"): download_sscait_maps(args.map_dir) check_vnc_exists() if args.human and args.headless:
# todo: do we change this to something like "threshold" </s> m, n = int(request.args['m']), int(request.args['n'])	grant bob_pubkey = bytes.fromhex(request.args['bob_encrypting_key']) label = bytes.fromhex(request.args['label']) expiration_time = maya.MayaDT.from_iso8601( request.args['expiration_time'])
# todo: there's still some code duplication with static method gitrepo.get_git_dir() </s> if dot_git.is_file():	_get_dot_git def _get_dot_git(pathobj): dot_git = pathobj / '.git' with dot_git.open() as f: line = f.readline()
#todo - these are not currently implemented as properties, this means </s> for the simple case this uses the python splicing syntax, [122:150]	FeatureLocation self._end = ExactPosition(end) def __str__(self): (zero based counting) which GenBank would call 123..150 (one based counting).
# todo: this test requires manifold access, see: t88318502 </s> self._test_rcnn_model("coco-instancesegmentation/mask_rcnn_r_50_c4_3x.yaml")	TestScripting @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available") def testMaskRCNNC4(self): @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available") def testRetinaNet(self):
# todo log here </s> return false	auth_okta return False if response.status_code != 200:
# todo (#567): bucket the node as suspicious </s> return	learn_from_teacher_node self.known_nodes.mark_as(current_teacher.InvalidNode, current_teacher) self.log.warn(f"Teacher {str(current_teacher)} is invalid (hex={bytes(current_teacher.metadata()).hex()}):{e}.") except RuntimeError as e: if canceller and canceller.stop_now:
# todo(vek): fails until remove_fixed_ip() added </s> self.assertequal(resp.status_int, 202)	test_remove_fixed_ip req.headers['content-type'] = 'application/json' resp = req.get_response(fakes.wsgi_app()) self.assertEqual(last_remove_fixed_ip, ('test_inst', '10.10.10.1'))
# todo: re-enable </s> salt.utils.warn_until(	prep_jid def prep_jid(cachedir, sum_type, user='root', nocache=False): Return a job id and prepare the job id directory 'Boron', 'All job_cache management has been moved into the local_cache '
# todo: if input path was http, revert to that and try again. </s> return facts	inspect_sparkle_feed_url "There seems to be a problem with the developer's SSL " "certificate. (%s)" % err) xmlns = "http://www.andymatuschak.org/xml-namespaces/sparkle" try:
# todo: verify how pandas sorts column names </s> all_colnames = sorted(set(all_colnames))	generic for df in objs.types: all_colnames.extend(df.columns) all_data = [] for cname in all_colnames:
# todo(stephenfin): the mock of 'migrate_disk_and_power_off' should </s> with mock.patch(	test_cold_migrate_server_with_pci for hostname in ('test_compute0', 'test_compute1'): self.assertPCIDeviceCounts(hostname, total=2, free=1) 'nova.virt.libvirt.driver.LibvirtDriver' '.migrate_disk_and_power_off', return_value='{}',
# todo implement </s> ret = 1	hook_GetStringTypeW }) def hook_GetStringTypeW(ql, address, params): return ret
# todo check the actual transformation matrix. </s> self.assertless(mss, 1)	testICP - self.source.to_array(), axis=1) ** 2).mean()
#@todo: remove in 0.4.10 </s> def error(self, reason=none, type="parse"):	error raise Fail("%s error%s | Plugin out of date" % (type.capitalize(), ':' + str(reason) if reason else "")) if self.core.debug:
# @todo: releases </s> return	commits @click.argument('repo', default='') def commits(username):
# todo: remove this block once migration is complete </s> if latest and request.get.get('profile') and toggles.release_builds_per_profile.enabled(domain):	_safe_cached_download latest_enabled_build = get_latest_enabled_app_release(domain, user_location_id, parent_app_id) if not latest_enabled_build: latest_enabled_build = get_latest_enabled_build_for_profile(domain, request.GET.get('profile')) try:
pass # todo </s> def try_undo(self, *args):	try_undo
# todo: reimplement run_config_yaml against user process </s> assert schedule['status'] == 'running'	test_get_all_schedules if schedule['scheduleDefinition']['name'] == 'no_config_pipeline_hourly_schedule':
# todo it would be nice if we didn't need to copy this or run the </s> res = {	generate_vscode_script def generate_vscode_script(gdbpath, root, sysroot, binary_name, port, dalvik_gdb_script, solib_search_path): "name": "(gdbclient.py) Attach {} (port: {})".format(binary_name.split("/")[-1], port), "type": "cppdbg",
# todo(leofang): support multi-gpu callback (devices is ignored) </s> plan = config.get_static_plan('plan1d', fft_type, keys[:-3])	_exec_fft cache[keys] = plan else:  # has_callback cache[keys] = plan else:
# todo: replace with "yield from" when dropping python 2. </s> for __ in self._create_streams('web', video_id).items():	_get_streams return video_id = match.group(1) yield __ for __ in self._create_streams('webhd', video_id).items():
# todo cleanup/merge with above `simple_shell` once we have `subprocess.run` after dropping python 2 support? </s> assert subprocess.check_output(['where', 'pip'], env=env, universal_newlines=true).splitlines()[0] == os.path.join(installation_path, 'scripts', 'pip.exe')	build simple_shell(['python', get_pip_script], env=env, cwd="C:\\cibw") assert os.path.exists(os.path.join(installation_path, 'Scripts', 'pip.exe')) simple_shell(['python', '-m', 'pip', 'install', '--upgrade', 'pip'], env=env) simple_shell(['pip', '--version'], env=env)
# todo: remove after implementing django-constance </s> if not dfirtrack_config.csv_skip_existing_system:	system return redirect(reverse('system_list')) else: messages.warning(request, 'WARNING: Existing systems will be updated!')
assert self.stop_seq is none #todo: better handling of this situation </s> l.debug("initializing start sequence")	start_program def start_program(self): assert self.start_seq is None #TODO: Better handling of this situation self.start_seq = sequence_controller() for p in self.roaster:
# todo: multi dp route resolver needs to flood out stack ports </s> self.host_ping(v100_host, first_faucet_vip.ip)	test_path_no_vlans self.add_host_route(v100_host, v200_host_ip, first_faucet_vip.ip) self.add_host_route(v200_host, v100_host_ip, second_faucet_vip.ip) self.host_ping(v200_host, second_faucet_vip.ip) self.host_ping(v100_host, v200_host_ip.ip)
# todo(nate): temporarily disabled </s> if false:	test_wrong_jobstep_id jobstep = self.create_jobstep(jobphase) handler = ManifestJsonHandler(jobstep) fp = StringIO(self.json_file_format % '1') handler.process(fp)
# todo - if we want to enable single-direction </s> collision_lines_map.add((left_side_tile, "right"))	loadfile right_side_tile = (x,curr_y) curr_y += 1 collision_lines_map.add((right_side_tile, "left")) elif line_type is 'horizontal':
# todo: move this to sublime_lib; make it accept a point or a region. </s> def geteol(view, point):	getEOL return view.line(point).end()
# todo(remove this when 11.6.1 is no longer supported) </s> tmos_v = self._meta_data['bigip']._meta_data['tmos_version']	create def create(self, **kwargs): tmos_v = LooseVersion(tmos_v) if tmos_v < LooseVersion('11.6.0'):
# todo: handle temperr case (e.g. dns timeout) </s> if r[0] not in ["pass", "none"]:	spf_pass LOG.error("SPF error, mailbox %s, ip %s", mailbox.email, ip) else: LOG.error( "SPF fail for mailbox %s, reason %s, failed IP %s",
# todo: fixed by using realpath, but there should be a cleaner </s> db2.get(opj(realpath(path), '2git'))	_test_AnnexDB set(db2.get_obsolete()), {opj(realpath(path), p) for p in ['2git']}) assert_equal(db2.get_obsolete(), [])
response=none,  # todo: fix this, make param </s> field_list=nir_field_list, is_offline=false	lookup_whois nir_data = nir_whois.lookup( nir=nir, inc_raw=inc_raw, retry_count=retry_count, ) results['nir'] = nir_data
# todo: support unicode </s> num_total_chars = num_total_chars_set_string(a)	set_string_to_array def set_string_to_array(A): num_strs = len(A) str_arr = pre_alloc_string_array(num_strs, num_total_chars)
# todo: in #5022 </s> order_url = build_absolute_uri(order.get_absolute_url())	get_order_confirmation_markup def get_order_confirmation_markup(order: "Order") -> str: organization = get_organization() data = { "@context": "http://schema.org",
# todo: align series </s> print [getattr(s[0], op)(s[1]) for s in zip(self, other)]	_series_op print type(op), op if isinstance(other, GeoSeries): return Series([getattr(s[0], op)(s[1]) for s in zip(self, other)], index=self.index)
# .. todo :: disconnect done slot/signal </s> except exception, e:  # pylint: disable=w0703	aggregateResults QtGui.qApp.restoreOverrideCursor() self.completed() QtGui.qApp.restoreOverrideCursor() self.hideBusy()
# todo use properties here to infer mechanism and purview from </s> return mip(direction=direction,	_null_mip @staticmethod def _null_mip(direction, mechanism, purview): mechanism=mechanism, purview=purview,
return  # todo return placeholder "[loading]" track? </s> if sp_track.error != spotify.errortype.ok:	to_track_ref def to_track_ref(sp_track): if not sp_track.is_loaded: return  # TODO Return placeholder "[error]" track? if sp_track.availability != spotify.TrackAvailability.AVAILABLE:
# todo: request whole file from server </s> region = sublime.region(0, view.size())	apply_patches if cur_hash != patch_data['md5']: print "new hash %s != expected %s" % (cur_hash, patch_data['md5']) print "region", region MODIFIED_EVENTS.put(1)
# todo: should be able to change dash, plus and pipe </s> filename, fobj = get_filename_and_fobj(filename_or_fobj)	import_from_txt def import_from_txt(filename_or_fobj, encoding='utf-8', *args, **kwargs): kwargs['encoding'] = encoding contents = fobj.read().decode(encoding).strip().splitlines()
#todo generate the labels for the dict automatically from labels </s> result = {'time': time_array, 'lc_3to6kev': y1,	parse_obssumm_file dim = np.array(y1).shape[0] time_array = [reference_time_ut + timedelta(0,time_interval_sec*a) for a in np.arange(dim)] 'lc_12to25keV': y2, 'lc_25to50keV': y3, 'lc_50to100keV': y4, 'lc_100to300keV': y5,
# todo consider adding the individual tiles to the resource? </s> tilesets.append(tileset)	load_tmx path = resource.find_file(c.attrib['source']) tileset = TileSet.from_atlas(name, firstgid, path, tile_width, tile_height) resource.add_resource(name, tileset) elif c.tag == 'tile':
'lb': [], #todo on apache level config </s> 'staticfiles': ['10.176.162.109'],	production 'django_celery': ['192.168.100.60'], 'django_app': ['10.176.160.43', '10.176.163.85'], } if env.roles == []:
# todo: check error location </s> assert result.data == {'nest': none}	test_non_nullable_list_of_non_nullables assert len(result.errors) == 1 assert result.errors[0].message == 'Cannot return null for non-nullable type.'
#todo: this isn't actually most_recently_used (as defined in histories) </s> if( ( trans.user == none )	show hda_dict = {} try: and ( history_id == trans.security.encode_id( trans.history.id ) ) ): history = trans.history
# todo make "master" not hard-coded, fetch it from some metadata </s> branchname = default_branch	commitVolume self.output(commitId) volume = self._directory.child(volume) branch = volume.child("branches").child(branchName) commit = volume.child("commits").child(commitId)
return # todo: determine cause of double-callbacks </s> log.debug("manager's running experiment has finished")	_finish def _finish(self): if not self.is_running(): experiment = self._running_experiment self._clean_up()
# todo assert cls.__tablename__ == '' </s> with dbcleanup(session, cls):	test_LibraryDatasetDatasetAssociationTagAssociation model, session, library_dataset_dataset_association, tag, user): cls = model.LibraryDatasetDatasetAssociationTagAssociation user_tname, value, user_value = 'a', 'b', 'c' obj = cls(user=user, tag_id=tag.id, user_tname=user_tname, value=value)
# todo(rbharath): this will cause an issue with duplicates! </s> closest_nbr_locs = tf.nn.top_k(dists, k=m)[1]	compute_neighbor_list dists = tf.reduce_sum((tiled_coords - nbr_coords)**2, axis=3) dists = tf.reshape(dists, [N, -1]) split_closest_nbr_locs = [tf.squeeze(locs) for locs in tf.split_v(closest_nbr_locs, N)] nbr_inds = tf.reshape(nbr_inds, [N, -1])
# todo: update the tolerance after the ci moves to torch 1.10 </s> self.asserttrue(torch.allclose(outputs.logits[:, :4], expected_logits, atol=1e-2))	test_inference_diarization self.assertEqual(labels[0, :, 0].sum(), 258) self.assertEqual(labels[0, :, 1].sum(), 647)
# todo(bowen): check </s> sys.stdout = old_stdout	nostdout finally:
raise notimplementederror #todo </s> elif q.kw(i,"format"):	parse while i < l: if q.kw(i,"RETURN"): raise NotImplementedError #TODO elif q.kw(i,"REQUEST"):
# todo: handle those values correctly </s> return	testAUC def testAUC(self, clf): if isinstance(clf, MulticlassClassifier): clf.states._changeTemporarily(enable_states = ['values']) for ds in [datasets['uni2small'], datasets['uni3small']]:
#    todo: </s> self._db.commit()	load else: if retry: self.load(self, id, full, retry=False) else:
# todo: issue an warning </s> pass	InterfacesForm c.call('routes.sync') except Exception: def done(self, *args, **kwargs): super(InterfacesForm, self).done(*args, **kwargs)
# todo: if the editor is badly set up, this fails </s> subprocess.popen(editor)	open_cb editor = make_custom_editor_command(path, line) if editor: else: os_open(path)
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_psrldq_2 res = 0x00000000000000000000000000000000 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init)
# todo implement. </s> return 'ok'	AsynchronousEASGD iteration = data['iteration'] worker_id = data['worker_id'] @app.route('/shutdown', methods=['GET']) def shutdown():
# todo: skips header parsing </s> line0 = lines[iline].strip().lower()	read_part line0 = lines[iline].strip().lower() elif '*rotary inertia' in line0: data_lines = [] while not line0.startswith('*'):
return -1 # todo: followup after decision around returning none </s> layer_name = self.vol.layer_name	helper_session_id if hasattr(self, "Session"): if self.Session == 0: symbol_table_name = self.get_symbol_table().name kvo = self._context.memory[layer_name].config['kernel_virtual_offset']
# todo(crcrpar): support botorch v0.4.0. </s> "botorch<0.4.0 ; python_version>'3.6'",	get_extras_require "dask[dataframe]", "dask-ml", "fastai", "optax",
# todo: better check for train_flag. </s> if key == rec_layer.target and (	get_output used_keys.add(rec_layer.target)  # we always need the target of the recurrent layer for key in sorted(used_keys): rec_layer.network.train_flag is False or self.parent_net.search_flag) and not rec_layer._cheating: continue
# todo: fix this poor naming convention to better support regression tasks. </s> probs = probs.squeeze()	spearman_corr def spearman_corr(gold, _, probs): corr, p_value = spearmanr(gold, probs) return {"spearman_corr": corr}
# todo: make the get_closest_value to return region </s> value, value_index = self.get_closest_value(	get_current_CSS_value parsed_declaration = re.search(r'^(\s*)(-[a-zA-Z]+-)?([a-zA-Z0-9-]+)(\s*(?: |\:))((?:(?!\!important).)+)', declaration) declaration_index = declaration_index + parsed_declaration.start(5) parsed_declaration.group(5), declaration_index,
# todo: check/show the limits of low and high </s> return none	tune_option_validate return k
# todo: change when multiple envs </s> mean_actions = self.actor_net(latent)	get_policy_stats state = th.FloatTensor(state).to(self.device) latent = self.shared_net(state) action_std = th.ones(mean_actions.size()) * self.log_std.exp() action_distribution = Normal(mean_actions, action_std)
# make user perm on doctype 'todo' in assignment rule (unrelated doctype) </s> add_user_permissions(get_params(user, "doctype", "todo", applicable=["assignment rule"]))	test_user_perm_on_new_doc_with_field_default ], unique=0) doc.insert() frappe.set_user("new_doc_test@example.com") new_doc = frappe.new_doc("Doc A")
# clone it: (todo: use install here, once it is redone) </s> annexrepo(c_path, o_path, create=true)	test_get_recurse_dirs origin = Dataset(o_path).create(force=True) origin.save("Initial", auto_add_changes=True) ds = Dataset(c_path) file_list = ['file1.txt',
# todo: check travis build and remove skip when test passed. </s> if ctx.exception.errno == errno.eagain and os.environ.get('travis'):	test_disconnect_from_nonbinded_addr with self.assertRaises(OSError) as ctx: tr.disconnect('ipc:///some-addr')  # non-bound addr raise unittest.SkipTest("Travis has a bug, it returns " "EAGAIN for unknown endpoint")
# todo: skipped due to gh-4436 </s> yield skip_if_on_windows(skip_ssh(_test_binary_data)), 'datalad-test'	test_binary_data def test_binary_data(): yield _test_binary_data, None
# todo node_tags? </s> elem_types, elem_tags, _ = gmsh.model.mesh.getelements(dim, e)	extract_to_meshio cell_sets[name] = [[] for _ in range(len(cells))] for e in gmsh.model.getEntitiesForPhysicalGroup(dim, tag): assert len(elem_types) == len(elem_tags) assert len(elem_types) == 1
# todo stub </s> pass	create_catalyst def create_catalyst(self, original_df):
#todo a tester </s> if 'usasports.live' in url:	showHosters token = base64.b64decode(token[0]) sHosterUrl = 'https://telerium.tv/'+m3u+token + '|referer='+url oRequestHandler = cRequestHandler(url) sHtmlContent2 = oRequestHandler.request()
# todo will i ever return false? </s> is_head = next((true for branch in branches if sha == branch.commit.sha), none)	_check_permissions node_settings.user, node_settings.repo, branch ) else: is_head = True
# todo extend to nonbinary nodes </s> if i not in self._input_indices and i in self.context.node_indices:	__init__ else: self._dimension_labels.append(None) past_tpm_on = past_tpm_on.sum(i, keepdims=True) / 2 past_tpm_off = past_tpm_off.sum(i, keepdims=True) / 2
# todo(luotao): use clone() method to flush the program.desc in force, </s> program = program.clone()	_depthwise_conv_mkldnn current_op.desc.set_type("conv2d") i = i + 1
raise exception('not valid monitor')  # todo: custom exception </s> return super(monitorsuite, self).addtest(monitor)	add_monitor elif isinstance(monitor, (MonitorBase, MonitorSuite)):
common_path=prefix,  # todo: add key? </s> action="local",	make_inline_attachments_decision ld = local_diff[k] md = MergeDecision( conflict=False, local_diff=[ld],
#todo _rule_for_states needs to made into a generator </s> self._update_node_observed_status(node)	add_states n for n in range(len(states))]
# todo: consider removing; so many system accounts/groups exist, it's likely to fail </s> else:	has_expected_permissions elif sid == admins_sid: continue if not (permissions == FULL_CONTROL and ace_type == ACE_TYPE_DENY): return False
# todo: handle timeout </s> return self.__socket.recv(1024)	read @keyword timeout: the maximum time in millisecond to wait before a message can be reached @type timeout: :class:`int`
# todo/fixme: check that the shapes are correct! </s> for i in range(len(self.phi)):	load def load(self, group): Load the state of the node from a HDF5 file. phii = group['phi%d' % i][...] self.phi[i] = phii
# todo: a smart way of choosing the number of streams, see #612. </s> if isinstance(size, (tuple, list)) and all([isinstance(i,int) for i in size]):	n_streams def n_streams(self, size): r = 1 for s in size:
# todo parse to dataframe </s> req_url = "http://www.google.com/trends/topcharts/chart"	topcharts def topcharts(self, chart_payload): req = self.ses.post(req_url, params=chart_payload) results = json.loads(req.text)
# todo: errors </s> con = sc.connect_to_service_finish(results)	rest_post_connected def rest_post_connected(self, sc, results, command, data, callback, error_callback, callback_data): if con == None: return
# todo: figure out if both android:name and name tag exist which one to give preference </s> _value = tag.get(self._ns(attr)) or tag.get(attr)	is_tag_matched return True for attr, value in attribute_filter.items(): if _value != value: return False
# todo: how to create a super instance using continuations? </s> vals = field_values + self.auto_values	constr_proc_cont super_field_values, field_values = split_list(field_values, split_position) something, env, cont = self.super_type.constructor().call(super_field_values, env, cont) immutable_vals, mutable_vals = [], [] for idx, val in enumerate(vals):
marked[id(atom)] = atom # since marked means "it's been appended to the todo list" </s> while todo:	marksingle todo = self.selatoms.values() # list of atoms we must still mark and explore (recurse on all unmarked neighbors) for atom in todo: newtodo = [] for atom in todo:
# todo: make it really async. </s> self.database_name = database_name	__init__ self.server_name = server_name
# todo (straya): implement </s> self.log.info("generator rejected")	WalletNode ): The full node rejected our request for generator
""" todo: documentation </s> return false	streaming @property def streaming(self):
# todo(shivaniagrawal): rescaling might be expensive for softmax; move </s> weights = [	_ConcatWeights if self._transpose_weight_params: concat_axis = 0 self.AqtWeight(self.QWeight(theta['weight_%d' % i]), feature_axis=-1) for i in range(p.num_shards)
### todo: change to support different metrics. </s> if ((cur_epoch_idx + 1) % self.config.early_stop_epoch) == 0:	tune_model for cur_epoch_idx in range( self.config.epochs): loss = self.train_model_epoch(cur_epoch_idx, tuning=True) if patience_left > 0 and previous_loss <= loss: patience_left -= 1
# todo: add logging </s> pass	parse_flatness def parse_flatness(self, token='i', params='i'):
# todo make sure we can still read an unconstrained successor </s> self._windup_to_unconstrained_successor()	_write_with_ROP chain_bvv = self.crash.state.solver.BVV(chain.payload_str()) self.crash.state.add_constraints(chain_mem == chain_bvv) glob_data = self.crash.state.memory.load(addr, len(data)) data_bvv  = self.crash.state.solver.BVV(data)
# todo also test these! </s> continue	test_classifiers continue if Clf in [MultinomialNB, BernoulliNB]: clf = Clf() clf.fit(X, y)
# todo: write </s> return dict.fromkeys(key_strings, callback)	string_keys_to_dict def string_keys_to_dict(key_strings, callback):
# todo: implement </s> completed remotely.	whenExecuted @return: a L{Deferred} that fires with C{None} when the work has been
# todo(ecastill) create a signature and do a look up </s> pass	__call__ ret_dtype = dtype else: if ret_dtype is None: dtype = args[0].dtype
# todo: shouldn't this be the other way around?! </s> self.assertequal([], re)	test_parser_re_link w = WMLParser( response ) re, parsed = w.get_references() self.assertEqual(len(parsed), 1) self.assertEqual(u'http://www.w3af.com/index.aspx', parsed[0].url_string)
# todo: for backward compatibility only, remove if not used anymore </s> def get_zone_id(self):	get_zone_id return get_zone(key='id')
# todo: implement smarter approach to merging </s> else:	processPage u'A claim for %s already exists. Skipping' % claim.getID()) if claim.getType() == 'wikibase-item': match = re.search(pywikibot.link_regex, value)
# todo: we may want to log this as soon as mobile ucr stops hitting this </s> if not isinstance(filter, dynamicchoicelistfilter):	get_choices def get_choices(data_source, filter, search_term=None, limit=20, page=0): return [] table = get_indicator_table(data_source)
# todo: base on ssa instead. </s> needs_check = generator.decidevariableneedscheck(variable),	_generateExpressionCode to_name     = to_name, variable    = variable, emit        = emit, context     = context
# todo: log exception </s> continue	_write_missing_config conf = module.DEFAULTCONF except Exception as e: ConfNeedsWrite = True config_object.add_section(module)
# todo: hack so every disk is not synced independently during boot </s> if os.path.exists('/tmp/.sync_disk_done'):	_event_devfs if data['cdev'] not in disks: return middleware.call('notifier.sync_disk', data['cdev']) try:
# todo: perhaps we might merge (without duplicates) </s> if aggregates and measures:	aggregate if not cell: cell = Cell(self.cube) raise ArgumentError("Only aggregates or measures can be " "specified, not both")
# todo: is there a way to do this without eval?  eval allows arbitrary </s> return eval(selector_string, namespace, {})	eval_selector def eval_selector(selector_string, namespace): try: except NameError as e: missing_var = parseNameNotFound(e)
# todo: https://github.com/turicas/brasil.io/issues/209 </s> result = {	format_spreadsheet_rows_as_dict } This is an auxiliary method used by covid19.forms.StateSpreadsheetForm with the uploaded file 'total': {}, 'importados_indefinidos': {},
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_interact_arguments "--sea-level-pressure", type=float, metavar="PRESSURE", default=101325, help="use PRESSURE Pa as sea level pressure (default: %(default)f)") p_measure = p_operation.add_parser( "measure", help="read measured values")
# todo(boris-42): make it work through assertisinstance </s> self.assertequal(str(type(engine_inst)), str(e))	test_get_engine engine_inst = deploy_engine.EngineFactory.get_engine(e.__name__, None)
# todo fix redirect loop here </s> app.config['security_unauthorized_view'] = '/login'	add_config_from_configparser_to_app app.config['SECURITY_PASSWORD_SALT'] = config.get('data_storage', 'password_salt').encode() app.config['SQLALCHEMY_DATABASE_URI'] = config.get('data_storage', 'user_database', fallback='sqlite:///') app.config['LOGIN_DISABLED'] = not config.getboolean('ExpertSettings', 'authentication') app.config['SECURITY_USER_IDENTITY_ATTRIBUTES'] = [{'email': {"mapper": uia_username_mapper, "case_insensitive": True}}]
pass # todo(denero) implement </s> def test_missing_field(self):	test_missing_field
# todo results from ml </s> return	_get_prev_device_behaviors @staticmethod def _get_prev_device_behaviors(endpoint):
# todo: remove this once connexion can validate enums with openapi3. </s> if field_name not in ['info', 'action_apis', 'condition_apis', 'transform_apis', 'device_apis', 'tags',	__func @permissions_accepted_for_resources(ResourcePermissions('app_apis', ['read'])) def __func(): 'externalDocs']: return Problem(BAD_REQUEST, 'Could not read app api.', '{} is not a valid field name.'.format(field_name))
# todo: if this is the initial load of logging config we might not </s> logger.debug('config file %s not found; skipping', filename)	_load_file '%s has errors, line %s has been ignored.', filename, linenos) except IOError:
f.write(self.pastie_content.encode('utf8'))  # todo error checking </s> f = open(directory + os.sep + self.id, 'w')	savePastie raise SystemExit('BUG: Content not set, sannot save')
# todo(kgriffs): uncomment when 3.0 development opens </s> def __repr__(self):	Response self._data = None return '<%s: %s>' % (self.__class__.__name__, self.status) def set_stream(self, stream, content_length):
# todo: what actually raises valueerror in the following code? </s> try:	read def read(self, *args, **kwargs): result = super(GzipStreamFile, self).read(*args, **kwargs) if result is None:
# todo: add a .parse() method that includes boths steps? </s> vi_cmd_data = self.parse_motion()	VintageState def run(self): if self.cancel_action: vi_cmd_data = self.parse_action(vi_cmd_data) if vi_cmd_data['must_blink_on_error']:
# todo: test coverage </s> my_newsletter = get_object_or_404(	archive_detail def archive_detail(request, newsletter_slug, year, month, day, slug): Newsletter.on_site, slug=newsletter_slug, visible=True )
# todo consider to use ansible's 'to_nice_yaml' from </s> with open("{}/config.yml".format(group_vars_dir), "w") as _f:	main if not os.path.isdir(group_vars_dir): os.makedirs(group_vars_dir) _f.write('---\n') for role, params in roles.iteritems():
# todo: error checking </s> self._cookie = none	logout headers = headers, )
if is_v6 or self.is_direct_mode() or batch or not allow_quick:  # todo: thin mode </s> modified = self.get_changed_files() if is_v6 else []	file_has_content For each input file states either file has content locally is_v6 = self.config.get("annex.version") == "6" find = self.find(files, normalize_paths=False, batch=batch) return [bool(find[f] and not (is_v6 and f in modified))
# todo: refactor this method. </s> if isinstance(event, event):	waitEvent def waitEvent(self, event, *channels, **kwargs):  # noqa event_object = event event_name = event.name
print_settings["bottom_thickness"] = none  # todo; can be different per extruder & per mesh </s> print_settings["bottom_thickness"] = none  # todo; can be different per extruder & per mesh	_onWriteStarted print_settings["travel_speed"] = None  # TODO; Can be different per extruder print_settings["cool_fan_enabled"] = None  # TODO; Can be different per extruder data["print_settings"] = print_settings submitted_data = urllib.parse.urlencode(submitted_data)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_addresses_wrong_type def test_fail_addresses_wrong_type(self):
# todo: implement </s> record0.write(pack('>i', 0xffffffff))	_generate_record0 record0.write(pack('>IIII', 0xffffffff, 0, 0xffffffff, 0xffffffff)) record0.write(pack('>I', 5)) record0.write(exth) record0.write(title)
# todo: implement </s> raise notimplementederror	to_gpu def to_gpu(self, device_num):
# todo: make the get_closest_value to return region </s> value, value_index = self.get_closest_value(	get_current_CSS_value parsed_declaration = re.search(r'^(\s*)(-[a-zA-Z]+-)?([a-zA-Z0-9-]+)(\s*(?: |\:))((?:(?!\!important).)+)', declaration) declaration_index = declaration_index + parsed_declaration.start(5) parsed_declaration.group(5), declaration_index,
# todo: check number of outputs is equal to the expected number </s> if hasattr(step, 'predict'):	_compute_step @staticmethod def _compute_step(Xs, cache, step): output_data = step.predict(*Xs) elif hasattr(step, 'transform'):
# todo debug </s> print logstring	logRule + "weekday=%d)" % item.element.weekday) logging.info("[%s]: %s" % (fileName, logString)) elif item.type == "monthday": logString = ("%s monthday " % spaceString
# todo: remove dep on parent </s> font.disablenotifications(	keyPressEvent del font[glyph.name] del self._glyphs[key] "Font.GlyphOrderChanged", self.parent()) font.glyphOrder = [glyph.name for glyph in self._glyphs]
# todo: i think this should use '$ fileregions' </s> return self.id1.get_segment(ea).bounds.end	SegEnd def SegEnd(self, ea):
# todo, pass complete checkpoint as state dictionary </s> mp_queue.put(best_model_path)	transfer_distrib_spawn_state_on_fit_end if self.trainer.global_rank == 0 and mp_queue is not None: rank_zero_warn('cleaning up ddp environment...') mp_queue.put(results) last_path = None
#todo_ismeal_quesataion: i prefer get_* for getters </s> if not specs:	quantum_elements def quantum_elements(self, specs=None): specs = self.get_specs() return self.__init_circuit, \
# todo: arrange </s> repo = self.remote.new_repo(self.token)	test_create_repo def test_create_repo(self): Test: create/edit a repo object self.assertTrue(self.remote.modify_repo(repo, "name", "testrepo0", self.token)) self.assertTrue(self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token))
# todo expression </s> f.write(u"expression todo")	print_Jump f.write(u"jump ") if stmt.expression: else: f.write(stmt.target)
# todo: check for error 'toomanyregistrationsfortargetid' </s> rule.conditions = conditions	modify_rule if action_target_group_arn not in target_group_arns: raise ActionTargetGroupNotFoundError(action_target_group_arn) rule.actions = actions return [rule]
# todo(b/160795287): deprecate estimator based executor. </s> fn_args_dict = fn_args.get_dict()	run_fn fn_args: Holds args used to train the model as name/value pairs. schema = io_utils.parse_pbtxt_file(fn_args.schema_file, schema_pb2.Schema()) fn_args_dict['serving_model_dir'] = os.path.join(fn_args.model_run_dir, path_utils.SERVING_MODEL_DIR)
# todo: 'package' won't work with unpack() </s> d['filename'] = u.path.rsplit('/', 1)[-1] or 'package'	main d['usemd5'] = '#' U = parse_url(package) if is_url: d['import_tests'] = 'PLACEHOLDER'
# todo(ruoyu): deprecate this in favor of pre_execution once migration to </s> return exec_properties	resolve_exec_properties exec_properties['warm_start_from']))
# todo: custom gremlin method </s> pass	create_edge_property_key def create_edge_property_key():
# todo: unit-test this method. </s> return self.unicode(b, encoding)	read encoding = self.file_encoding
# todo: what could go wrong here? </s> return [be.message(pconn.identity, text).send()]	__message__ "No such backend: %s" % pconn.backend.title)
# todo: 重构，写到socket fd里面的都是合法的 </s> logging.info("write to fd %x data:%r"%(fd, data))	_handle_write return len(data) if fd not in self._virtual_files: return count file = self._virtual_files[fd]
#todo: check the data! </s> count = 0	test_filtered_multiple_sources pipe_def = self._get_pipe_def("pipe_c1cfa58f96243cea6ff50a12fc50c984.json") p = pipe2py.compile.parse_and_build_pipe(pipe_def, verbose=True) for i in p: count += 1
# todo: to be implemented </s> raise notimplementederror()	start _start_batch_ce(ce_name=ce_name, min_vcpus=min_vcpus, desired_vcpus=desired_vcpus, max_vcpus=max_vcpus) elif cluster_section.get_param_value("scheduler") == "slurm": else: LOGGER.info("Starting compute fleet: %s", args.cluster_name)
# todo: get rid of warnings </s> assert_equal(expected_html, result.value)	footnotes_are_appended_to_text with open(test_path("footnotes.docx"), "rb") as fileobj: result = mammoth.convert_to_html(fileobj=fileobj, generate_uniquifier=lambda: 42)
# todo: remove parametrized workaround once collection structure contains parametrization. </s> if x.name == name or x.name.split("[")[0] == name:	matchnodes has_matched = False for x in rep.result: resultnodes.extend(self.matchnodes([x], nextnames)) has_matched = True
# todo numberpadding? </s> s = s + '0' * (1 + exp - len(s))	do_makeboxes_real if len(s) < exp + 1: evaluation.message('NumberForm', 'sigz') if exp < 0: s = '0' * (-exp) + s
# todo: order matters? </s> g = np.sum(g, axis=1)	filter_activation_matrix G[:, :] = 0 G[max_idx] = idx + 1 G = median_filter(G[:, np.newaxis], R) return G.flatten()
# todo: must be implemented </s> pass	range_from_chapters def range_from_chapters(crawler, times=0):
# todo: differentiate wrt some inputs only </s> ans1, out_tangents = jvp_fd(fun, vals, in_tangents)	vjp_matches_fd in_cotangents = gen_vals(fun.out_vars) fun = partial(eval_fun, fun) ans2, vjpfun = vjp(fun, *vals) out_cotangents = vjpfun(in_cotangents)
pass # todo </s> def _kill(self):	_kill @register(r'^kill$')
):  # todo? this equates the outputs </s> pipe_id = util.pythonise(pipe_wire['tgt']['id'])	build_pipe pipe_wire['tgt']['id'] != '_INPUT' and pipe_wire['src']['id'].startswith('_OUTPUT') kargs["%(id)s" % {'id': pipe_id}] = steps[ util.pythonise(pipe_wire['src']['moduleid'])]
# todo: also use backward pass </s> hs = fws	get_selective_model ) fws = [states[1][0][1] for states in rnn_result] h = tf.concat(1, hs) logits_flat = tf.contrib.layers.linear(h, 5*target_size)
# todo: test me @jmcarp </s> if not node.has_permission(auth.user, admin):	node_register_template_page ) else: raise HTTPError(http.FORBIDDEN) registered = False
# todo: implement this, mandatory </s> super(myseriesannotator, self).__init__(fmt=fmt, labels=labels)	__init__
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo test not just the 'starts with </s> raise usageerror(	parseArgs deployment_config = safe_load(deployment_config) except YAMLError as e: "Deployment config could not be parsed as YAML:\n\n" + str(e) )
else: # todo: find all defaults location for .wine , or request it directely to the user if not found. </s> installer = "wine /root/.wine/drive_c/python27/python.exe /root/.wine/drive_c/python27/scripts/pyinstaller-script.py"	main if sys.platform == "darwin": # On osx, the default .wine directory is located on $HOME/.wine/ installer = "wine " + os.environ['HOME'] + "/.wine/drive_c/Python27/python.exe " + os.environ['HOME'] + "/.wine/drive_c/Python27/Scripts/pyinstaller-script.py" exe = "wine " url      = args.url
# todo not sure from here, did not found variables inside the emulator </s> "dwnumberofprocessors": 0x4.to_bytes(length=4, byteorder='little'),	hook_GetNativeSystemInfo byteorder='little'), "dwActiveProcessorMask": 0x3.to_bytes(length=ql.pointersize, byteorder='little'), "dwProcessorType": 0x24a.to_bytes(length=4, byteorder='little'), "dwAllocationGranularity": (ql.heap.page_size * 10).to_bytes(length=4, byteorder='little'),
# todo: add compute method </s> def build_outputs(self, inputs):	build_outputs return Data((1,), self, 0), Data((1,), self, 1)
# todo: add multi_log_processor </s> def __init__(	__init__ self, resource: Resource = Resource.create(),
# todo implement this effectively </s> print('run unit tests')	run_tests def run_tests(c):
# todo find out if this is good because of sparcity... </s> 'prefers_data_normalized': false,	get_meta_information 'handles_numerical_features': True, 'prefers_data_scaled': False, 'handles_multiclass': True, 'handles_multilabel': True,
# todo: it's broken </s> iou["properties"]["path"] = image_path	dump ports.append(port.dump()) image_path = iou["properties"]["path"] return iou
# @todo: "smart" & ssh keys for non-localhost </s> "connection": "local",	setup_setting_apply appname = r.application playbook = [{"hosts": host, "remote_user": remote_user, "become_method": "sudo",
#@todo: remove in 0.4.10 </s> def wait(self, seconds=0, reconnect=none):	wait return _wait(self, seconds, reconnect)
# todo: remove in v8 </s> self._global_context['blog_desc'] = self.config.get('blog_description')	__init__ self._GLOBAL_CONTEXT['blog_title'] = self.config.get('BLOG_TITLE') self._GLOBAL_CONTEXT['blog_description'] = self.config.get('BLOG_DESCRIPTION') self._GLOBAL_CONTEXT['blog_url'] = self.config.get('SITE_URL', self.config.get('BLOG_URL')) self._GLOBAL_CONTEXT['body_end'] = self.config.get('BODY_END')
# todo: if empty, add 'pass' </s> except:	deparse_code if deparsed.ast[-1] == RETURN_NONE: deparsed.ast.pop() # remove last node pass deparsed.gen_source(deparsed.ast, customize)
# todo(bcipolli): add a link, with querystring args that auto-checks this video in the topic tree </s> messages.warning(request, _("this video was not found! you can download it by going to the update page."))	video_handler_backbone if not video["available"]: if request.is_admin: elif request.is_logged_in: messages.warning(request, _("This video was not found! Please contact your teacher or an admin to have it downloaded."))
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_bundles_wrong_type def test_fail_bundles_wrong_type(self):
# self.todolist was loaded with old identifier settings </s> todolist = load_file_to_todolist("test/data/listcommandtest.txt")	test_list51 def test_list51(self): config(p_overrides={('topydo', 'identifier_alphabet'): '0123456789abcdef', ('topydo', 'identifiers'): 'text'}) command = ListCommand(["-F", "%i", "Foo"], todolist, self.out, self.error) command.execute()
pass # todo </s> def add_data(data):	add_data
# todo: make class for that </s> self.visualization.visstate.params.valueaxes.append({	some_defaults def some_defaults(self): "id": "ValueAxis-1", "labels": {
# todo xxx graalvm change </s> raise unittest.skiptest("not supported")	ContextTests @needs_sni def test_sni_callback(self): ctx = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER) self.assertRaises(TypeError, ctx.set_servername_callback)
# todo: syn-103: remove "origin" and "destination" keys. </s> "origin": origin,	_expect_edu "edus": [ { "destination": destination, "edu_type": edu_type,
# todo: +kwargs </s> else:	pull with remote.repo.git.custom_environment(**GitRepo.GIT_SSH_ENV): return remote.pull(**pull_kwargs) return remote.pull(**pull_kwargs)
# todo remove when we remove dispersy </s> from tribler.community.search.community import searchcommunity	search_for_torrents if self.dispersy is None: return nr_requests_made for community in self.dispersy.get_communities(): if isinstance(community, SearchCommunity):
# todo yoon </s> train an rnnlm based on the data given by data_path	train_rnnlm def train_rnnlm(data_path, output_path):
## todo other builtins prototype hacks. see above. </s> 'array.prototype.append = function(a) {this.push(a);};',	create_webworker '	}, 100', ')', ] for name in dir(window):
# todo: can be removed in v0.2.19 </s> break	pop_quest_artifact self.pop_artifact(bag_artifact)
# todo: fix </s> from .numpy_ops import numpyops	ngrams def ngrams(self, n: int, keys: Array) -> Array: numpy_ops = NumpyOps() return self.asarray(
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
return "" # todo: followup after decision around returning none </s> unix_time = unix_time - 11644473600	helper_create_time unix_time = self.CreateTime.QuadPart // 10000000 if unix_time == 0: return str(datetime.datetime.utcfromtimestamp(unix_time))
# todo: figure out exact thresholds to use here </s> if cmin > nmax or cmax < 0:	ImageLayerState cmin, cmax = coord.min(), coord.max() nmax = self.layer.shape[icoord] return np.ones(pixel_coords[0].shape) * np.nan cmin = max(0, cmin)
#todo: fix this off-by-one when materials become 0-indexed </s> return [material(index=materials[i]+1) for i in range(n.value)]	MaterialFilter n = c_int32() _dll.openmc_material_filter_get_bins(self._index, materials, n) @bins.setter def bins(self, materials):
raise skiptest  # todo: figure out why this randomly started failing. </s> qs = {'a': 1, 'w': 4, 'format': 'json'}	test_discussion_filter_forum def test_discussion_filter_forum(self): forum_vals = ( (1, 4),
pass  # todo </s> def render(self):	render
# todo: assert </s> assert 0	test_get_repo_config_for_system Test: get repository configuration of a system result = self.remote.get_repo_config_for_system("testprofile0")
# todo_recorders - need to pass in parent info instead of none </s> metadata = create_local_meta(none, self.pathname)	record_iteration Record an iteration of the current System. self.iter_count += 1 update_local_meta(metadata, (self.iter_count,)) self._rec_mgr.record_iteration(self, metadata, method=inspect.stack()[1][3])
# todo: mock this test (failed in #493) </s> def test_write_all_albums_from_artist(tmpdir):	test_write_all_albums_from_artist expect_tracks = 282 text_file = os.path.join(str(tmpdir), "test_ab.txt")
# todo generator </s> list(save_dataset_hierarchy(	handle_dirty_datasets None if mode == 'save-before': {d: [d] for d in dpaths}, base=base,
# todo: this is wrong in some cases, please fix it </s> self._parser_modified = value	_set_parser_modified def _set_parser_modified(self, value):
# todo(jmcarp) handle multiple results better </s> committee = committee.query.filter_by(committee_id=committee_id).first()	get page_num = args.get('page', 1) per_page = args.get('per_page', 20) reports_class, specific_fields = reports_model_map.get( committee.committee_type,
# todo: use a better name </s> self.r = none	CheckAndRepairResultsRenderer super(CheckAndRepairResultsRenderer, self).__init__() self.client = client if results: self.r = ICheckAndRepairResults(results)
# todo: also check for match *[0-9]* </s> for tag in hg_tags:	default_describe node, tag = line.split() git_tags[tag] = node if tag in git_tags: break
# todo: this is a guess. make sure this is correct. </s> return "random()"	get_random_function_sql def get_random_function_sql():
# todo: should this raise? </s> return none	geo_shape_repository if url: return pywikibot.Site(url=url, user=self.username())
raise notimplementederror()  # todo: fix </s> predictions = np.array(	decode_probs def decode_probs(probs, decoder, language_model): if decoder is True and language_model is not None: [decode.beam_search(probs_indiv, language_model) for probs_indiv in tqdm(probs)])
# todo: do_cert? </s> certs = none	scan_serial logging.debug("\t\tTLSv1.2 scan.") tlsv1_2 = scanner.run_scan_command(server_info, Tlsv12ScanCommand()) logging.debug("\tDone scanning.") return sslv2, sslv3, tlsv1, tlsv1_1, tlsv1_2, certs
# \todo we didn't return `unicode` here because </s> return self._qtwidget().toplaintext().encode( "utf-8" )	getText return self._qtWidget().toPlainText() else :
# todo this help should be modified in case of webaccs. </s> cls.echo('then update the vhost to activate ssl with :')	activate_ssl (vhost, '--altnames=%s' % ','.join(altnames) if altnames else '')) cls.echo('$ gandi vhost udpate %s --ssl' % vhost) return True
# todo: update this to support categorical </s> if torch.sum(1-alphas[:,0]) > torch.sum(alphas[:,0]):	_alphas alphas[j,0] = row[1] alphas[j,1] = row[0] alphas_col = alphas[:,1] else:
#todo - use sql for this, much more efficient! </s> return len(self.adaptor.list_bioentry_ids(self.dbid))	__len__ def __len__(self):
# todo: this here to avoid having to manually clean up after </s> if self.date < datetime(2015, 8, 25):	_sync_log_was_old def _sync_log_was_old(): _assert = soft_assert(to=['czue' + '@' + 'dimagi.com'], exponential_backoff=False) _assert(False, 'patching sync log {} to remove missing case ID {}!'.format(
# todo: waffle here </s> index_posts.delay([instance.id])	update_post_search_index return from forums.tasks import index_posts
# todo: show deprecation message in the future. </s> translator_services = copy.deepcopy(translator_config['services'])	get_translator self.root, ) if service is not utils.SENTINEL: valid_service_kinds = [each['service']
raise notimplementederror #todo </s> elif self.action == "append":	Action raise NotImplementedError #TODO elif self.action == "PREPEND": raise NotImplementedError #TODO elif self.action == "MERGE":
# todo: re-enable after server bug is fixed </s> self.assertequal(r.end_node, b)	test_relationship self.assertEqual(r.start_node, a)
# todo: try wrapping idapython_execscript in a safe handler instead </s> idaapi.idapython_execscript(script, env)	handle_connection env = create_env() print(f"Executing {script}")
# todo: throw special accessdeniederror </s> raise valueerror('tbw permission denied')	assert_request_user_has_permission permission_type=permission_type) if not has_permission:
# todo: remove the duplicate entry for config_name, by </s> update['config_name'] = master_namespace	_initialize_namespaced_update if not namespace: namespace = MASTER_NAMESPACE if namespace == MASTER_NAMESPACE: assert valid_config(update)
# todo: add support for windows </s> raise notimplementederror("windows support is not implemented")	_generate_build_script def _generate_build_script(self, platform, sources, build_name): if sys.platform == "win32" or sys.platform == "cygwin": sh = [ "#!/bin/sh",
#todo : manage more than just quit </s> def manage_signal(self, sig, frame):	Shinken self.ichecks = None self.ibroks = None print "\nExiting with signal", sig self.poller_daemon.shutdown(True)
# todo: - torch.abs(h_emb + r_emb - t_emb) </s> score = - torch.sum(torch.abs(h_embs + r_embs - t_embs))	compute_score def compute_score(self, h_embs, r_embs, t_embs):
# todo do we actually need to round here? </s> return round(dist, config.precision)	measure else: validate.measure(config.MEASURE)
# todo: properly parse </s> for val in values:	updateValues def updateValues(self, values): if val not in self._values: self.addValue(val)
self.mapping = create_mapping() # todo: optimize by focusing only on this new test </s> else:	_file_deleted def _file_deleted(self, path): if basename(path).startswith("test_"): run_tests(mapping.get(path, []))
# todo: use shlex.quote as soon as a newer python version is available. </s> quoted_file_path = pipes.quote(path)	run_rubocop_on if not path: return rubocop_cmd = self.rvm_cmd + ' -S rubocop ' + quoted_file_path self.run_shell_command(rubocop_cmd)
# todo(leofang): how about ptds? </s> stream_ptr = stream.ptr if stream.ptr != 0 else none	__cuda_array_interface__ def __cuda_array_interface__(self): stream = cupy.cuda.get_current_stream() desc = { 'shape': self.a.shape,
# todo remove this asap, as soon as the test has been fixed </s> if self.is_running_on_public_ci():	test_checksums @attr('invalid') def test_checksums(self): self.skipTest("Too buggy to run on public CI") tar = self._simple_download(append=True)
# todo remove? </s> if false and user_stmt is not none:	_prepare_goto eval_stmt = self._get_under_cursor_stmt(goto_path) if not is_completion: eval_stmt.start_pos = user_stmt.end_pos scopes = self._evaluator.eval_statement(eval_stmt)
#set limit to 25 --> currently hardcoded --> todo: add a setting for this ? </s> subelement(root, "limit").text = "25"	addVideoNodesForTag Rule = SubElement(root, "rule", {"field":"tag","operator":"is"}) SubElement(Rule, "value").text = tagname Rule2 = SubElement(root, "rule", {"field":"inprogress","operator":"true"}) try:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: remove this skip after fixing </s> if sys.version[0] == 3:	test_circle_draw @requires_application() def test_circle_draw(): raise SkipTest with TestingCanvas() as c:
# todo(nakago): investigate why the test fails. </s> params = ()	test_backward_gpu model_no_dropout.to_gpu() if int(chainer.__version__[0]) <= 2: else: params = tuple(model_no_dropout.params())
# todo - needs tests </s> return {	payments_settings def payments_settings(request): "STRIPE_PUBLIC_KEY": settings.STRIPE_PUBLIC_KEY, "PLAN_CHOICES": settings.PLAN_CHOICES,  # possibly nuke
# todo: assert </s> repo = self.remote.get_repo("testrepo0")	test_get_repo Test: Get a repo object
# todo implement. </s> yaml = self.master_model.to_yaml()	_train def _train(self, rdd, nb_epoch, batch_size, verbose, validation_split):
# todo: optimize me </s> return itertools.chain([self], self.get_descendants_recursive(lambda n: n.primary))	node_and_primary_descendants :param node Node: target Node
# todo(dylan): better error handling here </s> r = requests.get(request_url)	handle request_url = "http://%s/static/data/subtitledata/srts_by_language/%s.json" % (settings.CENTRAL_SERVER_HOST, language) try: available_srts = set((r.json)["srt_files"]) except:
# todo: actually test this once i figure out how to do this in py.test </s> logging.error("interrupted by user")	download jobs.get(0xFFFF) except KeyboardInterrupt:  # pragma: no cover return 1 except requests.exceptions.ConnectionError as err:
# todo add </s> pass	set_startup_hook def set_startup_hook(function=None):
# todo: seems to be doing < rather than <= ??? </s> xpath.add_condition("@x0 <= %s" % x1)	PDFQueryTranslator def xpath_overlaps_bbox_function(self, xpath, fn): x0,y0,x1,y1 = map(float, fn.arguments[0].value.split(",")) xpath.add_condition("@y0 <= %s" % y1) xpath.add_condition("@x1 >= %s" % x0)
# todo: warn on error </s> if (declaration.type == 'declaration' and	parse_declarations important_declarations = [] for declaration in tinycss2.parse_declaration_list(input): not declaration.name.startswith('-')): value = tinycss2.serialize(declaration.value).strip()
#ng_required="false",   # todo: validation </s> data_bind='value: first_name',	NewMobileWorkerForm crispy.Field( 'first_name', ), crispy.Field(
# todo: sort the functionality by name and by vuln class </s> for functionality_name in functionality:	createMenuItems functionality = data["functionality"] bugcatcher_menu = JMenu("Send to Bug Catcher") vulns = functionality[functionality_name]["vulns"] menu_vuln = JMenu(functionality_name)
# todo: reformat or delete </s> camera.trackbodyid = 0	sawyer_door_env_camera_v3 def sawyer_door_env_camera_v3(camera): camera.distance = 1.0 cam_dist = 0.25
# todo: handle other file operations other than just extend/write </s> yield f.read()	_follow_freebsd if not events: continue
# todo testing </s> sys.stdout.write('asking masterdriver to forward the cov')	do_ConfirmedCOVNotifcationRequest if not isinstance(apdu, ConfirmedCOVNotificationRequest): _log.error() self.vip.rpc.call(PLATFORM_DRIVER, 'forward_bacnet_cov_value', apdu.initiatingDeviceIdentifier,
# todo tell it to some human operator </s> pass	delete os.remove(self._path + key + '.json') except OSError:
#todo: where is the set_param method?! </s> pass	test_classification_set_params_returns_self params = automl.get_params()
# todo: implement me </s> reduction='mean').shape == ()	_test_smoke_mean gamma=2.0,
# todo find a better way of checking for no pregenerated thresholds </s> self.code_gen_dict["$pragmas$"].append(	pragmas ) if self.calc_tmem() != 0: ( "#pragma HLS ARRAY_PARTITION variable=threshs.m_thresholds "
self.current_height = self.current_height * 2 #todo </s> return result	layer_upsample result = nn.Upsample((h, w), mode="bilinear") self.current_width = self.current_width * 2 #TODO
# todo: remove in ros 1.3 </s> def _paramiko_hostkeyentry_from_line(line):	_paramiko_HostKeyEntry_from_line paramiko 1.7.4 has a bad from_line implementation. This uses code from 1.7.6 to manually load fields = line.split(' ')
pass # todo </s> def test_selectmultiplefield():	test_SelectMultipleField
# todo: python-components: for now, we call each preprocessor's graph_fn directly. </s> if self.backend == "python" or get_backend() == "python":	reset def reset(self): for preprocessor in self.preprocessors.values():  # type: PreprocessLayer preprocessor.reset()
# todo only return results within uri roots given by ``uris`` </s> if query is none:	search def search(self, query=None, uris=None): query = {} self._validate_query(query)
# todo(gibi): remove this when live migration is fully supported and </s> self._turn_off_api_check()	test_live_migrate_with_qos_port_reschedule_success def test_live_migrate_with_qos_port_reschedule_success(self): self._start_compute('host3') compute3_rp_uuid = self._get_provider_uuid_by_host('host3')
# todo: remove in sopel 8 </s> if hasattr(func, 'rule') and isinstance(func.rule, basestring):	clean_callable else: func.event = [event.upper() for event in func.event] LOGGER.warning( 'The `rule` attribute of %s.%s should be a list, not a string; '
# todo: we should filter out some of these columns </s> expected_property_names = [	test_reference_for_forms self.domain, self.app, DATA_SOURCE_TYPE_RAW, form_data_source._id, ) "doc_id", "inserted_at", "completed_time", "started_time", "username", "userID", "@xmlns", "@name", "App Version", "deviceID", "location", "app_id", "build_id", "@version", "state", "last_sync_token",
# todo docstring </s> if self.weekly_release is not none:	client_release updated for marketing releases, pre-releases and weekly releases but not documentation releases. return self.release + 'dev' + parsed_version.weekly_release elif self.pre_release is not None:
if func=='tag':  # todo </s> res = ""	XML_ExpandLine if cmd.startswith('VAL('): func,el,name = XML_processParams(cmd[len('VAL('):], src) elif func=='attrib': res = el.get(name,'')
# todo : include project name here! </s> model_group = os.path.dirname(rel_path)	__project_sources if fnmatch.fnmatch(filename, "*.sql"): indexed_files[full_source_path].append(rel_path) model_name  = os.path.splitext(filename)[0] model = (model_group, model_name)
pass  # todo </s> def close(self):	close
#       todo: update for d4 </s> [+++-, ++-+, +-++, -+++, +---, -+--, --+-, ---+]	CrystalOfSpinsMinus sage: D.list()
# todo could keep_trailing_newline fix this better? </s> f.write(b'\n')	run with open(output, 'wb') as f: f.write(rendered.encode('utf-8')) else: parsed = parse_xml(state, file)
# todo: check if user has access to this topic/poll </s> choice = get_object_or_404(	voters @login_required def voters(request, pk): CommentPollChoice.objects.unremoved(), pk=pk
# todo tell it to some human operator </s> pass	__init__ self._store[ent[:-5]] = entity(data) except OSError: except IOError: pass
# todo: arrange </s> system = self.remote.get_item_handle("system", "testsystem0", self.token)	test_copy_system def test_copy_system(self): Test: copy a system object self.assertTrue(self.remote.copy_system(system, "testsystemcopy", self.token)) assert 0
#todo do it better 21/08/13 12:36:08 </s> for s,need in tmp_cuttor.cut_to_sentence(txt):	summarize3 tmp_cuttor.set_stage1_regex(re.compile('(\d+)|([a-zA-Z]+)', re.I|re.U)) sentences = [] if need: sentences.append(s)
# todo(gibi): remove when nova only supports compute newer than </s> source_service = objects.service.get_by_host_and_binary(	_migrate "microversion") raise exc.HTTPBadRequest(explanation=msg) context, instance.host, 'nova-compute') if source_service.version < MIN_COMPUTE_MOVE_BANDWIDTH:
# must remove folder or copytree fails todo: smarter version </s> shutil.rmtree(join(folder, 'js'))	prepareResourcesJS shutil.copytree(join(jsPath, 'php'), join(folder, 'php')) shutil.copytree(join(jsPath, 'js'), join(folder, 'js'))
# todo: raise an invalidparameters instead and stop using cli_ui in gitlabform.gitlab </s> cli_ui.fatal(	add_share_to_group ) except NotFoundException: f"Group {share_with_group_name} not found.", exit_code=EXIT_INVALID_INPUT,
# todo implement. </s> yaml = self.master_model.to_yaml()	_train def _train(self, rdd, nb_epoch, batch_size, verbose, validation_split):
# todo stub </s> pass	build_sqlite_connection_string def build_sqlite_connection_string(file_path):
raise notimplementederror()  # todo </s> not implemented yet!	list_settings_profiles .. WARNING::
# todo: more efficient implementation (lua script per shard?) </s> for channel in self._group_channels(group):	send_group def send_group(self, group, message): Sends a message to the entire group. self.send(channel, message)
# todo: wobble </s> return einsum('i...,ij...->j...', vector, rotation)	spin ])
# todo: implement this </s> return true	check_dirtied def check_dirtied(self):
# todo materials are read-only for now </s> return mo	update if opplanreslist: self._processOperationPlanResource(mo, opplanreslist)
# todo i18n text entries </s> for choice in choices:	_map_populate_value_box current = setting._value.get(str(key_choice)) # just in case the persisted value is missing some keys if choices: valueBox.append(str(int(choice)), str(choice)) if current is not None:
# todo implement for all channels </s> def _handle_toggle(self, message):	_handle_toggle return None
# todo: remove </s> if user.is_moderator:	for_update_or_404 def for_update_or_404(self, pk, user): return get_object_or_404(self._access(user=user), pk=pk) else:
# todo(b/142684737): the multi-processing api might change. </s> beam_pipeline_args=['--direct_num_workers=%d' % direct_num_workers])	_create_pipeline metadata_connection_config=metadata.sqlite_metadata_connection_config( metadata_path),
# todo test </s> def subsystem(s):	subsystem state_length(s.state, s.network.size) state_reachable(s.state, s.network.tpm)
# todo: must be implemented </s> pass	range_from_chapters def range_from_chapters(crawler, times=0):
# todo-blocker(jamalex): re-enable this conditional once tastypie endpoints invalidate cached session value </s> request.session["points"] = compute_total_points(user)	generate_status data["is_logged_in"] = True data["username"] = user.get_name() data["points"] = request.session["points"] if request.session["points"] else 0 data["user_id"] = user.id
# todo get variable size </s> val = frame.evaluateexpression('*(void**)' + addr)	generate_return_string val = module.FindFirstGlobalVariable(target, name) if not val: if val.summary: name += '\n' + val.summary
#todo: test me </s> return (v - self.position).cpvunrotate(self.rotation_vector)	world_to_local def world_to_local(self, v):
# todo: wobble </s> spin = rot_z(t.gast / 24.0 * tau)	ITRF_to_GCRS2 def ITRF_to_GCRS2(t, rITRF, vITRF): position = mxv(spin, array(rITRF)) velocity = mxv(spin, array(vITRF))
assert self.start_seq is none #todo: better handling of this situation </s> assert self.stop_seq is none #todo: better handling of this situation	restart_program def restart_program(self): assert self.restart_seq is None #TODO: Better handling of this situation L.debug("Initializing restart sequence")
# todo(unno): sub for boolean array is deprecated in numpy>=1.13 </s> self.check_array_broadcasted_op(operator.sub, no_bool=true)	test_broadcasted_sub def test_broadcasted_sub(self):
# todo: check how to sort inputs for multichannel inputs </s> output = np.asarray([output], dtype=np.float32).reshape(	execute_node mask = 2 ** (odt.bitwidth() - 1) output = [-(x & mask) + (x & ~mask) for x in output] 1, out_pix, k * k * ifm_ch )
# todo: find out if this method is used anywhere, remove if not. </s> return self._batch_size	batch_size The size of the batches used for monitoring
# todo: self.assertfalse(prop.is_valid(np.bool8(true))) </s> self.asserttrue(prop.is_valid(np.int8(0)))	test_Float try: import numpy as np self.assertTrue(prop.is_valid(np.int8(1))) self.assertTrue(prop.is_valid(np.int16(0)))
# todo: handle case where the creation is rejected for some reason (should </s> assert 'request_id' in create_response	CrushRuleViewSet rule_data['steps'] = request.DATA['steps'] create_response = self.client.create(fsid, CRUSH_RULE, rule_data) return Response(create_response, status=status.HTTP_202_ACCEPTED) else:
# todo: legacy behavior, should remove after new case processing </s> if xformdoc.form_id not in self.case.xform_ids:	update_from_case_update if case_update.version: self.case.version = case_update.version legacy_soft_assert(False, "xform_id missing from case.xform_ids", { 'xform_id': xformdoc._id,
# todo: revert this pushing all 3 changes at once, its' just a </s> self.call(	push_abandoned def push_abandoned(self, review_branch, tracking_branch): :returns: None 'push', self._remote,
# todo - use smarter weights (e.g. hamming window) </s> k[indices] += 1	apply indices = samples_window.crop( window, mode='center', fixed=self.duration) y[indices] += predictions[i, :, :] y = (y.T / np.maximum(k, 1)).T
# todo handle custom_objects </s> self.model = dict_to_model(serialized_model)	SparkWorker def __init__(self, serialized_model, train_config, master_optimizer, master_loss, master_metrics, custom_objects): self.train_config = train_config self.master_optimizer = master_optimizer
# todo take this as input from outside? </s> minv = {}	_locationsToBoxes def _locationsToBoxes(self): locations = self.locations maxV = {} for l in locations:
# update new ratings kodi 17 - todo get ratingid for updates from embydb </s> if self.kodi_version > 16:	add_update all_episodes = emby.getEpisodesbyShow(itemid) self.add_episodes(all_episodes['Items'], None) ratingid =  self.kodi_db.create_entry_rating() self.kodi_db.add_ratings(ratingid, showid, "tvshow", "default", rating, votecount)
# todo: we do not currently have type-safety for keys suitable for decoding *and* </s> if headers is none:	encode needs to be used to verify the payload, as long as the payload contains enough information to also validate the key used was correct. headers = {} return pyjwt.encode(payload, key, algorithm=algorithm, headers=headers)  # type: ignore
# todo: remove when support for django 1.3 is dropped </s> import django	patch_postgis_bad_geomery_escape escaped. Monkey patch PostGISAdapter to fix this issue. see https://code.djangoproject.com/ticket/16778 if django.VERSION < (1, 4): from django.db import connections
# todo: replace with "yield from" when dropping python 2. </s> for __ in streams.items():	_get_vod_streams if res.headers.get("Content-Type") == "application/f4m+xml": streams = HDSStream.parse_manifest(self.session, res.url) yield __ elif res.headers.get("Content-Type") == "application/vnd.apple.mpegurl":
raise notimplementederror # todo </s> def latest_offsets(self):	latest_offsets
# todo: let the globe return the semimajor axis always. </s> return coords	ellipse coords += ([easting], [northing])
summary=f'added book "{b.title}"', # todo shelf? </s> eid=b.bid	get_events events.append(Event( dt=b.date_added, )) return sorted(events, key=lambda e: e.dt)
# todo: must be implemented </s> pass	get_range_using_urls def get_range_using_urls(self, crawler):
# todo: log exception </s> value = var_entry.entry[key].encode('ascii', errors='ignore')	_get_version_info value = var_entry.entry[key].encode('ascii') except Exception as e: results[key] = value
"""todo doc me""" </s> table = [['foo', 'bar', 'baz'],	test_profile_default def test_profile_default(): ['A', 1, 2], ['B', '2', '3.4'],
""" type setting - todo explain """ </s> return discovery['contact']	get_contact def get_contact(self,obj):
# todo, pass complete checkpoint as state dictionary </s> mp_queue.put(best_model_path)	transfer_distrib_spawn_state_on_fit_end if self.trainer.global_rank == 0 and mp_queue is not None: rank_zero_warn('cleaning up ddp environment...') mp_queue.put(results) last_path = None
pass # todo </s> subcommand = config.default_action	run self.todolist = TodoList.TodoList(todofile.read()) except Exception: if len(sys.argv): subcommand = sys.argv[1]
# todo: fix inconsistent handling of water </s> special = {"d": "d", "hw": "h", "ow": "o", "wat": "o",	parse_symbol def parse_symbol(sym): "wat": "O", "OH": "", "OH2": ""} m = re.findall(r"w?[A-Z][a-z]*", sym)
# todo : map extra parents to the extras file </s> pass	pseudo_import_git_commit p2 = self.map_hg_get(sha) if len(commit.parents) > 2: if (not (p2 == "0"*40) or (p1 == "0"*40)): self.renames[cs] = {}
# todo(mordred) when this changes to rest, force interface=admin </s> if self.cloud_config.get_api_version('identity') != '3':	update_user user = self.get_user(name_or_id) kwargs['user'] = self.get_user_by_id(user['id'], normalize=False) kwargs.pop('domain_id', None) kwargs.pop('description', None)
# todo (a8): add user to models </s> setattr(result, 'user', none)	obj_get result.branch = result.revision.branch setattr(result, 'result', result) return result
# todo(yuriyz): change to 404 (bug 1200517) </s> self.assertequal(response.status_int, 500)	test_delete_chassis response = self.get_json('/chassis/%s' % cdict['uuid'], expect_errors=True) self.assertEqual(response.content_type, 'application/json') self.assertTrue(response.json['error_message'])
# todo context, etc (allophones) </s> return sorted(self.lexicon.phonemes.keys(), key=lambda s: self.lexicon.phonemes[s]["index"])	get_class_labels def get_class_labels(self):
# todo: restore this code after resolution of the following issue: </s> if not isinstance(cube.data, numpy.ma.maskedarray):	linear src_points = numpy.append(src_coord.points, src_coord.points[0] + modulus) data = numpy.append(cube.data, cube.data[tuple(coord_slice_in_cube)],
# todo: does the gas and from defaulting in `sendtransaction` make sense for this? </s> return self.sendtransaction(new_transaction)	replaceTransaction self.web3, current_transaction, new_transaction )
svg = apply_template(svg, {"maxpoints":maxpoints, "trdn": trdn, "msg":"", "valuemid":"0.5", "timemid":"10s", "datapoints":"","init_max_y": "false", "max_y": 0, "seconds_scale":0, "y_shift": 0, "zero": 0, "l_y":"[0,0,0,0,0,0,0,0,0,0]"}) # todo templating engine </s> if width and height: svg = svg.replace('height="210" width="610"', 'height="%s" width="%s"' % (height, width)) # todo: switch to templating	plotwh svg = apply_template(svg, {"MAXPOINTS":MAXPOINTS, "TRDN": trdn, "MSG":"", "VALUEMID":"0.5", "TIMEMID":"10s", "DATAPOINTS":"","INIT_MAX_Y": "false", "MAX_Y": 0, "SECONDS_SCALE":0, "Y_SHIFT": 0, "ZERO": 0, "L_Y":"[0,0,0,0,0,0,0,0,0,0]"}) # TODO templating engine else: image_views += 1 return flask.Response(svg,  mimetype= 'image/svg+xml', headers={'Cache-Control': 'no-cache, no-store, must-revalidate', 'Pragma': 'no-cache'})
# todo: remove patch and update test once calculation_magic is implemented </s> @mock.patch("sentry.tasks.low_priority_symbolication.calculation_magic", lambda x, y: true)	test_has_metric_not_in_lpq @freeze_time(datetime.fromtimestamp(0)) def test_has_metric_not_in_lpq(self, store) -> None: store.increment_project_event_counter(17, 0)
# todo: delet this when all preprintproviders have a mapping </s> from osf.models.subject import subject	top_level_subjects return self.subjects.filter(parent__isnull=True) else: if len(self.subjects_acceptable) == 0: return Subject.find(Q('parent', 'isnull', True))
).consume()  # todo see issue 170 </s> for instance in reservation["instances"]:	load_ec2_instances Region=region, aws_update_tag=aws_update_tag, instanceid = instance["InstanceId"] monitoring_state = instance.get("Monitoring", {}).get("State", "")
# todo: sort out blending on non-rgb systems </s> self._screen.print_at(char, x, y, fg, attr, bg)	update fg, attr, bg = particle.colours[ max(min(pos, len(particle.colours)-1), 0)] else: self.particles.remove(particle)
#todo: fix this </s> return list( self )	keys def keys( self ): "return a list of all node names or net's keys"
# todo: make a signature attribute for transactions </s> if self._transaction is not none \	sendall db.time = now sig.check_arity(fields[1:]) and func_name not in ('exec', 'discard', 'multi', 'watch'): self._transaction.append((func, sig, fields[1:]))
# todo!! add more assertions for the smaller subsystems </s> def test_complexes(standard, flushdb):	test_complexes complexes = list(compute.complexes(standard)) assert standard_example_is_correct(complexes[7])
# todo todo todo </s> self.set_progression(src, ((float)(page_idx+1) / self.doc.nb_pages),	__on_page_thumbnailing_page_done_cb line_iter = self.lists['pages']['model'].get_iter(page_idx) self.lists['pages']['model'].set_value(line_iter, 0, thumbnail) _("Thumbnailing ..."))
# todo: the following skipped suite and fixtures should be enabled </s> return ['api-key']	_filter_headers def _filter_headers(self):
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_metadata_copyrights_are_equal_for_all_fonts_success def test_result_metadata_copyrights_are_equal_for_all_fonts_success(self): self.assertInSuccess('test_metadata_copyrights_are_equal_for_all_fonts', run_set(p, 'result'))
# todo: either fix this or remove this code </s> return false	_all_ratings_equal def _all_ratings_equal(self, tracks = None): Returns True if the rating of the tracks for this widget is equal if not tracks: tracks = self._get_tracks()
pass  # todo(zcd) </s> def test_check_forward_backward_with_scale(self):	test_check_forward_backward_with_scale
# todo: use pybossa uploader! only for debugging: </s> open('/tmp/%d_%s_task_json.zip' % (app.id, name), 'wb').write(memzip.getvalue())	export_json memfile.write(str(line)) memzip = make_onefile_memzip(memfile, '%s_task.json' % name) json_task_run_generator = respond_json("task_run", app.id) if json_task_run_generator is not None:
# todo: move navigation part to base class </s> self.pressed_item = self.itemat(event.pos())	mousePressEvent def mousePressEvent(self, event): node = self.nodeFromInstance(self.pressed_item) self.pressedPin = self.findPinNearPosition(event.pos())
#todo gtk3: workaround here for bug https://bugzilla.gnome.org/show_bug.cgi?id=680638 </s> self._widget.enable_model_drag_source(gdk.modifiertype.button1_mask,	on_object_select_row o = model.get_value(node,1) targets += [target.target_data_atom() for target in o.__class__.DROP_TARGETS] [], Gdk.DragAction.COPY | Gdk.DragAction.MOVE)
# todo: use tx hash instead of key to avoid multiple queries for the same tx </s> unconf_key_ids = list(set([utxo.key_id for utxo in utxos]))	scan if account_id is not None: utxos.filter(DbKey.account_id == account_id) for key_id in unconf_key_ids: self.transactions_update(key_id=key_id)
# todo test </s> self.unconstrained_effect_repertoire(purview))	effect_info return utils.emd(self.effect_repertoire(mechanism, purview),
# todo: log. </s> continue	download_all ) except ValueError: request = self.create_getdatarequest( {dresponse.provider: files}, methods, info
# todo: add other exceptions here </s> return false	send self._send_fallback_message(msg)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
#todo(wwolf) get correct value for these </s> "updated": "2010-10-09t11:30:00z",	dispatch "id": "v1.0", "status": "DEPRECATED", }, ]
# todo. create readme file in <output_dir> </s> step = arguments['--step']	main output_dir = Path(arguments['<output_dir>']) output_dir = output_dir.expanduser().resolve(strict=False) if step is not None: step = float(step)
extruder_stack.userchanges.setproperty(key, "value", new_value)  # todo: nested property access, should be improved </s> if extruder_stack != self._active_container_stack and extruder_stack.getproperty(key, "value") != new_value:	copyValueToExtruders for extruder_stack in extruder_stacks:
# todo(harlowja): the bug 1214083 is causing problems </s> log.debug(_("%(flow)s has moved into state %(state)s from state"	flow_log_change def flow_log_change(state, details): " %(old_state)s") % {'state': state, 'old_state': details.get('old_state'),
heading=x.heading, # todo include the rest? </s> tags=list(x.tags),	to_note return OrgNote( created=created,
# todo: maybe a chardet integration </s> file_descriptor.seek(-3, 1) # rewind of 3 chars	detect_encoding bom = file_descriptor.read(3) if not bom in cls.BOMS: return cls.DEFAULT_ENCODING return cls.BOMS[bom]
# todo: replace with a call to dt.timestamp() when we drop python 2.7 </s> ts = calendar.timegm(dt.utctimetuple()) + (dt.microsecond / 1000000)	_format_token return str(calendar.timegm(dt.utctimetuple())) if token == "x": return str(int(ts * 1000000)) if token == "ZZZ":
# todo use **kwargs instead 4 dummy parameters </s> super(telegramclient, self).reconnect(	reconnect def reconnect(self, new_dc=None, a=None, b=None, c=None, d=None): device_model=self.device_model, system_version=self.system_version,
# todo assert the key passes deeper validation </s> priv_key = 'rsa1024:' + ''.join(r.strip().split('\n')[1:-1])	migrate_hs_dir if not r.startswith('-----BEGIN RSA PRIVATE KEY-----\n'): raise Exception('%s does not have the right format!') add_raw_config(self.store_new, u'private', u'tor_onion_priv_key', True, priv_key) with open(hn_path, 'r') as f:
# todo: refactor into some kind of utility </s> def add_note(col, data):	add_note from anki.notes import Note model = col.models.byName(data['model'])
fp = open("%s.py" % name, "w")   # todo: confirm file overwrite </s> print >>fp, pipe2py.compile.parse_and_write_pipe(	setUp name = "pipe_2de0e4517ed76082dcddf66f7b218057" pipe_def = self._get_pipe_def("%s.json" % name) self.context, pipe_def, pipe_name=name) fp.close()
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	list_instances def list_instances(self):
tasks = ss("#todo-list>li") </s> visit("http://todomvc.com/examples/troopjs_require/#/")	test_create_task def test_create_task(): for task_text in ["1", "2", "3"]: s("#new-todo").set(task_text).press_enter()
# todo unordered float </s> return fcomp(ir, instr, a, b)	fucomp def fucomp(ir, instr, a=None, b=None):
# todo: uncomment when notificationsubscription is implemented </s> save=true,	add_project_created_log log_date=instance.date_created,
# todo: make this a hard error, instead of a silent overwrite </s> logging.warning("kvm: overriding disk_cache setting '%s' with 'none'"	_GenerateKVMBlockDevicesOptions if instance.disk_template in constants.DTS_EXT_MIRROR: if disk_cache != "none": " to prevent shared storage corruption on migration", disk_cache)
# todo: parallelize the following loop with joblib </s> for doc in raw_documents:	_build_vectors_and_vocab max_df = self.max_df max_features = self.max_features term_count_dict = {} # term => count in doc for term in self.analyzer.analyze(doc):
# todo: documentation pending </s> parameters	save_weights_to_npz_dict def save_weights_to_npz_dict(self, filepath, sess=None): ---------- filepath
self.assertequals(status, 200) # todo: 202 when asynchronous </s> status, body = self.post(path, body)	test_install options=options,)
if abs(skew) > 5: # todo: make configurable </s> self.setmessage('date', rs.date_incorrect,	checkClock skew = self.response.parsed_hdrs.get('date', 0) - \ self.response.header_timestamp + self.response.parsed_hdrs.get('age', 0) clock_skew_string=relative_time( self.response.parsed_hdrs.get('date', 0),
# todo: automate this: batch in -> batch out; time in -> time out; batch+time in -> batch+time out, etc.. </s> flat_logits._batch_rank = 0 if self.input_space.time_major is false else 1	_graph_fn_get_state_values_and_logits ) state_value = tf.squeeze(state_value, axis=-1) if self.input_space.has_time_rank: flat_logits._time_rank = 0 if self.input_space.time_major is True else 1
# todo: what about other auth types? </s> auth_hdrs = urllib3.make_headers(proxy_basic_auth=mgr.proxy.auth)	__init__ mgr = urllib3.proxy_from_url(proxy_url, **kwargs) if mgr.proxy.auth: mgr.proxy_headers.update(auth_hdrs) self._con_pool = mgr
# todo(kpy): remove support for legacy urls in mid-january 2012. </s> import legacy_redirect	get_repo_and_action scheme, netloc, path, _, _ = urlparse.urlsplit(request.url) parts = path.lstrip('/').split('/') if legacy_redirect.get_subdomain(request): repo = legacy_redirect.get_subdomain(request)
# todo: we have no format to save volumes yet! </s> vols = imageio.mvolread(dname4, 'dicom')	test_functions raises(ValueError, imageio.volsave, dname4, vol) raises(ValueError, imageio.volsave, dname4, np.zeros((100, 100, 100, 3))) assert isinstance(vols, list) assert len(vols) == 1
# todo(hub-cap): turn this into middleware </s> context = context.reddwarfcontext(	delete def delete(self, req, tenant_id, id): auth_tok=req.headers["X-Auth-Token"], tenant=tenant_id)
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	get_host_ip_addr def get_host_ip_addr(self):
raise notimplementederror #todo </s> elif q.kw(i,"format"):	parse while i < l: if q.kw(i,"RETURN"): raise NotImplementedError #TODO elif q.kw(i,"REQUEST"):
# todo: replace with `keras.backend.pad` </s> return tensorflow.pad(x, paddings, mode="constant")	pad_masks difference = keras.backend.max([0, difference]) paddings = ((0, 0), (0, difference), (0, 0), (0, 0), (0, 0))
# todo: fix this </s> items = list(collection.list())	discover yield collection if depth != "0": if items: for item in items:
# todo(cmaloney): switch to a sane http server </s> def wsgi_app(env, start_response):	wsgi_app subscriber.handle_event(json.load(env['wsgi.input'])) start_response('200 OK', [('Content-Type', 'text/html')])
# todo this should be a return and printed elsewhere </s> print("ideal cutoff is %0.2f, yielding tpr of %0.2f and fpr of %0.2f" % (cutoff, best_tpr, best_fpr))	roc_curve best_fpr = fpr[ind] cutoff = thresh[ind] if show_all_cutoffs is True: print('%-7s %-6s %-5s' % ('Thresh', 'TPR', 'FPR'))
# todo: needs further implementation </s> return datatablesheader(	headers @property def headers(self): DataTablesColumn(self.selected_location_type), DataTablesColumn(
# todo: wait for event instead. </s> sublime.set_timeout_async(	open_and_apply_edits if view: if view.is_loading(): lambda: view.run_command('lsp_apply_document_edit', {'changes': file_changes}), 500
# todo: convert non uris to file uris. </s> for line in data.readlines():	parse_m3u def parse_m3u(data): if not line.startswith('#') and line.strip(): yield line
#todo - two staight lines is only a good approximation for small </s> p.lineto(x0+middle_radius*startsin, y0+middle_radius*startcos)	_draw_arc_arrow reverse=False) p.lineTo(x0+outer_radius*headsin, y0+outer_radius*headcos) p.lineTo(x0+inner_radius*headsin, y0+inner_radius*headcos) p.closePath()
# todo(kevinbenton): remove after bug/1666493 is resolved </s> log.debug("requesting with ip request: %s port: %s ip: %s "	add_auto_addrs_on_network_ports 'mac': port['mac_address']} ip_request = factory.get_request(context, port, ip) "for subnet %s and ipam_subnet %s", ip_request, port, ip, subnet, ipam_subnet)
# todo xxx postremora: uncomment when remora goes away </s> u = user.objects.get(email='john.connor@sky.net').get_profile()	test_success r = self.client.post('/en-US/firefox/users/register', data, follow=True) assert u.confirmationcode eq_(len(mail.outbox), 1)
# todo log here </s> return false	auth_okta ) except httplib.HTTPException: if response.status_code != 201: return False
#todo finish me </s> def adjust_cors(s3wrapper):	adjust_cors rules = s3wrapper.get_cors_rules()
# todo should we pass? </s> pass	WeatherCache else: _log.debug("In WeatherCache: Invalid request type: ?", request_type) self.trim_period = cache_period try:
# todo: add parameters to supply to integratedgradients.attribute? </s> outputs = visualizer.visualize()	test_one_feature score_func=None, ) for output in outputs: contribs = torch.stack(
# todo: consider returning an empty [] rather than raising </s> assert_raises(attributeerror, keyed_tuple.keys)	test_empty eq_(str(keyed_tuple), '()') eq_(keyed_tuple.__dict__, {}) assert_raises(AttributeError, keyed_tuple._asdict) def should_raise():
#todo fixme: we should provide an option to create the page </s> continue	main if not item.exists(): pywikibot.output('%s doesn\'t have a wikidata item :(' % page) for claim in real_claims: pywikibot.output('Adding %s --> %s' % (claim.getID(), claim.getTarget().getID()))
# todo: something a bit less heavy than eval </s> terms = safe_eval(terms)	run terms = template.template(self.basedir, terms, inject) if '{' or '[' in terms: if not isinstance(terms, list): raise errors.AnsibleError("a list is required for with_nested")
# todo ... </s> self.error("preprocessor: if-evaluation not yet implemented; cannot check '" + condstr + "'")	cpreprocess_evaluate_cond def cpreprocess_evaluate_cond(state, condstr): return True # may be more often what we want :P
# todo: implement </s> pass	_parse_conllu def _parse_conllu(result: str):
# todo: convert to casetransaction object </s> case_action = commcarecaseaction.from_parsed_action(	get_stock_actions )) else: submit_time, user_id, xform, AbstractAction(CASE_ACTION_COMMTRACK) )
if not is_checksum_address(checksum_address):  # todo: more? </s> raise runtimeerror("invalid certificate checksum address encountered: {}".format(checksum_address))	__write_tls_certificate pseudonym = certificate.subject.get_attributes_for_oid(NameOID.PSEUDONYM)[0] checksum_address = pseudonym.value if host and (host != common_name_on_certificate): raise ValueError('You passed a hostname ("{}") that does not match the certificat\'s common name.'.format(host))
# todo remove .as_posix when requiring python 3.6 </s> with open(ele_filename.as_posix(), "w") as fh:	write ) ) fh.write("# This file was created by meshio v{}\n".format(__version__)) for cell_type, data in filter(lambda c: c.type == "tetra", mesh.cells):
# todo(ivanlei): should score the vt results here and only add them if they're interesting </s> if 1 == report.get('response_code'):	_lookup_iocs for md5 in reports.keys(): report = reports[md5] self._threat_info_by_iocs[md5] = reports[md5]
# todo: weather data source changed, temporarily disable, will enable it later when new data source is available. </s> self._is_temp = is_temp	CitiBikeTopology super().__init__() self._data_pipeline["trip"] = CitiBikePipeline(topology, trip_source, station_info, is_temp) def __del__(self): if self._is_temp:
#todo fix this </s> media_type = __getmediatype()	Main utils.Debug("Manual %s of '%s' is unsupported." % (args['action'], media_type)) elif args['action'] == 'togglewatched': if media_type in ['movie', 'show', 'season', 'episode']: data = {'media_type': media_type}
# todo save the error to the plugin </s> plugin_db_setting.save()	_init_plugins get_plugin_error(error, do_log=True, log_name='init') plugin_db_setting.active = False self.plugins_inactive[plug_key] = plugin_db_setting logger.info(f'Loaded integration plugin {plugin.slug}')
# todo: changing 'detailpage' to 'embedded' allows age-restricted content </s> url = 'https://www.youtube.com/get_video_info?html5=1&el=detailpage&video_id=' + vid	get_real_download_url vid = get_youtube_id(url) if vid is not None: r = requests.get(url) if not r.ok:
# todo this closure is ugly. it also doesn't work with </s> module = self._evaluator.wrap(self._parser.module())	get_completions def get_completions(self, user_stmt, path, dot, like): names, level, only_modules, unfinished_dotted = \ helpers.check_error_statements(module, self._pos)
# xxx todo: is there a better approach to handle the absense of a </s> app.config['whoosh_base'] = './whoosh_index'	whoosh_index if indx is None: if not app.config.get('WHOOSH_BASE'): wi = os.path.join(app.config.get('WHOOSH_BASE'), model.__class__.__name__) schema, primary = _get_whoosh_schema_and_primary(model)
# todo: update authors' num_sounds (when not handled via trigger) </s> if self.pack:	change_moderation_state self.save() if (current_state == 'OK' and new_state != 'OK') or (current_state != 'OK' and new_state == 'OK'): self.pack.process()
# todo(remove this when 11.6.1 is no longer supported) </s> if 'name' in kwargs:	_fixup_name def _fixup_name(self, kwargs): kwargs['name'] = fqdn_name('Common', kwargs['name']) else:
# todo: support negative indexes </s> if index < 0:	node_of_index def node_of_index(self, index): raise IndexError current_index = 0
# todo: this is just an example filter. </s> filters = filters or {}	available_backends def available_backends(self, filters=None): backends = self.backends for key, value in filters.items(): backends = {name: config for name, config in backends.items() if
# todo: this really shouldn't be in this class </s> return duration(seconds).getdisplaystring(durationformat.format.short)	formatDuration @pyqtSlot(int, result = str) def formatDuration(self, seconds: int) -> str:
# todo 目前仅在 华泰子类 中实现 </s> return self.get_today_trade()	today_trade 返回当天交易记录。 :return:
# todo: set n_chunks to 1 when signals.shape[1] is small enough </s> n_chunks = 10	_detrend regressor /= np.sqrt((regressor ** 2).sum()) regressor = regressor[:, np.newaxis] chunk_size = max(signals.shape[1] // n_chunks, 1) n_chunks = (signals.shape[1] - 1) // chunk_size + 1
# todo reenable </s> return result	values result = list(chain.from_iterable(self._evaluator.eval_element(v) for v in self._items()))
#todo wrap the lp api or use library </s> owner, ppa = url.split('/')[3:5]	clean_selected_ppa if not key_fingerprint: try: lp_url = 'https://launchpad.net/api/beta/~%s/+archive/%s' % (owner, ppa) req =  Request(lp_url)
# todo: improve error message(add error code) </s> raise validationerror(	validate_update_list self.validate_data_list(values, self.partial) else: "Expected data of form {'pk': 'data'..}"
#todo would be better to log.exception here </s> err_msg = "could not create a scan and match sample with avconv: %s. " % e	dynamic_data raise OSError  # handle errors in except except OSError as e: if err_output is not None: err_msg += "stderr: '%s'" % err_output
# todo #1497 #1358 </s> receipt = self.staking_agent.remove_unused_stake(staker_address=self.checksum_address,	_remove_unused_stake @save_receipt def _remove_unused_stake(self, stake_index: int) -> TxReceipt: stake_index=stake_index) return receipt
# todo: encode data ids and decode ids. </s> params = util.params( inputs, sanitize = false )	create inputs = payload[ 'inputs' ] inputs['runtool_btn'] = 'Execute' template, vars = tool.handle_input( trans, params.__dict__ ) output_datasets = vars[ 'out_data' ].values()
# todo: clarify variable names </s> for (nam2, int2) in list(system.interfaces.items()):	write_v4_config host = system.interfaces[interface["interface_master"]]["dns_name"] if ip is None or ip == "": if nam2.startswith(interface["interface_master"] + ".") \ and int2["ip_address"] is not None \
# todo(b/155239129): used list_physical_device in `setup` for gpu tests. </s> device).device.endswith(device))	test_embed_tensorflow_computation_succeeds_with_gpu self._get_embed_tensorflow_computation_succeeds_with_device(
# todo data alignment stuff </s> if 'byteoffset' in pysparse.json['indices'].keys():	read fmt = '<' + (fmt_char * component_nb) stride = struct.calcsize(fmt) offset = pysparse.json['indices']['byteOffset'] else:
""" only show the top todo. """ </s> command = listcommand(["-n", "1"], self.todolist, self.out, self.error)	test_list32 def test_list32(self): command.execute() self.assertEqual(self.output, "|  1| (C) Foo @Context2 Not@Context +Project1 Not+Project\n")
#todo: fix this. </s> self._static = none	rebuild self.listener.pause() try: self.build() except Exception, e:
# todo(dcramer): we need create a public api for 'sort_value' </s> context = serialize(results, request.user)	get results = list(search.query(**query_kwargs)) GroupMeta.objects.populate_cache(results) for group, data in zip(results, context): data['sortWeight'] = group.sort_value
# todo: fix inconsistent handling of water </s> special = {"hw": "h", "ow": "o", "wat": "o",	_parse_symbol Returns: A string with the parsed symbol. None if no parsing was possible. "wat": "O", "OH": "", "OH2": "", "NO3": "N"} parsed_sym = None
""" type setting - todo explain """ </s> return discovery['changeset']	get_changeset def get_changeset(self,obj):
pass  # todo </s> "draw circle from start to end."	circle def circle(start, end):
# todo also test these! </s> continue	test_classifiers_train continue if Clf in [MultinomialNB, BernoulliNB]: with warnings.catch_warnings(record=True): clf = Clf()
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
from gi.repository import glib  # todo: to fix </s> import gi	applyDelimiter else: if encoded: gi.require_version('Gtk', '3.0') res.append(GLib.markup_escape_text(TypeConvertor.encodeNetzobRawToGivenField(tmp, field)))
# todo: check return value of attachthreadinput properly </s> if isinstance(keys, six.text_type):	TypeKeys window_thread_id = win32functions.GetWindowThreadProcessId(self, 0) win32functions.AttachThreadInput(win32functions.GetCurrentThreadId(), window_thread_id, win32defines.TRUE) aligned_keys = keys.encode(locale.getpreferredencoding(), 'ignore') elif isinstance(keys, six.binary_type):
# todo add verbose output </s> return self._localedir	LocalizationModel @property def localedir(self): @localedir.setter def localedir(self, new_localedir):
# todo: make truly async </s> self._cloudstorage_client = storage.client()	__init__ def __init__(self):
# todo: what happens to sysex messages? </s> value = event.message	recv if num_events == 1: event = buffer[0] p = Parser() while value:
# todo handle second-order transitions (trigrams) </s> def count_trans(y, n_classes):	count_trans Parameters ----------
# todo: fix this case with correct thresholding </s> multitask_scores = evaluator.compute_model_performance(	test_multiclass_classification_singletask model = dc.models.MultitaskClassifier(1, 5, n_classes=5) evaluator = Evaluator(model, dataset, []) sklearn.metrics.roc_auc_score, n_classes=5) assert len(multitask_scores) == 1
# todo remove set! duplicates should not be normal </s> comp_str = str(sorted(set([str(c) for c in completions])))	completion_test fails += 1 else: if comp_str != correct: print 'Solution not correct, received %s, wanted %s' % \
#todo really dereference item? (sample pipe seems to suggest so: surprising) </s> value = reduce(lambda i,k:i.get(k), [item] + key['subkey'].split('.')) #forces an exception if any part is not found	pipe_rssitembuilder try: if "subkey" in key: else: value = util.get_value(conf[key], kwargs)
d.addcallback(self.failunlessisbardottxt) # todo: check headers </s> u_filename = u"n\u00e9wer.txt" # n e-acute w e r . t x t	test_GET_FILEURL_named save_url = base + "?save=true&filename=blah.txt" d.addCallback(lambda res: self.GET(save_url)) u_fn_e = urllib.quote(u_filename.encode("utf-8")) u_url = base + "?save=true&filename=" + u_fn_e
# todo: use slotssequenceelement to render this. </s> return ""	servers_with_corrupt_shares_p return tag
# todo - is there a less expensive way to get these from the database </s> context['to_build'] = [part for part in part.objects.filter(buildable=true) if part.need_to_restock()]	get_context_data context['starred'] = [star.part for star in self.request.user.starred_parts.all()] context['to_order'] = [part for part in Part.objects.filter(purchaseable=True) if part.need_to_restock()] return context
"""todo: is this function deprecated? </s> :param root:	updateModel def updateModel(root, fix = False): :param fix: :return:
#todo: we also need to account for presto/groups/comps metadata </s> pulp.server.util.create_repo(repo_path, checksum_type=repo["checksum_type"])	add_package except OSError: log.error("Link %s already exists" % pkg_repo_path) self.objectdb.save(repo, safe=True)
os.remove(zip_path) # todo: caching (at least for the public version?) </s> return response	download response["Content-Disposition"] = \ "attachment; filename={0}.zip".format(tutorial.slug)
#todo resend message + throttling </s> generated_token = totp(token.seed)	verify_computer token = user.token if token.method in ('call', 'sms'): if token.method == 'call': call(to=token.phone,
# todo: replicate complete behaviour of urllib.urlopener.retrieve </s> def retrieve( self, url, filename = none, reporthook = none, data = none ):	retrieve def mkdtemp( target_filepath ): destination_directory = tempfile.mkdtemp()
# todo add verbose output </s> return self._domain	LocalizationModel @property def domain(self): @domain.setter def domain(self, new_domain):
# todo/fixme: combine and refactor all these rotation transformations </s> if inv is not none:	Gaussian % (self.name, mu, Cov)) def rotate(self, R, inv=None, logdet=None, Q=None): invR = inv else:
# todo: check success summary the same way. </s> `failed_because` method.	test_summary_wrong_reason Summary classes should verify failure reason passed to the
# todo: all constant stuff should be calculated once, make this a class or something </s> def expand_template(template, namespace):	expand_template Currently, only Tempita templates are supported. @param template: The template, in preparsed form, or as a string (which then will be preparsed).
return rc  # todo?: parse </s> rc = self.__request__(method, url)	sbsSets url = 'sbs/sets'
pass # todo: raise exception here </s> else:	remove_signal del self.__signals[signal]
# todo better way to test this? </s> self.assertequal(0, count)	test_end_request count += 1
# todo (@awaelchli): standardize this across all plugins in lightning and lite. related refactor: #7324 </s> models = [self._setup_model(model) for model in models]	_setup_models_and_optimizers The returned objects are expected to be in the same order they were passed in. The default implementation will call :meth:`_setup_model` and :meth:`_setup_optimizer` on the input lists. optimizers = [self._setup_optimizer(optimizer) for optimizer in optimizers] return models, optimizers
# todo(phawkins): enable test after jaxlib 0.1.22 is released. </s> check_dtypes=true)	testIssue776 self.assertAllClose(onp.zeros(3,), api.grad(f)(onp.ones(3,)),
# todo: we need to pick the rank from `comm_shm`, not `comm`, </s> comm = none	_initialize @singledispatch def _initialize(iet): for i in iet.parameters: if isinstance(i, MPICommObject):
# todo: move upstream </s> @register_kl(transformeddistribution, transformeddistribution)	_kl_transformed_transformed def _kl_transformed_transformed(p, q): if p.transforms != q.transforms:
# todo: hack by genie for temporary markdown support </s> async def add_user_messages_e2e(self, messages, line_num):	MarkdownStoryReader ) self.current_step_builder.add_user_messages(parsed_messages) if not self.current_step_builder: raise StoryParseError(
# todo: a better way. loot at https://github.com/openmined/pysyft/issues/5249 </s> if parts[-2] == "return_types":	solve_real_type_functions parts = path.split(".") klass_name = parts[-1] modu = getattr(sys.modules["torch"], "return_types") else:
return none  # todo better error handling here </s> session_dict = response.json()	get_new_token except Exception, e: log.error(e) if u'error' in session_dict: log.error("Error when getting authenticated user: %s" % session_dict['error'])
# todo handle valueerror </s> if type != none:	optionalproperty value = config.extra_props[name] del config.extra_props[name] value = type(value) self.__dict__[name] = value
# todo(hartikainen): make this consistent such that there's no need </s> evaluation_env.render_rollouts(paths)	_evaluate logger.record_tabular(key, value) if hasattr(evaluation_env, 'render_rollouts'): iteration = epoch * self._epoch_length batch = self._evaluation_batch()
# todo: add explicit close of file_system. </s> return file_system_searcher.filesystemsearcher(file_system, mount_point)	GetSourceFileSystemSearcher else: mount_point = self._source_path_spec.parent
# todo for windows: </s> subprocess.check_call(kill_cmd, shell=true)	main if size_GB_done >= SIZE_GB_MAX or cnt_403_retry >= CNT_403_RETRY: kill_cmd = "screen -r -S %s -X quit" % screen_name print('\n') break
# update cache todo: should this be in the txn? </s> for (index, cache_service) in enumerate(self.cache.services):	update_app_service service ) if service.token == cache_service.token: self.cache.services[index] = service
# todo this dosn't work yet </s> class myexception(exception):	testWithTraceback e = IndexError(5).with_traceback(tb) self.assertIsInstance(e, IndexError) pass e = MyException().with_traceback(tb)
# todo: move this hard-coded mixin/manager injections to maybe a model </s> if self.dataset.slug == "socios-brasil" and self.name == "empresa":	get_dynamic_model_mixins def get_dynamic_model_mixins(self): mixins = [DatasetTableModelMixin] from core import data_models mixins.insert(0, data_models.SociosBrasilEmpresaMixin)
# todo, awni, get rid of this on next pytorch update </s> hx = autograd.variable(torch.zeros(x.shape[0], x.shape[2]), requires_grad=false).cuda()	decode_step if state is None: ax, sx = None, None else: hx, ax, sx = state
# todo(john sirois): clean this up when build parse refactoring is tackled. </s> unused_resolved_resources = self.resources	resolve def resolve(self): for resolved in super(WithResources, self).resolve(): yield resolved
# todo: errors </s> con = sc.connect_to_service_finish(results)	rest_post_connected def rest_post_connected(self, sc, results, command, data, callback, error_callback, callback_data): if con == None: return
# todo(henry-nash): add implementation here. </s> pass	expand_schema This is run manually by the keystone-manage command before the first keystone node is migrated to the latest release.
elif not self.urls and not self.packages:  #@todo: remove in 0.4.10 </s> self.fail("no link grabbed")	decrypt if link.strip(): self.urls = [link.strip()]  #@TODO: Remove `.strip()` in 0.4.10
# todo(jblespiau): we can simply use buf.xla_shape() when version 0.1.58 is </s> xla_shape = getattr(buf, "xla_shape", buf.shape)()	from_dlpack client = getattr(backend, "client", backend) buf = xla_client._xla.dlpack_managed_tensor_to_buffer(dlpack, client) assert not xla_shape.is_tuple() aval = core.ShapedArray(xla_shape.dimensions(), xla_shape.numpy_dtype())
os.rename(checkpoint_handler._saved[-1][1][-1], os.path.join(tb_logger.writer.log_dir, weights_name))  # todo: pr in ignite to have better access to saved file paths (cleaner) </s> tb_logger.close()	train trainer.run(train_loader, max_epochs=args.n_epochs) if args.local_rank in [-1, 0] and args.n_epochs > 0:
#todo change to native framework call, when plex allows token in header </s> request = urllib2.request(self.getlistsurl, headers=myheader)	LIST myHeader = {} myHeader['X-Plex-Token'] = users[user]['accessToken'] playlists = XML.ElementFromString(urllib2.urlopen(request).read()) result = {}
# todo: move to data class </s> return burp_menu	createMenuItems burp_menu.append(bugcatcher_menu)
# todo: enable once pyyaml requirement resolved with python 3.8 </s> return json.dumps(items, indent=indent, sort_keys=true)	to_json items = {key: getattr(self, key) for key in dir(self) if key in self.OUTPUT_FIELDS}
srcs = [sourcelayer(n_out=n_in, x_out=x_in, name='')] #todo </s> params = { 'sources': srcs, 'n_out': info[1], 'activation': info[2][1], 'dropout': drop, 'name': info[3] + "_bw", 'mask': self.mask }	initialize x_in = self.x for info, drop in zip(self.hidden_info, dropout[:-1]): name = params['name'] if info[0] == 'forward':
# todo we could potentially check at the unit level and only reject </s> raise valueerror("file %r was rejected because its x-pootle-revision is too old." % (file.name))	_import_file store, created = Store.objects.get_or_create(pootle_path=pootle_path) if rev < store.get_max_unit_revision(): except Exception as e: raise ValueError("Could not create %r. Missing Project/Language? (%s)" % (file.name, e))
# todo(b/134950354): test embedding column for non-eager mode only for now. </s> if not tf.executing_eagerly():	testCombinedFeatureColumnInput specs[indicator_key] = tensor_spec.TensorSpec([1], tf.int32) expected_dim += len(vocab_list) embedding_key = 'embedding_key' embedding_dim = 3
# todo: fix client side code to send us time information then adapt the next line </s> date_format = self.form_item.data['date_format'].split(' ')[0]	DateField wtf_field_class = wtforms.StringField def save_data(self, registration, value): value = datetime.strptime(value, date_format).isoformat() return super(DateField, self).save_data(registration, value)
raise deprecatedtest # this test is now broken. todo: fix it. </s> site = make_test_site()	test_one_to_many def test_one_to_many(): aliases = {'local_id' : 'id','local_name' : 'name',  'remote_label' : 'manies.label'} items = list(site.view('test_ap', aliases,{}))
# todo: not for checkbox (should have checkbox class) </s> for field in self.fields:	__init__ kwargs['error_class'] = BootstrapErrorList super(BootstrapForm, self).__init__(*args, **kwargs) self.fields[field].widget.attrs['class'] = 'form-control'
# todo: handle fancy-index copies by allocating a buffer and </s> next_index = self._subset_iterator.next()	next def next(self): return self._raw_data[next_index]
# todo: remove when botfactory can force everything to be unthreaded </s> time.sleep(0.1)	test_isup_command_requests_error irc.pm(user, '.isup {}'.format(url)) while bot.running_triggers: assert len(bot.backend.message_sent) == 1, ( '.isup command should output exactly one line')
# todo: check nd reply is valid </s> self.asserttrue(self.packet_outs_from_flows(nd_replies))	nd_for_controller 'ipv6_dst': str(ip_gw_mcast), 'neighbor_solicit_ip': str(dst_ip)})
#todo factorisation with accessor code ? </s> fmt_char = pysparse.gltf.fmt_char_dict[pysparse.component_type]	read pysparse.gltf.bufferViews[pysparse.json['values']['bufferView']].read() pysparse.bufferView = pysparse.gltf.bufferViews[pysparse.json['values']['bufferView']] component_size = struct.calcsize(fmt_char) component_nb = pysparse.gltf.component_nb_dict[pysparse.type]
#todo: check y.lod_level = 0 dtype </s> helper.append_op(	lod_reset if y is not None: check_type(y, 'y', (Variable), 'lod_reset') type="lod_reset", inputs={'X': x, 'Y': y}, outputs={'Out': out})
# todo: find out how to get global usernames </s> def block(mastodon, rest):	block @command
# todo check for other files </s> stream = open(config_file, 'r')	config switch_found = None if config_file: obj_doc = yaml.safe_load(stream) stream.close()
# todo remove this assertion and test </s> assert x.shape[1] == 2	odt all simplices containing x_i). import scipy.optimize mesh = MeshTri(X, cells, flat_cell_correction=None) initial_stats = gather_stats(mesh)
# todo: replace with "yield from" when dropping python 2. </s> for stream in streams.items():	_get_streams streams = HLSStream.parse_variant_playlist(self.session, m3u8_url, namekey="pixels") yield stream
# todo username </s> sudo = args.pushy('ssh+sudo:{hostname}'.format(	disk_zap for hostname, disk in args.disk: LOG.debug('zapping %s on %s', disk, hostname) hostname=hostname, ))
assert study_id == 0  # todo </s> self.trials[trial_id].params[param_name] = value	report_param def report_param(self, study_id, trial_id, param_name, value):
#todo: it should be a way to use relative path for platform independent. </s> conffile = str(qtcore.qdir.homepath()) + '/.config/wordforge/pootling.conf'	showDialog self.ui.progressBar.setValue(0) self.show() self.pickleTMObj = pickleTM(confFile)
# todo: use new schema from caso_full when its ready </s> return {	state_row population = case.estimated_population_2019 state_name = case.state "city": state_name, "city_ibge_code": case.city_ibge_code,
#convert regex to python format: todo use a common routine for this </s> replace = re.sub('\$(\d+)', r'\\\1', replace)   #map $1 to \1 etc.   #todo: also need to escape any existing \1 etc.	pipe_strregex match = util.get_value(rule['match'], kwargs) #todo use subkey? replace = util.get_value(rule['replace'], kwargs) #todo use subkey? rules.append((match, replace)) for item in _INPUT:
# todo: really dirty. figure out a better way. </s> exchange_trades = [x for x in exchange_trades if x.identifier not in all_set]	query_trades only_cache=only_cache, ) if self.premium is None: trades = self._apply_actions_limit(
raise exceptions.mpdnotimplemented  # todo </s> underscore, dash, dot and colon.	subscribe already. The name may consist of alphanumeric ASCII characters plus
# todo log here </s> return none	get_user_id return None if response.status_code != 200: data = response.json() if 'id' in data:
# todo: verify behavior </s> self.assert_received(self.debugger, [])	test_active_exception ])
# todo: how does this work with backfilling? </s> if hasattr(event, "replaces_state"):	_persist_event_txn "state_key": event.state_key, } vals["prev_state"] = event.replaces_state self._simple_insert_txn(
# todo: 289 </s> def __init__(self, is_me=true,	__init__ dht_port=None, federated_only=False,
# todo(mnaser): remove this in patch resolving the issue </s> self.assertin(volume_id, self.cinder.reserved_volumes)	test_delete_with_reserved_volumes self.api.delete_server(server['id'])
# todo: jrk: chunking times points needs to be simplified </s> parallel, p_time_gen, _ = parallel_func(_predict_slices, n_jobs)	_GeneralizationAcrossTime test_times['times'] = times_list self.test_times_ = test_times n_estimators = len(self.train_times_['slices']) n_chunks = min(n_estimators, n_jobs)
# todo replace hardcoded integers into constants/flags from cffi </s> if major_value == 851968 and minor_value == 2529639107:	validate_gss_status assert C.GSS_S_NO_CONTEXT == 524288 assert C.GSS_S_DEFECTIVE_TOKEN == 589824 raise CredentialsCacheNotFound( minor_status_str
#todo: implement more realistic closing semantics </s> pass	shutdown Currently a no-op on this MockSocket object.
# todo: nix unittest for pytest </s> for exception in (typeerror("onoz"), unsupportedalgorithm("oops")):	test_load_ecdsa_transmutes_crypto_exceptions def test_load_ecdsa_transmutes_crypto_exceptions(self): path = _support("test_ecdsa_256.key") with patch( "paramiko.ecdsakey.serialization.load_der_private_key"
# todo: use a contextmanager to ensure we always delete the callback from the list. </s> del self.neighbours_callbacks[remote]	wait_neighbours pass logger.debug('timed out waiting for neighbours response from {}'.format(remote)) return [n for n in neighbours if n != self.this_node]
# todo: clean this up. shouldn't we move the "checks" stuff to the </s> if not cmk_base.checks.check_info:	add_wato_static_checks_to_checks def add_wato_static_checks_to_checks(): global checks return static = []
# todo: enable this check when #1502 is fixed </s> res = self.app.get(url_for(controller='group', action='read', id='group_00', page=2))	test_group_read res = self.app.get(url_for(controller='group', action='read', id='group_00')) assert 'href="/group/group_00?page=2' in res assert 'href="/group/group_00?page=1' in res
# todo: fix this, is it needed? </s> data = await self.recv_data(reader, 512, tls_obj)	RDP data = await self.recv_next_tpkt(reader, tls_obj) else: if not data: logger.debug("Expected ErectDomainRequest. Got Nothing.")
# todo fitness? </s> return list(np.flip(ds, axis=0)) # most recent	rank_ds def rank_ds(self, ds):
# todo: check if a success http code can be returned with an empty body </s> return shareentity	shareEntities_delete raise endpoints.NotFoundException("shareEntity not found.") shareEntity.key.delete()
# todo: add for morph targets data. </s> min_index = min(indices)	extract_primitive_floor process_bone = False bone_max = bone_index max_index = max(indices) for old_index in indices:
# todo(hirofumi): apply character lm here </s> if c == self.space:	BeamSearchDecoder else: new_p_nb = np.logaddexp(p_b + p_t, p_nb + p_t) pass clmstate = None
# todo: move load and cpuload to sysinfo </s> if load1m > 2:	mon_turbo print("Total CPU usage:", cpuload, "%") print("Total system load:", load1m, "\n") print("High load, would turn turbo boost: on") if cur_turbo == "0":
except (testtransactionfailed, validationerror, valueerror):  # todo: 1950 </s> if crash_on_failure:	batch_deposits deposited_stakers, receipt = allocator.deposit_next_batch(sender_address=self.deployer_address, gas_limit=gas_limit) raise message = "Failed allocations batch"
# todo: large gains also expected when precalculating psi. </s> psi = tf_signal.frame(y, k, 1, axis=-1)[:, :t - delay - k + 1, ::-1]	get_correlations_narrow T = dyn_shape[-1] D = dyn_shape[0] Psi_conj_norm = ( tf.cast(inverse_power[None, delay + K - 1:, None], Psi.dtype)
# todo don't use exceptions to control program flow </s> @_template_renderer("builds.html")	builds def builds(request): queryset = Build.objects.all()
#todo: actually optimize free space on the texture. </s> if max_width == none or max_height == none:	merge_frames merge all given frames of this slp to a single image file. frames = [(PNG, (width, height), (hotspot_x, hotspot_y)), ... ] for (_, (w, h), _) in frames: if w > max_width:
from vyper.old_codegen.expr import expr  # todo rethink this circular import </s> value, gas = none, none	get_gas_and_value def get_gas_and_value(stmt_expr, context): for kw in stmt_expr.keywords: if kw.arg == "gas":
# todo: figure out a solution that doesn't require hoping that </s> self.worker.config['url'] = 'http://localhost:9999/'	test_send_sms_noconn def test_send_sms_noconn(self): A 'connection refused' error should be retried. self.worker.failure_published = FailureCounter(1) msg = self.make_outbound("outbound")
# todo find out what's wrong </s> warnings.warn('stroud-secrest\'s scheme x for e_3^r has degree 3, not 7.')	_x (D, pm(3, eta)), ] return 3, data
# todo: exit codes (not only for this, but for other exceptions) </s> if action == "destroy":	ursula emitter.echo(str(e), color='red', bold=True) click.get_current_context().exit(1) if dev: message = "'nucypher ursula destroy' cannot be used in --dev mode - There is nothing to destroy."
# todo: use different flag than .reentrant </s> pos = colorsorter._transform_point(pos)	schedule_sphere color, pos, radius, ColorSorter._debug_transforms() ##### if ColorSorter._parent_csdl and ColorSorter._parent_csdl.reentrant: if drawing_globals.use_c_renderer and ColorSorter.sorting: if len(color) == 3:
# todo: move error code and field outside function </s> raise validationerror(	validate_storefront_url "`ALLOWED_CLIENT_HOSTS` configuration." ) {"redirectUrl": error_message}, code=AccountErrorCode.INVALID
# todo: deprecated - remove in version 0.10 </s> if isinstance(training_trackers, string_types):	train "Pass appropriate featurizer " "directly to the policy instead.") logger.warn("Passing a file name to `agent.train(...)` is " "deprecated. Rather load the data with "
# todo debug </s> print ("counter for rule with order '%d' "	_updateRule % alertLevel.level + "Resetting rule.") % ruleStart.order + "for alert level '%d' has reached its limit. "
# todo: error handling like numba callwrappers.py </s> native_val = unbox_array(types.array(dtype=dtype, ndim=1, layout='c'), arr_obj, c)	lower_unbox_series native_val = unbox_str_series(string_array_type, arr_obj, c) else: c.pyapi.decref(arr_obj) return native_val.value
#todo need gutter of scrollbar - how do we get that? </s> if event.button == 1:	on_diffmap_button_press_event def on_diffmap_button_press_event(self, area, event): size_of_arrow = 14 diffmapindex = self.diffmap.index(area)
# todo: add test here as well </s> assert chatcommands.iswlu("http://stackoverflow.com/users/4622463/angussidney", original_msg=msg) == \	test_whitelisted_users assert chatcommands.addwlu("http://stackoverflow.com/users/4622463/angussidney", original_msg=msg) == \ "User whitelisted (`4622463` on `stackoverflow.com`)." "User is whitelisted (`4622463` on `stackoverflow.com`)." assert chatcommands.rmwlu("http://stackoverflow.com/users/4622463/angussidney", original_msg=msg) == \
raise notimplementederror # the below does most probably not work anymore todo </s> if ticket is none:	show_dependencies TESTS:: TODO ticket = self._current_ticket() try:
# todo: type ignored -- breaks liskov substitution. </s> entry.set_repr_style("short")	_prunetraceback for entry in excinfo.traceback[1:-1]:
# todo: remove when botfactory can force everything to be unthreaded </s> time.sleep(0.1)	test_isup_command_ok irc.pm(user, '.isup example.com') while bot.running_triggers: assert len(bot.backend.message_sent) == 1, ( '.isup command should output exactly one line')
# todo: marker </s> row = (self.mceditbutton, self.viewdistancedown, label("view distance:"), self.viewdistancereadout,	LevelEditor self.sessionLockLabel.tooltipText = "Session Lock is being used by MCEdit" self.sessionLockLabel.mouse_down = self.mouse_down_session self.viewDistanceUp, self.viewButton, self.viewportButton, self.recordUndoButton, Row((self.sessionLockLabel, self.sessionLockLock), spacing=2))
if source_file:  # todo: should we error here or something if the source_file doesn't exist? </s> try:	generate_file Generates a new image file by processing the source file and returns the content of the result, ready for saving. fp = source_file.storage.open(source_file.name) except IOError:
# todo remove comment parameter. </s> constraints = []	Z3Solver @property def constraints(self): for c, comment in self._constraints: if comment:
self.info          = {}  #@todo: remove in 0.4.10 </s> self.last_notify   = 0	setup def setup(self): self.notifications = 0
# todo: log modification too? </s> before = unicode(tb_ann)	_edit_span assert False, error_msg else: tb_ann.start = int(start) tb_ann.end = int(end)
# todo: maybe change this later to push some more info, not just the </s> message = socket.inet_aton(self.config.get("global", "clientip"))	do_hello def do_hello(self): self.send(common.CONTROL_CHANNEL_BYTE, common.CONTROL_INIT+message, (self.server_tuple, None))
# todo(pradeep): try not to use the function of a member object. may be expose </s> padded_corpus_indices = self.nn_solver.data_indexer.pad_indices(corpus_indices,	train_lsh corpus_indices.append(mapped_indices[index][0]) self.sentence_index[len(self.sentence_index)] = line self.max_sentence_length) encoder_input = numpy.asarray(padded_corpus_indices, dtype='int32')
# todo: this logic does not prevent duplicate test cases, need to address this in the future. </s> if self.proto == socket.sock_dgram:	transmit if not data: data = node.render() MAX_UDP = 65507 if os.name != "nt" and os.uname()[0] == "Darwin":
# todo (aron): add i18n by varying the language of the topic tree here </s> topictree = get_flat_topic_tree()	add_full_title_from_topic_tree @staticmethod def add_full_title_from_topic_tree(entry, video_title_dict): entry_kind = entry['entity_kind'] entry_name = entry['entity_id']
# todo: abstract and require implementation? </s> return job.download_as_string()	_readContents raise NoSuchJobException(jobStoreID)
self.asserttrue(greps(out, "zzc.service.pid")) # todo ? </s> cmd = "{systemctl} start zzb.service -vv"	test_4105_systemctl_py_kill_in_stop self.assertEqual(end, 0) self.assertFalse(greps(out, "zzb.service.pid")) out, end = output2(cmd.format(**locals())) logg.info(" %s =>%s\n%s", cmd, end, out)
# todo(shardy): may be able to remove when the bug above is fixed </s> nova = client.client(con.service_user, con.service_password,	authenticate raise exception.AuthorizationFailure() try: con.tenant, con.auth_url, proxy_token=token_id,
# todo: exception for coalesces that represents all sub_specs tried </s> class coalesce(object):	Coalesce def __init__(self, *sub_specs, **kwargs): self.sub_specs = sub_specs
self._cloud_flow_complete_message.addaction("", i18n_catalog.i18nc("@action", "review your connection"), "", "", 1) # todo: icon </s> self._cloud_flow_complete_message.actiontriggered.connect(self._onreviewcloudconnection)	_createCloudFlowCompleteMessage image_caption = i18n_catalog.i18nc("@info:status", "Connected!") )
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: separate the service from the node model </s> def __init__(self, node: qrlnode):	__init__ self.node = node
# todo simplify </s> print('training logistic_regression')	logistic_regression def logistic_regression(self): trained_model = self._dsm.logistic_regression() self.print_metrics(trained_model)
# todo: keep the parser around, so we don't have to </s> p = parser()	recv event = buffer[0] value = event.message while value: byte = value & 0xff
# todo hacky console hacky-ness </s> self.root.main.console.configure(state="normal")	_select_bind def _select_bind(self, event): self.tag_configure(self._last, background="") self.root.main.console.delete(1.0, "end") self.root.main.console.configure(state="disabled")
# todo: handle this in the ui </s> import traceback	_layoutEngineOTLTablesRepresentationFactory addOpenTypeFeaturesFromString(otf, font.features.text) except: print(traceback.format_exc(5)) for name in ("GDEF", "GSUB", "GPOS"):
# todo: serialize the policy </s> return response('policy created!', status=200)	create_policy new_policy = drone_alice.create_policy(bob, label, m, n, federated=federated_only)
# todo: direct use of json['error'] is deprecated; use display_message('...', 'error') instead. </s> mods_json['error'] = 'text span contained new-line, rejected'	save_span else: mods_json = {} mods_json['duration'] = 3 txt_file_path = document + '.' + TEXT_FILE_SUFFIX
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo make fetch_result _not_ a pd.dataframe </s> fetch_result = processresult(	test_first_row_is_header_empty_values def test_first_row_is_header_empty_values(self): pd.DataFrame({'A': ['', 'x'], 'B': [2, 3]}) )
# todo it may be interesting to get a random bucket among the acquirable </s> if maybe_old_bucket is not none:	LockServer if len(acquirable_buckets) == 0: return None, remaining old_bucket = maybe_old_bucket
# todo: support format.binary once it is supported in </s> if format not in self._supported_formats:	inject def inject(self, span_context, format, carrier): raise opentracing.UnsupportedFormatException propagator = propagators.get_global_httptextformat()
# todo(leofang): test float16 ('e') once cupy/cupy#5346 is resolved </s> @testing.for_dtypes('iilqfd')	test_shfl @unittest.skipIf(runtime.is_hip, 'HIP is not yet supported') def test_shfl(self, dtype): @jit.rawkernel()
# todo use the json output option to ceph so we can stop scraping </s> if output:	_check_pgs_active_and_clean def _check_pgs_active_and_clean(self, output): try: _, total_stat, pg_stat, _ = output.replace(';', ':').split(':')
# todo: test for the _correct_ revision_id value. </s> if not activity.revision_id:	_update_resource if not activity.id: assert False, "activity object has no id value" assert False, "activity has no revision_id value" assert activity.timestamp >= before and activity.timestamp <= after, \
# todo: only works for ratios </s> res.metadata['reference_kpi'][dk['name']] = re.sub('([a-za-z_]+)/', '', dk['formula'])	delta kpis_to_analyse.update([dk['name']]) self.kpis.loc[:,dk['name']] = eval(re.sub('([a-zA-Z_]+)', r'self.kpis.\1.astype(float)', dk['formula'])) if kpi_subset is not None: kpis_to_analyse.intersection_update(kpi_subset)
# todo: make this config driven </s> metadata = {}	deb_package_metadata def deb_package_metadata(package): result = deb_query(package) if result.success:
# todo: figure out a better way of handling rpc style calls. </s> if self.request.arguments.has_key('interrupt'):	KernelActionHandler class KernelActionHandler(web.RequestHandler): def get(self, kernel_id): self.application.interrupt_kernel(kernel_id) if self.request.arguments.has_key('restart'):
# todo: remove this method in v2.5 </s> elif self._values['enabled'] in booleans_true:	disabled elif self._values['enabled'] in BOOLEANS_FALSE: return True return False elif self._values['state'] == 'enabled':
# todo: this is a temporal fix </s> pixels = np.rollaxis(pixels, 0, len(pixels.shape))	hog ValueError Window step unit must be either pixels or cells if mode not in ['dense', 'sparse']: raise ValueError("HOG features mode must be either dense or sparse")
# todo: distinguish between urllib and urllib2 contracts </s> class tufdownloadmixin( object ):	TUFDownloadMixin def tuf_open( self, tuf_updater, data = None ): filename, headers = self.tuf_retrieve( tuf_updater, data = data )
node.test = gast.call(gast.attribute( # todo any over dim 0 </s> test, gast.name('any', gast.load(), none), none), [], [])	visit_While raise NotImplementedError("cannot process while-else") test = node.test return self.visit_loop(node, test)
# todo: identify the specific structure we're finding and document this a bit better </s> pointer = context.object("pdbscan!unsigned long long",	method_kdbg_offset seen = set()  # type: typing.Set[int] for result in results: offset = result + 8, layer_name = physical_layer_name)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo log this </s> pass	_get_search_index pass except widx.IndexVersionError as e:  # (msg, version, release=None) except widx.OutOfDateError as e: raise e
kind = 'article'  # todo: recognise pages </s> yield (post_title, content, slugify(post_title), post_creadt, author,	dc2fields content = content.replace('\\n', '') post_format = "html" categories, tags, kind, post_format)
# todo: when merging forward to 0.56.x.x, arnold lights use </s> node = plug.node()	__plugMetadata if name == "noduleLayout:visible" and plug.getInput() is not None and not plug.node().getName().startswith( "__" ) : return True if isinstance( node, GafferArnold.ArnoldShader ) : key = plug.node()["name"].getValue()
# todo: warn the user if mode of ensemble </s> out = self.ensemble.output(x)	predict def predict(self, X): y = self.combiner.combine(out) return y
# todo: the following reproduces the old behavior of </s> i.clear()	_transformBlock for i in block.component_objects(Integral, descend_into=True): i.parent_block().reclassify_component_type(i, Expression) i._constructed = False i.construct()
# todo we could reload the message </s> return self.input_chat	Forward async def get_input_chat(self): Returns `input_chat` but will make an API call if necessary.
# todo change to native framework call, when plex allows token in header </s> opener = urllib2.build_opener(urllib2.httphandler)	sendReq else: try: request = urllib2.Request(sizeURL) request.add_header(
# todo(b/151468119): remove this branch after next release. </s> run_inference = importlib.import_module('tfx_bsl.beam.run_inference')	_run_model_inference from tfx_bsl.public.proto import model_spec_pb2 except ImportError: model_spec_pb2 = importlib.import_module('tfx_bsl.proto.model_spec_pb2') saved_model_spec = model_spec_pb2.SavedModelSpec(
# todo: fill this in </s> )	_run_job_flow_instances _InstanceGroups=[],
# todo: log exception </s> pass	get_notes hit['_source']['text'] = Markup.escape(hit['_source']['text']) except Exception as e: return jsonify(response)
# todo (t65593688): this should be removed after </s> with torch.no_grad():	test_tokens def test_tokens(self): model = Seq2SeqModel.from_config( Seq2SeqModel.Config(
# todo cachew for all commits? </s> def commits() -> iterator[commit]:	commits for r in repos(): log.info('processing %s', r)
# todo: fetch that from the api with paraminfo </s> special_names = set('deleteglobalaccount', 'patrol', 'rollback',	preload_tokens type='|'.join(types)).submit() else: 'setglobalaccountstatus', 'userrights', 'watch')
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
aiplayer.load_abstract_buildings(self.session.db) # todo: find a better place for this </s> if pirate_enabled:	init_new_world ret_coords = point.to_tuple() if any(isinstance(player, AIPlayer) for player in self.players): self.pirate = Pirate(self.session, 99998, "Captain Blackbeard", Color()) self.session.ingame_gui.message_widget.add(self.max_x/2, self.max_y/2, 'NEW_WORLD')
#todo(#212): use a map construct instead of unrolling. </s> lhs = batching.move_dim_to_front(lhs, lhs_bdim)	conv_general_dilated_batch_rule lhs_dim, rhs_dim, out_dim = dimension_numbers if lhs_bdim is not None and rhs_bdim is not None: rhs = batching.move_dim_to_front(rhs, rhs_bdim) outputs = [
# todo: handle dbkeys </s> params = util.params(inputs, sanitize=false)	_create inputs[k] = v self._patch_library_inputs(trans, inputs, target_history) incoming = params.__dict__ use_cached_job = payload.get('use_cached_job', False) or util.string_as_bool(inputs.get('use_cached_job', 'false'))
# todo move interpretation of data into column config </s> if row[report_column.column_id] == sub_col.expand_value:	_get_query_results for sub_col in get_expanded_column_config(self.config, report_column, 'en').columns: ui_col = report_column.column_id + "-" + str(counter) r[ui_col] = 1 else:
# todo replace with collections.sequence subclass </s> spotify.error.maybe_raise(self.error)	tracks def tracks(self): Will always return :class:`None` if the search isn't loaded. if not self.is_loaded: return None
# todo delete me </s> import yaml	parseSDFJoint sensors = parseSDFSensors(joint.findall('sensor')) jointdict['annotations'] = {'sdf': sdfannos} print('JOINT:', yaml.dump(jointdict)) print('POSE:', yaml.dump(pose))
# todo find out if this is good because of sparcity... </s> 'prefers_data_normalized': false,	get_meta_information 'handles_numerical_features': True, 'prefers_data_scaled': False, 'handles_multiclass': True, 'handles_multilabel': True,
#todo(chris): implement service_catalog </s> self.service_catalog = none	authenticate ["nova"][0]["publicURL"] self.auth_token = body["auth"]["token"]["id"]
# todo(ylc/zhifengc): add this to a policy module and test it. </s> def tpuordinalfunction(task_id, shard_index_in_host):	TPUOrdinalFunction if p.use_per_core_infeed: return shard_index_in_host
# todo: check if output is spent </s> o.spent = none	gettransaction i.value = value[0] for o in t.outputs: return t
# todo note this is inefficient since we are running the raw dataframe through the pipeline twice. </s> results = self.make_predictions_with_k_factors(dataframe, number_top_features=number_top_features)	make_original_with_predictions_and_features Returns: pandas.core.frame.DataFrame: dataframe.drop([self.prediction_column], axis=1, inplace=True) results = pd.concat([dataframe, results], axis=1)
except exception:  # todo: refactor this... </s> e = sys.exc_info()[0]	test_PUT_InvalidData try: self.assertEqual(self.rnw.PUT(url, data), 'json') self.assertEquals(e, TypeError)
# todo: find better ways... </s> bucket_size = (len(entries) // 10) + 1	save_entries def save_entries(entries, dir=u'.'): offset = 0 for i in range(1, 11):
# todo: set the following parameter </s> config[setting_name('social_auth_disconnect_pipeline')] = (	_parse_config ) config[setting_name('DISCONNECT_REDIRECT_URL')] = ()
# todo: this isn't a true unit test.  it depends on the test cmakelists.txt file having been written correctly. </s> generator = platform.get_best_generator()	test_fortran_compiler def test_fortran_compiler(): platform.write_test_cmakelist(["Fortran"]) assert(generator is not None)
# todo - verify contents </s> self.client.logout()	testDashboard2 response = self.client.get('/dashboard/', {'view': 'outgoing'}) self.assertEqual(response.status_code, 200)
duration_s = none  # todo </s> recording_software_name = none  # todo	_recording_update_legacy_from_v1_15_to_pprf_2_0 start_time_system_s = None  # TODO start_time_synced_s = None  # TODO recording_software_version = None  # TODO recording_name = None  # TODO
# todo: document! </s> ku, kv = _generalized_kraus(q_oper, thresh=thresh)	choi_to_stinespring def choi_to_stinespring(q_oper, thresh=1e-10): assert(len(kU) == len(kV)) dK = len(kU)
# todo: more arguments possible: objectdb etc. </s> if not exists(join(path, '.git')):	__init__ if url is not None: Repo.clone_from(url, path) self.repo = Repo.init(path, True) else:
# todo: t196619 </s> assert(self.limited_module)  # some modules do not have a prefix	set_namespace namespace restriction. @raises KeyError: a namespace identifier was not resolved param = self.site._paraminfo.parameter('query+' + self.limited_module, 'namespace')
# end todo </s> return einsum('cbi->bic', (sqrt_ggn, ))	backpropagate_sqrt_ggn sqrt_ggn = sqrt_ggn.view(num_classes, batch, in_features)
# todo: change to 'internal cat' (issue 1013) </s> tok = token(id.lit_chars, runtime.no_spid, '__cat')	RunCommandSub len(simple.redirects) == 1 and simple.redirects[0].op.id == Id.Redir_Less): cat_word = compound_word([tok]) simple.words.append(cat_word)
#! todo: consider making linestyle configurable </s> def nyquist_grid(cl_mags=none, cl_phases=none):	nyquist_grid Usage =====
# todo: convert to utils.retry </s> retries = 0	_WaitForSync for dev in instance.disks: lu.cfg.SetDiskID(dev, node) degr_retries = 10 # in seconds, as we sleep 1 second each time while True:
# todo: can cause an endless loop for single track repeat. </s> self.next()	play self.core.tracklist.mark_unplayable(tl_track) if on_error_step == 1: elif on_error_step == -1: self.previous()
# todo: avoid dummy and generate func here when inlining is possible </s> def _impl(df, level=none, drop=false, inplace=false,	reset_index_overload def reset_index_overload(df, level=None, drop=False, inplace=False, col_level=0, col_fill=''): col_level=0, col_fill=''): return hpat.hiframes.pd_dataframe_ext.reset_index_dummy(df, inplace)
# todo: may test with codecs.open passing an encoding </s> temp = tempfile.namedtemporaryfile(delete=false)	test_export_to_csv_fobj def test_export_to_csv_fobj(self): self.files_to_delete.append(temp.name) rows.export_to_csv(utils.table, temp.file)
# todo(hub-cap): turn this into middleware </s> context = context.reddwarfcontext(	index def index(self, req, tenant_id): auth_tok=req.headers["X-Auth-Token"], tenant=tenant_id)
#todo - is there a nice way to return an interator and </s> records = list(seqio.parse(child.stdout, new_format))	emboss_piped_SeqIO_convert child.stdin.close() child.stderr.close() child.stdout.close() return records
#todo: use universe / ag that </s> pass	time_forces except NoDataError:
# todo: verify </s> mass = elem.mass()	build_Mgg etype = elem.type if etype in ['CROD', 'CONROD', 'CTUBE']: nid1, nid2 = elem.nodes i1 = dof_map[(nid1, 1)]
# todo: for some reason this test used non-stadard slashing parameters (#354) </s> deployment_parameters[1] = 300	adjudicator secret_hash = testerchain.interface.w3.keccak(adjudicator_secret) deployment_parameters = list(slashing_economics.deployment_parameters) deployment_parameters[3] = 2 contract, _ = testerchain.interface.deploy_contract(
# todo(higumachan): remove this "if" section after tensorflow supports python 3.7. </s> if not _available:	test_keras_pruning_callback def test_keras_pruning_callback(): pytest.skip('This test requires keras ' 'but this version can not install keras(tensorflow) with pip.')
# todo: add attachements to test notebook </s> from zim.fs import file	testSingleFile def testSingleFile(self): folder = self.setUpFolder('single', mock=tests.MOCK_ALWAYS_REAL) file = folder.file('test.html')
# xxx todo </s> raise notimplementederror()	remove_from_postmortem_exclusion_list elif bits not in (32, 64): raise NotImplementedError("Unknown architecture (%r bits)" % bits)
# todo: use actual github clone string used by github </s> clone='git://github.com/%s.git' % gid,	massgit login=g.user.login, full_name=gid, is_github=True )
# todo: dump to file </s> if self._in == none:	close def close(self): return self._in.close()
)  # todo(unilight): is changing to ilens_ds_st right? </s> loss = loss + enc_attn_loss	forward enc_attn_loss = self.attn_criterion( att_ws, ilens_ds_st, ilens_ds_st report_keys += [{"enc_attn_loss": enc_attn_loss.item()}] if "decoder" in self.modules_applied_guided_attn:
#ack = self.serial_port.read() # todo: use ack </s> self.sendcommand(145) # 10010001	SetRightLaserOff def SetRightLaserOff(self):
# todo: does this need to be smarter? </s> indexedtokens["until"] = until[:8]	validateComponentsForCalDAV indexedTokens["UNTIL"] = "%sT000000" % (until,) else: newValue = u";".join(["%s=%s" % (k,v) for k,v in indexedTokens.iteritems()]) rrule.setValue(newValue)
# todo: how to check it? meybe we can omit this test </s> pass	test_uniformfill def test_uniformfill():
# @todo: resolve makedeps in case if it was specified by provides, </s> retry_interactive_command(	_remove_make_deps bold_line(self.package_name) )) [ 'sudo',
# todo: figure out way to paramaterize node['osds'] for this test </s> for osd in node["osds"]:	test_osd_services_are_running def test_osd_services_are_running(self, node, Service): assert Service("ceph-osd@%s" % osd).is_running
# @todo: also add 'clear' button to clear all elements & start from a blank slate </s> stable = current.s3db.event_scenario	event_rheader editable = current.auth.s3_has_permission("UPDATE", "event_incident", record_id) if editable and r.method == "plan": query = (stable.incident_type_id == incident_type_id) & \ (stable.deleted == False)
# todo(py3.7): add required=true </s> return int(arg, 2)	bits def bits(arg):
# todo: confirm necessity of this session clearing and lay out mechanics. </s> tuf.download._sessions = {}	test_https_connection download.unsafe_download(expired_https_url, target_data_length) os.environ['REQUESTS_CA_BUNDLE'] = good_cert_fname logger.info('Trying HTTPS download of target file: ' + good_https_url) download.safe_download(good_https_url, target_data_length)
# todo: is this required for a visual operation? </s> vi_cmd_data['motion_required'] = false	vi_u def vi_u(vi_cmd_data): vi_cmd_data['motion']['command'] = 'no_op' vi_cmd_data['motion']['args'] = {}
return none  # todo better error handling here </s> body = response.json()	validate_email except Exception, e: log.error(e) is_valid = body['is_valid'] if is_valid:
# todo: why the reverse order? </s> rotatef(rotz, 0, 0, 1)	transform def transform(x, y, z, rotx, roty, rotz, sx, sy, sz, cx, cy, cz): translatef(x - cx, y - cy, z - cz) rotatef(roty, 0, 1, 0) rotatef(rotx, 1, 0, 0)
# todo remove this in a future version </s> if lf_val == 'syslog':	_processes_from_section syslog = boolean(get(section, sy_key, False)) logfiles[sy_key] = syslog self.parse_warnings.append( 'For [%s], %s=syslog but this is deprecated and will '
# todo: remove in v8 </s> self._global_context['blog_desc'] = self.config.get('blog_description')	__init__ self._GLOBAL_CONTEXT['blog_title'] = self.config.get('BLOG_TITLE') self._GLOBAL_CONTEXT['blog_description'] = self.config.get('BLOG_DESCRIPTION') self._GLOBAL_CONTEXT['blog_url'] = self.config.get('SITE_URL', self.config.get('BLOG_URL')) self._GLOBAL_CONTEXT['body_end'] = self.config.get('BODY_END')
# todo: it would be nice to be async about this. set 1 second timeout. </s> r = requests.post(service, data=payload_json, timeout=1,	version_check payload_json = json.dumps(payload, sort_keys=True) try: headers=headers) except Exception:
# todo: find a way to make the reduction only once, so we don't need to clone. </s> if isinstance(value, torch.tensor):	__sync if not sync_dist or not dist_available: return value value = value.clone() else:
# todo: checks for being not outside of this repository </s> and opj('annex', 'objects') in os.readlink(str(filepath))  # realpath ok	file_has_content return filepath.exists() and ( filepath.is_symlink() ) return self._check_files(self.find, quick_check,
# todo: i can't manage the import issue, can you? </s> new_self, new_args = syft.frameworks.torch.hook_args.hook_method_args(cmd, self, args)	handle_method_command cmd, self, args = command print("Logtensor logging method", cmd) new_command = (cmd, new_self, new_args) response = type(new_self).handle_method_command(new_command)
if self._ndim == 3: # todo: use hasz </s> array = c_double * 3	ctypes @property def ctypes(self): return array(self.x, self.y, self.z) else:
# todo: something a bit less heavy than eval </s> return safe_eval(terms)	listify_lookup_plugin_terms pass if '{' or '[' in terms: if isinstance(terms, basestring): terms = [ terms ]
# todo: replace xrange (could fail with 32-bit python 2.x). </s> for x in xrange(self.bytelength - 1):	setoffset if newoffset < self.offset: shiftleft = self.offset - newoffset data[x] = ((data[x] << shiftleft) & 255) + \ (data[x + 1] >> (8 - shiftleft))
# todo: verify set_fields </s> def match(self, in_port=none, vlan=none,	ValveTable self.flow_cookie = flow_cookie self.notify_flow_removed = notify_flow_removed eth_type=None, eth_src=None, eth_dst=None, eth_dst_mask=None,
# todo: handle external images </s> image_path.parent.mkdir(parents=true, exist_ok=true)	Template image_path = Path(f"images/{self.key}/{lines}.jpg")
# @todo: copy relevant parts of translate toolkit internally to avoid external dependencies </s> from translate.convert.csv2po import main as csv2po	write_po from tempfile import NamedTemporaryFile from gluon.contenttype import contenttype f = NamedTemporaryFile(delete=False) csvfilename = "%s.csv" % f.name
# todo: remove this asap. </s> pkg_license = pkg.license	show return '' _dict = pkg.as_dict() if pkg_license: _dict['license'] = pkg_license.title
# todo: recursive </s> cls._fields = _get_input_fields(cls)	wrap return print_type(self.field) setattr(cls, "__repr__", repr_) cls.field = GraphQLInputObjectType(name=cls.__name__, fields=cls._fields) return dataclass(cls, repr=False)
# todo: should actually be implemented by annexrepo </s> (out, err) = \	_get_key_path def _get_key_path(self, key): self.runner(['git', 'annex', 'contentlocation', key], cwd=self.path) return opj(self.path, out.rstrip(os.linesep))
# todo consolidate to base class </s> carbon.seteventlooptimernextfiretime(self._timer, ctypes.c_double(0.0))	post_event return
# todo: check that body contains link to dashboard and email prefs. </s> assert encoded_body in email[3]	check_email encoded_body = self.mime_encode(body, name)
# todo - log details about the test </s> return result	check_mvip_connection test = self.sfe.test_connect_mvip(mvip=self.mvip) result = test.details.connected except: err = get_exception()
# avdn: todo </s> self.assertequal(format(123.456, '.4'), '123.5')	test_issue5864 def test_issue5864(self):
# todo: also deal with empty directories </s> remote_files = self._list_remote_files_with_info(cmd["source_paths"])	_cmd_download total_size = 0 completed_files_size = 0 target_dir = cmd["target_dir"].rstrip("/").rstrip("\\") download_items = []
# todo: replace by plugins_info.move_to_end('coretasks') for python 3 </s> core_info = plugins_info.pop('coretasks')	setup (plugin.name, (plugin, is_enabled)) for plugin, is_enabled in plugins.enumerate_plugins(self.config)) plugins_info['coretasks'] = core_info load_success = 0
# todo(tsileo): also update following (it's in the object) </s> app.logger.info(f"actor cached for {iri}")	task_cache_actor upsert({MetaKey.ACTOR: actor.to_dict(embed=True)}), ) if activity.has_type([ap.ActivityType.CREATE, ap.ActivityType.ANNOUNCE]): Tasks.cache_attachments(iri)
raise mpdnotimplemented # todo </s> def _commands(self):	_commands @register(r'^commands$')
# todo: implement versioning on all subclasses </s> pass	target def target(self):
# todo: avoid using default index? </s> with option_context(	__setitem__ if len(self) != len(value): raise ValueError("Length of values does not match length of index") "compute.default_index_type", "distributed-sequence",
# todo: update the tolerance after the ci moves to torch 1.10 </s> self.asserttrue(torch.allclose(hidden_states_slice, expected_hidden_states_slice, atol=1e-2))	test_inference_base [[[0.0577, 0.1161], [0.0579, 0.1165]], [[0.0199, 0.1237], [0.0059, 0.0605]]] )
# todo: check nd reply is valid </s> self.asserttrue(self.packet_outs_from_flows(nd_replies))	test_nd_for_controller 'ipv6_dst': str(ip_gw_mcast), 'neighbor_solicit_ip': str(dst_ip)})
# todo: test with ion/ioff </s> frame_width = getattr(cmd, "frame_width", none)	_try_add_matplotlib_info if (type(value).__name__ == "Figure" and type(value).__module__ == "matplotlib.figure"): frame_height = getattr(cmd, "frame_height", None) if frame_width is not None and frame_height is not None:
# todo bind a log_cb to this config ^^ </s> self._brokers = {}	Cluster def __init__(self, seed_hosts): self.config = {"metadata.broker.list": seed_hosts} self._topics = {} self.update()
# todo 0.24: raise a valueerror instead of a warning </s> with pytest.warns(deprecationwarning,	test_random_state_shuffle_false @pytest.mark.parametrize('Klass', (KFold, StratifiedKFold)) def test_random_state_shuffle_false(Klass): match='has no effect since shuffle is False'): Klass(3, shuffle=False, random_state=0)
# todo[k]: remove when t853 is properly fixed. </s> def test_draft_deletions_include_version(db):	test_draft_deletions_include_version with session_scope() as db_session: with db_session.no_autoflush:
sort_by = none if 'sort_by' not in parameters else parameters['sort_by'][0]  # todo check integer! </s> sort_asc = true if 'sort_asc' not in parameters else bool(int(parameters['sort_asc'][0]))	sanitize_parameters first = 1 if 'first' not in parameters else int(parameters['first'][0])  # TODO check integer! last = 50 if 'last' not in parameters else int(parameters['last'][0])  # TODO check integer! query_filter = None if 'filter' not in parameters else parameters['filter'][0] md_type = None if 'type' not in parameters else parameters['type'][0]
# todo: use widgets.dialog </s> ret = wx.messagedialog(self._editor, 'apply changes?', 'source changed',	_ask_and_apply def _ask_and_apply(self): style=wx.YES_NO | wx.ICON_QUESTION).ShowModal() if ret == wx.ID_YES:
#todo: search recursively under stage.path instead of only within </s> buildlogpath = join_path(package.stage.source_path, 'spack-build.out')	createTestOutput buildLogPath = spack.install_layout.build_log_path(spec) else: with open(buildLogPath, 'rb') as F: buildLog = F.read() #TODO: this may not return all output
# todo: replace with "yield from" when dropping python 2. </s> for stream in self._get_smil_streams(filename):	_get_streams filename = source["file"] if filename.endswith(".smil"): yield stream elif filename.startswith("/"):
# todo: empty. </s> expected = """	test_context_representation def test_context_representation(): Empty.x() Context()
# todo(brett.cannon) implement </s> pass	test_path_hooks def test_path_hooks(self):
# todo - diff </s> self.assertequal(request.status, 'p')	testReviewDetail1 self.assertEqual(request.bugs_closed, '')
# @todo: set metadata on file: org, location, disaster, date </s> s3sqlinlinecomponent("document",	customise_project_programme_resource (T("Master Budget"), "budget"), "currency", label = T("Response Plan"), fields = ["file"],
# todo: call out to the ceph cluster to check the </s> self._delete(cluster_id, pool_id)	test_create_args "pool[%s]!=%s (actually %s)" % (var, val, pool[var]))
pass  # todo(nnorwitz): impl </s> elif isinstance(node, ast.union) and node.body is not none:	_DetermineUses _AddTemplateUse('', node.bases, node.namespace) elif isinstance(node, ast.Struct) and node.body is not None: pass  # TODO(nnorwitz): impl return file_uses, decl_uses
# todo: handle situations where share is password protected </s> path = string.replace(path,'/', '\\')	list_path def list_path(self, shareName, path, password = None): path = ntpath.normpath(path) if len(path) > 0 and path[0] == '\\':
raise  # todo: what if our seed node fails verification? </s> return potential_seed_node	from_seed_and_stake_info certificate_filepath=certificate_filepath) except potential_seed_node.InvalidNode:
#todo: handle common wiki templates for type guessing </s> return ' '.join([ str(p) for p in node.params ])	read_field print(type(node)) if isinstance(node, Template): return str(node).strip(' \n')
# todo make more robust (timeout low? server returns error?) </s> await asyncio.wait_for(self.network.broadcast_transaction(funding_tx), 5)	Peer remote_sig = payload['signature'] chan.receive_new_commitment(remote_sig, []) chan.open_with_first_pcp(remote_per_commitment_point, remote_sig) return chan
# todo: verify </s> manager.content.uninstall(self.consumer_id, units)	test_content_uninstall units = [unit,]
# todo: test without file </s> def test_impl():	test_unique_str_parallel def test_unique_str_parallel(self): df = pq.read_table('example.parquet').to_pandas() return (df.two.unique() == 'foo').sum()
# todo: error handling </s> try:	get_or_create_work def get_or_create_work(olkey): work = Work.objects.get(openlibrary_key=olkey) except ObjectDoesNotExist:
# todo: look this up in one query </s> for user_id in obj['collaborator_ids']:	get_by_mbid obj['collaborator_ids'] = playlist_collaborator_ids.get(obj['id'], []) collaborators = [] user = db_user.get(user_id) if user:
# todo also check for motion codec parameter support </s> return 'h264_omx' in codecs	has_h264_omx_support if not binary: return False
# todo(phawkins): we currently set dtype=false because we aren't as </s> self._checkagainstnumpy(onp_fun, lnp_fun, args_maker, check_dtypes=false)	testQuantile onp_fun = partial(getattr(onp, op), axis=axis, keepdims=keepdims) lnp_fun = partial(getattr(lnp, op), axis=axis, keepdims=keepdims) self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
# todo: order of attributes is not assured; allow for any order. </s> match = '<part-group number="1" type="start">'	testStaffGroupsB match = '<group-symbol>bracket</group-symbol>' self.assertEqual(raw.find(match) > 0, True) self.assertEqual(raw.find(match) > 0, True) match = '<part-group number="1" type="stop"/>'
# todo(john sirois): this hacks around a direct but undeclared dependency </s> with subsystem_instance(jvm):	test_antlr source='main.py', dependencies=[antlr_target, antlr3]) with self.dumped_chroot([binary]) as (pex_builder, python_chroot): pex_builder.set_entry_point('test.main:word_up')
# todo: what about '_type'? </s> }	_build_contribution_api_data 'track': contrib.track.title if contrib.track else None, 'session': contrib.session.title if contrib.session else None, return data
#todo: check login_required? </s> if( ( trans.user == none )	display hda_dict = {} try: and ( history_id == trans.security.encode_id( trans.history.id ) ) ): history = trans.history
# todo discont: use offsets instead (note need for int conversion) </s> if (tb_ann.start == start and tb_ann.end == end	__create_span for tb_ann in ann_obj.get_textbounds(): try: and tb_ann.type == type): found = tb_ann
# todo(juice): maybe it would be ok to extend the test to validate </s> when(db_models.databasemodelbase).find_by(instance_id=any()).thenraise(	test_check_for_heartbeat_exception def test_check_for_heartbeat_exception(self): exception.ModelNotFoundError) when(agent_models.AgentHeartBeat).is_active(any()).thenReturn(None)
# todo: remove summary when bug 862603 lands. </s> 'summary__text': {'query': q, 'boost': 0.3, 'type': 'phrase'},	name_query more = { 'description__text': {'query': q, 'boost': 0.8, 'type': 'phrase'}, } analyzer = _get_locale_analyzer()
# todo: assert metrics. </s> metrics = code_analysis_server.metrics(	test_get_metrics error = repository.get_metrics(commit, metrics["spaces"]) assert not error "file.cpp", int i = 1;
# todo: this currently aligns based on phrases, not words </s> spans = [c] if isinstance(c, temporaryspan) else c.get_arguments()	get_horz_aligned_ngrams def get_horz_aligned_ngrams(c, attrib='words', n_min=1, n_max=1, lower=True): for span in spans: if span.sentence.table is None: continue
# todo: why do we have the --branch and --single-branch tags here, this causes problems </s> run('git clone --depth=1 --branch %s --single-branch %s .' % (env.git_bundles_tag, env.git_bundles_repo_url))	build run('mkdir -p %s' % src_dir_b) with cd(src_dir_b): bundles_dir = "/".join([src_dir, 'bundles']) run('rm -rf %s' % (bundles_dir.rstrip('/')))
fname = fnames[0]  # todo handle multiple notebooks </s> current_notebook = parser.parse(open(fname))	diff output = subprocess.check_output("git ls-files --modified".split()) fnames = output.splitlines() head_version_show = subprocess.Popen( ['git', 'show', 'HEAD:' + fname],
# todo: remove hardcoded http </s> protocol = 'http'	send_campaign_email_test def send_campaign_email_test(email, recipient_list): site = get_current_site(request=None) unsubscribe_path = reverse('subscribers:unsubscribe_manual', kwargs={ 'mailing_list_uuid': email.campaign.mailing_list.uuid
# todo: obtain path lock or make operation atomic in sqlite </s> if not self.state_store.is_dirty(path):	_clean_path def _clean_path(self, path): return self.state_store.set_cleaning(path)
# todo: set content_length </s> if self._content_type == 'application/x-www-form-urlencoded':	_perform_method_overloading self._stream = super(APIRequest, self).stream self._content_type = self.headers.get('Content-Type') body = self.get_data() data = url_decode_stream(io.BytesIO(body))
# @todo: build better caption rather than just using raw comments </s> caption = description = row.comments or ""	inv_timeline send_date = send_date.isoformat() recv_date = recv_date.isoformat() link = URL(args = [row.id]) eappend({"start": send_date,
# todo: use different flag than .reentrant </s> pos = colorsorter._transform_point(pos)	schedule_sphere color, pos, radius, ColorSorter._debug_transforms() ##### if ColorSorter._parent_csdl and ColorSorter._parent_csdl.reentrant: if drawing_globals.use_c_renderer and ColorSorter.sorting: if len(color) == 3:
# todo: write the wavelet transform </s> w_d = self.wavelet_transform_delta	C_d C_d = 1 Y_0 = self.wavelet s = np.expand_dims(self.scales, 1) real_sum = np.sum(W_d.real / s ** .5, axis=0)
# todo: automate this (by lookup from nn). </s> internal_states_space=impalaagent.standard_internal_states_space,	test_impala_distributed_functionality_actor_part state_space=env.state_space, action_space=env.action_space, execution_spec=dict( mode="distributed",
# todo render mock data before response, support more functions </s> params = {	_format_respose_data def _format_respose_data(self, flow): 'ip': config.get('ip'), 'port': config.get('mock.port')
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_pass_addresses_only def test_pass_addresses_only(self):
# todo: trigger via dummy audio? </s> self.playback.on_stream_changed(none)	trigger_stream_changed def trigger_stream_changed(self):
# todo: reformat or delete </s> camera.trackbodyid = 0	sawyer_torque_reacher_camera def sawyer_torque_reacher_camera(camera): camera.distance = 1.0 cam_dist = 0.3
# todo: fire_switch_state_change(self, channel_index, old_switch_state, switch_state, from_myself) </s> else:	_handle_push_notification for sensor in payload['temperature']: self._update_client_data(sensor) l.error("Unknown/Unsupported namespace/command: %s" % namespace)
# todo: rewrite tests </s> pass	test_resend_form_does_nothing_if_not_in_db def test_resend_form_does_nothing_if_not_in_db(self):
# todo: the following was copy/pasted from the histogram viewer, maybe </s> def _broadcast_end_computation(self):	_broadcast_end_computation
#@todo: remove in 0.4.10 </s> def error(self, reason=none, type="parse"):	error raise Fail("%s error%s | Plugin out of date" % (type.capitalize(), ':' + str(reason) if reason else "")) if self.core.debug:
# todo: will removed when django 1.7 will be deprecated </s> def test_reload_schema(self):	test_reload_schema f = SchemaDataBag._meta.get_field('data') original_schema = list(f.schema)
# todo: bytes vs str </s> request_line = request_line.decode()	_handle def _handle(self, reader, writer): request_line = yield from reader.readline() method, path, proto = request_line.split() headers = {}
# todo what happens with the background thread here? </s> self._reconnect(new_dc=e.new_dc)	_invoke 'attempting to reconnect at DC {}'.format(e.new_dc) ) return self._invoke(sender, call_receive, *requests) except ServerError as e:
# todo: remove when transition to python3 complete </s> return transferfunction(num, den, dt)	__truediv__ den = polymul(self.den[0][0], other.num[0][0])
body = {}  # todo: not clear what this is supposed to be </s> return self._try(self._monitors.create, body, **params)	monitors_create def monitors_create(self, **params):
# todo: enable once there's a user.avatar property returning a wrapper with avatar-style methods </s> res.append({	get_suggested_categories if any(p.isSuggestionsDisabled() for p in categ.iterParents()): continue 'score': score, 'categ': categ,
# todo this might not cover all cases </s> if 'condition' in statement and \	get_s3_bucket_secure_transport bucket_info['secure_transport'] = 'Disabled' for statement in bucket_info['policy']['Statement']: 'Bool' in statement['Condition'] and \ 'aws:SecureTransport' in statement['Condition']['Bool'] and \
# todo: for backward compatibility only, remove if not used anymore </s> def get_project_id(self):	get_project_id return get_project(key='id')
# todo do a check for the flip condition </s> if allow_flip:	add_point_on_face for others in combinations(face, len(face) - 1): self.add_simplex(others + opposing + (pt_index,)) self.flip_if_needed(others + opposing)
# todo: wait for an event instead of spinning. </s> while not (self._prev_whdr.dwflags & whdr_done):	feed raise RuntimeError("Error writing wave data: code %d" % res) if self._prev_whdr: time.sleep(0.005) res = winmm.waveOutUnprepareHeader(self._waveout, LPWAVEHDR(self._prev_whdr), sizeof(WAVEHDR))
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pcmpeqb res = 0x000000ff0000ff00ff00000000ffff00 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo: add and store preprocessing errors. </s> logging.error('unable to decode username.')	_ParseFileData username = row[0].decode('utf-8') except UnicodeDecodeError: continue try:
# todo: check if integerctype, not just ctype.arith (floats, etc.) </s> if (left.ctype.type_type == ctype.pointer and	make_nonarith_plus_code def make_nonarith_plus_code(self, left, right, il_code): right.ctype.type_type == CType.ARITH): arith_op, pointer_op = right, left
# todo: add standard commands </s> def add_handler(cmd):	create_menus def create_menus(self): toga.Group.FILE.order = 0 action = cmd.action def handler(sender, event):
oldsize = self.size # todo: remove </s> self.size = 8 + 4 + 4 + sum([atom.calsize() for atom in self.body[1]])	stsd_atom atom.write(stream) def calsize(self): assert oldsize == self.size, '%s: %d, %d' % (self.type, oldsize, self.size) # TODO: remove return self.size
# todo(seemuch): remove this contrib import </s> from tensorflow.contrib.tpu.python.tpu.datasets import streamingfilesdataset	get_dataset file_pattern = os.path.join( epoch_data_dir, rconst.SHARD_TEMPLATE.format("*")) dataset = StreamingFilesDataset( files=file_pattern, worker_job=popen_helper.worker_job(),
# todo: allow_stderr_warning is used for the --build deprecation, </s> allow_stderr_warning=true,	test_no_clean_option_blocks_cleaning_after_wheel 'simple', expect_temp=True, ) if not use_new_resolver:
# todo: this property is only used by the mvpformindicatorpillow </s> instance.initial_processing_complete = true	run unfinished_submission_stub.save() cases = case_db.get_changed() assert XFormInstance.get_db().uri == CommCareCase.get_db().uri docs = xforms + cases
# todo: stderr=_stderr_file, </s> shell=false,	start self.subproc = subprocess.Popen( cmd, ) else:  # py3
# todo(dave) implement </s> quota_enforcer = quota.quotaenforcer('my_resource')	test_should_raise def test_should_raise(self): self.project.id = None exception = self.assertRaises(
# todo: replace with copy and copy_file </s> if "application\ support" not in source_dir:	_copy_dir if len(invalid.intersection(set(source_dir.split("/")))) != 0: return command = "cp -aRp '" + source_dir + "' '" + backup_path + "/" + source_dir.split("/")[-2] + "'" elif "Sublime" in source_dir:
# todo no need for .view(1, -1) </s> score += self.end_transitions.view(1, -1)	_compute_normalizer next_score = torch.logsumexp(next_score, dim=1) score = next_score * mask[i].unsqueeze(1) + score * (1 - mask[i]).unsqueeze(1) return torch.logsumexp(score, dim=1)
else fn)  # todo: change to jscommand </s> return self	type fn.__str__ = lambda: f'type: {keys}' self.wait.command(fn if self.config.type_by_js
# todo: consider adding some better error handling for bad/failed requests. </s> except:	identify try: km.set(email, properties) if i < ANALYTICS_RETRIES - 1: time.sleep(ANALYTICS_SLEEP)
# todo: remember that we are now out of sync and try again </s> logger.exception(	_handle_device_updates return except Exception: "Failed to handle device list update for %s", user_id )
# todo test cachetag </s> iassertequal(expectation, cat4)	test_cat ['E', None, None]]
# todo: remove this ``expectedfailure`` </s> self.assertlistequal(data2, [])	test_successful_read_transaction data2 = list(Test.objects.all())
# todo(stephenfin): fix these various bugs in a follow-up </s> table = sa.table('shadow_instance_extra', meta, autoload=true)	_create_shadow_tables LOG.exception('Exception while creating table.') raise idx = sa.Index('shadow_instance_extra_idx', table.c.instance_uuid) idx.create(migrate_engine)
# todo: hack: this is papering over a bug elsewhere. </s> fullname = fullname.rstrip('.')	load_module def load_module(self, fullname): _v and LOG.debug('Importer.load_module(%r)', fullname) self._refuse_imports(fullname) event = threading.Event()
# todo check if lus can be more than one token </s> if annotations['lu'] in lines[i]:	produce_training_data lines[i].insert(1, str(i)) lines[i].append(annotations['frame']) lines[i].append('B-LU') else: lines[i].append('O')
# todo: somehow caused by circular import under python3 refactor </s> if sys.version_info > (3, 0):	stopServer def stopServer(self): if not self.wsgiserver: if gevent_present:
# todo: remove one day </s> if google_cloud_storage_conn_id:	__init__ google_cloud_storage_conn_id: Optional[str] = None ) -> None: warnings.warn( "The google_cloud_storage_conn_id parameter has been deprecated. You should pass "
# todo uncomment the actual test below after we have implemented the l1 attack </s> **self.attack_param)	test_adv_example_success_rate_l1 NotImplementedError, self.help_adv_examples_success_rate, ord=1,
raise notimplementederror # todo </s> list every item in entry_point that match request	search def search(self, entry_point, request):
# todo: handle marker? </s> if vers:	list_function_versions conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) vers = conn.list_versions_by_function(FunctionName=functionname) return vers else:
# todo: remove me when auto_now_add is enabled (post-migration) </s> user.date_registered = dt.datetime.utcnow()	test_create_unconfirmed username=email, password='foobar', fullname=name ) user.save() assert user.is_registered is False
# todo: scale and translation could be merged into a single network </s> with tf.variable_scope("scale"):	backward mask = self.get_mask(inputs, dtype=inputs.dtype) masked_inputs = inputs * mask scale = self.scale_fn(masked_inputs) with tf.variable_scope("translation"):
# todo add stuff from will's pull req </s> if error:	mempool_check_conditions_dict elif cvp.opcode is ConditionOpcode.ASSERT_BLOCK_AGE_EXCEEDS: error = mempool_assert_block_age_exceeds(cvp, unspent, mempool) return error return None
pass # todo </s> def _make_entries(self):	_make_entries
# # fixme: # todo: remove me </s> try:	unpack_url to_crawl['domain'] = url_unpack['domain'] to_crawl['domain'] = to_crawl['domain'].lower() url_host = url_unpack['host'].decode() except:
# todo: once per second, reinspect any projects marked dirty. </s> files_to_reinspect = []	run '-o', MODULE_INSPECTOR_EXE_PATH, '-outputdir', MODULE_INSPECTOR_OBJ_DIR]) with self.dirty_files_lock: files_to_reinspect = self.dirty_files
# todo: finish this. </s> def mkdir(self, path, mode):	mkdir pass
# todo: test for first revision on last page. </s> offset = url_for(controller='revision', action='list')	test_list_format_atom revision1 = revisions[0] try: res = self.app.get(offset + '?format=atom') print res
assert study_id == 0  # todo(akiba) </s> self.trials[trial_id].system_attrs[attr_name] = attr_value	set_trial_system_attr def set_trial_system_attr(self, study_id, trial_id, attr_name, attr_value):
# todo: let users specify a base mac address </s> return "00:00:ab:%02x:%02x:%02d" % (random.randint(0x00, 0xff), random.randint(0x00, 0xff), adapter_id)	_get_random_mac def _get_random_mac(self, adapter_id):
# todo: make an ascii-art bar </s> return "%.1f%%" % (100.0 * progress)	render_progress def render_progress(self, ctx, data): progress = data.get_progress()
return self.name # todo: probably we should raise an exception here </s> except exception:	_get_current_rich_text return pattern.DocumentRange.GetText(-1)
# todo: proper content negotiation </s> data = request.get_json()	validate_payload validate = func.__apidoc__.get('validate', False) if model and validate and hasattr(model, 'validate'): model.validate(data, self.refresolver) return func(*args, **kwargs)
# todo: make it really async. </s> self.facade = facade	__init__ self.server_name = server_name
# todo: break this tuplet stuff into a helper function shared for <note>, <rest>, and <chord> </s> if elem.get('m21tupletsearch') is not none:	noteFromElement if duration.convertTypeToNumber(post.duration.type) > 4: post.beams.fill(post.duration.type, elem.get('m21Beam')) post.m21TupletSearch = elem.get('m21TupletSearch') post.m21TupletNum = elem.get('m21TupletNum')
# todo: validate triplet of states </s> if not indices and not main_complex:	true_events Returns: events (tuple(actions)): List of true events in the main complex main_complex = compute.main_complex(network, current_state) elif not main_complex:
# todo: check if format matches </s> pass	_envelope_job properties['connection'] = SOLR_URL.get() if destination['isTargetExisting']: else: client = SolrClient(request.user)
# todo: make this cleaner/faster </s> output_list = p_out.split('\n')	tag_sentence print(cp_err) raise output_list_filtered = [l for l in output_list if not l.startswith('loading the models')] output_list_filtered = [l for l in output_list_filtered if not l == 'done']
# todo / fixme : to be actually implemented later .... </s> raise notimplementederror	test_permission_legacy_app_propagation_on_ssowat def test_permission_legacy_app_propagation_on_ssowat(): app_install("./tests/apps/legacy_app_ynh", args="domain=%s&path=%s" % (maindomain, "/legacy"), force=True)
# todo: check against cygwin before removing </s> else:	init_pexpect_session_environment else: shutit.fail('Should not get here: environment reached but with unique build_id that matches, but object not in existence') if self.file_exists('/cygdrive'):
#todo: does not keep case </s> ('to it', 'to them'),	test__plnounoun ('mother-in-law', 'mothers-in-law'), ('about me', 'about us'), ('from it', 'from them'), ('with it', 'with them'),
# todo: cleanup </s> def test_find_repo(self, createrepo):	TestRepo Test: Get a repo object repo = self.remote.get_repo("testrepo0") Test: find a repo object result = self.remote.find_repo({"name": "testrepo0"}, self.token)
#todo: once package/file api are merge to contentapi, replace this check with global content_search </s> if filename.endswith('.rpm'):	run else: filename, checksum = f, None pkgobj = self.service_api.search_packages(filename=filename, checksum=checksum) else:
# todo: self._line_structures is a work-around and this needs </s> self._line_structures[1] = ('logline', log_line)	_ParseCommentRecord for member in comment[7:].split(): log_line += self._LOG_LINE_STRUCTURES.get(member, self.URI)
self.assertequal(end, 1) ## todo real = 0 </s> top = output(top_recent.format(**locals()))	test_4090_simple_service_RemainAfterExit out, end = output2(cmd.format(**locals())) logg.info(" %s =>%s\n%s", cmd, end, out) logg.info("\n>>>\n%s", top) self.assertFalse(greps(top, testsleep))
# todo: refactor </s> if func_name == 'tofile':	_run_call_array self._array_counts[lhs][0] = self._array_counts[arr.name][0] self._array_sizes[lhs][0] = self._array_sizes[arr.name][0] if self._is_1D_arr(arr.name): _fname = args[0]
# todo make fetch_result _not_ a pd.dataframe </s> fetch_result = processresult(pd.dataframe({'a': ['nums', '3']}))	test_first_row_is_header_autocast_dtypes def test_first_row_is_header_autocast_dtypes(self): result = scrapetable.render(pd.DataFrame(), P(first_row_is_header=True),
# todo extend to inputs with shape (n_samples, 1) </s> cover = onedimensionalcover(n_intervals=1)	test_filter_values_covered_by_single_interval ) def test_filter_values_covered_by_single_interval(filter_values): interval_masks = cover.fit_transform(filter_values) assert_almost_equal(
#todo: check the data! </s> self.asserttrue(count > 0)	test_fetchpage_loop for i in p: count += 1
# todo: support ddof </s> def _impl(df, axis=none, skipna=none, level=none, ddof=1, numeric_only=none):	var_overload @overload_method(DataFrameType, 'var') def var_overload(df, axis=None, skipna=None, level=None, ddof=1, numeric_only=None): return hpat.hiframes.pd_dataframe_ext.var_dummy(df) return _impl
prng = randomstate() # todo: seed it </s> r_jump, r_op = prng.rand(2)	_smepdpsolve_single_trajectory raise NotImplemented("SME PDP solver not yet completed") phi_t = np.copy(psi_t) jump_times = [] jump_op_idx = []
# todo(b/178173737): use tf.math.segment_sum once a fast version is </s> float_pv = tf.linalg.matvec(	loss_function tf.math.exp(-fixed_discount_rates * fixed_leg_calc_times + fixed_settle_rates * settle_times_fixed)) calc_groups_float, float_cashflows * calc_discounts_float_leg)
"""@todo: docs. contribution is welcome.""" </s> self.logger.close()	__del__ def __del__(self):
#todo: unit tests </s> user = auth.user	get_all_registrations_smart_folder @must_be_logged_in def get_all_registrations_smart_folder(auth, **kwargs): contributed = user.node__contributed nodes = contributed.find(
# todo: will need some code for the tabs active terms to work </s> self.get_toplevel().last_active_term = none	on_vte_focus_in self.terminator.last_focused_term = self if self.get_toplevel().is_child_notebook(): else: self.get_toplevel().last_active_term = self.uuid
# todo: use other libraries. </s> def addr(func_name):	Addr if func_name == '/bin/sh': addr = get_bin_sh_str()
# xxx todo: read incrementally to reduce memory usage. </s> data = fileobj.read()	cromulate def cromulate(fileobj, opts): pre_code, data = data.split(MARKER_START_FUNCS, 1) data, post_code = data.split(MARKER_END_FUNCS, 1)
#todo fixme: we need to check that we aren't adding a duplicate </s> item.addclaim(claim)	main pywikibot.output('Adding %s --> %s' % (claim.getID(), claim.getTarget().getID()))
# todo(crcrpar): make this works </s> cls.__doc__ += """	experimental_class cls.__name__, version), category=UserWarning) .. note:: Added in version {} as experimental feature. The interface can change in the future.
# todo: fails because of missing svg support </s> assert_pixels('inline_image_' + filename, 8, 8, image, '''	test_images )) def test_images(filename, image): <style> @page { size: 8px }
# todo: the assert below fails. </s> assert predictval is not none	test_model_output predictval = output_instance.detach().numpy()[0][0]
# todo: remove in version 3.10 </s> with warnings.catch_warnings(record=true):	test_can_read_list_permissions request = factory.get('/', HTTP_AUTHORIZATION=self.credentials['readonly']) object_permissions_list_view.cls.filter_backends = (DjangoObjectPermissionsFilter,) warnings.simplefilter("always") response = object_permissions_list_view(request)
# todo(b/142684737): the multi-processing api might change. </s> beam_pipeline_args=['--direct_num_workers=%d' % direct_num_workers],	_create_pipeline trainer, model_analyzer, model_validator, pusher ],
# todo verify </s> form = {'ajax': '1', 'pn': 'p1', 'htv': 'm'}	top30in30 def top30in30(self): req_url = "http://www.google.com/trends/hottrends/hotItems" req = self.ses.post(req_url, data=form)
# todo: autosummon option to a specific channel </s> del stack	_get_variable finally:
# todo: make sure secret values are masked </s> return super(packconfigscontroller, self)._get_all(**kwargs)	get_all Handles requests: GET /configs/
# todo: use triple factory </s> rotate.forward_owa(torch.zeros(16, 3, dtype=torch.long))	test_rotate rotate = RotatE(triples_factory=self.factory) self.assertIsNotNone(rotate) rotate.forward_cwa(torch.zeros(16, 2, dtype=torch.long)) rotate.forward_inverse_cwa(torch.zeros(16, 2, dtype=torch.long))
# todo: make test method </s> xmpp	test_xmpp except: return False return True
# todo: cleaning of facts should eventually become part of taskresults instead of vars </s> vars_copy.update(namespace_facts(result['ansible_facts']))	_execute vars_copy.update(result['ansible_facts']) else: if C.INJECT_FACTS_AS_VARS: vars_copy.update(clean_facts(result['ansible_facts']))
# todo: remove temporary workaround once https://github.com/python-babel/babel/issues/415 has been resolved. </s> babel_415_workaround = program_update_levels[self._update_level]['title'] if self._update_level in program_update_levels else 'unknown'	_display_results else: if self._show_always: QMessageBox.information( self._parent,
if not config.testnet:  # todo </s> return	compose def compose (db, source, contract_id, gasprice, startgas, value, payload_hex): block = blocks.Block(db, util.last_block(db)['block_hash']) code = block.get_code(contract_id)
# todo: implement it </s> if self.form.validate():	PasswordResetCompleteHandler return self.render_template('password_reset_complete.html', **params) def post(self, token): token = User.token_model.query(User.token_model.token == token).get() user = User.get_by_id(int(token.user))
# todo is a division by moving avg factor needed for variance? </s> a = scale / np.sqrt(epsilon + variance)	batchnorm_to_affine variance = tg.get_initializer(new_model, n.input[4]) epsilon = 1e-5 B = bias - (A * mean) nodes_to_remove += [n]
#todo: implement xml support </s> return "whatever, we don't have xml yet"	create_token return '{"token": "abcdefghijklmnopqrstuvwxyz"}' elif accept_header == 'application/xml': else: return '{"token": "abcdefghijklmnopqrstuvwxyz"}'
#todo(qos): support all the optional parameters </s> return [rule_obj.to_dict() for rule_obj in	get_policy_bandwidth_limit_rules sorts=None, limit=None, marker=None, page_reverse=False): rule_object.QosBandwidthLimitRule.get_objects(context)]
# todo: should use ".handle_quick_operation" action in the future </s> try:	RequestSession pass async def reject(self, reason: str = ''): if self.ctx['request_type'] == 'friend': await self.bot.set_friend_add_request(**self.ctx,
#todo: check if/where this is used; if not used externally - remove </s> return self.marker_detector.marker_min_confidence	Surface_Tracker @property def marker_min_confidence(self) -> float: @marker_min_confidence.setter def marker_min_confidence(self, value: float):
# todo: accept only exported keys </s> return getattr(self, 'get_' + key)()	state_get def state_get(self, key):
# todo: check the data! </s> self.asserttrue(len(list(pipe)) > 0)	test_split pipe_def = self._get_pipe_def(pipe_file) pipe = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def)
# todo: raise an invalidparameters instead and stop using cli_ui in gitlabform.gitlab </s> cli_ui.fatal(	remove_share_from_group ) except NotFoundException: f"Group {share_with_group_name} not found.", exit_code=EXIT_INVALID_INPUT,
# todo: move to "worklockeconomics" class #1126 </s> now = testerchain.w3.eth.getblock(block_identifier='latest').timestamp	deploy_worklock @pytest.fixture(scope="module", autouse=True) def deploy_worklock(testerchain, agency, test_registry, token_economics): start_bid_date = now + (60 * 60)  # 1 Hour end_bid_date = start_bid_date + (60 * 60)
#todo: how to handle language change? clear and populate again? </s> month = datetime.date(2000, i, 1).strftime('%b')	_initialize self._months_nums = dict() for i in xrange(1, 13): self.add_to_store(self._monthsStore, month) self._months_nums[month] = i
# todo hack! include image digest, needed for the downstream notifications handler </s> if 'image_digest' not in last_evaluation_result:	perform_policy_evaluation last_evaluation_result = obj_store.get_document(userId, 'policy_evaluations', last_evaluation_record['evalId']) last_final_action = last_evaluation_result['final_action'].upper() last_evaluation_result['image_digest'] = imageDigest except:
# todo: fix with stubber / before send event </s> with mock.patch('botocore.endpoint.endpoint._send') as _send:	test_get_export 'accepts': 'application/yaml' } _send.return_value = mock.Mock( status_code=200, headers={}, content=b'{}')
# todo: remove when bug 862603 lands. </s> 'summary': {'type': 'string', 'analyzer': 'snowball'},	get_mapping }, 'status': {'type': 'byte'}, 'support_email': {'type': 'string', 'index': 'not_analyzed'},
'i': ('i', [{'j': 'j'}]),  # todo: support true for cases when the value should simply be mapped into the field name? </s> 'n': ('n', lambda n: n.upper())})  # d.e[0] or d.e: (callable to fetch 0)	_main ret = glom(val, {'a': 'a.b', 'e': 'd.e', print('in: ', val) print('got:', ret)
# todo aggregation only supports # of docs matching </s> counter = 0	_get_query_results for report_column in self.top_level_db_columns: if report_column.type == 'expanded': for sub_col in get_expanded_column_config(self.config, report_column, 'en').columns: ui_col = report_column.column_id + "-" + str(counter)
# todo: import refactor - figure out which group this needs :( </s> if isinstance(identity, e_group):	getGroup def getGroup(self, identity, *args, **kwargs): return identity elif isinstance(identity, (int, float, basestring)):
# todo: fix this issue </s> self.assertequal(str(np.amin(samples)), '-0.1')	test_multichannel augmenter = Compose([Clip(a_min=-0.1, a_max=0.1, p=1.0)]) samples = augmenter(samples=samples, sample_rate=sample_rate) self.assertEqual(str(np.amax(samples)), '0.1') self.assertEqual(samples.dtype, np.float32)
# @todo: move this to link table? </s> self.pr_person_id(label=t("contact"),	S3ContentModel label=T("Body")), self.gis_location_id(), readable = False, writable = False,
# todo: check syntax, values? </s> pass	warning @GenericHeaderSyntax def warning(self, name, values):
# todo: if self.outputs["edge indices"].islinked: edgeindices = loft.calcedgeindices() </s> if self.outputs["polygon indices"].islinked: polygonindices = loft.calcpolygonindices()	evaluateLoft if valid: if self.outputs["Vertices"].isLinked: vertices = loft.calcVertices() if vertices is None: vertices = Vector3DList() if edgeIndices is None: edgeIndices = EdgeIndicesList()
# # todo: delete when 0.4.0 is available </s> self._x_position = 0	_will_reset self._time_left = 0
# todo: header fields might vary across file types, thus prior sensing would be needed </s> header_fields = file_headers[0].keys()	main file_headers = [nib.load(f).header for f in files] if opts.header_fields == 'all': else: header_fields = opts.header_fields.split(',')
# todo: default to 'next' when redoc 2.0.0 is released. </s> redoc_version = self._app.config.get(	_openapi_redoc redoc_url = self._app.config.get('OPENAPI_REDOC_URL', None) if redoc_url is None: 'OPENAPI_REDOC_VERSION', 'latest') if redoc_version == 'latest' or redoc_version.startswith('v1'):
# todo -- parallelize this </s> file_generator = getattr(protocol, subset)()	fun preprocessors=self.preprocessors_) metric = DetectionErrorRate() for current_file in file_generator: uri = get_unique_identifier(current_file)
# todo implement </s> ret = 1	hook_GetStringTypeW }) def hook_GetStringTypeW(ql, address, params): return ret
pass  # todo </s> def create(self, name):	create
# todo: in the future we'll probably need to keep a request history </s> node.data_requests.delete(id=_req.id)	accept_or_deny_request ] = _req.id node.store[UID.from_string(_req.object_id)] = tmp_obj else: _req_owner = current_user.verify_key == _req.verify_key
# todo : an "invalid campaign popup" </s> self.__show_invalid_scenario_file_popup(e)	_update_infos campaign_info = SavegameManager.get_campaign_info(file = self.__get_selected_map()) if not campaign_info: return self.current.findChild(name="map_difficulty").text = _("Difficulty: ") + unicode(campaign_info.get('difficulty', ''))
# todo: review why this is now unused </s> evt1 = spiderfootevent("leaksite_url", link, self.__name__, event)	handleEvent + r"[^a-zA-Z\-\_0-9]", res['content'], re.IGNORECASE) is None: continue self.notifyListeners(evt1) evt2 = SpiderFootEvent("LEAKSITE_CONTENT", res['content'], self.__name__, evt1)
# todo type_min/type_max </s> for input_data in [[np.nan, 2., np.nan, 3., np.inf, 1, -1000],	test_series_max return S.max() hpat_func = hpat.jit(test_impl) [8, 31, 1123, -1024], [2., 3., 1, -1000, np.inf]]:
# todo(jakevdp): remove when minimum jaxlib is has extension version 4 </s> if self._thread_local_state.enable_x64 is none:	x64_enabled return lib.jax_jit.get_enable_x64() else: self._thread_local_state.enable_x64 = bool(self.read('jax_enable_x64')) return self._thread_local_state.enable_x64
# todo(b/141131288): enable test once complex sort is supported on tpu. </s> if (jnp.issubdtype(dtype, jnp.complexfloating)	testLexsort for axis in (-1, *range(len(shape) - 1)))) def testLexsort(self, dtype, shape, input_type, axis): and jtu.device_under_test() == "tpu"): self.skipTest("complex sort not supported on TPU")
# todo: add modified date </s> self._storage.store({	function if not details: file_path, file_hash = self._download_and_hash(urls) 'file_hash': file_hash, 'file_path': file_path,
# todo: extract real data length: </s> self._init_handler(type_nxt, buf[off:])	IP6 off += length self.opts.extend(opts) return off def direction(self, other):
except exception as err:  # todo: what exception?! (socket error; authentication error; ...?) </s> raise jsonapierror(str(err))	post try: response = requests.post(self.baseurl + path, data, header=self.header, auth=self.auth, **kwargs) return JsonApiRequest.json_or_error(response)
#todo: unit tests </s> user = auth.user	get_all_registrations_smart_folder @must_be_logged_in def get_all_registrations_smart_folder(auth, **kwargs): contributed = user.node__contributed nodes = contributed.find(
# todo: obviously incrementing the rows individually is bad. how </s> for i, word_piece_slice in enumerate(wp_indices):	get_aligned_tensor tensor = xp.zeros((len(doc), wp_tensor.shape[-1]), dtype="f") wp_weighted = wp_tensor / xp.array(align_sizes, dtype="f").reshape((-1, 1)) tensor[i] = wp_weighted[word_piece_slice,].sum(0) return tensor
# todo: add doc string </s> def __init__(self, incoming_graph_data=none, **attr):	StellarGraphBase class StellarGraphBase: super().__init__(incoming_graph_data, **attr) self._node_type_attr = attr.get("node_type_name", GLOBALS.TYPE_ATTR_NAME)
# todo: add test and check this more throughroughly. </s> if hasattr(layer, "kernel") or hasattr(layer, "depthwise_kernel") or hasattr(layer, "pointwise_kernel"):	contains_kernel def contains_kernel(layer): Check whether the layer contains a kernel. return True else:
# todo: make this test real </s> self.assertequals(locator.get_relative_template_location(view), ('', 'foo.txt'))	test_get_relative_template_location__template_path__file_name view.template_path = 'foo.txt'
# todo: wer計算するときに消していい？ </s> wer_mean += compute_wer(ref=str_true.split('_'),	do_eval_wer str_true = re.sub(r'[\'<>]+', '', str_true) str_pred = re.sub(r'[\'<>]+', '', str_pred) hyp=str_pred.split('_'), normalize=True)
# todo: combine with the 'canonicalization' that is part of the gemm optimizer. </s> f(sv,sv,sv)	cmp f = theano.function([a,b,c],0.2*c *a*T.dot(a,b),mode=mode_blas_opt) topo = f.maker.env.toposort() f = theano.function([a,b,c],c * a*0.2*T.dot(a,b),mode=m2) topo = f.maker.env.toposort()
# todo return empty list if not loaded </s> spotify.error.maybe_raise(self.error)	tracks def tracks(self): Will always return :class:`None` if the search isn't loaded. if not self.is_loaded: return None
# todo debug </s> print logstring	logRule + "maxTimeAfterPrev=%.2f)" % ruleElement.maxTimeAfterPrev) logging.info("[%s]: %s" % (fileName, logString)) spaceString = "" for i in range(spaces):
# todo: update with misconfigurationexception when auto mode is removed in v1.3 </s> if mode == 'auto':	__init_monitor_mode f"`mode` can be auto, {', '.join(mode_dict.keys())}, got {mode}" ) rank_zero_warn( "mode='auto' is deprecated in v1.1 and will be removed in v1.3."
# todo: rewrite this to use dict in all code and not different independent vars </s> boot_part_size = part_sizes['boot']	run return part_sizes = get_part_sizes(disc_size, empty_space_size, gpt_bios_grup_part_size, uefisys_part_size) lvm_pv_part_size = part_sizes['lvm_pv'] swap_part_size = part_sizes['swap']
# todo: collations are not supported, but the default ones needed </s> match = next(filter_.itertext()).lower()	_text_match def _text_match(vobject_item, filter_, child_name, attrib_name=None): See rfc4791-9.7.5. children = getattr(vobject_item, "%s_list" % child_name, []) if attrib_name:
raise exception  # todo (key not found in columns) </s> else:	__init__ break if not key_matched: for k in key: if isinstance(columns[0], basestring):
# todo: use fsevents' sincewhen parameter instead of the current </s> if self.persistent:	__process_queues else: self.monitored_paths[path].monitoring = True FSMonitor.generate_missed_events(self, path)
# todo: if not default behavior: have to specify in decorator (see design_problems.txt). </s> state_value._batch_rank = 0 if self.input_space.time_major is false else 1	_graph_fn_get_state_values_and_logits flat_logits._time_rank = 0 if self.input_space.time_major is True else 1 logits = self.call(self.reshape.apply, flat_logits) logits._batch_rank = 0 if self.input_space.time_major is False else 1 if self.input_space.has_time_rank:
# todo(mattrobenolt): remove servicedelegator check </s> settings.sentry_tsdb in (	validate_snuba settings.SENTRY_SEARCH == 'sentry.search.snuba.SnubaSearchBackend' or settings.SENTRY_TAGSTORE == 'sentry.tagstore.snuba.SnubaCompatibilityTagStorage' or 'sentry.tsdb.redissnuba.RedisSnubaTSDB', 'sentry.utils.services.ServiceDelegator',
# todo: remove the false when enabling the crawler. </s> if input_use_url_crawler and false:	main urls = [input_uri] vulnerable_urls = [] Logging.info("Started crawler...") urls = Crawler.get_instance().get_urls(input_uri)
#todo: we may want to deal with error nicely </s> logging.debug('api timeout error : ' + api_url)	voip_rates response = requests.get(api_url, auth=(request.user, request.user), timeout=1.0) except requests.exceptions.Timeout: if response and response.status_code == 200: rate_list = response.content
raise notimplementederror  # todo </s> generator = none	compute def compute(self, X, batch_size=32, verbose=0): return self.compute_generator(generator, verbose=verbose)
# todo: skips header parsing </s> params_map = get_param_map(word)	read_assembly set_ids, iline, line0 = read_set(lines, iline, line0, params_map) elif word.startswith('elset'): name = params_map['elset'] iline += 1
# todo: arrange </s> repo = self.remote.new_repo(self.token)	test_create_repo def test_create_repo(self): Test: create/edit a repo object self.assertTrue(self.remote.modify_repo(repo, "name", "testrepo0", self.token)) self.assertTrue(self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token))
#todo basket column. </s> t = {"continuousvariable": "c", "discretevariable": "d",	save_tab_delimited f.write("\t".join([str(j.name) for j in domain_vars])) f.write("\n") "StringVariable": "string", "Basket": "basket"} f.write("\t".join([t[type(j).__name__] for j in domain_vars]))
# todo: fix in detectors. </s> return	render_pupil_3d self.render_ellipse(image, el, color=(0, 0, 255, conf)) if pupil_position["model_confidence"] <= 0.0: eye_ball = pupil_position.get("projected_sphere", None) if eye_ball is not None:
# todo: this can be formulated more efficiently </s> sqrt_ggn = einsum('boc->cbo', (sqrt_ggn_out, )).contiguous()	weight_diag_ggn num_classes = sqrt_ggn_out.size(2) assert tuple(sqrt_ggn_out.size())[:2] == (batch, out_features) sqrt_ggn = sqrt_ggn.view(num_classes * batch, out_features) sqrt_ggn = sqrt_ggn.view(num_classes * batch, out_channels, out_x * out_y)
# todo(nzw0301): remove the upper version constraint when the callback supports </s> "pytorch-lightning>=1.0.2,<1.5.0",	get_extras_require "tensorflow-datasets", "pytorch-ignite", "skorch", "catalyst>=21.3",
# todo generator </s> handle_dirty_dataset(ds, mode=if_dirty)	__call__ for ds_path in content_by_ds: ds = Dataset(ds_path) content = [ap['path'] for ap in content_by_ds[ds_path] if ap.get('type', None) != 'dataset' or ap['path'] == ds.path]
# todo: systemhistory_user_id </s> )	save systemhistory_old_value = self.previous_systemstatus.systemstatus_name, systemhistory_new_value = self.systemstatus.systemstatus_name, systemhistory.save() self.previous_systemstatus = self.systemstatus
# todo check more types here </s> raise notimplementederror	out_as_str return str(self.holder) else:
# todo: varref should store a token instead of a string! </s> e_die('divide by zero (name)')	ArithEvaluator error_expr = node.right  # node is Binary if error_expr.tag == arith_expr_e.VarRef: elif error_expr.tag == arith_expr_e.ArithWord: e_die('Divide by zero', word=node.right.w)
# todo: askr, undocumented! </s> def ledsetlayout( self, mode ):	LaunchpadLPX self.midi.RawWriteSysEx( [ 0, 32, 41, 2, 12, 0, mode ] ) time.wait(10)
# todo: this is a hack to make a rule know </s> if slot.value == "none" and slot.as_feature():	get_parsing_states for key, slot in tracker.slots.items(): if slot is not None: slot_id = f"slot_{key}_None" state_dict[slot_id] = 1
# todo: remove one day </s> def get_client(self) -> container_v1.clustermanagerclient:  # pylint: disable=missing-docstring	get_client warnings.warn("The get_client method has been deprecated. " "You should use the get_conn method.", DeprecationWarning)
# todo: fix within gitpython or build a fully functional </s> out = self.repo.git.push(rm.name, refspec, porcelain=true,	push with rm.repo.git.custom_environment( GIT_SSH_COMMAND="datalad sshrun"): universal_newlines=True, **kwargs) for line in out.splitlines():
# todo: fails because of missing svg support </s> assert_pixels('inline_image_' + filename, 8, 8, image, '''	test_images )) def test_images(filename, image): <style> @page { size: 8px }
# todo: take into account /ipfs/(hash), first check if this is correct fmt </s> if '://' not in path:  # isaipfshash	fetchRemoteCDXJFile fileContents = '' path = path.replace('ipfs://', '') print 'No scheme in path, assuming IPFS hash and fetching...' dataFromIPFS = IPFS_API.cat(path)
#todo has not (clc) </s> .has('.bank 1')	test_move_sprite_minus_five_on_y_with_eight_tiles .and_then('STA $0217') .and_then('STA $021F') .and_then('mario:')
# todo move this to augmenters.size </s> def compute_croppings_to_reach_multiples_of(arr, height_multiple,	compute_croppings_to_reach_multiples_of width_multiple): See :func:`imgaug.imgaug.compute_paddings_for_aspect_ratio` for an
#todo publish don't write </s> yield log.msg("submit sm resp %s" % repr(kwargs))	submit_sm_resp sent_sms.transport_msg_id = transport_msg_id sent_sms.save()
# todo: remove when new appium is out </s> return self	hide_keyboard self.execute(Command.HIDE_KEYBOARD, data)
# todo: note that this won't work for nans </s> dem_mask = np.where(dem.ravel() == nodata_in)[0]	raise_nondraining_flats dem = self._input_handler(data, apply_mask=apply_mask, properties=grid_props, ignore_metadata=ignore_metadata, metadata=metadata, **kwargs) a = np.arange(dem.size) top = np.arange(dem.shape[1])[1:-1]
#self.resetacount() #@todo implement </s> else:	handlePremium if self.api_data["size"] / 1024 > info["trafficleft"]: self.log.info(_("%s: Not enough traffic left" % self.__name__)) url = self.api_data["mirror"] self.download(url, get={"directstart":1}, cookies=True)
# todo: i should make sure to escape single quotes here </s> self._cursor.execute("""	put if safe and self.has_key(key): raise ValueError("Already have key %s" % key) INSERT INTO %s SELECT '%s', %s
# todo: finish this... </s> self.check_page_for_string ( check_str )	add_library_info_template check_str = "Create a new information template for library '%s'" % library_name
# todo: remove need for --no-strict-optional </s> run_mypy(['--no-strict-optional'] + modules)	test_stdlibsamples modules.append(f) if modules:
# todo: no testpath exercises this code... </s> log.debug("starting thread...")	AnimThread self.do_run = do_run def run(self): self.do_run() log.debug("Thread Complete")
# todo: the logic here for ion concentration setting is in two </s> if len(ion_elts) == 1:	PourbaixDiagram for entry in ion_entries: ion_elts = list(set(entry.composition.elements) - elements_HO) entry.concentration = conc_dict[ion_elts[0].symbol] \ * entry.normalization_factor
# todo candidate for move to system/osi as not btrfs related </s> smap = {	convert_to_KiB def convert_to_KiB(size): 'KiB': 1, 'MiB': 1024,
# todo: ... </s> pass	test_retry def test_retry(self):
# todo(danms) once libvirt has support for lxc hotplug, </s> domxml = virt_dom.xmldesc(libvirt.vir_domain_xml_secure)	attach_volume raise exception.DeviceIsBusy(device=mount_device) raise self._conn.defineXML(domxml)
help='') # todo </s> parser.add_argument(	main '--sop', action='store_true', '--pos', action='store_true',
# todo(b/197746608): finds a safer way of reconstructing the metric, </s> keras_metric = none	finalize_metric def finalize_metric(metric: tf.keras.metrics.Metric, values): try: keras_metric = type(metric).from_config(metric.get_config())
# todo: implement </s> return patches	blast_radius_upgrade :rtype: list patches = []
# todo(elliot): what info do we need for this recipe type? </s> pass	handle_app_input pass if recipes[i]["name"] == "ds":
# todo: figure out how to best show this kind of warning to the </s> return self.to_bytes(obj.__name__)	_to_bytes return self._code_to_bytes(obj, context) elif inspect.ismodule(obj): elif inspect.isclass(obj): st.warning(('Streamlit does not support hashing classes. '
# todo: another solution should be used here. this is a hack for compatibility reasons. to resolve the gadget address calculation of segments of elf files have a different base address if calculated segment.virtualaddress - segment.offset </s> offset = section.offset - (binary.originalimagebase - (section.virtualaddress - section.offset))	_searchPopPopRet disassembler = self.__getCs(binary.arch) code = section.bytes toReturn = [] pprs = binary.arch.pprs
# todo: take care of theano to keras port: </s> ))	vgg16 net["in"], 2, "conv_1", 64, activation=activation, net.update(base.conv_pool( net["conv_1_pool"], 2, "conv_2", 128,
# todo: this should be solved via plugins </s> migrate_foreignkey(self.app_label, self.model, 'instruction_instruction', self.model, self.orm)	move_self_foreignkeys migrate_foreignkey(self.app_label, self.model, 'media_usage', self.model, self.orm) if 'jaknato.instruction' in settings.INSTALLED_APPS:
# todo dm: once we do distributed launching, this needs to be done per node not per cluster </s> telemetry.indexsize(self.cfg, self.metrics_store)	start telemetry.NodeStats(self.cfg, es, self.metrics_store), telemetry.IndexStats(self.cfg, es, self.metrics_store), ] t = telemetry.Telemetry(self.cfg, es, self.metrics_store, devices=cluster_telemetry)
# todo: return errors in a universal way </s> print("shivyc: error: cannot write output file '{}'"	main s_file.write(s_source) except IOError: .format("out.s")) s_file.close()
# todo: implement this here </s> raise notimplementederror	confusion def confusion(self, X, Y, **kwargs):
# todo: test multi-line lambdas </s> self.arcs.add((start, -start))	add_arcs_for_code_objects self.arcs.add((-1, start))
# todo: find a way to invalidate the system df cache. </s> drop_cols = ['timestamp_y']	get_valid_df return table_df if table != 'system': if self.system_df.empty: return self.system_df
# todo(yanase): which dtype should we use? float or float32? </s> return float(np.nanmedian(np.array([	get_median_intermediate_result_over_trials def get_median_intermediate_result_over_trials(self, study_id, step): all_trials = self.get_all_trials(study_id) t.intermediate_values[step] for t in all_trials if step in t.intermediate_values
# todo: copy doesn't really work as expected, i.e., the reference </s> high_hdlr = copy(self.high_hdlr) if self.high_hdlr else handlers.stderr_hdlr()	get_logger if not lggr_name in self.loggers: self.loggers.add(lggr_name) low_hdlr = copy(self.low_hdlr) if self.low_hdlr else handlers.stdout_hdlr() self.update_hdlr(high_hdlr, self.high_level, formatter=self.high_formatter)
if posix and not sunos:  # todo: sunos </s> name1 = unix_socket_path().__enter__()	test_connections socks.append(create_socket(socket.AF_INET6, socket.SOCK_STREAM)) socks.append(create_socket(socket.AF_INET6, socket.SOCK_DGRAM)) name2 = unix_socket_path().__enter__() s1, s2 = unix_socketpair(name1)
# todo: write units tests </s> self._check_signal_dimension_equals_one()	estimate_thickness For details see: Egerton, R. Electron Energy-Loss Spectroscopy in the Electron Microscope. Springer-Verlag, 2011. axis = self.axes_manager.signal_axes[0] total_intensity = self.integrate_simpson(axis.index_in_array).data
# todo(@awav): check it </s> def test_logjac(self):	TransformTests for _t, x, _y in zip(transforms, xs_np, ys_np): assert_allclose(x.reshape(x_np.shape), x_np) We have hand-crafted the log-jacobians for speed. Check they're correct wrt a tensorflow derived version
# todo: for some reason on osx a unix socket cannot be </s> if osx:	bind_unix_socket def bind_unix_socket(type=socket.SOCK_STREAM, suffix="", mode=0o600): Return a (sock, filemame) tuple. file = tempfile.mktemp(prefix=TESTFILE_PREFIX, suffix=suffix) else:
# todo: catch and report error if possible </s> self._client._loop.create_task(self._client.send(	_onRequestIntercepted if (not self._userRequestInterceptionEnabled and self._protocolRequestInterceptionEnabled): 'Network.continueInterceptedRequest', { 'interceptionId': event['interceptionId'],
#todo resend message + throttling </s> generated_token = totp(token.seed)	verify_computer token = user.token if token.method in ('call', 'sms'): if token.method == 'call': call(to=token.phone,
# todo: unittest </s> assert(len(roi_specs) == 1)	_proc_block debug('SLC_', 'For %r query returned ids %r' % (f, roi_fids)) if is_datasetlike(roi_specs): roi_fids = roi_specs.samples[0] else:
print('warning: exception during driver init, {}'.format(ep.name))  # todo: use proper logger </s> return none	safe_load driver = driver_init() except: if driver is None: print('WARNING: driver init returned None, {}'.format(ep.name))  # TODO: use proper logger
# todo change to check for error when the functionality changes. currently acts as though it doesn't exist </s> url = "/api/v2/nodes/?filter[notafield]=bogus"	test_incorrect_filtering_field def test_incorrect_filtering_field(self): res = self.app.get(url) node_json = res.json['data']
# todo: raise an error if finaloutputslot has len=0.  that means the user didn't load a batch dataset into the project. </s> opclusterizemaster.input.connect( finaloutputslot )	prepare_master_cluster_operator def prepare_master_cluster_operator(cluster_args, finalOutputSlot, secondaryOutputSlots, secondaryOutputDescriptions): opClusterizeMaster = OperatorWrapper( OpClusterize, parent=finalOutputSlot.getRealOperator().parent ) opClusterizeMaster.ProjectFilePath.setValue( cluster_args.project ) opClusterizeMaster.OutputDatasetDescription.setValue( cluster_args.output_description_file )
# todo check the op returned a view </s> if dmap and idx in dmap:	summary_memory vmap = getattr(node.op, 'view_map', None) for idx, v in enumerate(val): node_memory_saved_by_inplace += v elif vmap and idx in vmap:
# todo (@awaelchli): standardize this across all plugins in lightning and lite. related refactor: #7324 </s> models = [self._setup_model(model) for model in models]	_setup_models_and_optimizers The returned objects are expected to be in the same order they were passed in. The default implementation will call :meth:`_setup_model` and :meth:`_setup_optimizer` on the input lists. optimizers = [self._setup_optimizer(optimizer) for optimizer in optimizers] return models, optimizers
#@todo: move to utils in 0.4.10 </s> def timestamp():	timestamp return int(time.time() * 1000)
# todo straya </s> async def retry_sending_after_reorg(self, records: list[transactionrecord]):	WalletStateManager Rolls back and updates the coin_store and transaction store. print("Doing reorg...") Retries sending spend_bundle to the Full_Node, after confirmed tx get's excluded from chain because of the reorg.
# todo: this test actually isn't very useful right now, but it will make sense </s> self.assertequal(optional[int], union[int, none])	test_optional def test_optional(self):
# todo(tonyg/slamm): is this assertion correct? </s> self.assertequalwithintolerance(	testHttpFetch data = urllib.urlopen(TEST_URL).read() self.assertEqual('\x00' * num_bytes, data) 1300, self.interval_timer.get_interval('end'))
# todo: add mode, state </s> self._fillpaths.append((path, color))	PathIconEngine self._size = size def addFillPath(self, path, color=Qt.black): def addStrokePath(self, path, color=Qt.black, width=.9, antialiasing=False):
# todo: content-type is hard-coded but ideally should be retrieved; </s> route_doc = """	api_doc_gen for route in sorted(routes, key=lambda a: a[0]): url, rh = _get_tuple_from_route(route) Content-Type: application/json {1}
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	get_spice_console def get_spice_console(self, instance):
# todo create correct test data </s> channel.unlock(lock1, locksroot, 'x' * 32)	test_settle ]) lock1 = str(LOCK.as_bytes) HASHLOCK4 = sha3('y' * 32) LOCK_AMOUNT4 = 23
# todo(stevemar): assert returned fields </s> + ' ' + self.object_name)	test_object_save self.openstack('object save ' + self.CONTAINER_NAME
# todo(yuriyz): change to 404 (bug 1200517) </s> self.assertequal(response.status_int, 500)	test_update_not_found response = self.patch_json('/chassis/%s' % uuid, {'extra': {'a': 'b'}}, expect_errors=True) self.assertEqual(response.content_type, 'application/json') self.assertTrue(response.json['error_message'])
# todo: remove this - cura-4482 </s> material = self._global_container_stack.material	_onGlobalContainerChanged except TypeError: pass material.nameChanged.disconnect(self._onMaterialNameChanged) quality = self._global_container_stack.quality
# todo: update once calculation_magic is implemented </s> assert store.get_lpq_projects() == {17}	test_is_eligible_not_lpq _update_lpq_eligibility(project_id=17, cutoff=10)
# rbarlow_todo: convert this callrequest into a celery task call </s> call_request = callrequest(manager.install_content, args, kwargs, weight=weight, tags=tags, archive=true, asynchronous=true)	consumer_content_install_itinerary tags = [resource_tag(dispatch_constants.RESOURCE_CONSUMER_TYPE, consumer_id), action_tag('unit_install')] call_request.add_control_hook(dispatch_constants.CALL_CANCEL_CONTROL_HOOK, cancel_agent_request) call_request.reads_resource(dispatch_constants.RESOURCE_CONSUMER_TYPE, consumer_id)
# todo: assume the fixup size is four bytes, probably bad. </s> fixupva += 4	get_basic_block_rule fixupva = idaapi.get_next_fixup_ea(va) fixups.append(fixupva) while fixupva < va + size: fixupva = idaapi.get_next_fixup_ea(fixupva)
# todo: assert </s> repo = self.remote.get_repo("testrepo0")	test_get_repo Test: Get a repo object
# todo: implement </s> :param type: either 'shows' or 'movies'	mark_expired Mark which cached shows/movies have updated information on trakt.
# todo in python 2.7 and later, this should be </s> primary_keys = set(primary_key_value(inst) for inst in relationship)	search_relationship query = session_query(session, related_model) relationship = getattr(instance, relation) query.filter(primary_key_value(related_model) in primary_keys) return search(session, related_model, filters=filters, sort=sort,
kwargs['application'] = application.objects.get(client_id=credentials['client_id'])  # todo: this should be cached one day </s> kwargs.update(credentials)	dispatch scopes, credentials = self.server.validate_authorization_request(uri, http_method, body, headers) kwargs['scopes'] = scopes self.oauth2_data = kwargs self.oauth2_data['user_id'] = request.user.id
except httperror as e:  # @todo ask for server instead </s> print("cannot get the json from nordvpn.com, manually specifiy a server\	displayServers try: response = requests.get(url, headers=headers).json() using '-s' for example '-s au10'") exit()
# todo(dolph): can be uncommented pending bug 968519 </s> user)	test_create_null_user_name user['id'],
# todo: expect_match should work with emit() </s> m = state.expect_match(	scan_command_edit if k == '+': state.ignore() r'(?:f(?:ile)?f(?:ormat)?|(?:file)?enc(?:oding)?|(?:no)?bin(?:ary)?|bad|edit)(?=\s|$)', lambda: VimError(ERR_INVALID_ARGUMENT))
# todo should this be handled differently when there are multiple ratings? </s> p_item = p_items[0]	rate if not p_items: return True if t_item and t_item.rating_advanced == p_item.user_rating: return True
# todo see if this can be better done in one query </s> return pagination(query.paginate(page, per_page), page, per_page, query.count())	paginated_instances def paginated_instances(self, page, per_page, where=None, sort=None): query = self.instances(where, sort)
# todo: get rid of sleep hack!! </s> self.assertraises(scopenotfoundexception, self.cm.get_scope, 'somerandomname')	testGetScopeNoScope def testGetScopeNoScope(self):
# todo stop guessing </s> remove_entire_file = (location.get('line') or location.get('column')) is none	parse_lint_result for location in issue_xml.findall('location'): filepath = location.get('file') issue = Issue(filepath, remove_entire_file) issue.add_element(issue_xml.get('message'))
if isinstance(err, asyncio.cancellederror):  # todo: remove when updated to 3.8 </s> raise err	BlobAnnouncer log.debug("failed to announce %s, could only find %d peers, retrying soon.", blob_hash[:8], peers) except Exception as err: log.warning("error announcing %s: %s", blob_hash[:8], str(err)) async def _announce(self, batch_size: typing.Optional[int] = 10):
''' todo: change conditional to return on non-http responses </s> to reduce branch depth'''	main iter = TextRecordParser(**textRecordParserOptions) for entry in iter(warc): if entry.record.rec_type != 'response' or \ entry.get('mime') in ('text/dns', 'text/whois'):
# todo(pep612): fix for paramspectype </s> if isinstance(tvar, paramspectype):	get_target_type skip_unsatisfied: bool ) -> Optional[Type]: return None assert isinstance(tvar, TypeVarType)
# compare filesizes todo print analysis of this :) </s> cmd = "ls -l '%s.ttf'*" % filename	ttx_process cmd = "ttx -i '%s.ttx'" % filename run(cmd, cwd=_out, log=log) run(cmd, cwd=_out, log=log) cmd = "rm  '%s.ttf.orig'" % filename
#change status of todo </s> todo.status = 'closed'	test_notification_to_assignee notification.send_to_all_assignees = 1 notification.save() todo.save() email_queue = frappe.get_doc('Email Queue', {'reference_doctype': 'ToDo',
# todo: figure out way to paramaterize node['osd_ids'] for this test </s> for osd_id in node["osd_ids"]:	test_osd_services_are_running def test_osd_services_are_running(self, node, Service): assert Service("ceph-osd@%s" % osd_id).is_running
# todo: should we concatenate preprocessed_s and preprocessed_last_s_prime? </s> state_values_pi, logits_pi, current_internal_states = \	define_api_methods_single initial_internal_states = self_.call(fifo_output_splitter.split, records) preprocessed_last_s_prime = self_.call(preprocessor.preprocess, last_s_prime) self_.call(policy.get_baseline_output, preprocessed_s, initial_internal_states) bootstrapped_values, _, _ = \
# todo: implement </s> completed remotely.	whenExecuted @return: a L{Deferred} that fires with C{None} when the work has been
1  # todo: fill in identifier </s> )	profile_transfer amount, target, result.wait() profiling.print_all_threads()
# todo: this should be configurable. some people may want such </s> logger.error(	dispatch _exc = _stack_format(e, style='color', show_vals='like_source', truncate_vals=5000, add_summary=True, source_lines=20) if _stack_format else _format_exc 'Caught an exception, cid:`%s`, status_code:`%s`, e:\n%s\n`%s`', cid, status_code, _exc_sep, _exc) try:
# todo test </s> def get_outputs_from_cm(index, connectivity_matrix):	get_outputs_from_cm connections to.""" return tuple(i for i in range(connectivity_matrix.shape[0]) if
# todo: add for morph targets data. </s> result_primitives = []	extract_primitives attributes[weight_id] = [] attributes[weight_id].extend(weights[bone_index]) for material_name, primitive in material_name_to_primitives.items(): export_color = True
# todo: remove when #980 has been merged </s> info['url'] = formats[-1]['url']	_real_extract 'upload_date': upload_date, } info['ext'] = determine_ext(formats[-1]['url']) return self.video_result(info)
except exception:  # todo: refactor this... </s> e = sys.exc_info()[0]	test_GET_InvalidData try: self.assertEqual(self.rnw.GET(url, data), 'json') self.assertEquals(e, TypeError)
# todo: remove this once there's proper support in upstream jinja </s> if func_name == 'ungettext':	_wrap Returns either a translated string or a lazy-translatable object, depending on whether there is a session language or not (respectively) args = (trim_inner_whitespace(args[0]), trim_inner_whitespace(args[1])) + args[2:] else:
# todo: find another way ... </s> if self.page_bottom >= self.position_y or first:	lines if not is_empty_line(line): self.position_y += line.height self.save() yield line
# todo: use the solution we implement once #134 gets fixed </s> a = next(cursor)	testFloat8 'pymssqlFloatTest', (5.44451787074e+39,)) assert abs(a[0] - 5.44451787074e+39) < 0.000001
# todo: rate limiting </s> p_ctx.active = true	handle_rpc contexts = self.get_contexts(request) p_ctx, others = contexts[0], contexts[1:] if p_ctx.in_error: return self.handle_error(p_ctx, others, p_ctx.in_error)
# todo complete this method </s> partition, kptlist, dtype)	eeccsd return ipccsd(eom, nroots, koopmans, guess, left, eris, imds,
# todo: avoid building interpolant every time? </s> return _interpolate(epochs, self.epochs, self._coordinates)	sample if epochs is None: return self._coordinates
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> sampleparameters = {	test_acquire_token_with_user_pass def test_acquire_token_with_user_pass(self): "tenant" : "common", "authorityHostUrl" : "https://login.windows.net",
# todo need copy? </s> x = mesh.node_coords.copy()	get_new_points def get_new_points(mesh): cells = mesh.cells["nodes"] jac_x = jac_uniform(x, cells)
# todo: signal to the user that they should reload their data! </s> return	_on_message_handler self._sequence_id = self._fetch_sequence_id(self._state) self._messenger_queue_publish() log.error("MQTT error code %s received", error) return
# todo: simulate short dataset with known properties and use it </s> events = find_events(targets=ds.sa.targets, chunks=ds.sa.chunks)	test_hrf_modeling skip_if_no_external('nipy') # ATM relies on NiPy's GLM implementation ds = load_example_fmri_dataset(literal=True) tr = ds.a.imghdr['pixdim'][4] for ev in events:
# todo(b/186451541): reduce the number of calls to model_fn. </s> self.assertequal(mock_model_fn.call_count, 2)	test_construction_calls_model_fn mock_model_fn = mock.Mock(side_effect=TestModel) federated_evaluation.build_federated_evaluation(mock_model_fn)
# todo: this crashes if a feature is already named 'index'. </s> index_name = 'index'	predict_proba index_name = data.index.name if index_name is None: idx_to_image_map = data[['image']] idx_to_image_map = idx_to_image_map.reset_index(drop=False)
# todo: convert into a proper api to detect read-only layouts </s> self._focus += 1	Frame break except IndexError: self._clear() def _clear(self):
# todo: location </s> return util.trim_nulls({	object_to_json if author and 'objectType' not in author: author['objectType'] = 'person' 'type': [types.get(obj.get('objectType'))], 'properties': {
# todo: make this configurable </s> self.semiadditive_function = "max"	QueryBuilder "(cube '%s', dimension '%s')" % (self.cube.name, dim.name)) def aggregation_statement(self, cell, drilldown=None, aggregates=None, split=None, attributes=None, summary_only=False):
# todo: the stuff </s> pass	TraktShow setattr(self, col, trakt_series.get(col)) for genre in trakt_series.get('genres', ()): self.expired = False
# todo: docs and comments </s> softmaxes = {}	test def test(model, criterion, data_loader, device, print_freq, metric_logger): labels = {} model.eval()
# todo (t65593688): this should be removed after </s> with torch.no_grad():	test_tokens def test_tokens(self): model = Seq2SeqModel.from_config( Seq2SeqModel.Config(
# reasons why we said no. todo: allow configurable error messages </s> raise synapseerror(403, "not allowed to publish room")	create_room if is_public: if not self.config.is_publishing_room_allowed(user_id, room_id, room_alias): preset_config = config.get( "preset",
self.button_align_test = wx.button(self, label="align test")    # todo maybe align left? </s> self.button_apply = wx.button(self, label="apply")	WallpaperSettingsPanel self.sizer_settings_right.Add(self.sizer_setting_paths, 0, wx.CENTER|wx.EXPAND) self.button_help = wx.Button(self, label="Help")                # TODO maybe align left? self.button_close = wx.Button(self, label="Close") self.sizer_bottom_buttonrow.Add(self.button_help, 0, wx.ALIGN_LEFT|wx.ALL, 5)
# todo: requires special treatment? </s> current_unit = unit_line.variants[0].line[0]	_unit_line_to_game_entity :type unit_line: ..dataformat.converter_object.ConverterObjectGroup if isinstance(unit_line, GenieVillagerGroup): else: current_unit = unit_line.line[0]
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> response = util.create_response()	test_validation_off @httpretty.activate def test_validation_off(self): wire_response = response['wireResponse'] util.setup_expected_client_cred_token_request_response(200, wire_response, response['authority'])
# todo(berrange): why do we bother converting the </s> data["cpu_info"] = jsonutils.dumps(self._get_cpu_info())	get_available_resource data["hypervisor_version"] = self._host.get_version() data["hypervisor_hostname"] = self._host.get_hostname()
# todo: just a quick hack (screen is erased before scrolling begins). </s> string = " " + string + " " # just to avoid artifacts on full width characters	LedCtrlString time.wait(waitms) elif direction == self.SCROLL_RIGHT: for n in range( (len(string) + 1) * 8 - 7, 0, -1 ): if n <= len(string)*8:
accept_federated_only=self.federated_only)  # todo: 466 </s> listeners = self._learning_listeners.pop(node.checksum_public_address, ())	remember_node node.verify_node(self.network_middleware,  # TODO: Take middleware directly in this class? force=force_verification_check, address = node.checksum_public_address self.__known_nodes[address] = node
# todo: rate limiting </s> p_ctx.active = true	handle_rpc contexts = self.get_contexts(request) p_ctx, others = contexts[0], contexts[1:] if p_ctx.in_error: return self.handle_error(p_ctx, others, p_ctx.in_error)
).consume()  # todo see issue 170 </s> aws_update_tag=aws_update_tag,	load_ec2_instance_network_interfaces InstanceId=instance_id,
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
# todo: attachments. </s> pass	_migrate_form_attachments def _migrate_form_attachments(sql_form, couch_form):
if key.endswith('_state'): # todo: kludge </s> subkey = key[:-len('_state')]	keyCallback def keyCallback(key, persistent, ctor): subblock = getattr(block, subkey) if subkey not in seen:
# todo(b/182621549): for sobol sequences, dimension should be known at graph </s> dim = tf.get_static_value(dim)	_mvnormal_quasi skip = 0 if random_type == RandomType.SOBOL: if dim is None: raise ValueError('For Sobol sequences, dimension should be known at graph'
# todo(piyush): current api-site doesn't contain this api description. </s> post_body = json.dumps(kwargs)	create_user_ec2_credentials def create_user_ec2_credentials(self, user_id, **kwargs): resp, body = self.post('/users/%s/credentials/OS-EC2' % user_id, post_body)
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
""" todo. """ </s> self._server.application.setvolume(int(volume * 100))	volume_set def volume_set(self, volume): self.update_ha_state()
# todo: consider implement it through calling self.collected </s> return collection(	Collection 'that is collected(lambda e: e.all(selector)).first ... o_O ', FutureWarning) by = to_by(css_or_xpath_or_by) Locator(f'{self}.all({by})', lambda: flatten([webelement.find_elements(*by) for webelement in self()])),
# todo: alot of stuff here </s> self.out_parsing = true	decode_out else:
# todo remove this method. should be handled in importtask creation. </s> try:	read_item def read_item(self, path): If an item could not be read it returns None and logs an error. return library.Item.from_path(path) except library.ReadError as exc:
# todo: figure out how to mock open </s> with patch(	test_get_config_tuple_from_egrc_when_present def test_get_config_tuple_from_egrc_when_present(): 'ConfigParser.RawConfigParser.readfp', return_value='read_file'
# todo: remove str() when dropping support for py37. </s> return [*args, str(script_entrypoint), *sys.argv[1:]]	get_child_arguments script_entrypoint = py_script.with_name('%s-script.py' % py_script.name) if script_entrypoint.exists(): raise RuntimeError('Script %s does not exist.' % py_script) else:
# todo fixme insert and replace instead </s> engine.execute(alala.table_hash.delete())	wrapper if len(datas) > 0: engine.execute(alala.table_data.insert().values(datas)) # TODO chunks?? engine.execute(alala.table_hash.insert().values([{'value': h}])) return datas
# todo: refactor. </s> if version.at_least(test.sync_cx, (2, 5, 4)):	test_authenticate self.assertFalse("mike" in [u['user'] for u in users]) finally: yield self.db.command({"dropAllUsersFromDatabase": 1}) else:
# todo: remove warning check once deprecated </s> hits = tree.intersection((1012821.80, 229228.26), objects=true)	test_merge_geo tree = first.sindex with pytest.warns(FutureWarning, match="`objects` is deprecated"): res = [first.loc[hit.object]["BoroName"] for hit in hits] assert res == ["Bronx"]
# todo: test this </s> self.logger.info(f"forwarded htlc has failed, {reason}")	Peer self.logger.info("htlc forwarded successfully") else: await self.fail_htlc(chan, htlc.htlc_id, onion_packet, reason) @log_exceptions
# todo: move to serializer </s> path = self.get_argument('path')	get def get(self): max_width = self.get_argument('max_width', 320) max_height = self.get_argument('max_height', 240)
# todo: ... </s> returns	classify_output target: Array-like prediction: Array-like ------- numpy.array
# todo: change the way we do it (csv dialect may change, encoding </s> file_header = open_compressed(filename).readline().strip().split(",")	handle start_time = time.time() progress = ProgressBar(prefix="Importing data", unit="bytes") table_schema = table.schema schema = OrderedDict(
# todo: does this import need to be delayed because </s> from pyomo.solvers.plugins.solvers.persistent_solver import \	initialize import pyomo.environ import pyomo.solvers.plugins.smanager.phpyro PersistentSolver self._init_start_time = time.time()
# todo counts as yes if vhnd was not available </s> return len([c for c in self.cases	vhnd_monthly @property def vhnd_monthly(self): if c.preg_attended_vhnd or c.child_attended_vhnd])
# todo this should depend on the error (even more granularity) </s> try:	LNWallet blacklist = self.handle_error_code_from_failed_htlc(failure_msg, sender_idx, route, peer) if blacklist: short_chan_id = route[sender_idx + 1].short_channel_id except IndexError:
# todo: this also needs to trigger filter_specs.updateui to switch to </s> self.sigfilterdesigned.emit()	store_entries ))
# todo: i put dummy() to fix below, remove the comments after a while. </s> self.asserttrue(tsm is not none) # fix: i see this fails sometimes?	test_basic dummy() # call dummy to force ctx name to be retrieved again. self.assertTrue(tsa is not None)
# todo: optimizer state gets cast to fp16 and back to fp32 for </s> weight, bias, input = make_half_precision_params()	test_state_dict_memory_efficient @pytest.mark.xfail def test_state_dict_memory_efficient(): optimizer = Adam([weight, bias], lr=1e-3, precision=Precision.MEMORY_EFFICIENT_MIXED_PRECISION) state_dict_test(optimizer, weight, bias, input)
# todo assert cls.__tablename__ == '' </s> with dbcleanup(session, cls):	test_VisualizationTagAssociation def test_VisualizationTagAssociation(model, session, visualization, tag, user): cls = model.VisualizationTagAssociation user_tname, value, user_value = 'a', 'b', 'c' obj = cls(user=user, tag_id=tag.id, user_tname=user_tname, value=value)
# todo: log errors to log file </s> pass	Kubeshell except  Exception, e: print(str(e)) @registry.add_binding(Keys.F9) def _(event):
# todo: use optparse command options instead. </s> project_dir = sys_argv[1]	run_tests sys_argv: a reference to sys.argv. try: sys_argv.pop() except IndexError:
# todo - fix this to be more efficient, so we don't parse the file twice </s> def _get_xmlns(self, stream):	_get_xmlns xml_string = get_xml_string(stream) try:
# todo: alltoall not yet implemented on xla:cpu </s> if jtu.device_under_test() == "cpu":	testGradOfPswapaxes def testGradOfPswapaxes(self): device_count = xla_bridge.device_count() device_count = 1 shape = (device_count, 1, device_count)
self.progressbar.set_text("sorting ... ") # todo(jflesch): i18n/l10n </s> gtk_refresh()	_docsearch_callback self.progressBar.set_text("Reading '" + document + "' ... ") # TODO(Jflesch): i18n/l10n elif step == DocSearch.INDEX_STEP_SORTING:
# todo: support other output fields </s> return [	split def split(self, ops: Any, lengths: List[int]) -> List["Activations"]: last_hiddens = ops.unflatten(self.lh, lengths) Activations(lh, [], [], [], is_grad=self.is_grad) for lh in last_hiddens
# todo: deprecated - remove in version 0.10 </s> if isinstance(training_trackers, string_types):	train "Pass appropriate featurizer " "directly to the policy instead.") logger.warn("Passing a file name to `agent.train(...)` is " "deprecated. Rather load the data with "
raise exception  # todo </s> self.key = key	__init__ if isinstance(columns[0], basestring): if key not in columns: elif isinstance(columns[0], tuple): key_matched = False
pass #todo fix imports </s> def visit_import(self, node):	visit_Import
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
# todo: test coverage for this branch </s> not_subscribed = true	unsubscribe_user }) except Subscription.DoesNotExist: if not_subscribed: messages.info(request,
#todo kajak doesnt like my consumer-list -> research why? </s> return sig	createSignal consumer.append(noderef)
#@todo: remove in 0.4.10 </s> def error(self, reason=none, type="parse"):	error raise Fail("%s error%s | Plugin out of date" % (type.capitalize(), ':' + str(reason) if reason else "")) if self.core.debug:
# todo: remove </s> g.members = members	members _(u'User %r not authorized to edit members of %s') % (g.user, id)) g.group_dict = group_dict extra_vars = {
pass     # todo: </s> def _reload(self, packages):	_reload
# todo: remove in 21.08 </s> if os.path.exists(cache_audio_dir):	copy_cache Args: cache_audio_dir (path): path containing .wav files dest = util.get_cache_directory('tts/' + 'Mimic2') files = os.listdir(cache_audio_dir)
# todo: handle index/keyerror here when we overrun a segment </s> flags = self.getflags(ea)	Head while not self.isHead(flags): ea -= 1 return ea
# todo: confirm is uri </s> if uri.startswith(self.__uri__.string):	relative_uri def relative_uri(self, uri): return uri[len(self.__uri__.string):] else:
return response(status=400)  # todo </s> if new_address in self.reserved_addresses:	register return Response(status=400)  # TODO if not eth_utils.is_checksum_address(new_address): return Response(status=400)  # TODO try:
# # todo: # fixme: </s> pass	update_tag_last_seen r_serv_tags.hset('tag_metadata:{}'.format(tag), 'last_seen', tag_last_seen) else:
# todo(sdake) the parameters to delete operations are highly suspect </s> service=self.fake_service)	test_service_create version='1.0',
# todo: direct use of json['error'] is deprecated; use display_message('...', 'error') instead. </s> j_dic['error'] = 'unable to parse the following line(s):<br/>{}'.format(	enrich_json_with_data ) if ann_obj.failed_lines: '\n<br/>\n'.join( ['{}: {}'.format(
# todo: should be injected </s> facade = awsfacade()	LambdasConfig class LambdasConfig(ResourceConfig): async def fetch_all(self, credentials, region, partition_name='aws', targets=None): functions = {} for raw_function in facade.get_lambda_functions(region):
# todo: try simply using all possible fields instead of extracting features manually. </s> bugs['data'].append(data)	go res = str(res) data[f.__name__] = res bugs['title'].append(bug['summary']) bugs['comments'].append(' '.join([c['text'] for c in bug['comments']]))
# todo: make truly async </s> async def get_database_instances(self, project_id):	CloudSQLFacade request = self._cloudsql_client.backupRuns().list_next(previous_request=request, previous_response=response) return backups database_instances = [] request = self._cloudsql_client.instances().list(project=project_id)
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
# todo: try reconnecting? </s> raise	_listen if self._closed: return except OSError as exc: if exc.errno == 9:  # socket closed
#self.resetacount() #@todo implement </s> else:	prepare if self.api_data["size"]/1024 > info["trafficleft"]: self.log.info(_("%s: Not enough traffic left" % self.__name__)) self.url = self.get_file_url() self.pyfile.name = self.get_file_name()
#todo: remove this transformation </s> all_owned_schemas = {dbobject.qualified_name: owner for dbobject, owner in all_owned_schemas_raw.items()}	determine_schema_privileges read_schemas = {s.qualified_name for s, _ in read_schemas_and_owners} all_owned_schemas_raw = dbcontext.get_all_schemas_and_owners() role_owned_schemas = {s for s, owner in all_owned_schemas.items() if owner == role} write_schemas.update(role_owned_schemas)
# todo(dcramer): ideally we could just send the signal to the subprocess </s> self.task.date_finished = datetime.utcnow()	_cancel self._logreporter.terminate() self._logreporter.save_chunk('Task was cancelled\n') db.session.add(self.task) db.session.commit()
# todo: this method should not require the training data. </s> if self.features_inputter is not none and "train_features_file" in metadata:	_serving_input_fn_impl def _serving_input_fn_impl(self, metadata): self._initialize(metadata) _ = self.features_inputter.make_dataset(metadata["train_features_file"]) return self._get_serving_input_receiver()
# todo: tab completion for the local system. </s> candidates, possible_completion = complete(current_word, ls)	perform_tab_completion "-w %d 2>/dev/null\r" % context.window_size[1]).encode("ascii")) ls = self.read_all_output().split("\r\n") if possible_completion: if possible_completion[-1] != '/' and not candidates:
# todo find out what is best used here! </s> 'preferred_dtype' : none}	get_meta_information 'is_deterministic': True, 'handles_sparse': True,
#todo - look at alphabet? </s> assert false, example1	test_the_complement mapping = maketrans("CGcg","GCgc") else : continue self.assertEqual(str1.translate(mapping), str(comp))
# todo: docstring </s> vsc_fid = int(args['frameid'])	on_setExpression @async_handler def on_setExpression(self, request, args): pyd_tid, pyd_fid = self.frame_map.to_pydevd(vsc_fid) safe_repr_provider.set_format(
# todo: reactivate after fixing alternative selection </s> def test_system_importer_file_csv_form_based_form_empty(self):	SystemImporterFileCsvFormbasedFormbasedFormTestCase
# todo add brief documentation what that means </s> if self.add_layer_norm_after:	Adapter if self.residual_before_ln: output = output + residual_input output = self.adapter_norm_after(output) if not self.residual_before_ln:
# todo: else: req.warning('...') </s> req.redirect(req.href.admin(cat, page))	PermissionAdminPanel if (subject, action) not in all_permissions: perm.grant_permission(subject, action) elif req.args.get('add') and subject and group: req.perm.require('PERMISSION_GRANT')
# todo :: move arbitray path construction to storagelayout object </s> url = '{0}/wal_{1}/{2}.lzo'.format(	wal_restore NB: Postgres doesn't guarantee that wal_name == basename(wal_path), so both are required. self.layout.prefix.rstrip('/'), FILE_STRUCTURE_VERSION, wal_name) logger.info(
# todo(developer): uncomment and set to a path to your audio file. </s> with open(speech_file, 'rb') as audio_file:	transcribe_file_with_multilanguage from google.cloud import speech_v1p1beta1 as speech client = speech.SpeechClient() content = audio_file.read() audio = speech.types.RecognitionAudio(content=content)
# todo debug </s> print logstring	logRule + "end=%d)") % ruleElement.element.end logging.info("[%s]: %s" % (fileName, logString)) elif ruleElement.type == "second": logString = ("%s second " % spaceString
# todo: implement clock interrupt. </s> self.set_cf()	int1a self.clear_cf() elif ah in [6, 7, 9]: elif ah == 8: pass
# todo: avoid dummy and generate func here when inlining is possible </s> def _impl(df, axis=none, skipna=none, level=none, numeric_only=none):	_impl return hpat.hiframes.pd_dataframe_ext.max_dummy(df)
#todo: algorithm is cryptfiltername, specified in the /cf dictionary </s> return (-1,'decrypt not supported yet')	decrypt return (0, stream) else:
# todo: remove in future release </s> if force_masquerade and not get_masquerade(zone):	add_port_fwd salt.utils.warn_until('Neon', 'add_port_fwd function will no longer force enable masquerading in future releases. Use add_masquerade to enable masquerading.') add_masquerade(zone) cmd = '--zone={0} --add-forward-port=port={1}:proto={2}:toport={3}:toaddr={4}'.format(
# todo: we want to create a state group for this set of events, to </s> prev_group = none	resolve_state_groups state_group = sg break delta_ids = None for old_group, old_ids in state_groups_ids.iteritems():
# todo: remove this as soon it is fixed in plaidml. </s> opt_kwargs["clipnorm"] = 1.0	get_optimizer if (self.config.get("clipnorm", False) and keras.backend.backend() != "plaidml.keras.backend"): logger.debug("Optimizer kwargs: %s", opt_kwargs) return Adam(**opt_kwargs)
# todo: limit/check colorcode </s> if colorcode is none:	LaunchpadPro number = min( number, 99 ) number = max( number, 0 ) colorcode = LaunchpadPro.COLORS['white'] self.midi.RawWrite( 144, number, colorcode )
# todo: simplify this via aliasing methods in `frappe.qb` </s> if frappe.db.db_type == "mariadb":	set_encrypted_password .insert(doctype, name, fieldname, encrypt(pwd), 1) ) query = ( query.on_duplicate_key_update(Auth.doctype, doctype)
# self.skiptest("unfinished (bad functionality?)") # todo </s> testname = self.testname()	test_4037_notify_service_functions_with_reload_user if not os.path.exists("/usr/bin/socat"): self.skipTest("missing /usr/bin/socat") testdir = self.testdir() self.notify_service_functions_with_reload("user", testname, testdir)
# todo: change to us-east-1 </s> credentials = read_creds('default')	test_get_cloudwatch_region def test_get_cloudwatch_region(self): service_config = {'regions': {'us-east-1': {}}} get_cloudwatch_region(params = {'region': 'us-east-1', 'creds': credentials, 'cloudwatch_config': service_config})
# todo these args are locked and can not be changed </s> return cv2.gaussianblur(old, (7, 7), 0)	turn_blur def turn_blur(old: np.ndarray) -> np.ndarray:
# todo: figure out, what's going on with v6 here! </s> import time	test_AnnexRepo_status eq_(stat, ar.status()) if ar.config.getint("annex", "version") == 6: time.sleep(1) ar.add('first', git=True)
# todo: do not require xml directly here. </s> def __init__( self, name, attrib, element_tests ):	TestCollectionOutputDef class TestCollectionOutputDef( object ): self.name = name self.collection_type = attrib.get( "type", None )
# todo partially update stored playlists? </s> u'%d track(s) removed from playlist "%s"', len(tracks), playlist.name())	tracks_removed logger.debug(u'Callback called: '
# todo: the following skipped suite and fixtures should be enabled </s> return ['api-key']	_filter_headers def _filter_headers(self):
# todo: use spoolup options to fetch main value </s> repamount = repamountbase * (1 + calculatespoolup(repspoolmax, repspoolpercycle, cycletime, spooltype.scale, 1))	handler repSpoolMax = container.getModifiedItemAttr("repairMultiplierBonusMax") repSpoolPerCycle = container.getModifiedItemAttr("repairMultiplierBonusPerCycle") fit.extraAttributes.increase("armorRepair", repAmount / cycleTime, **kwargs)
# todo - this is what i get back from kafka at the moment, clearly it's wrong </s> response = protocol.offsetcommitresponse(	test_offset_fetch_response def test_offset_fetch_response(self): buffer('\x00\x00\x00\x00') )
# todo: avoid dummy and generate func here when inlining is possible </s> def _impl(df, n=5):	_impl return hpat.hiframes.pd_dataframe_ext.head_dummy(df, n)
raise exceptions.mpdnotimplemented  # todo </s> deletes ``songpos`` from the playlist ``name.m3u``.	playlistdelete ``playlistdelete {NAME} {SONGPOS}``
# todo: need to end xa state here </s> self.do_rollback(connection.connection)	do_rollback_twophase ):
# todo: remove temporary workaround once https://github.com/python-babel/babel/issues/415 has been resolved. </s> babel_415_workaround = self.toolbar_buttons[action]['label']	_insert_item list_item.setToolTip(_('Drag and Drop to re-order')) if action in self.TOOLBAR_BUTTONS: list_item.setText(_(babel_415_workaround)) list_item.setIcon(icontheme.lookup(self._get_icon_from_name(action), icontheme.ICON_SIZE_MENU))
# todo: some kind of value escape </s> return ",".join(["%s=%s" % kv for kv in zip(self.keys, values)])	_render_key def _render_key(self, values):
# todo: enable output dtype selection. </s> result = eval(cmd).astype(kwargs['dtype'])	calc part) logger.debug("Translated cmd: %r", cmd) dst.write(result, i) sys.exit(0)
# todo: handle bidi </s> translate_x = child.left	block_container_layout resolve_one_percentage(child, 'bottom', new_box.height) if child.left != 'auto' and child.right != 'auto': elif child.left != 'auto': translate_x = child.left
#todo - should the default be gapped(single_letter_alphabet) instead? </s> def clustaliterator(handle, alphabet = single_letter_alphabet) :	ClustalIterator The entire file is loaded at once, but the SeqRecord objects are only created "on request".
# todo(tr3buchet) - remove comment in multi-nic </s> ips = db.fixed_ip_get_all_by_instance(admin_context, instance['id'])	spawn VMHelper.create_vbd(self._session, vm_ref, vdi_ref, 0, True) admin_context = context.get_admin_context() for network in db.network_get_all_by_instance(admin_context, instance['id']):
# todo: cartopy has had two formatters for a while but we use newer one </s> if not self._gridliners:	CartopyAxes value = (value + 180) % 360 - 180 return type(self)._add_gridline_label(self, value, axis, upper_end) gl = self.gridlines(zorder=2.5)  # below text only gl._axes_domain = _axes_domain.__get__(gl)  # apply monkey patches
# todo: remove hardcoded ad-hoc behaviors. </s> start = addr('__libc_start_main')	get_bin_sh_str def get_bin_sh_str(): found = gdb.execute('find {}, +2000000, "/bin/sh"'.format( start), to_string=True)
# todo: do we need to skip config.add_slack variable here? </s> var_filter = (lambda v: v[1].is_integer()) if discrete_only \	generate_norm2sq_objective_function discrete_only: Bool only optimize on distance between the discrete variables else (lambda v: v[1].name != 'MindtPy_utils.objective_value' and 'MindtPy_utils.MindtPy_feas.slack_var' not in v[1].name)
raise skiptest  #todo: figure out why this randomly started failing. </s> qs = {'a': 1, 'w': 4, 'format': 'json', 'thread_type': 2,	test_discussion_filter_locked def test_discussion_filter_locked(self): 'forum': 1, 'q': 'locked'} response = self.client.get(reverse('search'), qs)
# todo: will probably need to make this configurable at some point </s> return []	sql_column_indexes @property def sql_column_indexes(self):
pass  # todo </s> **kwargs):	move_camera frame_center=None,
return 0.5, 1 #todo make a better angle here. </s> else:	getangle if smashbot_state.position.x > 0: if wanted_angle > lowest_angle: if wanted_angle < lowest_angle: return 0.5, 1 #TODO make a better angle here.
raise exceptions.mpdnotimplemented  # todo </s> underscore, dash, dot and colon.	subscribe already. The name may consist of alphanumeric ASCII characters plus
@unittest.skip('not written')  # todo: finish! </s> raise notimplementederror	test_namedtuple def test_namedtuple(self):
# todo: clean up </s> for _ in range(num_hosts)	gossipsubs )
# todo: add test </s> if cut == y_cuts[-1]:	split_textline break else: cut_text.append((cut[0] - 1, c, obj)) elif isinstance(obj, LTAnno):
# todo: give a vanilla example </s> .. math::	feca def feca(predicted_power, df_appliances_ground_truth): fraction = \\sum_n min \\left (
# todo figure this out </s> arch = 'x86_64-linux-gnu'	env def env(self, root): envs = self.ubuntu.env(root) envs.extend([
# urwid.text( " todotxt-machine ", align='center' ), </s> urwid.text( ('header_file', "{0}  {1} ".format(message, self.todos.file_path)), align='right' )	create_header ('header_todo_done_count', " {0} Done ".format(self.todos.done_items_count())), ]), ]), 'header')
# todo(b/141131288): enable complex-valued sorts on tpu. </s> if (onp.issubdtype(key_dtype, onp.complexfloating) and (	testSortKeyValAgainstNumpy for axis in [-1, len(shape) - 1])) def testSortKeyValAgainstNumpy(self, shape, key_dtype, val_dtype, axis): (jtu.device_under_test() == "cpu" and jax.lib.version <= (0, 1, 47)) or jtu.device_under_test() == "tpu")):
# todo: make this configurable? </s> maxresults = 200	list language=unicode(get_lang()), ) try: dayHorizon = int(request.params.get('days', 5))
# todo commit hash </s> prefix = f"https://github.com/nix-community/nur-combined/tree/master/repos/{repo}"	resolve_source def resolve_source(pkg: Dict, repo: str, url: str) -> str: position = pkg["meta"].get("position", None) if position is not None and position.startswith("/nix/store"):
# todo: vip1019 </s> raise invalidtypeexception("returning structs not allowed yet, see vip1019", self.stmt)	parse_return typ=None, pos=getpos(self.stmt)) elif isinstance(sub.typ, StructType): elif isinstance(sub.typ, TupleType): if not isinstance(self.context.return_type, TupleType):
# look & feel todo:turn on.. </s> ret2= core.segmap(img2)	test_segment_input_output_spec_check ret1= core.segmap(img1)
#todo - can we raise the error before the unit test function </s> raise missingexternaldependencyerror(\	real_test except RenderPMError, err : if str(err).startswith("Can't setFont(") : "Check the fonts needed by ReportLab if you want " "bitmaps from Bio.Graphics\n" + str(err))
# todo: groupby executor, spawn many _as_completed coroutines </s> raise notimplementederror("wait on many event loops not yet supported")	wait loop = first(fs).executor.loop else: return sync(loop, _wait, fs, timeout, return_when)
# todo(aron): move these client test cases to their own test class </s> self.client.login_teacher(data={"username": self.teacher_username,	teacher_cant_edit_facilities elem = self.browser.find_element_by_css_selector('a.edit-facility') self.assertEquals(elem.value_of_css_property("display"), "none", "edit-facility is still displayed!") "password": self.teacher_password}, facility=self.facility)
self.mdbx = maestral()  # todo: create or get daemon instead? </s> self.setup_ui_linked()	load_maestral self.quit() else:
# todo: fire_switch_state_change(self, channel_index, old_switch_state, switch_state, from_myself) </s> elif namespace == hub_temperature:	_handle_push_notification for sensor in payload['mode']: self._update_client_data(sensor) for sensor in payload['temperature']: self._update_client_data(sensor)
# todo: refactor. </s> if (yield version.at_least(self.cx, (2, 5, 4))):	test_copy_db_auth yield self.cx.drop_database(target_db_name) finally: yield self.db.command({'dropAllUsersFromDatabase': 1}) else:
#todo: unit tests </s> user = auth.user	get_all_projects_smart_folder @must_be_logged_in def get_all_projects_smart_folder(auth, **kwargs): contributed = user.node__contributed nodes = contributed.find(
# @todo: remove this if in 0.6 </s> if isinstance(node_id, node):	ex_list_ip_addresses def ex_list_ip_addresses(self, node_id): node_id = node_id.id uri = '/servers/%s/ips' % node_id
# todo: convert position ordering according to data axistags </s> self.handleeditorleftclick(self.imageindex, position5d)	_handleEditorLeftClick def _handleEditorLeftClick(self, position5d):
# todo: should this fail instead? </s> self.assert_vsc_received(received, [	test_unknown_reason tid = self.send_event(10, 99999) received = self.vsc.received self.expected_event( reason='pause',
# todo(developer): uncomment and set the following variables </s> job = client.job_path(project_id, location_id, job_id)	delete_scheduler_job from google.api_core.exceptions import GoogleAPICallError client = scheduler.CloudSchedulerClient() try: client.delete_job(job)
# todo(sbdchd): move to queries </s> token = await client.get_token_for_install(	PR self.log.warning("problem") return installation_id=self.installation_id )
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> sampleparameters = {	test_acquire_token_with_user_pass def test_acquire_token_with_user_pass(self): "tenant" : "common", "authorityHostUrl" : "https://login.windows.net",
# todo: this should be done on evoked </s> stcs = dics_epochs(epochs, forward_surf_ori, noise_csd, data_csd,	test_dics_epochs assert_true(-1 < tmax < 1) assert_true(0. < np.max(max_stc) < 20.) pick_ori="normal", return_generator=True) stc_normal = stcs.next()
irregular_dims = ['time', 't']  # todo: get irregular dims from dataset_type </s> dims = dataset_type.gridspec['dimension_order']	get_data_for_dims @staticmethod def get_data_for_dims(dataset_type, groups, geobox): dt_data = { 'dims': dims,
# todo: actually test this once i figure out how to do this in py.test </s> logging.error("interrupted by user")	args_download jobs.get(0xFFFF) except KeyboardInterrupt: return 1 except requests.exceptions.ConnectionError as err:
# todo make this configurable </s> def get_prefix_color(prefix):	get_prefix_color if prefix == "&": return "lightgreen"
# todo get function data </s> pass	filter_name result.append(Instance(par.parent.parent)) else: else: result.append(scope.parent)
# todo: test with multiple responses </s> x = standardscaler().fit_transform(x)	test_regressor_pickle X, y = boston.data, boston.target X, y = shuffle(X, y, random_state=0) y = StandardScaler().fit_transform(y) succeeded = True
# todo task python? </s> if context.verbose:	pipe_fetchpage content = unicode(request.read(), request.headers['content-type'].split('charset=')[-1]) print "............FetchPage: content ................." print content.encode("utf-8")
#todo: use invalidation time </s> params: chat to dump, mediaid of the profile photo in the db	dump_chat def dump_chat(self, chat, photo_id): Returns -""" timestamp = round(time.time())
# todo: weather data source changed, temporarily disable, will enable it later when new data source is available. </s> weekday = cur_datetime.weekday()	_update_station_extra_features return self._last_date = cur_datetime holiday = cur_datetime in self._us_holidays weather = 0
# todo: instead of discarding pending jobs, maintain them </s> for njob in node.pending_jobs:	run_job logger.debug(traceback.format_exc()) if node.pending_jobs: self.finish_job(cluster, njob, DispyJob.Cancelled) if cluster.status_callback and dispy_node:
#todo load this from somewhere </s> pad_data = [-1.46374, -0.151816, -0.161173, 0.0686325, 0.0231148, -0.154613,	allocate_devices device.targets[:l, q] = self.data.targets[self.data.seq_start[s] + batch.start[1]:self.data.seq_start[s] + batch.start[1] + l] if self.pad_batches: -0.105614, 0.00550198, 0.0911985, 0.00502809, 0.0512826, -0.0181915, 0.0225053, -0.00149681, 0.0782062, 0.0412163, 0.0526166, -0.0722563,
# todo: verify logic for create -- we shouldn't 'annexify' non-annexified </s> annex = get_repo_instance(filedir)	_dataset_auto_get else: try: lgr.log(2, "Got the repository %s id:%s containing %s", annex, id(annex), filedir) except (RuntimeError, InvalidGitRepositoryError) as e:
# todo: test 2b: planilha deployed mais atualizada que total (deployed) </s> if date in cases:	get_state_data for spreadsheet in spreadsheets: date = spreadsheet.date continue report_data = reports.get(date, defaultdict(list))
# todo find out what is best used here! </s> 'preferred_dtype': np.float32}	get_properties 'is_deterministic': True, 'handles_sparse': False,
# todo(dspasovski): fix this. </s> raise skiptest	TestIndexLanding @mock_es def test_good_cat(self): r = self.client.get(self.url) eq_(r.status_code, 200)
asynchronous=false, # todo: (true) when jconnor fixes </s> archive=true,)	install resources=resources, weight=0, result = execution.execute_async(self, call_request) return result
# todo(lbragstad): move this test to tests/test_v3_assignment.py </s> self.head(member_url, expected_status=404)	test_delete_user_before_removing_role_assignment_succeeds self.delete(member_url, expected_status=204)
# todo: check for valid values </s> self._noteheadparen = value	_setNoteheadParen def _setNoteheadParen(self, value):
# todo: convert into a proper api </s> self._focus += 1	Frame break except IndexError: if self._focus >= len(self._layouts): self._focus = 0
# todo: check arp reply is valid </s> self.asserttrue(self.packet_outs_from_flows(arp_replies))	arp_for_controller 'arp_source_ip': '10.0.0.1', 'arp_target_ip': '10.0.0.254'})
# todo(solitude): remove this. </s> data.update({'pattern': 'account.payment'})	preapproval key = result['key'] else: try: result = paypal.get_preapproval_key(data)
#todo: remove this transformation </s> personal_schemas_dbo = set(dbcontext.get_all_personal_schemas())	determine_schema_privileges are owned by this role (and because they are in the write section then they will also be granted read privileges as well). personal_schemas = {dbo.qualified_name for dbo in personal_schemas_dbo} write_schemas_and_owners = dbcontext.get_role_current_nondefaults(role, 'schemas', 'write')
@unittest.skip('not written')  # todo: finish! </s> def test_empty(self):	test_empty raise NotImplementedError
# todo update this code once keras > 2.0.4 is released </s> try:	validate time.sleep(10) continue embedding = keras.models.load_model( weights_h5, custom_objects=CUSTOM_OBJECTS,
# todo: move to base class </s> self.factor = self.transform().m22()	zoom def zoom(self, scale_factor): futureScale = self.factor * scale_factor if futureScale <= self._minimum_scale:
# todo hmm... ok, need to document reload() </s> yield dotpolar	prepare import my.reading.polar as polar reload(polar)
# todo: move this to an integration test? </s> test_data = os.urandom(40)	test_evaluate_cfrag assert u_xcoord == adjudicator_contract.functions.UMBRAL_PARAMETER_U_XCOORD().call() assert u_ycoord == adjudicator_contract.functions.UMBRAL_PARAMETER_U_YCOORD().call() h = hash_to_curvebn(test_data, params=umbral_params,
# todo: federation should provide one method to send, </s> payload = handle_create_payload(entity, content.author)	send_content entity = make_federable_entity(content) if entity: url = "https://%s/receive/public" % settings.SOCIALHOME_RELAY_DOMAIN send_document(url, payload)
# todo debug </s> print logstring	logRule + "end=%d)") % ruleElement.element.end logging.info("[%s]: %s" % (fileName, logString)) elif ruleElement.type == "minute": logString = ("%s minute " % spaceString
common_path=prefix,  # todo: add key? </s> action="local",	make_inline_attachments_decision ld = local_diff[k] md = MergeDecision( conflict=False, local_diff=[ld],
# todo: make an ascii-art bar </s> return "%.1f%%" % (100.0 * progress)	render_progress_hash def render_progress_hash(self, ctx, data): progress = data.get_progress()[0]
# todo: take this out later </s> if self.name in ['setuptools']:	InstallRequirement try: install_args = [sys.executable] install_args.append('setup.py') else:
# todo(ytknzw): add more specific assertion with the test case. </s> study = create_study()	test_plot_intermediate_values trial.report(2.0, step=1) return 0.0 study.optimize(lambda t: objective(t, True), n_trials=1) figure = plot_intermediate_values(study)
# todo - update this to use content-disposition instead of file_name </s> urllib.urlretrieve(url, to_file)	_download_latest url = url + ("received_count=%s" % received_count) print "Hitting %s" % url print "Downloaded %s" % to_file
# todo - temporary workaround while yt bug not fixed </s> offset += limit	AddSubscriptions thumb=GetThumbFromSnippet(item), )) if 'pageInfo' in res and ( res['pageInfo']['totalResults'] > offset
# todo(yuriyz): change to 404 (bug 1200517) </s> self.assertequal(response.status_int, 500)	test_delete_port_byid response = self.get_json('/ports/%s' % pdict['uuid'], expect_errors=True) self.assertEqual(response.content_type, 'application/json') self.assertTrue(response.json['error_message'])
# todo: this is a jump. </s> vi_cmd_data['motion']['command'] = '_vi_forward_slash'	vi_forward_slash def vi_forward_slash(vi_cmd_data): vi_cmd_data['motion']['args'] = {'mode': vi_cmd_data['mode'], 'count': vi_cmd_data['count'], 'search_string': vi_cmd_data['user_motion_input']} vi_cmd_data['count'] = 1
# todo : documentation pending </s> weights_names = [w.name for w in weights]	save_weights_to_hdf5 def save_weights_to_hdf5(f, weights, sess=None): f.attrs['weights_names'] = weights_names  # 'layer_name/weight_name' save_val_list = tf_variables_to_numpy(weights, sess)
# todo: check output </s> def test_hierarchy_iprint2(self):	TestSolverPrint prob.setup(check=False) output = run_model(prob) prob = Problem() model = prob.model
# todo implement </s> return {}	cancel_subscription @param r: the S3Request instance @param attr: controller attributes
# todo: handle name clashing </s> self._file_cache[name] = file_path	wrapper else: file_path, file_hash = details['file_path'], details['file_hash'] return func(*args, **kwargs)
# todo (jack): add script to dump msmarco to ext_host:ext_port </s> proxy.start()	main args = parser.parse_args(argv) proxy = Proxy(**vars(args)) times = [[], []] for i in range(proxy.kwargs['laps']):
# todo: configurable timeout </s> tornado.ioloop.ioloop.instance().add_timeout(time.time() + 0.1, self.iter_events)	iter_events if e.errno != zmq.EAGAIN: raise Exception() except: print sys.exc_info(), 'exception in main wait loop'
# todo wtf is that/?? </s> ax.set(ylim=(none, 200))	plot_hr plt.figure(figsize=(15,4)) ax = sns.pointplot(tss, uu, markers=" ") plt.show()
# hack to support saving/loading pytorch models. todo: improve </s> if hasattr(layer, '_model') and not isinstance(layer._model, model):	to_bytes i = 0 for layer in queue: weights.append(layer.to_bytes()) elif hasattr(layer, u'_mem'):
# todo make this not terrible </s> m_response = requests.post('http://127.0.0.1:8000/webforms/get-xml/' + session_id)	render_xml def render_xml(request, domain, session_id): json_response = json.loads(m_response.text) form_data_xml = json_response["output"]
# todo: non-numeric columns should be ignored automatically </s> def test_impl(n):	test_sum1 def test_sum1(self): df = pd.DataFrame({'A': np.arange(n)+1.0, 'B': np.arange(n)+1}) return df.sum()
# todo: add for all fields </s> continue_system_importer_file_csv = false	check_row def check_row(request, row, row_counter, model): if not row[model.csv_column_system - 1]: messages.error(request, "Value for system in row " + str(row_counter) + " was an empty string. System not created.")
# todo email submitter </s> pass	notify_rejected def notify_rejected(self, event):
# todo: prune_services=false in future release </s> prune_services=none,	present prune_port_fwd=False, services=None, interfaces=None, prune_interfaces=False,
# todo: handle non-normalizable combining chars. probably need to use </s> text = unicodedata.normalize('nfc', text)[0]	render_cell def render_cell(text, bold=False, italic=False): width = wcwidth(text) bitmap_char = render_char(text, bold, italic, width)
# todo in python 2.7 and later, this should be </s> result['relationships'] = dict((rel, cr(model, instance, rel))	DefaultSerializer return result cr = create_relationship for rel in relations) return result
# todo: consider tag.blend_clipping_elements. </s> if len(layer.clip_layers):	_get_object shape = paste(self._viewport, layer.bbox, shape) alpha = shape * 1.  # Constant factor is always 1. color, _, _ = composite( layer.clip_layers,
# todo: review </s> headless = self.resource.get("dialect", {}).get("header") is false	read_byte_stream_create source = self.resource.source remote = self.resource.remote headless = headless or self.resource.format != "csv" byte_stream = MultipartByteStream(source, remote=remote, headless=headless)
# todo: remove at some point </s> if self.debug:	best_match def best_match(self, name, year=None): movies = self.search(name) import yaml print yaml.safe_dump(movies)
# todo: remove this log </s> print("gather node for data path '{}'".format(channels[0].data_path))	__gather_node export_settings ) -> gltf2_io.Node: if blender_object.type == "ARMATURE": blender_bone = blender_object.path_resolve(channels[0].data_path.rsplit('.', 1)[0])
# todo: ideally we'd know whether this was a folder </s> meta = self._tree_meta_cache.get(path, "")	get_entries path = os.path.join(base, name) state = self._tree_cache.get(path, STATE_NORMAL) yield Entry(path, name, state, isdir=False, options=meta)
# todo: #154 - some auto-updater logic? </s> try:	learn_from_teacher_node for version, node_bytes in versions_and_node_bytes: if version > 1:  # TODO: Unhardcode this; pass version from Learner to here somehow. nickname, _ = nickname_from_seed(checksum_address) display_name = cls._display_name_template.format(cls.__name__, nickname, checksum_address)
# todo alert? </s> _log.error("volttron agent user not found at {}".format(agent_dir))	remove_agent_user except KeyError:
if lang is none:  # todo: remove in v8 </s> utils.logger.warn("rendertags.slugify_tag_name() called without language!")	slugify_tag_name def slugify_tag_name(self, name, lang): lang = '' if self.site.config['SLUG_TAG_PATH']:
# todo error on missing levels </s> pass	determine_attributes markers = dict(zip(style_levels, self.default_markers)) elif isinstance(markers, dict): else: markers = dict(zip(style_levels, markers))
# todo: include regression tests for when tvtk is installed </s> os.chdir(curdir)	test_warppoints if m.no_tvtk(): yield assert_raises, ImportError, m.WarpPoints rmtree(tempdir)
# todo: should modify to parallel execution. </s> kwargs = {"seed_id_locations": secrets.randbits(64)}	gt_master p_kwargs={"a_shape": shape_x, "b_shape": shape_y}, ) res_shares = [a.__gt__(b, **kwargs) for a, b in zip(x.child, y.child)] return res_shares  # type: ignore
raise mpdnotimplemented # todo </s> def _sticker_list(self, type, uri):	_sticker_list @register(r'^sticker list "(?P<type>[^"]+)" "(?P<uri>[^"]+)"$')
# todo: add cn to domains? </s> crypto_util.get_sans_from_csr(	obtain_certificate_from_csr :rtype: tuple return self._obtain_certificate( csr.data, OpenSSL.crypto.FILETYPE_ASN1), csr)
w, h = tiledsurface.n, tiledsurface.n # todo: support for other sizes </s> thumbnail_pixbuf = self.save_doc_to_file(filename, self.scratchpad_doc, export=export, **options)	save_scratchpad x, y, w, h =  self.scratchpad_doc.model.get_bbox() if w == 0 and h == 0: if not export: self.scratchpad_filename = os.path.abspath(filename)
# todo: since these a delete request, shouldn't use request body. put pointer </s> pointer_node_id = request.json.get('pointernodeid')	remove_pointer_from_folder def remove_pointer_from_folder(auth, pid, **kwargs): in `node.nodes`. if pointer_node_id is None: raise HTTPError(http.BAD_REQUEST)
# todo: old requirement, remove in future versions? </s> self.signout	TestPrivacyWebPublic res = self.app.get(url, follow_redirects=True) dom = BeautifulSoup(res.data) self.signin(email=self.root_addr, password=self.root_password) res = self.app.get(url, follow_redirects=True)
# todo replace with to_bytes() when available in utils.py </s> if isinstance(password, six.text_type):	password_decrypt :return: The clear test :rtype: bytes password = password.encode('utf8') bkey = create_key_from_password(password)
# todo: retrieve old days count </s> 'days': 0	Defcon 'data': { 'enabled': True, } }
# todo: if "pkg" in facts["inspections"] then use pkgcopier. </s> if "relative_path" in facts:	generate_pkg_recipe } }) recipe.append_processor({ "Processor": "AppPkgCreator",
args.get('thread_config'),  # todo deprecate </s> args.get('max_workers'),	main args.get('timestamp'), args.get('services'), args.get('skipped_services'), args.get('regions'), args.get('fetch_local'), args.get('update'),
>>> from torch import nn  # todo: import nn for all doctests </s> >>> from typing import list	register_module_validator The signature of every validator is always the same: >>> import opacus  # TODO: import opacus for all doctests >>> @register_module_validator(nn.Linear)  # TODO: change nn.Linear to MyCustomModule mock ... def validate(module: nn.Module, **kwargs) -> List[opacus.validators.errors.UnsupportedError]:
#todo - reconsider this </s> self.tree.delete(child_id)	_clear_tree for child_id in self.tree.get_children():
# todo: check if valid cdxj here before returning </s> return filecontents	fetchRemoteCDXJFile fileContents = ipwbConfig.fetchRemoteFile(path) return fileContents
# todo: break out into separate view / template </s> 'node_registrations' : [	_view_project for meta in node_to_use.registered_meta or [] ], { 'registration_id' : registration._primary_key,
# todo: figure out way to paramaterize node['osds'] for this test </s> for osd in node["osds"]:	test_osd_services_are_running def test_osd_services_are_running(self, node, Service): assert Service("ceph-osd@%s" % osd).is_running
# todo: non-json response contents </s> return response.text	unmarshal_response_inner value=content_value, )
# todo: flag to expressionreplacementvisitor to only replace </s> for obj in list(instance.component_data_objects(objective,	setup_sensitivity remove_named_expressions=True, ) active=True, descend_into=True)):
## \todo there should really be a method to map from plug to parameter. </s> parameter = plug.node().parameterhandler().parameter()	__fixedLineHeight def __fixedLineHeight( plug ) : for name in plug.relativeName( plug.node() ).split( "." )[1:] : if not isinstance( parameter, IECore.CompoundParameter ) :
# todo: should use input_axes here, but the workflow always gives out </s> opreordercompare.axisorder.setvalue('zyxc')	_test_boundarybased_segmentation_with_multicut opReorderCompare = OpReorderAxes(parent=opReaderCompare) opReorderCompare.Input.connect(opReaderCompare.Output) compare = opReorderCompare.Output[:].wait() assert numpy.array_equal(result, compare)
#todo: check for continous or discrete, only continuous supported right now </s> dico = 'c'	gram Wc = gram(sys,'c') Wo = gram(sys,'o') D,V = np.linalg.eig(sys.A)
# todo: this is wrong. they must be motions. </s> def vi_ctrl_y(vi_cmd_data):	vi_ctrl_y vi_cmd_data['motion_required'] = False vi_cmd_data['_repeat_action'] = True
for node in pynode.gltf.scene.nodes.values(): # todo if parent is in another scene </s> if node.index == parent:	set_parent if parent is None: return if node.is_joint == True: bpy.ops.object.select_all(action='DESELECT')
# todo: create a queue for all orders and make it auto-complete when all the orders are processed </s> test_collector.collect(n_episode=10)	tianshou test_collector = Collector(policy, envs) policy.eval()
# todo: drop me after the domain-allocation switch, as this method </s> return tuple(j + i + k for i, (j, k) in zip(self.shape_domain, self._halo))	shape_with_halo Shape of the domain plus the read-only stencil boundary associated with this :class:`Function`.
# xxx/todo: remove this when sdk 1.4.3 is released </s> if key == 'prospective_search_path':	start_dev_appserver if isinstance(connection, DatabaseWrapper): for key, path in connection._get_paths().items(): continue arg = '--' + key
# todo(dcramer): we're selecting source twice which is a waste of resources </s> results = db.session.query(	get_passing_builds Build.date_created.desc(), ).limit(1).subquery() Project.id, aliased(Build, build_subquery),
# todo: we need a better way to create model instances and stay compatible with </s> state = instance_state(model)	create_model try: model = self._manager.new_instance() self._manager.dispatch.init(state, [], {}) form.populate_obj(model)
self.end(266) #todo# too long? </s> self.coverage()	test_5037_notify_service_functions_with_reload_user self.rm_testdir()
# todo: kill this </s> inspection = inspection[0]	domain_is_redirect if not inspection: return False return (inspection.get("Redirect") is True)
# todo: rewrite tests </s> pass	test_resend_confirmation_get def test_resend_confirmation_get(self):
# todo: update cache </s> pass	update_app_service Args: service(ApplicationService): The updated service.
# todo(samueldmq): change the below to get_head_action for </s> get_action='list_grants',	append_v3_routers path='/OS-INHERIT/domains/{domain_id}/users/{user_id}/roles/' 'inherited_to_projects', rel=build_os_inherit_relation( resource_name='domain_user_roles_inherited_to_projects'),
# todo: raise warning if computed output is already in cache. </s> cache.update(zip(step.outputs, listify(output_data)))	fit else: raise TypeError('{} does not implement predict or transform!'.format(step.name))
# todo replace all that with ssh-copy-id </s> server.append("mkdir -p ~/.ssh")	install_keys_on_host def install_keys_on_host(hostname, sshkeydata): server = utils.ScriptRunner(hostname) server.append("chmod 500 ~/.ssh") server.append("grep '%s' ~/.ssh/authorized_keys > /dev/null 2>&1 || "
# todo: make legacy detection non-reliant on side </s> if master_namespace not in original:	rewrite_config with open(filepath, 'r') as config: original = yaml.safe_load(config) original = {MASTER_NAMESPACE: original} else:
# todo: estimate fees </s> if input_arr is none:	create_transaction if not utxos: return None selected_utxos = self._select_inputs(total_amount, utxos) from pprint import pprint
# todo: exit codes are currently ignored on windows. </s> @skip_win32	test_exit_code def test_exit_code(self): ip.system_piped('exit 1')
# todo: use value_op for this type of retrieval instead </s> x1_val, x2_val = x1.value.tensor, x2.value.tensor	test_sequential_side y_val = main_effect() assert np.allclose(y_val, y_np) x1_np = x_np.sum() + (x_np.sum() + 2) x2_np = x_np.mean() + (x_np.mean() + 3)
'''todo: add docs''' </s> self.views.append(view)	register_view_for_update def register_view_for_update(self, view):
# todo: exact match. </s> raise keyerror(key)	_find return elif page.is_leaf(): else: if entry_number == 0:
limit = 20  # todo: change to setting </s> opts = '\n'.join('%s. opt' % x for x in range(limit))	test_markdown_poll_choice_limit_ok def test_markdown_poll_choice_limit_ok(self): Should not exceed the limit comment = "[poll name=foo]\n" + opts + "\n[/poll]" md = Markdown(escape=True, hard_wrap=True)
# todo add installation logic for torch </s> result.append(pkey + '==' + pversion)	generate (pkey == 'tensorflow' or pkey == 'tf-nightly'): pkey = pkey + '-gpu' return result
# todo: implement subdomains for slate tensors </s> if kinfo.subdomain_id != "otherwise":	_organize_assembly_calls kinfo = split_kernel.kinfo if kinfo.kernel: raise NotImplementedError("Subdomains not implemented.") args = [c for i in kinfo.coefficient_map
# todo: in 0.6.0 change this to "disabled": false </s> "enabled": true,	test_server_bridge_proxy "dev_type": "tap", "dh": "dh.pem", "key": "key.pem", "mode": "server",
# todo: this switch between 64 and 128 is a hack for now. we should have a separate cli option for size </s> image = rotate_image(image, face.r * -1)	convert image = rotate_image(image, face.r) image = converter.patch_image(image, face, 64 if "128" not in self.arguments.trainer else 128) else: image = converter.patch_image(image, face, 64 if "128" not in self.arguments.trainer else 128)
# todo(datapipe-1509|abrar): currently we have </s> raise importerror	get_connection try: if force_avoid_internal_packages: from replication_handler.models.connections.yelp_conn_connection import YelpConnConnection return YelpConnConnection(
# todo: delete from application_services_regex where id=this service </s> pass	update_app_service Args: service(ApplicationService): The updated service.
# todo: fix clone issue </s> assert self.x_train.shape[0] > cof_.n_neighbors_	test_check_parameters cof_.fit(self.X_train)
# todo(iceboy): projection. </s> rdoc['udoc'], rdoc['pdoc'] = await asyncio.gather(	RecordMainConnection async def on_record_change(self, e): rdoc = await record.get(objectid.ObjectId(e['value'])) user.get_by_uid(rdoc['uid']), problem.get(rdoc['domain_id'], rdoc['pid'])) self.send(html=self.render_html('record_tr.html', rdoc=rdoc))
# todo(#362): add server authentication with thrift 0.12. </s> transport = thttpclient(url)	get_http_transport url = 'https://%s:%s/%s' % (host, port, http_path) log.debug('get_http_transport url=%s', url) else: url = 'http://%s:%s/%s' % (host, port, http_path)
# todo : documentation pending </s> if not isinstance(variables, list):	tf_variables_to_numpy def tf_variables_to_numpy(variables, sess=None): var_list = [variables] else:
# todo: checking for executability is a hack; use file extension </s> if os.access(self._script_path, os.x_ok):	_executable def _executable(self, steps=False): return [os.path.join(self._working_dir, self._wd_mgr.name('file', self._script_path))]
# @todo this needs to be using domain fronting to defeat censorship </s> response = requests.get(endpoint)	censorship_obtain_builtin_bridges Retrieves the list of built-in bridges from the Tor Project. endpoint = "https://bridges.torproject.org/moat/circumvention/builtin" self.censorship_builtin_bridges = response.json() self.log(
"""@todo add progressbar for multisite. ensure the other one is hidden first.""" </s> try:	test_progressbar_url_file_hidden_in_ennumerate_plugins @patch.object(ProgressBar, 'set') def test_progressbar_url_file_hidden_in_ennumerate_plugins(self, p): self.scanner.enumerate_plugins(self.base_url, self.scanner.plugins_base_url, hide_progressbar=True)
# todo: add classname if bound method </s> def contracts_checker(unused, *args, **kwargs):	contracts_decorate for x in accepts_dict]) is_bound_method = 'self' in all_args do_checks = not all_disabled() if not do_checks:
# todo check behavior when not loaded </s> return album(sp_album=lib.sp_albumbrowse_album(self._sp_albumbrowse))	album @property def album(self):
# todo: determine actual time left. </s> time_left = 60	cool_your_jets def cool_your_jets(err): Sent by Flask Limiter. message = ('Cool your jets! Please wait {0} seconds before making' ' another search.').format(time_left)
# todo replace with more general current_expression_attribute </s> match = current_word(cursor_offset, line)	current_object_attribute def current_object_attribute(cursor_offset, line): if match is None: return None
# todo(slaweq): change this to neutron floating ips and turn neutron </s> mock_nova.floating_ips.list.return_value = [fake_floating_ip]	test_create_server_no_wait mock_nova.servers.create.return_value = fake_server mock_nova.servers.get.return_value = fake_server self.assertEqual( self.cloud._normalize_server(
# todo: below lines are commented out to ensure that </s> return super(guidmixinqueryset, self).get(*args, **kwargs)	get self.query.add_distinct_fields('id')
ridedatafileset(item=self).publish() #todo: use a more gentle message </s> return list	sort_keywords list = self.keywords.sort() self.mark_dirty() return None
# todo: we should should distinguish sub-subflows here </s> return storagepool.retrieve(finished_name, self.finished[finished_name])	finished_task_result :param finished_name: name of finished node in parent subflow :return: result of finished task in parent subflow
# todo: rf to use --batch where possible instead of splitting </s> return self._run_command_files_split(	_run_annex_command env = self.add_fake_dates(env) try: self.cmd_call_wrapper.run, cmd_list,
# todo(b/160795287): deprecate estimator based executor. </s> fn_args_dict = fn_args.get_dict()	run_fn fn_args: Holds args used to train the model as name/value pairs. schema = io_utils.parse_pbtxt_file(fn_args.schema_file, schema_pb2.Schema()) fn_args_dict['serving_model_dir'] = os.path.join(fn_args.model_run_dir, path_utils.SERVING_MODEL_DIR)
total = db.get_count(query=query)  # todo(nsatterl): possible race condition? </s> found = 0	get_alerts alert_details = list() alerts = db.get_alerts(query=query, limit=limit) if len(alerts) > 0: severity_count = defaultdict(int)
# todo: add 'downcast' when value parameter exists </s> return dataframe(internal)	fillna else:
# todo add validation tests </s> return errors	validateLink def validateLink(link): errors = []
# todo make this cancellable with is_cancellable_behavior </s> @connection.on_connection_thread()	BehaviorComponent look_around_in_place_request = protocol.LookAroundInPlaceRequest() return await self.grpc_interface.LookAroundInPlace(look_around_in_place_request) async def find_and_roll_block(self) -> protocol.RollBlockResponse: This behavior will move into position as necessary based on relative
#@todo: move to utils in 0.4.10 </s> def timestamp():	timestamp return int(time.time() * 1000)
# todo(b/145514490): this is a bit heavy handed, there maybe caches where </s> self._cache = {}	CachingExecutor await cached_value.target_future except Exception as e: raise e return cached_value
# todo: replace with path transformation functions </s> saved_path = xl_workbook.properties().get(kw.path)	save_workbook def save_workbook(xl_workbook, path): if (saved_path != '') and (path is None): xl_workbook.save()
assert len(config['sources']) == 1  # todo: merge multiple sources </s> for source in config['sources']:	make_tasks query = dict(time=(datetime(2011, 1, 1), datetime(2011, 2, 1))) workflow = GridWorkflow(index, grid_spec=get_grid_spec(config)) data = workflow.list_cells(product=source['product'], cell_index=(15, -40), **query) masks = [workflow.list_cells(product=mask['product'], cell_index=(15, -40), **query)
#todo: remove expressions </s> secondnotenachschlag.duration.quarterlength = self.quarterlength	Trill firstNoteNachschlag.duration.quarterLength = self.quarterLength secondNoteNachschlag = copy.deepcopy(srcObject) secondNoteNachschlag.transpose(transposeIntervalReverse, inPlace = True) nachschlag = [firstNoteNachschlag, secondNoteNachschlag]
# todo make this configurable </s> if not user.spoken_recently:	_leave if message: tags = self._message_tags(user, leave_type) tags.append(SCRIPT_NAME + "_smart_filter") message = self._membership_message(user, leave_type)
# todo allow repose (is not affected by createfrompose) </s> return self.headpos.copy()	getRestHeadPos def getRestHeadPos(self): The head position of this bone in world space.
# todo: this updates the resolution upon initialization and makes the </s> self.scale.set_value(xres)	_on_resolution_changed def _on_resolution_changed(self, resolution, pspec): xres, __ = resolution.resolution
# todo: udpoutgoing style buffer </s> for s in r:	udp_select if not r and not e: return while True: try:
pass  # todo: why ignore unicodedecodeerror? </s> for elt in modelxbrl.modeldocument.xmlrootelement.iter("{http://www.w3.org/1999/xhtml}a", "{http://www.w3.org/1999/xhtml}img"):	referencedFiles addReferencedFile(fact, elt) except (XMLSyntaxError, UnicodeDecodeError): addReferencedFile(elt, elt) return referencedFiles
# todo: these 2 little simplifications can reduce test time by 30-40%, to do in test framework </s> save(client.cache.settings_path, "")	test_recipe_modes configs.append((mode, "liba/1.1.1@user/stable", "b41d6c026473cffed4abded4b0eaa453497be1d2")) client = TestClient() save(client.cache.default_profile_path, "") def _assert_recipe_mode(liba_ref, package_id_arg):
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> tests happy path plus a call to the cache only function acquiretoken which should find the token from the	test_happy_path_cached_token_2 @httpretty.activate def test_happy_path_cached_token_2(self): previous call to acquire_token_with_client_credentials. response_options = { 'noRefresh' : True }
# todo: adjust dimension order for tf2 broadcasting </s> return tf.compat.v1.where(	maybe_sample auxiliary_inputs) if self.next_inputs_fn is None: sample_ids, maybe_concatenate_auxiliary_inputs(outputs), base_next_inputs)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_transaction_null def test_fail_transaction_null(self): ``transaction`` is null.
# todo: discriminate between worksheet & workbook ranged names </s> self.rangednames = np.zeros(shape = (int(self.app.activeworkbook.names.count),1), dtype=[('id', 'int_'), ('name', 's200'), ('formula', 's200')])	get_rangednames def get_rangednames(self): for i in range(0, self.app.ActiveWorkbook.Names.Count): self.rangednames[i]['id'] = int(i+1)
# todo: fix this in a cleaner way </s> if isinstance(evidence_card, np.ndarray):	TabularCPD if evidence is not None: if not isinstance(evidence, (list, set, tuple)): evidence_card = list(evidence_card) elif isinstance(evidence, str):
# todo: does this need to be made more efficient? </s> p1 = future.futureparser()	compile elif mode == "exec": graph = pyassem.PyFlowGraph("<module>", as_tree.filename) p2 = future.BadFutureParser() walk(as_tree, p1)
# todo add </s> pass	set_startup_hook def set_startup_hook(function=None):
# todo configurable </s> ssh_config_file = os.path.join(os.path.expanduser("~"), ".ssh", "config")	paramiko_connection if host is None: host = task.host client = paramiko.SSHClient() client._policy = paramiko.WarningPolicy()
except exception:  # todo - which exceptions? </s> return ''	get_self_selection try: return w.selection_get()
# todo hotfix for unnecessary weights in old adapters </s> unexpected_keys = [k for k in loading_info["unexpected_keys"] if "adapter_attention" not in k]	test_load_lang_adapter_from_hub adapter_name = model.load_adapter("fi/wiki@ukp", "text_lang", config=config, loading_info=loading_info) self.assertEqual(0, len(loading_info["missing_keys"])) self.assertEqual(0, len(unexpected_keys)) self.assertIn(adapter_name, model.config.adapters.adapters)
# todo: is there a nicer way to do this? if i add a new grep plugin i won't </s> plugin.end()	test_options_for_grep_plugins plugin.get_long_desc()
#todo move this up to not be a nested method </s> def circular_sequence(seq):	circular_sequence i = 0 while True:
#todo - complete implementation of these apis </s> return faults.fault(faults.alreadyattached(e))	attach_resource except exception.AlreadyAttached as e:
# todo: use nestedbuffers instead of saving by value </s> self.pubexp = self[pubexp_start:modulus_start][::-1][-4:]	PubkeyEntry pubexp_start = 0x40 modulus_start = pubexp_start + pubexp_size self.modulus = self[modulus_start:modulus_start + modulus_size][::-1] self.version = struct.unpack('<I', self[0x0:0x4])[0]
# todo: clear last object inspector requests dictionary </s> pass	_reset_environment def _reset_environment(self):
# todo implement. </s> import numpy as np	generate_code from keras import *
async_pub['tag'],  # todo: fix </s> async_pub['jid'],  # todo: fix	wrapper low, user, False,  # Don't daemonize
# todo: check to make sure time points match </s> self.xout = np.array(x)	__init__ self.noutputs = 1 if len(self.yout.shape) < 2 else self.yout.shape[0] self.ninputs = 1 if len(self.yout.shape) < 3 else self.yout.shape[-2] self.nstates = self.xout.shape[0] self.uout = np.array(u)
#todo: check the data! </s> count = 0	test_filtered_multiple_sources pipe_def = self._get_pipe_def("pipe_c1cfa58f96243cea6ff50a12fc50c984.json") p = pipe2py.compile.parse_and_build_pipe(pipe_def, verbose=True) for i in p: count += 1
# todo assert cls.__tablename__ == '' </s> with dbcleanup(session, cls):	test_HistoryDatasetAssociationTagAssociation model, session, history_dataset_association, tag, user): cls = model.HistoryDatasetAssociationTagAssociation user_tname, value, user_value = 'a', 'b', 'c' obj = cls(user=user, tag_id=tag.id, user_tname=user_tname, value=value)
# todo: unit tests </s> user = auth.user	get_all_projects_smart_folder @must_be_logged_in def get_all_projects_smart_folder(auth, **kwargs): contributed = user.node__contributed nodes = contributed.find(
pass # todo </s> if '%s' in args:	parse_exec pass # TODO generate tmp page if '%d' in args: if hasattr(page, 'source') and isinstance(page.source, File): args[args.index('%s')] = page.source.path
# todo: make return values consistent across both *repo classes! </s> return [{u'file': f, u'success': true} for f in files]	add msg = self._get_added_files_commit_msg(files) self.commit(msg=msg)
# todo: move this into onnx main library </s> pass	get_type def get_type(onnx_type):
# todo: this should be a separate test </s> self.asserttrue(iterable.closed, true)	test_captures_error_in_iteration with self.assertRaises(ValueError): response = list(response) self.assertEquals(len(self.client.events), 1) event = self.client.events.pop(0)
# todo:@zhui add support for 'mean', 'max', 'min' function. </s> assert reduce_func == "sum", "only implement 'sum' function right now. maybe you can update paddlepaddle version to fix this problem."	_send_recv feature (Tensor): the node feature of a graph. reduce_func (str): 'sum', 'mean', 'max', 'min' built-in receive function. src, dst = self.edges[:, 0], self.edges[:, 1] msg = self.send(
# todo(eric ayers) not really part of the test, just to detect the cache poisoning </s> after_support_dir = config.from_cache().getdefault('pants_supportdir')	DISABLED_test_gen_tasks_options_reference_data Goal.by_name('jack').install(TaskRegistrar('jill', DummyTask)) oref_data = reflect.gen_tasks_options_reference_data() self.assertEquals(before_support_dir, after_support_dir) self.assertTrue(len(oref_data) > 0,
# todo: funcbody </s> self.assertequal(14, p._pos)	testFunctionFancyArgs self.assertIsNotNone(node)
# todo: this is a work around for infinite blocking wait in storage </s> if self._parse_error_queue:	_Main self._storage_writer_complete_event.set() self._event_object_queue.Close() self._parse_error_queue.Close() logging.debug(u'Storage writer (PID: {0:d}) stopped.'.format(self._pid))
# todo: change config values instead of overwriting files on disk </s> conf_path = os.path.expanduser('~/.tcms.conf')	_fixture_setup tcms_api.TCMS._connection = None tcms_api.Config._instance = None conf_fh = open(conf_path, 'w') conf_fh.write("""[tcms]
# todo: duplicate + replace with psutil.getloadavg()? (available in 5.6.2) </s> load1m, _, _ = os.getloadavg()	set_turbo def set_turbo(): cpuload = p.cpu_percent(interval=1) if load1m > 2:
# todo move to another thread if we need to process messages </s> logger.debug(u'starting glib main loop')	run_inside_try def run_inside_try(self): self.setup() loop = gobject.MainLoop() loop.run()
# todo: stop ignoring these once we have proper handling for these messages. </s> ignored_commands = (commands.transactions, commands.newblock, commands.newblockhashes)	FastChainSyncer msg: protocol._DecodedMsgType) -> None: peer = cast(ETHPeer, peer) if isinstance(cmd, ignored_commands): pass
# todo: createpropertyconditionex with propertyconditionflags_ignorecase </s> conditions.append(self.iuia.createpropertycondition(self.uia_dll.uia_namepropertyid, name))	build_condition conditions.append(self.iuia.CreatePropertyCondition(self.UIA_dll.UIA_ControlTypePropertyId, control_type)) if name: if isinstance(content_only, bool): conditions.append(self.iuia.CreatePropertyCondition(self.UIA_dll.UIA_IsContentElementPropertyId,
#todo: define tests which check db contents </s> test_gdf = gdf() # test default configuration	test_GDF_get_ndarrays def test_GDF_get_ndarrays(self): "Test GDF get_ndarrays function" ndarray_dict = test_gdf.get_ndarrays(self.TEST_2D_DIMENSION_RANGE_DICT) ndarray_dict = test_gdf.get_ndarrays(self.TEST_2D_DIMENSION_RANGE_DICT, ndarray_type_tags=['LS5TM'])
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_interact_arguments @classmethod def add_interact_arguments(cls, parser): p_convert = p_operation.add_parser( "convert", help="convert VGM to PCM using OPL hardware")
# todo: may test with codecs.open passing an encoding </s> temp = tempfile.namedtemporaryfile(delete=false)	test_export_to_txt_fobj def test_export_to_txt_fobj(self): self.files_to_delete.append(temp.name) rows.export_to_txt(utils.table, temp.file)
# todo: remove this log once we find out what's causing oom </s> log.info('running readthedocs.projects.tasks.fileify. locals=%s', locals())	fileify return project = version.project if not commit: log.warning(
# todo: and netcdf writer will be more generic </s> pass	create_storage_unit_from_datasets except OSError:
# todo: find these references and ensure they are closed </s> if is_win:	close if self.git: self.git.clear_cache() gc.collect() gitdb.util.mman.collect()
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_interact_arguments @classmethod def add_interact_arguments(cls, parser): p_convert = p_operation.add_parser( "convert", help="convert VGM to PCM using OPL hardware")
# todo generator </s> installed_datasets = [r['path'] for r in get_results	__call__ **common_kwargs ) if r.get('type') == 'dataset' and r['status'] == 'ok'] failed = [r['path'] for r in get_results
# todo: summary hash for new current id </s> output_data_container.append(current_id, output.data_inputs, output.expected_outputs)	handle_transform context ) return output_data_container
# todo - make this work on loop with more than two links </s> flt_parallel = lambda loop: round(loop.calc_angle(),3) == 3.142	make_railing loops = list(set(loops)) if remove_colinear: flt_mid = lambda loop: loop.link_loop_next in loops and loop.link_loop_prev in loops loops = [l for l in loops if not (flt_parallel(l) and flt_mid(l))]
# todo: is region (lla | atn | odn | others?) important? </s> self._mqtt.ws_set_options(	__init__ ) self._mqtt.enable_logger() path="/chat?sid={}".format(self._session_id), headers=self._create_headers )
# todo: if first epsilon, repeat with smaller epsilons </s> return	GradientAttack _, is_adversarial = a.predictions(perturbed) if is_adversarial:
# time.sleep(40)  # todo: should remove after polling get. </s> res = res.reconstruct()	test_mpc_matmul_public res = op(mpc_tensor_1, value_2) res.block_with_timeout(secs=40) expected = op(value_1, value_2) assert (res == expected).all()
pass # todo </s> def handle_request(self, input):	handle_request
# todo: what actually raises valueerror in the following code? </s> try:	read def read(self, *args, **kwargs): result = super(GzipStreamFile, self).read(*args, **kwargs) if result is None:
# todo: determine if this is object store safe and what needs to be </s> files_path = "%s_files" % input_file[0:-len(".dat")]	__upload_input_files input_upload_response = self.client.upload_input(input_file) self.file_renames[input_file] = input_upload_response['path'] if os.path.exists(files_path): for extra_file in os.listdir(files_path):
# todo(slaweq): change this to neutron floating ips and turn neutron </s> mock_nova.floating_ips.list.return_value = [fake_floating_ip]	test_rebuild_server_wait mock_nova.servers.rebuild.return_value = rebuild_server mock_nova.servers.list.return_value = [active_server] self.cloud.name = 'cloud-name' self.assertEqual(
# todo: confirm change made it to elasticserach </s> self.assertequal(self.domain, change_meta.domain)	test_xform_pillow_sql self.assertEqual(form.form_id, change_meta.document_id)
# todo: remove this monkeypatch once upstream class is fixed. </s> _patch_zone(zone)	_create_record ttl = self.ttl with localzone.manage(self.filename, self.origin, autosave=True) as zone: if zone.add_record(name, rtype, content, ttl=ttl):  # pylint: disable=no-member result = True
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_tags_contents_invalid def test_fail_tags_contents_invalid(self):
# todo: check degree minute second formats? </s> self._check_latitude_longtiude_range(geopoints)	GeoPointType geopoints = [decimal.Decimal(points[0].strip()), decimal.Decimal(points[1].strip())] return geopoints except decimal.DecimalException, e:
# todo: keep map sorted chronologically </s> for script in self._revision_map.values():	_get_origin def _get_origin(self): if script.downgrade is None \ and script.upgrade in self._revision_map:
# todo: implement bip45/67/electrum/? </s> if self.scheme == 'bip32':	updateutxos network, account_id, acckey = self._get_account_defaults(network, account_id) if depth is None: depth = 5 else:
# todo implement. </s> raise notimplementederror	convert_to_weights def convert_to_weights(self, sample):
# todo at_list </s> f.write(u" at todo")	print_Show f.write(' '.join(stmt.imspec[0])) if stmt.imspec[3] is not None: if stmt.imspec[2] is not None: f.write(u" as %s" % (stmt.imspec[2], ))
#rnn_cell = tf.nn.rnn_cell.dropoutwrapper(rnn_cell, input_keep_prob=1.0, output_keep_prob=1.0)  # todo: custom values (warning: no dropout when testing !!!, possible to use placeholder ?) </s> rnn_cell = tf.nn.rnn_cell.multirnncell([rnn_cell] * self.args.num_layers, state_is_tuple=true)	_build_network return tf.matmul(X, W) + b  # [batch_size, NB_NOTE] rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(self.args.hidden_size, state_is_tuple=True)  # Or GRUCell, LSTMCell(args.hidden_size) initial_state = rnn_cell.zero_state(batch_size=self.args.batch_size, dtype=tf.float32) def loop_rnn(prev, i):
# todo: assert metrics. </s> metrics = code_analysis_server.metrics(	test_get_metrics error = repository.get_metrics(commit, metrics["spaces"]) assert not error "file.cpp", int i = 1;
delattr(self, "runtime_coef") # todo, better way to pass variables from initialiszer </s> if(bias):	linear if hasattr(self, 'runtime_coef'): w *= self.runtime_coef bias = self.get_bias([output_dim], trainable=trainable) return tf.matmul(net, w) + bias
# todo: https://github.com/turicas/brasil.io/issues/210 </s> result = {	format_spreadsheet_rows_as_dict } This is an auxiliary method used by covid19.forms.StateSpreadsheetForm with the uploaded file 'total': {}, 'importados_indefinidos': {},
# todo: then we can pull the descriptor out of the tile_spec </s> su_descriptor = index_netcdfs([filename[7:]])[filename[7:]]	create_storage_unit except OSError: pass return StorageUnit([dataset.id for dataset in datasets], mapping,
# todo add locales </s> raise yunohosterror("domain_name_unknown", domain=domain)	domain_set_settings domains = _load_domain_settings() if not domain in domains.keys(): setting_set = False if ttl is not None:
# todo: large gains also expected when precalculating psi. </s> psi = tf_signal.frame(y, k, 1, axis=-1)[..., :t - delay - k + 1, ::-1]	get_correlations D = dyn_shape[1] T = dyn_shape[2] Psi_conj_norm = ( tf.cast(inverse_power[:, None, delay + K - 1:, None], Psi.dtype)
# todo legacy method to be removed/refactored </s> from corehq.apps.locations.models import location	locations @property def locations(self): from corehq.apps.commtrack.models import SupplyPointCase def _get_linked_supply_point_ids():
# resume normal sphinx.ext.autodoc operation </s> return super(functiondocumenter, self).format_name()	format_name Format the function name if not hasattr(self.module, '__func_alias__'): if not self.objpath: return super(FunctionDocumenter, self).format_name()
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo[k]: fix this properly. </s> if transaction.command == 'delete' and transaction.object_type == 'message':	_format_transaction_for_delta_sync if transaction.command != 'delete': delta['attributes'] = transaction.snapshot if transaction.snapshot and 'version' in transaction.snapshot: delta['version'] = transaction.snapshot.get('version')
# todo (fpliger):   this handles pandas api change so users do not experience </s> try:	AttrSpec def _generate_items(self, df, columns): if self.sort: df = df.sort_values(by=columns, ascending=self.ascending) except AttributeError:
# todo remove me (das2 experiment) </s> if address[0] == "130.161.211.209":	send def send(self, address, data): print "%.1f %30s -> %15s:%-5d %4d bytes" % (time(), "???", address[0], address[1], len(data)) with self.sendqueue_lock:
# todo: is that right? </s> pass	unindex_documents es.delete(index, doc_type=Document._meta.db_table, id=doc_id) except pyes.exceptions.NotFoundException:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
#@todo: move to utils in 0.4.10 </s> def timestamp():	timestamp return int(time.time() * 1000)
# todo change to native framework call, when plex allows token in header </s> opener = urllib2.build_opener(urllib2.httphandler)	setWatched else: print 'Ged do alternative http get' request = urllib2.Request(target) request.add_header(
# todo: just access the original event position, rather </s> p1 = np.array(event.last_event.pos)[:2]	PanZoomCamera event.handled = True elif 2 in event.buttons: p2 = np.array(event.pos)[:2] p1c = event.map_to_canvas(p1)[:2]
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pcmpeqb res = 0x000000ff0000ff00ff00000000ffff00 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo: change logic to support "--submit" </s> for group in groups:	UnmapNetwork else: groups = [groups] op = opcodes.OpNetworkDisconnect(group_name=group, network_name=network,
# todo/fixme: are these correct.. </s> return np.linalg.inv(wishart_rand(nu, v))	invwishart_rand def invwishart_rand(nu, V):
# todo: duplicate + replace with psutil.getloadavg()? (available in 5.6.2) </s> load1m, _, _ = os.getloadavg()	set_turbo def set_turbo(): cpuload = p.cpu_percent(interval=1) if load1m > 2:
# todo featureparams nameids </s> t.table.featurelist.maplookups(lookupmap)	_preMerge lookupMap = dict(enumerate(t.table.LookupList.Lookup))
# todo: revert this. </s> from astropy.tests.helper import quantity_allclose	calculate_distance A new frame instance with all the attributes of the original but now with a third coordinate. if (isinstance(self._data, SphericalRepresentation) and not (self.distance.unit is u.one and quantity_allclose(self.distance, 1*u.one))):
start_time_system_s = none  # todo </s> start_time_synced_s = none  # todo	_recording_update_legacy_from_v1_15_to_pprf_2_0 info_csv = utils.read_info_csv_file(rec_dir) recording_uuid = None #TODO duration_s = None  # TODO recording_software_name = None  # TODO
"tabsize": 4,  # todo: fetch these from the project settings / global settings </s> "insertspaces": true	LspFormatDocumentCommand }, "options": { } }
# todo: account for distance from mid </s> total_bid_size = books.bids.apply(lambda x: x.amount.sum())	get_imbalance def get_imbalance(books): Returns imbalances between bids and offers for DataFrame of book data total_ask_size = books.asks.apply(lambda x: x.amount.sum()) return total_bid_size - total_ask_size
# todo only return results within uri roots given by ``uris`` </s> if not query:	search def search(self, query=None, uris=None): return self._get_all_tracks() uris = query.get('uri', [])
# todo: cannot be loaded with plugins; improve this solution </s> pd = helpers.import_from_plugin("pandas", plugin="pandas")	create_parser def create_parser(self, resource): try: if resource.format == "pandas" or isinstance(resource.source, pd.DataFrame): return PandasParser(resource)
# todo: uncomment when adding support for literal hex bytes </s> print(bytearray(b'hello world   ').islower())	test_islower print(bytearray(b'hello world').islower())
# todo: rewrite using six.b() </s> result_list = []	form_as_bytes def form_as_bytes(self): for x in self.__convert_to_list(): if isinstance(x, type(six.b("blah"))):
uploader.upload_file(file, container='export') # todo: right container folder?! </s> finally:	export_csv zip.close() file = FileStorage(filename='%d_%s_task_csv.zip' % (app.id, name), stream=zipped_datafile) zipped_datafile.close() finally:
# todo: avoid dummy and generate func here when inlining is possible </s> def _impl(df, axis=none, skipna=none, level=none, numeric_only=none,	_impl min_count=0): return hpat.hiframes.pd_dataframe_ext.prod_dummy(df)
return skiptest("test doesn't pass yet")  # todo(frostig) </s> def fun(x):	testLogSoftmax def testLogSoftmax(self): return x - np.log(np.sum(np.exp(x))) pfun, axis_name = papply(fun, 5)
# todo better check would be if the node is linked to the output and actually used </s> return utils_node.has_nodes(node_tree, "luxcorenodetexrandomperisland", true)	uses_random_per_island def uses_random_per_island(node_tree):
# todo: non-numeric columns should be ignored automatically </s> def test_impl(n):	test_var1 def test_var1(self): df = pd.DataFrame({'A': np.arange(n)+1.0, 'B': np.arange(n)+1}) return df.var()
description = ''  # todo(wking): store descriptions </s> self[key] = description	_handler_repository_created self, sender, namespace, repository, value): key = '{0}/{1}'.format(namespace, repository) self.save()
# todo document </s> def big_mip(subsystem):	big_mip cm = subsystem.network.connectivity_matrix num_components, _ = connected_components(cm) if cm != None else (1, None)
).consume()  # todo see issue 170 </s> if instance.get("securitygroups"):	load_ec2_instances AWS_ACCOUNT_ID=current_aws_account_id, aws_update_tag=aws_update_tag, for group in instance["SecurityGroups"]: neo4j_session.run(
# todo scope the cleanup to the current project - https://github.com/lyft/cartography/issues/381 </s> cleanup_gcp_instances(neo4j_session, common_job_parameters)	sync_gcp_instances instance_list = transform_gcp_instances(instance_responses) load_gcp_instances(neo4j_session, instance_list, gcp_update_tag)
# todo: error handling </s> backend = plugin.get(backend, backend)()	Graph super(Graph, self).__init__() if not isinstance(backend, Backend): self.__backend = backend self.__identifier = identifier # TODO: Node should do this
# todo(kpy): this only works for subdomains that have a single fixed </s> if date:	to_local_time def to_local_time(self, date): current subdomain.  For convenience, returns None if date is None.""" if self.config.time_zone_offset: return date + timedelta(0, 3600*self.config.time_zone_offset)
# todo: one day this can be removed (once all our users have updated) </s> old_colab_dir = os.path.realpath(os.path.expanduser(os.path.join('~', '.floobits')))	main sublime.log = lambda d: G.CHAT_VIEW and G.CHAT_VIEW .run_command('floo_view_set_msg', {'data': d}) utils.reload_settings() if os.path.isdir(old_colab_dir) and not os.path.exists(G.BASE_DIR): print('renaming %s to %s' % (old_colab_dir, G.BASE_DIR))
# todo new message here </s> self.failure_publisher.publish_message(message(	send_failure def send_failure(self, message, reason=None): log.msg("Failed to send: %s reason: %s" % (message, reason)) message=message.payload, reason=reason), require_bind=False)
# todo: xxx </s> pass	test_rss_online @attr(online=True) def test_rss_online(self):
# todo(mattjj): if we instead lower directly to lax.gather, we can probably </s> out = lax.index_take(arr_sliced, flat_idx, axes)	_rewriting_take flat_idx = tuple(mod(ravel(x), arr_sliced.shape[i]) for i, x in zip(axes, idx_advanced)) shape_suffix = tuple(onp.delete(_shape(arr_sliced), axes)) out = lax.reshape(out, idx_advanced[0].shape + shape_suffix)
# todo(nmigen-0.2): remove this </s> @classmethod	wrap @deprecated("instead of `Statement.wrap`, use `Statement.cast`") def wrap(cls, obj):
# todo this should be a return and printed elsewhere </s> print('area under roc curve (auc): %0.2f' % area)	roc_curve fpr, tpr, thresh = skmetrics.roc_curve(true_values, predictions) area = skmetrics.auc(fpr, tpr) d = (fpr - 0) ** 2 + (tpr - 1) ** 2 ind = np.where(d == np.min(d))[0]
# todo(tianjianlu): fails on a100 gpu. </s> @jtu.skip_on_devices("gpu")	testQdwhWithOnRankDeficientInput for m, n in zip([10, 12], [10, 12]) for log_cond in np.linspace(1, 4, 4))) def testQdwhWithOnRankDeficientInput(self, m, n, log_cond): a = jnp.triu(jnp.ones((m, n))).astype(_QDWH_TEST_DTYPE)
# todo: remove in v1.2 </s> def test_radius_neighbors_classifier_kwargs_is_deprecated():	test_radius_neighbors_classifier_kwargs_is_deprecated extra_kwargs = { "unused_param": "",
# todo: worry about concurrency </s> with open(path, "r+b") as file:	create_dummy def create_dummy(self, path): self.api.upload(file, path) os.remove(path)
# todo(rbharath): there should be some automatic check to ensure that all </s> model_params = {"nb_hidden": 10, "activation": "relu",	test_multitask_keras_mlp_ECFP_classification_hyperparam_opt input_transformers = [] task_type = "classification" "dropout": .5, "learning_rate": .01, "momentum": .9, "nesterov": False,
# todo ... </s> print()	main def main(): print("Find odd numbers via method:") print("Find divisible by 6 via lambda:") print()
# todo: change logic to c_leq based on benchmarking </s> transformationfactory('contrib.deactivate_trivial_constraints')\	solve_NLP_subproblem fix_nlp.tmp_duals[c] = c_geq * max( 0, c_geq*(rhs - value(c.body))) .apply_to(fix_nlp, tmp=True, ignore_infeasible=True) with SuppressInfeasibleWarning():
# todo(ochang): remove this once migrated to python 3. </s> stream = io.bufferedwriter(io.bytesio())	run_one_test_parallel suite = unittest.loader.TestLoader().loadTestsFromNames(test_modules) if sys.version_info.major == 2: else: stream = io.StringIO()
# todo handle algorithm </s> cipher = self._block(key)	decrypt_bulk :return: Plaintext data :rtype: bytes ciphertext = edata[cipher.NONCE_SIZE:] nonce = edata[:cipher.NONCE_SIZE]
# todo: reproduce and submit traceback to issue 41 </s> hosts = host.filter_by_fqdn(hostname)	_update def _update(hostname, ipaddr): ipaddr = str(ipaddr)  # bug in dnspython: crashes if ipaddr is unicode, wants a str! num_hosts = len(hosts) if num_hosts == 0:
# todo: add location info </s> bvs_part = self.unsafe_arith.parsevarref(val.s, runtime.no_spid)	_ResolveNameOrRef new_name = val.s if 0:  # for declare -n log('bvs %s', bvs_part) else:
# todo: a lousy way of propagating what will usually be </s> log.error("error running realm %s: %s", argv, msg)	Realm log.info("Realm discover stderr:\n%s" % stderr) except OSError as msg: return self.packages = ["realmd"]
# todo model? </s> cut_result_json = os.path.join(data_home, 'cut_result.json')	classify compress_rate: float = 0.2, limit: int = None): res = None stable = None
# todo(b/160795287): deprecate estimator based executor. </s> fn_args_dict = fn_args.get_dict()	run_fn fn_args: Holds args used to train the model as name/value pairs. schema = io_utils.parse_pbtxt_file(fn_args.schema_file, schema_pb2.Schema()) fn_args_dict['serving_model_dir'] = os.path.join(fn_args.model_run_dir, path_utils.SERVING_MODEL_DIR)
# todo(guillermooo): remove this by 1.0 </s> transplant_settings('preferences.sublime-settings',	plugin_loaded def plugin_loaded(): 'Dart - Plugin Settings.sublime-settings') pass
# todo(stevemar): assert returned fields </s> self.openstack('container show ' + self.container_name)	test_container_show def test_container_show(self):
# todo: set optimum flags for platform+compiler combo, see </s> spec = self.spec	setup_environment def setup_environment(self, spack_env, run_env): spack_env.set('CC', spec['mpi'].mpicc) spack_env.set('FC', spec['mpi'].mpifc)
#todo: check this with robot </s> translation, angles_as_deg = dco.transformation_matrix_to_coordinates(m_robot_new, axes='sxyz')	compute_head_move_compensation ) m_robot_new = M_current_head @ m_change_robot_to_head new_robot_position = list(translation) + list(angles_as_deg) return new_robot_position
# todo: can be done faster by custom code </s> return self.getelemfirstindexfromline(line) == line	isFirstLineOfElem def isFirstLineOfElem(self, line):
# todo(jk0): this will eventually need to take ssl into consideration </s> return "http://%s:%d" % (flags.glance_host, flags.glance_port)	_construct_glance_url def _construct_glance_url():
# todo: list is incomplete, to be completed for missing languages. </s> self.doc_subpages = {	__init__ 'simple': self.alphabetic } '_default': ((u'/doc', ), ['en']
# todo: use optparse command options instead. </s> project_dir = sys_argv[1]	run_tests sys_argv: a reference to sys.argv. try: sys_argv.pop() except IndexError:
log_importance_weight = none  # todo: check the reason/behavior for this </s> variable = variable(distribution=distribution, value=value, address_base=address_base, address=address, instance=instance, log_prob=log_prob, log_importance_weight=log_importance_weight, observed=true, name=name)	observe log_importance_weight = float(log_prob) else: _current_trace.add(variable)
raise exceptions.mpdnotimplemented  # todo </s> .. versionadded:: mpd protocol 0.19	addtagid removed from the queue.
raise mpdnotimplemented # todo </s> def _sticker_get(self, type, uri, name):	_sticker_get @register(r'^sticker get "(?P<type>[^"]+)" "(?P<uri>[^"]+)" "(?P<name>[^"]+)"$')
# todo: requires special treatment? </s> current_unit = unit_line.variants[0].line[0]	_unit_line_to_game_entity :type unit_line: ..dataformat.converter_object.ConverterObjectGroup if isinstance(unit_line, GenieVillagerGroup): else: current_unit = unit_line.line[0]
# todo: verify this is a windows image </s> valid_kernel = none	_method_layer_pdb_scan physical: bool = True, progress_callback: constants.ProgressCallback = None) -> Optional[ValidKernelType]: virtual_layer_name = vlayer.name physical_layer_name = self.get_physical_layer_name(context, vlayer)
# todo: move instead of copy to save time? </s> source_file = os.path.join( os.path.abspath( job_wrapper.working_directory ), hda_tool_output.from_work_dir )	build_command_line hda_tool_output = job_tool.outputs.get( joda.name, None ) if hda_tool_output and hda_tool_output.from_work_dir: if in_directory( source_file, job_wrapper.working_directory ): try:
# todo: remove when the time is right. </s> if policy.expiration > end_of_policies_probationary_period:	Alice self.remember_node(node=handpicked_ursula) policy = self.create_policy(bob=bob, label=label, **policy_params) raise self.ActorError(f"The requested duration for this policy (until {policy.expiration}) exceeds the " f"probationary period ({END_OF_POLICIES_PROBATIONARY_PERIOD}).")
# todo: test for the _correct_ revision_id value. </s> if not activity.revision_id:	_update_resource if not activity.id: assert False, "activity object has no id value" assert False, "activity has no revision_id value" assert activity.timestamp >= before and activity.timestamp <= after, \
# todo: need support mint and other distro based on ubuntu. </s> if gnomeversion.distributor == 'ubuntu':	parse_codename def parse_codename(): data = open('/etc/lsb-release').read() dict = {}
# todo: currently mnn python binding have mem leak when creating mnn.tensor </s> tmp_input_shape = (batch, height, width, channel)	validate_yolo_model_mnn image_data = preprocess_image(img, model_input_shape) image_shape = img.size[::-1] input_elementsize = reduce(mul, tmp_input_shape) tmp_input = MNN.Tensor(tmp_input_shape, input_tensor.getDataType(),\
#todo support host caches on multiple datastores </s> def get_host_cache(host_ref, host_cache_manager=none):	get_host_cache Returns a vim.HostScsiDisk if the host cache is configured on the specified host, other wise returns None
# todo: encode / escape key </s> headers['x-bz-info-%s' % (key)] = value	_perform_upload headers['X-Bz-Content-Sha1'] = sha1.hexdigest() for key, value in meta_data: upload_data = self.ex_get_upload_data( container_id=container.extra['id'])
pass  # todo - should this do something </s> press "add user" on users tab	on_btadduser_clicked def on_btadduser_clicked(self, widget, data=None):
# todo: test jacobian </s> mod = linearfactormodel(data.portfolios, data.factors)	test_linear_model_parameters def test_linear_model_parameters(data): res = mod.fit() f = mod.factors.ndarray
# @todo: this has a chance to spam the user with notifications </s> return	push_to_websocket def push_to_websocket(msg): if not clients: main_io_loop = app.instance.web_server.io_loop for client in clients:
# todo: log the reason? </s> self.retry()	clientConnectionFailed def clientConnectionFailed(self, unused_reason):
# todo: implement me </s> self.logmessage("%s %s" % (msg.__class__, vars(msg)), 4)	ChildDepth def ChildDepth(self, msg):
# todo: support aa </s> aa = []	pad_batch po = pad_batch([x.po for x in batch], xp=xp) ah = [pad_batch(list(seq), xp=xp, to=to, axis=1) for seq in zip(*[x.ah for x in batch])] return Activations(lh, po, ah, aa)
# todo: replace with something efficient </s> "target_type": {target: type(i) for i in data[target]},	Task self._meta = {"nrow": data.shape[0], "ncol": data.shape[1], "feature_type": {col: {type(i) for i in data[col]} for \ col in self._spec['features']}}
# todo: seems weird to deal with here. implement this by registring some handler? </s> _, id, prop, txt = command.split(' ', 3)	_receive_command logging.info('JS - ' + command[5:].strip()) elif command.startswith('PROP '): from .mirrored import Mirrored import json
# todo: remove mocking when storedfilenode is implemented </s> with mock.patch('osf_models.models.abstractnode.update_search'):	test_wikis_not_returned_for_withdrawn_registration withdrawal = self.registration.retract_registration(user=self.user, save=True) token = withdrawal.approval_state.values()[0]['approval_token'] withdrawal.approve_retraction(self.user, token) withdrawal.save()
# todo use get_site_base_path </s> with open(os.path.join(os.path.dirname(conf.__file__), 'app', 'patches','patch.log'),'a') as patchlog:	add_to_patch_log def add_to_patch_log(tb): import conf, os patchlog.write('\n\n' + tb)
# todo: for dev, store hash of .cpp and .h files on extension build inside version_dev, then when </s> if semvar(self.client_version).version[:2] != server_version[:2]:	check_version else: server_version = semvar(server_version).version raise RuntimeError( 'Server and client major/minor version do not match - server is %s and client is %s' %
# todo: pytest mark.parametrize once nose removed. </s> def test_columnize_medium():	test_columnize_medium size = 40 items = [l*size for l in 'abc']
# todo(yanase): check values </s> assert len(weights) == 1	test_calculate_without_prior consider_endpoints=consider_endpoints, weights_func=default_weights) assert len(mus) == 1 assert len(sigma) == 1
time.sleep(1)  # todo: avoid race conditions in other way </s> self.server.start()	setUp self.server = threading.Thread(target=self._run_server, args=(certs, sock))
# todo(b/171936854): move all methods to non-experimental api. </s> if use_experimental_api:	configure_optimizer use_experimental_api=True): if use_float16: optimizer = ( tf.keras.mixed_precision.experimental.LossScaleOptimizer(
# todo: this is all just debugging stuff and can be removed </s> if false:	_GaussianTemplate self.phi = phih self.g = self.g + dg uh = [ui.copy() for ui in uh] gh = self.g.copy()
# todo remove this sleep to reproduce http://tracker.ceph.com/issues/8107 </s> import time	test_pg_creation self._create(cluster_id, pool_name, pg_num=64) pool_id = self._assert_visible(cluster_id, pool_name)['id'] time.sleep(10) updates = {
# todo: reenable </s> raise last_error	_do_batched_insert if last_error is not None:
# todo(user): remove after 184 is out. </s> finalized_filenames = cls.get_filenames(mapreduce_state)	finalize_job state = cls._State.from_json(mapreduce_state.writer_state) files.finalize(state.filenames[0]) state = cls._State(finalized_filenames, []) mapreduce_state.writer_state = state.to_json()
# passamos por todos os lugares e nao atingimos o objetivo </s> return none	bfs explored.add(child) frontier.push(Node(child, current_node))
cursor.execute("""select * from todo </s> where id = %s and userid = %s	remove invalid = [] for taskid in tasks: AND active = 1""", taskid, id) if cursor.rowcount == 0:
# todo assert content of the template by matching expected template </s> print(yaml.dump(generated_template))	test_head_node_construct generated_template = load_yaml_dict(os.path.join(tmpdir, f"{output_file}.template.json"))
# todo: unit test! </s> get the feature sizes for the specified node types.	node_feature_sizes def node_feature_sizes(self, node_types=None): Args: node_types: A list of node types. If None all current node types
# todo: this is not thread-safe! </s> logging.warning('status: reparsing all transactions.')	reparse def reparse (db, block_index=None, quiet=False): to the end of that block. cursor = db.cursor() if block_index:
# todo: is this a duplicate? </s> def filter_nonzero(data, combine=none):	filter_nonzero Filter non-zero features of data according to a certain combining function Parameters
# todo document </s> def _null_bigmip(subsystem):	_null_bigmip This is the MIP associated with a reducible subsystem. return BigMip(subsystem=subsystem, cut_subsystem=subsystem, phi=0.0,
# todo when would we use a replay memory without next-states? </s> if self.next_states:	_computation_get_records def _computation_get_records(self, num_records): indices = tf.range(start=0, limit=self.read_variable(self.size)) terminal_indices = self.read_variable(self.record_registry['terminal']) indices = tf.boolean_mask(tensor=indices, mask=tf.logical_not(x=terminal_indices))
# todo: remove the dirty variable once #2004 is pushed </s> if dirty:	_ParseJournalEntry else: pid = None event_data = SystemdDirtyJournalEventData() else:
loop=asyncio.new_event_loop(),  # todo: this doesn't work without this </s> )	test_issue_605 token=self.bot_token, run_async=False, new_message = self.web_client.chat_postMessage(channel=self.channel_id, text=self.text) self.assertFalse("error" in new_message)
# todo: can we just remove the leading spaces from the </s> trans_dict[lang].update({row["property"]: row[lang]})	upload_translations if not (lang_with_defaults == lang and row[lang] == default_trans[row["property"]].lstrip(" ")): if error_properties: message = _("We found problem with following translations:")
# todo username </s> sudo = args.pushy('ssh+sudo:{hostname}'.format(	disk for (hostname, disk) in args.disk: log.debug('Preparing host %s disk %s', hostname, disk) hostname=hostname, ))
# todo: auxiliary_vars </s> self.train_step(inputs, targets)	_train_step self.train_step(inputs, targets, auxiliary_vars) elif backend_name == "pytorch":
# todo: use triple factory </s> rotate.forward_owa(torch.zeros(16, 3, dtype=torch.long))	test_rotate rotate = RotatE(triples_factory=self.factory) self.assertIsNotNone(rotate) rotate.forward_cwa(torch.zeros(16, 2, dtype=torch.long)) rotate.forward_inverse_cwa(torch.zeros(16, 2, dtype=torch.long))
# todo: raise specific exception? </s> raise runtimeerror('site already has an access point named %r.'	register raise RuntimeError('Access point already registered.') if name in self.access_points: % name) access_point.site = self
# todo: replace the pickle here with something else </s> data = dill.loads(bytes_data)	model_from_bytes def model_from_bytes(model, bytes_data): metas = data['metas'] weights = data['weights']
#todo todo todo todo todo todo todo todo todo </s> pub_key.generate_key(crypto.type_rsa, 1024)	gen_RSA_key :rtype: An RSA key as an `pyopenssl.OpenSSL.crypto.PKey` pub_key = crypto.PKey() return pub_key
# todo: check this, reconstruct might not work </s> k.reconstruct()	_transformBlock block.reclassify_component_type(i, Expression) for k in block.component_objects(Objective, descend_into=True):
# todo: handle index/keyerror here when we overrun a segment </s> flags = self.getflags(ea)	NextHead while not self.isHead(flags): ea += 1 return ea
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
# todo: currently job processes are not maintained. perhaps it is better / safer approach, </s> if psutil:	_dispy_setup_process proc_pid = msg['pid'] job_reply = msg['job_reply'] try: proc_pid = psutil.Process(proc_pid)
# todo(b/145514490): this is a bit heavy handed, there maybe caches where </s> self._cache = {}	CachingExecutor await cached_value.target_future except Exception as e: raise e return cached_value
# todo(b/185726968): replace with shared v1 test_util. </s> is_v1_apis = hasattr(tf, 'assign')	testPruneScope_NotNeededForTF2SavedModel def testPruneScope_NotNeededForTF2SavedModel(self): if is_v1_apis: return
# todo refactor this </s> if number_of_columns < 3:	calculate_rfmtry def calculate_rfmtry(number_of_columns, type): message = "You need more than two columns to tune hyperparameters." raise ValueError(message)
# todo: unused now, but will be necessary to compute the adjoint </s> self.expr = expr	Interpolation Evaluates to a list of Eq objects. def __init__(self, expr, offset, increment, self_subs, interpolator, callback): self.offset = offset self.increment = increment
# todo: we should slice and input that to the model. </s> st = choice(range(0, len(x_train_elt) - max_length + 1))	KerasConverter break if len(x_train_elt) >= max_length: kx_train[c_train] = x_train_elt[st:st + max_length] ky_train[c_train] = y
# todo(leonidbeynenson): think on _get_extra_compress_args </s> extra_args = self._get_extra_train_args(args)	convert_train_args_to_compress_args def convert_train_args_to_compress_args(self, model_template_path, args): update_args = self.__map_args(args, self.train_to_compress_update_args_map) update_args.update(extra_args) template_folder = os.path.dirname(model_template_path)
# todo: check that this works using lvm on luks </s> if self.luks:	get_mount_devices mount_devices = {} mount_devices["/boot"] = boot_device mount_devices["/"] = luks_device else:
# todo: webext instrumentation doesn't support req_call_stack yet. </s> assert stack_frames == call_stack_inject_image	test_parse_http_stack_trace_str stack_frames = parse_http_stack_trace_str(stacktrace)
# todo: if the arrays could be drawn as shorts istead of floats, it </s> gl.glvertexpointerd(self.vertices)	glDraw if self.vertices.size == 0 or self.colors.size == 0: return GL.glColorPointerd(self.colors) GL.glEnableClientState(GL.GL_VERTEX_ARRAY)
# todo: renable when regions are sorted out. </s> eq_(str(app.addonpremium.get_price()), "0.00")	test_put_free_inapp eq_(res.status_code, 202)
raise tipgusnotfound # todo right error </s> if not filelookedat:	admin_get_single except NotOneError: store.close() store.close() raise TipGusNotFound # TODO right error
#todo migrate to remove this hack </s> user.email_verifications[token]['confirmed'] = false	confirm_user_get 'user_merge': user_merge.email}) else: user.save() return verified_emails
# todo(b/178225158): deprecate in favor of the reporting libray when ready. </s> return_reporting_fn: optional[callable[[int, float], none]] = none	run_eval def run_eval( root_dir: Text, ) -> None: Args:
# todo: check that the performance measure is within some range </s> merge_baseline(num_runs=1, flow_params=merge2, render=false)	test_merge merge_baseline(num_runs=1, flow_params=merge0, render=False) merge_baseline(num_runs=1, flow_params=merge1, render=False)
## todo: # fixme: remove me </s> if not paste_childrens:	get_all_pastes_domain paste_parent = father.replace(self.paste_directory, '')[1:] paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(paste_parent)) paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(father)) for children in paste_childrens:
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
# todo: kill this </s> inspection = inspection[0]	domain_canonical if not inspection: return False return (inspection.get("Canonical URL"))
# todo: remove logging </s> log.exception(	handle_bounce_reply_phase content_type = msg.get_content_type().lower() if content_type != "multipart/report" or envelope.mail_from != "<>": "Handle auto responder %s %s %s", content_type, envelope.mail_from, msg )
# todo: when repo.save_trial(trial) </s> pass	test_hyperparams_json_repository_should_be_observable_with_file_system_changes def test_hyperparams_json_repository_should_be_observable_with_file_system_changes(): repo: HyperparamsJSONRepository = HyperparamsJSONRepository()
# todo(brett.cannon) implement </s> pass	test_path_hooks def test_path_hooks(self):
# todo: remove the following line when issue #71 (preserve the trajdataframe index during preprocessing operations) is solved. </s> stdf.reset_index(inplace=true, drop=true)	stops leaving_time=leaving_time, no_data_for_minutes=no_data_for_minutes, min_speed_kmh=min_speed_kmh).reset_index(drop=True) stdf.parameters = tdf.parameters stdf.set_parameter(constants.DETECTION_PARAMS, arguments)
# todo: skips header parsing </s> iline += 1	read_abaqus_inp pass elif word.startswith('material'): line0 = lines[iline].strip().lower() word = line0.strip('*').lower()
# todo find out if this is good because of sparcity... </s> 'prefers_data_normalized': false,	get_meta_information 'handles_numerical_features': True, 'prefers_data_scaled': False, 'handles_multiclass': True, 'handles_multilabel': True,
ret = type(spec)() # todo: works for dict + ordereddict, but sufficient for all? </s> for field, sub_spec in spec.items():	glom raise elif isinstance(spec, dict): ret[field] = self.glom(target, sub_spec, _path=path, _inspect=next_inspector) elif isinstance(spec, list):
# todo: should this move to case.rebuild? </s> if not case.xform_ids:	hard_rebuild_case case.domain = domain rebuild_case_from_actions(case, actions) if not found: return None
# todo this should depend on the error (even more granularity) </s> try:	handle_error_code_from_failed_htlc blacklist = True if blacklist: short_chan_id = route[sender_idx + 1].short_channel_id except IndexError:
# todo force-exit taskgroup, to clean-up </s> await group.spawn(peer.trigger_force_close(channel_id))	LNWallet await group.spawn(peer._message_loop())
# todo: needs further implementation </s> return datatablesheader(	headers @property def headers(self): DataTablesColumn(self.selected_location_type), DataTablesColumn(
# todo: update db, add new tx to db + update spend utxo's </s> return txid	send raise WalletError("Could not send transaction: %s" % srv.errors) _logger.info("Succesfully pushed transaction, returned txid: %s" % txid)
# todo: use r.op.span_id to print error with location </s> util.error('%d: %s', fd1, posix.strerror(e.errno))	_PushDup posix.dup2(fd1, fd2) except OSError as e: posix.dup2(new_fd, fd2) posix.close(new_fd)
""" todo: better test here """ </s> timestamp = generate_timestamp()	test_generate_timestamp def test_generate_timestamp(self): self.assertTrue(isinstance(timestamp, unicode)) self.assertTrue(int(timestamp))
# todo: don't compute this realtime, store it in db </s> durations = list(sound.objects.filter(user=self.user).values_list('duration', flat=true))	get_total_uploaded_sounds_length def get_total_uploaded_sounds_length(self): return sum(durations)
# weird problems happen in the parallel run -- todo - figure it out </s> for dsm in dsms:	_ls_web full_fmt += u"  {ds.annex_local_size!S}/{ds.annex_worktree_size!S}" formatter = LsFormatter() ds_str = format_ds_model(formatter, dsm, full_fmt, format_exc=path_fmt + u"  {msg!R}") print(ds_str)
# todo: expect_match should work with emit() </s> m = state.expect_match(	scan_command_edit if k == '+': state.ignore() r'(?:f(?:ile)?f(?:ormat)?|(?:file)?enc(?:oding)?|(?:no)?bin(?:ary)?|bad|edit)(?=\s|$)', lambda: VimError(ERR_INVALID_ARGUMENT))
# todo round to f2dot14? </s> loc = normalizelocation(loc, axes)	main fvar = varfont['fvar'] axes = {a.axisTag:(a.minValue,a.defaultValue,a.maxValue) for a in fvar.axes} print("Normalized location:", loc) gvar = varfont['gvar']
# todo: fix this </s> items = list(collection.list())	discover yield collection if depth != "0": if items: for item in items:
# todo use the faster method </s> lang.flush_cache(false)	handle_language def handle_language(self, lang, **options): lang.get_stats() lang.get_mtime()
# todo: add option to preserve original key names </s> input_encoding = input_encoding or default_input_encoding	csv_clean - Output dialect: excel - Output encoding: UTF-8 sample_size = 1024 * 1024 fobj = open_compressed(source, mode="rb")
marked[id(atom)] = atom # since marked means "it's been appended to the todo list" </s> while todo:	marksingle todo = self.selatoms.values() # list of atoms we must still mark and explore (recurse on all unmarked neighbors) for atom in todo: newtodo = [] for atom in todo:
# todo: consider a better home for this code </s> try:	_make_registrations relib.make_registrations() opcode_intercept.make_registrations() import icontract icontract._checkers._assert_invariant = lambda *a, **kw: None
# todo: remove args after modifying all dependent files </s> pass	train @abstractmethod def train(self, train_data, device, args=None):
# todo: sorting the batch will result in various local metrics being broadcasted </s> batch = super().batchify(obs_batch, sort)	batchify Add action and attribute supervision for batches. Store history vec as context_vec. context_vec, hist_lens_ = self._pad_tensor( sum([obs_batch[i]['context_vec'] for i in batch.valid_indices], [])
(status, output) = commands.getstatusoutput(command) # todo: replace with subprocess call! </s> if status != 0:	exec_local command = ' '.join(command) log.info('Executing command: ' + str(command)) msg = 'Command failed: {cmd}\nOutput:\n{output}'.format(cmd=command, output=output) log.error(msg)
# todo: notify something here. </s> pass	get_image image = StockImage.get(name) except StockImageException: return image
self.assertequal(end, 1) ## todo real = 0 </s> top = output(top_recent.format(**locals()))	test_4090_simple_service_RemainAfterExit out, end = output2(cmd.format(**locals())) logg.info(" %s =>%s\n%s", cmd, end, out) logg.info("\n>>>\n%s", top) self.assertFalse(greps(top, testsleep))
# todo: the following dtypes are not currently supported. </s> })	test_dataframe_roundtrip 'a_str': np.str_('foo'), 'a_unicode': np.unicode_('bar'), decoded_df = self.roundtrip(df) assert_frame_equal(decoded_df, df)
# todo: test accessibility of {training_,}confusion{,s} of </s> "metadataset. got %s error" % results)	testNMinusOneCVWithMetaDataset msg="We should generalize while working with "
# todo: this is a jump. </s> vi_cmd_data['motion']['command'] = '_vi_forward_slash'	vi_forward_slash def vi_forward_slash(vi_cmd_data): vi_cmd_data['motion']['args'] = {'mode': vi_cmd_data['mode'], 'count': vi_cmd_data['count'], 'search_string': vi_cmd_data['user_motion_input']} vi_cmd_data['count'] = 1
#todo - merge this with _write_multi_line method? </s> contig = record.annotations.get("contig","")	_split_contig def _split_contig(self, record, max_len): "Returns a list of strings, splits on commas.""" if isinstance(contig, list) or isinstance(contig, tuple): contig = "".join(contig)
# todo - verify contents </s> self.client.logout()	testDashboard4 'group': 'devgroup'}) self.assertEqual(response.status_code, 200)
# @todo: replace with a link to popup a datatable of the list of updates </s> f = r.function	customise_event_team_resource "status_id", ) group_represent = ertable.group_id.represent if f == "group":
# todo: check if this is correct </s> (typemapitem.method_handle_item, {typemapitem.field_id_item, typemapitem.method_id_item}),	_get_dependencies (TypeMapItem.CALL_SITE_ITEM, {TypeMapItem.METHOD_HANDLE_ITEM, TypeMapItem.STRING_ID_ITEM, TypeMapItem.METHOD_ID_ITEM}), (TypeMapItem.MAP_LIST, set()), (TypeMapItem.TYPE_LIST, {TypeMapItem.TYPE_ID_ITEM}),
# todo: other types that can have series inside? </s> return typ	if_series_to_unbox if isinstance(typ, types.Set): return types.Set(if_series_to_unbox(typ.dtype))
# todo: we should raise exn:fail:contract </s> raise schemeexception("fail_contract")	do_struct_type_make_constructor def do_struct_type_make_constructor(struct_type): if struct_type.inspector != values_struct.current_inspector: return struct_type.constr
# todo: finish </s> result = connection.execute(query, {"created_for_id": user_id})	get_playlists_created_for_user with ts.engine.connect() as connection:
# todo test cases </s> return self._promise	execute self._thread.start()
# todo: allow "a, a, b" when typing "aaab" </s> def catch_misses(event):	catch_misses if event.event_type == event_type and state.index and event.scan_code not in allowed_keys_by_step[state.index]: for part in steps[:state.index]:
# todo: log exception </s> return false	delete_index delete_indices.do_action() except Exception as e:
# steps = 0 # todo </s> print("dupli export took %.3fs" % (time() - start))	create_session transformations = array("f", duplis.matrices) luxcore_scene.DuplicateObject(src_name, dst_name, count, transformations) engine.update_progress(index / len_objs) if engine.test_break():
# todo: create/clear alarm_data folder </s> logging.info("received action from manager")	got_action def got_action(self, ch, method, properties, body): if(self.active): threads = [] for act in self.actions:
# todo: create spatial index to speed up the clip </s> label_flag = 'vector'	main except rasterio.RasterioIOError: label_df = geopandas.read_file(label_file) img_src = rasterio.open(image_file) rows = img_src.meta['height'] // height if drop_last else img_src.meta['height'] // height + 1
federated_only=self.federated_only,  # todo: 466 </s> )	node_metadata_exchange def node_metadata_exchange(self, request: Request, query_params: QueryParams): nodes = self._node_class.batch_from_bytes(request.body, for node in nodes: if node.checksum_public_address in self._node_tracker:
#todo: add some meaningful test </s> cal = yield self.calendarundertest(name="calendar", home="10000000-0000-0000-0000-000000000001")	test_groupChangeLargerSpanningEvent ) yield self._verifyObjectResourceCount("10000000-0000-0000-0000-000000000001", 1) cobjs = yield cal.objectResources() for cobj in cobjs:
# todo: implement an external validation mechanism that can be omitted at runtime if desired. </s> self.validate()	NodeGene self.aggregation = aggregation self.activation = activation def validate(self): assert type(self.bias) is float
# todo ... </s> if isinstance(token, cclosingbracket):	cpre3_parse state = 0 elif state == 5: # "[" bracket if token.brackets == curCObj._bracketlevel: state = 0
# see https://git-annex.branchable.com/todo/output_of_wanted___40__and_possibly_group_etc__41___should_not_be_polluted_with___34__informational__34___messages/ </s> lines_ = [	get_wanted ) lines = out.rstrip('\n').split('\n') l for l in lines if not re.search(
# todo: add some kind of "ding" sound to all of these messages </s> ship = worldobject.get_object_by_id(ship_worldid)	buy_resource def buy_resource(self, ship_worldid, resource_id, amount): if resource_id not in self.buy_list: if ship.owner == self.session.world.player:
""" todo: write desc here """ </s> recipients = emailnotification.retrieve_recipients(old_status, new_status)	notify_users @staticmethod def notify_users(old_status, new_status): for recipient in recipients: subject = 'test subject'
# todo add code </s> self.poll.question = "yours"	test_update def test_update(self): self.poll.save() p = Poll.objects.get()
# todo: modify it to find the optimal elimination order </s> if not elimination_order:	query working_factors = {node: [factor for factor in self.factors[node]] for node in self.factors} elimination_order = list(set(self.variables) - set(variables) -
# todo remove get_media_references </s> multimedia = app.get_media_references()	get_app_view_context if app.get_doc_type() == 'Application': try: except ProcessTimedOut as e: notify_exception(request)
# todo(yuriyz): change to 404 (bug 1200517) </s> self.assertequal(response.status_int, 500)	test_delete_port_byaddress response = self.get_json('/ports/%s' % pdict['uuid'], expect_errors=True) self.assertEqual(response.content_type, 'application/json') self.assertTrue(response.json['error_message'])
# todo(shardy): remove when we no longer support essex </s> nova = client.client(username=con.username,	authenticate no_cache=True) except TypeError: api_key=con.password, project_id=con.tenant,
# todo: validate that the 'name' in the guide matches the name we're actually displaying. </s> if pronunciation and not pronunciation.get("key"):	load_pronunciation_key pronunciation_guide = { p["id"]["govtrack"]: p for p in rtyaml.load(open(settings.PRONUNCIATION_DATABASE_PATH)) } pronunciation = pronunciation_guide.get(person.id) pronunciation["key"] = [] for namepart in pronunciation["respell"].split(" // "):
# todo check executions for dict contents </s> pass	process is_exe |= is_execution(assignee) if is_exe: else: details = par.assignment_details
'i': ('i', [{'j': 'j'}]),  # todo: support true for cases when the value should simply be mapped into the field name? </s> 'n': ('n', lambda n: n.upper())})  # d.e[0] or d.e: (callable to fetch 0)	_main ret = glom(val, {'a': 'a.b', 'e': 'd.e', print('in: ', val) print('got:', ret)
# todo unify </s> if num_components == 1:	_write_node_data else: fmt = ' '.join(['{}'] + ['{!r}'] * num_components) + '\n' for k, x in enumerate(point_data[key]): fh.write(fmt.format(k+1, x).encode('utf-8'))
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: clean up this event print out. we probably want something </s> if suffix == 'new':  # skip "new" events	run if not self.opts.get('quiet', False): for suffix, ret in self.get_async_returns(async_pub['tag']): continue elif suffix == 'ret':  # for "ret" just print out return
# todo: checking _unique_instances might be superfluous here </s> if self._repo is annexrepo._unique_instances.get(	repo if self._repo is not None and realpath(self.path) == self._repo.path: if isinstance(self._repo, AnnexRepo): self._repo.path, None) and self._repo.is_valid_annex(allow_noninitialized=True): return self._repo
# todo: can this be optimized to avoid duplicating the anchors? </s> anchors = np.broadcast_to(anchors, (config.batch_size,) + anchors.shape)	build if mode == "training": anchors = self.get_anchors(config.IMAGE_SHAPE) anchors = KL.Lambda(lambda x: tf.constant(anchors), name="anchors")(input_image) else:
# todo: 'flags_definition', 'spectral_definition'? </s> for key, value in var_params.get('attrs', {}).items():	_write_variable_to_netcdf data_var = netcdf_writer.create_variable(nco, name, variable, **var_params) data_var[:] = netcdf_writer.netcdfy_data(variable.values) setattr(data_var, key, value)
# todo: this is not thread‐safe! </s> follow_cursor = db.cursor()	follow def follow (db): logging.info('Status: RESTART') initialise(db)
return -1 # todo: followup after decision around returning none </s> vollog.log(constants.loglevel_vvv, "cannot access _eprocess.session.sessionid at {0:#x}".format(self.vol.offset))	helper_session_id except exceptions.PagedInvalidAddressException:
# todo add brief documentation what that means </s> if not self.residual_before_ln:	Adapter if self.add_layer_norm_after: output = self.adapter_norm_after(output) output = output + residual_input return output, attention, down, up
annot.annotation_metadata.annotator.email = "todo"  # todo </s> annot.annotation_metadata.annotator.name = name	fill_annoatation_metadata annot.annotation_metadata.origin = "Cerulean Mountain Trust"
# todo fix hack </s> for (rank,) in cursor:	_get_loom_rank colno = %d cursor = bdb.sql_execute(gather_data_sql) return rank
"""todo: not implemented""" </s> notimplementederror("prs welcome")	percent_rank @symbolic_dispatch def percent_rank(x):
# todo clean up, make configurable </s> self.plugins_ = []	loadPlugins def loadPlugins(self, plugins): for plugin in plugins: p = plugin(self)
replace = util.get_value(rule['replace'], kwargs) #todo use subkey? </s> replace = re.sub('\$(\d+)', r'\\\1', replace)   #map $1 to \1 etc.   #todo: also need to escape any existing \1 etc.	pipe_strregex for rule in conf['RULE']: match = util.get_value(rule['match'], kwargs) #todo use subkey? rules.append((match, replace)) for item in _INPUT:
# todo: change 2312 by an always closed/non-http port </s> invalid_target = 'http://localhost:2312/'	test_strategy_verify_target_server def test_strategy_verify_target_server(self): core = w3afCore() target = core.target.get_options() target['target'].set_value(INVALID_TARGET)
#        todo: these are not in metadata. should they be? </s> return u'\n'.join(ans)	__unicode__ if self.rights is not None: fmt('Rights', unicode(self.rights))
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
if testname == "tests5": continue # todo </s> f = open(filename)	buildTestSuite for filename in html5lib_test_files('tree-construction'): testName = os.path.basename(filename).replace(".dat","") tests = f.read().split("#data\n") for index, test in enumerate(tests):
# todo: implement </s> return patches	blast_radius_upgrade :rtype: list patches = []
# todo(fsiddi): use proper exception filtering </s> except:	register_worker try: f = urllib.urlopen(BRENDER_SERVER + '/connect', params) print "[Warning] Could not connect to server to register"
# todo: clean mixed precision api when tensorflow requirement is updated to >=2.4. </s> wrapper_class = none	_add_mixed_precision_wrapper def _add_mixed_precision_wrapper(optimizer): wrapper_kwargs = {} if compat.tf_supports("keras.mixed_precision.LossScaleOptimizer"):
# todo assert responses, swipe down </s> tx = self.client.nem_sign_tx(self.client.expand_path("m/44'/1'/0'/0'/0'"), {	test_nem_signtx_mosaic_creation_levy self.setup_mnemonic_nopin_nopassphrase() with self.client: "timeStamp": 74649215, "fee": 2000000,
# todo extend to nonbinary nodes </s> if i not in self._input_indices and i in self.context.node_indices:	__init__ else: self._dimension_labels.append(None) past_tpm_on = past_tpm_on.sum(i, keepdims=True) / 2 past_tpm_off = past_tpm_off.sum(i, keepdims=True) / 2
# todo: update authors' num_sounds (when not handled via trigger) </s> if self.pack:	change_moderation_state self.save() if (current_state == 'OK' and new_state != 'OK') or (current_state != 'OK' and new_state == 'OK'): self.pack.process()
# todo pydocs </s> def __init__(self, service, project_id):	BigQueryBaseCursor class BigQueryBaseCursor(object): self.service = service self.project_id = project_id
# todo: use upstream implementation when available </s> key_a, key_b = random.split(self._random_state)	beta_gen _support_mask = constraints.unit_interval def _rvs(self, a, b): gamma_a = standard_gamma(key_a, a, shape=self._size) gamma_b = standard_gamma(key_b, b, shape=self._size)
# todo: append to current tree </s> pass	add_from_file self.objects = {} else:
# todo: check syntax, values? </s> values = [v.lower() for v in values]	content_encoding @GenericHeaderSyntax def content_encoding(self, name, values): return values
# todo: replace all of this string templating with a function that accepts </s> monkey_cmdline = (	start ) if OperatingSystem.Windows == SystemInfoCollector.get_os(): MONKEY_CMDLINE_WINDOWS % {"monkey_path": self._config["destination_path"]} + monkey_options
# todo: skipna should be implemented. </s> only_numeric=false)	all lambda col: F.min(F.coalesce(col.cast('boolean'), F.lit(True))),
""" todo: documentation </s> return false	streaming @property def streaming(self):
#     # todo: add an exception message </s> raise parsererror	DateTimeParser if "YY" in fmt_tokens and match.end() != len(string):
# todo: test with intercept </s> x = standardscaler().fit_transform(x)	test_regressor_pickle X, y = boston.data, boston.target X, y = shuffle(X, y, random_state=0) y = StandardScaler().fit_transform(y) succeeded = True
# todo data alignment stuff </s> if 'byteoffset' in pyaccessor.json.keys():	read fmt = '<' + (fmt_char * component_nb) stride = struct.calcsize(fmt) offset = pyaccessor.json['byteOffset'] #TODO use pyaccessor.byteOffset else:
assert oldsize == self.size, '%s: %d, %d' % (self.type, oldsize, self.size) # todo: remove </s> return self.size	Atom else: self.size = 8 + sum([atom.calsize() for atom in self.body])
# todo: verify exception type once those exists </s> dotfile(name, target).remove()	test_remove for x in range(2, times): with pytest.raises(Exception): assert not target.check() assert name.check(file=1, link=0)
#todo: this is just for backwards compatibility. it should be removed in v0.98 with p2.6 </s> llist = cp.get("loggers", "keys")	_install_loggers @classmethod def _install_loggers(cls, cp, handlers): llist = string.split(llist, ",") llist.remove("root")
# todo: can cause an endless loop for single track repeat. </s> self.next()	play self.core.tracklist.mark_unplayable(tl_track) if on_error_step == 1: elif on_error_step == -1: self.previous()
raise mpdnotimplemented # todo </s> def _commands(self):	_commands @register(r'^commands$')
document_type='commcarecasesql',  # todo: should this be the same as the couch models? </s> document_subtype=case.type,	_change_meta_from_sql_case data_source_type=data_sources.CASE_SQL, data_source_name='case-sql',  # todo: this isn't really needed. domain=case.domain, is_deletion=False,
# todo: account for line widths and style </s> data = []	Line3DBox super(Line3DBox, self).process_option(name, value) def to_json(self): for line in self.lines: data.append({
#todo: figure out why unicode sometimes causes an issue with loading after pickling </s> if self.words is not none:	save_joblib def save_joblib(self, filename): temp_words = self.words self.words = None
#todo use calendar </s> retdate += timedelta(days = 30*deltavalue)	parseDate retDate += timedelta(days = 365*deltaValue) elif "month" in d: elif "week" in d: retDate += timedelta(weeks = deltaValue)
# todo: remove when support for django 1.4 is dropped </s> from django.db.models.fields import field	patch_db_field_compare object it's compared to isn't a Field instance. Let's monkey patch it! see https://code.djangoproject.com/ticket/17851 try: assert Field() != None
# todo: remove this fallback logic with rally 1.0 </s> if "action_metadata_present" in params:	BulkIndex if "pipeline" in params: bulk_params["pipeline"] = params["pipeline"] logger.warning("Your parameter source uses the deprecated name [action_metadata_present]. Please change it to " "[action-metadata-present].")
# todo could this be vectorized? </s> result = [largeprecisiontensor._split_number(x.item(), internal_precision, internal_type)	create_tensor_from_numpy internal_precision = type_precision[internal_type] - 1 original_shape = ndarray.shape for x in np.nditer(ndarray, flags=["refs_ok"])] new_shape = original_shape + (len(max(result, key=len)),)
# todo more specific exception type? </s> raise exception('encountered circular group structure')	_nc4_group_from_path parent = parent.groups[key] if parent in visited: elif len(path) > 0: visited.add(parent)
# todo fixme : should we revoke all access tokens when application inactivated? seems likely. </s> obj.save()	perform_destroy raise PermissionDenied obj.active = False
except exception:  # todo: what could happen here? </s> log('warning', 'ws failed to create websocket connection. attempt {} of {}.'.format(attempt, max_attempts))	setup_websocket ws.send("155-questions-active") return ws return None
# todo: endianness support </s> fmt = '<%d%s' % (num_words, num2fmt[wordsize])	read_memory return raw_mem else: mem = list(unpack(fmt, raw_mem)) if num_words == 1:
# todo: add logger here </s> print(''.join('!! ' + line for line in lines))	make_screenshot exc_type, exc_value, exc_traceback = sys.exc_info() lines = traceback.format_exception(exc_type, exc_value, exc_traceback) browser.quit() return {"success": None, "error": True}
# todo: add 3ph loads </s> sg = net["sgen"]	_get_p_q_results_opf b = hstack([b, l["bus"].values]) net["res_load"].index = net["load"].index if len(sg) > 0: sgen_is = _is_elements["sgen"]
#set limit to 25 --> currently hardcoded --> todo: add a setting for this ? </s> subelement(root, "limit").text = "25"	addVideoNodesForTag SubElement(Rule, "value").text = tagname SubElement(root, "order", {"direction":"descending"}).text = "dateadded" Rule2 = SubElement(root, "rule", {"field":"playcount","operator":"is"}) SubElement(Rule2, "value").text = "0"
# todo: implement toolpath.get_meta_data() </s> generator.add_moves(calculated.path, calculated.filters)	run_export for toolpath in source: calculated = toolpath.generate_toolpath() generator.finish() target.close()
# todo: this should now raise an exception </s> model = dqn.modelwrapper(	test_model_predict_wrong_shape def test_model_predict_wrong_shape(): state_axes=1, action_size=2, batch_size=3, model=small_model )
# todo(hartikainen): once tfp.bijectors.chain supports conditioning, </s> return y	_inverse for bijector in reversed(self.flow): y = bijector.inverse(y, **conditions.get(bijector.name, {}))
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_pass_happy_path def test_pass_happy_path(self):
# todo: compare col/row widths before/after - not implemented yet </s> range('sheet1', 'a1:d4').value = 'test_string'	test_autofit_range def test_autofit_range(self): Range('Sheet1', 'A1:D4').autofit() Range('Sheet1', 'A1:D4').autofit(0)
# todo: add a bunch more here to ensure sane config file </s> self.verify_values('disable_banner', [true, false])	verify_config_is_sane self.verify_values('transport', ['HTTP','HTTPS','BOTH'])
# todo: remove anytime in 2016 </s> _assert(false, "status in filter wasn't set - this isn't expected to be possible")	get_prefix_and_key_for_filter_results_and_parsed_params prefix = "%s %s" % ("status", prefix) else: def _get_key(): if parsed_params.module is not None and parsed_params.get_module_int() is None:
# todo: validate that liveactions for actionexec are all deleted. </s> if request_error:	delete 'an error. HTTP result is: %s', actionexec_db.id, result) (result, request_error) = self._issue_liveaction_delete(actionexec_db.id) LOG.warning('DELETE of Live Actions for actionexecution_id="%s" encountered ' 'an error. HTTP result is: %s', actionexec_db.id, result)
# todo: proper content negotiation </s> data = request.get_json()	validate_payload validate = func.__apidoc__.get('validate', False) if model and validate and hasattr(model, 'validate'): model.validate(data, self.refresolver) return func(*args, **kwargs)
# todo detect for typeerror: duplicate base class str, </s> try:	py__mro__ for lazy_cls in context.py__bases__(): for cls in lazy_cls.infer(): mro_method = py__mro__(cls) except AttributeError:
#todo load this from somewhere </s> pad_data = [-1.46374, -0.151816, -0.161173, 0.0686325, 0.0231148, -0.154613,	assign_dev_data device.targets[:l, q] = dataset.targets[dataset.seq_start[s] + batch.start[1]:dataset.seq_start[s] + batch.start[1] + l] if pad_batches: -0.105614, 0.00550198, 0.0911985, 0.00502809, 0.0512826, -0.0181915, 0.0225053, -0.00149681, 0.0782062, 0.0412163, 0.0526166, -0.0722563,
# todo - check and if we don't have category, take the only placement that exists in current site </s> self._main_placement = get_cached_object(	Topic if not hasattr(self, '_main_placement'): try: Placement, target_ct=ContentType.objects.get_for_model(self.__class__),
# todo: expect_match should work with emit() </s> m = state.expect_match(	scan_command_edit if k == '+': state.ignore() r'(?:f(?:ile)?f(?:ormat)?|(?:file)?enc(?:oding)?|(?:no)?bin(?:ary)?|bad|edit)(?=\s|$)', lambda: VimError(ERR_INVALID_ARGUMENT))
# todo: use slotssequenceelement to render this. </s> return ""	return_to return tags.div(tags.a("Return to file/directory.", href=return_to))
# todo: until we get it working. </s> if facts.is_from_app_store():	generate_absolute_recipe by this function! keys = recipe["keys"] warn_about_app_store_generation(facts, recipe["type"]) return
pass  # todo </s> def get_complete_frac(self):	get_complete_frac
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_psrldq_1 res = 0x00000000000000123456781234567812 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init)
# # todo: # fixme: </s> pass	update_tag_last_seen r_serv_tags.hset('tag_metadata:{}'.format(tag), 'last_seen', tag_last_seen) else:
# todo: uncomment assert when #23880 is fixed </s> with connection.schema_editor() as editor:	test_rename_field cursor.execute("INSERT INTO test_rnfl_pony (blue, weight) VALUES (1, 1)") cursor.execute("DELETE FROM test_rnfl_pony") operation.database_backwards("test_rnfl", editor, new_state, project_state) self.assertColumnExists("test_rnfl_pony", "pink")
raise exceptions.mpdnotimplemented  # todo </s> underscore, dash, dot and colon.	subscribe already. The name may consist of alphanumeric ASCII characters plus
# todo(mattjj): remove this logic when allreduce pred supported on cpu / gpu </s> if convert_bool:	_xla_unshard padded = c.DynamicUpdateSlice(padded, c.Reshape(x, None, [1] + dims), idxs) out = c.CrossReplicaSum(padded, xla.axis_groups(axis_env, axis_env.names[-1])) nonzero = c.Ne(out, c.Constant(onp.array(0, dtype=onp.float32))) out = c.ConvertElementType(nonzero, xb.dtype_to_etype(onp.bool_))
# todo: warn/error: check if this var has units: assigning </s> pass	_GeneralVarData % (type(val),)) if type(val) in native_numeric_types or val is None: else: if self.parent_component()._units is not None:
# todo(#12314): add a good error about invalid data type. </s> raise invalidlockfileerror("")	from_lockfile requirements_digest = get_or_raise("requirements_invalidation_digest") if not isinstance(requirements_digest, str): try: interpreter_constraints = InterpreterConstraints(
#todo: note that i'm passing a dc to the fuzzablerequest and it's not </s> fr = fuzzablerequest(self.url, method='get', dc={'a': ['b']},	test_dump_case01 '']) headers = Headers([('Hello', 'World')]) headers=headers) self.assertEqual(fr.dump(), expected)
# todo: write tests </s> beam some things together. the function beams :class:`note` and :class:`chord` objects, but	beamTogether def beamTogether(someThings): everything else is ignored. :param things: An iterable of things to beam together.
# todo special function redirection and __getattr__ redirection </s> return self._data.values()[0].shape	shape def shape(self):
# todo: this really shouldn't be in this class </s> return duration(seconds).getdisplaystring(durationformat.format.short)	formatDuration @pyqtSlot(int, result = str) def formatDuration(self, seconds: int) -> str:
# todo: find a better way to enforce this. </s> fmt = "#b%0{0}d".format(op3_var.size - op1_var.size)	_translate_str elif oprnd1.size < oprnd3.size: expr = (op1_var == smtlibv2.EXTRACT(op3_var, 0, op1_var.size)) imm = smtlibv2.BitVec(op3_var.size - op1_var.size, fmt % 0) constrs = [(imm == smtlibv2.EXTRACT(op3_var, op1_var.size, op3_var.size - op1_var.size))]
# todo: what does constructor of gitconfigparser, in case file doesn't exist? </s> parser = gitconfigparser(gitmodule_path)	get_module_parser from os.path import exists gitmodule_path = opj(repo.path, ".gitmodules") parser.read() return parser
# todo: some regressors have extra options in their predict method, and they return a tuple of arrays. </s> if hasattr(step, 'predict'):	_compute_step @staticmethod def _compute_step(Xs, cache, step): output_data = step.predict(*Xs) elif hasattr(step, 'transform'):
# todo: do not use bare except </s> except:	eval_stderr try: obj.stderr = uval.std_dev() obj.stderr = 0
# todo: do some more checks here. currently it only tests that they </s> self.remote_node.pdo.read()	test_read def test_read(self): self.local_node.pdo.read()
# todo subject.cn from cert? </s> shutil.rmtree(app_path)	test_simple_app lib_hashes = self.assert_common_signed_hashes(lib_info, -2, -1) assert '-3' not in lib_hashes return app_info
# todo if the database is not found we should build it, otherwise just run the tests. </s> c.run('echo "running unit tests"')	run_unit_tests @task def run_unit_tests(c): c.run('nosetests -v', warn=True)
# todo: handle agg_columns. </s> self.assert_eq(	test_apply pdf.groupby("b")["a"].apply(lambda x: x + x.min()).sort_index(), ) kdf.groupby(["a", "b"]).apply(lambda x: x + x.min()).sort_index(), pdf.groupby(["a", "b"]).apply(lambda x: x + x.min()).sort_index(),
# todo: check if subdirs is empty </s> latest_subdir = max(subdirs, key=os.path.getmtime)	get_latest_subdir if os.path.isdir(full_path): subdirs.append(full_path) return latest_subdir
# todo implement for stride != kernel_size </s> if ctx.kernel_size != ctx.stride:	AvgPool2D @staticmethod def backward(ctx, grad_output): raise NotImplementedError("GPU AvgPool2D.backward() with stride != kernel_size not implemented") orig_shape, = ctx.saved_tensors
# @todo: display better tick labels for date range (e.g. 06/01 - 06/05) </s> formatter = matplotlib.dates.dateformatter('%y/%m')	plot axes.xaxis.grid(False, 'major') axes.legend() axes.xaxis.set_major_formatter(formatter) axes.fmt_xdata = matplotlib.dates.DateFormatter('%Y:%m')
# todo: make the multiplication sign configurable </s> expmul = r'\times'	test_latex_repr [1.2e34, 2.3e-4, 2.3e-45], .1) for var, H, suffix in zip(['s', 'z'], [Hc, Hd],
# todo: not all values have exact matches in flexget, need to update flexget qualities </s> cp_to_flexget = {'br-disk': 'remux',  # not a perfect match, but as close as currently possible	on_task_input parsedurl.path)) entries = [] '1080p': '1080p', '720p': '720p',
# todo: normalization for other languages </s> return text	extractnumber if lang_lower.startswith("en"): return extractnumber_en(text)
# todo: write this </s> pass	catmull_rom_prism def catmull_rom_prism():
# todo unordered float </s> if a is none and b is none:	fcomi def fcomi(ir, instr, a=None, b=None): a, b = float_st0, float_st1 elif b is None:
# todo make more robust - some folks will write .jsx and others </s> name = componentpath.split('/').pop().split('.')[0]	load_components for componentPath in data: componentData = data[componentPath] prop_names = componentData.get('props', {}).keys() for prop in default_props:
# todo: also create an activity detail recording what exactly changed in </s> if not context.get('defer_commit'):	user_update from ckan.logic.action.create import activity_create activity_create(context, activity_dict) model.repo.commit() return user_dictize(user, context)
# todo: cache this - it's a big time-waster when libraries get big </s> if is_built(shutit, shutit_module_obj):	start_all shutit_module_obj = shutit.shutit_map[module_id] if run_order == -1 or shutit_module_obj.run_order <= run_order: if not shutit_module_obj.start(shutit): shutit.fail('failed to start: ' + module_id, \
raise notimplementederror # todo </s> def log_likelihood(self,data=none,changepoints=none,**kwargs):	_HMMPossibleChangepointsMixin raise NotImplementedError # TODO
pass  # todo </s> self._current_command = []	__init__ self._device = None
# todo: check if this different handling of none and '' has </s> if result == none:	print_result def print_result(self, result): log.warn('%s %s' % (messages.module_sql_console.no_data, messages.module_sql_console.check_credentials)
# todo: use k-way merge instead of sort </s> sort_state_o = sortstatecl(out_data, n_out, ())	parallel_sort recv_disp = hpat.hiframes_join.calc_disp(recv_counts) hpat.distributed_api.alltoallv(key_arr, out_data, send_counts, recv_counts, send_disp, recv_disp) hpat.timsort.sort(sort_state_o, out_data, 0, n_out, ()) return my_rank, out_data
# todo: move this to pyresample </s> def prepare_resampler(source_area, destination_area, resampler=none, **resample_kwargs):	prepare_resampler if resampler is None: LOG.info("Using default KDTree resampler")
# todo(b/132888123): consider other options to avoid possible bugs here. </s> return fn(*args, **kwargs)	_unpack_and_call kwargs[name] = element_value
# todo: add keep parameter </s> return self._rank(method, ascending)	rank Name: b, dtype: float64
# todo: backport the windows implementation </s> pass	__enter__ soft limit to 0. if sys.platform.startswith('win'): else: try:
#todo implement extra options </s> py_options = self.check_options(options)	Skip def apply(self, name, n, types, m, evaluation, options): 'Skip[InputStream[name_, n_], types_, m_, OptionsPattern[Skip]]' record_separators = py_options['RecordSeparators'] word_separators = py_options['WordSeparators']
# todo when dns server is ipv6 </s> self._sock = socket.socket(socket.af_inet, socket.sock_dgram,	handle_events self._loop.remove(self._sock) self._sock.close() socket.SOL_UDP) self._sock.setblocking(False)
# todo: it would be nice to set the size header here </s> self._req.setheader('content-encoding', self._content_encoding)	open if self._content_encoding:
pass # todo: explain </s> pass # todo: explain	status502 def status502(self):        # Bad Gateway
# todo: handle in cleaner way </s> if self.multi_task:	_get_augmented_label_matrix and m is the number of sources, with values in {0,1,...,k} - offset: Create indicators for values {offset,...,k} t = len(L) n, m = L[0].shape
# todo maybe we can figure out a version string </s> return true	install if self.check_package_flag(name, 'forceinstalled'): self.log.debug("Package {} is assumed installed.".format(name)) r = recipe.get_recipe(name) for pkgr in self.get_packagers(name):
# todo(devcamcar): implement filter by user. </s> with self.driver() as drv:	get_projects def get_projects(self): project_list = drv.get_projects() if not project_list:
# population dynamics (todo: registry) </s> for pop in self.model.populations:	step get_signal(self.signals,enc.sig), enc.weights.T) step_lif( self.populations[pop]['ic'],
# todo pick a runtime that is lightly loaded </s> self.replicate(actor.id, random.choice(list(possible_placements)), callback=cb)	_update_requirements_placements return print "PLACEMENTS", possible_placements _log.analyze(self.node.id, "+ END", {}) except:
include_base = true  # todo: make option </s> lines = format_merge_render_lines(	builtin_merge_render remote = as_text_lines(remote) marker_size = 7  # git uses 7 by default base, local, remote, base_title, local_title, remote_title,
# todo: error checking </s> json_obj = json.loads(content)	add_card headers = headers, ) list_id = "" for board in json_obj.boards:
# todo: we change the type here, maybe we should change it earlier? </s> self._positive_regressor_init_knot_scale = np.array(self._positive_regressor_init_knot_scale)	_set_knots_scale_matrix ) self._positive_regressor_knot_scale[self._positive_regressor_knot_scale < 1e-4] = 1e-4 self._positive_regressor_init_knot_scale[self._positive_regressor_init_knot_scale < 1e-4] = 1e-4 if self._num_of_regular_regressors > 0:
return wikidata_key  # todo </s> def resolve_key(wikidata_key, *_):	resolve_key
# todo: should it end with a slash? </s> url = self.make_url(url, ophandle)	run if path: url += "/" + escape_path(path) resp = do_http("POST", url) if resp.status not in (200, 302):
# todo(todd): exception (404) </s> raise exception("missing controller")	handler ) if not controller: rv = controller.process(path, environ) if type(rv) is tuple:
# todo: support for multiple message versions </s> except exception.stopextraction:	Job self.extractor.category, msg[1] ) pass def handle_url(self, url, kexwords):
# todo check performance </s> if len(idx) != self.ndim:	__getitem__ def __getitem__(self, idx): raise ValueError('subscripts must be complete') sidx = ones(len(self.vals))
@unittest.skip('not written')  # todo: finish! </s> raise notimplementederror	test_int def test_int(self):
# todo - fix meta.submission to point to real submission </s> self._remove_handled(meta.submission.submission)	remove_instance_matching_schema try: meta = Metadata.objects.get(raw_data=instance_id, formdefmodel=formdef_id) meta.delete() except Metadata.DoesNotExist:
# rbarlow_todo: convert this callrequest into a celery task call </s> call_request = callrequest(manager.install_content, args, kwargs, weight=weight, tags=tags, archive=true, asynchronous=true)	consumer_content_install_itinerary tags = [resource_tag(dispatch_constants.RESOURCE_CONSUMER_TYPE, consumer_id), action_tag('unit_install')] call_request.add_control_hook(dispatch_constants.CALL_CANCEL_CONTROL_HOOK, cancel_agent_request) call_request.reads_resource(dispatch_constants.RESOURCE_CONSUMER_TYPE, consumer_id)
# todo: spawn process which sets resource limits and then calls </s> reactor.spawnprocess(protocol, self.executable,	spawn_sandbox def spawn_sandbox(self, protocol): args=self.args, env={}, path=self.path)
# todo: this is a temporal fix </s> pixels = np.rollaxis(pixels, 0, len(pixels.shape))	glyph else: pixels = pixels negative_weights = -pixels scale = np.maximum(pixels.max(), negative_weights.max())
# todo: the peer node url needs to be fixed. </s> peer_cli_envs["core_peer_address"] = "{}.{}:{}".format(	join_peers peer_cli_envs["CORE_PEER_TLS_ROOTCERT_FILE"] = "{}/{}/peers/{}/tls/ca.crt".format( dir_node, org_name, peer_node.name + "." + org_name) peer_node.name, org_name, "7051") peer_cli_envs["FABRIC_CFG_PATH"] = "{}/{}/peers/{}".format(
# todo: disclaimer!!! this is a temporary hack to escape from current "ddos attack" </s> agent = request.meta.get("http_user_agent", "").lower().strip()	middleware def middleware(request): if not agent or agent in settings.BLOCKED_WEB_AGENTS: raise Ratelimited()
# todo add binary column (after dropping support for python 2.7) </s> df = pd.dataframe({	test_to_sql @with_cursor() def test_to_sql(self, cursor): 'col_int': np.int32([1]), 'col_bigint': np.int64([12345]),
result = np.hstack((result, result_a))  # todo: https://github.com/tensorlayer/tensorlayer/issues/288 </s> if result is none:	predict result = result_a else: if len(X) % batch_size != 0: dp_dict = dict_to_one(network.all_drop)
# todo scope the cleanup to the current project - https://github.com/lyft/cartography/issues/381 </s> cleanup_gke_clusters(neo4j_session, common_job_parameters)	sync_gke_clusters gke_res = get_gke_clusters(container, project_id) load_gke_clusters(neo4j_session, gke_res, project_id, gcp_update_tag)
# todo: convert to a python xml thing </s> xml=unicode(toc.tostring())	updatePdf toc=self.pdf.document.toc() if toc: soup=BeautifulSoup(xml) tempMarks=[]
# todo(mattjj): remove this special case, used for debugging on cpu </s> if xb.get_replica_count() == 1:	join_arrays def join_arrays(x): dims = c.GetShape(x).dimensions() return c.Reshape(x, None, (1,) + tuple(dims))
# todo: arrange </s> result = self.remote.remove_system("testsystem0", self.token)	test_remove_system def test_remove_system(self): Test: remove a system object self.assertTrue(result) assert 0
# todo-me move sorting and add more sorting options </s> aired_lst = sorted(child_lst, key=operator.itemgetter(1))	get_content pass if isinstance(child_lst[0], list): play_lst = [x[0] for x in aired_lst] else:
# todo(karita): make all scorers batchfied </s> if args.batchsize == 1:	recog_v2 pre_beam_score_key=None if args.ctc_weight == 1.0 else "decoder" ) non_batch = [k for k, v in beam_search.full_scorers.items() if not isinstance(v, BatchScorerInterface)] if len(non_batch) == 0:
singleton=false,  # todo: re-enable </s> )	render tag='renderers', pack=pack, rend = FilterDictWrapper(ret, '.render') if not check_render_pipe_str(opts['renderer'], rend):
# todo this is a workaround since exceptions are currently not correctly stacked </s> pass	fullmatch return self._search(pattern, string, pos, default(endpos, -1)) except RuntimeError: return self.__compile_cpython_sre().fullmatch(string, pos, default(endpos, maxsize()))
# todo put an index.html in front of this bucket </s> yield effect(uploadtos3recursively(	upload_python_packages '--dist-dir={}'.format(scratch_directory.path)], cwd=top_level.path) source_path=scratch_directory, target_bucket=target_bucket,
# todo: handle other hosts </s> raise notimplementederror	get_interface_names return iface_list else:
# todo : real error </s> print "error : bad expression near", tmp	eval_complex_cor_pattern tmp = tmp.strip() if stacked_par == 1 and tmp != '': continue if stacked_par > 1:
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pcmpeqb res = 0x000000ff0000ff00ff00000000ffff00 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo use proper tx, txindex, sibling.  should also test that </s> txblockhash = b1	testVerifyTxPrePost res = self.c.verifyTx(tx, txIndex, sibling, txBlockHash) assert res == 0 res = self.c.verifyTx(tx, txIndex, sibling, txBlockHash) assert res == 0
# todo(b/133761055): update all callers and make this an error condition to </s> if self._eval_shared_model.construct_fn is none:	start_bundle def start_bundle(self): construct_fn = make_construct_fn( self._eval_shared_model.model_path,
# todo: saved searches </s> self.field_metadata.add_grouped_search_terms(	initialize_dynamic except: traceback.print_exc() self.pref('grouped_search_terms', {})) self._search_api.change_locations(self.field_metadata.get_search_terms())
# todo: logging </s> return none	_get_cert cert_url = self._data.get('SigningCertURL') if not cert_url: try: import requests
#todo - this should probably syslog </s> pass	log def log(self, num, msg):
# todo(stephenfin): remove this in a future major version </s> password_group.add_argument(	SetServer ), ) '--root-password', action="store_true",
# todo allow exporting poseunits </s> action = none	exportFbx mesh.name = fbx_utils.getMeshName(mesh, skel) if useAnim: else: action = None
# todo(mordred) when this changes to rest, force interface=admin </s> if self.cloud_config.get_api_version('identity') != '3':	update_user user = self.get_user(name_or_id) kwargs['user'] = self.get_user_by_id(user['id'], normalize=False) kwargs.pop('domain_id', None) kwargs.pop('description', None)
pass # todo </s> def handle_request(self, input):	handle_request
# todo: this could be a property.  handle it! </s> pass	get_node_by_heading found_nodes.append(node) except AttributeError: for node in node.content: OrgDataStructure.get_node_by_heading(node, heading, found_nodes)
# todo: remove for all locales generated by the doc. </s> self.podcache.document_cache.remove(doc)	file_updated doc = self.get_doc(dep_path) self.routes.remove_document(doc) self.podcache.collection_cache.remove_document(doc) doc = self.get_doc(dep_path)
# todo: implement </s> (default) or a billing address.	assign_address_to_request The `shipping` parameter controls whether the address is a shipping address
# todo message </s> ds.add(db_path)	__call__ sort_keys=True) db_fp.close() elif exists(db_path): ds.remove(db_path)
# todo: must be implemented </s> pass	range_from_chapters def range_from_chapters(crawler, times=0):
# todo: ensure that if multiple flags are provided, the *last* one overrides </s> if arg.p:	Pwd posix.strerror(e.errno)) return 1 pwd = libc.realpath(pwd) print(pwd)
# todo we could reload the message </s> pass	Forward await self.get_input_chat()) except ValueError: return self._chat @property
# todo: rate should not have to be inversed </s> rate = 1 / math.exp(-graph[start][end])	calculate_profit_ratio_for_path start = path[i] end = path[i + 1] money *= rate return money
# todo(cmaloney): test user provided parameters are present. all the </s> assert 'master_quorum' in expanded_config	test_load_expanded_config expanded_config = json.load(f)
# todo: move to base class </s> return self._manipulationmode	Canvas @property def manipulationMode(self): @manipulationMode.setter def manipulationMode(self, value):
# todo: revisit for potential behaviour / type checking. </s> self._use_derived_rgb_to_xyz_matrix = value	use_derived_RGB_to_XYZ_matrix value : bool Attribute value.
"""todo: explain what this is testing </s> or at least use explicit variable names...	test_infer_dim_1 def test_infer_dim_1(): n, p = 1000, 5 X = randn(n, p) * .1 + randn(n, 1) * np.array([3, 4, 5, 1, 2]) \
# todo(b/148082271): remove this line once tft 0.22 is used. </s> transformed_features.pop(_transformed_name(_label_key), none)	serve_tf_examples_fn parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec) transformed_features = model.tft_layer(parsed_features) return model(transformed_features)
# todo: no coverage here </s> manager = none	__compile_class if manager is not None: if manager.class_ is not self.class_: elif manager.mapper: raise sa_exc.ArgumentError(
# todo - fix this problem with bad imports from bitbucket </s> data = {}	_get_bitbucket_commits data = json.loads(page) except ValueError, e: return data.get("changesets", [])
# todo: make sure we only call get_transform if the transform for </s> tr = self._line.get_transform(map_from='visual', map_to='canvas')	_prepare_draw return False if self.axis_label is not None: x1, y1, x2, y2 = tr.map(self.pos)[:,:2].ravel() if x1 > x2:
# todo(twd2): improve here: </s> mdoc['sender_udoc']['gravatar_url'] = (	HomeMessagesView mdoc = await message.add(self.user['_id'], udoc['_id'], content) mdoc['sender_udoc'] = await user.get_by_uid(self.user['_id'], user.PROJECTION_PUBLIC) template.gravatar_url(mdoc['sender_udoc']['gravatar'] or None)) mdoc['sendee_udoc'] = udoc
# todo generator </s> if refpath:	_get options=['--from=%s' % source] if source else [], jobs=jobs)) if ds_path != refpath: for lr in results:
# todo(mjanusz): remove circular reference between canvas and seed policies. </s> self.canvas = weakref.proxy(canvas)	BaseSeedPolicy **kwargs: other keyword arguments del kwargs self.coords = None self.idx = 0
# todo scope the cleanup to the current project - https://github.com/lyft/cartography/issues/381 </s> cleanup_gcp_instances(neo4j_session, common_job_parameters)	sync_gcp_instances instance_list = transform_gcp_instances(instance_responses) load_gcp_instances(neo4j_session, instance_list, gcp_update_tag)
raise notimplementederror()  # todo </s> not implemented yet!	list_settings_profiles .. WARNING::
# todo(okuta): check type </s> return _statistics._ndarray_nanvar(a, axis=axis, dtype=dtype, out=out,	nanvar return a.var(axis=axis, dtype=dtype, out=out, ddof=ddof, keepdims=keepdims) ddof=ddof, keepdims=keepdims)
except (keyerror,valueerror): # todo, handle errordetail messages </s> sys.stdout.write(line)	readAndPrintSreamingBuildStatus print(lineJson["errorDetail"]["message"]) print(lineJson["error"]) output += chunk return output
# todo: cp.get_file will also match cp.get_file_str. this is the </s> if module:	argspec salt '*' sys.argspec ret = {} comps = module.split('.') comps = filter(None, comps)
# todo move this to spotify.luigi.hdfs </s> @classmethod	hdfs_reader def hdfs_reader(cls, path): raise NotImplementedError()
# todo: why is there a benchmark? </s> trade_start_time = "2017-01-01"	tianshou def tianshou(self): self._init_qlib() trade_end_time = "2020-08-01" benchmark = "SH000300"
# todo use deepcopy() here </s> return polygonsonimage(polygons, shape)	on else: polygons = [poly.project(self.shape, shape) for poly in self.polygons]
# todo: log. </s> continue	download_all ) except ValueError: request = self.create_getdatarequest( {dresponse.provider: files}, methods, info
# todo assert exit code != 0 </s> self.assertequal(dvol.voluminous.getoutput(),	test_commit_no_message_raises_error try: dvol.parseOptions(ARGS + ["-p", self.tmpdir.path, "commit"]) ["You must provide a commit message"]) except UsageError:
# todo: this might be too slow because of the addition </s> data = reduce(lambda res, (key,val): res + int(val)*[key], data.iteritems(), [] )	get data = int(data) if data else 0 elif config.get('compress', False): if config.get('read_cast'): data = map(config.get('read_cast'), data)
min_stake=0)  # todo: handle customized min stake here. </s> except nodeseemstobedown:	get_external_ip_from_default_teacher teacher = Ursula.from_teacher_uri(teacher_uri=top_teacher_url, federated_only=federated_only, return client = NucypherMiddlewareClient()
# todo: fix this 405 method not allowed error </s> data = rv.data.decode('utf-8')	test_model_list_order rv = client.post('/model1view/list?_oc_Model1View=field_string&_od_Model1View=desc', follow_redirects=True) self.clean_data()
# todo: same code as for batch gradient, but with sum_batch = true </s> return bias_grad.view(shape)	bias shape = module.bias.shape
# todo: launch visitor on node </s> return node	visit_Call def visit_Call(self, node): if not isinstance(node.func, ast.Name): fc_name = node.func.id new_name = fc_name
# todo: this pattern seems to repeat a lot, maybe we should have a sensible default? </s> config = fluffconfig(overrides=dict(dialect=dialect))	_get_linter def _get_linter(dialect="ansi"): return Linter(config=config)
self.entity_bin = serialize(value, to_bytes=true)  # todo: techdebt fix </s> def entity(self, value: any) -> none:	entity @entity.setter
# todo legacy method to be removed/refactored </s> from corehq.apps.locations.models import location	locations @property def locations(self): from corehq.apps.commtrack.models import SupplyPointCase def _get_linked_supply_point_ids():
except exception:  # todo: be specific </s> icap = false	parse_insta_json icap = icap.replace('\n', ' ') icap = (icap[:256] + u'…') if len(icap) > 256 else icap if ivideo is True: botmessage = "[insta] Video by "
# todo: get runname from model-dir </s> summary_dir = os.path.join(config.train.log_dir, config.train.run_name)	evaluate_once 'filenames': [],  # Filenames. TODO: Remove. } with tf.Session() as sess: sess.run(ops['init_op'])
## todo: # fixme: remove me </s> if not paste_childrens:	get_all_pastes_domain paste_parent = father.replace(self.paste_directory, '')[1:] paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(paste_parent)) paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(father)) for children in paste_childrens:
# todo: make sure the image is present or pull it </s> base_image = "registry.fedoraproject.org/fedora:28"	test_build_basic_image_with_labels x_y = "x=y" basic_playbook_path = os.path.join(data_dir, "basic_playbook.yaml") target_image = "registry.example.com/ab-test-" + random_word(12) + ":oldest" cmd = ["build", "-l", a_b, x_y, "--",
# todo: ask where to install the bootloader (if the user wants to install it) </s> import bootloader	start_installation fs_devices[boot_partition] = "ext2" fs_devices[root_partition] = "ext4" bl = bootloader.BootLoader(self.settings) bl.ask()
# todo: we can't do this; this shells out for each selection change... </s> actions["vcpush"] = true	update_actions_for_paths _vc.STATE_NONE, _vc.STATE_IGNORED) for s in states) actions["VcUpdate"] = True actions["VcAdd"] = all(s not in ( _vc.STATE_NORMAL, _vc.STATE_REMOVED) for s in states)
# todo: start here </s> pass	_combine_emr_configurations Configurations may also be a :py:class:`~mrjob.conf.ClearedValue` wrapping said type of list.
# todo: enable gpu tests on jenkins </s> if ctx == mx.context("gpu") and not check_gpu_support():	test_jitter_synthetic_gp @pytest.mark.parametrize("float_type", [np.float32, np.float64]) def test_jitter_synthetic_gp(jitter_method, float_type, ctx) -> None: return batch_size = 1
# todo(tdurakov): remove dict to object conversion once rpc api version </s> got_migrate_data_object = isinstance(dest_check_data,	check_can_live_migrate_source is_volume_backed = compute_utils.is_volume_backed_instance(ctxt, instance) migrate_data_obj.LiveMigrateData) if not got_migrate_data_object:
# todo: revise this when build deps are in dag_hash </s> norm = read_separately.normalized().copy(deps=stored_deptypes)	test_read_and_write_spec with open(spec_path) as spec_file: read_separately = Spec.from_yaml(spec_file.read()) self.assertEqual(norm, spec_from_file) conc = read_separately.concretized().copy(deps=stored_deptypes)
# todo(b/155804245) sanitize the names so that they're valid python names </s> execution_parameters[param_name] = (	create_container_component output_channels[output_name] = channel for param_name, parameter_type in parameters.items(): component_spec.ExecutionParameter(type=parameter_type)) tfx_component_spec_class = type(
#todo: add check/warning if theta or phi outside appropriate ranges </s> r1 = l * r ** (l - 1) * sph_harmonic(l, m, theta, phi)	grad_out_comp Compute gradient of RHS of V(r) spherical expansion having form Ylm(theta, phi) * (r ** l) theta1 = r ** (l - 1) * np.sqrt((2 * l + 1) * factorial(l - m) / (4 * np.pi * factorial(1 + m))) * \
raise  # todo </s> self.check_config_tree(configuration_dir=self.config_root)	initialize_configuration os.mkdir(self.known_metadata_dir, mode=0o755)        # known_metadata except FileExistsError: return self.config_root
# todo: take care of theano to keras port: </s> ))	vgg16 net["in"], 2, "conv_1", 64, activation=activation, net.update(base.conv_pool( net["conv_1_pool"], 2, "conv_2", 128,
# todo: has to be refactored. </s> try:	write_v4_config template_file = "/etc/cobbler/dhcp.template" blender_cache = {} f2 = open(template_file, "r") except Exception:
# todo: implement @plist </s> return 1	dotFromElement ============ None.
# todo: assert metrics. </s> metrics = code_analysis_server.metrics(	test_get_metrics error = repository.get_metrics(commit, metrics["spaces"]) assert not error "file.cpp", int i = 1;
# todo: docs for this. </s> return self.plugin_options.get(plugin, {})	get_plugin_options def get_plugin_options(self, plugin):
# todo: move to base class </s> self._manipulationmode = value	manipulationMode @manipulationMode.setter def manipulationMode(self, value): if value == CanvasManipulationMode.NONE: pass
# todo: remove in v8 </s> self._global_context['blog_desc'] = self.config.get('blog_description')	__init__ self._GLOBAL_CONTEXT['blog_title'] = self.config.get('BLOG_TITLE') self._GLOBAL_CONTEXT['blog_description'] = self.config.get('BLOG_DESCRIPTION') self._GLOBAL_CONTEXT['blog_url'] = self.config.get('SITE_URL', self.config.get('BLOG_URL')) self._GLOBAL_CONTEXT['body_end'] = self.config.get('BODY_END')
# todo should we be using a python library for this? </s> try:	fetch if dependency in self.urls: url = self.urls[dependency]['url'] print '[x] Fetching %s.' % info['filename'] subprocess.check_call([WGET, '-O', filepath, url])
# todo: check whether the graph execution is resolved correctly. </s> run_partial.delay([], 'full', description)	test_run_partial def test_run_partial(description):
# todo: currently mnn python binding have mem leak when creating mnn.tensor </s> input_elementsize = reduce(mul, input_shape)	yolo_predict_mnn image_data = preprocess_image(image, (height, width)) image_shape = image.size tmp_input = MNN.Tensor(input_shape, input_tensor.getDataType(),\ tuple(image_data.reshape(input_elementsize, -1)), input_tensor.getDimensionType())
# todo: move part of this to card type. </s> for card_type_id in alias_needed:	load self.load_failed = True raise PluginError(stack_trace=True) card_type_name = card_type_id.rsplit(".", 1)[1] card_type_name.replace("ALIAS_", "")
# todo: change this when data is avaialable </s> sum_other = 40 + 0.5 + 0.5 + 18 + 8 + 5 + 5 + 2	fetch_production fuel_name = item['fuel_name'] if iso == 'CAISO' and fuel_name == 'other': obj['production']['gas']     = obj['production'].get('gas', 0.0) + float(item['gen_MW']) * 40 / sum_other obj['production']['oil']     = obj['production'].get('oil', 0.0) + float(item['gen_MW']) * 0.5 / sum_other
pass  # todo </s> def __cleanup(configuration):	__cleanup
# todo: remove with v1 deprecation </s> if (kwargs.get('name', none) and not kwargs.get('group', none) and	read def read(self, pk=None, fail_on_no_results=False, fail_on_multiple_results=False, **kwargs): not self._is_full_v1_name(kwargs['name'])): kwargs.setdefault('query', [])
# todo -- get a list of these from the api </s> projects = {}	issues def issues(self): issues = self.api.maniphest.query(status='status-open') issues = list(issues.iteritems())
# todo actually it is already well tested in base calss actionlog </s> @pytest.mark.parametrize('ev_type', restartlog.events)	test_restart_log_append_api def test_restart_log_append_api(log_file_path, ev_type): restart_log = RestartLog(log_file_path)
# todo: needs input cleansing and validation </s> try:	create_policy Character control endpoint for creating a policy and making arrangements with Ursulas. bob_pubkey = bytes.fromhex(request.args['bob_encrypting_key']) label = bytes.fromhex(request.args['label'])
# todo not supported yet </s> assert a == 'а' and b == 'б' and c == 'в'	test_destructuring a, b, c = "\u0430\u0431\u0432"
#todo: how to use the same data for both transformers </s> @pytest.mark.xfail(strict=true, reason="c2 and ngraph generates own data")	test_fc def test_fc(): net = core.Net("net")
# fix: https://github.com/certtools/intelmq/issues/1720 # todo: find better fix </s> if '/' in value:	validate_ip if value == '0.0.0.0': return None return None if harmonization.IPAddress.is_valid(value, sanitize=True):
uploader.upload_file(file, container='export') # todo: right container folder?! </s> finally:	export_json zip.close() file = FileStorage(filename='%d_%s_task_run_json.zip' % (app.id, name), stream=zipped_datafile) zipped_datafile.close() finally:
# todo: implement me </s> assert pytest.approx(coord[1, 1, 1].item(), 1.0)	_test_batch2_n2 assert pytest.approx(coord[1, 1, 0].item(), 1.0)  # bottom-right
# todo implement this function </s> pass	set_intent_event :param intent_event: instance of IntentEvent :return:
# todo: try/except this call. </s> self._append_svg(data['image/svg+xml'])	_handle_pyout self._append_plain_text(self.output_sep) self._append_html(self._make_out_prompt(prompt_number)) self._append_html(self.output_sep2) else:
# todo: handle this </s> print "[papyon]", contact, "left a conversation"	on_conversation_user_left def on_conversation_user_left(self, contact):
# todo: this can be formulated more efficiently </s> sqrt_ggn = einsum('boc->cbo', (sqrt_ggn_out, )).contiguous()	backpropagate_sqrt_ggn num_classes = sqrt_ggn_out.size(2) assert tuple(sqrt_ggn_out.size())[:2] == (batch, out_features) sqrt_ggn = sqrt_ggn.view(num_classes * batch, out_features) sqrt_ggn = sqrt_ggn.view(num_classes * batch, out_channels, out_x, out_y)
if not version_2_79_or_older():  # todo </s> col = box.column(align=true)	draw box = layout.box() col = box.column(align=True) row = col.row(align=True) row.scale_y = 0.75
# todo find which file is being downloaded with this item. </s> itemmodel = item()	downloadItemCompleteEvent def downloadItemCompleteEvent(event): item = itemModel.load(event.info['id'], force=True) files = list(itemModel.childFiles(item=item, limit=2))
raise notimplementederror # todo </s> list every item in entry_point that match request	search def search(self, entry_point, request):
# todo: also improve 'crash-start' detection (to reduce lag when server fails to start) </s> time.sleep(0.1)	auto_server_start launch_server_daemonized() for _ in range(100): # Check server availability for next 10 seconds s = self.connect() if s is not None: break
# todo: remove in favor of a proper per-module selection </s> parser.add_argument('--force-module-branch-type',	_parse_args parser.add_argument('--no-turbo', action='store_false', dest='turbo_mode', default=None, help='do not automatically select entries (safer).') help='Set to beta if you want to test beta versions of all modules. Please report breakage.') if platform.system() == "Darwin":
# todo: error sound </s> return	append_bo @param branch_office: Set to add a specific one, else the selected one gets added. if len(self.widgets) >= self.MAX_ENTRIES: self.instance.route.append(branch_office) self.add_gui_entry(branch_office)
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_copyright_matches_pattern_success def test_result_copyright_matches_pattern_success(self): self.assertInSuccess('test_copyright_matches_pattern', run_set(p, 'result'))
# todo: check ping response </s> self.asserttrue(self.packet_outs_from_flows(echo_replies))	icmpv6_ping_controller 'ipv6_dst': 'fc00::1:254', 'echo_request_data': bytes('A'*8, encoding='UTF-8')})
pass # todo </s> def test_insert(self):	test_insert
# todo, pass also best score </s> if last_path is not none and not self.trainer.testing:	__recover_child_process_weights if self.trainer.checkpoint_callback: self.trainer.checkpoint_callback.best_model_path = best_path ckpt = torch.load(last_path, map_location=lambda storage, loc: storage) model.load_state_dict(ckpt)
# todo: bash path completion </s> self.cfg['build']['ctrlc_passthrough'] = true	challenge reset_container_name = if resetting, send user to this container if challenge_type == 'command': print shutit_util.colour('32','''\nChallenge!''') help_text = shutit_util.colour('32','''\nType 'help' or 'h' to get a hint, 'exit' to skip, 'shutitreset' to reset state.''')
# todo: we should throw here, i don't like this. </s> log.info('limit "%s" specified, defaulting to max value of "%s"',	ResourceController offset = int(offset) if limit and int(limit) > self.max_limit: limit, self.max_limit) limit = self.max_limit
# todo: missing objects? </s> fields='    ' + '\n    '.join(fields_text))	get_model_definition ordering = {ordering}
#todo, multipart raw submissions need further parsing capacity. </s> instance = request.raw_post_data	post instance = request.FILES['xml_submission_file'].read() else: try: doc = post_xform_to_couch(instance)
# todo: this is untested. </s> _lib.x509_free(copy)	add_extra_chain_cert add_result = _lib.SSL_CTX_add_extra_chain_cert(self._context, copy) if not add_result: _raise_current_error()
# todo allow other audio devices but the default. </s> inputfile = '-d'	__call__ raise ValueError('Combining files not supported yet.') elif src is None: else: raise ValueError("Invalid input.")
# todo(sahid): we should never configure a driver backend for </s> conf.driver_name = none	_set_config_VIFVHostUser def _set_config_VIFVHostUser(self, instance, vif, conf, host=None): designer.set_vif_host_backend_vhostuser_config( conf, vif.mode, vif.path)
raise notimplementederror #todo, implement! </s> def testsubclass(self, cls, subset, subclass):	testsubclass
# todo: not for checkbox (should have checkbox class) </s> return self._html_output(	as_div def as_div(self): normal_row=DIV_TEMPLATE, error_row=u'%s',
# self.assertisnotnone(cursor.service_processing_time_in_millis)  # todo flaky test </s> self.assertisnotnone(cursor.output_location)	test_fetchone self.assertIsNotNone(cursor.total_execution_time_in_millis) self.assertIsNotNone(cursor.query_planning_time_in_millis) self.assertIsNone(cursor.data_manifest_location) self.assertIsNone(cursor.encryption_option)
# todo check if result is in scope -> no evaluation necessary </s> n = dynamic.check_flow_information(flow_scope, name_str,	filter_name print 'b',  flow_scope, name_str, result while flow_scope: position) print
## todo: # fixme: remove me </s> try:	analyse result_query = 0 if resource_path is not None: resource_path = resource_path.decode() except:
@pytest.mark.skip()  # todo: fix this </s> self.assertisnotnone(response)	test_issue_560_success response = client.conversations_list(exclude_archived="true")
# todo: this completion may not be good, since it resets to 0 later. </s> history.append([note_hot, beat_input, completion_input, style])	generate beat_input = compute_beat(i, NOTES_PER_BAR) completion_input = np.array([i / (len(inspiration) - 1)]) composition = [] N = NOTES_PER_BAR * bars
# todo: add longer frame data </s> sock = ws.websocket()	testSend def testSend(self): sock.set_mask_key(create_mask_key) s = sock.io_sock = sock.sock = HeaderSockMock("data/header01.txt")
# todo - retrieve from config </s> timelimit = 2	find_users def find_users (self, criteria, sattrs=None, searchlimit=0, opts=None): If the results are truncated, counter will be set to -1.""" search_fields_conf_str = "uid,givenName,sn,telephoneNumber,ou,title" search_fields = string.split(search_fields_conf_str, ",")
# todo: refactor things like the augment_punct call </s> sents = augment_punct(sents)	build_combined_spanish_dataset conllu_file = common.find_treebank_dataset_file(treebank, udbase_dir, dataset, "conllu", fail=True) sents.extend(read_sentences_from_conllu(conllu_file)) else: conllu_file = common.find_treebank_dataset_file("UD_Spanish-AnCora", udbase_dir, dataset, "conllu", fail=True)
# todo return empty list if not loaded </s> spotify.error.maybe_raise(self.error)	playlists image URI for matching playlists. Will always return :class:`None` if the search isn't loaded. if not self.is_loaded: return None
#todo: fix auditor+south </s> logout(request)	logout request.base_template = settings.BASE_TEMPLATE from django.contrib.auth import logout if next_page is None: redirect_to = request.REQUEST.get(redirect_field_name, '')
# todo, still to work out this </s> self.fields[two[1]] = len(data)	pack for each in data: answer += each.getDataReferents() return answer if data is None:
# todo: need to kill db connections in order to drop database </s> session.begin_nested()	restart_savepoint if transaction.nested and not transaction._parent.nested:
# todo scope the cleanup to the current project - https://github.com/lyft/cartography/issues/381 </s> cleanup_gcp_instances(neo4j_session, common_job_parameters)	sync_gcp_instances instance_list = transform_gcp_instances(instance_responses) load_gcp_instances(neo4j_session, instance_list, gcp_update_tag)
# todo: slow </s> t = vector((tangents[i * 3], tangents[i * 3 + 1], tangents[i * 3 + 2]))	ExportGeometry nora = na.values for i in range(0, vertex_count): n = Vector((nora[i * 3], nora[i * 3 + 1], nora[i * 3 + 2])) v = t - n * n.dot(t)
# todo error reporting over the master event bus </s> self.event.fire_event({'minions': minions}, clear_load['jid'])	_prep_pub clear_load['jid'] = jid delimiter = clear_load.get('kwargs', {}).get('delimiter', DEFAULT_TARGET_DELIM) new_job_load = { 'jid': clear_load['jid'],
# steps = 0 # todo </s> print("dupli export took %.3fs" % (time() - start))	create_session transformations = array("f", duplis.matrices) luxcore_scene.DuplicateObject(src_name, dst_name, count, transformations) engine.update_progress(index / len_objs) if engine.test_break():
# todo(devcamcar): how to assert this succeeded? </s> user = client.users.update_tenant(user, 'bar')	test_user_create_update_delete self.assertFalse(user.enabled) user = client.users.update_password(user, 'password2') client.users.delete(user.id) self.assertRaises(client_exceptions.NotFound, client.users.get,
# todo: handle the `scoped` attribute </s> content = [style.text]	find_style_elements def find_style_elements(document): for style in document.iter('style'): for child in style: content.append(child.tail)
# todo stub </s> yield self.cache_defer	get_app_service_rooms def get_app_service_rooms(self, service): logger.info("get_app_service_rooms -> %s", service) defer.returnValue([RoomsForUser("!foo:bar", service.sender, "join")])
#todo:  we make render response return a string if passed a string?? </s> return params	_tg_validate params = controller.tg_info.validator.to_python(params)
#todo: dataset/hda by id (from history) or check_ownership for anon user </s> hda = self.get_history_dataset_association( trans, history, id,	get_history_dataset_association_from_ids and ( history_id == trans.security.encode_id( trans.history.id ) ) ): history = trans.history check_ownership=False, check_accessible=True ) else:
# todo: work out a nice fix for this failure. </s> memory["last_code"] = segment	_eval if s.name == "newline": anchor = memory["last_code"] memory["since_code"] = [] else:
1  # todo: fill in identifier </s> )	profile_transfer amount, target, result.wait() profiling.print_all_threads()
# todo (server): process request, send response </s> self.ui.showmessage("attempting to create controlled room suffix with password '{}'...".format(controlpassword))	createControlledRoom def createControlledRoom(self): controlPassword = RoomPasswordGenerator.generate_password() self._protocol.requestControlledRoom(controlPassword)
# todo: errors </s> con = sc.connect_to_service_finish(results)	rest_post_connected def rest_post_connected(self, sc, results, command, data, callback, error_callback, callback_data): if con == None: return
# todo(amotoki): due to neutron bug 1378525, neutron disables </s> self.mox.replayall()	_test_router_update_get "dvr", "update")\ .AndReturn(dvr_enabled) url = reverse('horizon:%s:routers:update' % self.DASHBOARD, args=[router.id])
# todo: implement purge manually or call it on the command line </s> subrepo.update(commit_hash, clean=true)	checkout_existing subrepo.pull()
#todo: a single softmax'd vector?? </s> if not numerator.type.dtype.startswith('float'):	softmax_simplifier def softmax_simplifier(numerators, denominators): for numerator in list(numerators): continue if not numerator.type.broadcastable == (False, False):
# todo this context is probably not right. </s> analysis.add(next(iter(types)), 'value-error-too-many-values', part,	unpack_tuple_to_dict part = next(parts) except StopIteration: message="ValueError: too many values to unpack (expected %s)" % n) else:
# * todo heading 1 --> </s> vim.current.window.cursor = (2, 0)	test_circle_through_todo_states def test_circle_through_todo_states(self): Todo.toggle_todo_state() self.assertEqual(vim.current.buffer[1], '* TODO Heading 1')
# todo find out what is best used here! </s> 'preferred_dtype' : none}	get_meta_information 'is_deterministic': True, 'handles_sparse': True,
## todo: remove shared kwargs </s> return allevents	list_lambda_logs update_next_token(lambdaresponse, kwargs)
# todo: handle winddownset event (see #1193) </s> return receipt	StakingEscrowAgent receipt = self.blockchain.send_transaction(contract_function=contract_function, sender_address=staker_address) def staking_parameters(self) -> Tuple: parameter_signatures = (
# todo cache? </s> def test_disappearing():	test_disappearing saves = get_events(all_=True) favs = [s.kind for s in saves if s.text == 'favorited']
# todo: update consumer (agent) </s> pass	repo_deleted collection.remove(bind, safe=True) for consumer_id,repos in BindCollection(deleted):
raise skiptest("buggy")  # todo(mattjj): fix </s> n = 4 * xla_bridge.device_count()	testSoftPmapAxisIndex def testSoftPmapAxisIndex(self): def f(x): return x * lax.axis_index('i')
# @todo: pheonix </s> tree.setitemtext(childid, 1, "level %d" % int(level) if isinstance(level, float) else level)	populateSkillTreeSkillSearch level, dirty = sChar.getSkillLevel(char.ID, id)
# todo also test these! </s> continue	test_classifiers continue if Clf in [MultinomialNB, BernoulliNB]: clf = Clf() clf.fit(X, y)
# todo: remove cache clearing once upstream issues regarding non-batch </s> acquisition_function.model.train()	gen_candidates Tensor: The set of generated candidates Tensor: The acquisition value for each t-batch. acquisition_function.model.eval() options = options or {}
#todo - mutableseq? </s> try :	get_feature_nuc f_seq = parent_seq[f.location.nofuzzy_start:f.location.nofuzzy_end] if f.strand == -1 : f_seq = f_seq.reverse_complement() except AttributeError :
# todo: handle more complex metric specifications and labeling </s> def _get_response(self):  # todo: add timeout	GraphiteClient self.check_time = None self.timeout = int(dehumanize_time(self.config.get('timeout', '5s'))) try: str_data = urlopen(self.url, self.timeout)
# todo(b/160795287): deprecate estimator based executor. </s> fn_args_dict = fn_args.get_dict()	run_fn fn_args: Holds args used to train the model as name/value pairs. schema = io_utils.parse_pbtxt_file(fn_args.schema_file, schema_pb2.Schema()) fn_args_dict['serving_model_dir'] = os.path.join(fn_args.model_run_dir, path_utils.SERVING_MODEL_DIR)
# todo: remove this method in v2.5 </s> elif self._values['enabled'] in booleans_false:	Parameters elif self._values['enabled'] in BOOLEANS_TRUE: return True return False elif self._values['disabled'] in BOOLEANS_FALSE:
# todo add cleanup </s> def get_ips(units):	get_node_ips d.addCallback(lambda _: client.add(node_2_name, image)) d.addCallback(lambda _: client.list()) docker = Client() prefix = u'flocker--' + namespace + u'--'
# todo: should prob be called before update? </s> for n in range(n):	prob N, T = obs.shape[:2] if self.policy.scale is not None: obs[n, :, :] = obs[n, :, :].dot(self.policy.scale) + \ self.policy.bias
# todo: remove this log statement when invoking this method on each iteration of the goal state loop (currently it is invoked only on a new goal state) </s> logger.info("fetching extensions goal state [correlation id: {0} etag: {1}]", correlation_id, etag)	update_extensions_goal_state if etag is not None: headers['if-none-match'] = etag vm_settings, response_headers = self.fetch(url, headers, ok_codes=[httpclient.OK, httpclient.NOT_MODIFIED]) if vm_settings is None:
# todo per-sync cached results </s> @classmethod	merged def merged(cls, media, marked, include_ratings=False, extended='min'): return Trakt.User.get_merged(media, marked, include_ratings, extended)
# todo: avoid dummy and generate func here when inlining is possible </s> def _impl(df, n=5):	_impl return hpat.hiframes.pd_dataframe_ext.head_dummy(df, n)
if not is_old_django: # todo: remove when pre-csrf token templatetags are no longer supported </s> response = self.client.get('/more/csrf_token_test/')	test_csrf_token def test_csrf_token(self): is_old_django = getattr(settings, 'OLD_DJANGO', False) # TODO: remove when pre-CSRF token templatetags are no longer supported self.assertContains(response, "<input type='hidden' name='csrfmiddlewaretoken'")
# todo:liberate - move this to a more generalized tag enhancement package? </s> return false	allows_deletion_by return True
#todo(cp16net): need to set the return code correctly </s> return wsgi.result(views.instanceview(server).data(), 201)	show def show(self, req, tenant_id, id): server = models.Instance(proxy_token=req.headers["X-Auth-Token"], uuid=id).data()
# todo: modifiers </s> self._pyvis_canvas.events.key(name='press', key=key, text=text)	keyPressEvent key = self._processKey(event) text = str(event.text())
#@todo: remove in 0.4.10 </s> if type == "hook":	remove_plugins for dir in ("userplugins", rootplugins): py_filename  = fs_join(dir, type, name + ".py") py_filename  = fs_join(dir, type + "s" , name + ".py") pyc_filename = py_filename + "c"
# todo: handle fancy-index copies by allocating a buffer and </s> next_index = self._subset_iterator.next()	next def next(self): return self._raw_data[next_index]
weigts (float): weights #todo: batched? </s> return np.abs(circuit(*weights)-1)	cost def cost(weights, batched): Args:
#temporarily select a random music file to play. todo: replace with proper playlist </s> music = glob.glob('content/audio/music/*.ogg')	init self.log = fifelog.LogManager(self.engine, 1 if logToPrompt else 0, 1 if logToFile else 0) self.engine.init() print music self.eventmanager = self.engine.getEventManager()
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: 289 </s> policyauthor.__init__(self, *args, **kwargs)	Alice Character.__init__(self, *args, **kwargs) if kwargs.get('is_me') and not federated_only: self.federated_only = federated_only def generate_kfrags(self, bob, label, m, n) -> List:
# todo: s_vectors[sent_adr] += eps </s> continue	train_average_np sent_adr = obj[1] if not len(sent): mem = zeros(size, dtype=REAL) eff_sentences += 1
# @todo: pheonix </s> if event:	populateSkillTree childId = tree.AppendItem(root, name, imageId, data=('group', id)) tree.AppendItem(childId, "dummy") event.Skip()
# todo: fix populus support this via an deploy argument </s> if "jsonfile" in c.registrar.registrar_backends:	main Issuer = c.provider.get_base_contract_factory('Issuer') if not issuer_address: del c.registrar.registrar_backends["JSONFile"] assert master_address, "You need to give master-address"
# todo(asalkeld) support versions </s> opts = '-b -y'	_handle_gem_packages def _handle_gem_packages(self, packages): very basic support for gems for pkg_name, versions in packages.iteritems(): if len(versions) > 0:
# todo generator </s> list(save_dataset_hierarchy(	__call__ if dataset and dataset.is_installed(): _discover_trace_to_known(dataset.path, [], content_by_ds) content_by_ds, base=dataset.path if dataset and dataset.is_installed() else None,
# todo(tetsuro): remove this or condition when all </s> sa.or_(	_get_usages_by_provider_tree usage.c.used, ]).select_from(usage_join).where( rpt.c.root_provider_id.in_(root_ids), rpt.c.id.in_(root_ids)
# todo return xml fragment </s> response = httpresponse(content="no\n\n")	validation_failure def validation_failure(self): response.content_type = 'text/plain' return response
# todo alert? </s> raise runtimeerror(stderr)	set_agent_user_directory_permissions "{directory}: {stderr}.".format( user=volttron_agent_user, directory=agent_dir, stderr=stderr))
# todo: py3 branch fails here </s> assert str(c) == expected	bam_stream_bam None	0	chr2L	161	255	5M	*	0	0	GATAA	IIIII	NM:i:0	NH:i:1""")
# todo: raise </s> pass	parse_document_matcher return document_matchers.comment_reference else:
# todo: is this test is writing to the default system directory and ignoring updates to the passed filepath? </s> user_input = f'0\n' + f'{insecure_development_password}\n' + f'y\n'	stakeholder '--lock-periods', token_economics.minimum_locked_periods, '--force') click_runner.invoke(nucypher_cli, stake_args, input=user_input, catch_exceptions=False) init_args = ('stake', 'set-worker',
# todo: log exception </s> return none	add_tag return result except Exception as e:
# todo inference in python now missing postprocessing glue code </s> return none	export_scripting ts_model.save(os.path.join(args.output, "model.ts")) dump_torchscript_IR(ts_model, args.output)
# todo: clean up proposed streams here as well? -dan </s> self.proposed_streams = none	cancel_proposal notification_center.post_notification('SIPSessionProposalRejected', self, NotificationData(originator='local', code=0, reason='SIP core error: %s' % str(e), proposed_streams=proposed_streams)) except InvitationDisconnectedError, e: self.greenlet = None notification = Notification('SIPInvitationChangedState', e.invitation, e.data)
# todo unordered float </s> if a is none and b is none:	fcomi def fcomi(ir, instr, a=None, b=None): a, b = float_st0, float_st1 elif b is None:
raise notimplementederror # todo </s> list every item in entry_point that match request	search def search(self, entry_point, request):
downsized_img = transform.resize(gray_img, (84, 84), mode='constant')  # todo: check resizing doesn't cause problems </s> return torch.from_numpy(downsized_img).float()  # return 2d image tensor	_state_to_tensor def _state_to_tensor(state): gray_img = color.rgb2gray(state)  # TODO: Check image conversion doesn't cause problems
# todo: make this more portable with shutil etc. </s> phlsys_subprocess.run_commands("mkdir " + self.path)	setUp def setUp(self): phlsys_subprocess.run("git", "init", workingDir=self.path) self.clone = phlsys_git.GitClone(self.path)
# todo(andym): delete this once personas are migrated. </s> if not waffle.switch_is_active('personas-migration-completed'):	authors_other_addons .exclude(id=self.addon.id) .filter(type=amo.ADDON_PERSONA)) return (qs.filter(persona__author=self.author) .select_related('persona'))
# todo don't- delete if track is on local nets </s> self.board.removenative(track)	remove_tracks if containing: if not bounding_box.Contains(track_bb): else: if not bounding_box.Intersects(track_bb):
# todo append masters as named-instances as well; needs .designspace change. </s> gx = ttfont(master_ttfs[base_idx])	main style = instance[4] instance_list.append((style, loc)) _add_fvar(gx, axes, instance_list) print("Setting up glyph variations")
# todo: log discarded bytes? </s> return 'request discarded due to invalid crc.'	decode_in if not self.valid_crc(self.in_data[1:]): self.in_parsing = False if self.in_data[2] in self.command_map: return self.command_map[self.in_data[2]]()
# todo(guillermooo): implement a vs a register. </s> super().__setitem__(key.lower(), value)	__setitem__ if key in ('%', '#'): raise ValueError('invalid register key: %s' % key)
# todo: support aggregation functions sum, count, etc. </s> if func_name not in _supported_rolling_funcs:	_handle_rolling def _handle_rolling(self, lhs, rhs, obj_var, func_name, label): _supported_rolling_funcs = ['sum'] raise ValueError("only {} supported in rolling".format( ", ".join(_supported_rolling_funcs)))
# todo: move to base class </s> return self._manipulationmode	Canvas @property def manipulationMode(self): @manipulationMode.setter def manipulationMode(self, value):
pattern = re.compile(".*-\s"+"(\[[\sx]\]).*") # pattern for a markdown todo-list () </s> for i,line in enumerate(body):	toggle_checked_property_markdown checked = lambda x: "[x]" in x body = [line.encode('utf-8') for line in markdown_body.split("\n")] if pattern.match(line) is not None and item in line: if (checked(line)):
# todo: order of attributes is not assured; allow for any order. </s> match1 = '<words default-y="45.0" font-weight="bold" justify="left">very slowly</words>'	testExportMetronomeMarksD self.assertEqual(mm.number, None) p.insert(0, mm) match2 = '<per-minute>' raw = fromMusic21Object(p)
# todo: fix self.cursor_x >= w </s> if self.cursor_x == 0:	main_k_home def main_k_home(self, h, w): line = self.output.lines[self.win_y + self.cursor_y] while self.cursor_x < len(line):
# todo pydocs </s> @apply_defaults	BigQueryCheckOperator class BigQueryCheckOperator(CheckOperator): def __init__( self,
# todo - verify exit code is 0. using check_output() hangs, not sure why. tried shell=true which doesn't help </s> output = subprocess.getoutput("./backintime --config test/config backup")	test_local_snapshot_is_successful Install crontab: done Config test/config profile 'Main profile' is fine.''', re.MULTILINE)) self.assertRegex(output, re.compile(''' Back In Time
return deserialize(self.binary, from_bytes=true)  # todo: techdebt fix </s> @object.setter	BinObject @property def object(self): def object(self, value): self.binary = serialize(value, to_bytes=True)  # TODO: techdebt fix
# todo is this necessary? what if an error is raised within the generator? </s> gen.close()	_fasta_to_biological_sequence seq = curr_seq break if seq is None: raise FASTAFormatError(
# todo is there a way to actually test that the creds work? </s> passed("verified npm credentials")	check_npm_creds try: token = os.environ['NPM_TOKEN'] return token except Exception:
# todo: hack </s> network_authentication_id = container_stacks[0].getmetadataentry("network_authentication_id")	read stack = container_stacks[0] if self._resolve_strategies["machine"] == "override": network_authentication_key = container_stacks[0].getMetaDataEntry("network_authentication_key") container_stacks[0].deserialize(archive.open(global_stack_file).read().decode("utf-8"))
# todo: modifiers </s> self._pyvis_canvas.events.key(action='press', key=key, text=text)	on_key_press except Exception: text = ''
if dt_def is none:  # todo: check for errors </s> raise valueerror("invalid series.dt")	_run_assign if rhs_type == series_dt_methods_type: dt_def = guard(get_definition, self.func_ir, rhs.value) rhs.value = dt_def.value return self._run_DatetimeIndex_field(assign, assign.target, rhs)
# todo: how to handle not found authorname </s> result = db.session.query(db.authors).filter(db.authors.sort == auth.lstrip().strip()).first()	fill_indexpage error = False for auth in sort_authors: if not result: error = True
# todo: may test file contents </s> temp = tempfile.namedtemporaryfile(delete=false)	test_3_export_to_json_filename def test_3_export_to_json_filename(self): self.files_to_delete.append(temp.name) rows.export_to_json(utils.table, temp.name)
raise notimplementederror # todo </s> def em_step(self):	EM_step
# todo: catch unacceptable types (str, dict, etc) to avoid errors for other.child below </s> if is_acceptable_simple_type(other) or len(self.child) == len(other.child):  # type: ignore	__sub__ self, other: Union[RowEntityPhiTensor, AcceptableSimpleType] ) -> RowEntityPhiTensor: new_list = list() for i in range(len(self.child)):
# todo: temporary hack until they fix </s> return [	remove_deprecation_from_recwarn def remove_deprecation_from_recwarn(recwarn): item for item in recwarn if "TerminalReporter.writer" not in repr(item.message)
# todo: this isn't going to work right.  when we do incremental </s> if post.updated is not none:	extract_post d['created'] = post.thread.created d['updated'] = post.thread.last_post.created updated_since_epoch = time.mktime(post.updated.timetuple()) d['age'] = (time.time() - updated_since_epoch) / AGE_DIVISOR
# todo, this is not working if the action is not active (nla case for example) </s> trans, rot, scale = pose_bone_if_armature.matrix_basis.decompose()	gather_keyframes if isinstance(pose_bone_if_armature, bpy.types.PoseBone): bpy.context.scene.frame_set(frame) target_property = channels[0].data_path.split('.')[-1] key.value = {
# todo: use shape inference to figure out how large of an array </s> array_result = self.alloc_array(elt_t, niters)	transform_Map elt_t = expr.type.elt_type nested_args = [self.index_along_axis(arg, axis, i) for arg in args] self.blocks.push() call_result = self.invoke(fn, nested_args)
# todo remove above two lines </s> ds.run_procedure('setup_yoda_dataset')	test_procedure_discovery ds.add('.') ds.save() ok_clean_git(ds.path) ds.config.add(
# todo (elliot): put this in the preferences. </s> if prefs.get("stripdevelopersuffixes", false) is true:	generate_munki_recipe "manually add one to the munki recipe.") keys["Input"]["pkginfo"]["description"] = " " keys["Input"]["pkginfo"]["developer"] = strip_dev_suffix(facts.get("developer", '')) else:
raise notimplementederror # the below does most probably not work anymore todo </s> for branch in self.git.local_branches():	prune_closed_tickets TESTS:: TODO if self.git.is_ancestor_of(branch, MASTER_BRANCH): self._UI.show("Abandoning %s"%branch)
raise exceptions.mpdnotimplemented  # todo </s> underscore, dash, dot and colon.	subscribe already. The name may consist of alphanumeric ASCII characters plus
#remove the already-done entry (this connects to the other todo, </s> self.spendstate.loaded_schedule = self.spendstate.loaded_schedule[1:]	startMultiple w.statusBar().showMessage("Waiting for confirmation to restart..") txid = self.spendstate.loaded_schedule[0][5] self.waitingtxid=txid+":0" self.restartTimer.timeout.connect(self.restartWaitWrap)
# todo: other types like boolean </s> typ_val = _h5_typ_table[data_t.dtype]	gatherv_overload def gatherv_overload(data_t): assert isinstance(data_t, types.Array) func_text = ( "def gatherv_impl(data):\n"
# todo: add broadcasting to get_rotation_matrix2d for center </s> angle = angle.expand(tensor.shape[0])	rotate if center is None: center: torch.Tensor = _compute_tensor_center(tensor) center = center.expand(tensor.shape[0], -1) rotation_matrix: torch.Tensor = _compute_rotation_matrix(angle, center)
return ""  # todo: followup after decision around returning none </s> unix_time = unix_time - 11644473600	get_exit_time unix_time = self.ExitTime.QuadPart // 10000000 if unix_time == 0: return str(datetime.datetime.utcfromtimestamp(unix_time))
# todo move this to augmenters.size </s> def compute_croppings_to_reach_exponents_of(arr, height_base,	compute_croppings_to_reach_exponents_of width_base): For given axis size ``S``, cropped size ``S'`` and base ``B``
# todo(denero) fix user plumbing using @requires_authenticated_user </s> user = users.get_current_user()	post return create_api_response(400, 'Missing required field %s' % field) return self.submit(user, request.json['assignment'], request.json['messages'])
# todo update docstring </s> run ``flocker-deploy`` with given configuration files.	flocker_deploy def flocker_deploy(testcase, deployment_config, application_config): :param FilePath deployment: A YAML file describing the desired deployment configuration.
# todo(elliot): what info do we need for this recipe type? </s> pass	handle_app_input pass if recipes[i]["name"] == "absolute": if recipes[i]["name"] == "sccm": pass
# todo: the following skipped suite and fixtures should be enabled </s> return ['authorization']	_filter_headers def _filter_headers(self):
# todo: handle this case </s> self.assertequals(self.editor.get_end(), self.editor.get_insert())	test_next_word_stopping_at_capitals2 self.editor.next_word()
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_METADATA_fullname_matches_postScriptName_success def test_result_METADATA_fullname_matches_postScriptName_success(self): r = run_set(p, 'result') success_tests = r['success']
#todo: this can probably just be removed now? </s> self.listeners[listener] = element	addEventListener listener = Callback(element, cb, False) element.add_event_listener(event_name, listener, False, None)
# todo: what if there weren't enough contracts approved to distribute n kfrags?  we need to raise notenoughqualifiedursulas. </s> contract.activate(kfrag, ursula, result)	match_kfrags_to_found_ursulas kfrag = self.assign_kfrag_to_contract(contract)
# todo remove comment parameter. </s> if isinstance(constraint, bool):	Z3Solver return declarations def add(self, constraint, comment=None): if not constraint: self._status = 'unsat'
# todo: add highlighting line </s> return	mouseDoubleClickEvent new_line = self._lines.index(new_line[0]) self.verticalScrollBar().setValue(new_line) except IndexError: pass
# todo(lyarwood): test drivervolumeblockdevice.driver_detach in </s> conn_info_str = '{"foo": "bar"}'  # has no 'connector'.	test_detach_volume_evacuate_legacy case because nova does not have the info to get the connector for the original (evacuated) host. self._test_detach_volume_evacuate(conn_info_str)
tokenize_with_offsets=not use_sp_model,  # todo(b/181866850): drop this. </s> experimental_disable_assert=true,  # todo(b/175369555): drop this.	test_exported_callables preprocess = tf.saved_model.load(self._do_export( ["d", "ef", "abc", "xy"], do_lower_case=True, use_sp_model=use_sp_model)) def fold_dim(rt):
# todo: remove unescape_entities when mako html safe comes in </s> 'title': sanitize.unescape_entities(result['title']),	format_result 'contributors': result['contributors'], 'wiki_link': result['url'] + 'wiki/', 'url': result['url'], 'is_component': False if parent_info is None else True,
#todo: fix auditor+south </s> return httpresponseredirect(redirect_to)	login if request.session.test_cookie_worked(): request.session.delete_test_cookie() else: #failed login failed= form.data['username']
# todo: extract to _tmp and then move in a single command so we </s> lgr.debug("extracting {self._archive} under {path}".format(**locals()))	_extract_archive def _extract_archive(self, path): if exists(path): lgr.debug(
"""todo doc me""" </s> table = [['foo', 'bar', 'baz'],	test_profile_default def test_profile_default(): ['A', 1, 2], ['B', '2', '3.4'],
# todo verify </s> form = {'ajax': '1', 'pn': 'p1', 'htv': 'm'}	top30in30 def top30in30(self): req_url = "http://www.google.com/trends/hottrends/hotItems" req = self.ses.post(req_url, data=form)
# todo: add a 'comment' to the calculation </s> calc._set_state(calcstates.retrieved)	retrieve_finished_for_authinfo execlogger.error("Error parsing calc {}, I set it to RETRIEVED anyway. " "Traceback: {}".format(calc.uuid, buf.read())) else: execlogger.error("Error retrieving calc {}".format(calc.uuid))
'''todo: add docs''' </s> def __init__(self, app_model, app_controller, layout_class=none):	BaseView class BaseView(object): self.model = app_model self.controller = app_controller
# todo: check arp reply is valid </s> self.asserttrue(self.packet_outs_from_flows(arp_replies))	arp_for_controller 'arp_source_ip': '10.0.0.1', 'arp_target_ip': '10.0.0.254'})
# todo: here we should check for the leverage based on the config value </s> return	on_order_submission self.sell_orders[base_asset].append(np.array([order.qty, order.price]))
# todo created_at? </s> title=r['title'],	parse_file website = r.get('website') yield Subscription( url=website, id=r['id'],
# todo curves </s> for i in range(len(commands)):	specializeCommands op,args = commands[i] if op in {'rmoveto', 'rlineto'}:
conv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1), # todo: change to kernel_size=1, padding=0? </s> ])	create_mobilenetv1_ssd Conv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1), Conv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1), return SSD(num_classes, base_net, source_layer_indexes, extras, classification_headers, regression_headers)
time.sleep(5)  # todo: for some reason, events do not trigger instantly </s> notifications = list(test_folder.get_events(subscription_id, watermark))	test_pull_notifications i1.subject = get_random_string(8) i1.save(update_fields=['subject']) self.assertEqual(len(notifications), 1) notification = notifications[0]
# todo ... </s> if isinstance(token, cclosingbracket):	cpre3_parse curCObj = _CBaseWithOptBody(parent=parent) elif state == 50: # enum if token.brackets == curCObj._bracketlevel: state = 0
#todo: kvick we should rename 'short_circuit' to something like 'disable_service_start' </s> if config.get('short_circuit', false):	_configure_chroot log.critical('Installation of provisioning config failed') return False if not self._deactivate_provisioning_service_block(): log.critical('Failure short-circuiting files')
# todo: the stuff </s> pass	update pass for actor in trakt_movie.get('actors', ()): self.expired = False
return  #todo disabled for now, see #2151 for details </s> savecommand( savegamemanager.create_multiplayer_autosave_name() ).execute(self)	autosave def autosave(self): self.ingame_gui.show_popup(_("Not possible"), _("Save/load for multiplayer games is not possible yet"))
# todo: push stream to experiment exchange </s> pass	build stream=True, ):
# todo: remove force_masquerade parameter in future release </s> def add_port(zone, port, permanent=true, force_masquerade=none):	add_port Allow specific ports in a zone. .. versionadded:: 2015.8.0
# todo(b/148082271): remove this line once tft 0.22 is used. </s> transformed_features.pop(_transformed_name(_label_key), none)	serve_tf_examples_fn parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec) transformed_features = model.tft_layer(parsed_features) return model(transformed_features)
# todo other source types </s> return card	object_model @property def object_model(self):
# todo: add dirty file </s> return os.write(fh, data)	write os.lseek(fh, offset, 0)
# todo: i can't manage the import issue, can you? </s> new_self, new_args = syft.frameworks.torch.hook_args.hook_method_args(cmd, self, args)	handle_method_command cmd, self, args = command print("Logtensor logging method", cmd) new_command = (cmd, new_self, new_args) response = type(new_self).handle_method_command(new_command)
raise notimplementederror  # todo </s> def perturbate(self):	perturbate
dot_product_threshold = 0.95 # todo(ntonci): add to parameters </s> i = 0	align dq_B_H_vec_filtered = best_dq_B_H_vec_filtered n_quaternions = len(dq_W_E_vec_filtered) while i < len(dq_W_E_vec_filtered): dq_W_E_i = dq_W_E_vec_filtered[i]
# todo: make a tests that asserts that an observer can receive updates from the hyperparamsjsonrepository </s> repo: hyperparamsjsonrepository = hyperparamsjsonrepository()	test_hyperparams_json_repository_should_be_observable_with_file_system_changes def test_hyperparams_json_repository_should_be_observable_with_file_system_changes(): pass
# todo special case for b=c? </s> if b==c: continue	sample_K b = edges[i][1] c = edges[j][1] a = np.random.randint(0, n) k = Ka(D, m, b, c, a)
# todo: proper distinction between text and bytes. </s> replacements = 0	replace Returns: The number of replacements made. if self.content: with decoded(self):
# todo: this can unnecessarily suspend the starting of a build, in </s> log.msg("starting build %s.. pinging the slave %s" % (build, sb))	_ping def _ping(ign): return sb.ping()
# todo (rtibbles): sort out the status of quizzes, and either reinstate them or remove them. </s> ex_status = "complete"	user_progress else:
# todo: remove the check for deprecated check_docs after the extension has been removed </s> if (	cb_enable_all_extensions def cb_enable_all_extensions(self, option_name: str, value: None) -> None: for filename in os.listdir(os.path.dirname(extensions.__file__)): filename.endswith(".py") and not filename.startswith("_")
# todo: support auto-alignment, need a context object for this, eg: </s> self.stream.write("{:>{align}}:{:<5} {:9} {}\n".format(	CodePrinter Handle event and print filename, line number and source code. If event.kind is a `return` or `exception` also prints values. filename = event.filename or "<???>" basename(filename), event.lineno,
# reasons why we said no. todo: allow configurable error messages </s> raise synapseerror(403, "not allowed to publish room")	RoomCreationHandler user_id, room_id, room_alias ): directory_handler = self.hs.get_directory_handler() if room_alias:
# todo?: self.assert_eq(kdf.loc['a':'o', 'b':'d'], pdf.loc['a':'o', 'b':'d']) </s> self.assert_eq(kdf.loc[kdf.b > 0, 'b'], pdf.loc[pdf.b > 0, 'b'])	test_loc2d_duplicated_columns self.assert_eq(kdf.loc['j':'q', 'B'], pdf.loc['j':'q', 'B']) self.assert_eq(kdf.loc['j':'q', ['B']], pdf.loc['j':'q', ['B']])
# todo : test with/without leave_one_out </s> corpus = {'a.txt': 'lorem sit amet', 'b.txt': 'lorem ipsum'}	test_train_supervised_model_leave_one_out def test_train_supervised_model_leave_one_out(tmp_path): tmp_corpus = create_corpus(corpus, tmp_path) tmp_ref = tmp_path / 'ref.json'
# todo: implement </s> raise notimplementederror	remove_chat raise RuntimeError("No MSRP chat stream is active within this SIP session")
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> sampleparameters = {	test_acquire_token_with_user_pass def test_acquire_token_with_user_pass(self): "tenant" : "common", "authorityHostUrl" : "https://login.windows.net",
headers = csv_reader[0] # todo check size </s> csv_reader = csv_reader[1:]	_get_data if TASK_SERVER.RESULT_CACHE.get(): csv_reader = caches[CACHES_CELERY_QUERY_RESULT_KEY].get(_result_key(notebook)) # TODO check if expired else: f = storage.open(_result_key(notebook))
# todo remove in v8 </s> self.compile_html(source, dest, is_two_file)	compile def compile(self, source, dest, is_two_file=False, post=None, lang=None):
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
#todo: maybe it could be better as a background task </s> query = cls.query(cls.expiration_date <= datetime.datetime.now())	cleanupAssociations @classmethod def cleanupAssociations(cls): expired = query.fetch(keys_only=True) ndb.delete_multi(expired)
# todo: replace with below line when numba supports np.isin in nopython mode </s> values = set(values)	hpat_pandas_series_isin_impl def hpat_pandas_series_isin_impl(self, values): data_len = len(self._data) result = numpy.empty(data_len, dtype=numpy.bool_)
# todo: use correct priority instead of 0 </s> self.wconfd.client().updatelockswaiting(self._wconfdcontext, 0, request)	_AcquireLocks logging.debug("Trying %ss to request %s for %s", timeout, request, self._wconfdcontext) pending = utils.SimpleRetry(False, self.wconfd.Client().HasPendingRequest, 0.1, timeout, args=[self._wconfdcontext])
# todo: remove this skip after fixing </s> if sys.version[0] == 3:	test_regular_polygon_draw2 @requires_application() def test_regular_polygon_draw2(): raise SkipTest with TestingCanvas() as c:
# todo add locales </s> raise yunohosterror("no_setting_given")	domain_set_settings setting_set = True if not setting_set: with open(DOMAIN_SETTINGS_PATH, 'w') as file: yaml.dump(domains, file)
# todo in the future it's possible we'll want to break this out by action_type, in order to track </s> if period:	split_periods period = None elif base_action_type == 'consumption': period.add(tx)
@deprecated_alias(net='prev_layer', end_support_version=1.9)  # todo remove this line for the 1.9 release </s> def __init__(self, prev_layer, scale=2, act=tf.identity, name='subpixel_conv1d'):	__init__ def _PS(I, r): X = tf.transpose(I, [2, 1, 0])  # (r, w, b)
# todo: make pull request to get this custom vgg feature accepted </s> with slim.arg_scope(vgg.vgg_arg_scope()):	FCN_32s number_of_classes) upsample_filter_tensor = tf.constant(upsample_filter_np) logits, end_points = vgg.vgg_16(processed_images, num_classes=number_of_classes,
# todo: check the status before chaging the port </s> _xml = """<ribcl version="2.0">	configure_http_port .. code-block:: salt '*' ilo.configure_http_port 8080 <LOGIN USER_LOGIN="adminname" PASSWORD="password"> <RIB_INFO MODE="write">
# todo(emfree): remove after status overhaul. </s> if account.sync_state != 'running':	create_account account.client_id = response.get('client_id') account.client_secret = response.get('client_secret') account.sync_state = None return account
# todo: site-wide announcements. </s> return jingo.render(request, 'dashboards/questions.html',	questions def questions(request): announcements, etc.""" {'actions': _actions(Answer, request)})
# todo use blenddata </s> target = getattr(bpy.data, collection_name)[collection_key]	BpyIDRefProxy Save this proxy into bl_instance.attr_name collection_name, collection_key = self._blenddata_path if isinstance(bl_instance, T.bpy_prop_collection): logging.warning(f"Not implemented: BpyIDRefProxy.save() for IDRef into collection {bl_instance}{attr_name}")
1  # todo: fill in identifier </s> )	profile_transfer amount, target, result.wait() profiling.print_all_threads()
# todo: implement link-local handling in networkmanager backend and move this test into commontests() </s> self.assert_iface(self.dev_e_client, ['inet6 fe80:'], ['inet 169.254.'])	test_link_local_ipv6 self.generate_and_settle()
# todo check. </s> oprnd0 = tb.read(instruction.operands[0])	_translate_movd oprnd1 = tb.read(instruction.operands[1]) tmp0 = tb.temporal(oprnd0.size)
# todo: need token </s> val = self.mem.get(node.name)	ArithEvaluator cflow, and then the integer result can be ArithEval.Result() if node.tag == arith_expr_e.RightVar: if val.tag == value_e.Undef: return 0
# todo: add at least reflection tests before adding notimplemented version </s> def prioid(context, *args):	prioid *musicpd.org, current playlist section:* ``prioid {PRIORITY} {ID...}``
# todo: need to account for feature engineering here - probably need to have hardcoded test data rather than calculating splits here </s> for train_i, validation_i in cv_scheme.split(input_df, target_df):	expected_sentinels target_df = data[["target"]] input_df = data.drop(["target"], axis=1) train_sentinels.append((input_df.iloc[train_i, :], target_df.iloc[train_i, :])) validation_sentinels.append(
# todo: this decompose is used because of cache </s> return qc.decompose()	evolve expansion_mode, expansion_order) qc.append(instruction, quantum_registers)
except exception:  # todo - which exceptions? </s> pass  # skip unparsable tag	_parse_feature try: feature.qualifiers[feature_element.tag.replace(NS, '')] = feature_element.text self.ParsedSeqRecord.features.append(feature)
preprocessed_state_space=spaces.floatbox(shape=(2,)),  # todo: remove once auto preprocessor space inference done. </s> action_space=env.action_space	test_apex_assembly "configs/apex_agent_for_random_env.json", state_space=env.state_space, ) worker = SingleThreadedWorker(environment=env, agent=agent)
# todo: cleanup </s> def test_find_repo(self, createrepo):	TestRepo Test: Get a repo object repo = self.remote.get_repo("testrepo0") Test: find a repo object result = self.remote.find_repo({"name": "testrepo0"}, self.token)
# todo use base64 data </s> if handle not in self._ap_memif_handles:	_request__write_block8 def _request__write_block8(self, handle, addr, data): raise exceptions.Error("invalid handle received from remote memory access") self._ap_memif_handles[handle].write_memory_block8(addr, data)
# todo: total_reward isn't always greater than 95 even with a working implementation </s> environment = gym.make('dependentenv-v0')	test_dependent_environment def test_dependent_environment(): agent = Agent( environment.observation_space,
# todo: maybe change this later to push some more info, not just the </s> message = socket.inet_aton(self.config.get("global", "clientip"))	do_hello def do_hello(self): self.send(common.CONTROL_CHANNEL_BYTE, common.CONTROL_INIT+message, (self.server_tuple, None))
# todo: add support for heteroskedasticsingletaskgp </s> if isinstance(batch_model, heteroskedasticsingletaskgp):	batched_to_model_list >>> batch_gp = SingleTaskGP(train_X, train_Y) >>> list_gp = batched_to_model_list(batch_gp) raise NotImplementedError( "Conversion of HeteroskedasticSingleTaskGP currently not supported."
# todo add options to maodify the sorted by key and the header options </s> matrix = sorted(matrix, key=lambda endpoint: endpoint[1])	do_ignore endpoint.endpoint_data['ipv6']]) if len(matrix) > 0: matrix.insert(0, ['Name', 'MAC Address', 'Segment', 'Port', 'VLAN', 'IPv4', 'IPv6'])
# todo: handle this </s> context.pop_state()	RasterImage context.transform(concrete_width, 0, 0, concrete_height, 0, 0) context.draw_x_object(image_name)
1  # todo: fill in identifier </s> )	profile_transfer amount, target, result.wait() profiling.print_all_threads()
# todo: keep # pylint: disable=fixme </s> self.standard_button_min_width_px: final = 90	__init__ self.small_button_min_width_str = "75px" self.tiny_button_min_width_str = "60px" else: self.main_button_min_width_str = "100px"
# todo: preallocate! </s> i, j, v = [], [], []	edgeCurl if getattr(self, '_edgeCurl', None) is None: self.number() F = self._faces sign_edge = zip([-1,1,-1,1],[FEDGE0, FEDGE1, FEDGE2, FEDGE3])
# todo: user info </s> get_current_hub().capture_internal_exception()	processor except Exception:
# todo: use weight scaling factor if provided, xavier's default else </s> self.weights = sharedx(	DenoisingAutoencoder borrow=True ) .5 * rng.rand(conf['n_vis'], conf['n_hid']) * conf['irange'], name='W',
#todo: consider if this should check if the store belongs to a </s> return self.name.startswith('pootle-terminology')	is_terminology @property def is_terminology(self):
start_time_synced_s = none  # todo </s> duration_s = none  # todo	recording_update_pupil_mobile_to_pprf_2_0 recording_uuid = None #TODO start_time_system_s = None  # TODO recording_software_name = None  # TODO recording_software_version = None  # TODO
# todo improve precision </s> warnings.warn("the cohen-gismalla schemes are only given in single-precision.")	__init__ def __init__(self): self.name = "CohenGismalla(2)" self.degree = 1
# todo: rewrite tests </s> pass	test_resend_confirmation_get def test_resend_confirmation_get(self):
# todo: expose from marshal </s> def _r_long(int_bytes):	_r_long XXX Temporary until marshal's long function are exposed. x = int_bytes[0]
pass #todo: update pupil invisible recording to pupil capture v1.15 format </s> def _recording_update_pupil_mobile_to_v1_15(rec_dir: str):	_recording_update_pupil_mobile_to_v1_15
# todo action required that updates the endpoint </s> endpoints = []	collect_on def collect_on(self, args): eps = self._get_endpoints(args, -1) for endpoint in eps:
# todo: why the reversal? </s> verts.extend([px * sinr, py, px * cosr])	lathe for r in range (0, rl): cosr, sinr = from_polar_rad(pr * r) norms.extend([-sinr * dy, dx, -cosr * dy]) tex_coords.extend([tcx * r, tcy])
# todo index.json only if htmlsections in doc key.. </s> return fn.endswith("index.html") or fn.endswith("index.json")	is_index_page def is_index_page(self, doc): fn = doc.output().name
# todo: start here </s> for path in self._fs.ls(dir_path):	_archive_dir tar_gz_name = ( self._working_dir_mgr.name('dir', dir_path, name) + '.tar.gz') pass
#todo: refractor asap </s> return render(request, "analysis_copy.html", context)	analysis_copy project=analysis.project, access=required_access)
#todo enable again when farm is upgraded incl. the new offset calc </s> start = prediction["answers"][0]["offset_start"]	test_finder_offsets top_k_reader=5) assert prediction["answers"][0]["offset_start"] == 11 end = prediction["answers"][0]["offset_end"]
# todo: better exception handling </s> try:	process_track os.path.basename(ref_file)[:-4], "File names are different %s --- %s" \ % (os.path.basename(est_file)[:-4], os.path.basename(ref_file)[:-4]) one_res = compute_gt_results(est_file, ref_file, boundaries_id, labels_id, config,
# todo(lyarwood): test drivervolumeblockdevice.detach in </s> self._test_detach_volume()	test_detach_volume def test_detach_volume(self):
# update new uniqueid kodi 17 - todo get uniqueid_id for updates from embydb </s> if self.kodi_version > 16:	add_update ratingid =  self.kodi_db.create_entry_rating() self.kodi_db.add_ratings(ratingid, movieid, "movie", "default", rating, votecount) uniqueid =  self.kodi_db.create_entry_uniqueid() self.kodi_db.add_uniqueid(uniqueid, movieid, "movie", imdb, "imdb")
# todo_recorders - need to pass in parent info instead of none </s> metadata = create_local_meta(none, type(self).__name__)	_run_iterator self._iter_count += 1 norm = self._iter_get_norm() update_local_meta(metadata, (self._iter_count,)) self._rec_mgr.record_iteration(self, metadata, abs=norm, rel=norm / norm0)
# todo: test for first revision on last page. </s> offset = url_for(controller='revision', action='list')	test_list_long self.create_100_revisions() try: res = self.app.get(offset) self.assert_click(res, '2', 'Revision 2')
# todo add the ability to `git reset --hard` the dataset tree on failure </s> lgr.info("== command exit (modification check follows) =====")	__call__ except CommandError as e: cmd_exitcode = e.code run_info = { 'cmd': cmd,
# todo(jflesch): i18n / l10n </s> score = 0	_compute_ocr_score Current implementation: The score is the number of words only made of 4 or more letters ([a-zA-Z]) prog = re.compile(r'^[a-zA-Z]{4,}$') for word in txt.split(" "):
breaks = {}   # todo: support more than one breakpoint per line </s> for line in response.decode('utf-8').splitlines():	ProcessResponse def ProcessResponse(self, response): pattern = re.compile("([^:]+):(\d+)") try: fields = re.split("\s+", line)
# todo: verify that workid is the primary key someplace. </s> yield workitem.dowork()	perform workItemClass = WorkItem.forTable(tableSyntax) workItem = yield workItemClass.load(workID)
# todo: test me. </s> @stashed_action.setter	stashed_action def stashed_action(self, name): self.settings.vi['stashed_action'] = name
# todo: process the remaining tag types. </s> print captures['name']	_handle_match buffer.append(captures['whitespace']) captures['whitespace'] = '' fetch = lambda view: unicode(view.get(captures['name'])) if captures['tag'] == '!':
# todo add inv parameter? </s> params1d = [	SigmoidContrast def __init__(self, gain=10, cutoff=0.5, per_channel=False, name=None, deterministic=False, random_state=None): iap.handle_continuous_param( gain, "gain", value_range=(0, None), tuple_to_uniform=True,
# todo: should allow multi_dimensional inputs/outputs </s> layer_sizes=(*translation_hidden_sizes, inputs.shape.as_list()[-1]))	translation_wrapper return feedforward_net( inputs,
# todo: change to nih:sha-256; hashes </s> document.add_namespace('data', 'urn:hash::sha1:')	generate_provDoc document.add_namespace('run', 'urn:uuid:') document.add_namespace('engine', 'urn:uuid:') WorkflowRunID="run:"+WorkflowRunUUID ro_base = "arcp://uuid,"+WorkflowRunUUID+"/"
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_interact_arguments @classmethod def add_interact_arguments(cls, parser): p_convert = p_operation.add_parser( "convert", help="convert VGM to PCM using OPL hardware")
# todo partially update stored playlists? </s> len(tracks), new_position, playlist.name())	tracks_moved u'%d track(s) moved to position %d in playlist "%s"',
# xxx todo alignement check </s> return [m2_expr.expraff(a, b)], []	movdqu def movdqu(ir, instr, a, b):
raise  # todo: what if our seed node fails verification? </s> return potential_seed_node	learn_from_seednode potential_seed_node.verify_node(self, accept_federated_only=accept_federated_only) except potential_seed_node.InvalidNode:
# todo this thing is too wide </s> row.operator("luxcore.material_copy", text=str(material.users))	lux_mat_template_ID row.prop(material, "name", text="") if material.users > 1: row.prop(material, "use_fake_user", text="") row.operator("luxcore.material_copy", text="", icon=icons.DUPLICATE)
# todo: prepopulate </s> forms = gen_unprocessed_growth_monitoring_forms()	test_gen_unprocessed_growth_monitoring_forms prepopulated = [{'foo': True}] assert forms == prepopulated
# todo(mriedem): remove this conversion when all neutronv2 apis are </s> self.instance = fake_instance.fake_instance_obj(self.context,	test_deallocate_for_instance_port_not_found def test_deallocate_for_instance_port_not_found(self): **self.instance) port_data = self.port_data1
# todo: expect_match should work with emit() </s> m = state.expect_match(	scan_command_edit if k == '+': state.ignore() r'(?:f(?:ile)?f(?:ormat)?|(?:file)?enc(?:oding)?|(?:no)?bin(?:ary)?|bad|edit)(?=\s|$)', lambda: VimError(ERR_INVALID_ARGUMENT))
# todo(mordred) when this changes to rest, force interface=admin </s> with _utils.shade_exceptions("failed to list endpoints"):	list_endpoints :raises: ``OpenStackCloudException``: if something goes wrong during the openstack API call. endpoints = self.manager.submit_task(_tasks.EndpointList()) return endpoints
# todo: there is probably a robuster way than a sleep. </s> time.sleep(0.5)	post ) except requests.ConnectionError: else: break
# todo check </s> return self.mimetype.split('/')[-1]	encoding @interfacedoc def encoding(self):
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_METADATA_style_matches_postScriptName_success def test_result_METADATA_style_matches_postScriptName_success(self): r = run_set(p, 'result') success_tests = r['success']
# todo action required that updates the endpoint </s> endpoints = []	collect_on def collect_on(self, args): eps = self._get_endpoints(args, -1) for endpoint in eps:
1  # todo: fill in identifier </s> )	profile_transfer amount, target, result.wait() profiling.print_all_threads()
# todo: verify the key is rsa </s> blocklen = ceil_shift(key.size(),3)	sign If the key length is not sufficiently long to deal with the given hash algorithm. em = EMSA_PKCS1_V1_5_ENCODE(m, blockLen) sig = key.sign(em)
#create and insert todo on remote site </s> todo = frappe.get_doc(dict(doctype='todo', description=description, assigned_by='administrator'))	insert_into_master def insert_into_master(self, master, description): return master.insert(todo)
# todo(akshakya): validate shapes of params. </s> if len(params) != len(self.param_ids):	CvxpyLayer a list of optimal variable values, one for each CVXPY Variable supplied to the constructor. raise ValueError('A tensor must be provided for each CVXPY ' 'parameter; received %d tensors, expected %d' % (
# todo: move log_pi and correction under self.log_pi_for() </s> (self.distribution.log_p_t	log_diagnostics self.distribution.mu_t, self.distribution.log_sig_t, - self._squash_correction(self.distribution.x_t)), ),
# todo: remove when old stats are removed </s> old_stats = tp0.get_stats(include_children=false)	test_data_tool_tp_get_stats def test_data_tool_tp_get_stats(tp0): stats = tp0.data_tool.get_stats(include_children=False) assert ( sorted(old_stats.keys())
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_interact_arguments @classmethod def add_interact_arguments(cls, parser): p_convert = p_operation.add_parser( "convert", help="convert VGM to PCM using OPL hardware")
# todo: handle agg_columns. </s> kdf = kdf[	GroupBy for i in range(groupkey_length) ] [s.rename(label) for s, label in zip(self._groupkeys, groupkey_labels)] + [kdf._kser_for(label) for label in kdf._internal.column_labels]
# todo? don't consider the empty set here </s> for output_indices in utils.powerset(indices):	all_blackboxes Blackbox: The next |Blackbox| of ``indices``. for partition in all_partitions(indices): blackbox = Blackbox(partition, output_indices) try:  # Ensure every box has at least one output
# todo: remove this when fixed in: https://github.com/seleniumhq/selenium/issues/767 </s> self.browser.service.process.send_signal(signal.sigterm)	teardown_browser def teardown_browser(self): self.browser.close() self.browser.quit()
# todo: implement </s> pass	result_by_parent_name @staticmethod def result_by_parent_name(parent_name):
# todo proper error messages </s> result = {}	read_check_options def read_check_options(options): keys = list(options.keys()) if "System`AnchoredSearch" in keys:
# @todo: this has a chance to spam the user with notifications </s> return	push_message def push_message(msg): if not clients: main_io_loop = app.instance.web_server.io_loop for client in clients:
# todo: this is not the best place to configure rank? why is rank not </s> train_image.set_shape((none, none, 3))	evaluate train_image = train_dataset['image'] train_bboxes = train_dataset['bboxes'] train_image = tf.expand_dims(train_image, 0) pretrained_dict = pretrained(train_image, is_training=False)
# todo: add lon lats ! </s> satscene.orbit = mda[orbit_idx + 111:orbit_idx + 116]	load500 orbit_idx = mda.index("ORBITNUMBER")
self.assertequals(status, 200) # todo: 202 when asynchronous </s> status, body = self.post(path, body)	test_install options=options,)
# todo: pandas returns dataframe, maybe return namedtuple instread of </s> res = "count    " + str(a_count) + "\n"\	_column_describe_impl q50 = hpat.hiframes_api.quantile(A, .5) q75 = hpat.hiframes_api.quantile(A, .75) "mean     " + str(a_mean) + "\n"\ "std      " + str(a_std) + "\n"\
# todo(harlowja): the bug 1214083 is causing problems </s> log.debug(_("%(flow)s has moved into state %(state)s from state"	flow_log_change def flow_log_change(state, details): " %(old_state)s") % {'state': state, 'old_state': details.get('old_state'),
# todo: pytorch v0.4 has torch.where function </s> nrm_trn_idx = torch.from_numpy(np.where(trn_lbl.numpy() == nrm_cls_idx)[0])	get_mnist2_anomaly_dataset Returns: [tensor] -- New training-test images and labels. abn_trn_idx = torch.from_numpy(np.where(trn_lbl.numpy() != nrm_cls_idx)[0]) nrm_tst_idx = torch.from_numpy(np.where(tst_lbl.numpy() == nrm_cls_idx)[0])
# todo change when v4 web3.py will released </s> assert another_mock_dht_key == escrow().getminerid(miner_addr, 1).encode('latin-1')	test_publish_dht_key assert len(stored_miner_dht_keys) == 2 assert another_mock_dht_key == stored_miner_dht_keys[1]
# todo: handle values and aggfunc options </s> in_vars = {}	_handle_crosstab index_arg = self._get_arg('crosstab', rhs.args, kws, 0, 'index') columns_arg = self._get_arg('crosstab', rhs.args, kws, 1, 'columns') out_typ = types.intp out_types = {'__dummy__': out_typ}
self._cloud_flow_complete_message.addaction("", i18n_catalog.i18nc("@action", "review your connection"), "", "", 1) # todo: icon </s> self._cloud_flow_complete_message.actiontriggered.connect(self._onreviewcloudconnection)	_onCloudPrintingConfigured ) if self._application.getMachineManager().activeMachineHasNetworkConnection: self._cloud_flow_complete_message.show() active_machine = self._application.getMachineManager().activeMachine
# todo(sirp): snet=false for now, however, if the instance of </s> swift_conn = conn_class(	get (user, key, authurl, container, obj) = \ cls._parse_swift_tokens(parsed_uri) authurl=authurl, user=user, key=key, snet=False) (resp_headers, resp_body) = swift_conn.get_object(
# todo: expose from marshal </s> def _r_long(int_bytes):	_r_long XXX Temporary until marshal's long function are exposed. x = int_bytes[0]
engine = sel.bind  # todo: get engine from select </s> with engine.connect() as conn:	select_to_iterator @convert.register(Iterator, sa.sql.Select, cost=300.0) def select_to_iterator(sel, dshape=None, **kwargs): result = conn.execute(sel) if not dshape:
# act as if we were to install the packages in todownload </s> for po in todownload:	findDependencies if True: self.doTsSetup() self.tsInfo.addInstall(po) self.localPackages.append(po)
#todo, multipart raw submissions need further parsing capacity. </s> instance = request.raw_post_data	get_instance_and_attachment attachments[key] = item else: return instance, attachments
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo - this isn't actually the correct way to set the vary header, </s> response.headers['allow'] = ', '.join(self.allowed_methods)	dispatch except ErrorResponse, exc: response = exc.response response.headers['Vary'] = 'Authenticate, Accept' response.headers.update(self.headers)
# todo: fix this to show all configs </s> stage_configurations = self.stage.stage_configurations().all()	get_form def get_form(self, form_class): form = form_class(**self.get_form_kwargs()) for config in stage_configurations.filter(prompt_me_for_input=True):
# todo bucket-owner-read and bucket-owner-full-control </s> return next((ca['acl'] for ca in canned_acls if ca['grants'] == non_owner_grants), 'custom')	determine_mode grants = acl.grants non_owner_grants = [grant for grant in grants if not (grant['Grantee'].get('ID') == owner['ID'] and grant['Permission'] == 'FULL_CONTROL')]
# todo...or leave it to user's default? </s> })	create_activity Returns: dict with 'id' and 'url' keys for the newly created Facebook object data = urllib.urlencode({'message': activity.get('content').encode('utf-8'), resp = json.loads(self.urlopen(API_FEED_URL, data=data).read()) resp['url'] = self.post_url(resp)
assert 'not both' in res.stdout  # todo: stderr </s> res = cc.fail_1(['glom', '--spec-file', basic_spec_path + 'abra', '--target-file', basic_target_path])	test_usage_errors res = cc.fail_1(['glom', '--target-file', basic_target_path, BASIC_SPEC, BASIC_TARGET]) assert 'target' in res.stdout assert 'could not read spec file' in res.stdout  # TODO: stderr res = cc.fail_1(['glom', '--spec-file', basic_spec_path, '--target-file', basic_target_path + 'abra'])
# todo: check that stack trace is maintained. </s> m2 = m1.excluding('shapeopt')	test_2 topo = f1.maker.fgraph.toposort() assert not any(isinstance(n.op, tensor.basic.Reshape) for n in topo) f2 = theano.function([x], r, mode=m2) topo = f2.maker.fgraph.toposort()
# todo: distributed search messages need to implemented </s> self.logmessage("%s %s" %(msg.__class__, vars(msg)), 4)	ChildDepth def ChildDepth(self, msg):
# todo extend to nonbinary nodes </s> if i not in self._input_indices and i in self.context.node_indices:	__init__ else: self._dimension_labels.append(None) past_tpm_on = past_tpm_on.sum(i, keepdims=True) / 2 past_tpm_off = past_tpm_off.sum(i, keepdims=True) / 2
# todo(mriedem): consider skipping the microversion discovery </s> cinderclient(	attachment_delete def attachment_delete(self, context, attachment_id): try: context, '3.27').attachments.delete(attachment_id) except cinder_exception.ClientException as ex:
# fixme todo we should verify we get a circuit_new event for </s> timeout = 10	test_build_circuit_nottimedout path.append(FakeRouter("$%040d" % x)) path[0].flags = ['guard'] clock = task.Clock() d = build_timeout_circuit(self.state, clock, path, timeout, using_guards=True)
# todo: wobble </s> return einsum('i...,ij...->j...', vector, rotation)	spin ])
# todo: vectorize with numpy </s> for value in np.nditer(current_values):	validate_quant_values updated_values = current_values has_to_be_rounded = False if not dtype.allowed(value): has_to_be_rounded = True
# todo: avoid magic string value </s> norm_type = u'reference'	_set_normalizations if new_norm_id not in old_norms: new_id = ann_obj.get_new_id('N') new_norm = NormalizationAnnotation(new_id, norm_type, ann.id, new_norm_id[0],
raise tornado.gen.return(notebook)  # todo py2: replace by return </s> notebook.metadata.kernelspec.language = all_kernel_specs[kernel_name]['spec']['language']	fix_notebook notebook.metadata.kernelspec.display_name = all_kernel_specs[kernel_name]['spec']['display_name']
# todo: hack </s> container_copy = copy.deepcopy(container)	_writeContainerToArchive file_in_archive.compress_type = zipfile.ZIP_DEFLATED if type(container) == ContainerStack and (container.getMetaDataEntry("network_authentication_id") or container.getMetaDataEntry("network_authentication_key")): container_copy.removeMetaDataEntry("network_authentication_id") container_copy.removeMetaDataEntry("network_authentication_key")
# todo(ytknzw): add more specific assertion with the test case. </s> study = create_study()	test_plot_intermediate_values trial.report(2.0, step=1) return 0.0 study.optimize(lambda t: objective(t, True), n_trials=1) figure = plot_intermediate_values(study)
# todo(mattjj): remove this special case, used for debugging on cpu </s> dims = c.getshape(x).dimensions()	split_array def split_array(shape, x): if xb.get_replica_count() == 1: return c.Reshape(x, None, dims[1:]) else:
from _devbuild.gen.id_kind_asdl import id  # todo: fix circular dep </s> return self.token.id == id.controlflow_return	IsReturn def IsReturn(self):
mock = create_mock_json('tests/resources/list_race_details.json')  # todo </s> mock_response.return_value = mock	test_list_scores @mock.patch('betfairlightweight.endpoints.scores.Scores.request') def test_list_scores(self, mock_response): mock_update_keys = mock.Mock() response = self.scores.list_scores(mock_update_keys)
# todo(mattjj): test that constants/literals are set up properly </s> self.assertallclose(f1_vjp(x), f2_vjp(x), check_dtypes=true)	test_jarrett_jvps _, f2_vjp = api.vjp(f2, x)
# todo present sample rate configuration using source.get_sample_rates().values() </s> self.__osmo_device = osmo_device	__init__ **kwargs): Source.__init__(self, name=name, **kwargs) self.freq = freq = 98e6 self.correction_ppm = 0
# todo: project_id is tried to load with api_key. do not load it? </s> assert data['exception_cls'] == 'attributeerror', data	test_query_announcement res = self.app_get_json(url + '?api_key=' + owner.api_key) data = json.loads(res.data) res = self.app.get(url + "?title=wrongvalue") data = json.loads(res.data)
# todo: this shader is drawn over regular shader now, drawing only one should speed it up </s> vs = ci['vertices']	render batch.draw(shader) if(pcv.dev_depth_enabled): l = ci['current_display_percent']
#todo, is this the best way to handle </s> self.background_rectangle = backgroundrectangle(self)	add_background_rectangle def add_background_rectangle(self): self.submobjects = [ self.background_rectangle,
# todo: we haven't read all the data, actually </s> break	handle_tcp raise Exception('failed to send all data') if should_break: finally: sock.close()
#set limit to 25 --> currently hardcoded --> todo: add a setting for this ? </s> subelement(root, "limit").text = "25"	addVideoNodesForTag SubElement(Rule, "value").text = tagname SubElement(root, "order", {"direction":"descending"}).text = "dateadded" Rule2 = SubElement(root, "rule", {"field":"playcount","operator":"is"}) SubElement(Rule2, "value").text = "0"
# todo recover multiple tables at the same time. </s> for table in self.values():	TableManager await self._on_recovery_started() await self.app.consumer.pause_partitions(assigned) await self._recover_from_changelog(table, assigned) await self.app.consumer.resume_partitions({
#todo(nmakhotkin) we should use thing such a decorator here </s> abort(400, e.message)	post execution.target_task) except ex.MistralException as e: return Execution.from_dict(values)
# todo: check ping response </s> self.asserttrue(self.packet_outs_from_flows(echo_replies))	test_icmp_ping_controller 'ipv4_dst': '10.0.0.254', 'echo_request_data': bytes('A'*8, encoding='UTF-8')})
# todo(huangyp): make sure striding won't cross segment boundaries. </s> tf.logging.warning('each segment in the packed input should has length '	_FunnelAttention if stride > 1: assert p.funnel_pool_tpl.begin_intact == 0 'divisible by stride.') sub_list += [
# todo: add logging </s> pass	parse_intent def parse_intent(self, token='ri', params='n'):
# todo: not implemented yet </s> messager.warning('text search not implemented yet, sorry!')	search_text def search_text(directory, text): return format_results(SearchMatchSet('empty'))
# todo it is a bit hack-ish, is it possible to find a more generic fix? </s> fwk_pattern = 'python.framework/versions/2\.[0-9]/python$'	remove_python_framework_dir :return: The same list as `cmd`, but without the element of the form mentioned above, if one exists. rval = [element for element in cmd if (re.search(fwk_pattern, element) is None
# todo: scale and translation could be merged into a single network </s> with tf.variable_scope("scale"):	backward mask = self.get_mask(inputs, dtype=inputs.dtype) masked_inputs = inputs * mask scale = self.scale_fn(masked_inputs) with tf.variable_scope("translation"):
# todo: handle marked with template </s> return none	should_archive_thread duration = str2localized_duration(self.site, re_t.group(1)) return ('duration', duration)
# todo - mux support </s> for signal, value in zip(signals, bitstruct.unpack(fmt, data)):	Frame signals = sorted(self.signals, key=lambda s: s.getStartbit()) signals_values = OrderedDict() signals_values[signal.name] = signal.raw2phys(value, decodeToStr) return signals_values
# todo: use shlex.quote as soon as a newer python version is available. </s> quoted_file_path = pipes.quote(file_path)	check_file def check_file(self, file_path): rvm_cmd = os.path.expanduser('~/.rvm/bin/rvm-auto-ruby') rubocop_cmd = rvm_cmd + ' -S rubocop ' + quoted_file_path self.run_shell_command(rubocop_cmd)
categories = category.objects.filter(status=1)  # todo: fix magic number </s> nav_cates = []	post_list except EmptyPage: posts = paginator.page(paginator.num_pages) cates = [] for cate in categories:
except exception:  # todo: refactor this... </s> e = sys.exc_info()[0]	test_GET_InvalidData try: self.assertEqual(self.rnw.GET(url, data), 'json') self.assertEquals(e, TypeError)
#todo: add method </s> userinfo = {'ratings': {}}	doManualRating elif utilities.isShow(media_type): summaryInfo = {'title': 'Show Title', 'year': 2015, 'ids': {'imdb': data['imdbnumber']}} elif utilities.isMovie(media_type): summaryInfo = {'title': 'Movie Title', 'year': 2015, 'ids': {'imdb': data['imdbnumber']}}
# todo(stephenfin): enable this once we drop use of </s> api_db_group: api_db_opts,	list_opts def list_opts(): return {
# todo: fails because of missing svg support </s> assert_pixels('inline_image_' + filename, 8, 8, image, '''	test_images )) def test_images(filename, image): <style> @page { size: 8px }
# todo(b/134526360): xla doesn't support integer dots, so we emit a sum of </s> if onp.issubdtype(lhs.dtype, onp.integer):	dot Returns: An array containing the product. lhs_shape = onp.shape(lhs) lhs_ndim = len(lhs_shape)
# todo: remove uris, replacing it with support in query language. </s> raise notimplementederror	lookup :rtype: :class:`mopidy.models.Track`
# todo get tri-letter dimensionality from fit-transform as input shape </s> params['input_shapes'] = [(90000, 10), (90000, 50)]	get_default_params params = super().get_default_params() params['optimizer'] = 'sgd' params.add(engine.Param('w_initializer', 'glorot_normal')) params.add(engine.Param('b_initializer', 'zeros'))
return none  #todo: fix logic above, inserting this just to fix warnings </s> return d	get_byte_range f.close()
# todo(sloria): test me </s> return {'result': node_addon.to_json(user), 'status': 200}, 200	dropbox_import_user_auth node_addon.save()
chunk_size = 150 # todo: tune </s> for addresses_chunk in chunks(list(instructions), chunk_size):	_async_clear_instructions def _async_clear_instructions(self, instructions): Internal routine for asynchrnous instruction clearing. self._clear_instructions(addresses_chunk) if self._repaint_requested:
n)  # todo: access alice's private key inside this method. </s> policy = policy.from_alice(	create_policy_group alice_priv_enc = self.owner._crypto_power._power_ups[EncryptingPower].priv_key kfrags, pfrag = self.owner.generate_rekey_frags(alice_priv_enc, bob, m, alice=self.owner, bob=bob,
# todo: remove verify ssl config when working without it. </s> responses = balearicislands(ses, verify=false).get_all()	fetch_exchange raise NotImplementedError('This parser is not yet able to parse past dates') ses = session or Session() if not responses: raise ParserException("ES-IB", "No response")
# todo: add json schema validation </s> testcase = {	load_testcases_folder testcases_items_mapping = load_folder_content(testcases_folder_path) for testcase_file_path, testcase_items in testcases_items_mapping.items(): "config": { "path": testcase_file_path
# todo: re-enable this when we bring back unsubscribe (bug 802379). </s> assert ': brazil and united kingdom.' in msg.body	test_email_for_two_new_regions assert ' added two new ' in msg.body
# todo maybe this could be moved to trakt.media class? </s> return trakt.request(	get_trakt_ratings @staticmethod def get_trakt_ratings(media, retry=True): 'user/ratings/%s.json' % media, param=Prefs['username'],
# todo: see get_scale_factor() to choose 72 px on hidpi </s> return getattr(self.row.icon._impl, "native_" + str(32))	get_icon def get_icon(self, row): row.icon.bind(self.interface.factory)
# todo: error handling? </s> update = await self.update_status('started', task_id, session)	PipelineStepRunner } container = await docker_client.containers.run(config=config) if wait_on_completion: await container.wait()
# todo: check if releasing locks early still makes sense </s> _releaselocks(self.lu, locking.level_node_res)	_ExecDrbd8DiskOnly cstep += 1 self._RemoveOldStorage(self.target_node, iv_names) else: _ReleaseLocks(self.lu, locking.LEVEL_NODE_RES,
node = self.__read_metadata(filepath=metadata_path, federated_only=federated_only)  # todo: 466 </s> known_nodes.add(node)	LocalFileBasedNodeStorage for filename in filenames: metadata_path = os.path.join(self.metadata_dir, filename) return known_nodes @validate_checksum_address
# todo: load state into here </s> current_chunk +	inner_mapper new_schedule.extend( [CallKernel(kernel_name=new_kernel_name)] + [ReturnFromKernel(kernel_name=new_kernel_name)]) new_schedule.extend(
# todo(b/158462888): use aggregete losses that works with replicas. </s> tf.reduce_sum(input_tensor=tf.square(v)) * self._policy_l2_reg	l2_regularization_loss policy_vars_to_regularize, vf_vars_to_regularize) policy_l2_losses = [ for v in unshared_policy_vars_to_regularize ]
# todo: modifiers </s> self._pyvis_canvas.events.key(name='press', key=key, text=text)	keyPressEvent key = self._processKey(event) text = str(event.text())
categories = category.objects.filter(status=1)  # todo: fix magic number </s> nav_cates = []	post_list except EmptyPage: posts = paginator.page(paginator.num_pages) cates = [] for cate in categories:
# todo: duplicate + replace with psutil.getloadavg()? (available in 5.6.2) </s> load1m, _, _ = os.getloadavg()	set_turbo def set_turbo(): cpuload = p.cpu_percent(interval=1) if load1m > 2:
# todo: fix self.cursor_x >= w </s> line = self.win_y + self.cursor_y	main_cmd_next_bracket def main_cmd_next_bracket(self, h, w): x = self.cursor_x char = self.output.lines[line][x]
# todo: seems like a pandas' bug when fill_value is not none? </s> self.assert_eq(kdf.groupby(['b'])['a'].shift().sort_index(),	test_shift self.assert_eq(kdf.groupby('a').shift().sort_index(), pdf.groupby('a').shift().sort_index()) pdf.groupby(['b'])['a'].shift().sort_index(), almost=True) self.assert_eq(kdf.groupby(['a', 'b'])['c'].shift().sort_index(),
# todo: make sure limits are deterministic then update this </s> assert self.viewer.axes.get_ylabel() == 'world 0'	test_basic assert self.viewer.state.x_att_world is self.image1.id['World 1'] assert self.viewer.state.x_att is self.image1.pixel_component_ids[1] assert self.viewer.state.y_att_world is self.image1.id['World 0'] assert self.viewer.state.y_att is self.image1.pixel_component_ids[0]
pass # todo: explain </s> pass # todo: explain	status500 def status500(self):        # Internal Server Error
# todo: support speedy mode for running the script </s> shell("make scriptconfig script=kconfiglib/allmodconfig.py "	test_allmodconfig 'make allmodconfig', for each architecture. Runs the script via 'make scriptconfig', so kinda slow even in speedy mode. "PYTHONCMD='{}'".format(sys.executable)) shell("mv .config ._config")
# todo: find a better random value </s> return datetime.date.today()	_auto_value return datetime.datetime.now() elif prop.type == datetime.date: elif prop.type == float: return uuid.uuid4().int / float(uuid.uuid4().int)
# todo test this </s> raise usageerror(str(e))	parseArgs deployment_configuration=deployment_config) except ConfigurationError as e:
# todo(yuriyz): change to 404 (bug 1200517) </s> self.assertequal(response.status_int, 500)	test_update_not_found response = self.patch_json('/ports/%s' % uuid, {'extra': {'a': 'b'}}, expect_errors=True) self.assertEqual(response.content_type, 'application/json') self.assertTrue(response.json['error_message'])
# todo: we should throw here, i don't like this. </s> msg = 'limit "%s" specified, maximum value is "%s"' % (limit, self.max_limit)	ResourceController raise ValueError(msg) if int(limit) > self.max_limit: raise ValueError(msg) eop = offset + int(limit) if limit else None
tol = 0.15  # todo(skye): can we be more precise? </s> jtu.check_grads(np_fn, args_maker(), order=1, atol=tol, rtol=tol)	testRfftfreq self._CompileAndCheck(np_fn, args_maker, check_dtypes=True) if dtype in inexact_dtypes: jtu.check_grads(np_fn, args_maker(), order=2, atol=tol, rtol=tol)
# todo the following way of testing is highly sensitive to small changes </s> assert len(model.graph.input) == 32	test_brevitas_to_onnx_export ) model = onnx.load(export_onnx_path) assert len(model.graph.node) == 33 assert len(model.graph.output) == 1
# todo: clean up </s> for _ in range(num_hosts)	gossipsubs )
# todo(kumar) remove this when validator is fixed, see bug 620503 </s> apps = os.path.join(os.path.dirname(validator.__file__),	_validator import validator.constants validator.constants.SPIDERMONKEY_INSTALLATION = settings.SPIDERMONKEY 'app_versions.json') return validate(upload.path, format='json',
# todo: take namespace into account, currently doesn't matter since </s> with session_scope() as db_session:	headers_for_message @jsonify def headers_for_message(self, message_id): message = db_session.query(Message).filter(Message.id==message_id).one() return message.headers
# todo: support steps and times (motion blur) </s> print("dupli export took %.3fs" % (time() - start))	create_session transformations = array("f", duplis.matrices) luxcore_scene.DuplicateObject(src_name, dst_name, count, transformations) engine.update_progress(index / len_objs)
#todo respect et mapped namespaces </s> itag = xml.tag.split('}', 1)[-1]	__str__ xml = self.xml newoutput = [stringbuffer] if '}' in xml.tag: ixmlns = xml.tag.split('}', 1)[0][1:]
# todo: handle timeout </s> return self.__socket.recv(1024)	read @keyword timeout: the maximum time in millisecond to wait before a message can be reached @type timeout: :class:`int`
# todo extend to nonbinary nodes </s> return (np.ones(number_of_states) /	uniform_distribution np.ndarray: The uniform distribution over the set of nodes. number_of_states = 2 ** number_of_nodes number_of_states).reshape([2] * number_of_nodes)
# todo: remove # pylint: disable=fixme </s> self.main_button_min_width_str = "110px"	__init__ self.button_font_size_pt = self.button_font_size_pt - 2 if platf == 'Windows': self.small_button_min_width_str = "75px" self.tiny_button_min_width_str = "60px"
# todo: remove </s> if user.is_moderator:	for_update_or_404 def for_update_or_404(self, pk, user): return get_object_or_404(self._access(user=user), pk=pk) else:
#todo - check annotation </s> if old.id != new.id and old.name != new.name :	compare_records return False for old, new in zip(old_list, new_list) : return False if str(old.seq).upper() != str(new.seq).upper() :
# todo(hvy): whether a pruned trials should have an actual value can be discussed. </s> frozen_trial = self._storage.get_trial(trial_id)	_run_trial str(e)) _logger.info(message) last_step = frozen_trial.last_step if last_step is not None:
# todo(higumachan): remove this "if" section </s> if not _available:	test_keras_pruning_callback_observation_isnan def test_keras_pruning_callback_observation_isnan(): pytest.skip('This test requires keras ' 'but this version can not install keras(tensorflow) with pip.')
# todo watch out because urllib.unquote will blow up on unicode text </s> msg = self.server.backend.message(session_id, urllib.unquote(text))	do_GET session_id = match.group(1) text = match.group(2) self.server.backend.route(msg) self.send_response(200)
# todo: edit once we get the properly labeled entity ids from nalanda </s> entry['description'] = entry['entity_id']	add_full_title_from_topic_tree entry['title'] = entry['description'] = entry_name except KeyError: return entry
# todo: think about moving this to model_eval mtry function </s> if not mtry:	deploy use_saved_model=use_saved_model) elif self.modeltype == 'regression' and method == 'rf': mtry = math.floor(len(self.X_train.columns.values)/3) algorithm = RandomForestRegressor(n_estimators=trees,
# todo: enable non-windows methods in configure </s> env_build = autotoolsbuildenvironment(self)	_build_autotools def _build_autotools(self): with tools.environment_append(env_build.vars): with tools.chdir("CPP/7zip/Bundles/LzmaCon"):
# todo: there’s a vertical 0.5px shift on the second page </s> assert_pixels('collapsed_border_tfoot', 22, 36, '''	test_tables_10 @assert_no_logs def test_tables_10(): ______________________ __RRRRRRRRRRRRRRRRRR__
# todo: move to base class </s> return self.transform().m22()	currentViewScale def currentViewScale(self):
# todo check the op returned a view </s> if dmap and idx in dmap:	count_running_memory view_of = {} for v in val: node_memory_saved_by_inplace += v elif vmap and idx in vmap:
# todo complete this method </s> return none	mask_frozen_ee def mask_frozen_ee(eom, vector, kshift, const=LARGE_DENOM):
# todo(dcramer): this should respect rate limits/etc and use the normal </s> try:	send def send(self, project, **kwargs): manager = EventManager(kwargs) data = manager.normalize()
# todo: theme me </s> for d, l in zip(drawings, labels):	draw da = g.geom.draw_legend(g.data.iloc[i], g.params, da) drawings.append(da) e = HPacker(children=[d, l], align='left', pad=0, sep=hgap) entries.append(e)
# todo: temporary hack until they fix </s> handle_tr_writer_deprecation()  # todo: temporary hack	run def run(testdir, path="report.html", *args): path = testdir.tmpdir.join(path) result = testdir.runpytest("--html", path, *args)
# todo: we need to pick the rank from `comm_shm`, not `comm`, </s> comm = none	_initialize @singledispatch def _initialize(iet): for i in iet.parameters: if isinstance(i, MPICommObject):
# todo: this is untested. </s> _raise_current_error()	_handle_bio_errors raise ValueError("unknown bio failure") else:
# todo check error message </s> assert app_is_not_installed("whatever.nope", "legacy_app")	test_legacy_app_install_unknown_domain with pytest.raises(YunohostError): install_legacy_app("whatever.nope", "/legacy")
return self.edited # fallback todo log? </s> m = re.fullmatch(r'(\w+) (\d+)\w+, (\d+)', title)	created title = self.title if title is None: if m is None: return self.edited # fallback TODO log?
# todo: custom exception (?) </s> raise invalidcredsexception()	_authenticate self.host = server if scheme is "https" and self.secure is not 1: super(RackspaceConnection, self).connect()
return 0.0 #todo - return nan or none here? </s> else :	calc_at_skew t = sequence.count('T') + sequence.count('t') if a+t == 0 : return (a-t)/float(a+t)
# todo: add test case </s> def test_urlparse():	test_urlparse from pydu.compat import urlparse pass
# todo need to send a browse request for the object to be populated </s> return utils.load(self, timeout=timeout)	load :type timeout: float :returns: self
# todo: handle errors better </s> abort(	patch except sqlalchemy.exc.IntegrityError as e: db.session.rollback() code=http_exceptions.Conflict.code, message="Could not update team details."
# todo allow first arg to be string name, class type or func </s> self.remove_augmenters_(func=func, parents=parents)	remove_augmenters_inplace def remove_augmenters_inplace(self, func, parents=None):
#todo?# self.asserttrue(greps(err, "unit zzz.service not for --user mode")) </s> self.assertequal(out.strip(), "unknown")	bad_usermode_notify_service_functions_with_reload logg.info(" %s =>%s \n%s\n%s", cmd, end, err, out) self.assertEqual(end, 3) logg.info("== 'start' shall start a service that is NOT is-active ") cmd = "docker exec {testname} {systemctl} start zzz.service -vv {quick}"
# todo packages for cloudera not available on lucid yet, using karmic for the moment (beta 1) </s> "deb http://archive.cloudera.com/debian karmic-cdh3b1 contrib",	ec2_ubuntu_environment "deb http://downloads.mongodb.org/distros/ubuntu 10.4 10gen",
# todo: support more than just lists </s> self._unify(tlist(node.target.type), node.iter.type,	visit_For def visit_For(self, node): node = self.generic_visit(node) node.target.loc, node.iter.loc) return node
# todo: this is a good place to detect and react to a loop, </s> if eth_src in vlan.host_cache:	learn_host_on_vlan_port in_port = port.number ofmsgs = [] host_cache_entry = vlan.host_cache[eth_src] if host_cache_entry.port_num == in_port:
# todo: move the logic somewhere else </s> main_ind = [()]	multinomial_helper size = size + (dim_len,) out_size = size+(pvals.shape[-1],) n_ind = [()] pvals_ind = [()]
pass  # todo: implement. </s> def add_chain_context_pos(self, location, prefix, glyphs, suffix, lookups):	add_chain_context_pos
# todo: segwit stuff </s> if not self.address:	update_unlocking_script self.unlocking_script_unsigned = self.redeemscript elif self.script_type in ['p2wpkh', 'p2sh_p2wpkh']: self.address = self.keys[0].address() elif self.script_type in ['p2wsh', 'p2sh_p2wsh']:
# todo: warning? exception? </s> return s	to_unicode elif isinstance(s, bytes): return s.decode(encoding)
raise notimplementederror # todo </s> self._betan = none	__init__ def __init__(self): self._substatemap = np.concatenate([np.arange(len(obs_distns)) for obs_distns in self.model.obs_distnss])
# todo: remove when materialized paths are fixed in the payload returned from waterbutler </s> if not item['materialized'].startswith('/'):	find_and_create_file_from_metadata and return the new file. for item in children: item['materialized'] = '/' + item['materialized'] if item['kind'] == 'folder':
# todo: documentation pending </s> if not os.path.exists(save_dir):	save_ckpt def save_ckpt(self, sess=None, mode_name='model.ckpt', save_dir='checkpoint', global_step=None, printable=False): logging.error("directory {} doesn't exist.".format(save_dir)) return
# end todo </s> x = unfold_func(module)(module.input0)	weight_diag_ggn sqrt_ggn = sqrt_ggn.view(num_classes * batch, out_features) sqrt_ggn = sqrt_ggn.view(num_classes * batch, out_channels, out_x * out_y) X = X.repeat(num_classes, 1, 1) sqrt_ggn = einsum('bml,bkl->bmk', (sqrt_ggn, X)).contiguous()
# todo: this implementation should be revisited </s> epi_rec = msg15nativetrailerrecord().seviri_l15_trailer	read_epilogue def read_epilogue(self): epilogue = np.dtype(epi_rec).newbyteorder('>') with open(self.filename, "rb") as fp_:
#todo: allow changing of default roles associated with history </s> if trans.user:	history_set_default_permissions @web.expose def history_set_default_permissions( self, trans, **kwd ): if 'set_permissions' in kwd: history = trans.get_history()
# todo(@awav): may need them for other models </s> _, _, z = rng.randn(n, input_dim), rng.randn(n, output_dim), rng.randn(m, input_dim)	test_other_models_full_cov_samples num_samples): samples_shape = (num_samples, Ntest, output_dim) Xtest = rng.randn(Ntest, input_dim) model_gp = model_setup.get_model(Z)
# todo: move to ab callback </s> self.session.add_event(event.event_contact_remove_succeed, account)	_handle_action_remove_contact papycontact = self.address_book.contacts.search_by('account', account) self.address_book.delete_contact(papycontact)
# todo debug </s> print logstring	logRule + "remoteId=%d)" % ruleElement.element.remoteSensorId) logging.info("[%s]: %s" % (fileName, logString)) elif ruleElement.type == "weekday": logString = ("%s weekday " % spaceString
# todo: add this back in once we've merged back the refactored users code </s> self.assertequals(users_count, 1)	testStealCommCareUser users_count = CouchUser.view("users/by_commcare_username_domain", key=[self.domain, self.commcare_username]).total_rows
# todo consolidate this and pr plotter into 1 function </s> color_iterator = itertools.cycle(['b', 'g', 'r', 'c', 'm', 'y', 'k'])	pr_plot_from_thresholds save (bool): False to display the image (default) or True to save it (but not display it) debug (bool): verbost output. plt.figure() plt.xlabel('Recall')
#todo: call _update_node_rule_for_parents </s> self._update_node_parents(set(itertools.chain(*ebunch)))	add_edges_from nx.DiGraph.add_edges_from(self, ebunch)
#todo avoid doing this since a may be a different sparse type </s> ah = aslinearoperator(asmatrix(a).h)	cgnr AH = A.H else: A,M,x,b,postprocess = make_system(A,M,x0,b,xtype) dimen = A.shape[0]
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_unexpected_parameters def test_fail_unexpected_parameters(self):
# todo: code not valide as accountcode moved to his own model accountcode </s> user = userprofile.objects.get(accountcode=accountcode).user	push_asterisk_cdr print_shell(shell, "No VoipPlan created for this user/accountcode") try: except: user = User.objects.filter(is_superuser=True)[0]
#todo: kvick we should rename 'short_circuit' to something like 'disable_service_start' </s> if config.get('short_circuit', true):	_teardown_chroot config = self._config.plugins[self.full_name] log.debug('Tearing down chroot at {0}'.format(self._mountpoint)) if not self._activate_provisioning_service_block(): log.critical('Failure during re-enabling service startup')
# todo: logging </s> geth_cmd = ["geth --dev"]  # warning: changing this may have undesireable effects.	auto_geth_dev_ipc_provider Provider backend https:// github.com/ethereum/eth-tester geth_process = subprocess.Popen(geth_cmd, stdout=subprocess.PIPE, shell=True, preexec_fn=os.setsid) time.sleep(10)  #TODO: better wait with file socket
# todo: is that right? </s> pass	unindex_posts es.delete(index, doc_type=Post._meta.db_table, id=post_id) except pyes.exception.NotFoundException:
# todo: remove this when domain decomposition is merged </s> warnings.warn('this feature is not yet implemented in a release ' \	set_dd_mesh_upper_right def set_dd_mesh_upper_right(self, upper_right): 'version of openmc') if not isinstance(upper_right, tuple) and \
#     # todo: add an exception message </s> raise parsererror	DateTimeParser if "YY" in fmt_tokens and match.end() != len(string):
# todo: check pdf content? how? </s> png_bytes = html.write_png(stylesheets=[css])	test_low_level_api pdf_bytes = html.write_pdf(stylesheets=[css]) assert pdf_bytes.startswith(b'%PDF') document = html.render([css], enable_hinting=True) page, = document.pages
#todo: this should be reading from the wcs object </s> xmin = self.center.x - self.dimensions[0] / 2. * self.scale.x	xrange @property def xrange(self): xmax = self.center.x + self.dimensions[0] / 2. * self.scale.x return u.Quantity([xmin, xmax])
# todo: change logic to c_leq based on benchmarking </s> transformationfactory('contrib.deactivate_trivial_constraints')\	solve_NLP_subproblem fix_nlp.tmp_duals[c] = c_geq * max( 0, c_geq*(rhs - value(c.body))) .apply_to(fix_nlp, tmp=True, ignore_infeasible=True) with SuppressInfeasibleWarning():
# todo: how to handle not found authorname </s> result = db.session.query(db.authors).filter(db.authors.sort == auth.lstrip().strip()).first()	order_authors error = False for auth in sort_authors: if not result: error = True
group_id='test-consumer',  # todo: what belongs here? </s> bootstrap_servers=[settings.kafka_url],	_get_consumer return KafkaConsumer( topic, consumer_timeout_ms=100,  # todo: what belongs here?
# todo: use weight scaling factor if provided, xavier's default else </s> self.weights = sharedx(	_initialize_weights if irange is None: irange = self.irange (.5 - rng.rand(nvis, self.nhid)) * irange, name='W',
# todo: clocksignal, resetsignal </s> raise notimplementederror	eval postcommit) else:
# todo: looks like xform has a add_bind method. look into it. </s> ns = "{%s}" % xform_root.nsmap[none]	handle tag = hidden_value_path.replace("/data/", "") data_node.append(etree.Element(ns+tag)) itext_node = xform_root[0][1].find(ns+"itext") bind_node = etree.Element(ns+"bind")
# todo: accept these via quirks? </s> if os.environ.get('libusb_bus') and os.environ.get('libusb_port'):	appropriate_for_environment Determines if the current environment seems appropriate for using the libusb backend. return True if os.environ.get('LIBUSB_ADDRESS'):
# todo(sloria): test me </s> def render_dropbox_file(file_obj, client=none):	render_dropbox_file cache_name = file_obj.get_cache_filename(client=client) node_settings = file_obj.node.get_addon('dropbox')
# todo refactor like in https://github.com/guardicore/monkey/pull/1528 because </s> setup_data_dir(str(default_data_dir))	_setup_default_config default_config = server_config_handler.load_server_config_from_file(DEFAULT_SERVER_CONFIG_PATH) default_data_dir = default_config.data_dir server_config_path = server_config_handler.create_default_server_config_file(default_data_dir) config = server_config_handler.load_server_config_from_file(server_config_path)
pass  # todo </s> def get_xl_sheet(xl_workbook, sheet_name_or_index):	get_xl_sheet
# * todo heading 1 --> </s> vim.evalresults['g:org_todo_keywords'] = ['todo', 'started', 'done',	test_circle_through_todo_states_with_more_states def test_circle_through_todo_states_with_more_states(self): '|'] vim.current.window.cursor = (2, 0)
# todo: split into a function + context manager </s> with make_tempfile(mkdir=true) as new_home:	prep_tmphome [datalad "log"] exc = 1 pass _TEMP_PATHS_GENERATED.append(new_home)
# todo: fix with stubber / before send event </s> with mock.patch('botocore.endpoint.endpoint._send') as _send:	test_get_export 'accepts': 'application/yaml' } _send.return_value = mock.Mock( status_code=200, headers={}, content=b'{}')
# todo: fix highlight handling </s> title = ''.join(data.xpath('.//div[@class="desc"]/h4/a/text()'))	search continue cover_url = ''.join(data.xpath('.//div[@class="img"]/a/img/@data-original')) title2 = ''.join(data.xpath('.//div[@class="desc"]/h5/a/text()')) if title2:
# todo what about "check_existing" ? </s> id_ = collection.load(filepath)	bpy_data_ctor_sounds collection = getattr(bpy.data, collection_name) filepath = proxy.data("filepath") id_.name = proxy.data("name") return id_
# todo: i think this should use '$ fileregions' </s> return self.id1.get_segment(ea).bounds.start	SegStart def SegStart(self, ea):
#todo: error checking </s> return expression('list', *self.chunks(l.get_leaves(), n.get_int_value(), n.get_int_value()))	apply_no_overlap def apply_no_overlap(self, l, n, evaluation): 'Partition[l_List, n_Integer]'
# todo (t65593688): this should be removed after </s> with torch.no_grad():	test_tokens def test_tokens(self): model = Seq2SeqModel.from_config( Seq2SeqModel.Config(
# todo: support for multiple message versions </s> except exception.stopextraction:	Job self.extractor.category, msg[1] ) pass def handle_url(self, url, kexwords):
# todo: remove in sopel 8 </s> self.__setup_plugins_check_manual_url_callbacks(name)	setup_plugins if plugin.has_setup(): plugin.setup(self) plugin.register(self) except Exception as e:
pass  # todo </s> def get_xl_sheet(xl_workbook, sheet_name_or_index):	get_xl_sheet
raise mpdnotimplemented # todo </s> def _commands(self):	_commands @register(r'^commands$')
recording_software_version = none  # todo </s> recording_name = none  # todo	recording_update_pupil_mobile_to_pprf_2_0 duration_s = None  # TODO recording_software_name = None  # TODO system_info = None #TODO new_info_file = RecordingInfoFile.create_empty_file(rec_dir)
# todo: read this from config </s> display_error_count = 5	_render_itemized_in_progress_state self.prompt.write(_('... completed')) self.prompt.render_spacer() num_errors = min(len(data['error_details']), display_error_count) if num_errors > 0:
# todo: trigger via dummy audio? </s> self.playback.on_end_of_stream()	trigger_end_of_stream def trigger_end_of_stream(self):
# todo(kgriffs): measure initial time, and keep iterating until </s> for x in range(num_iterations * jit_warming_multiplier):	profile if PYPY: print('JIT warmup...') func() print('Ready.')
# todo refactor with np.tile or sth </s> norms = norms[..., np.newaxis, np.newaxis]	activation_importance if len(weight.shape) > 2: norms /= activation.shape[1] * activation.shape[2] norms = np.repeat(norms, weight.shape[2], axis=1) norms = np.repeat(norms, weight.shape[3], axis=2)
assert study_id == 0  # todo </s> self.trials[trial_id].intermediate_values[step] = intermediate_value	set_trial_intermediate_value def set_trial_intermediate_value(self, study_id, trial_id, step, intermediate_value):
# todo also check for motion codec parameter support </s> return 'h264_omx' in codecs	has_h264_omx_support if not binary: return False
# todo: custom gremlin method </s> pass	create_vertex_property_key def create_vertex_property_key():
# todo: must be implemented </s> pass	range_from_chapters def range_from_chapters(crawler, times=0):
# todo(ytknzw): add more specific assertion with the test case. </s> figure = plot_param_importances(study, params=["param_b"])	test_plot_param_importances plot_param_importances(study, evaluator=MeanDecreaseImpurityImportanceEvaluator()) assert figure.has_data() is True assert figure.has_data() is True with pytest.raises(ValueError):
# todo: move this function somewhere else </s> def get_translation_stats(directory, dir_stats):	get_translation_stats stats = [ {'title': _("Total"),
# todo: account for line widths and style </s> if self.vertex_colors is none:	polygon_3d_box def polygon_3d_box(self): face_color = self.face_color else:
# todo: add test here as well </s> assert chatcommands.isblu("4622463 stackoverflow", original_msg=msg) == \	test_blacklisted_users assert chatcommands.addblu("4622463 stackoverflow", original_msg=msg) == \ "User blacklisted (`4622463` on `stackoverflow.com`)." "User is blacklisted (`4622463` on `stackoverflow.com`)." assert chatcommands.rmblu("4622463 stackoverflow", original_msg=msg) == \
# todo delete? we should search for valid parser </s> needs_dot = not dot and path	completions if not path and not isinstance(user_stmt, pr.Import): pass comps = [] comp_dct = {}
help='') # todo </s> parser.add_argument(	main '--pos', action='store_true', dest='equation', help='Boolean equation to be analyzed, enclosed with quotes.\n'
# todo: remove save parameter </s> self.save()	add_permission if save:
#@todo: move this and other methods out of this file , into a general </s> from utilities.prefs_constants import dnabaseindicatorsangle_prefs_key	get_all_available_dna_base_orientation_indicators reference_indicator_dict = {}, skip_isStrandChunk_check = False): from utilities.prefs_constants import dnaBaseIndicatorsDistance_prefs_key indicators_angle = env.prefs[dnaBaseIndicatorsAngle_prefs_key]
#todo: write a doc string for this method </s> if hasattr(r.type,"broadcastable") and r.type.broadcastable[i]:	shape_ir @staticmethod def shape_ir(i, r): return T.constant(1.0,dtype='int64') else:
codecs.decode('5050827d08000000d00745b2cf451b00', 'hex'),  # tcp random cmd_ack_ok todo: generate proper sequenced response </s> codecs.decode('5050827d10000000dc053b59d0983500f401ae4301000000f19449000000120c07130906', 'hex'), # tcp prepare_data 1011	test_tcp_live_connect codecs.decode('5050827d04020000dd05942c96631500f801000001000e0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003830380000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003832310000000000000000000000000000000000000000000300000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003833350000000000000000000000000000000000000000000400000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003833310000000000000000000000000000000000000000000500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003833320000000000000000000000000000000000000000000600000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003836000000000000000000000000000000000000000000000c0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000383432000000000000000000000000000000000000000000','hex'), #DATA directly(not ok) codecs.decode('5050827d08000000d00745b2cf451b00', 'hex'), # tcp random CMD_ACK_OK TODO: generate proper sequenced response codecs.decode('5050827df8030000f401ae4301000000f19449000000120c07130906', 'hex'), # reg_event! codecs.decode('5050827d08000000d007fcf701003200', 'hex'),  # tcp CMD_ACK_OK
# todo: i believe this code here can be improved </s> selected_clf = np.zeros(competences.shape[0], dtype=np.int)	select selected_clf[row] = self.rng.choice(indices) elif self.selection_method == 'random': best_competence = competences[np.arange(competences.shape[0]), best_index] for row in range(competences.shape[0]):
# todo: handle "other" </s> return cls(elem.attrib,	Sequence @classmethod def from_element(cls, elem): symbol=check_str(get_child_text(elem, 'symbol'), r'\S{1,10}'), accession=get_child_as(elem, 'accession', Accession),
# todo: change this to be architecture independent </s> return re.match('ret', mnem)	is_ret mnem = GetMnem(ea)
# todo: catch errors </s> state = state.subreqs.get(self.req_type, none)	load_saved_test ]) if self.req_type: formatter.start_output() formatter.set_red(state)
# todo: i18n </s> message_box = qmessagebox(	show_error_dialog def show_error_dialog(message: str, details: str=None): Convenience method for showing an error dialog. QMessageBox.Critical, "Error",
# todo: this sucks. do a real approximation with something like dvsa. </s> if any('aeg_stdin' in name for name in val.variables):	_find_unconstrained_memory_buffers for subaddr in range(addr, addr + size): val = self.crash.state.memory.load(subaddr, 1) if not any(c.op == '__eq__' for c in self.crash.state.solver.constraints if not c.variables - val.variables):
if self.scene.world != none and self.scene.world.node_tree != none and 'background' in self.scene.world.node_tree.nodes: # todo: parse node tree </s> background_node = self.scene.world.node_tree.nodes['background']	get_camera_clear_color def get_camera_clear_color(self): col = background_node.inputs[0].default_value strength = background_node.inputs[1].default_value
# todo: figure out how to import pycache files </s> if root.endswith("__pycache__"):	import_plugins for path in plugins.__path__: for root, _, files in os.walk(path, followlinks = True): continue for f in files:
# todo: chose a better hook position :) </s> if ex != unicornafl.uc_afl_ret_called_twice:	start_afl os._exit(0)  # that's a looot faster than tidying up. except unicornafl.UcAflError as ex: raise
# todo this should be more modular </s> if 'bindings' in response:	__fetch_service if 'items' in response: targets += response['items'] targets += response['bindings'] if 'accounts' in response:
# todo; to change this to checkpoint_callbacks to include static ucr </s> checkpoint_callback=ucr_processor	get_ucr_es_case_pillow event_handler = KafkaCheckpointEventHandler( checkpoint=checkpoint, checkpoint_frequency=1000, change_feed=change_feed, ) return ConstructedPillow(
# todo check if this always works? </s> col = bpy.data.collections.get('collection')	draw_object_only_with_vertices mesh = bpy.data.meshes.new('mesh') obj = bpy.data.objects.new(name, mesh) col.objects.link(obj) bm = bmesh.new()
# todo: format the inputs' directory name </s> if fdir.startswith('_'):	_list_input_dir fdir = dirs[0] dirs = dirs[1:] continue fnames = gfile.ListDirectory(fdir)
# todo add assertions </s> self.assertequal(poll.objects.count(), 0)	test_delete p = Poll.objects.get() p.delete()
# todo: create a default location to save for the specific deployment </s> filepath = request.args.get('filepath')	model_deployment_script_create model_version_id): content = request.args.get('content') with open(filepath, "w") as f: f.write(content)
pass # todo </s> def process_core_message(self, message):	process_core_message
# todo: test this block </s> self.title = self.title or self.content_object.page_title	update_from_related_object self.heading = self.heading or self.content_object.meta_title elif hasattr(self.content_object, 'page_title'): self.heading = self.heading or self.content_object.page_title elif hasattr(self.content_object, 'title'):
# todo: use a contextmanager to ensure we always delete the callback from the list. </s> del self.pong_callbacks[pingid]	wait_pong except asyncio.futures.TimeoutError: logger.debug('timed out waiting for pong with pingid {}'.format(encode_hex(pingid))) return got_pong
# todo debug </s> print logstring	logRule + "end=%d)") % item.element.end logging.info("[%s]: %s" % (fileName, logString)) elif item.type == "second": logString = ("%s second " % spaceString
# todo: revise exception taxonomy </s> except exception as e:	_assign_certified_key_info include_info=True) signature["keyid"] = signature["keyid"] or signature["short_keyid"] log.info(e) continue
# todo: we lose the response code, so we can't check this </s> self.failunless("bar.txt" in self._foo_node.children)	test_PUT_NEWFILEURL_replace d = self.PUT("/vdrive/global/foo/bar.txt", self.NEWFILE_CONTENTS) def _check(res): new_uri = self._foo_node.children["bar.txt"] new_contents = self.files[new_uri]
# todo(qijun) the default decimal is 7, but numpy.dot and eigen.mul </s> numpy.testing.assert_almost_equal(actual, expect, decimal=3)	test_all actual = numpy.array(scope.get_var(out_name).get_tensor()) expect = getattr(self, out_name)
# todo page_size = size of each result page </s> client = asset_v1.assetserviceclient()	search_all_iam_policies def search_all_iam_policies(scope, query=None, page_size=None): from google.cloud import asset_v1 response = client.search_all_iam_policies( scope, query=query, page_size=page_size)
# todo untested </s> 1/0	_raise_ssl_error raise ZeroReturnError() elif error == _api.SSL_ERROR_WANT_X509_LOOKUP: raise WantX509LookupError() elif error == _api.SSL_ERROR_SYSCALL:
#todo: implement mp3 cd support </s> device = str(cd.getproperty("block.device"))	device_from_udi if not cd.GetProperty("volume.disc.has_audio"): return #not CD-Audio cddev = CDDevice( dev=device) return cddev
# todo is this check necessary; this was an assertion which are disabled in <4000 which is good </s> if value != '' and not value.isdigit():	set_action_count def set_action_count(view, value: str) -> None: raise ValueError() set_session_view_value(view, 'action_count', value)
# todo log here </s> return none	get_factor_id ) except httplib.HTTPException: if response.status_code != 200: return None
# todo: match channel against [a-za-z0-9:._-]+ </s> raise exceptions.mpdnotimplemented  # todo	unsubscribe ``unsubscribe {NAME}`` Unsubscribe from a channel.
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
# todo: detect how directories being present is being handled. </s> plugins = self.plugins_get()	enumerate_plugins def enumerate_plugins(self, url): found_plugins = [] for plugin in plugins:
# todo(tobyboyd): remove eager flag when tf 1.0 testing ends. </s> if flags_obj.enable_eager and not keras_common.is_v2_0():	run Raises: ValueError: If fp16 is passed as it is not currently supported. tf.compat.v1.enable_eager_execution() dtype = flags_core.get_tf_dtype(flags_obj)
# todo(solitude): remove this. </s> data.update({'pattern': 'account.payment'})	preapproval key = result['key'] else: try: result = paypal.get_preapproval_key(data)
# todo: consider filtering by location type </s> return match_district(domain, xlsx_facility_name)	match_facility def match_facility(domain, xlsx_facility_name): Given facility name taken from the spreadsheet, return the name and id of the matching location in HQ.
# todo: error handling like numba callwrappers.py </s> native_val = unbox_array(types.array(dtype, 1, 'c'), arr_obj, c)	codegen native_val = unbox_categorical_array(data_typ, arr_obj, c) else: c.pyapi.decref(series_obj) c.pyapi.decref(arr_obj)
return # todo raise error </s> self.backend.playback.stop().get()	Stop if not self.get_CanControl(): logger.debug(u'%s.Stop not allowed', PLAYER_IFACE)
# todo(leofang): test newer rocm versions </s> if axes == (0, 1) and cupy.cuda.runtime.is_hip:	test_default_fft_func fft_func = _default_fft_func(ca, s=s, axes=axes, value_type='R2C') if enable_nd: assert fft_func is _fft else:
data = {}  # todo: do we need public key handling now? </s> data['__model__'] = self.__class__.__name__	serialize def serialize(self): for key in self.__dict__.keys(): public_key = key.lstrip('_')
# todo better logging </s> try:	connect def connect(self): ssh = SSHClient() ssh.set_missing_host_key_policy(AutoAddPolicy())
f="'f.${def3}.${def3}'"             #todo </s> else:	test_3250_nonrecursive_expand_variables D="'D.${DEF1}.${DEF2}'"             #TODO E="'E.${DEF1111}.def5.${DEF2222}'" A="'A.def1'" B="'B.def2.def3'"
# todo: actually kill the start/stop/restart/status command for 5.11 </s> if command in ['start', 'stop', 'restart', 'status'] and not in_developer_mode:	main sys.stderr.write("Unknown command: %s\n" % command) return 3 logging.error('Please use supervisor to manage the agent') return 1
# # fixme: # todo: remove me </s> try:	unpack_url else: to_crawl['domain_url'] = '{}://{}:{}'.format(to_crawl['scheme'], new_url_host, to_crawl['port']) to_crawl['tld'] = url_unpack['tld'].decode() except:
# todo: once the translation process is able to correctly extract </s> }))	UntagTestCase 'env': untag.UntagParamRegex('prod'),
# todo generator </s> return sorted(content_by_ds) if _return_datasets else results	__call__ else:
# todo: doc </s> batch = self.textdata.sentence2enco(question)	singlePredict def singlePredict(self, question): if not batch: return None
# todo (abhikpal, 2017-06-06) </s> pass	check_support def check_support(self):
raise notimplementederror  # todo </s> def __init__(self, perturbation_function, steps, recompute_analysis=false):	__init__
# todo(qingqing) : redirect c++ ostream to python stream. </s> core.disable_profiler(key_map[sorted_key])	profiler 'ave': core.EventSortingKey.kAve, }
# todo: if either ref or ref0 are not scalar and the output is </s> if distrib_out:	_setup_scaling for i in range(cols.shape[1])] src_indices = np.ravel_multi_index(dimidxs, global_shape_out) raise RuntimeError("vector scalers with distrib vars " "not supported yet.")
# todo(b/132329316) remove when `xla.compile` allows tf.device(tpu). </s> return strategy.experimental_run_v2(step)	forward step = lambda: golden.create_all_variables(mod)
# todo: allow user to return an ordereddict </s> outputs = pack(outputs)	__call__ else: outputs = func(brick, *inputs, **kwargs) for i, output in enumerate(outputs): if isinstance(output, tensor.Variable):
#todo - introduce an annotated alignment class? </s> alignment._annotations = gr	next records.append(record) alignment = MultipleSeqAlignment(records, self.alphabet) return alignment else:
# todo: docs and comments </s> feats = {}	extract_feats model, data_loader, device, ): model.eval() header = "Test:"
# todo: remove this method in v2.5 </s> elif self._values['disabled'] in booleans_true:	disabled if self._values['state'] == 'disabled': return True return True elif self._values['disabled'] in BOOLEANS_FALSE:
# todo: ensure encoding </s> browsers.append(browser.group(1))	get_browser_versions if 'more' in browser.group(1).lower(): continue count += 1 if count == settings.BROWSERS_COUNT_LIMIT:
#todo - parse it? </s> self.assertequal(0, stdoutdata.count("***** no hits found *****"))	Pairwise self.assertEqual(10, stdoutdata.count("Query= "))
# todo kill state dependency </s> fix_eol_cursor(view, state(view).mode)	on_post_save if view.settings().get('vintageous_modeline', False): do_modeline(view)
# todo: should we only export keys with signing capabilities? </s> main_public_key = none	gpg_export_pubkey stdin=subprocess.PIPE, stderr=subprocess.PIPE) key_packet, junk = process.communicate() sub_public_keys = {} packet_start = 0
# naive implementation (todo) </s> content = re.sub(r"^\s*#.*", "", content)	_normalize_content def _normalize_content(self, content): content = re.sub('<!--.*?-->', "", content, flags=re.DOTALL) content = "\n" + content + "\n"
pass # todo </s> def handle_request(self, input):	handle_request
# todo: write code to add selected indirect virtual deps to </s> for atom in selected_atoms[pkg]:	_add_pkg_dep_string print "Candidates:", selected_atoms vardb = self._frozen_config.roots[dep_root].trees["vartree"].dbapi try: atom = portage.dep.Atom(atom)
# todo: if !blocking... </s> wait = true	__shared_acquire wait = False if self.__nwait_exc > 0: self.__wait(self.__turn_shr) while self.__exc is not None:
# todo: compare to plain for loop through the labels </s> sel = n.array([], dtype=n.int16)	getSampleIdsByLabels if not operator.isSequenceType(labels): labels = [ labels ] for label in labels: sel = N.concatenate((sel, N.where(self.__labels==label)[0]))
# todo: determine why this even happens, as it shouldn't be possible </s> if not new_clip.get("reader"):	addClip new_clip["title"] = filename new_clip["image"] = thumb_path return  # Do nothing start_frame = 1
# todo: instead of hiding..., which may consume memory... why don't killing? </s> self.hide()	clusterGraphWidget dotcode = self._generateDotCode(response_list, distance_function=callable_object) except Exception, e: msg = 'Please review your customized code. An error was raised on run time: "' msg += str(e) + '"'
raise exception('lol') #todo fixme </s> for exclude in excludes:	update_excludes exclude_file = open(exclude_file_path, 'w') except IOError: exclude_file.write("%s\n", exclude)
# todo(zshi) remove this check when classic drivers are removed </s> except exception:	node_cache_bios_settings LOG.warning('BIOS settings are not supported for node %s, ' 'skipping', node.uuid) msg = (_('Caching of bios settings failed on node %(node)s.') % {'node': node.uuid})
'value': t.text,  # only for easy identification during debugging. todo: delete </s> 'xmlnode': t	create_text_dict 'width': t_width, 'height': t_height, } update_text_dict_pos(text, pt(int(t.attrib['left']), int(t.attrib['top'])))
# todo: work around this </s> return [ c for c in completions if no_special_chars_re.match(c[0]) ]	on_query_completions end_time = time.clock() log('time to get completions: {0} seconds'.format(end_time - begin_time)) return []
# todo add options to modify the columns </s> matrix.append([endpoint.machine.name,	do_clear if vlan.startswith('VLAN'): vlan.split('VLAN')[1] endpoint.endpoint_data['mac'], endpoint.endpoint_data['segment'],
# todo: test logging messages. </s> self.manager.startup()	test_startup def test_startup(self): self.assertEquals(self.states, [])
# todo: can we assume reverse=false? </s> signals.m2m_changed.send(	clear ) self.through.objects.filter(**self._lookup_kwargs()).delete() sender=self.through, action="post_clear", instance=self.instance, reverse=False,
1  # todo: fill in identifier </s> )	profile_transfer amount, target, result.wait() profiling.print_all_threads()
# todo: prepare this above </s> statement = self.secondary.insert()	save if self.secondary is not None: self.secondaryjoin.accept_visitor(setter) statement.echo = self.mapper.echo statement.execute(**setter.associationrow)
# todo: we need to insert a linebreak here, but there is no </s> text = "\n" + text	insert_at_line def insert_at_line(self, line, text): if line >= self.get_line_count(): it = self.get_iter_at_line_or_eof(line) self.insert(it, text)
# todo: find a better random value </s> return datetime.date.today()	auto_value return datetime.datetime.now() elif prop.type == datetime.date: elif prop.type == float: return uuid.uuid4().int / float(uuid.uuid4().int)
# todo: remove once typeshed supports literal types </s> assert isinstance(ov, _winapi.overlapped)	read while True: ov, err = _winapi.ReadFile(self.connection, self.READ_SIZE, overlapped=True) assert isinstance(err, int) try:
# todo: fix circular imports </s> from website.addons.osffiles.model import nodefile	get_file def get_file(self, path, version): from website.addons.osffiles.exceptions import ( InvalidVersionError,
# todo disconnect pub/sub </s> pass	HelloWorldMessage print() finally: async def lang_changer_dealer(self): deal = self.ctx.socket(zmq.DEALER)
# todo: add at least reflection tests before adding notimplemented version </s> def prioid(context, *args):	prioid *musicpd.org, current playlist section:* ``prioid {PRIORITY} {ID...}``
# todo(pkilambi): process the output as needed </s> return out	pod_show try: out = utils.execute('kubectl', 'describe', 'pod', uuid) except Exception as e: LOG.error("Couldn't delete pod  %s due to error %s" % (uuid, e))
# todo: remove in 1.4 </s> warn_deprecation(	from_string @classmethod def from_string(cls, b): "Please use req.from_bytes", '1.3',
# todo: adjust for dhcpv6 </s> self.server = server	set_server server = "<<inherit>>"
# todo: perhaps unify all data collection in one single context. </s> elif key == 'vi_state_expecting_user_input':	on_query_context elif operator == sublime.OP_NOT_EQUAL: return not vintage_state.expecting_register if operator == sublime.OP_EQUAL: return vintage_state.expecting_user_input
# todo(hub-cap):fix this ugly hack! </s> global uuid	DBaaSAgent self.create_database(databases) PREPARING = False UUID = uuid def update_status(self):
# todo: this can be removed for cartopy > 0.14.3 </s> if hasattr(self.ax, 'projection') and 'transform' in kwargs:	plot_arrow if self.arrows: self.arrows.remove() trans = kwargs['transform'] try:
)  # todo: figure out the number of frames independent of 3d detection </s> self.pause_switch = none	__init__ self.eye_frame_num[1] = len( self._pupil_data_store[1, "3d"] self.detection_paused = False for eye_id in range(2):
# todo: add the parts to the music << >> </s> for p in group.parts:	makeBlock music = ly.dom.Simr() node.insert(0, music) ly.dom.Comment("Part {0}".format(p.part.title()), music) for g in group.groups:
# todo: gpt </s> devices = self.get_devices()	get_mount_devices def get_mount_devices(self): in modify_grub_default() in bootloader.py """ mount_devices = {} mount_devices['/boot'] = devices['boot']
#todo: does not keep case </s> ('me', 'us'),	test__plnoun ('I', 'we'), ('you', 'you'), ('mine', 'ours'), ('child', 'children'),
# todo: refactor common tests for all models, e.g. shape checking </s> scores = model.forward_owa(triples)	test_conv_kb batch_size = 16 triples = torch.zeros(batch_size, 3, dtype=torch.long) assert scores.shape == (batch_size, 1) scores = model.forward_cwa(triples[:, :2])
# todo: a possibility to call different wine binaries </s> try:	_get_source_dir_proposals yield "~/.wine/" + STANDARD_PATH_IN_32BIT_WINEPREFIX yield "~/.wine/" + STANDARD_PATH_IN_64BIT_WINEPREFIX tmp_reg_file = 'aoe_temp.reg' if not subprocess.call(('wine', 'regedit', '/E', tmp_reg_file,
# xxx todo: real error handling, as this is probably going to </s> if new_firmware:	_copyDriverDiskFiles except IOError as e: log.error("failed to copy driver disk files: %s" % e.strerror) for kernel in self.kernelVersionList: log.info("recreating initrd for %s" % kernel)
# todo: warn if field has_choices but not in table.filtering </s> print("creating filter indexes...", end="", flush=true)	create_filter_indexes def create_filter_indexes(self): start = time.time() Model = self.table.get_model(cache=False)
# @todo: crud strings </s> return dict(disease_disease_id = disease_id,	DiseaseDataModel Field("description"), *s3_meta_fields()) ) @staticmethod
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo xxx: bug 593055 </s> self.user_profile.save()	test_confirm_resend self.user_profile.confirmationcode = "code"
# todo: why don't we return a arraywithunit? </s> return val.__class__([floatwithunit(i, unit_type=unit_type,	wrapped_f unit_type = _UNAME2UTYPE[unit] if isinstance(val, collections.Sequence): unit=unit) for i in val]) elif isinstance(val, collections.Mapping):
# todo: remove </s> if user.is_moderator:	for_update_or_404 def for_update_or_404(self, pk, user): return get_object_or_404(self._access(user=user), pk=pk) else:
# todo.before_release: hack for reproducing the exact results we have in </s> puck_position = np.random.uniform(	reset target_pos[1] += 1.0 qpos[self.TARGET_INDS] = target_pos low=[0.3, -1.0], high=[1.0, -0.4]), qpos[self.PUCK_INDS] = puck_position
# todo: higher dimensions? happens often in statistics </s> if len(dimensions) != 2:	_matrix from sage.matrix.constructor import matrix dimensions = mat.do_slot("dim") raise TypeError (nrow, ncol) = dimensions
pass  # todo: implement this </s> def do_editor_save(self):	do_editor_save
and not self.allow_all_insecure):  # todo: remove after release </s> logger.debug("not searching %s for urls, it is an "	_get_queued_page and not link.trusted and not normalize_name(req.name).lower() in self.allow_insecure "untrusted link and cannot produce safe or " "verifiable files." % link)
# todo(tsileo): handle tombstone </s> if not is_api_request():	outbox_activity_replies @app.route('/outbox/<item_id>/replies') def outbox_activity_replies(item_id): abort(404) data = DB.outbox.find_one({'id': item_id, 'meta.deleted': False})
# todo: pytest.warns is not supported until pytest >= 2.8.0, whose </s> assert warnings.warn.call_args_list == [call(any, deprecationwarning)]	test_spawn_managed_thread_backwards_compat_warning Event().wait() container.spawn_managed_thread(wait, *args, **kwargs) container.kill()
# todo: min() and max() patches do nothing useful at present. just remove? </s> with notracing():	_min def _min(*values, key=lambda x: x, default=_MISSING): if len(values) <= 1: if not values:
# todo: specific exceptions, useful error reporting </s> import sys	generate_span_type_html f.close() except: print >> sys.stderr, 'brat htmlgen.py: error reading %s' % __event_type_hierarchy_filename event_type_hierarchy = __default_event_type_hierarchy
# todo: get rid of this after we remove reconfig. </s> if setup_mode == 'full':	_setup_procs self.options._parent_name = self.msginfo self.recording_options._parent_name = self.msginfo self._vectors = {} orig_comm = comm
# todo: write tests for handle_status </s> if key == 'nodata':	handle_status def handle_status(self, key, value): :raises: :exc:`ValueError` if the status message is unknown. self.nodata = True elif key == 'ENC_TO':
# todo: fix clone issue </s> mad(np.array([1, 2, 3])))	test_mad assert_equal([0.6745, 0.0, 0.6745],
# todo: replace with "yield from" when dropping python 2. </s> for stream in chain.from_iterable(streams):	_get_streams for item in playlist: streams = map(self._create_stream, item["sources"]) yield stream
# todo: check if a valid ipfs hash </s> print 'no scheme in path, assuming ipfs hash and fetching...'	fetchRemoteCDXJFile path = path.replace('ipfs://', '') if '://' not in path:  # isAIPFSHash dataFromIPFS = IPFS_API.cat(path) print 'Data successfully obtained from IPFS'
# xxx todo - do not duplicate code!!! </s> if dev_data is not none and save_dir is not none:	train_pytorch reader.train(torch_optimizer, train_data, batch_size, max_epochs=epochs, hooks=hooks, l2=l2, clip=clip_value) reader.load(save_dir) result_dict = evaluate_reader(reader, dev_data, batch_size)
# todo: return proper searchable iterator </s> data = {"id": self.id, "first": 48}	fetch_images def fetch_images(self): (j,) = self.session._graphql_requests( _graphql.from_query_id("515216185516880", data)
#todo, multipart raw submissions need further parsing capacity. </s> instance = request.raw_post_data	post instance = request.FILES['xml_submission_file'].read() else: try: doc = post_xform_to_couch(instance)
# todo: check if we can use orm to do that </s> sub_query = """	subcategories_with_contents_count @staticmethod def subcategories_with_contents_count(category, handle_types): SELECT COUNT(*) FROM `tutorialv2_publishedcontent` INNER JOIN `tutorialv2_publishablecontent`
# todo: flip this around when statemutability is output instead </s> if "payable" in abi and abi["payable"]:	test_json_interface_implements code = f"import jsonabi as jsonabi\nimplements: jsonabi\n{code}" compile_code(code, interface_codes={"jsonabi": {"type": "json", "code": abi}}) del abi["payable"] abi["stateMutability"] = "payable"
pass  # todo(zcd) </s> def test_check_forward_backward_with_scale(self):	test_check_forward_backward_with_scale
# todo: switch to split tokenizing? much faster </s> tokens = nlp.tokenizer(text)	span_tokenize def span_tokenize(self, text): return ([t.text for t in tokens], [(t.idx, t.idx + len(t.text)) for t in tokens])
# todo: deleting remote folders involves reimplementing </s> except exception as e:	trash_or_confirm try: gfile.delete(None) raise RuntimeError(str(e))
# todo: other types than can have series inside: list, set, etc. </s> return typ	if_series_to_array_type if isinstance(typ, (types.Tuple, types.UniTuple)): return types.Tuple(list(map(if_series_to_array_type, typ.types)))
# todo: add more checks here? </s> if magic in ['fws', 'cws']:	_isSWF if len(body) > 5: magic = body[:3] return True return False
# todo: handle case where end of sysex is reached too </s> manifacturer = self._bytes[1]	put_byte self._messages.append(opcode2msg[opcode]) elif opcode == 0xf7: data = tuple(self._bytes[2:]) msg = opcode2msg[0xf0](manifacturer=manifacturer, data=data)
# todo add verbose output </s> return self._domain	LocalizationModel @property def domain(self): @domain.setter def domain(self, new_domain):
try: # todo: fix this. if not in the scanning workbench, _drawmachine() fails. </s> self.scanningworkbench.sceneview._drawmachine()	onMachineSettings ret = MachineDialog.ShowModal() if ret == wx.ID_OK: except: pass
# todo(rakhmerov): why is it here? this module is too generic. </s> if not input_string:	get_dict_from_string def get_dict_from_string(input_string, delimiter=','): return {} raw_inputs = input_string.split(delimiter)
# todo: if py3k, override unpickler.find_class(). </s> pass	_load mypickle.find_global = None except AttributeError: d = mypickle.load() except SystemExit:
# todo verify permission type for the provided resource type </s> resolver = resolvers.get_resolver_for_permission_type(permission_type=permission_type)	user_has_permission if not cfg.CONF.rbac.enable: return True result = resolver.user_has_permission(user_db=user_db, permission_type=permission_type) return result
# todo: use rule in raw table not default chain policy </s> try:	disable_panic_mode if not self._panic: raise FirewallError(NOT_ENABLED) self._set_policy("ACCEPT", "all") except Exception, msg:
# self.assertisnotnone(cursor.query_planning_time_in_millis)  # todo flaky test </s> self.assertisnotnone(cursor.output_location)	test_fetchone self.assertIsNotNone(cursor.query_queue_time_in_millis) self.assertIsNotNone(cursor.total_execution_time_in_millis) self.assertIsNone(cursor.data_manifest_location) self.assertIsNone(cursor.encryption_option)
# todo: remove in v2.8 </s> if self.template_language == exporttemplatelanguagechoices.language_django:	embed_link return '' context = {'obj': obj} template = Template(self.link) return template.render(Context(context))
# todo tests </s> class actionlogdata:	ActionLogData def __init__(self, when: datetime): if not isinstance(when, datetime):
# todo: support multiple selections </s> rowcol = view.rowcol(selections[0].b)	get_as_encoded_address if len(selections) > 0: win = view.window() else: win, view, rowcol = _MARKS.get(name, (None,) * 3)
# todo: add primitive_type info in debug info later. </s> _logger.debug(	get_gradient grad_records = self._array_grad_records.get(current_array, []) for grad_record in grad_records: 'Calling derivative func "{}"'.format(grad_record.grad_func)) grad = grad_record.grad_func(self._get_cached_gradient(grad_record.result))
# todo(paul): why does 'event' not have a 'user' object? </s> user = self.hs.parse_userid(event.user_id)	send_message Raises: SynapseError if something went wrong. assert(user.is_mine) if stamp_event:
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
raise notimplementederror()  # todo </s> default: ``none``).	create_settings_profile :param str from_profile_name: Clone the given profile name (optional,
# todo(mlavalle) this notification should be updated to publish when </s> registry.notify(address_group, events.after_create, self,	create_address_group self.add_addresses(context, ag.id, fields) ag.update()  # reload synthetic fields context=context, address_group_id=ag.id) return self._make_address_group_dict(ag)
# todo: remove once elasticsearch v6.x is deprecated. </s> client_major_version = self._getclientmajorversion()	WriteHeader }], } if client_major_version < 7: mappings = {self._document_type: mappings}
# todo: in the future we should be able to choose different providers </s> blob = spnego_negtokeninit()	login_extended sessionSetup['Parameters']['SessionKey']       = 0 sessionSetup['Parameters']['Capabilities']     = SMB.CAP_EXTENDED_SECURITY | SMB.CAP_USE_NT_ERRORS blob['MechTypes'] = ['+\x06\x01\x04\x01\x827\x02\x02\n'] auth = ntlm.NTLMAuthNegotiate()
# todo: rewrite tests </s> pass	test_resend_confirmation_post_sends_confirm_email @mock.patch('framework.auth.views.mails.send_mail') def test_resend_confirmation_post_sends_confirm_email(self, send_mail):
pass # todo </s> def upload_ssk(write_capability, new_version, uploadable):	upload_ssk
# todo deprecate? </s> def get_articles() -> sequence[article]:	get_articles return list(articles())
# todo: figure out if we really need to override these methods, or if there is a  bug in the default </s> return list_items(self.server, self.key, watched=watched)	items from video import list_items
# todo: with git <= 2.3 keep old mechanism: </s> with rm.repo.git.custom_environment(	fetch cnct = ssh_manager.get_connection(fetch_url) cnct.open() GIT_SSH_COMMAND="ssh -S %s" % cnct.ctrl_path): rm.fetch(refspec=refspec, progress=progress)  # TODO: progress +kwargs
# todo(mattjj,phawkins): improve this implementation </s> proxy = object()	flatten_axes def flatten_axes(treedef, axis_tree): dummy = tree_unflatten(treedef, [object()] * treedef.num_leaves) axes = []
# todo: warn on failure to delete? </s> raise	_delete_arc_equiv except DependingAnnotationDeleteError, e:
# todo maybe return suitable values for the last property </s> self.backuntil(lp.equalsignsetoverride, lx.space)	override return if lp.EqualSignSetOverride in tokenclasses: return completiondata.lilypond_markup self.backuntil(lp.DotSetOverride, lx.Space)
# todo(sloria): test me </s> return make_file_response(fileobject, metadata)	dropbox_download fileobject, metadata = client.get_file_and_metadata(path, rev=revision)
# todo: test require restart </s> tasks.restart_named(self.master, self.replicas[0])	test_chain_of_trust ] self.master.run_command(args) assert wait_until_record_is_signed( self.master.ip, example_test_zone, timeout=100
# todo: remove in v1.2 </s> if len(kwargs) > 0:	__init__ **kwargs, ): warnings.warn( "Passing additional keyword parameters has no effect and is "
# todo: allow for defining custom path param options in the </s> if '{locale}' in self.path_format_localized:	path_params_localized def path_params_localized(self): params = {} params['locale'] = [str(locale) for locale in self.locales] return params
# todo do a proper mro resolution. currently we are just listing </s> for lazy_cls in context.py__bases__():	py__mro__ mro.append(cls) mro = [context] for cls in lazy_cls.infer(): try:
# todo: check error location </s> assert result.data == {'nest': none}	test_non_nullable_list_of_nullables assert len(result.errors) == 1 assert result.errors[0].message == 'Cannot return null for non-nullable type.'
# #todo make sure we switch to correct units for machine when saving file </s> try:	setData else: tmpl = lambda s: self.imperial_text_template % s qualified = float(value) except Exception as e:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_min_weight_magnitude_string def test_fail_min_weight_magnitude_string(self): ``min_weight_magnitude`` is a string.
# @todo: extend entity_types within the template </s> need_response = t("activity group"),	DocumentLibrary inv_send = T("Sent Shipment"), inv_warehouse = T("Warehouse"), police_station = T("Police Station"), pr_group = T("Team"),
# todo: use a better measure then first tab </s> if hasattr(menu, '_tabs') and isinstance(menu._tabs[0], mainsquareoverviewtab):	_on_settler_level_change if hasattr(menu, "name") and menu.name == "build_menu_tab_widget": self.show_build_menu(update=True) instance = list(self.session.selected_instances)[0] instance.get_component(SelectableComponent).show_menu(jump_to_tabclass=type(menu.current_tab))
# todo(phawkins): enable test after jaxlib 0.1.22 is released. </s> jtu.check_grads(lax_op, args, order=1, atol=1e-3, rtol=3e-3, eps=1e-3)	testScipySpecialFun if test_autodiff:
# todo: replace with "yield from" when dropping python 2. </s> for stream in streams.items():	_get_streams if urlparse(ios_url).path.endswith(".m3u8"): streams = HLSStream.parse_variant_playlist(self.session, ios_url) yield stream if swf_url:
return cursor_offset, line #todo not implemented </s> def yank_prev_prev_killed_text(cursor_offset, line):	yank_prev_prev_killed_text @on('\x1by')
# todo: this operation will cause damage to disk data which should be limited </s> self.__gpt_unlabeldisk(from_disk)	zfs_detach_disk ret = self.__system_nolog('/sbin/zpool detach %s %s' % (volume.vol_name, label)) if from_disk: return ret
# todo: direct use of json['error'] is deprecated; use display_message('...', 'error') instead. </s> response['error'] = 'error selecting arc types!'	arc_types_html response['html']  = '<fieldset><legend>Type</legend>' + '\n'.join(inputs) + '</fieldset>' except: print 'Content-Type: application/json\n' add_messages_to_json(response)
# todo(user): remove after 184 is out. </s> finalized_filenames = cls.get_filenames(mapreduce_state)	finalize_job state = cls._State.from_json(mapreduce_state.writer_state) files.finalize(state.filenames[0]) state = cls._State(finalized_filenames, []) mapreduce_state.writer_state = state.to_json()
# todo: warning </s> return	handle_incident_dionaea_modules_python_ftp_command data = self.attacks.get(con) if not data: if "ftp" not in data: data["ftp"] = {}
# todo(israt) :add reduce func to suport the following reduce op </s> _test(fn.copy_u, fn.sum)	test_unary_copy_u assert(F.allclose(n_grad1, n_grad2))
# todo: add back: </s> if isinstance(obj, (dataframe, series)):	isna 2    False Name: b, dtype: bool return obj.isnull() else:
pass  # todo </s> def stop_ambient_camera_rotation(self):	stop_ambient_camera_rotation
# todo. optionally sort on birthdate </s> for child_handle in childlist:	display_ind_parents of.write('\t\t\t\t\t<ol>\n') childlist = [child_ref.ref for child_ref in child_ref_list] sibling.add(child_handle)   # remember that we've already "seen" this child if child_handle != self.person.handle:
# todo(yanase): remove number from system_attrs after adding trialmodel.number. </s> assert trials[0].system_attrs == {'number': 0}	test_create_new_trial_id assert trials[0].state == TrialState.RUNNING assert trials[0].user_attrs == {}
# todo: refactor me, please! </s> if self.__starting_step == 5:	_execute Execute all phases on the image. :return: None r = run_worker(self.__phases[0], self.__image_steps, config=self._args) self.__image_steps.append(r)
options['taskid'] = none # todo </s> agent = pulpagent(consumer)	uninstall if consumer is None: raise MissingResource(id) agent.uninstall_units(units, options)
# todo: use unshare() here </s> quiet_call(	BtrFS mpoint = st.enter_context( tempfile.TemporaryDirectory(suffix='.privmnt')) 'mount -t btrfs -o noatime,noexec,nodev -n --'.split() + [self.device, mpoint])
pass # todo </s> def _seek(self, time_position):	_seek
# todo: move to base class </s> super(canvas, self).drawbackground(painter, rect)	drawBackground def drawBackground(self, painter, rect): lod = self.getCanvasLodValueFromCurrentScale() self.boundingRect = rect
print("got result: >%s<" % buffer) # todo remove. </s> if (buffer != "" and  len(buffer) > 0 and buffer.strip() != ""):	_receiver result = to_unicode(conn.recv(SOCKET_SIZE)) buffer += result message = DictObject.objectify(json.loads(buffer)) self._queue.append(message)
# todo : documentation pending </s> try:	load_hdf5_to_weights def load_hdf5_to_weights(f, weights, sess=None): weights_names = list(f.attrs['weights_names']) except Exception:
# todo: should be able to use lib.godot_string_new_with_wide_string directly </s> lib.godot_string_name_new(ffi.addressof(info[i].signature), godot_string_from_pyobj(signature))	pybind_profiling_get_frame_data for i, item in enumerate(sorted_and_limited): signature, profile = item info[i].call_count = profile.last_frame_call_count info[i].total_time = int(profile.last_frame_total_time * 1e6)
raise notimplementederror # todo </s> list every item in entry_point that match request	search def search(self, entry_point, request):
# todo: this procedure would leave a clean dataset, but `run` cannot handle dirty </s> ds.add('code', to_git=true)	test_basics def test_basics(path): ds = Dataset(path).create(force=True) ds.add('.') ds.run_procedure('setup_yoda_dataset')
# todo: remove the following line when issue #71 (preserve the trajdataframe index during preprocessing operations) is solved. </s> ftdf.reset_index(inplace=true, drop=true)	filter max_loop=max_loop, ratio_max=ratio_max, include_loops=include_loops) ftdf.parameters = tdf.parameters ftdf.set_parameter(constants.FILTERING_PARAMS, arguments)
return  # todo return placeholder "[loading]" artist? </s> return models.ref.artist(uri=sp_artist.link.uri, name=sp_artist.name)	to_artist_ref def to_artist_ref(sp_artist): if not sp_artist.is_loaded:
# todo check the op returned a view </s> elif vmap and idx in vmap:	count_running_memory if dmap and idx in dmap: node_memory_saved_by_inplace += v node_memory_saved_by_view += v elif not isinstance(v, str):
#     todo </s> def write(self, path, data, offset, fh):	Filesystem with self.rwlock:
# todo check if this is getting updated </s> self.asserttrue(package.repo_description)	test_repo_handler_fetch else:
# todo split up into several dependent tests -- need to check how this </s> model = model.transform(setexecmode("rtlsim"))	test_fpgadataflow_fclayer_large_depth_decoupled_mode oshape = model.get_tensor_shape("outp") y_expected = y.reshape(oshape) model = model.transform(GiveUniqueNodeNames()) model = model.transform(CodeGen_ipgen("xc7z020clg400-1", 5))
# todo: verify that the post request was received </s> return	buildHeritrixJob config.heritrixCredentials_password), data=data, headers=headers, verify=False, stream=True)
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pcmpeqb res = 0x000000ff0000ff00ff00000000ffff00 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo use csv module </s> for line in req.files["bulk"]:	registration reverse(registration)) elif "bulk" in req.FILES: line_list = line.split(',') name = line_list[0].strip()
# todo: move this to a task queue </s> try:	create_user 'fields are missing.') else: res = requests.post(self.list_endpoint, auth=('nop', self.list_key),
# todo docstring </s> desired_node_applications = []	change_node_configuration def change_node_configuration(self, desired_configuration, hostname): docstring for change_node_configuration for node in desired_configuration.nodes: if node.hostname == hostname:
# todo: implement auto-dtype method in general parameters </s> zlp.data = zlp.data.astype('float32')	splice_zero_loss_peak_flog s.data[ith:2*ith] = s.data[ith-1] * np.hanning((ith)*4)[-ith:] pbar.update(i) s = self.deepcopy() Eaxis = s.axes_manager.signal_axes[0]
if self._ndim == 3: # todo: use hasz </s> array = c_double * 3	ctypes @property def ctypes(self): return array(self.x, self.y, self.z) else:
# todo: try/catch </s> for notifier in self.notifiers:	notify db.session.add(lo) db.session.commit() notifier.notify()
# todo(reedwm): remove manual casts once mixed precision can be enabled with a </s> x = backend.cast(x, 'float32')	trivial_model x = layers.Dense(1, name='fc1')(x) x = layers.Dense(num_classes, name='fc1000')(x) x = layers.Activation('softmax')(x) return models.Model(img_input, x, name='trivial')
# todo: determine proper template to use. </s> app_name + "." + recipe_format + ".recipe"] = "template tbd"	main if app_name + "." + recipe_format + ".recipe" not in existing_recipes: buildable_recipes[ print "\nExisting recipes: %s" % existing_recipes print "\nAvailable recipe formats: %s" % avail_recipe_formats
# todo: reuse code in ..diaggn.conv2d to extract the diagonal </s> return h_diag	bias_diagH (h_sqrt_view, h_sqrt_view)))
#todo: test me </s> self.velocity = self.velocity + j * self._body.contents.m_inv	apply_impulse def apply_impulse(self, j, r): world coordinates.""" self._body.contents.w += self._body.contents.i_inv* r.cross(j)
# todo: test this </s> style = element.style	handle_computed_display_float Computed values of the display and float properties according to http://www.w3.org/TR/CSS21/visuren.html#dis-pos-flo if get_value(style, 'display') == 'none': return # position and float do not apply, but leave them
# todo: this should be abstracted into a property/method or something </s> if region.inherited and not contents and hasattr(obj, 'parent_id') and obj.parent_id:	collect_items def collect_items(obj): contents = obj._content_for_region(region) return collect_items(obj.parent) return contents
# todo workaround for https://bugzilla.mozilla.org/show_bug.cgi?id=1411264 </s> el = self.page.find_element(by.css_selector, 'body')	set_as_top_of_range def set_as_top_of_range(self): self.find_element(*self._dropdown_toggle_locator).click() self.find_element(*self._set_top_of_range_locator).click()
# todo: add cntk </s> raise notimplementederror()	gather_nd return tensorflow.gather_nd(x, indices) else:
# todo: raise exception with preferred method </s> pass	concurrent_get result = future.result() except Exception: else: ret.extend(result["results"])
return s #todo return partial result instead of giving up </s> last_line = current_block.pop(len(current_block) - 2)	bad_empty_lines_removed current_block = complete_blocks.pop() if len(current_block) < 2: assert not last_line, last_line new_finished, new_valid = code_finished_will_parse('\n'.join(current_block))
# todo the calls to sleep were added in an attempt to make this tests </s> self.asserttrue(	test_mine_delete def test_mine_delete(self): Test mine.delete self.run_function( 'mine.send',
#todo pliki specjalne </s> pt = self.pathtype.get(tid)	open @_pathdec def open(self, tid, flags): if pt is not self.PathType.file: raise FuseOSError(errno.EINVAL)
# todo check argument kinds </s> if isinstance(t, callable):	is_more_general_arg_prefix def is_more_general_arg_prefix(t: FunctionLike, s: FunctionLike) -> bool: if isinstance(s, Callable): return all(is_proper_subtype(args, argt)
# todo: this assert is probably not valid in all cases. </s> assert self.inipath is not none	_getini return [legacy_path(str(dp / x)) for x in input_values] elif type == "paths": dp = self.inipath.parent input_values = shlex.split(value) if isinstance(value, str) else value
# todo: support grouping and stacking at the same time </s> if self.attributes['stack'].columns is not none:	add_renderer def add_renderer(self, group, renderer): self.renderers.append(renderer) label = self._get_label(group['stack']) elif self.attributes['group'].columns is not None:
# todo: each dp learns independently. an edge dp could </s> eth_src = pkt_meta.eth_src	_edge_dp_for_host Returns: Valve instance or None (of edge datapath where packet received) vlan_vid = pkt_meta.vlan.vid for other_dpid, other_valve in valves.iteritems():
# todo(ochang): remove this once migrated to python 3. </s> stream = io.bufferedwriter(io.bytesio())	run_one_test_parallel suite = unittest.loader.TestLoader().loadTestsFromNames(test_modules) if sys.version_info.major == 2: else: stream = io.StringIO()
# todo: move 'hardcoded' coordinate specs (name, units, etc) into tile_spec </s> su_descriptor = index_netcdfs([filename[7:]])[filename[7:]]	create_storage_unit except OSError: pass return StorageUnit([dataset.id for dataset in datasets], mapping,
# todo(crcrpar): annotate this correctly. </s> @functools.wraps(func)	new_func def new_func(*args: Any, **kwargs: Any) -> Any: warnings.simplefilter('always', UserWarning)
# todo: add notification related to command-line options for </s> raise errors.error(	_handle_identical_cert_request "OK", "Cancel") if response[0] == "cancel" or response[1] == 2: "User did not use proper CLI and would like " "to reinvoke the client.")
# todo: revise this when finat gets dual evaluation </s> basis = finat_element.fiat_equivalent.dual_basis()	_bezier_calculate_points M = np.empty([deg + 1, deg + 1], dtype=float) finat_element = create_base_element(function.function_space().ufl_element()) for i in range(deg + 1): for j in range(deg + 1):
# todo(tfmot): renable once savedmodel preserves step again. </s> if save_restore_fn.__name__ == '_save_restore_tf_model':	testPruneStopAndRestart_PreservesSparsity @parameterized.parameters(test_utils.save_restore_fns()) def testPruneStopAndRestart_PreservesSparsity(self, save_restore_fn): return begin_step, end_step = 0, 4
# todo make sure this works </s> log("creating sensors...", 'info')	buildModelFromDictionary rootlink['modelname'] = model['name'] rootlink.location = (0, 0, 0) if 'sensors' in model and model['sensors']: for sen in model['sensors']:
# todo github.com/clusterhq/flocker/pull/897#discussion_r19024474 </s> client_1 = mongoclient(self.node_1, self.external_port)	test_traffic_routed mongo client would not have to be installed. However, this uses pexpect to be as close as possible to the tutorial. database_1 = client_1.example database_1.posts.insert({u"the data": u"it moves"})
# todo debug </s> print ("each rule element evaluates "	_evaluateRuleElementsRecursively + "to triggered. Set 'and' rule " + "also to triggered.") + "to triggered. Set 'and' rule " + "also to triggered.")
# todo add weight regularization (l2) </s> self.ffn_layers = [	TransformerEncoderLayer self.mha = MultiHeadAttention(d_model, num_heads) self.dropout = tf.keras.layers.Dropout(rate) tf.keras.layers.LayerNormalization(epsilon=1e-6), tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)
# todo -- make sure it's always a scale </s> if isinstance(scale, scalepattern):	new_message root   = group_modi(kwargs.get("root", self.event["root"]), index) scale  = kwargs.get("scale", self.scale) midinote = scale.get_midi_note(degree, octave, root) else:
# todo: must be implemented </s> pass	range_from_chapters def range_from_chapters(crawler, times=0):
# todo: verify logic for create -- we shouldn't 'annexify' non-annexified </s> annex = get_repo_instance(filedir)	_handle_auto_get filedir = dirname(filepath) try: except RuntimeError as e: return
# todo change to check for error when the functionality changes. currently acts as though it doesn't exist </s> url = "/v2/users/?filter[fullname]=notmymom"	test_find_no_user_in_users def test_find_no_user_in_users(self): res = self.app.get(url) user_json = res.json['data']
# todo: handle /dev/null (windows equivalent?) for new or deleted files </s> if before.endswith('.ipynb') or after.endswith('ipynb'):	show_diff If we are diffing a notebook, show the diff via nbdiff. Otherwise, call out to `git diff`. nbdiffapp.main_diff(before, after) else:
# todo: the following reproduces the old behavior of </s> i.clear()	_transformBlock for i in block.component_objects(Integral, descend_into=True): i.parent_block().reclassify_component_type(i, Expression) i._constructed = False i.construct()
# todo only do these things if status is true </s> else:	process status = Actions( endpoint, self.s.sdnc).mirror_endpoint() endpoint.p_next_state = 'mirror' endpoint.queue()
# todo(frostig): might the following work? </s> return false, 'lhs not split, rhs split but not contracting'	cases else: assert ydim is not None assert False, 'unreachable'
# todo: still in progress </s> def __init(self):	BertForSNLI class BertForSNLI(BaseModel): super(BertForSNLI, self).__init__() def forward(self, words, segment_id, seq_len):
# todo find out what is best used here! </s> 'preferred_dtype' : none}	get_meta_information 'is_deterministic': True, 'handles_sparse': True,
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo(nnorwitz): enable test. </s> self.assertequal([], modifiers)	_testSimpleArray self.assertEqual([], templated_types)
raise mpdnotimplemented  # todo </s> lines.	channels Obtain a list of all channels. The response is a list of "channel:"
# todo transfer headers, and authenticated proxies: not sure how to do it in chrome yet </s> chrome_options = webdriver.chrome.options.options()	_start_chrome_headless_browser def _start_chrome_headless_browser(self): chrome_options.add_argument('headless') chrome_options.add_argument('disable-infobars')
# todo document me </s> cert = certificate(content=cert_pem)	check_consumer_cert def check_consumer_cert(cert_pem): subject = cert.subject() encoded_user = subject.get('CN', None)
)  # todo </s> get_domain_from_id(group_id), group_id, user_id, content,	create_group return self.transport_client.create_group(
# xxx lp: todo: verify key format </s> signature = ssl_crypto__keys.create_signature(key, self.payload)	sign def sign(self, key): and adds the signatures to its signature properties. """ self.signatures.append(signature)
#self.assertequal(  # todo: fix </s> self.asserttrue(response.context["user"].is_authenticated())	test_admin response = c.get("/admin/") self.assertEqual(response.status_code, 200)
# todo factor this out entirely </s> def get_requirements(args):	get_requirements results = [] for item in args:
# todo(kpy): remove support for legacy urls in mid-january 2012. </s> import legacy_redirect	get_repo_and_action scheme, netloc, path, _, _ = urlparse.urlsplit(request.url) parts = path.lstrip('/').split('/') if legacy_redirect.get_subdomain(request): repo = legacy_redirect.get_subdomain(request)
# todo: test this method </s> if self.path:	get_absolute_url def get_absolute_url(self): return self.path
# todo: axis = 1 </s> return self.columns	keys Index(['max_speed', 'shield'], dtype='object')
# .. todo:: report an error to the user </s> return false	addListEntry if theKey == 'category': if not self.setCategory(theValue): myItem = QtGui.QListWidgetItem(theKey + ':' + theValue) self.removeItemByKey(theKey)
#ack = self.serial_port.read() # todo: use ack </s> self.sendcommand(181) # 10110101	SetBothLaserOn def SetBothLaserOn(self):
# todo: this is untested. </s> _raise_current_error()	generate_key ) if not res == 1: if not _lib.DSA_generate_key(dsa): _raise_current_error()
model=pke.supervised.kea())  # todo: fix doc for model param </s> extension='txt', df=none, leave_one_out=true,	test_train_supervised_model_leave_one_out str(tmp_corpus), str(tmp_ref), str(tmp_model),
# todo - gitlab issue #27 - only write cost data of controllable and in service elements </s> el_is = net[el].loc[net[el].in_service & net[el].index.isin(	_make_objective if el == "dcline": idx = dcline_idx costs.loc[costs.element_type == el].element)].index c = costs.loc[(costs.element_type == el) &
# todo remove compatibility shims for anki 2.1.46 and lower. </s> get_conf = decks.get_config if hasattr(decks, 'get_config') else decks.getconf	from_collection def from_collection(cls, collection, deck_config_id): decks = collection.decks anki_dict = get_conf(deck_config_id) deck_config = DeckConfig(anki_dict)
# integer case, todo: bool, date etc. </s> func = lambda a: a	_run_call_series_dropna func = series_replace_funcs['dropna_float'] else: return self._replace_func(func, [series_var])
# todo: switch _ignore_connection_aborted for _ignore_transmission_error, or provide retry mechanism </s> if self._ignore_connection_aborted:	transmit self._fuzz_data_logger.log_fail("Target connection reset.") except sex.BoofuzzTargetConnectionAborted as e: self._fuzz_data_logger.log_info("Target connection lost (socket error: {0} {1}): You may have a " "network issue, or an issue with firewalls or anti-virus. Try "
# todo proper error message </s> raise typeerror	Seq2Seq shape = (batch_size, ) + (None, ) + (input_dim) else: if hidden_dim is None: hidden_dim = output_dim
# todo: re-enable custom_objects </s> clone = sequential.from_config(config)	clone_model config = model.get_config() try: except: clone = Model.from_config(config)
# todo: remove compatability hook </s> if not exists(pathjoin(target_dir,"esky-bootstrap.txt")):	chainload if exists(dirname(target_dir)): if not exists(pathjoin(target_dir,ESKY_CONTROL_DIR,"bootstrap-manifest.txt")): execv(sys.executable,sys.argv) raise
# todo(danms): remove this legacy fallback when secure rbac </s> if not conf.enforce_secure_rbac:	modify_image def modify_image(self): self._enforce('modify_image') check_is_image_mutable(self._context, self._image)
return user.affiliation  # todo: update </s> elif item == 'user_phone':	get_user_info return user.affiliation  # TODO: update elif item == 'user_institution': return user.phone elif item == 'user_city':
# todo: not all messages have running status </s> status_byte = read_byte(infile)	read_track if debug: _dbg('-> delta={}'.format(delta)) if status_byte < 0x80: if last_status is None:
# todo: arrange </s> repo = self.remote.new_repo(self.token)	test_create_repo def test_create_repo(self): Test: create/edit a repo object self.assertTrue(self.remote.modify_repo(repo, "name", "testrepo0", self.token)) self.assertTrue(self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token))
# todo: this should match on the app_label as well as the model name to avoid potential duplicate names </s> return {	get_custom_link_models def get_custom_link_models(): 'model__in': [model.split('.')[1] for model in CUSTOM_LINK_MODELS],
# todo: -------------------------------------------------------------------- </s> self._commit(commit_msg)	add_to_git self.annex_add_to_git(files)
# todo: evaluate history </s> h = agent.test(env, nb_episodes=2, visualize=false, nb_max_episode_steps=100)	test_cdqn agent.fit(env, nb_steps=400, visualize=False, verbose=0, nb_max_episode_steps=100)
# todo: per node message function </s> return msg_gathered	_default_msg_func msg_gathered += x
# todo allow user to edit a txt file in blender which contains the description or take readme? </s> return modelconf	modelConf SubElement(authorEL, 'name').text = "DUMMY" SubElement(authorEL, 'email').text = "dummy@dummy.mail"
# todo: check for expected warnings. </s> return self._run_master(sample_0_9_0b5)	test_config_0_9_0b5 def test_config_0_9_0b5(self):
#todo: dont unfold all, but allow enum_all() to work </s> tree_proc(self.tree, tree_item_unfold_deep, 0)	open_all msg_status('Project not opened') return files = [] def callback_collect(fn, item):
# xxx todo </s> section = "output"	write_output_config Write configuration options in section "output".
# todo also check for motion codec parameter support </s> return 'h264_omx' in codecs	has_h264_omx_support if not binary: return False
# todo: remove at some point </s> if update or upgrade:	packages if clean: yield 'yum clean all' yield _update(state, host) yield ensure_packages(
oldsize = self.size # todo: remove </s> self.size = 8 + 4 + 4 + len(self.body[1]) * 4	stss_atom write_uint(stream, sample) def calsize(self): assert oldsize == self.size, '%s: %d, %d' % (self.type, oldsize, self.size) # TODO: remove return self.size
# @todo this is out because of the issue noted in the code. we'll </s> def test_bookmark_sync(self):	BookieAPITest '/api/v1/admin/bmarks?with_content=true&api_key=' + API_KEY, status=200) self._get_good_request(content=True, second_bmark=True)
#     todo </s> def release(self, path, fh):	Filesystem def readdir(self, path, fh): return ['.', '..'] + cache.list_effective_nodes(path) return os.close(fh)
# todo: check success summary the same way. </s> expected = """	test_summary_reason_without_protocol Summary classes should deny to use `failed_because` method on stories defined without failure protocol. 'failed_because' method can not be used to check result of a story defined without failure protocol. Story returned result: SummaryWithSimple.z
self.button_help = wx.button(self, label="help")                # todo maybe align left? </s> self.button_align_test = wx.button(self, label="align test")    # todo maybe align left?	create_sizer_bottom_buttonrow def create_sizer_bottom_buttonrow(self): self.button_apply = wx.Button(self, label="Apply") self.button_close = wx.Button(self, label="Close")
# todo: remove this when distributed materials are merged </s> warnings.warn('this feature is not yet implemented in a release ' \	set_otf_mat_file def set_otf_mat_file(self, name): 'version of openmc') if not is_string(name):
time.sleep(1)  # delay, for last.fm latency. todo can this be removed later? </s> last_scrobble = lastfm_user.get_recent_tracks(limit=2)[0]	test_scrobble lastfm_user = self.network.get_user(self.username) self.network.scrobble(artist=artist, title=title, timestamp=timestamp) self.assertEqual(str(last_scrobble.track.artist).lower(), artist) self.assertEqual(str(last_scrobble.track.title).lower(), title)
# todo: test coverage of this branch </s> logger.exception(	update_request instance.send_activation_email(action='update') except Exception, e: 'Error %s while submitting email to %s.', e, instance.email)
# todo(nnorwitz): it would be good to warn about this. </s> self._addbacktoken(token)	_GetBases assert token.token_type == tokenize.NAME, token if token.name not in ('public', 'protected', 'private'): base, next_token = self.GetName() bases.append(base)
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_por res = ctx_init["xmm0"] | ctx_init["xmm1"] x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo: link user's osf account with orcid </s> pass	oauth_user_email_post user = get_user(email=clean_email) if user: else: remove_session(session)
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-regular.ttf')	test_result_metadata_designer_exists_in_profiles_csv_success def test_result_metadata_designer_exists_in_profiles_csv_success(self): self.assertInSuccess('test_metadata_designer_exists_in_profiles_csv', run_set(p, 'result'))
# todo necessary? </s> return edgestarts	specify_edge_starts ("bot" + index, 20 + i * 50 + j * 5000)]
#todo: remove the below </s> self.current_defaults = set([	__init__ self.default_acl_possible = self.object_kind in OBJECTS_WITH_DEFAULTS current_defaults = dbcontext.get_role_current_defaults(rolename, object_kind, access) or set() (grantor, common.ObjectName(schema), pg_priv_kind) for grantor, schema, pg_priv_kind in current_defaults ])
# todo: this function should probably move to somewhere in casexml.apps.stock </s> if stock_report_helper.tag not in stockconst.valid_report_types:	create_models_for_stock_report assert stock_report_helper._form.domain == domain domain = domain return report = StockReport.objects.create(
# todo: proper java error? </s> raise runtimeerror('could not find class \'%s\' for jnienv.' % name)	find_class clazz = self._class_loader.find_class_by_name(name) if clazz is None: return clazz.jvm_id
#todo tuplet: add variables for if it's the start of a tuplet </s> self.istuplet = false	VexflowChord self.accidentalDisplayStatus = None self.params = params self.tupletLength = 0 self._generateVexflowCode()
# todo: test for the _correct_ revision_id value. </s> if not activity.revision_id:	_update_resource if not activity.id: assert False, "activity object has no id value" assert False, "activity has no revision_id value" assert activity.timestamp >= before and activity.timestamp <= after, \
# todo: dynamically add/remove adapters </s> self._ports.clear()	_parseResponse self._settings[name] = value log.debug("number of adapters has changed: Ethernet={} Serial={}".format(self._settings["ethernet_adapters"], self._settings["serial_adapters"])) self._addAdapters(self._settings["ethernet_adapters"], self._settings["serial_adapters"]) elif name in self._settings and self._settings[name] != value:
# todo: cronjob (celery task) to delete stale tokens </s> except emailconfirmationrequest.doesnotexist:	confirm_email email_confirmation_request = EmailConfirmationRequest.objects.get( token=token, valid_until__gte=now()) return TemplateResponse(request, 'registration/invalid_token.html') user = email_confirmation_request.get_authenticated_user()
# todo: detect the zygote and run 'art-on' automatically. </s> root = os.environ["android_build_top"]	generate_gdb_script def generate_gdb_script(sysroot, binary_file, is64bit, port): symbols_dir = os.path.join(sysroot, "system", "lib64" if is64bit else "lib") vendor_dir = os.path.join(sysroot, "vendor", "lib64" if is64bit else "lib")
#todo: log all non-http errors to stderr </s> return {"error":str(exc)}	change_slot data = bottle.request.json except Exception as exc: try: if isinstance(value, andbug.Object):
# todo: refactor to not use a try/except </s> try:	_info if trace is None: trace = analyse(exctyp, value, tback) server = smtphost  # pylint: disable=undefined-variable except NameError:
# todo: require an api key on the basic auth header </s> file_storage = request.files.values()[0]	upload if len(request.files) != 1: return 'Need exactly one file', 400 data = file_storage.read() sha1sum = hashlib.sha1(data).hexdigest()
# todo lib </s> renamed_uuids = {	BpyDataCollectionDiff proxy_uuids = set(proxies.keys()) blender_uuids = set(blender_items.keys()) uuid for uuid in blender_uuids & proxy_uuids if proxies[uuid].data("name") != blender_items[uuid][0].name }
# no other choice fixme todo </s> return self.model.transform(x)	transform self.sklearnfit()
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
#todo - check annotation </s> if old.id != new.id and old.name != new.name :	compare_records return False for old, new in zip(old_list, new_list) : return False if str(old.seq).upper() != str(new.seq).upper() :
# todo: this test requires manifold access, see: t88318502 </s> self._test_rcnn_model("coco-instancesegmentation/mask_rcnn_r_50_fpn_3x.yaml")	TestScripting @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available") def testMaskRCNN(self): @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available") def testRetinaNet(self):
# todo: check that the performance measure is within some range </s> grid1_baseline(num_runs=1, sumo_binary="sumo")	test_grid1 Tests flow/benchmark/baselines/grid1.py
# todo debug </s> print logstring	logRule + "weekday=%d)" % ruleElement.element.weekday) logging.info("[%s]: %s" % (fileName, logString)) elif ruleElement.type == "monthday": logString = ("%s monthday " % spaceString
# todo: flush logs to generate a log per favorite event, so we can link </s> except:	on_data unsent=list(targets), ).get_or_save() logging.exception('Error processing message: %s', raw_data) return True
# todo: test this block </s> self.title = self.title or self.content_object.page_title	update_from_related_object self.heading = self.heading or self.content_object.meta_title elif hasattr(self.content_object, 'page_title'): self.heading = self.heading or self.content_object.page_title elif hasattr(self.content_object, 'title'):
# todo username </s> return 'aqbwdj5qap6lhhaaskvbnukyhj7eyremko5qka=='	get_monitor_secret def get_monitor_secret():
#    todo: maybe we're being too generic in this isinstance? </s> reason_msg = '%s: %s' % (error.__class__.__name__,	get_exception_reason elif isinstance(error, httplib.HTTPException): error.args) return reason_msg
# todo(brett.cannon) implement </s> pass	test_path_hooks def test_path_hooks(self):
# todo: fix with stubber / before send event </s> with mock.patch('botocore.endpoint.endpoint._send') as _send:	test_get_export 'accepts': 'application/yaml' } _send.return_value = mock.Mock( status_code=200, headers={}, content=b'{}')
# todo: error </s> return	handle_ACK if call_id not in self._callids: logger.warn("Given Call-ID does not belong to any session: exit") try: self._callids[call_id].handle_ACK(msg)
# todo: implement this </s> pass	_showUploadSuccessMessage def _showUploadSuccessMessage(self):
# todo: hanlde multiple keys (index args) </s> index_arg = self._get_arg('crosstab', rhs.args, kws, 0, 'index')	_handle_crosstab def _handle_crosstab(self, lhs, rhs, label): kws = dict(rhs.kws) columns_arg = self._get_arg('crosstab', rhs.args, kws, 1, 'columns') in_vars = {}
# todo(b/161952382): replace with keras premade models and </s> deep = tf.keras.layers.densefeatures(deep_columns)(input_layers)	_wide_and_deep_classifier for colname in _transformed_names(_CATEGORICAL_FEATURE_KEYS) }) for numnodes in dnn_hidden_units: deep = tf.keras.layers.Dense(numnodes)(deep)
#todo load this from somewhere </s> pad_target = 189	assign_dev_data 0.0268245, -0.0277465, 0.258805, -0.187777, -2.3835, -1.42065] device.data[l:, q] = pad_data device.targets[l:, q] = pad_target chunking_active = self.data.chunk_size > 0
self.router.send_multipart([address, '', payload])  # todo, send job id </s> 'status': status})	execute_and_reply 'jobid': jobid,
# todo: self.assertfalse(prop.is_valid(np.bool8(true))) </s> self.asserttrue(prop.is_valid(np.int8(0)))	test_Complex try: import numpy as np self.assertTrue(prop.is_valid(np.int8(1))) self.assertTrue(prop.is_valid(np.int16(0)))
pass  # todo </s> def forceprintjob(self, print_job_uuid: str) -> none:	forcePrintJob
# todo check checksum match </s> pass	check_obj error = True else: results = [] try:
# todo: check num strings and support nan </s> data_start = getitem_c_arr(	_str_get_impl for i in numba.parfor.internal_prange(n): start_index = getitem_c_arr(arr._index_offsets, i) arr._data_offsets, start_index + ind) data_start += 1
pass  # todo </s> def start_threads(self):	start_threads
# todo: lacp timeout configurable. </s> if lacp_age > 10:	state_expire if port.dyn_lacp_up: lacp_age = now - port.dyn_lacp_updated_time self.logger.info('LACP on %s expired' % port) ofmsgs.extend(self.lacp_down(port))
# todo: better scoring algorithm </s> s1 = set(sen1.lower().split())	score_sentences :param sen2: (str) sentence :returns: score s2 = set(sen2.lower().split()) score = 0
self.assertfalse(greps(err, "unit zzz.service not for --user mode")) #todo </s> self.assertequal(out.strip(), "unknown")	bad_usermode_other_commands logg.info(" %s =>%s \n%s\n%s", cmd, end, err, out) self.assertEqual(end, 3) cmd = "docker exec {testname} {systemctl} is-failed zzz.service -vv" out, err, end = output3(cmd.format(**locals()))
# todo: make these http requests asynchronous. not easy since we don't </s> url = api_retweets_url % tweet['id']	get_activities if fetch_shares: for tweet in tweets: tweet['retweets'] = json.loads(self.urlread(url)) return total_count, [self.tweet_to_activity(t) for t in tweets]
# todo: determine if this puts the case properties in the expected order. </s> case_properties={	_ucla_form_modifier case_name=hidden_value_path, case_type='task', 'task_responsible': '/data/task_responsible', 'task_due': '/data/task_due',
# todo: check if we can avoid py3 specific here </s> return "" if x is none else binary_type.decode(x)	decode_if_not_None def decode_if_not_None(x):
# todo: deprecate </s> return self.parse_authorization_code(code, client)	query_authorization_code :return: authorization_code object if hasattr(self, 'parse_authorization_code'): raise NotImplementedError()
# todo implement. </s> yaml = self.master_model.to_yaml()	_train def _train(self, rdd, nb_epoch, batch_size, verbose, validation_split):
# todo(piyush): current api-site doesn't contain this api description. </s> uri = '/agents/%s/l3-routers' % agent_id	add_router_to_l3_agent def add_router_to_l3_agent(self, agent_id, **kwargs): return self.create_resource(uri, kwargs)
# todo(piyush): current api-site doesn't contain this api description. </s> patch_body = json.dumps({'user': kwargs})	update_user_own_password def update_user_own_password(self, user_id, **kwargs): resp, body = self.patch('OS-KSCRUD/users/%s' % user_id, patch_body) self.expected_success(200, resp.status)
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
# todo: write a unit test </s> return self.find_all(subtree, terminal=true, breadth_first=breadth_first)	get_terminals def get_terminals(self, breadth_first=False):
# todo: more tests </s> self.checker.visit_listcomp(node)	test_non_iterable_in_listcomp with self.assertAddsMessages(message):
# todo: test for the _correct_ revision_id value. </s> if not activity.revision_id:	_update_resource if not activity.id: assert False, "activity object has no id value" assert False, "activity has no revision_id value" assert activity.timestamp >= before and activity.timestamp <= after, \
# todo: triage </s> max_workers = min(len(context.pdfinfo), context.options.jobs)	exec_concurrent def exec_concurrent(context): context.log.info("Start processing %d pages concurrent" % max_workers) with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_METADATA_postScriptName_matches_internal_fontname_success def test_result_METADATA_postScriptName_matches_internal_fontname_success(self): r = run_set(p, 'result') success_tests = r['success']
# todo: use pabot options here </s> _copy_output_artifacts(options)	_report_results outputs = [] # type: List[str] for index, _ in pabot_args['argumentfiles']: outputs += [_merge_one_run(os.path.join(outs_dir, index), options, tests_root_name, stats, outputfile=os.path.join('pabot_results', 'output%s.xml' % index))]
# todo(ihrachys): replace with port.create() once we get an object </s> self._port = db_api.create_object(self.context, models_v2.port,	_create_test_port def _create_test_port(self, network): {'name': 'test-port1', 'network_id': network['id'],
raise notimplementederror # the below does most probably not work anymore todo </s> cfg = self._config	_upload_ssh_key TESTS:: TODO try: with open(keyfile, 'r') as F:
# todo assert exit code != 0 </s> self.assertequal(dvol.voluminous.getoutput(),	test_create_volume_already_exists dvol = VoluminousOptions() dvol.parseOptions(ARGS + ["-p", self.tmpdir.path, "init", "foo"]) ["Error: volume foo already exists"])
# todo: add logging </s> pass	parse_clip_path def parse_clip_path(self, token='W', params=''):
# todo: create/clear alarm_data folder </s> threads = []	got_action def got_action(self, ch, method, properties, body): for act in self.actors: t = threading.Thread(name='thread-%s'%(act.id), target=act.execute)
# todo is this redundant now we have --dumptab? </s> print "[e] ignoring thing that isn't file or directory: " + f.get_name()	audit_program_files else:
# todo: add option for attentive reader </s> print('trainable variables (only embeddings): %d' % get_total_trainable_variables())	boe_nosupport_cands_reader_model varscope.reuse_variables() candidates_embedded = nvocab(candidates) question_encoding = tf.reduce_sum(question_embedded, 1) scores = logits = tf.reduce_sum(tf.expand_dims(question_encoding, 1) * candidates_embedded, 2)
# todo: remove? </s> const.requisitionactions.request	process_transactions const.StockActions.STOCKONHAND, const.StockActions.STOCKOUT, ): balances.append(tx)
raise notimplementederror  # todo... </s> elif self.wrap_mode != "wrap_around":	perform beam = beam_trans.transpose(*map(array_trans_dims_order.index, range(array.ndim))) if self.wrap_mode == "pad_zero": raise NotImplementedError beam_out[0] = beam
#todo: calculate _net_workarea for the monitor represented by </s> clipbox = usablearea.get_clipbox()	cycleDimensions if not usableArea: return None dims = [] for tup in dimensions:
remote.fetch()  ### todo: show progress </s> print(" done.")	_update_repository for remote in remotes: print(INDENT2, "Fetching", remote.name, end="...") repo_rebase = _read_config(repo, "pull.rebase") _update_branch(active)
# todo: the one_hot=true is only necessary because one_hot=false is </s> self.train = mnist(which_set = 'train', one_hot=true)	setUp def setUp(self): skip_if_no_data() self.test = MNIST(which_set = 'test', one_hot=True)
# todo : the section permission here should be ql_x86_a_priv_3, but i do n’t know why it can only be set to ql_x86_a_priv_0. </s> ql.gdtm.register_gdt_segment(5, 0, 0xfffff000, ql_x86_a_present | ql_x86_a_data | ql_x86_a_data_writable | ql_x86_a_priv_0 | ql_x86_a_dir_con_bit, ql_x86_s_gdt | ql_x86_s_priv_0)	ql_linux_x8664_register_ds_ss_es def ql_linux_x8664_register_ds_ss_es(ql): ql.register(UC_X86_REG_SS, ql.gdtm.create_selector(5, QL_X86_S_GDT | QL_X86_S_PRIV_0))
# todo: remove when support for django 1.4 is dropped </s> from django.db.models.fields import field	patch_db_field_compare object it's compared to isn't a Field instance. Let's monkey patch it! see https://code.djangoproject.com/ticket/17851 try: assert Field() != None
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
os.chdir(dest_dir)  # todo: error checking </s> return 0	Popd log('popd: directory stack is empty') return 1
# todo: verify that we need this for-loop </s> for key in kwds:	write_json ostream = sys.stdout option = copy.copy(SolverResults.default_print_options) setattr(option,key,kwds[key]) repn = self.json_repn(option)
# todo: remove </s> if use_cache:	__init__ self.cache_enabled = False self.cache = None self.setup_cache(backend='mongo', database=cache_db, use_compression=use_cache_compression)
# todo check the op returned a view </s> if dmap and idx in dmap:	summary_memory vmap = getattr(node.op, 'view_map', None) for idx, v in enumerate(val): node_memory_saved_by_inplace += v elif vmap and idx in vmap:
# todo(mierdin) will parameters always be here? fix this and the heinous thing below </s> schema = raw_inquiry["parameters"].get(	_transform_inquiry if not raw_inquiry.get("parent"): return None "schema", raw_inquiry["runner"]["runner_parameters"]["schema"]["default"]
# todo: do we need to skip config.add_slack variable here? </s> var_filter = (lambda v: v.is_integer()) if discrete_only \	generate_norm1_objective_function discrete_only: Bool only optimize on distance between the discrete variables else (lambda v: v.name != 'MindtPy_utils.objective_value' and 'MindtPy_utils.MindtPy_feas.slack_var' not in v.name)
# todo: allow iteration over regionfile self. (thus: for chunk in regionfile('region.mcr'): ... ) </s> for cc in self.get_chunk_coords():	iter_chunks Warning: this function returns a NBTFile() object, use Chunk(nbtfile) to get a Chunk instance. try: yield self.get_chunk(cc['x'], cc['z'])
# todo: test more stuff! </s> assert self.classifier_group['pytorch_network_path'].value == ""	test_classifier_serialization classifier.serialize_hdf5(self.classifier_group)
# todo find out what's wrong </s> warnings.warn('stroud\'s scheme has degree 0, not 3.')	__init__ self.dim = n self.degree = 0 p = numpy.array([ n+1, -3, 3/(n+2), -1/(n+2)/(n+3)
# todo: use dataopdict/tuple's new `map` method. </s> if hasattr(actions[some_key], "len"):	_execute "ERROR: Cannot flip Dict-action batch with dict keys if returned value is not a dict OR " \ "values of returned value are not np.ndarrays!" env_actions = [{key: value[i] for key, value in actions.items()} for i in range(len(actions[some_key]))] else:
# todo: rewrite tests </s> pass	test_resend_form_shows_alert_if_email_already_confirmed def test_resend_form_shows_alert_if_email_already_confirmed(self):
# todo(brett.cannon) implement </s> raise importerror	_default_hook def _default_hook(self, path): If the path will not work for the default hook then raise ImportError.
# todo(rosmaita): bug #1745003 </s> self.assertequal('private_id_2', rows[1]['id'])	TestOcataMigrate01Mixin self.assertEqual(4, len(rows)) self.assertEqual('private_id_1', rows[0]['id']) self.assertEqual('public_id', rows[2]['id'])
# todo: detect other runtimes </s> else:	do_deploy else: echo("-----> Could not detect runtime!", fg='red') echo("Error: Procfile not found for app '%s'." % app, fg='red') else:
# todo: change to deprecationwarning in version 1.1 </s> warnings.warn(	_body_file__set def _body_file__set(self, value): if isinstance(value, str): "Please use req.body = 'str' or req.body_file = fileobj", PendingDeprecationWarning,
# todo: replace with specific error when exceptions are refactored </s> raise xlwings.xlwingserror("getting or setting 'app.interactive' isn't supported on macos.")	interactive @interactive.setter def interactive(self, value):
# todo: merge with config_check_pre_system_cron </s> stop_system_importer_file_csv_run = false	config_check_run def config_check_run(model): if not model.csv_import_username: mainconfigmodel = MainConfigModel.objects.get(main_config_name = 'MainConfig')
# due to exponentiation being involved there is some fp error. todo: arrange to be able to assert the range, or duplicate the computation, instead of using exact constants </s> self.__check_parsed(	test_compressed_position_example_altitude def test_compressed_position_example_altitude(self): 'FOO>BAR:!/!!!!!!!!>S]S', facts=[
# todo(b/163904067): mimic defun' behavior and reset the step seed to </s> resetstepseed()	Forward @tf.function(input_signature=Flatten(fwd_sig), autograph=False) def Forward(*args): with RemoveAssertContext(remove=noinline), tf.device(device): xs = Pack(fwd_sig, args)
# todo(eric_k): unicorn@1.0.2rc1 doesn't like writing to </s> registers -= {"fs"}	_step registers -= set(["CF", "PF", "AF", "ZF", "SF", "IF", "DF", "OF"]) registers.add("EFLAGS") for reg in registers: val = self._cpu.read_register(reg)
# todo: where to store config? </s> return mysqldb.connect(	ingest_upload_atk_db_connection def ingest_upload_atk_db_connection(): host="localhost", user="root",
# todo: refactor accordingly when v3 websocket api is released </s> output["results"].update({	BittrexAPIOrderBookDataSource if _is_snapshot(msg): output["results"] = _decode_message(msg["R"]) "M": f"{output['results']['M'].split('-')[1]}-{output['results']['M'].split('-')[0]}" })
# todo: if we're going to implement ttl, it will be here. </s> self.known_nodes[ursula_interface_id] = \	follow_treasure_map if header != constants.BYTESTRING_IS_URSULA_IFACE_INFO: raise TypeError("Unknown DHT value.  How did this get on the network?") Ursula.as_discovered_on_network( dht_port=port,
#todo fixme: this is a very crude way of dupe checking </s> else:	addClaims if claim.getID() in item.get().get('claims'): pywikibot.output(u'A claim for %s already exists. Skipping' % (claim.getID(),)) pywikibot.output('Adding %s --> %s' % (claim.getID(), claim.getTarget().getID())) item.addClaim(claim)
# todo: replace with a getter </s> die(self.invalid_json_msg)	check_json pass else:
#  todo: test </s> ship.getmodifieditemattr("shipbonusics2"),	handler "maxGroupOnline",
# todo: union types don't work with scalar types </s> return self.field	_resolve_type )
# todo(releasesv2): add health data </s> self.browser.snapshot("organization releases v2 - detail")	test_detail self.browser.wait_until_not(".loading")
# todo: what happens to sysex messges here? </s> if len(self._bytes) == self._typeinfo.size:	feed pass if self._inmsg: opcode = self._bytes[0] data = self._bytes[1:]
# todo: speedup by allocating the denominator directly instead of constructing it by sum </s> return tf.slice(tens, [0, 0, second * single_batch_size], [m, n, single_batch_size])	half n = int(n)
# todo: revert this. </s> from astropy.tests.helper import quantity_allclose	hpc_to_hpc This converts from HPC to HPC, with different observer location parameters. It does this by transforming through HGS. if (heliopcoord.observer == heliopframe.observer or (quantity_allclose(heliopcoord.observer.lat, heliopframe.observer.lat) and
# todo: test pdb files with dna and rna too: </s> record.annotations["molecule_type"] = "protein"	iterate record = SeqRecord(Seq("".join(residues))) record.annotations = {"chain": chn_id} if chn_id in metadata: m = metadata[chn_id][0]
# todo: document </s> try:	create def create(self): self._phkey = _winreg.CreateKeyEx(self.surkey.phkey, self.name, 0, self.sam) except WindowsError as e:
annot.annotation_metadata.validation_and_reliability = "todo" #todo </s> annot.annotation_metadata.origin = metadata[1]	fill_annotation annot.annotation_metadata.annotation_tools = "Sonic Visualizer" annot.annotation_metadata.annotation_rules = "TODO" #TODO annot.annotation_metadata.annotator.name = metadata[annotation_id + 2] annot.annotation_metadata.annotator.email = "TODO" #TODO
# todo share code with check_argument_count in checkexpr.py? </s> if len(call.args) < 1:	process_typevar_declaration if callee.fullname != 'typing.typevar': return self.fail("Too few arguments for typevar()", s) return
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pslldq_2 res = 0x00000000000000000000000000000000 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init)
if np.any(coulomb_log):  # todo: something </s> clog = coulomb_logarithm(t_i,	fundamental_ion_collision_freq method=coulomb_log_method) coeff = np.sqrt(8 / np.pi) / 3 / 4 n_i, particles,
# todo change when v4 web3.py will released </s> assert miner_id == escrow.call().getminerid(miner, 0).encode('latin-1')	test_miner_id chain.wait.for_receipt(tx) assert 1 == escrow.call().getMinerIdsCount(miner) miner_id = os.urandom(32) tx = escrow.transact({'from': miner}).setMinerId(miner_id)
# todo: comment in visit_print also applies here </s> return self.visit(ast27.expr(	visit_Exec if new_locals is None: new_locals = ast27.Name("None", ast27.Load(), lineno=-1, col_offset=-1) ast27.Call( ast27.Name("exec", ast27.Load(), lineno=n.lineno, col_offset=-1),
# todo: handle multiple skip stacks </s> child_skip_stack, = skip_stack.values()	first_letter_to_box elif isinstance(child, boxes.ParentBox): if skip_stack: else: child_skip_stack = None
# todo clarify api - this should be pretty limited to support mainly confirming </s> return confirm_text(	confirm_amount icon_color: int = ui.GREEN,  # TODO cleanup @ redesign ) -> Awaitable[None]: ctx, br_type=br_type,
# todo remove </s> args = parsing.array(parsing.array.tuple, none, values=[])	handle_iterators else: try: generators += \ it.execute_subscope_by_name('__iter__', args)
raise skiptest("buggy")  # todo(mattjj): fix </s> n = 4 * xla_bridge.device_count()	testSoftPmapNested def testSoftPmapNested(self): @partial(soft_pmap, axis_name='i') @partial(soft_pmap, axis_name='j')
elif key_type == unicode:  # todo: change to 'str' on python3 </s> values = list(value)  # i'm not lazy, sorry	Table if key_type == int: self._rows[key] = self._make_row(value) if len(values) != len(self): raise ValueError('Values length ({}) should be the same as '
#todo: create a failed test if a dependency didn't install? </s> return handled[spec]	createTestOutput output.addTest(bId, package.installed, buildLogPath + '\n' + spec.to_yaml() + buildLog)
# todo(zchee): configurable and refactoring </s> if c.type == 'function':	completions kind = re.sub('\n|  ', '', c.description) info = c.docstring() word = c.name + '(' elif word == 'self' or c.type == r'class|module' and not \
# todo: when sharing moves into the store this should be replaced </s> lock = (yield self._createlock(userid, request))	inviteSingleUserToShare if principalURL is None: returnValue(False) yield self._acquireLock(lock) try:
# todo: systemhistory_user_id </s> )	save systemhistory_old_value = self.previous_systemstatus.systemstatus_name, systemhistory_new_value = self.systemstatus.systemstatus_name, systemhistory.save() self.previous_systemstatus = self.systemstatus
# todo(developer): uncomment and set the following variables </s> from google.cloud import automl_v1beta1 as automl	batch_predict_bq bq_output_uri, ): client = automl.TablesClient(project=project_id, region=compute_region)
:class:`goless.channelclosed` will be raised. (#todo) </s> if self._closed and not self.recv_ready():	recv If the channel is already closed, :class:`goless.ChannelClosed` will be raised. If the channel closes during a blocking ``recv``, raise ChannelClosed() got = self._recv()
# todo debug </s> print logstring	logRule + "monthday=%d)" % ruleElement.element.monthday) logging.info("[%s]: %s" % (fileName, logString)) elif ruleElement.type == "hour": logString = ("%s hour " % spaceString
# todo(jeremydw): thread pool. </s> threads = []	dump bucket.configure_website(main_page_suffix='index.html', error_key='404.html') paths_to_contents = pod.dump() for path, contents in paths_to_contents.iteritems(): thread = threading.Thread(target=self._upload, args=(bucket, path, contents))
# todo(iceboy): check if the user attended the contest. </s> rid = await record.add(self.domain_id, pdoc['doc_id'], record.type_submission, self.user['_id'],	ContestDetailProblemSubmitHandler tdoc, pdoc = await asyncio.gather(contest.get(self.domain_id, tid), problem.get(self.domain_id, pid)) lang, code, tid=tdoc['doc_id'], hidden=True) self.json_or_redirect(self.reverse_url('record_detail', rid=rid))
# todo: union types don't work with scalar types </s> graphql_type = graphqluniontype(	get_graphql_type_for_annotation else: is_optional = type(None) in types field_name, [type.field for type in types] )
# todo: implement meaningful test </s> self.core.setup()	test_restore_state def test_restore_state(self):
# todo: use closest instead of conditioning on single entry </s> if len(lineswithsameurir) == 1:	generateNoMementosInterface print('CDXJ lines with URI-R at {0}'.format(path)) print(linesWithSameURIR) fields = linesWithSameURIR[0].split(' ', 2) redirectURI = '/{1}/{0}'.format(unsurt(fields[0]), fields[1])
# todo: include urls explicitly in desc format </s> childdescs[key] = cell.description()	state_description } for key, cell in self.state().iteritems(): return description
# todo: refactor </s> if mode is none:	iterator conv_fn = None convert.append(conv_fn) if hasattr(self, '_iter_subset_class'): mode = self._iter_subset_class
# todo isolate this test </s> for name in ['language', 'lc_all', 'lc_ctype', 'lc_messages']:	test_default def test_default(self): os.environ[name] = '' os.environ['LANG'] = 'fr_FR.UTF-8'
# todo: provide a kernel which will describe how coordinates are extruded. </s> mesh = firedrake.extrudedmesh(m, layers, layer_height=0.1)	integrate_var_p0 m = UnitSquareMesh(2 ** power, 2 ** power) layers = 11 fs = firedrake.FunctionSpace(mesh, family, degree, name="fs") f = firedrake.Function(fs)
# todo: send finished </s> else:	got_action for t in threads: t.join() logging.debug("received action but wasn't active")
tol = 0.15  # todo(skye): can we be more precise? </s> jtu.check_grads(np_fn, args_maker(), order=1, atol=tol, rtol=tol)	testFft2 self._CompileAndCheck(np_fn, args_maker, check_dtypes=True) if dtype in inexact_dtypes: jtu.check_grads(np_fn, args_maker(), order=2, atol=tol, rtol=tol)
# todo: fixme for photos 4 </s> if self._db._db_version >= _photos_5_version:	live_photo @property def live_photo(self): return self._info["live_photo"] else:
# todo make "master" not hard-coded, fetch it from some metadata </s> branch = default_branch	_destroyNewerCommits def _destroyNewerCommits(self, commit, volume): commits = self.commitDatabase.read(volume, branch) commitIndex = [c["id"] for c in commits].index(commit) + 1
# todo: extend to inputs with shape (n_samples, 1) </s> cover = onedimensionalcover(n_intervals=1)	test_filter_values_covered_by_single_interval ) def test_filter_values_covered_by_single_interval(filter_values): interval_masks = cover.fit_transform(filter_values) assert_almost_equal(
#todo: this loop is pretty slow .. (parellize) </s> for iky in range(self.nky):	Jvec dky = np.r_[dky[0], dky] y = 0. ky = self.kys[iky] A = self.getA(ky)
# todo move to common? </s> def as_date(dd: dateish) -> date:	as_date if isinstance(dd, datetime): return dd.date()
# todo: give a vanilla example </s> .. math::	feca def feca(predicted_power, df_appliances_ground_truth): fraction = \\sum_n min \\left (
time.sleep(1)  # delay, for last.fm latency. todo can this be removed later? </s> last_scrobble = lastfm_user.get_recent_tracks(limit=2)[0]	test_scrobble lastfm_user = self.network.get_user(self.username) self.network.scrobble(artist=artist, title=title, timestamp=timestamp) self.assertEqual(str(last_scrobble.track.artist).lower(), artist) self.assertEqual(str(last_scrobble.track.title).lower(), title)
# todo(mordred) when this changes to rest, force interface=admin </s> if self.cloud_config.get_api_version('identity').startswith('2'):	delete_endpoint self.log.debug("Endpoint %s not found for deleting", id) return False endpoint_kwargs = {'id': endpoint['id']} else:
# todo: remove this when we depend on genshi >= 0.5 </s> out.write(stream.render('text'))	do_deploy out = open(dest, 'w') if arity(stream.render) == 3: else: stream.render('text', out=out)
# todo: test without file </s> def test_impl():	test_nunique_str_parallel def test_nunique_str_parallel(self): df = pq.read_table('example.parquet').to_pandas() return df.two.nunique()
# todo: make this configurable </s> service_identifier = "org.privacyidea"	create_challenge validity = int(get_from_config(lookup_for, validity)) user_idenitfier = "1289734" challenge = "1234567890" db_challenge = Challenge(self.token.serial,
# todo(harlowja): the bug 1214083 is causing problems </s> log.debug(_("%(flow)s has moved %(runner)s into state %(state)s with"	task_log_change def task_log_change(state, details): " result: %(result)s") % {'state': state, 'flow': str(details['flow']),
# todo: distinguish between text elements with actual whitespace </s> try:	_scraped_content tag: BeautifulSoup Tag Returns: string content_div = Facebook._div(tag, 0, 0) except IndexError:
# todo: require tests </s> return true	hessian_is_zero def hessian_is_zero(self):
# todo remove comment parameter. </s> constraints = []	constraints @property def constraints(self): for c, comment in self._constraints: if comment:
#todo need to hook up any inputs here </s> pargs.append("embed = %(embed_module)s" % {'embed_module':util.pythonise(module['conf']['embed']['value']['id'])})	write_pipe pargs.append("%(id)s = %(secondary_module)s" % {'id':util.pythonise(pipe['wires'][wire]['tgt']['id']), 'secondary_module':util.pythonise(pipe['wires'][wire]['src']['moduleid'])}) if module['type'] == 'loop': pymodule_name = "pipe%(module_type)s" % {'module_type':module['type']} pymodule_generator_name = "pipe_%(module_type)s" % {'module_type':module['type']}
# todo(b/155239129): used list_physical_device in `setup` for gpu tests. </s> self.assertequal(result, 10)	test_execution_of_tensorflow result = comp()
# todo: it seems that yahoo! converts relative links to </s> content = unicode(f.read(), 'utf-8')	pipe_fetchpage url = util.get_abspath(url) f = urlopen(url) if context and context.verbose: print '............Content .................'
# todo watch out because urllib.unquote will blow up on unicode text </s> msg = self.server.backend.message(session_id, urllib.unquote(text))	do_GET session_id = match.group(1) text = match.group(2) self.server.backend.route(msg) self.send_response(200)
# todo add binary column (after dropping support for python 2.7) </s> table_name = "to_sql_{0}".format(str(uuid.uuid4()).replace("-", ""))	test_to_sql @with_engine() def test_to_sql(self, engine, conn): df = pd.DataFrame( {
self.assertequals(status, 200) # todo: should be 202 </s> status, body = self.post(path, body)	test_install options=options,)
# todo. check if build_url_fname can be used. </s> newpath = "/".join(['..']*3 + [newpath])	display_first_image_as_thumbnail self.report.add_lnkref_to_photo(photo, lnkref) real_path, newpath = self.report.prepare_copy_media(photo) if constfunc.win(): newpath = newpath.replace('\\',"/")
raise notimplementederror #todo, implement! </s> def testsubclass(cls, subset, subclass):	testsubclass
# todo: deprecate `pages` in favor of `page_limit` since it is less confusing </s> if 'pages' in kwargs:	get_photos ) kwargs.pop('sleep') kwargs['page_limit'] = kwargs.pop('pages') options['reactions'] = kwargs.pop('extra_info', False)
# todo: arrange </s> repo = self.remote.new_repo(self.token)	test_create_repo def test_create_repo(self): Test: create/edit a repo object self.assertTrue(self.remote.modify_repo(repo, "name", "testrepo0", self.token)) self.assertTrue(self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token))
# todo: os.path doesn't make sense here as it's os-dependent </s> tmp_dir = os.path.dirname(slashless_path)	temporary_path num, self._trailing_slash()) if tmp_dir: self.fs.mkdir(tmp_dir, parents=True, raise_if_exists=False)
common_path=prefix,  # todo: add key? </s> action="remote",	make_inline_attachments_decision rd = remote_diff[k] md = MergeDecision( conflict=False, local_diff=[],
# xxx todo register a failure handler that reverses the local state </s> return "ok"	archive q = action.get_queue() q.enqueue(action.get_archive_fn(account), thread_id)
# todo: console auth doesn't have support for handling unknown providers </s> provider = provider_from_address(email_address)	handler_from_email user, domain = email_address.split('@') email_address = user + '@exchange.mit.edu' if provider == "unknown": provider = "eas"
#todo same issue with batch_size </s> if len(self.inputs) == 0:	channels def channels(self): raise ValidationException("gan.channels() requested but no inputs provided") return self.ops.shape(self.inputs[0])[-1]
# todo add locales </s> raise yunohosterror("domain_name_unknown", domain=domain)	domain_set_settings domains = _load_domain_settings() if not domain in domains.keys(): setting_set = False if ttl is not None:
# todo: tokenize attribute strings properly </s> df_chunk["attribute"] = df_chunk["attribute"].str.replace(';\"', '\"')	_read_gtf encoding="ascii", chunksize=chunksize)): df_chunk["attribute"] = df_chunk["attribute"].str.replace(";-", "-") pieces.append(df_chunk)
#todo - cookies? </s> curl_cmd = 'curl -l '	curl_from_request def curl_from_request(request): header = '' if request.headers:
# => todo: allow for passing a branch </s> cmd_list.append(repo.get_active_branch())	_update_repo if remote: cmd_list.append(remote) std_out, std_err = repo._git_custom_command('', cmd_list) lgr.info(std_out)
# todo: this wait=false can be problematic! </s> node.cmd(args, wait=false)	bootnodeservice return for args in service.getstartup(node, services):
# todo: may test with codecs.open passing an encoding </s> with open(self.filename) as fobj:	test_2_import_from_json_fobj def test_2_import_from_json_fobj(self): table = rows.import_from_json(fobj, encoding=self.encoding) self.assert_table_equal(table, utils.table)
# todo: arrange </s> repo = self.remote.new_repo(self.token)	test_create_repo def test_create_repo(self): Test: create/edit a repo object self.assertTrue(self.remote.modify_repo(repo, "name", "testrepo0", self.token)) self.assertTrue(self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token))
# todo: see get_scale_factor() to choose 72 px on hidpi </s> if icon and icon.bind(interface.factory):	pixbuf def pixbuf(clazz, interface, icon): pixbuf = None pixbuf = getattr(icon.bind(interface.factory), 'native_%i' % clazz.icon_size()).get_pixbuf() return pixbuf
rec_dict._proxy._handle.close() #todo - better solution </s> del rec_dict	simple_check rec_dict = SeqIO.index(filename, format, alphabet) self.check_dict_methods(rec_dict, id_list, id_list) if not sqlite3: return
# todo add support for more diverse obs_spec and action_spec </s> args:	aggregate def aggregate(self, exp_list): exp_list: Returns:
# todo: passing a broker client around isn't thread-safe </s> kwargs = {'broker_client':self.broker_client}	_after_init_accepted self.singleton_server.scheduler.create_edit('create', job_data) if has_cluster_wide_singleton: Thread(target=self.singleton_server.run, kwargs=kwargs).start() self.singleton_server.start_cluster_wide_scheduler_jobs(self.connector_server_keep_alive_job_time,
#todo: allow a,c,m recovery for unknown values </s> if any([x=='unknown' for x in [a,c,m]]):	lcg_prev_states c - (int) The addend for the LCG. m - (int) The modulus for the LCG. print 'a,c,m recovery not yet implemented.' return False
# todo: attributes should be freed </s> attr = pango.pango_attr_font_features_new(features.encode('ascii'))	create_layout ('%s %i' % (key, value)) for key, value in features.items()) try: except AttributeError: LOGGER.warning(
# todo: remove seaborn dependencies </s> sns.heatmap(df, ax=ax, cbar_ax=cbar_ax)	update_pcolor df = pd.DataFrame(Z, index=y.unique(), columns=x.unique()) cbar_ax = ax.get_figure().axes[1] ax.set_xlabel(xname) ax.set_ylabel(yname)
# todo: replace </s> tuple = np.reshape(a=triples[row_nmbr, start_of_columns_to_maintain:end_of_columns_to_maintain],	_compute_metrics start_of_columns_to_maintain, end_of_columns_to_maintain = column_to_maintain_offsets for row_nmbr, row in enumerate(triples): newshape=(1, 2)) tuples = np.repeat(a=tuple, repeats=all_entities.shape[0], axis=0)
# todo: investigate why this fails </s> pass	test_field_unique def test_field_unique(self):
recording_uuid = none #todo </s> start_time_system_s = none  # todo	_recording_update_legacy_from_v1_15_to_pprf_2_0 def _recording_update_legacy_from_v1_15_to_pprf_2_0(rec_dir): info_csv = utils.read_info_csv_file(rec_dir) start_time_synced_s = None  # TODO duration_s = None  # TODO
# todo: remove dependency on legacy_examples </s> environment_config = environmentconfig.build(	test_compile def test_compile(): composition, {'solids': {'add_four': {'inputs': {'num': {'value': 1}}}}}, )
#todo: remove expressions </s> secondnote.duration.quarterlength = self.quarterlength	Trill firstNote.duration.quarterLength = self.quarterLength secondNote = copy.deepcopy(srcObject) secondNote.transpose(transposeInterval, inPlace = True) trillNotes.append(firstNote)
# todo: this type conversion seems to be bottle neck </s> features = tf.divide(tf.cast(inputs, tf.float32),	_compute_dist NN outputs probabilities of K classes :return: Categorical distribution tf.constant(255.)) features = self.conv1(features)
# todo: raise unrecognized operator error </s> print("unrecognized operator: " + op_type)	create_neon_graph print(node.name) if node.op not in known_ops: continue inputs = []
# todo: remove? </s> const.requisitionactions.request	process_transactions const.StockActions.STOCKONHAND, const.StockActions.STOCKOUT, ): balances.append(tx)
# todo: extend </s> logits, loss, predict = predictor(output, targets, options["answer_size"])	conditional_reader_model options["repr_dim_output"], drop_keep_prob=options["drop_keep_prob"]) output = tf.concat(1, [states[0][1], states[1][1]]) print('TRAINABLE VARIABLES (embeddings + model): %d' % get_total_trainable_variables()) print('ALL VARIABLES (embeddings + model): %d' % get_total_variables())
# todo: uncomment these, when #1217 is fixed </s> link = link('http:/yo/pytest-1.0-py2.py3-none-any.whl')	test_link_package_versions_match_wheel def test_link_package_versions_match_wheel(self): result = self.finder._link_package_versions(link, self.search_name) assert result == [(self.parsed_version, link, self.version)], result
# todo(ytknzw): add more specific assertion with the test case. </s> study = create_study()	test_plot_intermediate_values trial.report(2.0, step=1) return 0.0 study.optimize(lambda t: objective(t, True), n_trials=1) figure = plot_intermediate_values(study)
# todo: remove this after we create the contents web service and directories are </s> dirs = nbm.list_dirs(path)	NotebookHandler nbm = self.notebook_manager if name is None: notebooks = [] index = []
# todo: os.path.expandvars, os.path.expanduser? is not needed here, isn't it? always? </s> ds.get(expanded_list)	run for item in args.path: expanded_list.extend(glob.glob(item))
# todo: check syntax </s> return values	allow @GenericHeaderSyntax def allow(self, name, values):
# todo: should this result be 0 instead of nan? </s> result = data.compute_statistic('sum', data.id['x'], subset_state=subset_state,	test_compute_statistic_efficient assert_allclose(result, 3840) assert data.elements_accessed == 3520 view=[2]) assert_allclose(result, np.nan)
# todo: if update_atlas: introduce charts via self._atlas </s> return self._total_space	total_space field=self._field, structure='topological', start_index=sindex)
# todo: we're reading the config file twice. </s> config.read_file(io.open(config_file, "rt", encoding="utf-8"))	_load_configuration logger.info(io.open(config_file, "rt", encoding="utf-8").read()) try: except AttributeError: config.readfp(io.open(config_file, "rt", encoding="utf-8"))
# todo(amotoki): due to neutron bug 1378525, neutron disables </s> api.neutron.router_update(isa(http.httprequest), router.id,	test_router_update_post_dvr_ha_disabled "dvr", "update")\ .AndReturn(False) name=router.name, admin_state_up=router.admin_state_up)\
# xxx todo register a failure handler that reverses the local state </s> return "ok"	archive q = action.get_queue() q.enqueue(action.get_archive_fn(account), thread_id)
# todo: replace `funcname` with func.__name__? </s> if not isinstance(units, list):	_check_quantity >>> from astropy import units as u >>> _check_quantity(4*u.T, 'B', 'f', u.T) units = [units] for unit in units:
# todo: remove debug statements after fixing in-toto/in-toto#171 </s> log.debug("{0} (stdout):{1}".format(command, process.stdout))	gpg_sign_object process = in_toto.process.run(command, input=content, stdout=in_toto.process.PIPE, stderr=in_toto.process.PIPE) log.debug("{0} (stderr):{1}".format(command, process.stderr)) signature_data = process.stdout
# todo: work out a way to set this based on the timespan of the data. </s> locator = mdates.autodatelocator(minticks=5, maxticks=25)	plot axes.xaxis.grid(False, "major") axes.legend() formatter = mdates.ConciseDateFormatter(locator) axes.xaxis.set_major_locator(locator)
except exception as error:  # todo: be specific </s> self.bot.error(exception=error)	_call except KeyboardInterrupt: raise
# todo: try mlp rather than bilinear </s> self.logits_second = tf.transpose(bilinear(emb_first, emb_node, name='logits_second'), [0, 2, 1])	_init emb_first_real = tf.boolean_mask(emb_node, mask_real) emb_first_real = tf.expand_dims(emb_first_real, axis=1) self.logits_second = tf.squeeze(self.logits_second, axis=-1) ac_first_mask = tf.one_hot(ac_first, depth=tf.shape(emb_node)[1], dtype=tf.bool, on_value=False, off_value=True)
# todo: score is the negative of the distance </s> first_part = torch.mm(r_k, projected_head)	compute_scores :param projected_tail: :return: second_part = alpha_k * projected_tail score = - torch.sum(torch.abs(first_part - second_part))
return # todo </s> try:	delayed_visit_getattr def delayed_visit_getattr(self, node): definition frame = node.frame() for infered in node.expr.infer():
# todo(lbragstad): sleeping after the response status has been checked </s> time.sleep(1)	test_user_update_own_password new_pass=old_pass, old_pass=new_pass) resp = self.non_admin_client.update_user_own_password(
except exception:   # todo </s> tx_index = 0	follow assert len(txes) == 1 tx_index = txes[0]['tx_index'] + 1 while True: block_count = bitcoin.rpc('getblockcount', [])
pass  # todo: 实盘需要检查 </s> def check_pending_order(self):	check_pending_order
field = column.field.replace('#', '') # todo: is this line needed? </s> return u"m{module.id}.{detail_type}.{d.model}_{field}_{d_id}.graph.key.{key}".format(	graph_configuration @pattern('m%d.%s.%s_%s_%s.graph.key.%s') def graph_configuration(module, detail_type, column, key): module=module, detail_type=detail_type,
# todo: better default date format </s> n = len(electricity.appliances) + len(electricity.mains)	_plot_missing_sample_using_rectanges def _plot_missing_sample_using_rectanges(electricity, ax=None, fig=None): colours = [plt.cm.Blues(c) for c in np.linspace(0.3, 0.9, n)] ylabels = []
# todo: look this up in one query </s> for user_id in obj['collaborator_ids']:	get_by_mbid obj['collaborator_ids'] = playlist_collaborator_ids.get(obj['id'], []) collaborators = [] user = db_user.get(user_id) if user:
# todo add typeerror if params are given. </s> return unite(self.py__iter__())	py__next__ @register_builtin_method('__next__') def py__next__(self):
# todo: add logging in case the post fails. </s> return pass_test(agent_id, message)	run_test docker_push_and_clean_up(img, name, user)
# todo: add option to only show error runs </s> return {	index @nav.active_section('runs') def index(self): 'runs': request.db.get_runs(sort_order='DESC'),
# todo: decide on replace= behavior, see #903 </s> req.content.seek(0)	_POST_mkdir_with_children return defer.succeed(self.node.get_uri()) # TODO: urlencode name = name.decode("utf-8") kids_json = req.content.read() kids = convert_children_json(self.client.nodemaker, kids_json)
persist=false  # todo: add log persistence </s> job_uuid='all',	push experiment_uuid=self.experiment_uuid,
# todo(b/182621549): for sobol sequences, dimension should be known at graph </s> dim = tf.get_static_value(dim)	_mvnormal_quasi skip = 0 if random_type == RandomType.SOBOL: if dim is None: raise ValueError('For Sobol sequences, dimension should be known at graph'
# todo: get the real security group of launch in here </s> security_group = "default"	run_instance inst = self.instdir.get(instance_id) if not FLAGS.simple_network: net = network.BridgedNetwork.get_network_for_project(inst['user_id'], inst['project_id'],
1  # todo: fill in identifier </s> )	profile_transfer amount, target, result.wait() profiling.print_all_threads()
# todo maybe include the _request here too? </s> seq = []	_extract_top_albums def _extract_top_albums(doc, network): for node in doc.getElementsByTagName("album"): name = _extract(node, "name")
# todo: consolidate?? </s> self.event = salt.utils.event.masterevent(self.application.opts['sock_dir'])	get self.set_header('Cache-Control', 'no-cache') self.set_header('Connection', 'keep-alive') self.write(u'retry: {0}\n'.format(400)) self.flush()
# todo: remove this skip after fixing </s> if sys.version[0] == 3:	test_circle_draw @requires_application() def test_circle_draw(): raise SkipTest with TestingCanvas() as c:
# todo(termie): we should probably return not founds instead of none </s> self.assertequals(delget_resp.body, '')	test_crud_user delget_resp = c.get_user(user_id=data['id'])
pass  # todo </s> raises valueerror if there is not at least one other nick in the group.	unalias_nick def unalias_nick(self, nick, alias):
#@todo: remove in 0.4.10 </s> def error(self, reason=none, type="parse"):	error raise Fail("%s error%s | Plugin out of date" % (type.capitalize(), ':' + str(reason) if reason else "")) if self.core.debug:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: match cciss* somehow </s> return {	_get_default_options @classmethod def _get_default_options(cls): "elevator" : "", "script"   : "",
# todo: parlist, dots, block </s> self.assertequal(7, p._pos)	testFuncBodyParListOnlyDots self.assertIsNotNone(node)
# todo:   seems like they could be combined </s> repeat_record.handle_payload_exception(e)	fire_for_record payload = self.get_payload(repeat_record) except Exception as e: raise url = self.get_url(repeat_record)
# todo debug </s> print logstring	logRule + "end=%d)") % item.element.end logging.info("[%s]: %s" % (fileName, logString)) elif item.type == "minute": logString = ("%s minute " % spaceString
# todo: this is a jump. </s> vi_cmd_data['motion']['command'] = '_vi_forward_slash'	vi_n def vi_n(vi_cmd_data): vi_cmd_data['motion']['args'] = {'mode': vi_cmd_data['mode'], 'count': vi_cmd_data['count'], 'search_string': vi_cmd_data['last_buffer_search']} vi_cmd_data['count'] = 1
# todo: data could be chunked, proxy needs to handle this </s> decoded = self.decoder.decode_in(data)	handle_in_data logger.debug('Received {0} bytes from outside to proxied service: {1}'.format(len(data), hex_data)) if self.decoder: logger.debug('Decoded request: {0}'.format(decoded)) session.add_event({'request': decoded, 'raw_response': ''})
# todo docstring </s> fake_gear = fakegearclient(units={})	test_proxy_needs_adding def test_proxy_needs_adding(self): api = Deployer(create_volume_service(self), gear_client=fake_gear) expected_destination_port = 1001
# todo: decode all field to string </s> self._stats = data	_decode if 'PMIC' in data['TEMP']: del data['TEMP']['PMIC'] for observer in self._observers: observer(self._stats)
1  # todo: fill in identifier </s> )	profile_transfer amount, target, result.wait() profiling.print_all_threads()
# todo: use weight scaling factor if provided, xavier's default else </s> self.weights = sharedx(	_initialize_weights if irange is None: irange = self.irange .5 - rng.rand(nvis, self.nhid) * self.irange, name='W',
# todo stub </s> def test_assign() -> none:	test_assign pass
# todo read2 is silently discarded </s> return true	TooShortReadFilter if self.too_short_outfile is not None: read1.write(self.too_short_outfile) return False
# todo: redo using locally established collection </s> test_url = "https://github.com/bpoldrack/examplecollection.git"	test_register_collection @with_tempfile def test_register_collection(m_path): test_name = test_url.split('/')[-1].rstrip('.git') assert_equal(test_name, 'ExampleCollection')
# todo: handle fancy-index copies by allocating a buffer and </s> next_index = self._subset_iterator.next()	next def next(self): return self._raw_data[next_index]
# todo deal with cached user dict here </s> users = get_all_users_by_domain(domain, group=group, user_filter=user_filter)	download_cases key = [domain, {}, {}] cases = CommCareCase.view(view_name, startkey=key, endkey=key + [{}], reduce=False, include_docs=True) groups = Group.get_case_sharing_groups(domain) workbook = WorkBook()
# todo: fix this, this is one of the few cases where using the config </s> enabled_logs = self.config.get("show", [])	__start_job_logging else: self.__logging_handlers[handler] = [name] if ('test' in enabled_logs and 'early' not in enabled_logs):
# todo: replace with isinstance(expr, bindabletypes) </s> if not isinstance(expr, (indexexpr, memberexpr, nameexpr)):	assign_type declared_type: Type, restrict_any: bool = False) -> None: return None if not expr.literal:
1/0  # conflict resolution todo </s> elif list_a[uid] != status[uid][0]:  # item update in a	get_actions if uid in list_a and uid in list_b: if list_a[uid] != status[uid][0] and list_b[uid] != status[uid][1]: prefetch_from_a.append(uid) actions.append(('update', uid, 'a', 'b'))
# todo(dtroyer): remove tenant_id when we clean up the sdk refactor </s> 'tenant_id': self.project.id,	test_create_all_options 'shared': True, 'description': self._network.description, 'project_id': self.project.id, 'is_default': True,
# todo some complication with -1 label </s> y_ = y	test_classifiers_classes continue if name in ["LabelPropagation", "LabelSpreading"]: elif name in ["RandomForestClassifier", "ExtraTreesClassifier"]: y_ = y_str_numbers
# todo: 289 </s> if self.federated_only and len(self.known_nodes) < params['n']:	Alice value=params['value'], handpicked_ursulas=handpicked_ursulas) good_to_go = self.block_until_number_of_known_nodes_is(number_of_nodes_to_know=params['n'], learn_on_this_thread=discover_on_this_thread,
# todo: deprecated, remove in 1.5.0 </s> "roles": user._roles	_save "apikey": user._apikey, "settings": user._settings, } with atomic_write(self._userfile, "wb", permissions=0o600, max_permissions=0o666) as f:
# todo: match channel against [a-za-z0-9:._-]+ </s> raise exceptions.mpdnotimplemented  # todo	unsubscribe ``unsubscribe {NAME}`` Unsubscribe from a channel.
# todo: arrange </s> repo = self.remote.new_repo(self.token)	test_create_repo def test_create_repo(self): Test: create/edit a repo object self.assertTrue(self.remote.modify_repo(repo, "name", "testrepo0", self.token)) self.assertTrue(self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token))
# todo: remove this after we create the contents web service and directories are </s> return basename	increment_filename The URL path of the notebooks directory
# todo: remove in sopel 8 </s> if hasattr(func, 'rule') and isinstance(func.rule, basestring):	clean_callable else: func.event = [event.upper() for event in func.event] LOGGER.warning( 'The `rule` attribute of %s.%s should be a list, not a string; '
# todo need more daa </s> pass	test_license_declared_by_group def test_license_declared_by_group(augur_db_routes):
pass # todo: explain </s> pass # todo: explain	status501 def status501(self):        # Not Implemented
# todo: docstring </s> eval_name = none	__get_variable_evaluate_name def __get_variable_evaluate_name(self, pyd_var_parent, var_name): if len(pyd_var_parent) > 3: var = pyd_var_parent + (var_name,)
# todo: choose one from the following two </s> config.add_no_good_cuts = true	solve config.init_strategy = "feas_pump" config.iteration_limit = 0 config.use_tabu_list = False if config.init_strategy == "feas_pump":
except exception:  # todo: be specific </s> pass	run try: stderr(trace) logfile = open(os.path.join(config.core.logdir, 'exceptions.log'), 'a') logfile.write('Critical exception in core')
continue  # todo should we store relations? </s> new_part = block()	messages_from_raw mimepart = encoding.to_message(part) if mimepart.is_multipart(): new_part.message = new_msg new_part.walk_index = i
# todo adding and removing tracks as if this was a regular list </s> return utils.sequence(	tracks return spotify.Track( sp_track=lib.sp_playlist_track(sp_playlist, key)) sp_obj=self._sp_playlist, add_ref_func=lib.sp_playlist_add_ref,
# todo: redundancy between all gaze mappers -> might be moved to parent class </s> audio.say("stopping calibration")	stop def stop(self): logger.info("Stopping Calibration") self.smooth_pos = 0,0
# todo: use <meta charset /> </s> try:	get_encoding def get_encoding(s): import charade.universaldetector u = charade.universaldetector.UniversalDetector()
# todo is pexpect thread safe, e.g. could we be blocked on this </s> pnum = self.con.expect('notification handle = .*? \r', timeout=.5)	_expect def _expect(self, expected, timeout=DEFAULT_TIMEOUT_S): try: if pnum == 0: self._handle_notification(self.con.after)
# todo; not sure what's wrong here. possible bug? </s> @pytest.mark.xfail	test_matcher_match_zero_plus def test_matcher_match_zero_plus(matcher): words = 'He said , " some words " ...'.split()
# todo ... </s> print()	main def main(): print("Find odd numbers via method:") print("Find divisible by 6 via lambda:") print()
# todo: show in display_problems() </s> show_invalid_depstring_notice(pkg, dep_string, str(e))	_add_pkg_deps uselist=self._pkg_use_enabled(pkg)) except portage.exception.InvalidDependString as e: del e continue
# todo: this should disappear once the image package is refactored </s> from pybug.image import intensityimage, depthimage	gaussian_pyramid order=order, mode=mode, cval=cval) for j, image_data in enumerate(pyramid): if (self.__class__ is IntensityImage or self.__class__ is DepthImage):
# todo: move trusted to be autodetected inside resource </s> resource = resource(source, trusted=true, **options)	describe_resource Returns: Resource: data resource resource.infer(stats=not nostats) if expand:
'expiration': (maya.now() + datetime.timedelta(days=3)).iso8601(),  # todo </s> }	test_character_control_lifecycle 'n': 1, 'label': random_label, response = alice_control_test_client.put('/grant', data=json.dumps(alice_request_data)) assert response.status_code == 200
# todo: deprecate </s> self._update_token(token)	_send_token_update token_update.send(self, name=self.name, token=token) if callable(self._update_token):
# todo: support all tzinfo subclasses by calling utcoffset() </s> if date_time.tzinfo is not none and\	datetime_obj_to_dtstruct dt.time.offset = 0 dt.time.ok = '\1' date_time.tzinfo.__class__ is TZFixedOffset: dt.time.offset = date_time.tzinfo.offset
dataarray = datasection.data.uint64s # todo implement 32 bit </s> for i, x in enumerate(dataarray):	getObjcMethNameAddress dataSection = ds.getSection(module=executablePath, name='__DATA.__objc_selrefs') charPointerType = target.GetBasicType(lldb.eBasicTypeChar).GetPointerType() if x != fileAddr: continue
# todo: workaround, remove it when xgboost is fixes </s> kwargs['n_gpus'] = -1	test_predict_sklearn_pickle kwargs['silent'] = 0 kwargs['objective'] = 'binary:logistic' model = XGBClassifier(**kwargs) model.fit(X, y)
# todo: log exception </s> return false	delete_by_task_id return True except Exception as e:
# todo: read xml lazily? </s> content_fobj.close()	import_from_ods content_fobj = ods_file.open("content.xml") xml = content_fobj.read()  # will return bytes ods_file.close() document = xml_from_string(xml)
# todo: specific exception </s> items[-1].append("(error)")	format_results items[-1].append(ann_obj.get_ann_by_id(ann.trigger).text) except: response['items'] = items return response
# todo: check correctness </s> for _ in range(leaf.d):	insert_point setattr(parent, side, branch) self.traverse(branch, op=self._increment_depth, inc=1) if parent: parent.n += 1
# todo(twilson) we can remove this when we require ovs>=2.12.0 </s> super(metadataagentovnsbidl, self).__init__(	__init__ None, connection_string, helper, leader_only=False) except TypeError: None, connection_string, helper) if chassis:
self.last_recv = self.targets[0].recv(10000)  # todo: remove magic number (10000) </s> if self._check_data_received_each_request:	transmit_fuzz try:  # recv if self._receive_data_after_each_request: self._fuzz_data_logger.log_check("Verify some data was received from the target.") if not self.last_recv:
# todo: candidate for move to system/hdparm </s> out, err, rc = run_command(	get_disk_power_status :return: single word sting of state as indicated by hdparm -C /dev/<disk> and if we encounter an error line in the output we return unknown. [HDPARM, '-C', '-q', '/dev/disk/by-id/%s' % dev_byid], throw=False) if len(err) != 1:
# todo: consider un-hardcoding this and plumbing pool_maxsize to requests.adapters.httpadapter. </s> self.thread_count = 1 if client == "offline" else 4	set_options verify=False if insecure else True, ) self.global_threads = ThreadPoolExecutor(max_workers=self.thread_count) self.log.debug("working with %s thread(s)" % self.thread_count)
#todo(chris): implement service_catalog </s> self.service_catalog = none	authenticate ["nova"][0]["publicURL"] self.auth_token = body["auth"]["token"]["id"]
# todo: separate agent model. </s> return self.eval_collector.collect_evaluation(itr)	evaluate_agent def evaluate_agent(self, itr):
# todo: this needs refactoring </s> if args.metadata_table:  # pragma: no cover	args_download assert args.refseq_category in EDefaults.REFSEQ_CATEGORIES.choices, \ "Unsupported refseq_category: {}".format(args.refseq_category) logging.info('Creating metadata file: %r', args.metadata_table) with open(args.metadata_table, 'wt') as metadata_table:
# todo(lyarwood): test drivervolumeblockdevice.driver_detach in </s> conn_info_str = '{"connector": {"host": "other-host"}}'	test_detach_volume_evacuate_mismatch For evacuate, if the stashed connector also has the wrong host, then log it and stay with the local connector. self._test_detach_volume_evacuate(conn_info_str)
# todo: errors </s> con = sc.connect_to_service_finish(results)	rest_post_connected def rest_post_connected(self, sc, results, command, data, callback, error_callback, callback_data): if con == None: return
# todo: group by storage type also? </s> storage_units_by_type.setdefault(stype, {}).setdefault(ptype, []).append(make_storage_unit(su))	get_data su.storage_mapping.match.metadata['instrument']['name'] ptype = su.storage_mapping.match.metadata['product_type'] if (len(storage_units_by_type)): raise RuntimeError('Data must come from a single storage')
return none # todo </s> def _currentsong(self):	_currentsong
# todo: implement </s> pass	_copyprotection def _copyprotection(self, arg):
# todo: make sure 'feature' contains only capitals and underlines </s> re_str = feature + r"\s*=.*;"	set_enable print(type(enables)) for feature in enables: target_str = feature + " = true;" print(feature)
# todo: factor out the logging? </s> log.msg('scheduling state write.')	eventually_write def eventually_write(): def actually_write(): log.msg('Performing state write...')
pass # todo </s> def try_undo(self, *args):	try_undo
# todo: action value doesn't exist for beta </s> self.unittest(	test_early_horizon_estimate baseline_objective = 'policy_gradient' baseline_optimizer = 'adam' exclude_bounded_action=True, reward_estimation=reward_estimation, baseline_objective=baseline_objective, baseline_optimizer=baseline_optimizer
# todo: write this method if possible </s> pass	address_transactions def address_transactions(self, addresslist):
# todo: winexe calls hang and the test fails by timing out. the same </s> self.override_profile_config(	test_win2012r2_winexe Tests creating and deleting a Windows 2012r2instance on EC2 using winexe (classic) 'ec2-win2012-test', {
raise notimplementederror # the below does most probably not work anymore todo </s> curbranch = self.git.current_branch()	merge - :meth:`gather` -- creates a new branch to merge into rather than merging into the current branch. if ticket == "dependencies": for dep in self._dependencies[curbranch]:
# todo add in/out fifo contributions </s> p = self.get_nodeattr("pe")	lut_estimation Y. Umuroglu, M. Leeser and K. Vissers - 12. Sep 2018 Q = self.get_nodeattr("SIMD") wdt = self.get_weight_datatype()
expr,  # todo rethink this circular import </s> )	lll_for_external_call def lll_for_external_call(stmt_expr, context): from vyper.old_codegen.expr import ( pos = getpos(stmt_expr) value, gas = get_gas_and_value(stmt_expr, context)
for people in annos:  # todo : speed up with affine transform </s> new_keypoints = []	keypoint_random_crop2 mask = mask[crop_range_y:crop_range_y + _target_width, :] new_joints = [] for keypoints in people: if keypoints[0] < 0 or keypoints[1] < 0:
# todo: remove this backward compatibility code (in_proj_weight) </s> prefix = name + '.' if name != '' else ''	upgrade_state_dict_named def upgrade_state_dict_named(self, state_dict, name): items_to_add = {} keys_to_remove = []
pass #todo: show multi select menu </s> elif len(game.main.session.selected_instances) == 1 and hasattr(game.main.session.selected_instances[0], 'show_menu'):	mouseReleased if evt.getButton() == fife.MouseEvent.LEFT and hasattr(self, 'select_begin'): if len(game.main.session.selected_instances) > 1: game.main.session.selected_instances[0].show_menu() del self.select_begin
# todo: waffle here </s> index_documents.delay([instance.id])	update_document_index return from wiki.tasks import index_documents
# todo: move this into demo/slow-completion.sh </s> if 1:	OshMain comp_builtins.Complete(['-E', '-A', 'command'], ex, comp_lookup) comp_builtins.Complete(['-D', '-A', 'file'], ex, comp_lookup) A1 = completion.WordsAction(['foo.py', 'foo', 'bar.py']) A2 = completion.WordsAction(['m%d' % i for i in range(5)], delay=0.1)
# todo: check error location </s> assert result.errors[0].message == 'cannot return null for non-nullable type.'	test_nulls_the_top_level_if_sync_non_nullable_field_returns_null assert result.data is None assert len(result.errors) == 1
# fixme: todo </s> return results	execute_query result.append(value) results.append(ResultRow(*result))
# todo: consult best practices for python and twisted logging. </s> logging.basicconfig(level=logging.info)	main def main(argv=sys.argv, _abort_for_test=False): log.startLoggingWithObserver(log.PythonLoggingObserver(loggerName='shinysdr').emit, False) argParser = argparse.ArgumentParser(prog=argv[0])
# todo: handle errors better </s> abort(	patch except sqlalchemy.exc.IntegrityError as e: db.session.rollback() code=http_exceptions.Conflict.code, message="Could not update user details."
# todo : look for alive and killing </s> while true:	do_main self.data_thread = threading.Thread(None, self.manage_brok_thread, 'datathread') self.data_thread.start() input = [srv.socket] inputready,_,_ = select.select(input,[],[], 1)
#todo: write log file. </s> def parse_topic(self):	ZhihuTopic if level.value >= self.debug_level.value: print(log_str) try: response = requests.get(self.url, headers = my_header)
# todo: this is a hack to make a rule know </s> if slot.value == "none" and slot.as_feature():	get_parsing_states for key, slot in tracker.slots.items(): if slot is not None: slot_id = f"slot_{key}_None" state_dict[slot_id] = 1
# todo: check the crs </s> try:	make_mask_seg if not Path(image_file).is_file(): raise ValueError('file {} not exits.'.format(image_file)) label_src = rasterio.open(label_file) label_flag = 'raster'
# todo: use sqlalchemy objects to do this </s> sql = "select c.id from notes as n inner join cards as c on c.nid = n.id";	latest_cards def latest_cards(self, col, req): args = [] if req.data.has_key('updated_since'):
# temporary hack to get pf api working. todo - remove </s> try:	get_user_activity_report def get_user_activity_report(request, ids, index, value, start_date, end_date, params): domain = Domain.objects.get(name='Pathfinder') except Domain.DoesNotExist:
@unittest.skip('not written')  # todo: finish! </s> self.assert_shortened([value], "[b'aaaaaaaaaaaaaaaaaa...aaaaaaaaa']")	test_bytes_large "b'" + 'A' * 43688 + "..." + 'A' * 21844 + "'")
# todo should we pass? </s> pass	WeatherCache else: _log.debug("In WeatherCache: Invalid request type: ?", request_type) self.trim_period = cache_period try:
# todo: find a better solution </s> if platform == "win":	_setupUI def _setupUI(self): super(RevHmContrib, self)._setupUI() default_size = QApplication.font().pointSize() for label in [self.form.fmtLabContrib]:
# todo (jconnor 2013-01-22) make this configurable </s> easy_handle.setopt(pycurl.ssl_verifypeer, default_ssl_verify_peer)	_build_easy_handle def _build_easy_handle(self): easy_handle = super(self.__class__, self)._build_easy_handle() self._add_ssl_ca_cert(easy_handle) self._add_ssl_client_cert(easy_handle)
# todo: test with unicode strings and non-ascii chars </s> message = "abcdefghijklmnopqrstuvwxyz"	test_enc_dec def test_enc_dec(self): print "\tMessage:   %s" % message encrypted = rsa.encrypt(message, self.pub)
# todo remove backwards compatability fix in a future version </s> if build_version < 11000:	plugin_loaded preferences = sublime.load_settings('Preferences.sublime-settings') build_version = int(preferences.get('neovintageous_build_version', 0)) preferences.set('neovintageous_build_version', 11000) preferences.set('vintageous_use_ctrl_keys', preferences.get('vintageous_use_ctrl_keys'))
# todo(pebaz): contents.extend([search down the '.'s!!]) </s> if module_file in contents:	find_module for search_path in [Path(i) for i in (path + sys.path + ['.']) if Path(i).is_dir()]: contents = [i.name for i in search_path.iterdir()] print('Found it in', search_path, module, module_file) module_path = search_path / module_file
# todo handle color, scatter, etc </s> if len(kind) != len(data.columns):	plot return f.show(**show_args) elif isinstance(kind, list): raise LanternException('Must specify type for each column') for i, k in enumerate(kind):
# todo(haoyuzhang): understand slowdown of setting learning phase when </s> tf.keras.backend.set_learning_phase(1)	run if flags_obj.skip_eval: if flags_obj.set_learning_phase_to_train: num_eval_steps = None validation_data = None
# todo implement this </s> return notimplementederror	get_app_path if self.app_path is not None: return self.app_path from droidbot import DroidBot out_dir = DroidBot.get_instance().options.output_dir
# todo(mierdin): note that this will always return true if rbac is not enabled </s> if rbac_utils.user_has_role(requester_user, role):	put LOG.info("Checking user %s is in role %s" % (requester_user, role)) LOG.info(rbac_utils.user_has_role(requester_user, role)) break else:
# todo: check complex data types possible for series for dataframes set column here </s> c = numba.pythonapi._boxcontext(context, builder, pyapi, env_manager)	codegen if context.enable_nrt: context.nrt.incref(builder, arr, arr_arg) py_arr = hpat.hiframes.boxing._box_series_data(arr.dtype, arr, arr_arg, c) cstr = context.insert_const_string(builder.module, col_name)
#todo : multi parent intelligence </s> id = par.get_id()	add_parent def add_parent(self, par): self.parents.append(id)
pass  # todo: replace this </s> try:	__eq__ that = eval(f'other.{attribute}') except Exception as exc_that: if this != that: return False
# todo(rbharath): this should be modified to contain a cluster split so </s> splitters = {	load_pdbbind dataset = deepchem.data.DiskDataset.from_numpy(features, labels) transformers = [] 'index': deepchem.splits.IndexSplitter(), 'random': deepchem.splits.RandomSplitter(),
# todo(b/186451541): reduce the number of calls to model_fn. </s> self.assertequal(mock_model_fn.call_count, 3)	test_construction_calls_model_fn p13n_eval.build_personalization_eval( mock_model_fn, p13n_fn_dict, _evaluate_fn, max_num_clients=1)
# todo(benjy): some more elegant way to coordinate how tasks claim targets. </s> interpreter = self.select_interpreter_for_targets(self.context.targets())	execute binary = target_roots[0] if isinstance(binary, PythonBinary): with self.temporary_pex_builder(interpreter=interpreter, pex_info=binary.pexinfo) as builder: chroot = PythonChroot(
# todo: this procedure would leave a clean dataset, but `run` cannot handle dirty </s> ds.add('code', to_git=true)	test_basics def test_basics(path): ds = Dataset(path).create(force=True) ds.add('.') ds.run_procedure('setup_yoda_dataset')
# todo: give users the ability to specify this via their profile </s> return self.fullname.split(' ')[-1]	surname appear in print. e.g.: "Jeffrey Spies" would be "Spies".
# todo: some type checking that a is invertible hence a valid key </s> i = self(self.inverse_key(a))	HillCryptosystem raise TypeError, "Argument M = %s does not encode in the cipher domain" % M def deciphering(self,A,C): return i(C) def enciphering(self,A,M):
#todo: milestones = get_milestones(target_url) </s> milestones = []	import_all_open_issues labels = get_labels(source_url) import_labels(labels) labels = get_labels(target_url) issues = get_issues(source_url)
#@todo: remove in 0.4.10 </s> def error(self, reason=none, type="parse"):	error raise Fail("%s error%s | Plugin out of date" % (type.capitalize(), ':' + str(reason) if reason else "")) if self.core.debug:
#  todo: test </s> ship.getmodifieditemattr("shipbonusics2"),	handler "maxGroupOnline",
pass  # todo </s> def get_xl_sheet(xl_workbook, sheet_name_or_index):	get_xl_sheet
# todo 2.0: do not delete them, only set is_deleted on parent </s> attachments = self.attached_items	remove_attachments def remove_attachments(self): Send 'deleted' signal to all attachments/folders if attachments: for folder in attachments['folders']:
pass # todo </s> def _playlistinfo(self, songpos=none, start=none, end=none):	_playlistinfo @register(r'^playlistinfo( ((?P<songpos>\d+)|(?P<start>\d+):(?P<end>\d+)*))*$')
## todo: # fixme: remove me </s> try:	analyse tld = url_parsed['tld'] if tld is not None: tld = tld.decode() except:
# todo: if worker does not execute preprocessing, next state is not preprocessed here. </s> self.agent.observe(	_observe def _observe(self, env_ids, states, actions, rewards, next_states, terminals): preprocessed_states=states, actions=actions, internals=[], rewards=rewards, next_states=next_states,
report_config = {}  # todo port to fooddata.from_request </s> report_config.update(	report_config @property def report_config(self): gap_type=self.request.GET.get('gap_type') or '', recall_status=self.request.GET.get('recall_status') or '',
# todo: how to handle not found authorname </s> result = db.session.query(db.authors).filter(db.authors.sort == auth.lstrip().strip()).first()	render_edit_book error = False for auth in sort_authors: if not result: error = True
except exception:  # todo: be specific </s> pass	search_urls try: url = iri_to_uri(url) if url not in seen: seen.add(url)
# todo implement this effectively </s> print('run formatter')	run_linter print('run linter')
if isinstance(err, asyncio.cancellederror):  # todo: remove when updated to 3.8 </s> raise	Ledger resolved, _, _ = await self.claim_search([], claim_ids=claim_ids) except Exception as err: log.exception("Resolve failed while looking up purchased claim ids:") resolved = []
# todo in python 2.7 and later, this should be </s> attributes = dict((k, (v() if callable(v) else v))	DefaultSerializer attributes = dict((column, getattr(instance, column)) for column in columns) for k, v in attributes.items())
raise notimplementederror #todo </s> elif self.action == "prepend":	Action actorselection.append(e) if self.action == "EDIT" or self.action == "ADD": raise NotImplementedError #TODO elif self.action == "APPEND":
# todo: move texture export to individual formats? this is practically smurf </s> if ioutils.textureexportenabled():	ExportModelOperator "ERROR", "ExportModelOperator") print(sys.exc_info()[0]) texture_path = '' for materialname in model['materials']:
# todo: the following skipped suite and fixtures should be enabled </s> @pytest.fixture(autouse=true)	skip_suite def skip_suite(self, request): if request.node.get_marker('ext_suite_1'):
# todo: import that elsewhere </s> from . import _control	shareConstant @ensureAtomicity def shareConstant(**kwargs): sendVariable = _control.execQueue.socket.sendVariable for key, value in kwargs.items():
# assert wrapped.type == 'multipolygon' todo: same as above </s> wrap = geometry.polygon([(14455356.755667, -5559752.598333),	test_wrap_dateline assert wrapped.type == 'Polygon' wrapped = wrap.to_crs(geog_crs, wrapdateline=True) (14455356.755667, -4447802.078667), (15567307.275333, -4447802.078667),
# todo(b/144127474): remove this manual cleanup once tf.wrap_function(...) </s> resources = []	_fn_to_return param_elements.append(param_fn(arg_part)) result_parts = wrapped_fn(*param_elements) for op in wrapped_fn.graph.get_operations(): if op.type == 'VarHandleOp':
# todo(jjma): find a better way of describing this error to user. </s> with self.assertraisesregexp(valueerror,	testSpanWidth name='s1', pattern='span{SPAN:2}/split1/*') ] 'Cannot find matching for split'): utils.calculate_splits_fingerprint_span_and_version(
# todo: log </s> print 'some exception'	_disbatch_local break except: print sys.exc_info() self.ret.append(chunk_ret)
# todo this needs to be done on the content but seems to be a non-trival </s> if context.verbose:	pipe_fetchpage content = unicode(request.read(), request.headers['content-type'].split('charset=')[-1]) print "............FetchPage: content ................." print content.encode("utf-8")
# todo: look in other supported bumpversion config locations </s> bumpversion = none	load @classmethod def load(cls, latest_version): bumpversion_config_path = Path('.bumpversion.cfg') if not bumpversion_config_path.exists():
await self._stream.reset()  # todo: specify error code </s> async def __aenter__(self):	Stream self._ended = True async def reset(self): return self async def __aexit__(self, exc_type, exc_val, exc_tb):
# todo: remove at some point </s> elif key.upper() in self.groups:	__getattr__ if key in self.groups: return self.groups[key] return self.groups[key.upper()] raise NoGroupError('No such group: {0}'.format(key))
# todo: remove in v.0.6 </s> x = np.array([[0, 0], [0, 1], [2, 0], [2, 1]])	TestLSML self.assertLess(csep, 0.8)  # it's pretty terrible def test_deprecation_num_labeled(self): y = np.array([1, 0, 1, 0]) lsml_supervised = LSML_Supervised(num_labeled=np.inf)
# todo: implement me </s> def _test_jit(self):	TestTverskyLoss loss = criterion(logits, labels) assert pytest.approx(loss.item(), 0.0) pass def _test_gradcheck(self):
raise mpdnotimplemented # todo </s> def sticker_find(self, type, uri, name):	sticker_find @register(r'^sticker find "(?P<type>[^"]+)" "(?P<uri>[^"]+)" "(?P<name>[^"]+)"$')
# todo: refactor, move to utils </s> form_data = self.get_form_data(form)	send_mail def send_mail(self, form, files=[]): message = self.compile_message(form_data) context_dict = self.get_form_data_context(form_data)
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_metadata_has_unique_style_weight_pairs_success def test_result_metadata_has_unique_style_weight_pairs_success(self): self.assertInSuccess('test_metadata_has_unique_style_weight_pairs', run_set(p, 'result'))
# todo check </s> return self.mimetype	format @interfacedoc def format(self):
# todo(sbauza): the current placement noauthmiddleware returns a 401 </s> return self._client.delete(	_fake_delete def _fake_delete(self, *args): (url,) = args[1:] url, endpoint_override="http://127.0.0.1:%s" % self.service.port,
# todo: rewrite tests </s> pass	test_resend_confirmation_get def test_resend_confirmation_get(self):
# todo: may use sys.stdout.encoding if output_file = '-' </s> output_encoding = output_encoding or sys.stdout.encoding or \	query_ def query_(input_encoding, output_encoding, input_locale, output_locale, verify_ssl, fields, source, query, destination): DEFAULT_OUTPUT_ENCODING if not query.lower().startswith('select'):
# todo: the one_hot=true is only necessary because one_hot=false is </s> self.train = mnist(which_set = 'train', one_hot=true)	setUp def setUp(self): skip_if_no_data() self.test = MNIST(which_set = 'test', one_hot=True)
# todo(slaweq): change this to neutron floating ips and turn neutron </s> mock_nova.floating_ips.list.return_value = [fake_floating_ip]	test_create_server_wait_server_error mock_nova.servers.get.return_value = build_server mock_nova.servers.list.side_effect = [[build_server], [error_server]] self.assertRaises( exc.OpenStackCloudException,
# todo: remove next if due specific hack </s> if not ext_properties:	find_target_dependency_packages for ext_property_node in ext_property_nodes: ext_properties.append(ext_property_node.get('Name')) ext_property_nodes = targets_file['tree']\ .xpath('//ns:PropertyGroup'
raise notimplementederror # todo </s> def generate_obs(self):	generate_obs
# todo: should be injected </s> facade = awsfacade()	RegionsConfig self._child_config_type = child_config_type async def fetch_all(self, credentials, regions=None, partition_name='aws', targets=None): for region in await facade.build_region_list(self._service, regions, partition_name): child = self._child_config_type()
# todo: support non-numericals like string </s> gen_nan_func = lambda a: np.full(len(a), np.nan)	_handle_concat_df all_colnames.extend(self._get_df_col_names(df)) all_colnames = sorted(set(all_colnames)) arg_names = ", ".join(['in{}'.format(i) for i in range(len(df_list))]) func_text = "def _concat_imp({}):\n".format(arg_names)
# todo(jflesch): update keyword index </s> self.__main_win.refresh_doc_list()	toggle_cb self.__main_win.refresh_label_list()
# todo: use utils.serialize_user </s> user = user.load(doc['id'])	_search_contributor users = [] for doc in docs: users.append({ 'fullname': doc['user'],
# todo: replace all of this string templating with a function that accepts </s> monkey_cmdline = (	upgrade opts.parent, opts.tunnel, opts.server, opts.depth ) MONKEY_CMDLINE_WINDOWS % {"monkey_path": WormConfiguration.dropper_target_path_win_64} + monkey_options
# todo: arrange </s> repo = self.remote.new_repo(self.token)	test_create_repo def test_create_repo(self): Test: create/edit a repo object self.assertTrue(self.remote.modify_repo(repo, "name", "testrepo0", self.token)) self.assertTrue(self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token))
# todo: get actual error message </s> raise _error('empty ec2subnetids')	_add_instances Ec2SubnetIds = Instances.pop('Ec2SubnetIds') if not Ec2SubnetIds: for subnet_id in Ec2SubnetIds: _validate_param_type(subnet_id, string_types)
# todo add assertions </s> p = poll.objects.get()	test_update self.poll.question = "yours" self.poll.save() self.assertEqual(p.question, "yours")
# todo(ralexstokes) look at better way to handle once we have fork choice in place </s> except finalizedheadnotfound:	BeaconChainSyncer try: finalized_head = await self.chain_db.coro_get_finalized_head(BeaconBlock) return best_peer if best_peer.head_slot <= finalized_head.slot:
# todo add options to maodify the sorted by key and the header options </s> matrix = sorted(matrix, key=lambda endpoint: endpoint[1])	do_remove endpoint.endpoint_data['ipv6']]) if len(matrix) > 0: matrix.insert(0, ['Name', 'MAC Address', 'Segment', 'Port', 'VLAN', 'IPv4', 'IPv6'])
# todo: remove in v8 </s> self._global_context['blog_desc'] = self.config.get('blog_description')	__init__ self._GLOBAL_CONTEXT['blog_title'] = self.config.get('BLOG_TITLE') self._GLOBAL_CONTEXT['blog_description'] = self.config.get('BLOG_DESCRIPTION') self._GLOBAL_CONTEXT['blog_url'] = self.config.get('SITE_URL', self.config.get('BLOG_URL')) self._GLOBAL_CONTEXT['body_end'] = self.config.get('BODY_END')
# todo: validate </s> assert (decoded['short_message'] == "error : test log")	test_setFormatter def test_setFormatter(formatted_logger): formatted_logger.error("test log")
# todo: optionally enable multiple selection </s> self.detailedlist.allowsmultipleselection = false	create self.detailedlist._impl = self self.detailedlist.columnAutoresizingStyle = NSTableViewUniformColumnAutoresizingStyle self.native.detailedlist = self.detailedlist column = NSTableColumn.alloc().initWithIdentifier('data')
# todo: should make sure that all the shapes conform here, </s> max_arg = adverb_helpers.max_rank_arg(args)	transform_TiledMap args = expr.args axis = syntax_helpers.unwrap_constant(expr.axis) niters = self.shape(max_arg, axis) tile_size = self.index(self.tile_param_array, self.nesting_idx)
"name": drug,  # todo: case_name? </s> "owner_id": "-",	get_drug_resistance_case_properties for drug in resistant_drugs: properties = { "sensitivity": "resistant", "drug_id": drug,
# todo: perform appropriate postgres1 conversion between python datetime/mxdatetime </s> return value	convert_bind_param def convert_bind_param(self, value):
# todo use bincount! </s> return self.degenerates().any()	has_degenerates def has_degenerates(self):
'label': {}, # todo fill with a localized key </s> 'presentation_order': 0,	get_dummy_fieldoption_list { 'id': 'feddad', 'score_points': 97.5, 'trigger_field': '',
# todo: error if row_identifier is none </s> event_identifier = identifiers.sqltableidentifier(	MergeAttributeContainers row_identifier = getattr( attribute_container, '_event_row_identifier', None) self._CONTAINER_TYPE_EVENT, row_identifier) attribute_container.SetEventIdentifier(event_identifier)
# todo: configurable timeout??? </s> syndic_dict['dead_until'] = time.time() + 60	_fire_master except SaltClientError: log.error('Unable to fire event to {0}, trying another...'.format(master)) log.critical('Unable to fire event on ANY master')
#todo add server-side input validation here (currently validated on client) </s> return commtrackactionconfig(**action)	mk_action action['name'] = make_action_name(action['caption'], payload['actions'])
# todo check if result is in scope -> no evaluation necessary </s> n = check_flow_information(evaluator, flow_scope,	_names_to_types flow_scope = self.name_str.parent.parent while flow_scope: self.name_str, self.position) if n:
pass # todo </s> shows a list of available song metadata.	_tagtypes ``tagtypes``
# todo(twd2): improve here: </s> mdoc['sendee_udoc']['gravatar_url'] = (	HomeMessagesView template.gravatar_url(mdoc['sender_udoc']['gravatar'] or None)) mdoc['sendee_udoc'] = udoc template.gravatar_url(mdoc['sendee_udoc']['gravatar'] or None)) self.json_or_redirect(self.referer_or_main, mdoc=mdoc)
gc.collect()  # todo: see first comment above </s> ok_(ds2 is not none)	test_Dataset_flyweight ok_(ds4.repo is ds1.repo) del ds1 ok_(ds2.repo is ds3.repo) if not on_windows:
# todo ... </s> data = sorted()	userLongDescription for key in mainKeys: data[key] = data.get(key, "").strip()
return 0  # todo </s> def runningtaskcount(self):	runningTaskCount @pyqtProperty(int, notify = statPolled)
# todo: timeline is global, get rid of it </s> for post in timeline:	gen_task_render_posts default_lang for lang in kw["translations"]: source = post.source_path dest = post.base_path
# todo: that’s a memory leak </s> return ffi.new('char[]', bytestring), bytestring	unicode_to_char_p The byte string must live at least as long as the pointer is used. bytestring = string.encode('utf8').replace(b'\x00', b'')
# todo: try/catch </s> m = importlib.import_module(module_name)	class_for_name def class_for_name(module_name, class_name): c = getattr(m, class_name) return c
# todo(jflesch): python 3 problem </s> import cairo	image2surface Convert a PIL image into a Cairo surface
# todo: log exception </s> continue	scan data[row[0]] = row[1] except Exception as e: if data: results.append((fname, data))
#todo fixme: we should provide an option to create the page </s> continue	main if not item.exists(): pywikibot.output('%s doesn\'t have a wikidata item :(' % page) for claim in real_claims: pywikibot.output('Adding %s --> %s' % (claim.getID(), claim.getTarget().getID()))
# todo: double-check dates </s> current_period = stakeholder.staking_agent.get_current_period()	stake password=password, worker_address=worker_address) bounded_date = datetime_at_period(period=current_period) min_worker_periods = STAKEHOLDER.staking_agent.staking_parameters()[7]
pass # todo </s> def test_recv(socket, conn):	test_recv
# todo: if compilation failed, we can't proceed; handle this. </s> '-outputdir', module_inspector_obj_dir])	run '-o', MODULE_INSPECTOR_EXE_PATH,
# todo: test with/without collection_dir and with/without df </s> corpus = {'a.txt': 'lorem sit amet', 'b.txt': 'lorem ipsum'}	test_compute_pairwise_sim_one_corpus def test_compute_pairwise_sim_one_corpus(tmp_path): corpus_dir = create_corpus(corpus, tmp_path) corpus_df, _ = create_df(corpus_dir, tmp_path)
# todo make it callable </s> def loss(self, scores, labels=none):	KgeLoss else: raise ValueError("train.loss") raise NotImplementedError()
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
# todo(b/131363314): the reference executor should support generating </s> self.skiptest('b/131363314')	test_produce_and_consume_infinite_tf_dataset def test_produce_and_consume_infinite_tf_dataset(self): @tff.tf_computation(tff.SequenceType(tf.int64)) def consume(ds):
# todo: fix this for opengl core </s> gl.glpushattrib(gl.gl_enable_bit | gl.gl_current_bit)	ImageMouseCursor self.hot_y = hot_y def draw(self, x, y): gl.glColor4f(1, 1, 1, 1) gl.glEnable(gl.GL_BLEND)
# todo: may use sys.stdout.encoding if output_file = '-' </s> output_encoding = output_encoding or sys.stdout.encoding or \	query_ def query_(input_encoding, output_encoding, input_locale, output_locale, verify_ssl, fields, source, query, destination): DEFAULT_OUTPUT_ENCODING if not query.lower().startswith('select'):
# todo: allow child connections </s> self.queue.put(slskmessages.childdepth(0))	send_have_no_parent self.has_parent = False self.queue.put(slskmessages.AcceptChildren(0)) self.queue.put(slskmessages.HaveNoParent(1)) self.queue.put(slskmessages.BranchRoot(self.config.sections["server"]["login"]))
# todo: remove when no longer checking for alias. </s> def get_existing_aliases():	get_existing_aliases files_to_alias = {} for basename in ALIAS_FILES:
# todo(guillermooo): use tokens to identify requests:file. </s> if errors.file != v.file_name():	display_error _logger.debug('no errors - aborting') return _logger.debug('different view active - aborting') return
# todo: not tested against real openmrs instance </s> return should(	get_how_to_update_person_attribute def get_how_to_update_person_attribute(person_uuid, attribute_uuid, value): 'POST', url('/person/{person_uuid}/attribute/{attribute_uuid}',
# todo: the button should be styled within inputspecs </s> self.inputspecs.color_design_button("ok")	updateAll - Update all plot widgets via the signal sigFilterDesigned if self.DEBUG: print("input_widgets.updateAll:\n",self.sender().objectName()) self.inputSpecs.load_all_specs() self.inputInfo.showInfo()
# todo: total hack below. implement more principled formatting. </s> def process(line):	process if not line.startswith('#'): return '    ' + line
# todo: unit tests </s> user = auth.user	get_all_projects_smart_folder @must_be_logged_in def get_all_projects_smart_folder(auth, **kwargs): contributed = user.node__contributed nodes = contributed.find(
# todo! don't use tostring(not unique for large arrays) </s> return not self.__eq__(other)	__ne__ def __ne__(self, other):
# todo(sdake) the parameters to delete operations are highly suspect </s> service=self.fake_service)	test_service_create version='1.0',
# todo: test coverage of this branch </s> my_initial = none	update_subscription my_initial = {'user_activation_code': activation_code} else: if request.POST: form = UpdateForm(
# todo(mordred) when this changes to rest, force interface=admin </s> if self.cloud_config.get_api_version('identity').startswith('2'):	create_service type_ = kwargs.pop('type', None) service_type = kwargs.pop('service_type', None) kwargs['service_type'] = type_ or service_type else:
# todo: change this to use assertsetequal: </s> self.assertequal(true, all(updateddoc[k] == originaldoc[k] for k in updateddoc.keys()	test_field_update self.assertEqual(len(updatedDoc.keys()), len(originalDoc.keys())) self.assertEqual(updatedDoc['popularity'], originalDoc['popularity'] + 5) if k not in ['_version_', 'popularity'])) self.solr.add([
raise exceptions.mpdnotimplemented  # todo </s> same as prio, but address the songs with their id.	prioid ``prioid {PRIORITY} {ID...}``
# todo: find a way to not depend on a specific font </s> line = make_text(string, 120, 'font-family: "arial"; font-size: 19px')	test_line_breaking text1, text2 = line.split_first_line() assert text2 == u'is a text for test' text1, text2 = line.split_first_line() assert text2 == u'text for test'
# todo non-spatial features, policy </s> self.screen = cnnblock(*screen_channels)	__init__ def __init__(self, screen_channels, minimap_channels): super().__init__() self.minimap = CNNBlock(*minimap_channels) self.fc1 = nn.Linear((32 + 32) * 5 * 5, 256)
# todo: go to "blazemeter" section for these settings by default? </s> self.client.address = self.settings.get("address", self.client.address)	_configure_client def _configure_client(self): self.client.logger_limit = self.settings.get("request-logging-limit", self.client.logger_limit) self.client.token = self.settings.get("token", self.client.token) self.client.timeout = dehumanize_time(self.settings.get("timeout", self.client.timeout))
#todo(cp16net): need to set the return code correctly </s> return wsgi.result(views.instancesview(servers).data(), 201)	index def index(self, req, tenant_id): servers = models.Instances(req.headers["X-Auth-Token"]).data()
# todo: jrk: chunking times points needs to be simplified </s> n_jobs = min(len(self.y_pred_[0][0]), check_n_jobs(self.n_jobs))	_GeneralizationAcrossTime y = np.array(y) self.y_true_ = y  # to be compared with y_pred for scoring parallel, p_time_gen, n_jobs = parallel_func(_score_slices, n_jobs) n_estimators = len(self.train_times_['slices'])
#todo: add the whole build log? it could be several thousand </s> jrf.addtest(bid, package.installed, buildlogpath + '\n' +	testinstall with open(buildLogPath, 'rb') as F: buildLog = F.read() #TODO: this may not return all output spec.to_yaml() + buildLog) with open(args.output, 'wb') as F:
# todo: we are losing information about tables which are views here </s> self.db_tables = [table_info.name for table_info in self.db_tables]	SQLDiff self.db_tables = self.introspection.get_table_list(self.cursor) if django.VERSION[:2] >= (1, 8): self.differences = [] self.unknown_db_fields = {}
# todo: since multiclass is done internally - we need to check </s> if not self.clf.mclf is none:	_call def _call(self, dataset): anal = self.clf.mclf.getSensitivityAnalyzer()
# todo assuming one_hot as default for now </s> one_hot = numpy.zeros((data_y.shape[0], 10), dtype = config.floatx)	design_matrix_view data_x = numpy.transpose(data_x, axes = [3, 2, 0, 1]) data_x = data_x.reshape((data_x.shape[0], 32 * 32 * 3)) for i in xrange(data_y.shape[0]): one_hot[i, data_y[i] - 1] = 1.
# # todo: should be able to just access the api from qml. </s> @pyqtslot(result = str)	DiscoverUM3Action if self._network_plugin: self._network_plugin.refreshConnections() def getLastManualEntryKey(self) -> str:
# todo: save state right here </s> [returnfromkernel(kernel_name=new_kernel_name)])	inner_mapper [CallKernel(kernel_name=new_kernel_name)] + current_chunk + else: new_schedule.extend(current_chunk)
#todo: handle arc segments </s> block = []	trochoid def trochoid(self, segment, step, radius, cw=True): if cw: u = 1
# todo: these should always be unicodes </s> jdk_home=text_type(self._zinc.underlying_dist.home),	_runtool timeout_seconds=15*60, description='run {} for {}'.format(tool_name, tgt), ) res = self.context.execute_process_synchronously_without_raising(
# todo: make more reliable, modularize </s> try:	gradient_reverse if(hasattr(layer, "activation") and layer.activation == keras.activations.relu): reversed_Ys = keras.layers.Activation("relu")(reversed_Ys) except (TypeError, AttributeError):
# todo: this test requires manifold access, see: t88318502 </s> class testtracing(unittest.testcase):	TestTracing def testMaskRCNNFPN(self): def inference_func(model, image):
# todo: find why this doesn't work with this set to 0 </s> memory_saving_gradients.min_checkpoint_node_size = 100	test_memory_method_saves_memory def test_memory_method_saves_memory(): old_gradients = tf.gradients def gradients_memory(ys, xs, grad_ys=None, **kwargs): return memory_saving_gradients.gradients(ys, xs, grad_ys,
# todo: select correct protocol based on alpn (?) </s> self.c.server_conn.protocol = http1.http1protocol(self.c.server_conn)	get_response_from_server try: if not self.c.server_conn.protocol: self.c.server_conn.send(self.c.server_conn.protocol.assemble(flow.request)) flow.response = HTTPResponse.from_protocol(
# todo remove in v8 </s> self.compile_html = self.compile	__init__ self.compiler = compiler self.compile = self.compiler.compile self.demote_headers = self.compiler.demote_headers and self.config['DEMOTE_HEADERS'] tzinfo = self.config['__tzinfo__']
# todo_recorders - need to pass in parent info instead of none </s> metadata = create_local_meta(none, self.pathname)	record_iteration Record an iteration of the current System. self.iter_count += 1 update_local_meta(metadata, (self.iter_count,)) self._rec_mgr.record_iteration(self, metadata, method=inspect.stack()[1][3])
# todo : pytest.mark.parametrise once nose is gone. </s> def test_collections_counter():	test_collections_counter class MyCounter(Counter): pass
# todo: this regex could change based on project req format </s> attributes[header_array[j]] = re.findall('[^\s;,]+', cell)	add_doorstop_items id_text = cell elif 'links' == header_array[j]: elif ('active' == header_array[j] or 'normative' == header_array[j] or 'derived' == header_array[j]) and all_strings: attributes[header_array[j]] = (cell == "True")
# todo: run in built-in terminal </s> error = reprounzip.destroy(self.directory, unpacker=self.unpacker)	_destroy def _destroy(self): if error: error_msg(self, *error)
# todo: scale and translation could be merged into a single network </s> with tf.variable_scope("scale", reuse=tf.auto_reuse):	backward_and_jacobian mask = self.get_mask(inputs, dtype=inputs.dtype) masked_inputs = inputs * mask scale = mask * self.scale_fn(masked_inputs) with tf.variable_scope("translation", reuse=tf.AUTO_REUSE):
# todo: this logic also exists in self.use_as_template() </s> for addon in settings.addons_available:	save saved_fields = super(Node, self).save(*args, **kwargs) if first_save and is_original and not suppress_log: if 'node' in addon.added_default: self.add_addon(addon.short_name, auth=None, log=False)
result = np.vstack((result, result_a))          # todo: https://github.com/tensorlayer/tensorlayer/issues/288 </s> return result	predict feed_dict.update(dp_dict) result_a = sess.run(y_op, feed_dict=feed_dict)
# todo: fails because of missing svg support </s> assert_pixels('inline_image_' + filename, 8, 8, image, '''	test_images )) def test_images(filename, image): <style> @page { size: 8px }
# todo: add .data and .grad to syft tensors </s> for p in nn_self.parameters():	module_float_precision_ def module_float_precision_(nn_self): parameters to normal float parameters""" p.float_precision_() return nn_self
"""the line which triggered the message"""#todo elaborate </s> s.match = match	__new__ s.event = event s.bytes = bytes The regular expression ``MatchObject_`` for the triggering line. .. _MatchObject: http://docs.python.org/library/re.html#match-objects
# todo: use lt and rt in profile as well </s> side = left if id else rigth	set_action self.button_widgets[id.name].update() elif id in TRIGGERS: before, profile.triggers[side] = profile.triggers[side], action self.button_widgets[id].update()
# todo unordered float </s> if a is none and b is none:	fcomi def fcomi(ir, instr, a=None, b=None): a, b = float_st0, float_st1 elif b is None:
# todo: is this safe? </s> self.cmd( 'iptables -f' )	config localIntf =  self.defaultIntf() self.cmd( 'sysctl net.ipv4.ip_forward=0' ) self.cmd( 'iptables -t nat -F' ) self.cmd( 'iptables -P INPUT ACCEPT' )
assert ts.nanosecond == 0 # todo: handle nanosecond (timestamps.pyx) </s> _time = str_2d(ts.hour) + ':' + str_2d(ts.minute) + ':' + str_2d(ts.second)	timestamp_isoformat_impl def timestamp_isoformat_impl(ts): res = str(ts.year) + '-' + str_2d(ts.month) + '-' + str_2d(ts.day) + 'T' + _time return res
pass # todo </s> def test_getitem_version_all_projection(self):	test_getitem_version_all_projection
# todo(mnaser): uncomment this in patch resolving the issue </s> self.assertin(volume_id, self.cinder.reserved_volumes)	test_delete_with_reserved_volumes self.assertIn(volume_id, self.cinder.reserved_volumes) self.api.delete_server(server['id'])
# todo query = 'query statement' </s> client = asset_v1.assetserviceclient()	search_all_iam_policies def search_all_iam_policies(scope, query=None, page_size=None): from google.cloud import asset_v1 response = client.search_all_iam_policies( scope, query=query, page_size=page_size)
# todo: validate schema </s> data = request.json	schedules return jsonify({'schedules': schedule_list}) if request.method == 'POST': if 'schedules' not in manager.config or not manager.config['schedules']: manager.config['schedules'] = []
# todo: add rotary inertia </s> mass = elem.mass()	build_Mgg Mbb[i3, i3] = Mbb[i3+1, i3+1] = Mbb[i3+2, i3+2] = mass / 3 elif etype == 'CQUAD4': nid1, nid2, nid3, nid4 = elem.nodes i1 = dof_map[(nid1, 1)]
# todo: remove in 21.08 </s> if cache_audio_dir:	main def main(cache_audio_dir): if not os.path.exists(cache_audio_dir): os.makedirs(cache_audio_dir)
#todo make more readable </s> days = 7 if self.week_view.get_active() else 30	get_facts by_activity[fact["name"]] += duration by_category[fact["category"]] += duration date_sort = lambda a, b: (b[2] < a[2]) - (a[2] < b[2]) totals["by_day"] = []
# todo: arrange </s> repo = self.remote.new_repo(self.token)	test_create_repo def test_create_repo(self): Test: create/edit a repo object self.assertTrue(self.remote.modify_repo(repo, "name", "testrepo0", self.token)) self.assertTrue(self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token))
# todo check response </s> result = self.diagnostics.write_data_by_identifier(0x02, [0x11])	test_write_data_by_identifier def test_write_data_by_identifier(self): self.assertIsInstance(result, bytearray, "Did not receive response") print("Result:", list(map(hex, result)))
# todo: use vendorid and productid </s> (vendor, product) = device	check_device def check_device(self, device): DEVICES is (VendorID, ProductID, Description) """ if vendor == "0x10de": return True
#todo: issue warning that this is an unsafe operation, but doing it cause user insists </s> try:	atomic_move e = get_exception() if unsafe_writes and e.errno == errno.EBUSY: try: out_dest = open(dest, 'wb')
# todo: add the rest of the api actions here and call them directly from the api controller </s> self.shareable_service = sharable.shareableservice(self.manager, self.serializer)	__init__ self.serializer = serializer
# todo(nakago): check why tolerance is high </s> def test_backward_cpu(model, data):	test_backward_cpu atom_data, adj_data, y_grad = data gradient_check.check_backward(model, (atom_data, adj_data), y_grad,
# todo: apply _is_zero attribute </s> self.set_name(name=name, latex_name=latex_name)	copy_from self._restrictions[dom] = rst.copy()
# todo: skips header parsing </s> iline += 1	read_abaqus_inp pass elif word.startswith('material'): line0 = lines[iline].strip().lower() word = line0.strip('*').lower()
# todo: error handling </s> backend = plugin.get(backend, 'backend')()	Graph backend = plugin.get('default', 'backend')() elif not isinstance(backend, Backend): self.__backend = backend self.__parser = None
# todo: remove this assert when we're confident the code is correct </s> assert((dates_to_insert_zeros & df.index).size == 0)	insert_zeros else: non_power_columns.append(col) zeros = pd.DataFrame(data=0, index=dates_to_insert_zeros,
# todo: check how to be writeable only from same group </s> raise exception("group jetson_stats does not exist!")	start gid = getgrnam(PIPE_JTOP_USER).gr_gid except KeyError: else: gid = os.getgid()
# todo debug </s> print "rule has triggered"	run logging.info("[%s]: Alert level " % self.fileName + "'%d' rules have triggered." % alertLevel.level) if alertLevel.smtpActivated:
# todo testing </s> sys.stdout.write('sending subcovrequest to device')	generate_COV_sub @RPC.export def generate_COV_sub(self, target_address, device_id, object_type, instance_number, lifetime=None): subscription = SubscriptionContext(target_address, device_id, lifetime) covRequest = SubscribeCOVRequest(
# todo: this is untested. </s> _raise_current_error()	add_client_ca self._context, certificate_authority._x509) if not add_result:
# todo: supports blocking queries and all consistency modes </s> params = {}	nodes Returns the nodes known about in the *dc* datacenter. *dc* defaults to the current datacenter of this agent. if dc: params['dc'] = dc
assert ts.nanosecond == 0 # todo: handle nanosecond (timestamps.pyx) </s> _time = str_2d(ts.hour) + ':' + str_2d(ts.minute) + ':' + str_2d(ts.second)	timestamp_isoformat_impl def timestamp_isoformat_impl(ts): res = str(ts.year) + '-' + str_2d(ts.month) + '-' + str_2d(ts.day) + 'T' + _time return res
# todo: must be implemented </s> pass	range_from_chapters def range_from_chapters(crawler, times=0):
# todo: weather data source changed, temporarily disable, will enable it later when new data source is available. </s> self._is_temp = is_temp	CitiBikeTopology super().__init__() self._data_pipeline["trip"] = CitiBikePipeline(topology, trip_source, station_info, is_temp) def __del__(self): if self._is_temp:
# truffle todo: revert </s> for __x in _rlistdir(dirname, dironly): yield __x	_glob2 assert _isrecursive(pattern) yield pattern[:0]
# todo should also include meta-chunk-hash </s> trailers = {'x-oio-chunk-meta-metachunk-size': metachunk_size,	test_copy_errors metachunk_size = 9 * length metachunk_hash = md5().hexdigest() 'x-oio-chunk-meta-metachunk-hash': metachunk_hash} resp, body = self._http_request(chunkurl, 'PUT', chunkdata, headers,
# todo: make truly async </s> self._resourcemanager_client = discovery.build('cloudresourcemanager', 'v1', cache_discovery=false, cache=memorycache())	__init__ def __init__(self):
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_METADATA_filename_matches_postScriptName_success def test_result_METADATA_filename_matches_postScriptName_success(self): r = run_set(p, 'result') success_tests = r['success']
# todo: add flex option to the implementation </s> widget.frame = self	add_content def add_content(self, position, widget, flex): for child in widget.interface.children: child._impl.container = widget
# todo: adapt units_qs once we allow filtering </s> units_qs = unit.store.units	process_submit state='pending', unit=unit.id) current_page = 'page' in request.POST and request.POST['page'] or 1 unit_rows = profile.get_unit_rows() preceding = units_qs.filter(index__lt=unit.index).count()
time.sleep(.0015) # todo: tune </s> return true	_async_paint_instructions if self._repaint_requested: return False
todo: factorize once #1457 is merged. </s> returns: longitude [deg east], latitude [deg north] and altitude [m]	_get_satpos def _get_satpos(self): Evaluate orbit polynomials at the start time of the scan. if self.satpos is None: a = self.mda['projection_parameters']['a']
# todo(thowe): there is a general smell here that this code is </s> auth = cloud.config['auth']	make_connection cloud = occ.get_one_cloud(opts.cloud, opts) opts.preferences.set_region(opts.preferences.ALL, cloud.region) if 'cacert' in cloud.config: auth['verify'] = cloud.config['cacert']
# todo: maybe[int] and maybe[simple_sum] are invalid </s> return c_type	_GetCppType elif type_name == 'maybe': c_type = _GetCppType(typ.children[0]) elif typ.resolved: if isinstance(typ.resolved, asdl_.SimpleSum):
# todo: replace with stream-changed </s> self._trigger_track_playback_started()	play if success: self.core.tracklist.mark_playing(tl_track) else: self.core.tracklist.mark_unplayable(tl_track)
# todo -- this must not block long (!) </s> self.worksheet.quit()	Worksheet_quit_sage class Worksheet_quit_sage(WorksheetResource, resource.Resource): def render(self, ctx): return http.Response(stream='done')
# todo: in 0.6.0 change this to "disabled": false </s> "enabled": true,	test_additional_properties "dev_type": "tap", "dh": "dh.pem", "key": "key.pem", "mode": "server",
# todo: re-enable this when https://github.com/ironlanguages/ironpython2/issues/10 is fixed </s> print e.message	test_deprecated_string_exception except TypeError, e:
# todo improve logged output </s> self.logger.debug(	start_vent_collector try: resp = requests.post(uri, data=json.dumps(payload)) 'Collector response: {0}'.format(resp.text)) except Exception as e:  # pragma: no cover
# todo: remnants from rllab -> gym conversion </s> self.set_state(qpos, qvel)	reset_model qvel[self.PUCK_INDS] = 0 qvel[self.TARGET_INDS] = 0 return self._get_obs()
# todo(ls): revert this loop to "yield from" </s> for __x in self.stack.format(): yield __x	format if self.exc_traceback is not None: yield 'Traceback (most recent call last):\n' for __x in self.format_exception_only(): yield __x
# todo(mriedem): move to objectlistbase.__init__ for empty lists. </s> instance_list = instance_obj.instancelist(objects=[])	_get_servers log_msg = _("Flavor '%s' could not be found ") LOG.debug(log_msg, search_opts['flavor']) if is_detail: instance_list.fill_faults()
# todo: remove references to _attachments once all forms have been migrated to riak </s> query = (formes()	_get_form_ids Each form has a multimedia attachment and meets the given filters. if not export_is_legacy: .domain(domain) .app(app_id)
# todo: this is quadratic complexity </s> existingmodules = [	load_mod_from_file if file_ext.lower() != '.py': return m for m in shutit.shutit_modules if getattr(m, '__module_file', None) == fpath
print("why would this happen?")  # todo </s> return	map_subscript array = self.knl.arg_dict[name] else: if not isinstance(array, lp.GlobalArg): print("Why would this happen?")  # TODO
# steps = 0 # todo </s> print("dupli export took %.3fs" % (time() - start))	convert transformations = array("f", duplis.matrices) luxcore_scene.DuplicateObject(src_name, dst_name, count, transformations)
# todo(akhmerov): implement. </s> pass	test_stop_workflow def test_stop_workflow(self):
# todo(b/159180073): clean raise after fixing dataset reduce. </s> num_gpu_devices = len(tf.config.list_logical_devices('gpu'))	_ensure_comp_runtime_compatible def _ensure_comp_runtime_compatible(comp: pb.Computation) -> pb.Computation: graph_def = serialization_utils.unpack_graph_def(comp.tensorflow.graph_def) if num_gpu_devices > 0: _add_skip_zero_before_finalize_dataset(graph_def)
# todo: filter values. </s> if ctx.needs_input_grad[4]:	backward indexB, valueB = transpose(indexB, valueB, k, n) _, grad_valueA = mm(indexC, grad_valueC, indexB, valueB, m, n, k) indexA, valueA = transpose(indexA, valueA, m, k) _, grad_valueB = mm(indexA, valueA, indexC, grad_valueC, k, m, n)
# todo: handle errors better </s> abort(code=http_exceptions.unprocessableentity.code, message="value is required")	_process_patch_operation processing_status (bool) - True if operation was handled, otherwise False. if 'value' not in operation: assert operation['path'][0] == '/', "Path must always begin with /" field_name = operation['path'][1:]
# todo: this would be a good candidate for refactoring into a testcase subclass shared across backends </s> expected_pks = ['3', '2', '1']	test_values_slicing reset_search_queries() self.assertEqual(len(connections['whoosh'].queries), 0) results = self.sqs.all().order_by('pub_date').values('pk') self.assertListEqual([i['pk'] for i in results[1:11]], expected_pks)
#todo print to stderr </s> print "need to pick a chamber"	scrape_legislation chamber_url = 'senate' else: session = session_number(int(year)) assert int(year) >=2005
time.sleep(40)  # todo: should remove after polling get. </s> mpc_1_2_3 = op(mpc_1_2, mpc_2_3)	test_tensor_abstraction_subsets time.sleep(40)  # TODO: should remove after polling get. mpc_2_3 = op(tensor_pointer_2, tensor_pointer_3) time.sleep(40)  # TODO: should remove after polling get. exp_res_1 = op(data_1, data_2)
# todo: deprecation 3.1 </s> @deprecated("deprecated. use log_cosh instead")	logtanh def logtanh(x): return log_tanh(x)
# todo: capture stdout for both the test assert and docs embedding </s> prob.run_model()	test_feature_iprint_neg1 ln_scipy.options['iprint'] = -1
# todo legacy method to be removed/refactored </s> from corehq.apps.locations.models import location	locations @property def locations(self): from corehq.apps.commtrack.models import SupplyPointCase def _get_linked_supply_point_ids():
# todo: figure out a dynamic way of doing this </s> channels = {	Silence voice_chat = None if isinstance(target_channel, VoiceChannel): "offtopic": Channels.voice_chat, "code/help 1": Channels.code_help_voice,
# todo(b/132329316) remove when `xla.compile` allows tf.device(tpu). </s> forward = with_soft_placement(forward)	assertCheckpointDistributionStrategy forward = tf.function(forward) if self.primary_device == "TPU": for index, variable in enumerate(variables): variable.assign(goldens.range_like(variable, start=index))
# todo: we should probably have a special folder just for header </s> import brian2.synapses as synapses	_load_module import numpy c_include_dirs.append(numpy.get_include()) synapses_dir = os.path.dirname(synapses.__file__) c_include_dirs.append(synapses_dir)
# todo - restore this when issue __ is fixed. </s> outputs = cr.get_case(case).outputs	test_fan_in_grouped for case in cases:
""" todo: check this description </s> get perfdatas from multiple services/hosts	perfs @declared def perfs(objs_ref, metric_name): objs = get_objects(objs_ref) r = []
#todo - use a context manager here once we drop python 2.6 </s> self.assertraises(valueerror, kcluster, data,	test_kcluster [ 1, 1, 1, 1, 1], [ 1, 1, 1, 1, 1]], int) **{"nclusters": nclusters, "mask": mask, "weight": weight, "transpose": 0, "npass": 100,
# todo: should change to 'bytes' on python3 </s> 'unsupported key type: str')	test_table_setitem_invalid_type self.assertEqual(exception_context.exception.message,
# todo: remove this - cura-4482 </s> material = self._global_container_stack.material	_onGlobalContainerChanged except TypeError: pass material.nameChanged.disconnect(self._onMaterialNameChanged) quality = self._global_container_stack.quality
# todo - this needs to be a timezone un-aware timestamp </s> user.password_updated = 0	test_it_returns_user_when_valid schema = ResetPasswordSchema().bind(request=pyramid_csrf_request) user = user_model.get_by_username.return_value appstruct = schema.deserialize({"user": "abc123", "password": "secret"}) assert appstruct["user"] == user
# todo(philday): add support for timeout (clean shutdown) </s> return self._migrationops.migrate_disk_and_power_off(context,	migrate_disk_and_power_off block_device_info=None, timeout=0, retry_interval=0): instance, dest, flavor,
# todo(laigd): remove this check when 312743821 is in the release. </s> if use_tf_function and tf.compat.v1.__version__ < '2.3.0':	testNestedMap ) def testNestedMap(self, use_tf_function): return FLAGS.call_defun_use_tf_function = use_tf_function
#todo(ziad): use a more sophisticated proxy </s> return response(status=resp.status, body=data)(env, start_response)	__call__ resp = conn.getresponse() data = resp.read()
# todo: log in browser as a session </s> return flask.redirect('/account')	login_post 'error': "The account does not exist or the password is wrong." }
# todo files listed here may not belong to the given camera </s> file_moment = datetime.datetime.fromtimestamp(os.path.getmtime(full_path))	_remove_older_files def _remove_older_files(dir, moment, exts): for full_path in _list_media_files(dir, exts): if file_moment < moment: logging.debug('removing file %(path)s...' % {
raise notimplementederror #todo </s> elif q.kw(i,"format"):	parse while i < l: if q.kw(i,"RETURN"): raise NotImplementedError #TODO elif q.kw(i,"REQUEST"):
# todo candidate for move to system/osi as not btrfs related </s> base_root_disk = root_disk()	scan_disks :param min_size: Discount all devices below this size in KB :return: List containing drives of interest cmd = ['/usr/bin/lsblk', '-P', '-o', 'NAME,MODEL,SERIAL,SIZE,TRAN,VENDOR,HCTL,TYPE,FSTYPE,LABEL,UUID']
# todo catch error here </s> cur.reload(registry)	menu_tab_reload registry = Registry.Registry(f)
# todo resource arns may contain wildcards, e.g. arn:aws:iam::*:role/admin -- </s> session.run(	load_group_policies for policy_name, policy_data in policies.items(): for role_arn in _find_roles_assumable_in_policy(policy_data): ingest_policies_assume_role, GroupName=group_name,
# todo: safe labels </s> return record	fact cursor.close()
# todo test </s> return self._item.aliases.keys()	keys_with_aliases def keys_with_aliases(self):
# todo: look into nonce prefix </s> nonce = nacl.utils.random(nacl.public.box.nonce_size)	user_msg def user_msg(self, name, msg): if self.init_pubkey(name): enc = self.boxes[name].encrypt(msg.encode('utf-8'), nonce) self.send_cmd.msg_enc_pubkey(name, enc)
# todo implement. </s> yaml = self.master_model.to_yaml()	_train def _train(self, rdd, nb_epoch, batch_size, verbose, validation_split):
# todo(yanase): implement maximization. </s> if _direction == structs.studytask.maximize:	optimize else: raise ValueError('Please set either \'minimize\' or \'maximize\' to direction.') raise ValueError( 'Optimization direction of study {} is set to `MAXIMIZE`. '
# todo: hack, this may be called differently in other agents (replace by root-policy). </s> variables = self_.call(self.policy._variables)	update_from_external_batch ): if isinstance(self.optimizer, MultiGpuSyncOptimizer): return self_.call(self_.sub_components["multi-gpu-sync-optimizer"].step, variables, *inputs) q_values_s = self_.call(policy.get_q_values, preprocessed_states)
# todo: some message needed? </s> pass	save_arc else: if old_type == "Equiv": else: assert old_type is None, 'attempting to change Equiv, not supported'
# todo: fix this upstream </s> for header in headers:	md_for for result in results: row = [] if (header != "HSTS Header") and (header != "HSTS Max Age") and (header != "Redirect To"): if result[header] is None:
# todo: remove </s> g.facet_titles = facets	_read extra_vars["query_error"] = True extra_vars["page"] = h.Page(collection=[]) g.page = extra_vars["page"] extra_vars["group_type"] = group_type
# todo: find out why pls and cca fail. ransac is random </s> if name not in ('plscanonical', 'cca', 'ransacregressor'):	check_regressors_train regressor.fit(X.tolist(), y_.tolist()) regressor.predict(X) assert_greater(regressor.score(X, y_), 0.5)
# todo: use upstream implementation when available </s> key_n, key_g = random.split(self._random_state)	t_gen _support_mask = constraints.real def _rvs(self, df): normal = random.normal(key_n, shape=self._size) half_df = df / 2.0
#doc [todo: get material from docstring of same method in assembly.py] </s> return self.obj_with_glselect_name.get( glname)	object_for_glselect_name def object_for_glselect_name(self, glname):
# todo: should this piece of data be global instead of local to each buffer? </s> self.settings.vi['last_character_search_forward'] = value	last_character_search_forward @last_character_search_forward.setter def last_character_search_forward(self, value):
)  # todo </s> chunk = res["chunk"]	get_users_in_group res = yield self.transport_client.get_users_in_group( group_id, requester_user_id, valid_entries = [] for entry in chunk:
# todo put this into the regression metric itself </s> cprediction = sanitize_array(prediction)	calculate_score score = dict() if task_type in REGRESSION_TASKS: for metric_ in REGRESSION_METRICS: func = REGRESSION_METRICS[metric_]
# todo: maybe foreginkyes should be taken from freeze orm </s> alter_foreignkey_to_int('recipes_oldrecipearticleredirect', 'new_id')	alter_self_foreignkeys alter_foreignkey_to_int('articles_articlecontents', 'article')
# todo check </s> return self.mimetype	format @interfacedoc def format(self):
# todo: remove </s> if user.is_moderator:	for_update_or_404 def for_update_or_404(self, pk, user): return get_object_or_404(self._access(user=user), pk=pk) else:
# todo: +kwargs </s> else:	pull with remote.repo.git.custom_environment(**GitRepo.GIT_SSH_ENV): return remote.pull(**pull_kwargs) return remote.pull(**pull_kwargs)
# todo: remove this line when counters are supported in build.py </s> raise invalidvalues	validate_content_list_token elif prototype in (('counter', ['IDENT', 'IDENT']), ('counters', ['IDENT', 'STRING', 'IDENT'])): style = args[-1] if style in ('none', 'decimal') or style in counters.STYLES:
# todo proper error messages </s> result = {}	check_options def check_options(options): trace_get = evaluation.parse('Settings`$TraceGet') if options["System`Trace"].to_python() or trace_get.evaluate(evaluation) == SymbolTrue:
# todo: remove check once pytorch avoids a copy for this case </s> if p.data_ptr() != p_data_fp32.data_ptr():	step p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32) p_data_fp32.addcdiv_(-step_size, exp_avg, denom) p.data.copy_(p_data_fp32) return loss
# todo: cleanly remove clipboard code if it is no longer needed </s> return httpresponsebadrequest('not implemented anymore')	discard_clipboard def discard_clipboard(request): if True: if request.method == 'POST': clipboard = Clipboard.objects.get(id=request.POST.get('clipboard_id'))
# todo: udpoutgoing style buffer </s> return	send_to except socket.error as se: if se.args[0] == EWOULDBLOCK:
# todo: really dirty. figure out a better way. </s> exchange_trades = [x for x in exchange_trades if x.identifier not in all_set]	query_trades only_cache=only_cache, ) if self.premium is None: trades = self._apply_actions_limit(
# todo proper error messages </s> result = {}	read_check_options def read_check_options(options: dict) -> dict: keys = list(options.keys()) if "System`AnchoredSearch" in keys:
# todo support multi-discrete actions </s> actions = torch.eye(self.body.action_dim)[actions.long()]	train actions = batch['actions'] if self.body.is_discrete: pdparams = self.calc_pdparam(states) action_pd = policy_util.init_action_pd(self.body.ActionPD, pdparams)
# todo: to be removed in v2.8.0 </s> return self.class_from_path_setting('section_menu_class_path')	SECTION_MENU_CLASS def SECTION_MENU_CLASS(self):
# todo: do_cert? </s> was_error = false	scan_parallel queue(Tlsv11ScanCommand()) queue(Tlsv12ScanCommand()) for result in scanner.get_results(): try:
# todo: i'm not quite clear on the difference between </s> message = self._read_sysex()	MidiFile message = self._read_meta_message() elif status_byte in [0xf0, 0xf7]: else: message = self._read_message(status_byte)
# todo: do data augmentation </s> image = _random_adjust_brightness(image, probability=0.3)	preprocess_for_train2 image = tf.cast(tf.image.resize_images(image, [height, width]), dtype=tf.uint8) img_shape = tf.shape(image) image = _random_adjust_contrast(image, 0.3)
# todo: approximate output size properly </s> curr_size = 101 + min(l_len, r_len) // 10	local_hash_join l_len = len(left_keys[0]) r_len = len(right_keys[0]) out_left_key = alloc_arr_tup(curr_size, left_keys) out_data_left = alloc_arr_tup(curr_size, data_left)
# todo: should allow multi_dimensional inputs/outputs </s> layer_sizes=(*translation_hidden_sizes, inputs.shape.as_list()[-1]))	translation_wrapper return feedforward_net( inputs,
end_tok = y.data[0, -1] # todo </s> if self.is_cuda:	beam_search x, y = self.collate(*batch) start_tok = y.data[0, 0] x = x.cuda() y = y.cuda()
# todo this makes self variables non-breakable. wanted? </s> if isinstance(name, er.instanceelement) \	_process_new if details and details[0][1] != '=': no_break_scope = True and not name.is_class_var: no_break_scope = True
# ensure_authorized_to('create', announcement) # todo: uncoment? </s> announcement_repo.save(announcement)	new_announcement announcement = Announcement(title=form.title.data, body=form.body.data) msg_1 = gettext('Annnouncement created!') flash('<i class="icon-ok"></i> ' + msg_1, 'success')
# todo - add some tests to this response </s> response = self.do_list(	test_list response = self.do_list() self.assertEqual(len(response), 0) { 'item': 10,
# todo: unit tests </s> user = auth.user	get_all_projects_smart_folder def get_all_projects_smart_folder(auth, **kwargs): contributed = user.node__contributed nodes = contributed.find(
# todo: return return value </s> output(self.appobj)	doInput def doInput(self, methodInput): for output in self._transitioner.transition(methodInput):
# todo have no idea if is cdecl or stdcall </s> flag = params["processinformationclass"]	_QueryInforationProcess def _QueryInforationProcess(self, address, params): dst = params["ProcessInformation"] pt_res = params["ReturnLength"]
# todo: remove this line when the time is right. </s> enforce_probationary_period(emitter=emitter, expiration=expiration)	grant else: expiration = click.prompt('Enter policy expiration datetime', type=click.DateTime(formats=DATETIME_FORMATS)) if not n: n = ALICE.n
# todo : pytest.mark.parametrise once nose is gone. </s> def test_collections_defaultdict():	test_collections_defaultdict a = defaultdict() a.default_factory = a
# todo: kernels need some sort of structured form </s> def r_2_log_r_2_kernel_derivative(r):	r_2_log_r_2_kernel_derivative r""" Derivative of the radial basis function for TPS.
pass # todo </s> def test_set_state(self):	test_set_state @SkipTest
##todo: not saving lines yet </s> if out_path is not none:	evaluate_regression store_embeddings(sentence, embedding_storage_mode) eval_loss /= len(sentences) with open(out_path, "w", encoding="utf-8") as outfile: outfile.write("".join(lines))
# todo: check the data! </s> self.asserttrue(len(list(pipe)) > 0)	test_union_just_other pipe_def = self._get_pipe_def(pipe_file) pipe = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def)
"""todo doc me""" </s> table = [['foo', 'bar', 'baz'],	test_profile_default def test_profile_default(): ['A', 1, 2], ['B', '2', '3.4'],
# @todo: remove this if in 0.6 </s> if isinstance(node_id, node):	ex_share_ip def ex_share_ip(self, group_id, node_id, ip, configure_node=True): node_id = node_id.id if configure_node:
# @todo: display better tick labels for date range (e.g. 06/01 - 06/05) </s> formatter = matplotlib.dates.dateformatter('%h:%m')	peek axes.xaxis.grid(False, 'major') axes.legend() axes.xaxis.set_major_formatter(formatter) axes.fmt_xdata = matplotlib.dates.DateFormatter('%H:%M')
# todo: implement </s> pass	hyphenate found.append((tokeniter.cursor(block, token, m.start(), m.end()), m.group())) if not found: if found: import hyphenator
# todo: kickoff syncing process with this peer </s> pass	Node (peer_has_equal_finalized_epoch and peer_has_higher_head_slot) ): await stream.close() async def say_hello(self, peer_id: ID) -> None:
# todo: checks for being not outside of this repository </s> return exists(target_path) and 'annex/objects' in str(target_path)	file_has_content if islink(filepath):                    # if symlink target_path = realpath(filepath)  # realpath OK return False return self._check_files(self.find, quick_check,
# todo change affinity on osx/linux </s> if sys.platform == 'win32':	lets_run_a_test rate = os.environ.get('TEST_RATE', '200') sitl_args += ['--speedup', str(speedup), '-r', str(rate)] sitl = Popen(['start', '/affinity', '14', '/realtime', '/b', '/wait'] + sitl_args, shell=True, stdout=PIPE, stderr=PIPE) else:
# todo: this is lazy, we should only reconfigure the drone(s) who are actually </s> if drone_edge:	_bait_user_changed drone_edge = db_session.query(DroneEdge).filter(DroneEdge.username == bait_user.username, DroneEdge.password == bait_user.password).first() self._reconfigure_all_clients()
# todo: fix with stubber / before send event </s> with mock.patch('botocore.endpoint.endpoint._send') as _send:	test_get_export 'accepts': 'application/yaml' } _send.return_value = mock.Mock( status_code=200, headers={}, content=b'{}')
# todo: is incref required? </s> context.nrt.incref(builder, arr_typ, arr)	lower_box_df else: arr_obj = box_array(arr_typ, arr, c) name_str = context.insert_const_string(c.builder.module, cname) cname_obj = pyapi.string_from_string(name_str)
# todo enable when it will not crash </s> self.setup_user_agent()	AppsWebView self.get_settings().set_property('enable-default-context-menu', False) self.get_settings().set_property('enable-plugins', False) self.load_uri('http://ubuntu-tweak.com/utapp/') self.connect('notify::title', self.on_title_changed)
# todo support startblock, endblock </s> self.disconnect()	_processBlockEvents if reg_num.disconnect:
#time = "todo" </s> annot.annotation_metadata.annotator.email = "todo"  # todo	fill_annotation_metadata annot.annotation_metadata.annotator.name = "TODO"
# todo?: does not match two subsequent variables or strings, such as  "start" # foo # bar # "end"  or  "start" # "end". </s> self.replace_all_re = re.compile(r'((?p<pre>"?)\s*(#|^)\s*(?p<id>[^\d\w]\w*)\s*(#|$)\s*(?p<post>"?))', re.unicode)	__init__ } self.ignore_nonstandard_types = ignore_nonstandard_types self.records = self._parse_records(customization=customization) self.entries_hash = {}
# todo: kernels need some sort of structured form </s> self.kernel_derivative = r_2_log_r_2_kernel_derivative	TPS kernel = r_2_log_r_2_kernel self.kernel = kernel self.K = self.kernel(pairwise_norms) self.P = np.concatenate(
# todo: check types </s> @type_callable(datetime.datetime)	type_timestamp def type_timestamp(context): def typer(year, month, day):  # how to handle optional hour, minute, second, us, ns?
# todo: delete this </s> self.streams = {}	__init__ self.file_manager = None self.exchange_rate_manager = None
# todo per-sync cached results </s> return trakt.user.get_library(media, marked, extended).get('data')	library def library(cls, media, marked, extended='min'):
# todo: hints </s> self.pause_point('pausing') # todo: message	challenge ok = False while not ok: check_command = follow_on_context.get('check_command') output = self.send_and_get_output(check_command,child=child,timeout=timeout,retry=1,record_command=record_command,echo=echo, loglevel=loglevel, fail_on_empty_before=False)
# todo: ... </s> dumpdata(get_dump_path(''), data)	dump_comments def dump_comments(data):
# todo: reformat or delete </s> camera.trackbodyid = 0	sawyer_xyz_reacher_camera def sawyer_xyz_reacher_camera(camera): camera.distance = 1.0 cam_dist = 0.3
# todo: error handling? </s> update = await self.update_status('started', task_id, session)	PipelineStepRunner } container = await docker_client.containers.run(config=config) if wait_on_completion: await container.wait()
# todo: remove in v2.8 </s> self.assertequal(self.filterset(params, self.queryset).qs.count(), 1)	test_type params = {'type': ContentType.objects.get(model='site').pk}
# todo: use pybossa uploader! only for debugging: </s> open('/tmp/%d_%s_task_json.zip' % (app.id, name), 'wb').write(memzip.getvalue())	export_json memfile.write(str(line)) memzip = make_onefile_memzip(memfile, '%s_task.json' % name) json_task_run_generator = respond_json("task_run", app.id) if json_task_run_generator is not None:
# todo: some streams were redirected, we need to manually work them </s> raise notimplementederror("win32elevate doesn't support elevating scripts with "	elevateAdminRights if reattachConsole and not all(stream.isatty() for stream in (sys.stdin, sys.stdout, sys.stderr)): "redirected input or output") if not ShellExecuteEx(ctypes.byref(executeInfo)):
# todo: add pi_stack and cation_pi to feature_types (it's not trivial </s> feature_types=["ecfp", "splif", "hbond", "salt_bridge"],	featurize_pdbbind featurizer = dc.feat.RdkitGridFeaturizer( voxel_width=16.0, ecfp_power=9, splif_power=9,
# todo: look at args for remotedata </s> workers = frequencies(w for dep in deps	decide_worker 'bob' deps = dependencies[key] for w in who_has[dep]) worker = min(workers, key=lambda w: len(stacks[w]))
# todo archive schema and return </s> logging.info('end extract')	extract def extract(): logging.info('Begin extract') pass
# todo(sano): deal with maximize task. </s> best_trial_model = min(completed_trial_models, key=lambda t: t.value)	get_all_study_summaries best_trial = None if len(completed_trial_models) > 0: best_param_models = [p for p in param_models if p.trial_id == best_trial_model.trial_id]
# todo: will work only if table.fields is ordereddict </s> csv_writer.writerow([type_.serialize(getattr(row, field))	export_to_csv csv_writer.writerow([field.encode(encoding) for field, _ in fields]) for row in table: for field, type_ in fields])
# todo: remove </s> g.code = [e.code]	error_handler if isinstance(e, HTTPException): extra_vars = {u'code': [e.code], u'content': e.description} return base.render( u'error_document_template.html', extra_vars), e.code
# todo: remove this when we depend on genshi >= 0.5 </s> return stream.render(method, doctype=doctype)	render_template try: if arity(stream.render) == 3: else: buffer = cStringIO()
#todo could use original filename to verify this </s> assert_true(filename.endswith('.mp3'))  # depends on specific file	assert_download def assert_download(sid=self.song.sid): filename, audio = self.mm.download_song(sid) assert_is_not_none(audio)
# todo: this is ugly use of exceptions; is there a better way to track whether in a given type of request? </s> from framework.sessions import get_session	get_request_and_user_id Fetch a request key and user id from either a Django or Flask request. Fall back on a process-global dummy object if we are not in either type of request try: req = request._get_current_object()
# todo: check for error 'toomanyrules' </s> rule = fakerule(listener.arn, conditions, priority, actions, is_default=false)	create_rule raise PriorityInUseError() self._validate_actions(actions) listener.register(rule) return [rule]
# todo: valid and invalid values for the rest of the user model's fields. </s> helpers.call_action('user_update', **user)	test_user_update_with_invalid_password with nose.tools.assert_raises(logic.ValidationError) as context:
# todo: what should we do here, tell user the repo </s> continue	render_group_info repo = get_repo(att.repo_id) if not repo: att.name = repo.name else:
# todo: break this tuplet stuff into a helper function shared for <note>, <rest>, and <chord> </s> if elem.get('m21tupletsearch') is not none:	noteFromElement if duration.convertTypeToNumber(post.duration.type) > 4: post.beams.fill(post.duration.type, elem.get('m21Beam')) post.m21TupletSearch = elem.get('m21TupletSearch') post.m21TupletNum = elem.get('m21TupletNum')
# todo(dcramer): we want to be less aggressive on disabling domains </s> cache.set(domain_key, error or '', 300)	fetch_file 'url': expose_url(url), } logger.warning('Disabling sources to %s for %ss', domain, 300, exc_info=True)
# todo: do i need this? </s> else:	get_cutting_plane_from_surface_elements iedges.append(iedge) p_planes.append(p_plane) raise RuntimeError('not implemented; %r' % element.type) edges = element.get_edges()
# todo: make sure values are actually adding/etc together! </s> assert isinstance(p + q, parameter)	test_parameter_left_literal_arithmetic @pytest.mark.parametrize("q", test_parameters) def test_parameter_left_literal_arithmetic(p, q): assert isinstance(p - q, Parameter) assert isinstance(p * q, Parameter)
# todo verify produced answer </s> cnv.forward(input_tensor).detach().numpy()	test_brevitas_trained_cnv_pytorch assert input_tensor.shape == (1, 3, 32, 32)
# todo: for logs </s> cls_loss = tf.identity(log_loss, name='log_loss')	loss cls_target = tf.one_hot(labels, depth=2) log_loss = tf.losses.log_loss(cls_target, cls_prob) rpn_bbox_target = tf.reshape(rpn_bbox_target, [-1, 4]) rpn_bbox_pred = tf.reshape(rpn_bbox_pred, [-1, 4])
fwd_from=none,  # todo select from the database </s> post_author=message_tuple[6],	message_from_tuple message=message_tuple[4], reply_to_msg_id=message_tuple[5], media=None  # TODO Select from the database
# todo remove once minimum required matplotlib version reaches 2.0 </s> if matplotlib_version < [2, 0]:	pa_plot size = min(width, height) fig = plt.figure(34, figsize=(size, size)) axis_facecolor_kwargs = dict(axisbg='#d5de9c') else:
# todo test this </s> def test_setuptools_version_remains_same(self):	test_setuptools_version_remains_same When :func:``create_artifacts`` finishes the version of ``setuptools`` installed before it was run is still installed.
# todo: cleanup and deprecate worker_address in config files, leaving only checksum_address </s> from nucypher.config.characters import ursulaconfiguration	checksum_address_from_filepath if filename == default_name: checksum_address = cls.peek(filepath=filepath, field='checksum_address') if isinstance(cls, UrsulaConfiguration): federated = bool(cls.peek(filepath=filepath, field='federated_only'))
# todo: remove once elasticsearch v6.x is deprecated. </s> if self._getclientmajorversion() < 7:	WriteHeader }], } mappings = {self._document_type: mappings} self._Connect()
continue  # todo: log this </s> extension.entry_point = entry_point	load_extensions extension = extension_class() except Exception: installed_extensions.append(extension) logger.debug(
return deserialize(self.mechanism_bin, from_bytes=true)  # todo: techdebt fix </s> @obj.setter	Mechanism @property def obj(self) -> Any: def obj(self, value: Any) -> None: self.mechanism_bin = serialize(value, to_bytes=True)  # TODO: techdebt fix
# todo: ... </s> pass	pull_child_entities This fulfills the third requirement of `DHIS2 Integration`_. .. _DHIS2 Integration: https://www.dropbox.com/s/8djk1vh797t6cmt/WV Sri Lanka Detailed Requirements.docx
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: handle output diffing with plugins? </s> if key.lower().startswith(_text_mimes):	add_mime_diff def add_mime_diff(key, avalue, bvalue, diffbuilder): dd = diff(avalue, bvalue) if dd:
@unittest.skip('not written')  # todo: finish! </s> self.assert_shortened([value], "[b'aaaaaaaaaaaaaaaaaa...aaaaaaaaa']")	test_bytes_large "b'" + 'A' * 43688 + "..." + 'A' * 21844 + "'")
# xxx todo fix earley to maintain correct order </s> assert set(res.children) == {'aa', 'a'} or res.children == ['aaa']	test_earley4 res = l.parse("aaa")
# todo: support ps fault-tolerance </s> raise runtimeerror(	get_model res = self._ps_stubs[ps_id].pull_dense_parameters(req) if not res.initialized: "PS pod %d cannot be initialized" % ps_id )
# todo: test me. </s> self.settings.vi['cancel_action'] = value	cancel_action def cancel_action(self, value):
# todo(yifanmai): merge _setup_if_needed into setup </s> self._model_load_seconds = model_load_seconds	_set_model_load_seconds def _set_model_load_seconds(self, model_load_seconds):
# todo(cmaloney): good exception catching, etc </s> def wsgi_app(env, start_response):	wsgi_app subscriber.handle_event(json.load(env['wsgi.input'])) start_response('200 OK', [('Content-Type', 'text/html')])
# todo: check if we can avoid py3 specific here </s> return "" if x is none else binary_type.decode(x)	decode_if_not_None def decode_if_not_None(x):
# todo: make this collection_type </s> collection_type = value.get("type", "")	load_data_dict for key, value in test_data.items(): is_dict = isinstance(value, dict) if is_dict and ("elements" in value or 'list' in collection_type or 'paired' in collection_type): elements_data = value.get("elements", [])
# todo: assert </s> self.remote.get_repos()	test_get_repos Test: Get repos
raise exceptions.mpdnotimplemented  # todo </s> underscore, dash, dot and colon.	subscribe already. The name may consist of alphanumeric ASCII characters plus
# todo: dispatch on name to validate content (string, number, tuple) </s> yield (name, value[0] if len(value) == 1 else tuple(value))	parse_rule_content state = VALUE elif token.type == "literal" and token.value == ";": state = NAME elif token.type == "whitespace":
# todo ... </s> buildcontrolonelinetext(control)	buildControlObject def buildControlObject(control): return control
# todo: provide a kernel which will describe how coordinates are extruded. </s> mesh = firedrake.extrudedmesh(m, layers, layer_height=0.1)	integrate_assemble_p0 m = UnitSquareMesh(2 ** power, 2 ** power) layers = 11 fs = firedrake.FunctionSpace(mesh, family, degree, name="fs") f = firedrake.Function(fs)
# todo(sbauza): the current placement noauthmiddleware returns a 401 </s> return self._client.put(	_fake_put def _fake_put(self, *args): (url, data) = args[1:] url, json=data, endpoint_override="http://127.0.0.1:%s" % self.service.port,
# todo: add initialization </s> pass	_initialize def _initialize(self):
# todo: make sure config exists in /etc </s> minerd_cmd = ["sudo", "minerd", "-u", config.username,	start_minerd else: subprocess.call(["sudo", "minerd", "--stop"]) app_config.TWO1_POOL_URL] try:
#todo - there could be room for both a responsiveness check and a valid </s> responsive_cause_index = none	DNSQuery tcp_valid = False udp_valid = False prev_index = None for i, retry in enumerate(response.history):
recording_uuid = none #todo </s> start_time_system_s = none  # todo	_recording_update_legacy_from_v1_15_to_pprf_2_0 def _recording_update_legacy_from_v1_15_to_pprf_2_0(rec_dir): info_csv = utils.read_info_csv_file(rec_dir) start_time_synced_s = None  # TODO duration_s = None  # TODO
return user.address  # todo: update </s> elif item == 'user_country':	get_user_info return user.phone elif item == 'user_city': return user.address  # TODO: update elif item == 'user_address':
# todo: remove unescape_entities when mako html safe comes in </s> 'name': sanitize.unescape_entities(node.title) if can_view else u'private component',	NodeProjectCollector to_expand = False return { 'kind': FOLDER, 'category': node.category,
# todo: remove this ``expectedfailure`` </s> self.assertlistequal(data2, [])	test_successful_read_transaction data2 = list(Test.objects.all())
# update new uniqueid kodi 17 - todo get uniqueid_id for updates from embydb </s> if self.kodi_version > 16:	add_update ratingid =  self.kodi_db.create_entry_rating() self.kodi_db.add_ratings(ratingid, movieid, "movie", "default", rating, votecount) uniqueid =  self.kodi_db.create_entry_uniqueid() self.kodi_db.add_uniqueid(uniqueid, movieid, "movie", imdb, "imdb")
# todo: look at the model to see which revision is last. </s> offset = url_for(controller='revision', action='list')	test_list_long self.create_100_revisions() try: res = self.app.get(offset) self.assert_click(res, '2', 'Revision 2')
#todo check that fn is callable </s> wrapped = partial(fn, **opts)	dispatcher else: pass update_wrapper(wrapped, fn) return wrapped
# todo-blocker (rtibbles): hook into unit settings/front end parameterization to replace '8'. </s> exercise["basepoints"] = settings.unit_points/(len(current_unit_exercises)*(8 + settings.fixed_block_exercises + settings.quiz_repeats))	get_exercise_data exercise["basepoints"] = 0 else: return exercise
# todo make this a private api </s> places = []	parse_code @staticmethod def parse_code(results): for result in results.get('RESULTS'): coordinates = result.get('COORDINATES', {})
# todo: adjust the childrenrect for axis, labels and legends </s> rx = (px - min_x) * self.childrenrect().width() / (max_x - min_x)	map_to_graph_cart min_x, max_x = self.data_range[axes[0]] min_y, max_y = self.data_range[axes[1]] ry = -(py - min_y) * self.childrenRect().height() / (max_y - min_y) return (rx, ry)
# todo-me return object </s> except exception as e:	sort_by_dates if ad_month == today.month: return [[video.ratingKey] + [str(video.originallyAvailableAt)]] return
raise notimplementederror  # todo </s> pass	sort def sort(self):
# @todo: move this field from this core table </s> pr_age_group(readable = false,	S3PersonModel past = 1320,  # Months, so 110 years ), writable = False, ),
pass # todo </s> def _swapid(self, songid1, songid2):	_swapid @register(r'^swapid (?P<songid1>\S+) (?P<songid2>\S+)$')
# @todo: remove these when we're certain that s3_get_foreign_key finds these properly </s> if table._tablename == "auth_user":	lookahead ktablename, key, multiple = s3_get_foreign_key(table[field]) if not ktablename: if field == "organisation_id": ktablename = "org_organisation"
# todo: exc_info. </s> chained.set_exception(error)	_connected_callback future.result() except Exception as error: else: if not self._get_member():
# todo: find a better random value </s> return datetime.datetime.now()	auto_value def auto_value(prop): if prop.type == datetime.datetime: elif prop.type == datetime.date: return datetime.date.today()
#todo change to native framework call, when plex allows token in header </s> request = urllib2.request(self.getlistsurl, headers=myheader)	LIST myHeader = {} myHeader['X-Plex-Token'] = users[user]['accessToken'] playlists = XML.ElementFromString(urllib2.urlopen(request).read()) result = {}
# todo verify results </s> self.assertsimpleerror("get request must contain a 'query' parameter", 400)	test_get_noquery b"/", )
# todo this is a bit jankey to be honest </s> apps.app_configs = ordereddict()	_reload_apps def _reload_apps(self): apps.apps_ready = apps.models_ready = apps.loading = apps.ready = False apps.clear_cache()
# todo implement for all channels </s> def _handle_toggle(self, message):	_handle_toggle return None
# todo fix this </s> return 'attachment' + filename.split('[')[1][:7]	identifier def identifier(filename):
# todo: refactor this method. </s> if isinstance(event, event):	waitEvent def waitEvent(self, event, *channels, **kwargs):  # noqa event_object = event event_name = event.name
# todo(crcrpar): support botorch v0.4.0. </s> "botorch<0.4.0 ; python_version>'3.6'",	get_extras_require "torchaudio==0.7.2", "allennlp<2.0.0", "fastai", ],
# todo: show message. </s> return none	_makeArchive except (IOError, OSError, BadZipfile) as error: Logger.log("e", "Could not create archive from user data directory: %s", error)
# todo: send email here? </s> except objectdoesnotexist as e:	participation_status 'reason': reason } resp = { 'status' : 400
sage: k.pari_nf() # not tested # todo: pari-2.13.0 </s> [y^2 - 100000000000000000000...]	pari_nf Since the discriminant is square-free, this also works:: sage: K.<a> = NumberField(x^2 - p*q, assume_disc_small=True) try: return self._pari_nf
# todo: must be implemented </s> pass	range_from_chapters def range_from_chapters(crawler, times=0):
# todo: do we need to skip config.add_slack variable here? </s> var_filter = (lambda v: v.is_integer()) if discrete_only \	generate_norm_inf_objective_function discrete_only: Bool only optimize on distance between the discrete variables else (lambda v: v.name != 'MindtPy_utils.objective_value' and 'MindtPy_utils.MindtPy_feas.slack_var' not in v.name)
@jtu.skip_on_devices("tpu")  # todo(mattjj, pfau): fails on tpu. </s> def testlugradofnonsquaresingularmatrix(self, matidx):	testLuGradOfNonSquareSingularMatrix {"testcase_name": "_matidx={}".format(matidx), "matidx": matidx} for matidx in range(2))) mat = nonsquare_singular_mats[matidx] jtu.check_grads(jsp.linalg.lu, (mat,), 1, atol=1e-1, rtol=1e-1)
pass # todo </s> def view(self, p_view):	view @view.setter
# todo: add a named tuple/dict version </s> return "%s{%s}" % (struct.__class__.__name__, ", ".join(fields))	toString fields = [field + "=" + str(getattr(struct, field)) for (field, _) in struct._fields_]
# todo: check / store delta. </s> create = true	extract_primitives n_morph = (temp_normals[0], temp_normals[1], temp_normals[2]) target_normals.append(convert_swizzle_location(n_morph)) for current_new_index in vertex_index_to_new_indices[vertex_index]: found = True
# todo also check for motion codec parameter support </s> return 'h264_omx' in codecs	has_h264_omx_support if not binary: return False
# todo: this check may hide a bug a should be removed. </s> if isodate is none:	ISO8601_to_day_str def ISO8601_to_day_str(isodate, tz=0): print a ISO8601 in DD/MM/YYYY formatted str isodate = datetime_null().isoformat() date = datetime(year=int(isodate[0:4]),
# todo(ytknzw): add more specific assertion with the test case. </s> study = create_study()	test_plot_intermediate_values trial.report(2.0, step=1) return 0.0 study.optimize(lambda t: objective(t, True), n_trials=1) figure = plot_intermediate_values(study)
# todo need copy? </s> x = mesh.node_coords.copy()	get_new_points def get_new_points(mesh): cells = mesh.cells["nodes"] jac_x = jac_uniform(x, cells)
return {} # todo </s> else:	handle_event return {} # TODO elif method == 'mining.get_temperature': log.error("Unhandled method %s with params %s" % (method, params))
# todo: fixme-  assumes only one topic (next two lines) </s> self.context.currentnode = self.topology.sources[0]	process return False record = self.queue.get() self.topology.sources[0].process(record.key(), record.value())
# todo: test event is in response </s> self.assertequal(self.response.status_code, 200)	test_occurences_api_works_with_and_without_cal_slug )
# todo(philday): add support for timeout (clean shutdown) </s> return self._migrationops.migrate_disk_and_power_off(context,	migrate_disk_and_power_off block_device_info=None, timeout=0, retry_interval=0): instance, dest, flavor,
'''todo: add docs''' </s> setattr(self.model, model_field, new)	on_change def on_change(self, attr, old, new, model_field): self.update_app()
# todo set only zero order </s> vals = nm.repeat(fun, nods.shape[0])	set_volume_dofs elif isinstance(fun, nm.ndarray): assert_(len(fun) == dpn) elif callable(fun): qp, weights = self.integral.get_qp(self.gel.name)
"deaths": "",  # todo: fix </s> "notes": "",	convert_row "discarded": new["casos_descartados"], "suspect": new["casos_suspeitos"], "source_url": new["boletim_url"],
#todo: check for continous or discrete, only continuous supported right now </s> dico = 'c'	gram Wc = gram(sys,'c') Wo = gram(sys,'o') D,V = np.linalg.eig(sys.A)
# todo: not actually sure this can ever happen. </s> self._ssl_want_read = true	send return super(SSLConnection, self).send(data) except SSL.WantReadError: debug("call: send(), err: want-read", inst=self) return 0
# todo: django 2.0: remove </s> return ''	get_full_name def get_full_name(self) -> str:
#        todo: need more info about log in procedure in game </s> elif not emulate:	__launch__ sku = 'FUT18AND' clientVersion = 21
# todo: this is repeated from flowdir </s> a = np.arange(dem.size)	raise_nondraining_flats ignore_metadata=ignore_metadata, metadata=metadata, **kwargs) dem_mask = np.where(dem.ravel() == nodata_in)[0] top = np.arange(dem.shape[1])[1:-1] left = np.arange(0, dem.size, dem.shape[1])
1  # todo: fill in identifier </s> )	profile_transfer amount, target, result.wait() profiling.print_all_threads()
# todo: implement the shit herein instead of collectionrepo </s> return self.repo.get_handles(self.branch)	CollectionRepoBranchBackend self.branch = branch def get_handles(self): def get_collection(self): return self.repo.get_collection(self.branch)
# todo: test this </s> self.logger.info(f"forwarded htlc has failed, {reason}")	Peer self.logger.info("htlc forwarded successfully") else: await self.fail_htlc(chan, htlc.htlc_id, onion_packet, reason) @log_exceptions
# todo(yamahata): creating volume simultaneously </s> volume_api.wait_creation(context, vol['id'])	_setup_block_device_mapping vol = volume_api.create(context, bdm['volume_size'], bdm['snapshot_id'], '', '') self.db.block_device_mapping_update( context, bdm['id'], {'volume_id': vol['id']})
# todo: for now, circumnavigate the detached head issue. </s> for subds in source.get_dataset_handles(recursive=true):	test_publish_simple def test_publish_simple(origin, src_path, dst_path): source = install(path=src_path, source=origin, recursive=True) AnnexRepo(opj(src_path, subds), init=True, create=False).git_checkout("master") target = GitRepo(dst_path, create=True)
# todo: the orderer node url needs to be fixed. </s> orderer_url=orderer.urls,	create_channel peer_channel_cli.create( channel=name, channel_tx=tx_path, orderer_tls_rootcert=rootcert
# todo: fix clone issue </s> assert_array_less(-0.1, pred_ranks)	test_predict_rank_normalized assert_array_less(pred_ranks, 1.01)
#todo_ismeal_quesataion: i prefer get_* for getters </s> if not specs:	quantum_elements def quantum_elements(self, specs=None): specs = self.get_specs() return self.__init_circuit, \
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_macintosh_platform_names_matches_windows_platform_success def test_result_macintosh_platform_names_matches_windows_platform_success(self): self.assertInSuccess('test_macintosh_platform_names_matches_windows_platform', run_set(p, 'result'))
# ^ todo: uncomment once federated ursula status pages work and remove skip (below) </s> @pytest.mark.skip("need to be modified to correctly use dash[testing] and only run on circleci")	test_render_ursula_status_page_with_known_nodes def test_render_ursula_status_page_with_known_nodes(federated_ursulas, dash_duo): ursula_config = UrsulaConfiguration(dev_mode=True, federated_only=True, known_nodes=federated_ursulas)
# todo: given trial, a repo, and an observer </s> repo: hyperparamsjsonrepository = hyperparamsjsonrepository()	test_hyperparams_json_repository_should_be_observable_in_memory def test_hyperparams_json_repository_should_be_observable_in_memory(): pass
# todo: refactor linodeexception, args[0] should be error_id </s> if e.args[0] == 5:	list_records data = self.connection.request(API_ROOT, params=params).objects[0] except LinodeException, e: raise ZoneDoesNotExistError(value='', driver=self, zone_id=zone.id)
# todo: add safety tests so that when something fails it fails with a good error </s> path = none	get_path def get_path(prompt): if 'ref' in prompt.attrib: path = prompt.attrib['ref']
# todo: configurable timeout </s> kill_timeout = 3	_stop def _stop(self, container): self.docker_client.stop(container.container_id, timeout=kill_timeout)
# todo: deal with scroll position </s> @objc.python_method	FGMainWindowController newWidth = max(newWidth, fontItem.minimumWidth) self._fontGroup.width = newWidth def setFontItemText(self, fontKey, fontItem, txt, isSelectedFont): fontPath, fontNumber = fontKey
# todo: finish </s> raise exception.notimplementederror	post_validate def post_validate(self, runner_context):
# fixme: todo </s> entry = entry._replace(postings=new_postings)	rebook_as_fifo new_postings.append(posting._replace( position=position.Position(new_lot, pos.number))) new_entries.append(entry) return new_entries, errors
# todo: deal with error </s> self.ttfont = none	UFOFont fontData, error = await compileUFOToBytes(self._fontPath, outputWriter) if fontData is None: self.shaper = None else:
# todo: use the device specific template here </s> return build_template_regexp(self.default_save_template)	build_template_regexp def build_template_regexp(self): from calibre.devices import build_template_regexp
# todo cid_map?? </s> assert any('victoria park' in c.summary for c in checkins)	test_checkins assert len(checkins) > 100
#todo: redo this with html parser instead of regex </s> def parse_page(res):	parse_page found = re.findall(parser_regex, res) no_results = re.search(r'No hits\. Try adding an asterisk in '
# todo[yanndupis]: get rid of these torch references when extending hook_args </s> response = sy.frameworks.torch.hook_args.hook_response(	handle_func_command new_command = (cmd, None, new_args_worker, new_kwargs) results[worker] = new_type.handle_func_command(new_command) cmd, results, wrap_type=cls, wrap_args=tensor.get_class_attributes() )
# todo do something with temp </s> self._remove_substates_from_subhmms()	HSMMSubHMMStates super(HSMMSubHMMStates,self).clear_caches() def resample(self,temp=None): super(HSMMSubHMMStates,self).resample() # resamples superstates self._resample_substates()
# todo: extend to other types </s> assert arr_typ == types.array(types.int64, 1, 'c'), "only in64 for nunique"	nunique_overload_parallel @overload(nunique_parallel) def nunique_overload_parallel(arr_typ): sum_op = hpat.distributed_api.Reduce_Type.Sum.value def nunique_par(A):
# todo: experiment with when to apply conv </s> note_octave = timedistributed(conv1d(octave_units, 2 * octave, padding='same'))(notes)	time_axis def f(notes, beat, style): time_steps = int(notes.get_shape()[1]) note_octave = Activation('tanh')(note_octave) note_octave = Dropout(dropout)(note_octave)
# todo: re-enable for hardware </s> for portno, config in list(interfaces_config.items()):	add_dp name, dp_config, port, interfaces_config, i, dpid_count, stack, n_tagged, tagged_vid, n_untagged, untagged_vid) stack = config.get('stack', None) if stack:
# todo ... </s> pass	_cpre3_handle_token def _cpre3_handle_token(self, stateStruct, token):
## todo: # fixme: remove me </s> paste_children = self.r_serv_metadata.smembers('paste_children:{}'.format(father))	get_all_domain_son paste_parent = father.replace(self.paste_directory+'/', '') paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(paste_parent)) paste_childrens = paste_childrens | paste_children for children in paste_childrens:
# todo: check that the birth date is not in the future </s> except valueerror, e:	is_valid try: birth_date = _get_birth_date(number) return False if len(number) == 10:
# todo make os portable for windows users </s> if not project_prefix.endswith("/"):	addfile file_exists = True project_prefix = self.project_dir project_prefix += "/" if sys.hexversion < 0x2060000:
# todo: distinguish between urllib and urllib2 contracts </s> return updater.__updaters.get( parsed_url.hostname )	get_updater parsed_url = urlparse.urlparse( url )
# todo: if the user supplied the full nonsymbolic image_shape and </s> rval = conv3d_fft(x, f)	_gpu_conv3d_to_fftconv f = node.inputs[1] f = gpu_from_host(f.dimshuffle(0, 4, 1, 2, 3)) rval = gpu_from_host(rval.dimshuffle(0, 2, 3, 4, 1))
# todo: fix signature of zip() in typeshed. </s> types, declared_types = cast(any, zip)(*clean_items)	check_multi_assignment_from_union assert declared_type is not None clean_items.append((type, declared_type)) self.binder.assign_type(expr, make_simplified_union(list(types)),
#todo - handle this with a pipe? </s> filename = "emboss/temp_%s.txt" % temp_format	check_SeqIO_to_EMBOSS if temp_format in skip_formats : continue temp_handle = open(filename,"w") SeqIO.write(records, temp_handle, temp_format)
# todo: why are we uncommenting html? </s> self.html = utils.make_html_element(	_parse_html def _parse_html(self): self.response.text.replace('<!--', '').replace('-->', ''), url=self.response.url, )
# todo: logs which are observably relevant should be sent to the client (e.g. the warning of refusing to have more receivers active) </s> logging.basicconfig(level=logging.info)	main def main(argv=sys.argv, _abort_for_test=False): log.startLoggingWithObserver(log.PythonLoggingObserver(loggerName='shinysdr').emit, False) argParser = argparse.ArgumentParser(prog=argv[0])
# todo: enumerate the axes in a message </s> if self.parsed_hdrs.has_key('date'):	checkCaching if "host" in vary: self.setMessage('header-vary', rs.VARY_HOST) apparent_age = max(0, int(self.timestamp - self.parsed_hdrs['date']))
# todo: check if page are exists </s> fd, filename = tempfile.mkstemp('w')	test_confluence_attach_file_2 space = 'SAN' title = 'atlassian-python-rest-api-wrapper' os.write(fd, b'Hello World - Version 1') name = os.path.basename(tempfile.mktemp())+".txt"
# todo: optimize </s> signatures = script.call_signatures()	_get_signature_text def _get_signature_text(self,script): signature_text = '' logger.info('signatures: %s', signatures) if len(signatures)>0:
# todo: remove this - cura-4482 </s> material = self._global_container_stack.material	_onGlobalContainerChanged except TypeError: pass material.nameChanged.disconnect(self._onMaterialNameChanged) quality = self._global_container_stack.quality
# todo: progress +kwargs </s> else:	pull GIT_SSH_COMMAND="ssh -S %s" % cnct.ctrl_path): remote.pull(refspec=refspec, progress=progress) remote.pull(refspec=refspec, progress=progress)
# todo: ... </s> pass	pull_child_entities This fulfills the third requirement of `DHIS2 Integration`_. .. _DHIS2 Integration: https://www.dropbox.com/s/8djk1vh797t6cmt/WV Sri Lanka Detailed Requirements.docx
# todo: (longer term) rather than abort, reject this candidate </s> assert (	_prepare self._dist = abstract_dist.get_pkg_resources_distribution() assert self._dist is not None, "Distribution already installed" self._name is None or self._name == canonicalize_name(self._dist.project_name)
# todo: dump content out for debugging in the future. </s> raise	func_wrapper for iframe in iframes: pass
# todo: remove with neuralnet.__setstate_050__ </s> return os.path.join('skorch', 'tests', 'model_0.5.0.pkl')	pickled_0_5_0_model_path
# todo: verify that workid is the primary key someplace. </s> yield workitem.dowork()	perform workItemClass = WorkItem.forTable(tableSyntax) workItem = yield workItemClass.load(workID)
parts.append(fmt[f:f_next])  # todo backslash-escapes, at least \n </s> f = f_next	Printf if f_next < 0: f_next = len(fmt) if f >= len(fmt): if v >= len(vals):
# todo: error detection </s> __connect()	serialize_item def serialize_item(obj, item): collection = mongodb[obj.collection_type()] data = collection.find_one({'name':item.name})
pass # todo </s> def try_undo(self, *args):	try_undo
# todo: uncomment this line once we're on django 1.11 </s> "email_count__avg": avg("email_count"),	statistics } inbox_aggregate = { "email_count__sum": Sum("email_count"), "email_count__min": Min("email_count"),
# todo what should the swissnum _actually_ be? </s> self.http_server = httpserver(self.storage_server, swissnum_for_test)	_setUp self.tempdir = self.useFixture(TempDir()) self.storage_server = StorageServer(self.tempdir.path, b"\x00" * 20) self.client = StorageClient( DecodedURL.from_text("http://127.0.0.1"),
# todo: hide and stop spinner </s> self.nmwidget.hbox.set_sensitive(false)	wireless_toggled self.state_changed(None, self.state) else:
# todo add code </s> p = poll.objects.get()	test_delete def test_delete(self): p.delete() self.assertEqual(Poll.objects.count(), 0)
# todo debug </s> print sensoralerttohandle	run print "check of sensor alerts with rules NOW" for sensorAlertToHandle in list(sensorAlertsToHandleWithRules): sensorAlertList = sensorAlertToHandle[0] alertLevel = sensorAlertToHandle[1]
v_plus = self.probe_dtz_no_ep(board) # todo: change to probe_dtz </s> board.pop()	probe_dtz_no_ep board.pop() continue if v_plus is None: return None
# todo: typing for pb </s> return pb_msg	to_pb )
# todo: set cookie </s> theme = settings['theme']	theme_loader theme = cookies['theme'].value if not os.path.isdir(os.path.join(THEME_LOC, theme)): except KeyError: theme = SETTINGS['Theme']
# todo: start here </s> pass	_fleet_for_same_role_satisfies def _fleet_for_same_role_satisfies(actual_fleet, req_fleet):
#todo test this </s> for op, v in op_poly_dict.items():	sum_ops_to_dtypes def sum_ops_to_dtypes(op_poly_dict): result = {} new_key = op.dtype if new_key in result:
# todo: use cli.output.write </s> print(result)	process_python_input result = eval(text, _globals, _locals) if result: except SyntaxError: exec(text, _globals, _locals)
assert study_id == 0  # todo </s> return copy.deepcopy(self.trials[trial_id])	get_trial def get_trial(self, study_id, trial_id):
# todo: lacp timeout configurable. </s> if lacp_age > 10:	_lacp_state_expire for port in lacp_up_ports: lacp_age = now - port.dyn_lacp_updated_time self.logger.info('LACP on %s expired' % port) ofmsgs.extend(self.lacp_down(port))
# todo: we probably don't need this? just to be safe </s> for i in range(len(norm_groups)):	step scale=self.cur_scale, grad_norms=norm_groups) updated_params = _unflatten_dense_tensors(self.fp16_groups_flat[i], self.fp16_groups[i]) for p,q in zip(self.fp16_groups[i], updated_params):
# todo: action value doesn't exist for beta </s> self.unittest(	test_early_horizon_estimate baseline_objective = 'policy_gradient' baseline_optimizer = 'adam' exclude_bounded_action=True, reward_estimation=reward_estimation, baseline_objective=baseline_objective, baseline_optimizer=baseline_optimizer
# todo: distinguish between text elements with actual whitespace </s> content = body_children[1].get_text(' ', strip=true)	m_html_timeline_to_objects story_body_container = story.find(class_='story_body_container') body_children = cls._divs(story_body_container) footer = story_body_container.find_next_sibling('div') footer_children = cls._divs(footer)
# todo: change the frontend to pass seconds instead. </s> expires_at = (now_in_seconds + one_hour_in_seconds) * 1000	test_login_not_active return {'sub': 'Mozilla-LDAP', 'email': test_ldap_user.email, 'exp': id_token_expiration_timestamp} monkeypatch.setattr(AuthBackend, '_get_user_info', userinfo_mock) test_ldap_user.is_active = False test_ldap_user.save()
# :todo: raises systemerror on python 2.6, </s> def testnumpyuint16(self):	Test info=dict(greyscale=True)).save(BytesIO()) if sys.version_info > (2, 6): numpy or self.skipTest("numpy is not available") rows = [[numpy.uint16(x) for x in range(0, 0x10000, 0x5555)]]
# todo: remove this once uses have migrated to that new interface. </s> bq_client = bigquery.client	init 'account in the Kernels Settings sidebar.') return bq_client(*args, **kwargs) bigquery.Client = lambda *args, **kwargs:  monkeypatch_bq( bq_client, *args, **kwargs)
# todo: this is untested. </s> _raise_current_error()	Context method_obj = method_func() if method_obj == _ffi.NULL: context = _lib.SSL_CTX_new(method_obj) if context == _ffi.NULL:
# todo: not clear that this is even used. </s> "mg_alphas": mg_alphas(),	__init__ env={ "GRAPL_LOG_LEVEL": "DEBUG", "JWT_SECRET_ID": secret.secret.arn, "USER_AUTH_TABLE": db.user_auth_table.name,
# todo: remove </s> xml = celementtree.xml(chunk)	XML_to_log_entry def XML_to_log_entry(self, chunk): log_entry = LogEntry() for key, value in xml.attrib.iteritems():
# todo(termie): optimize this call at some point and put it into the </s> roles_ref = []	authenticate tenant_id=tenant_ref['id'], extras=extras_ref) for role_id in extras_ref.get('roles', []): roles_ref.append(self.identity_api.get_role(context, role_id))
# todo use deepcopy() here </s> return polygonsonimage(polys_clean, shape=self.shape)	remove_out_of_image if not poly.is_out_of_image(self.shape, fully=fully, partly=partly) ]
# todo color </s> bottoms.append(	render if command_info.get('arguments'): command_args = [arg['name'] for arg in command_info['arguments']] ("class:bottom-toolbar.on", f"({comamnd_group}) {self.command_holder.command} {command_args}") )
# todo: use google-diff-patch-match library to diff the sources? </s> return diff(a, b)	diff_source def diff_source(a, b, compare="ignored"): "Diff a pair of sources."
direct = false  # todo: test on undirect, but too long atm </s> annex = annexrepo(repo_path, create=true, direct=direct)	test_add_archive_content_tar '1.dat': 'load2'}}) def test_add_archive_content_tar(repo_path): annex.annex_add('1.tar') annex.commit(msg="added 1.tar")
# todo: do the computation without the 'sr' enforcement </s> try:	inverse cinv_scal[(i,j)] = dom.scalar_field() for chart in dom.top_charts(): gmat = matrix( [[self.comp(frame)[i, j, chart].expr(method='SR')
# todo check </s> return self.mimetype	format @interfacedoc def format(self):
# todo: also use backward pass </s> hs = fws	get_selective_model ) fws = [states[1][0][1] for states in rnn_result] h = tf.concat(1, hs) logits_flat = tf.contrib.layers.linear(h, 5*target_size)
# todo: figure out a way to actually log this information without </s> self.terminate()	do_SIGINT def do_SIGINT(self, sig, stack):
# todo: implement </s> pass	remove_directory real_path = path if relative_to is None else os.path.join(relative_to, path) if recursive: else: gio.File(real_path).delete()
# todo: add this back in once we've merged back the refactored users code </s> populate_user_from_commcare_submission(self.sender, self.xform)	testUpdatePhoneNumberFromFormSubmission couch_user.create_commcare_user(self.xform.domain, self.username, 'password') couch_user.save() all_users = CouchUser.view("users/all_users") user = all_users.first()
# todo: tighten foolscap schema to require exactly 32 bytes. </s> self._write_file('plaintext_hashes', ''.join(hashes))	remote_put_plaintext_hashes def remote_put_plaintext_hashes(self, hashes): precondition(not self.closed)
# todo(harlowja): is there a better way to do this?? </s> return fs[path]	_get_item_path @staticmethod def _get_item_path(fs, path):
# todo check validity </s> self.content.append(content)	add_content def add_content(self,content):
# todo find if genesis block can be non-zero. why does 'earliest' option even exist? </s> at_header = chain.get_canonical_block_by_number(0).header	state_at_block at_header = chain.get_canonical_head() elif at_block == 'earliest': else: at_header = chain.get_canonical_block_by_number(hex_to_int(at_block)).header
# todo(tsileo): handle tombstone </s> if not is_api_request():	outbox_activity_likes @app.route('/outbox/<item_id>/likes') def outbox_activity_likes(item_id): abort(404) data = DB.outbox.find_one({'id': item_id, 'meta.deleted': False})
# todo: duplicate checking </s> self._disassociate.append((disassociation_type, value))	remove def remove(self, disassociation_type: LicenseDisassociationType, value: str):
# todo: how to check it? meybe we can omit this test </s> pass	test_uniformintfill f_ng = importer.get_op_handle("Y") ngt.make_transformer().computation(f_ng)()
# todo : pytest.mark.parametrise once nose is gone. </s> def test_unescape_glob():	test_unescape_glob assert path.unescape_glob(r"\*\[\!\]\?") == "*[!]?" assert path.unescape_glob(r"\\*") == r"\*"
# todo: do this when we want to switch off ctrl-c </s> shutit.get_default_child().sendline(r'')	ctrl_c_signal_handler print "You may need to wait for the command to complete for a pause point" shutit.cfg['build']['ctrlc_stop'] = True return print "CTRL-c twice to quit."
# todo add support for dynamically picking version of </s> version = 0	_find_coordinator_id_send_request name as a string. :return: A message future if version <= 0: request = GroupCoordinatorRequest[version](group_id)
# todo: create a lint plugin for that to enforce using the helper </s> password='password')	setUp self.client.login(username=self.tester.username,  # nosec:B106:hardcoded_password_funcarg
# todo implement. </s> model.train_on_batch(x, y)	AsynchronousEAMSGDWorker if self.iteration % self.communication_window == 0: pass self.iteration += 1 except StopIteration:
last = 50 if 'last' not in parameters else int(parameters['last'][0])  # todo check integer! </s> sort_by = none if 'sort_by' not in parameters else parameters['sort_by'][0]  # todo check integer!	sanitize_parameters Sanitize the parameters and check whether they exist first = 1 if 'first' not in parameters else int(parameters['first'][0])  # TODO check integer! sort_asc = True if 'sort_asc' not in parameters else bool(int(parameters['sort_asc'][0])) query_filter = None if 'filter' not in parameters else parameters['filter'][0]
# todo remove get_media_references </s> multimedia = app.get_media_references()	multimedia_ajax if app.get_doc_type() == 'Application': try: except ProcessTimedOut: notify_exception(request)
# todo: decompose=true </s> assert_allclose(new_wp.data, x, rtol=1e-12)	test_data_reconstruction_delete_nodes_nd assert_allclose(new_wp.reconstruct(update=True), x, rtol=1e-12)
# todo: move to base class </s> self.resetmatrix()	resetScale def resetScale(self):
# todo: no idea of how to check correct logging via any kind of assertion yet. </s> lgr = logging.getlogger('datalad.cmd')	test_runner_log_stdout def test_runner_log_stdout(): level_old = lgr.getEffectiveLevel() lgr.setLevel(logging.DEBUG)
# todo verify layer exists in geoserver? </s> name = data.name if name is none else name	add_layer if isinstance(data, RasterData): layer = SimpleLayer(name, self._remote, data=data, vis_url=vis_url, **kwargs) elif isinstance(data, RasterDataCollection):
# todo: this needs test coverage. </s> raise nodataafterdate()	HDF5DailyBarReader raise NoDataBeforeDate() if dt.asm8 > self.asset_end_dates[sid_ix]: return value def get_last_traded_dt(self, asset, dt):
# todo(sbauza): remove the service_id filter in a later release </s> _filter = or_(models.service.host == models.computenode.host,	compute_node_statistics def compute_node_statistics(context): models.Service.id == models.ComputeNode.service_id) result = model_query(context,
# todo: it would be more helpful just to quietly recreate the data source config from get params </s> return none	report_config_table return None except DataSourceConfigurationNotFoundError: else: return table
# todo make this a private api </s> places = []	parse_code @staticmethod def parse_code(results): for result in results.get('RESULTS'): coordinates = result.get('COORDINATES', {})
# todo: assert </s> systems = self.remote.get_systems(self.token)	test_create_system def test_create_system(self): Test: create/edit a system object system = self.remote.new_system(self.token) self.assertTrue(self.remote.modify_system(system, "name", "testsystem0", self.token))
# todo: cronjob (celery task) to delete stale tokens </s> except emailconfirmation.doesnotexist:	confirm_email email_confirmation = EmailConfirmation.objects.get( pk=pk, token=token, valid_until__gte=now()) return TemplateResponse(request, 'registration/invalid_token.html') proceed = False
raise valueerror("bucket might not exist")  # todo: create custom exception for easier handling </s> perm_write = permission.unknown	check_perm_write def check_perm_write(self, bucket): if bucket.exists != BucketExists.YES: timestamp_file = str(datetime.datetime.now().timestamp()) + '.txt' try:
# todo: remove this once all dependent code has been cleaned up </s> self.output_quantizer = self.output_quantizers[0]	QcQuantizeWrapper is_symmetric, enabled_by_default=is_output_quantized)] self._mode = QcQuantizeOpMode.PASSTHROUGH self._module_to_wrap = module_to_wrap
# todo: test, to be sure it doesn't mess things up </s> with open("/proc/sys/kernel/sched_child_runs_first") as f:	_perform_env_checks l.error("AFL Error: Suboptimal CPU scaling governor") raise InstallError("execute 'cd /sys/devices/system/cpu; echo performance | sudo tee cpu*/cpufreq/scaling_governor'") if not "1" in f.read(): l.error("AFL Warning: We probably want the fork() children to run first")
# todo(phawkins): enable when there is an lu implementation for gpu/tpu. </s> @jtu.skip_on_devices("gpu", "tpu")	testLuGrad for dtype in float_types() | complex_types() for rng in [jtu.rand_default()])) def testLuGrad(self, shape, dtype, rng): if not hasattr(lapack, "jax_getrf"):
# @todo: make a lookup table in diseasedatamodel: </s> field("probe_type"),	CaseTrackingModel table = define_table(tablename, case_id(), Field("probe_number", length = 64, unique = True, ),
self.pos_emb = posencoding(max_seq_len * 10, d_model) # todo: *10 fix </s> self.dropout_emb = nn.dropout(dropout)	Encoder self.d_model = d_model self.src_emb = nn.Embedding(src_vocab_size, d_model, padding_idx=data_utils.PAD,) self.layer_type = EncoderLayer if not weighted else WeightedEncoderLayer self.layers = nn.ModuleList(
# todo docstring </s> fake_gear = fakegearclient(units={})	test_proxy_needs_deleting def test_proxy_needs_deleting(self): api = Deployer(create_volume_service(self), gear_client=fake_gear) expected_destination_port = 1001
#invalidate any overlapping cached value #todo extract remaining valid bits </s> p = where	new_method value = get(obj, where, size,**kw_args) if isconcrete(where): while p <= where+size/8: if p in used:
# todo. check if build_url_fname can be used. </s> newpath = "/".join(['..']*3 + [newpath])	display_first_image_as_thumbnail self.report.add_lnkref_to_photo(photo, lnkref) real_path, newpath = self.report.prepare_copy_media(photo) if constfunc.win(): newpath = newpath.replace('\\',"/")
# todo: command+c for mac </s> tk.messagebox.showerror("internal error. use ctrl+c to copy",	_init_commands self.bind_all("<"+sequence+">", lambda e, cmd=item: cmd.execute(e), "+") except: traceback.format_exc()) if item.cmd_id == "step":
# todo: connect to actual event in playlist </s> self._emit_propchange('org.mpris.mediaplayer2.player', {	LoopStatus playlist = xl.player.QUEUE.current_playlist playlist.set_repeat_mode(state) 'LoopStatus': value,
# todo(lyarwood): remove the following in 16.0.0 pike </s> self._test_get_encryptor('cryptsetupencryptor',	test_get_encryptors self._test_get_encryptor('plain', cryptsetup.CryptsetupEncryptor) cryptsetup.CryptsetupEncryptor) self._test_get_encryptor(
#todo - introduce an annotated alignment class? </s> alignment._annotations = annotations	MafIterator assert len(records) == seq_count alignment = MultipleSeqAlignment(records, alphabet) yield alignment in_a_bundle = False
# todo: this is not the most efficient approach - refactor this functionality into util </s> resource_db = self.to_model(self)	get_pack_uid def get_pack_uid(self): pack_uid = resource_db.get_pack_uid() return pack_uid
raise notimplementederror  # todo </s> @asyncio.coroutine	XBoardProtocol @asyncio.coroutine def play(self): def analysis(self): raise NotImplementedError  # TODO
constant_liar: bool = false,  # todo(hvy): remove default value and fix unit tests. </s> ) -> tuple[dict[str, list[optional[float]]], list[tuple[float, float]]]:	_get_multivariate_observation_pairs study: Study, param_names: List[str], sign = 1 if study.direction == StudyDirection.MAXIMIZE:
#todo(cp16net): need to set the return code correctly </s> return wsgi.result(views.instancesview(servers).data(), 201)	index def index(self, req, tenant_id): servers = models.Instances(req.headers["X-Auth-Token"]).data()
# todo ... </s> pass	_cpre3_handle_token elif self._state == 6: # after expr + op
# xxx todo </s> if args.has_key('done') and args['done']:	__init__ DnsRequest.__init__(self, *name, **args) asyncore.dispatcher_with_send.__init__(self, *name, **args) self.donefunc = args['done'] else:
# todo error #248 </s> return package	fetch_metadata username, repo_name = package.repo_name().split('/') except ValueError: repo = self.github.repository(username, repo_name) if repo is None:
#todo 接上 qa bot </s> pass	getResponseForGeneralQA Listen user's input and return a response which is based on our knowledge base.
#todo add types and return and rtype </s> if len(self.docs['in']['params']) > 0:	_set_params def _set_params(self): self.docs['out']['params'] = list(self.docs['in']['params']) for e in self.element['params']:
# todo: use a bytearray? </s> buf = b''	iter_lines_buffered buffer. gap = len(sep) data = initial while True:
# todo(termie): this stuff should probably be moved to middleware </s> if not context['is_admin']:	validate_token def validate_token(self, context, token_id, belongs_to=None): Optionally, also ensure that it is owned by a specific tenant. user_token_ref = self.token_api.get_token(context['token_id']) creds = user_token_ref['extras'].copy()
# todo: account for conversion if tz other than gmt not specified </s> return d.strftime('%y%m%d%h%m%s')	iso8601ToDigits14 d = datetime.datetime.strptime(iso8601DateString, "%Y-%m-%dT%H:%M:%SZ")
# todo add binary column (drop support for python 2.7) </s> df = pd.dataframe({	test_generate_ddl def test_generate_ddl(self): 'col_int': np.int32([1]), 'col_bigint': np.int64([12345]),
#todo: add index to conc_call_agg </s> conc_call_agg.insert(	set_concurrentcall_analytic }) if not get_cc_obj: { "date": date_minprec,
# todo: remove getattr when https://github.com/rtfd/readthedocs.org/pull/3339 got merged </s> build_image = getattr(self.config, 'build_image', self.version.project.container_image) or docker_image	save_environment_json def save_environment_json(self): Save on disk Python and build image versions used to create the venv. data = { 'python': {
#todo show this </s> return ret	nodes ret.append(node.owner)
hda.dataset.job_id = job.id  # todo: can't add attr to dataset in __init__(). why? </s> trans.sa_session.add(hda.dataset)	store_dataset_job_id for hda in hdas: if type(hda) == 'HistoryDatasetAssociation':  # temporary/debugging trans.sa_session.flush()  # TODO: do we need this here? Or let the next flush handle this?
#todo: issue warning that this is an unsafe operation, but doing it cause user insists </s> try:	_unsafe_writes def _unsafe_writes(self, src, dest, exception): if exception.errno == errno.EBUSY: try: out_dest = open(dest, 'wb')
# todo: write me </s> raise notimplementederror('limiteddisk cache does not yet implement the .remove() method.')	remove def remove(self, layer, coord, format):
# todo(kan-bayashi): documentation and type hint </s> def piecewise_rational_quadratic_transform(	piecewise_rational_quadratic_transform inputs, unnormalized_widths,
# todo: implement me! </s> else:	custom_login else: print('disabled account') # Return an 'invalid login' error message. from django.contrib.auth.forms import AuthenticationForm form = AuthenticationForm()
# todo: should this raise ioerror? </s> mid = read_file(header_one_track + """	test_meta_messages def test_meta_messages(): 4d 54 72 6b  # MTrk 00 00 00 0c  # Chunk size
status = offline # todo: all the cases </s> self.callback_presence(presence(identifier=idd, status=status))	_handle_slack_event status = ONLINE else: elif t == 'message': channel = event['channel']
# todo: remove after py2.5 deprecation </s> if sys.version_info[:2] > (2, 5):	test_tab_2226_tblastn_011 self.assertEqual(0, hsp.query_frame) self.assertEqual(1, hsp.hit_frame) with warnings.catch_warnings(record=True) as w: warnings.simplefilter('always')
# todo(guillermooo): further restrict valid register names. </s> super().__setitem__(key.lower(), value)	__setitem__ if key in ('%', '#'): raise ValueError('invalid register key: %s' % key)
# todo(b/114938612): eventually remove this override. </s> validate=false)	test_preprocessing_fn os.path.join(testdata_path, 'example_gen_output/train/*'), coder=beam.coders.BytesCoder(), | 'DecodeTrainData' >> beam.Map(decoder.decode)) (transformed_examples, transformed_metadata), transform_fn = (
# todo: once/if we have gpu and language labels then we might be </s> filters = {	update else: stop(skip_names=["nginx-proxy", "update-server"]) "label": [ "_orchest_project_uuid",
# todo: use invalidation time </s> params: userfull to dump, mediaid of the profile photo in the db	dump_user def dump_user(self, user_full, photo_id): Returns -, or False if not added""" timestamp = round(time.time())
assert study_id == 0  # todo(akiba) </s> self.trials[trial_id].params_in_internal_repr[param_name] = param_value_in_internal_repr	set_trial_param def set_trial_param(self, study_id, trial_id, param_name, param_value_in_internal_repr): distribution = self.param_distribution[param_name] param_value_actual = distribution.to_external_repr(param_value_in_internal_repr)
# todo: move this test elsewhere. </s> selenium = self.selenium	test_collaborative_article_edition_and_editor_persistence def test_collaborative_article_edition_and_editor_persistence(self): find_element = selenium.find_element_by_css_selector author = ProfileFactory()
# todo: replace with copy function </s> self._pointcloud = deepcopy(pointcloud)	__init__ self._target = target if copy: self._labels_to_masks = deepcopy(labels_to_masks) else:
raise  # todo </s> except requests.exceptions.connectionerror:	remember_node certificate_filepath=certificate_filepath) except SSLError: self.log.info("No Response from known node {}|{}".format(node.rest_interface, node.checksum_public_address)) raise self.UnresponsiveTeacher
#todo fixme: we need to check that we aren't adding a duplicate </s> item.addclaim(claim)	main pywikibot.output('Adding %s --> %s' % (claim.getID(), claim.getTarget().getID()))
output = []  # todo should this be user defined? </s> for obj in objs:	serialize :param role: name of a role to use when serializing :returns: list of serialized objects output.append(self.get_mapper(obj=obj).serialize(role=role)) return output
# todo: this is a jump. </s> vi_cmd_data['motion']['command'] = '_vi_question_mark'	vi_question_mark def vi_question_mark(vi_cmd_data): vi_cmd_data['motion']['args'] = {'mode': vi_cmd_data['mode'], 'count': vi_cmd_data['count'], 'search_string': vi_cmd_data['user_motion_input']} vi_cmd_data['count'] = 1
#todo add movement callbacks to target if movable </s> self.fire_all_weapons(dest)	try_attack_target self.stop()
# xxx todo </s> _have_bgra = false	decode gdiplus.GdipBitmapUnlockBits(bitmap, byref(bitmap_data)) gdiplus.GdipDisposeImage(bitmap) if not _have_BGRA: import re
# todo add something like this in the future, its cleaner than the </s> return names	_sub_modules_dict for module_loader, name, is_pkg in mods: names[name] = SubModuleName(self, name)
# todo pass this to json shcema validation </s> for id_name in data.get('movie_identifiers'):	post 'message': 'list_id %d does not exist' % list_id}, 404 data = request.json if set(id_name.keys()) & set(allowed_ids) == set([]): return {'status': 'error',
# todo deal with pylint error in cleaner fashion than this </s> try:	render_POST if os.name == "nt": shutil.copy(uploaded_file.name, newpath) excp = WindowsError except Exception as e:
# todo(b/134950354): test embedding column for non-eager mode only for now. </s> if not tf.executing_eagerly():	testCombinedFeatureColumnInput state_specs[indicator_key] = tensor_spec.TensorSpec([1], tf.int32) expected_dim += len(vocab_list) embedding_key = 'embedding_key' embedding_dim = 3
# todo(b/130724878): these conversions should not be needed. </s> obj = obj.from_anon_tuple(server_state, 1)	test_save_and_load models.model_fn, server_optimizer_fn=server_optimizer_fn) server_state = iterative_process.initialize() export_dir = os.path.join(self.get_temp_dir(), 'ckpt_1') checkpoint_utils.save(obj, export_dir)
# todo add the ability to `git reset --hard` the dataset tree on failure </s> lgr.info("== command exit (modification check follows) =====")	__call__ except CommandError as e: cmd_exitcode = e.code run_info = { 'cmd': cmd,
# todo: allow answer to repeat previous or go back after a mistake </s> answer = pywikibot.input(u"which variant should be used? (<number>, [n]one, [g]ive up) ").lower()	assemble self.whereReport(page2, indent=8) while True: if answer: if answer == 'g':
# todo: @sbharadwajj implement and test </s> return true	hessian_is_zero def hessian_is_zero(self):
# todo: move this function to user.query </s> return dbsession.query(cls).filter(cls.email_address==email).first()	by_email_address @classmethod def by_email_address(cls, email):
'path': upload.filename,  # todo path? </s> 'urls': {	figshare_upload_file_as_article 'project': node.parent_id, 'node': node._primary_key, 'view': rv['urls']['view'], 'download': rv['urls']['download'],
reference_doctype='todo' and reference_name='{0}'""".format(todo.name)) </s> self.assertfalse(email_queue)	test_feedback_trigger todo.save(ignore_permissions=True) email_queue = frappe.db.sql("""select name from `tabEmail Queue` where frappe.delete_doc("ToDo", todo.name) communications = frappe.get_all("Communication", {
# todo: check syntax </s> return values	allow @GenericHeaderSyntax def allow(self, name, values):
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
# todo test errors </s> assert_array_equal(expect, actual)	test_advanced_indexing_2d_int actual = z[42, ix1]
#todo: add method </s> summaryinfo = {'title': 'movie title', 'year': 2015, 'ids': {'imdb': data['imdbnumber']}}	doManualRating userInfo = {'ratings': {}} elif utilities.isMovie(media_type): userInfo = {'ratings': globals.traktapi.getMovieRatingForUser(data['imdbnumber'])} if summaryInfo is not None:
# todo time complecity </s> bottoms.append(	command_syntax for argument in command_info["arguments"]: command_args.append(argument["name"]) ("class:bottom-toolbar.on", f"({comamnd_group}) {command} {command_args}") )
# todo(john sirois): https://github.com/pantsbuild/pex/issues/1059 </s> pex_root = cast(str, env.pex_root)	_spawn_pip_isolated pip_args.append("--no-cache-dir") command = pip_args + args with ENV.strip().patch( PEX_ROOT=cache or pex_root, PEX_VERBOSE=str(pex_verbosity), **(env or {})
# todo remove? </s> if isinstance(typ, instance) and len(typ.var_args):	iter_content for node in nodes: for typ in self._evaluator.eval_element(node): array = typ.var_args[0] if isinstance(array, ArrayInstance):
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pslldq_1 res = 0x78127486189234569800000000000000 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init)
# todo: test </s> for dn, vh in self.assoc:	choose_virtual_host TODO: This should return vhost of :443 if both 80 and 443 exist This is currently just a very basic demo version if dn == name: return vh
# todo(zhengzhenyu): handle this when the api supports creating </s> if field == 'tags':	get_new_instance instance = objects.Instance(context) for field in self.instance.obj_fields: continue if self.instance.obj_attr_is_set(field):
# todo: may test with codecs.open passing an encoding </s> with open(self.filename, 'rb') as fobj:	test_import_from_xls_fobj def test_import_from_xls_fobj(self): table = rows.import_from_xls(fobj) self.assert_table_equal(table, utils.table)
# todo map relationship backreferences using the django names </s> self.base.prepare(self.engine, reflect=true)	__init__ self.session, self.engine = make_session(self.connection_string) self.Base = automap_base() self.set_all_class_defaults()
# todo: переделать механизм pairs </s> pair = pairs.get(abbr[:2], none)	hayaku_extract return STATIC_ABBR[abbr] starts_properties = [] if pair is not None: starts_properties = [prop for prop in prop_iter if prop.startswith(pair)]
# todo(andym): use mock appropriately here. </s> self.old_version = amo.firefox.latest_version	TestAddonModels if hasattr(Addon, '_feature'): del Addon._feature amo.FIREFOX.latest_version = '3.6.15' def tearDown(self):
#@todo: remove in 0.4.10 </s> def error(self, reason=none, type="parse"):	error raise Fail("%s error%s | Plugin out of date" % (type.capitalize(), ':' + str(reason) if reason else "")) if self.core.debug:
# todo(henry-nash): we should issue an exception here since if </s> versionutils.report_deprecated_feature(	_get_domain_id_from_token 'explicitly include a domain for this resource to ' 'belong to.')) LOG,
# todo: unit test for this check </s> if not isinstance(other, rectangle):	__or__ def __or__(self, other):  # type: (Rectangle) -> Rectangle return NotImplemented x1, y1 = min(self.x, other.x), min(self.y, other.y)
# todo(jaypipes): remove once the pci tracker is always created </s> self.rt.pci_tracker = pci_manager.pcidevtracker(mock.sentinel.ctx)	test_drop_move_claim_on_revert cn = _COMPUTE_NODE_FIXTURES[0].obj_clone() self.rt.compute_nodes[_NODENAME] = cn pci_dev = pci_device.PciDevice.create( None, fake_pci_device.dev_dict)
# todo use location </s> enum = self.parse_coverage_()	parse_enum_ def parse_enum_(self): assert self.is_cur_keyword_("ENUM") self.expect_keyword_("END_ENUM") return enum
# todo: take namespace into account, currently doesn't matter since </s> with session_scope() as db_session:	is_mailing_list_message @namespace_auth def is_mailing_list_message(self, message_id): message = db_session.query(Message).filter(Message.id==message_id).one() return (message.mailing_list_info != None)
# todo: make the min-max values a setting? </s> queue = gst.element_factory_make('queue')	_setup_output process.exit_process() output = gst.Bin('output') queue.set_property('max-size-buffers', 0) queue.set_property('max-size-bytes', 0)
# todo: remove in 21.08 </s> log.warning("this method is deprecated, use phonemefile.load")	load_phonemes Arguments: key (str): Key identifying phoneme cache pho_file = os.path.join( mycroft.util.get_cache_directory("tts/" + self.tts_name),
# todo(mattjj,phawkins): improve this implementation </s> return batched_fun	vmap return tree_unflatten(out_tree(), out_flat)
# todo -- make sure more stringent and parse each kext in-memory so we only allow whitelist from .text </s> kmods = [(kmod.address.v(), kmod.address.v() + kmod.m('size'), kmod.name) for kmod in lsmod.mac_lsmod(obj_ref._config).calculate() if str(kmod.name) != "com.apple.kpi.unsupported"]	get_handler_name_addrs start = obj.Object("unsigned long", offset = s, vm = obj_ref.addr_space) end   = obj.Object("unsigned long", offset = e, vm = obj_ref.addr_space) kernel_symbol_addresses = obj_ref.profile.get_all_function_addresses() return (obj_ref, kernel_symbol_addresses, start, end, kmods)
# todo remove </s> args = parsing.array(parsing.array.tuple, none, values=[])	handle_iterators else: try: generators += \ it.execute_subscope_by_name('__iter__', args)
# todo(termie): optimize this call at some point and put it into the </s> roles_ref = []	authenticate tenant_id=tenant_ref['id'], extras=extras_ref) for role_id in extras_ref.get('roles', []): roles_ref.append(self.identity_api.get_role(context, role_id))
# todo separate msg </s> msg[4].encode('utf-8')  # head pose data; json	NetworkSetup msg[2],  # timestamp msg[3].encode('utf-8'),  # Blend Shape data; json ]) else:
# todo(himkt): remove `nltk` after solving </s> "nltk<3.6.6",	get_extras_require "torchvision==0.11.1+cpu ; sys_platform!='darwin'", "torchaudio==0.10.0", "allennlp>=2.2.0 ; python_version>'3.6'", "botorch>=0.4.0 ; python_version>'3.6'",
# todo(kan-bayashi): documentation and type hint </s> def piecewise_rational_quadratic_transform(	piecewise_rational_quadratic_transform inputs, unnormalized_widths,
# todo federate replies also when that is available in federation layer </s> if instance.is_local and not instance.parent:	content_post_save if kwargs.get("created"): notify_listeners(instance) federate_content(instance)
# todo: probably better to work out why they are occurring, but imo the </s> caplog.set_level(logging.warning)	test_sort_best_candidate__yanked_reason ): Test the log message with various reason strings. candidates = [ make_mock_candidate('1.0', yanked_reason=yanked_reason),
# todo test </s> if self.network.connectivity_matrix is not none:	__init__ self.past_state = past_state self.network = network self.connectivity_matrix = self.network.connectivity_matrix[ [[node.index] for node in self.nodes],
# todo: calculate mu-sigma for f1, accuracy, and roc_auc and make it selectable </s> cv_results['judgment_metric'] = np.mean(cv_results['mu_sigmas'])	atm_cross_val_small_multiclass rank_accuracies=rank_accuracies, mu_sigmas=mu_sigmas) cv_results['judgment_metric_std'] = np.std(cv_results['mu_sigmas']) return cv_results
# todo(solitude): when the migration of data is completed, we </s> pk = client.create_seller_for_pay(addon)	payments_confirm if request.method == 'POST' and form.is_valid(): if waffle.flag_is_active(request, 'solitude-payments'): client.patch_seller_paypal(pk=pk, data=form.cleaned_data) adp.update(**form.cleaned_data)
# todo: this should take a vector </s> return self._mesigmaderivmat	MeSigmaDerivMat )(np.ones(self.mesh.nE)) * self.sigmaDeriv
# todo(john-wood-w) allow until plugin validation is added. </s> self.unsupported_req = {	test_should_allow_add_new_order_unsupported_algorithm def test_should_allow_add_new_order_unsupported_algorithm(self): 'secret': { 'name': self.secret_name,
# todo col_type.python_type contains the type that </s> if not isinstance(val, _type_map[col_type]):	set_attrs setattr(self, prp.key, val) else: raise TypeError( "set_attrs() got a '%s' for keyword argument "
# todo: more unittests </s> self.failunlessequal(b.met2.__doc__, a.met1.__doc__)	testBorrowDoc self.failUnlessEqual(B.met1.__doc__, A.met1.__doc__)
# todo: check concurrent streams count and maybe wait </s> self._stream = protocol.processor.create_stream()	Stream if self._stream is None: protocol = await self._channel.__connect__() await self._stream.send_headers(self._request_headers) assert isinstance(message, self._send_type)
pass # todo </s> def search(self, query):	LibspotifyStoredPlaylistsController pass # TODO
# todo: this check is to maintain backwards compatibility with the old way of creating </s> if hasattr(self, 'package_form'):	new vars = {'data': data, 'errors': errors, 'error_summary': error_summary} self._setup_template_variables(context, {'id': id}) c.form = render(self.package_form, extra_vars=vars) else:
# todo: handle url == none </s> elif input == "esc": # close window	__handle_input if url != None: webbrowser.open(url) if self.viewing_answers: self.main_loop.widget = self.original_widget
weights[:,:-3] /= (bm * 3.0) # todo: use exact number of channels. </s> if semantic_weight: weights[:,-3:] /= (bs * semantic_weight)	evaluate_slices for idx, (bp, bm, bs, bh) in self.iterate_batches(*data, batch_size=layer.num_filters): weights = bp.astype(np.float32) layer.W.set_value(weights) cur_idx, cur_val, cur_match = self.compute_matches[l](f, history[idx])
# todo: fix broken buildpack / example app </s> self._test_example('example-dart')	_test_dart def _test_dart(self):
# todo: currently we are assuming that 'on error resume next' is being </s> log.debug('not setting ' + self.name + ", eval of rhs gave an error.")	Let_Statement self._handle_change_callback(self.name, context) else:
# todo: (step 11) gcs directory where kfp outputs are recorded </s> test_data_dir = "gs://{}/testdata".format(configs.gcs_bucket_name)  # pylint:disable=unused-variable	run metadata_config = kubeflow_dag_runner.get_default_kubeflow_metadata_config() tfx_image = os.environ.get('KUBEFLOW_TFX_IMAGE', None) stubbed_component_ids = ['CsvExampleGen', 'StatisticsGen',  # pylint:disable=unused-variable 'SchemaGen', 'ExampleValidator',
# todo: make these more configurable </s> host = getattr(settings, "ajax_proxy_host", "localhost")	request must be running for this to work), and return a tuple containing the returned HTTP status, content-type, and body. port = getattr(settings, "AJAX_PROXY_PORT", 8001) query = "?%s" % urllib.urlencode(get) if (get is not None) else ""
# todo: namelist, explist </s> self.assertequal(19, p._pos)	testStatLocalAssignmentWithValues self.assertIsNotNone(node)
# todo care for mro stuff (multiple super classes) </s> for s in self.base.supers:	Class return False names = self.base.get_defined_names() for cls in follow_statement(s): if as_instance:
return false  # todo: 2.0 return none </s> with _mixer_error_handling(self._mixer):	set_volume validation.check_integer(volume, min=0, max=100) if self._mixer is None: result = self._mixer.set_volume(volume).get() validation.check_instance(result, bool)
# todo: move install_time away from app_setting </s> app_setting(app_id, 'update_time', now)	app_upgrade else: now = int(time.time()) status['upgraded_at'] = now with open(app_setting_path + '/status.json', 'w+') as f:
# todo(okuta): check type </s> return core.moveaxis(a, source, destination)	moveaxis Array with moved axes. This array is a view of the input array. .. seealso:: :func:`numpy.moveaxis`
# todo: implement fs_type in dfvfs and remove this implementation </s> fs_info = file_system.getfsinfo()	_GetFileSystemTypeFromFileEntry if type_indicator != dfvfs_definitions.TYPE_INDICATOR_TSK: return type_indicator if fs_info.info: type_string = unicode(fs_info.info.ftype)
# todo: hack </s> if title.lower().find('.torrent') > 0:	feed_input elif not url.startswith('http://') or not url.startswith('https://'): url = urlparse.urljoin(pageurl, url) title = title[:title.lower().find('.torrent')] entry = Entry()
# todo: allow other formats? </s> for key, dimensions in config.podcast_album_art_sizes.iteritems():	save_album_art DBSession.add(podcast) DBSession.flush() file_path = im_path % (podcast.id, key, 'jpg') im.resize(dimensions, 1).save(file_path)
#todo(chris): implement service_catalog </s> self.service_catalog = none	authenticate ["nova"][0]["publicURL"] self.auth_token = body["auth"]["token"]["id"]
#todo: put the numpy.hstack() call in _load_and_unpack class to lazily load </s> dat = numpy.hstack([group[str(i)] for i in range(len(group))])	__enter__ % (self.label, kpti_kptj, self.label)) group = self.feri['%s/%d' % (self.label, k_id[0])] dat = _load_and_unpack(dat) return dat
self.setup() # todo: perhaps, remove this to pass path in context </s> current_ids = self.hasher.hash(	transform :param data_inputs: the data input to transform :return: transformed data inputs current_ids=None, hyperparameters=self.hyperparams,
# todo: logs which are observably relevant should be sent to the client (e.g. the warning of refusing to have more receivers active) </s> logging.basicconfig(level=logging.info)	configure_logging def configure_logging(): log.startLoggingWithObserver(log.PythonLoggingObserver(loggerName='shinysdr').emit, False)
# todo: change logic to c_leq based on benchmarking </s> transformationfactory('contrib.deactivate_trivial_constraints')\	solve_NLP_subproblem fix_nlp.tmp_duals[c] = c_geq * max( 0, c_geq*(rhs - value(c.body))) .apply_to(fix_nlp, tmp=True, ignore_infeasible=True) with SuppressInfeasibleWarning():
# todo: checking the cause of the large deviation </s> mol2 = molecule.from_file(os.path.join(test_dir, "si_cluster_rotated.xyz"))	HungarianOrderMatcherSiTest _, rmsd = self.mm.fit(mol2) def test_rotated_molecule(self): _, rmsd = self.mm.fit(mol2) self.assertAlmostEqual(rmsd, 1.025066171481399, places=6)
# todo: implement non-zero js </s> y_e = self.mesigmai*(self.edgecurlt*(self.mfmui*y_b))	e_from_b def e_from_b(self, y_b, txInd, tInd): if 'e' in self.p: y_e = y_e - self.MeSigmaI*self.p[txInd,'e',tInd]
logo_url = ''  # todo: add logo url </s> fields = []  # use build_field	serialize_event_to_context def serialize_event_to_context(event): event_context = get_event_context(event) url = add_notification_referrer_param(event_context.object_context.url, provider='slack',
# todo: clean code </s> not_done = 1. - tf.cast(done, dtype=tf.float64)	_compute_td_error_body @tf.contrib.eager.defun def _compute_td_error_body(self, states, actions, next_states, rewards, done): actions = tf.cast(actions, dtype=tf.int32) with tf.device(self.device):
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pcmpeqb res = 0x000000ff0000ff00ff00000000ffff00 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo: refactor accordingly when v3 websocket api is released </s> output["content"]["o"]["e"].update({	BittrexAPIUserStreamDataSource else: output["content"] = _decode_message(msg["M"][0]["A"][0]) "M": f"{output['content']['o']['E']['M'].split('-')[1]}-{output['content']['o']['E']['M'].split('-')[0]}" })
# todo if update_variable_bounds = false, this will not work as intended. </s> if value(v.lb) > var_lbs[v] + tol:	fbbt_block for v in _new_var_bounds.keys(): if v.lb is not None: improved_vars.add(v) var_lbs[v] = value(v.lb)
# todo: add_messages_to_json? </s> print dumps(result)	serve result['error'] = 'Not logged in' print 'Content-Type: application/json\n' else: directories()
# todo: use pybossa uploader! only for debugging: </s> open('/tmp/%d_%s_task_csv.zip' % (app.id, name), 'wb').write(memzip.getvalue())	export_csv memfile.write(str(line)) memzip = make_onefile_memzip(memfile, '%s_task.csv' % name) csv_task_run_generator = respond_csv("task_run", app.id) if csv_task_run_generator is not None:
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_metadata_copyright_contains_rfn_success def test_result_metadata_copyright_contains_rfn_success(self): self.assertInSuccess('test_metadata_copyright_contains_rfn', run_set(p, 'result'))
# todo: remove this method in v2.5 </s> elif self._values['disabled'] in booleans_false:	Parameters elif self._values['enabled'] in BOOLEANS_FALSE: return False return True elif self._values['disabled'] in BOOLEANS_TRUE:
# todo: get data init to work with tf_function compile #428 </s> def test_weightnorm_dense_train(self):	test_weightnorm_dense_train model = tf.keras.models.Sequential() model.add(
# todo: improve this. </s> self.view.erase_regions('vi_inc_search')	on_change def on_change(self, s): occurrence = reverse_search(self.view, s, 0, self.view.sel()[0].a) if occurrence:
@skipif('device-openmp')  # todo: still unsupported with openmp, but soon will be </s> assert np.all(u.data[1] == 6)	test_streaming_conddim_forward assert np.all(u.data[0] == 0)
# todo: better to use an inotify method that doesn't conflict with eventlets. </s> while true:	_config_file_stat @kill_on_exception(exc_logname) def _config_file_stat(self): if self.config_hashes: new_config_file_stats = valve_util.stat_config_files(
# todo: we also want to use pruned results </s> if param_name in t.params and t.value is trial.state.complete	get_param_result_pairs for t in self.trials
raise  # todo </s> except exception as e:	main usage_error(e, parser.get_usage()) except EzOutletResetError as e: raise  # TODO
# security todo - we _should_ be computing sha1(m), but we don't because that's the api. </s> if not (0 < r < self.q) or not (0 < s < self.q):	_verify def _verify(self, m, r, s): return False w = inverse(s, self.q)
# todo: assert </s> self.asserttrue(result)	test_find_profile Test: find a profile object result = self.remote.find_profile({"name": "testprofile0"}, self.token) assert 0
time.sleep(5)  # todo: for some reason, events do not trigger instantly </s> notifications = list(test_folder.get_events(subscription_id, watermark))	test_pull_notifications watermark = status_event.watermark i1 = self.get_test_item(folder=test_folder).save() self.assertEqual(len(notifications), 1) notification = notifications[0]
# todo: check to make sure time points match </s> if sys is not none:	__init__ self.nstates = self.xout.shape[0] self.uout = np.array(u) if sys.ninputs != self.ninputs: ValueError("System inputs do not match response data")
# todo: make it really async. </s> self.database_name = database_name	__init__ self.server_name = server_name
# todo: fill these in </s> out("""\	main GenBuiltinLookup(b, 'LookupAssignBuiltin', 'assign', f) GenBuiltinLookup(b, 'LookupSpecialBuiltin', 'special', f) bool IsControlFlow(Str* s) { assert(0);
todo = atomlist # list of atoms we must still mark and explore (recurse on all unmarked neighbors) </s> for atom in todo:	marksingle any sequence of bonds to the atoms in atomlist marked = {} # maps id(atom) -> atom, for processed atoms marked[id(atom)] = atom # since marked means "it's been appended to the todo list" while todo:
# todo: allow setting a placeholder dom element, or any widget parent </s> this.node = document.createelement('div')	_js_create_node def _js_create_node(self): flexx.get('body').appendChild(this.node);
return user.address  # todo: update </s> elif item == 'user_paid':	get_user_info return user.address  # TODO: update elif item == 'user_address': return  # TODO: update elif item == 'user_payment_id':
# todo: if the procpool has been exhausted this will block. </s> self.procpool.spawn(self.handle_request, body)	on_nova_message def on_nova_message(self, body, message): with self.messagesem: message.ack()
# todo: wrap backend call in error handling. </s> return backend.playback.get_time_position().get()	get_time_position backend = self._get_backend(self.get_current_tl_track()) if backend: else: return 0
# todo: how to check it? meybe we can omit this test </s> pass	test_gausianfill def test_gausianfill():
# todo: need to cleanup the named argument mess before it is possible. </s> pass	OptimizeFunctionCallArgsVisitor if star_dict_arg is not None: if star_dict_arg.isExpressionMakeDict(): elif star_dict_arg.isExpressionConstantRef(): pass
# todo sk: select standby db if necessary </s> return plproxy_config.proxy_db	db_for_read_write return DEFAULT_DB_ALIAS if app_label == FORM_PROCESSOR_APP: else: default_db = DEFAULT_DB_ALIAS
# todo?: self.assert_eq(kdf.loc['j':'q', 'b':'a'], pdf.loc['j':'q', 'b':'a']) </s> self.assert_eq(kdf.loc[kdf.b > 0, 'b'], pdf.loc[pdf.b > 0, 'b'])	test_loc2d_duplicated_columns self.assert_eq(kdf.loc['j':'q', 'B'], pdf.loc['j':'q', 'B']) self.assert_eq(kdf.loc['j':'q', ['B']], pdf.loc['j':'q', ['B']])
# todo ditto </s> continue	form else:
# todo(kevinbenton): this test should do something </s> pass	test_policy_enforcement def test_policy_enforcement(self):
# todo remove hardcoded path </s> languages = {'en', 'de'}	_read_wikidata def _read_wikidata(): with bz2.open('C:/Users/Sofie/Documents/data/wikidata/wikidata-20190304-all.json.bz2', mode='rb') as file: line = file.readline()
# todo multi-level import non-breakable </s> if isinstance(par, pr.import) and len(par.namespace) > 1:	process result.append(par) else: no_break_scope = True result.append(par)
# todo(ntamas): there are more possibilities; we could </s> else:	select filtered_idxs = sorted(es.graph.incident(value, mode="out")) func = None values = [e.source for e in es] if op == "in" or op == "notin":
@unittest.skip('not written')  # todo: finish! </s> self.assert_shortened([value], "[b'aaaaaaaaaaaaaaaaaa...aaaaaaaaa']")	test_bytes_large "b'" + 'A' * 43688 + "..." + 'A' * 21844 + "'")
# todo: check error location </s> assert result.data == {'nest': none}	test_non_nullable_list_of_nullables assert len(result.errors) == 1 assert result.errors[0].message == 'Cannot return null for non-nullable type.'
# todo: refactor this to be more uniform across sources </s> self.update_control_menu()	update_menu def update_menu(self):
# todo(wangqun):this method implement will be added after this </s> pass	validate_labels_executor_environment_variables def validate_labels_executor_environment_variables( mesos_slave_executor_environment_variables):
# todo: performance can be improved here by substituting expensive </s> bs = tf.shape(q)[0]	VI_Block tf.concat(2, [w, w_fb]), name="q") q = tf.transpose(q, perm=[0, 3, 1, 2]) rprn = tf.reshape(tf.tile(tf.reshape(tf.range(bs), [-1, 1]), [1, state_batch_size]), [-1]) ins1 = tf.cast(tf.reshape(S1, [-1]), tf.int32)
# todo check if result is in scope -> no evaluation necessary </s> n = _check_flow_information(self._name_context, flow_scope,	_names_to_types while flow_scope != self._name_context.get_node(): flow_scope = flow_scope.get_parent_scope(include_flows=True) self._name, self._position) if n is not None:
# todo: non-numeric columns should be ignored automatically </s> def test_impl(n):	test_mean1 def test_mean1(self): df = pd.DataFrame({'A': np.arange(n)+1.0, 'B': np.arange(n)+1}) return df.mean()
@jtu.skip_on_devices("tpu")  # todo(phawkins): re-enable </s> def testdirichlet(self, alpha, dtype):	testDirichlet for alpha in [[0.2, 1., 5.]] for dtype in [onp.float32, onp.float64])) key = random.PRNGKey(0) rand = lambda key, alpha: random.dirichlet(key, alpha, (10000,), dtype)
# todo: has some issues with datetime and sqlite </s> self._test_model('language', post_language_data)	test_language_api def test_language_api(self):
# todo here we could apply a "patch" to the native metadata, if desired </s> if isinstance(native_meta, list):	get_native_metadata continue if native_meta: meta.extend(native_meta) else:
# todo: if the input was compressed, compress the output? </s> part_file = open(part_path, 'w')	split_to_size part_dir = get_dir() part_path = os.path.join(part_dir, os.path.basename(input_file)) if clusters_per_file is None and part > 0: local_clusters_per_file.append(default_clusters)
# todo pseudo code: </s> pass	Play @dbus.service.method(dbus_interface=player_interface) def Play(self):
# todo: extend to other types like datetime? </s> def nunique_seq(a):	nunique_seq return len(set(A))
#todo: check for continous or discrete, only continuous supported right now </s> dico = 'c'	gram Wc = gram(sys,'c') Wo = gram(sys,'o') D,V = np.linalg.eig(sys.A)
# todo: candidate for move to system/hdparm </s> out, err, rc = run_command(	get_disk_APM_level it's setting equivalent of 255. If there is an error, such as can happen when APM is not supported, then we return 0. [HDPARM, '-B', '-q', '/dev/disk/by-id/%s' % dev_byid], throw=False) if len(err) != 1:
# todo: how do i make the __iter__ thread safe? </s> cursor = self._conn.execute('select information from data')	__iter__ def __iter__(self): for r in cursor: obj = cPickle.loads(r[0])
# todo: lets turn skipifmock into an annotation </s> pass	setUp except:
# todo: update consumer </s> collection.remove(bind, safe=true)	repo_deleted deleted.append(bind)
# todo: use self.translate() </s> ghat = np.dot(u.t, g)	gft_windowed_normalized C = np.reshape(C, (N, N), order='F') else: Ftrans = np.sqrt(N)*np.dot(U, (np.kron(np.ones((1, N)), ghat)*U.T)) C = np.empty((N, N))
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
# todo: when no longer supporting python 2.4 use finally: </s> raw_size += len( self.current_line )	next except ParseError, e: handle_parse_error( e ) if isinstance( self.seed_interval, ( Header, Comment ) ): return_val = self.seed_interval
# todo: test jacobian </s> mod = linearfactormodel(data.portfolios, data.factors)	test_linear_model_parameters def test_linear_model_parameters(data): res = mod.fit() f = mod.factors.ndarray
# todo: remove verify ssl config when working without it. </s> def fetch_island_data(zone_key, session):	fetch_island_data if zone_key == 'ES-CN-FVLZ': lanzarote_fuerteventura_data = LanzaroteFuerteventura(session, verify=False).get_all()
# self.assertnotequal(end, 0) todo </s> cmd = "{systemctl} is-failed zza.service"	test_4900_unreadable_files_can_be_handled cmd = "{systemctl} is-active zza.service" out, end = output2(cmd.format(**locals())) out, end = output2(cmd.format(**locals())) self.assertNotEqual(end, 0)
# todo(harlowja): should we be a little more cautious about </s> task_details = flow_details[task_state][0]	_task_result_fetcher task_state = task_and_state(task, states.SUCCESS) if task_state in flow_details: if task_details.metadata and 'result' in task_details.metadata: return (True, task_details.metadata['result'])
time.sleep(1)  # delay, for last.fm latency. todo can this be removed later? </s> last_scrobble = lastfm_user.get_recent_tracks(limit=2)[0]	test_scrobble lastfm_user = self.network.get_user(self.username) self.network.scrobble(artist=artist, title=title, timestamp=timestamp) self.assertEqual(str(last_scrobble.track.artist).lower(), artist) self.assertEqual(str(last_scrobble.track.title).lower(), title)
# todo parse out args instead of all endpoints </s> endpoints.append(endpoint)	show_devices sdnc.get_stored_endpoints() for endpoint in sdnc.endpoints: return endpoints
# todo: move to trim operator </s> bpy.ops.sequencer.gap_remove()	modal bpy.context.scene.frame_current = self.start_frame - 1
# todo: support steps and times (motion blur) </s> print("dupli export took %.3fs" % (time() - start))	create_session transformations = array("f", duplis.matrices) luxcore_scene.DuplicateObject(src_name, dst_name, count, transformations) engine.update_progress(index / len_objs)
# todo - can we support this? </s> seqs = [seq("tacaa", iupac.unambiguous_dna),	test_mixed_alphabets def test_mixed_alphabets(self): Seq("TACGC", IUPAC.ambiguous_dna), Seq("TACAC", IUPAC.extended_dna),
pass  # todo </s> elif action == 'execute':	multisig paint_multisig_contract_info(emitter, multisig_agent, token_agent) elif action == 'sign': pass  # TODO
# todo: with git <= 2.3 keep old mechanism: </s> with rm.repo.git.custom_environment(	push cnct = ssh_manager.get_connection(fetch_url) cnct.open() GIT_SSH_COMMAND="ssh -S %s" % cnct.ctrl_path): rm.push(refspec=refspec, progress=progress)
# hide numerical axis / todo: adapt for </s> p.left[0].axis_label_text_color = none	_show_teardown def _show_teardown(self): p = self.chart.plot p.left[0].axis_line_color = None p.left[0].major_label_text_color = None
# todo: documentation pending </s> parameters	save_weights_to_npz def save_weights_to_npz(self, filepath, sess=None): ---------- filepath
1  # todo: fill in identifier </s> )	profile_transfer amount, target, result.wait() profiling.print_all_threads()
# todo: proper java error? </s> raise runtimeerror('could not find class \'%s\' for jnienv.' % name)	find_class clazz = self._class_loader.find_class_by_name(name) if clazz is None: return clazz.jvm_id
# todo: remove this monkeypatch once upstream class is fixed. </s> _patch_zone(zone)	_update_record if identifier and content: with localzone.manage(self.filename, self.origin, autosave=True) as zone: if zone.update_record(identifier, content):  # pylint: disable=no-member result = True
# todo: gtk4 - use controllers dragsource and droptarget </s> pass	__init__ self.connect("drag-data-received", NamespaceView.on_drag_data_received) else: self._controller = tree_view_expand_collapse(self)
# todo: for now, circumnavigate the detached head issue. </s> for subds in source.get_dataset_handles(recursive=true):	test_publish_simple def test_publish_simple(origin, src_path, dst_path): source = install(path=src_path, source=origin, recursive=True) AnnexRepo(opj(src_path, subds), init=True, create=False).git_checkout("master") target = GitRepo(dst_path, create=True)
# todo: supposedly, we can put a <commit /> element in the same post body </s> if commit:	add if response.status != 200: raise SolrError(self._extract_error(response)) self.commit()
match = util.get_value(rule['match'], kwargs) #todo use subkey? </s> replace = util.get_value(rule['replace'], kwargs) #todo use subkey?	pipe_strregex rules = [] for rule in conf['RULE']: replace = re.sub('\$(\d+)', r'\\\1', replace)   #map $1 to \1 etc.   #todo: also need to escape any existing \1 etc. rules.append((match, replace))
# todo: remove the following line after implicit flat src deprecation release </s> 'orig_flat_src_indices': flat_src_indices,	add_input 'src_indices': src_indices, 'flat_src_indices': flat_src_indices, 'add_input_src_indices': src_indices is not None, 'units': units,
# todo change to check for error when the functionality changes. currently acts as though it doesn't exist </s> url = "/api/v2/nodes/?filter[notafield]=bogus"	test_incorrect_filtering_field def test_incorrect_filtering_field(self): res = self.app.get(url) node_json = res.json['data']
node = ursula.from_metadata_file(filepath=abspath(metadata_path), federated_only=self.federated_only)  # todo: 466 </s> known_nodes.add(node)	read_known_nodes for metadata_path in metadata_paths: if self.checksum_address not in metadata_path: self.known_nodes.update(known_nodes)  # TODO: Use non-mutative approach? return known_nodes
""" todo: set hyper-parameter </s> return tf.contrib.layers.batch_norm(input, decay=0.9, is_training=is_training)	_batch_norm def _batch_norm(input, is_training):
# todo: use triple factory </s> rotate.forward_owa(torch.zeros(16, 3, dtype=torch.long))	test_rotate rotate = RotatE(triples_factory=self.factory) self.assertIsNotNone(rotate) rotate.forward_cwa(torch.zeros(16, 2, dtype=torch.long)) rotate.forward_inverse_cwa(torch.zeros(16, 2, dtype=torch.long))
# todo handle 4 types of transition exceptions </s> pass	create_next next_status = TransactionStatusTransition.next(previous_transaction, action, provider) except Exception: except Exception: pass
pass  # todo </s> self._current_command = []	__init__ self._device = None
# todo: remove things that we don't want botleague agents to be able </s> return obz, reward, done, info	step self.recorder.step(obz, done, reward, dd_action, is_agent_action=dd_action.has_control)
# todo: batch </s> self._send(rest.request(self._graph_db, "put",	replace labels = [str(label) for label in flatten(labels)] if self.__uri__: self.__uri__, list(labels))) self.refresh()
pass  # todo: implement this </s> def do_test(self):	do_test
# todo: for backward compatibility only, remove if not used anymore </s> def get_os_type_id(self):	get_os_type_id return get_os_type(key='id')
# todo remove hardcoded path </s> languages = {'en', 'de'}	_read_wikidata def _read_wikidata(): with bz2.open('C:/Users/Sofie/Documents/data/wikidata/wikidata-20190304-all.json.bz2', mode='rb') as file: line = file.readline()
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_success_get_multiple_addresses You can continue adding digests to the address builder even after extracting an address.
# todo: add a value parameter as well </s> super(tag_byte_array, self).__init__(name=name)	TAG_Byte_Array id = TAG_BYTE_ARRAY def __init__(self, name=None, buffer=None): if buffer: self._parse_buffer(buffer)
# todo in python 2.7 or later, this should be a set literal. </s> assert set(['1', '4']) == set(person_ids[:2])	test_sorting_null_field person_ids = list(map(itemgetter('id'), people)) assert ['3', '2'] == person_ids[-2:]
# todo: for now, circumnavigate the detached head issue. </s> for subds in source.get_dataset_handles(recursive=true):	test_publish_simple def test_publish_simple(origin, src_path, dst_path): source = install(path=src_path, source=origin, recursive=True) AnnexRepo(opj(src_path, subds), init=True, create=False).git_checkout("master") target = GitRepo(dst_path, create=True)
# todo rethink/streamline the clamp_basetype logic </s> if _needs_clamp(right.typ, enc):	make_setter enc = right.encoding  # unwrap_location butchers encoding right = unwrap_location(right) _val = LLLnode("val", typ=right.typ) right = ["with", _val, right, ["seq", clamp_basetype(_val), _val]]
# todo: actually test this once i figure out how to do this in py.test </s> logging.error("interrupted by user")	download jobs.get(0xFFFF) except KeyboardInterrupt:  # pragma: no cover return 1 except requests.exceptions.ConnectionError as err:
# todo: handle count and minp </s> for i in range(0, min(win - 1, n) - offset):	roll_fixed_apply offset = (win - 1) // 2 if center else 0 output = np.empty(N, dtype=np.float64) output[i] = np.nan ind = 0
# @todo: remove this if in 0.6 </s> if isinstance(node_id, node):	ex_get_node_details def ex_get_node_details(self, node_id): node_id = node_id.id uri = '/servers/%s' % (node_id)
# todo: disconnect </s> return	Node except asyncio.TimeoutError: await stream.reset() self.logger.debug(f"Received the hello message {hello_other_side}") if not (await self._validate_hello_req(hello_other_side)):
# todo: test! </s> if not settings.st_unique_emails:	EmailUniqueMixin def clean_email(self): email = self.cleaned_data["email"] return email is_taken = User._default_manager\
#todo: remove this transformation </s> personal_schemas = set([schema.qualified_name for schema in personal_schemas_raw])	add_schema_ownerships changed (to be the same as the schema owner). personal_schemas_raw = dbcontext.get_all_personal_schemas() schemas_and_owners = dbcontext.get_all_schemas_and_owners() for schema, owner in schemas_and_owners.items():
# todo add verbose output </s> return self._domain	LocalizationModel @property def domain(self): @domain.setter def domain(self, new_domain):
# todo: progress +kwargs </s> except badname as e:	push try: rm.push(refspec=refspec, progress=progress, **kwargs) lgr.error("GitPython reported BadName Exception: %s" % e) pass
# todo: 1.1 refactoring </s> raise notimplementederror("requires to be updated to new query builder")	create_conformed_rollup * `replace` – if ``True`` then existing table will be replaced, otherwise an exception is raised if table already exists. naming = distill_naming(options) context = QueryContext(cube, mapper, schema=schema, metadata=self.metadata)
# todo handle 404 </s> self.client.minion_delete(pk)	delete def delete(self, request, pk): return Response(status=status.HTTP_204_NO_CONTENT)
# todo: implement </s> raise notimplementederror()	Controller def __init__(self, *args, **kwargs): super(Controller, self).__init__(*args, **kwargs) def _handle(self, key, is_press): raise NotImplementedError()
# todo: handle errors better </s> abort(	patch except sqlalchemy.exc.IntegrityError as e: db.session.rollback() code=http_exceptions.Conflict.code, message="Could not update user details."
# todo some settings in the right column are conditional: bezel corr, maybe diag inches? </s> self.sizer_setting_paths = wx.staticboxsizer(wx.vertical, self, "wallpaper paths")	WallpaperSettingsPanel self.sizer_settings_left.Add(self.sizer_setting_slideshow, 0, wx.CENTER|wx.EXPAND) self.sizer_settings_left.Add(self.sizer_setting_hotkey, 0, wx.CENTER|wx.EXPAND) self.sizer_settings_right.Add(self.sizer_setting_paths, 0, wx.CENTER|wx.EXPAND)
#todo rewrite this part of pdfkit.py </s> r = pdfkit.pdfkit('<html><head></head><body>hai!</body></html>', 'string')	test_stylesheet_adding_to_the_head def test_stylesheet_adding_to_the_head(self): css = open('testfiles/example.css') r.stylesheets.append(css)
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_interact_arguments @classmethod def add_interact_arguments(cls, parser): p_convert = p_operation.add_parser( "convert", help="convert VGM to PCM using OPL hardware")
pass  # todo </s> def playlist_create(self):	playlist_create @test(depends_on=[song_create])
# todo: implement </s> csv = ""	infer Sample INFER INTO: INFER columnstring FROM tablename WHERE whereclause WITH confidence INTO newtablename LIMIT limit; Argument newtablename == null/emptystring if we don't want to do INTO cellnumbers = [] return csv, cellnumbers
# todo: dispatch event via queue </s> self._logger.debug('dispatching trigger (trigger=%s,payload=%s)', trigger, payload)	dispatch assert(isinstance(payload, (type(None), dict)))
# todo: verify that workid is the primary key someplace. </s> yield workitem.dowork()	actuallyReallyExecuteWorkHere workItem = yield workItemClass.load(txn, workID) yield workItem.delete() except: f = Failure()
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: assert </s> print(close_p.describe(percentiles=np.arange(0.1, 0.9, 0.1)))	testClose close_p = D.features(D.instruments('csi300'), ['Ref($close, 1)/$close - 1'])
#todo: note that i'm passing a dc to the fuzzablerequest and it's not </s> fr = fuzzablerequest(self.url, method='get', dc={'a': ['b']},	test_dump_case01 '']) headers = Headers([('Hello', 'World')]) headers=headers) self.assertEqual(fr.dump(), expected)
### todo: code this! </s> pass	_handle_textarea_tag_outside_form def _handle_textarea_tag_outside_form(self, tag, attrs): Handler for textarea tag outside a form
# todo: remove this when hftransformersnlp is removed for good </s> if cached_component:	load Returns: the loaded component return cached_component return cls.create(meta, model_metadata)
# todo: refactor into one function that just takes nodes </s> for j in range(child_count):	set_scanner_count total_issues = 0 child_count = node.getChildCount() child = node.getChildAt(j) tree_param_name = child.toString()
except exception:  # todo - which exceptions? </s> start, stop = 1.0, self.sequence_id.index(end)	reverse try: start, stop = w.tag_ranges(SEL) seq = w.get(start, stop) seq = list(re.sub('[^A-Z]', '', seq))
# todo: kwargs </s> def _impl(df, axis=none, skipna=none, level=none, numeric_only=none):	max_overload @overload_method(DataFrameType, 'max') def max_overload(df, axis=None, skipna=None, level=None, numeric_only=None): return hpat.hiframes.pd_dataframe_ext.max_dummy(df) return _impl
# todo: udpoutgoing style buffer </s> for s in r:	udp_select if not r and not e: return while True: try:
# todo: find out whether qid is optional, and optimise if so </s> extra = {	discard def discard(self, n=-1, qid=-1, **handlers): "n": n, "qid": qid,
# todo: handle situations where share is password protected </s> pathname = string.replace(pathname,'/', '\\')	remove def remove(self, shareName, pathName, password = None): pathName = ntpath.normpath(pathName) if len(pathName) > 0 and pathName[0] == '\\':
# todo: replace with a proxy lookup that doesn't have any side effects </s> response = self._dispatch('post',	open if not formation: formation = self._session.formation "/api/formations/{}/calculate".format(formation)) if response.status_code == requests.codes.ok:  # @UndefinedVariable
# todo(dei): this code is not well-designed for large-scale scoring, where the </s> results = list(result_iter)	score_with_estimator checkpoint_path, = get_checkpoint_iterator(eval_checkpoint_step, model_dir) result_iter = estimator.predict(input_fn, checkpoint_path=checkpoint_path) if num_examples is None: targets = [r["targets"] for r in results]
# todo: use the walrus operator </s> return -sum(dist.pmf(c) * math.log2(dist.pmf(c)) for c in dist if dist.pmf(c) > 0)	entropy 1. `A Simple Explanation of Information Gain and Entropy <https://victorzhou.com/blog/information-gain/>`_ 2. `Calculating entropy <https://www.johndcook.com/blog/2013/08/17/calculating-entropy/>`_
pass  # @todo: </s> def enable(self):	enable
# todo: auxiliary_vars </s> outs = fetches(training, inputs, targets)	_run outs = fetches(training, inputs, targets, auxiliary_vars) elif backend_name == "pytorch": return None if outs is None else utils.to_numpy(outs)
# todo link subscribers_changed in docstring to callback docs </s> sp_subscribers = ffi.gc(	subscribers May be empty until you call :meth:`update_subscribers` and the ``subscribers_changed`` callback is called. lib.sp_playlist_subscribers(self._sp_playlist), lib.sp_playlist_subscribers_free)
# todo: larger gains expected with scipy.signal.signaltools.fftconvolve(). </s> psi = segment_axis(y, k, 1, axis=-1)[:, :t - delay - k + 1, ::-1]	get_correlations_narrow_v5 def get_correlations_narrow_v5(Y, inverse_power, K, delay): D, T = Y.shape Psi_conj_norm = inverse_power[None, delay + K - 1:, None] * Psi.conj() correlation_matrix = np.einsum('dtk,etl->kdle', Psi_conj_norm, Psi)
# todo results from ml </s> return	_get_device_behavior @staticmethod def _get_device_behavior(endpoint):
# todo: cmake imported target shouldn't be namespaced (waiting https://github.com/conan-io/conan/issues/7615 to be implemented) </s> self.cpp_info.filenames["cmake_find_package"] = "hana"	package_info def package_info(self): self.cpp_info.filenames["cmake_find_package_multi"] = "Hana" self.cpp_info.names["cmake_find_package"] = "hana"
# todo(developer): uncomment and set the following variables </s> client = automl.automlclient()	delete_dataset def delete_dataset(project_id, dataset_id): from google.cloud import automl_v1beta1 as automl dataset_full_id = client.dataset_path( project_id, "us-central1", dataset_id
# todo(nmigen-0.2): remove this </s> @classmethod	wrap @deprecated("instead of `Statement.wrap`, use `Statement.cast`") def wrap(cls, obj):
# todo: remove? </s> const.requisitionactions.request	process_transactions const.StockActions.STOCKONHAND, const.StockActions.STOCKOUT, ): balances.append(tx)
# todo: improve error handling </s> return make_response("error", status_code=500)	handle_ws ) if status_code != 200:  # pragma: no cover return make_response("OK", status_code=200) elif event_type == "MESSAGE":
pass  # todo: this </s> elif fi['type'] == 'keysect':	read data = cipher.decrypt(data)[before:len(data) - after] elif fi['type'] == 'twl': data = fi['content'][offset:offset + size] return data
# todo question: now that concurrent tags include vectorize tags, </s> for insn in kernel.instructions:	check_for_write_races def check_for_write_races(kernel): from loopy.kernel.data import ConcurrentTag for assignee_name, assignee_indices in zip( insn.assignee_var_names(),
# todo: change this to be architecture independent </s> return re.match('call', mnem)	is_call if re.match('call\s+far prt', mnem):  return None
except exception:  # todo - which exceptions? </s> pass  # skip unparsable tag	_parse_feature try: feature.qualifiers[feature_element.tag.replace(NS, '')] = feature_element.text self.ParsedSeqRecord.features.append(feature)
# todo add 'header' and 'background image' to testing </s> response = self.client.post(self.url, self.data)	test_create_team_member_default_team_type def test_create_team_member_default_team_type(self): self.assertEqual(response.status_code, status.HTTP_201_CREATED) self.assertEqual(Team.objects.all().count(), 1)
# todo add locales </s> raise yunohosterror("domain_name_unknown", domain=domain)	domain_set_settings domains = _load_domain_settings() if not domain in domains.keys(): setting_set = False if ttl is not None:
# todo: change to hostname </s> self.resetlastmanu	startDiscovery self.addManualDevice(address)
# todo: arrange </s> repo = self.remote.new_repo(self.token)	test_create_repo def test_create_repo(self): Test: create/edit a repo object self.assertTrue(self.remote.modify_repo(repo, "name", "testrepo0", self.token)) self.assertTrue(self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token))
# todo: something smarter? </s> from nbdime import diff	diff_single_cells def diff_single_cells(a, b): return diff(a, b)
# todo: allow specification of sentinel </s> build_sentinel = "file in source dir"	build sudo("make distclean") sudo("./configure %s" % ' '.join(flags)) if forcing['build'] or not exists(build_sentinel): sudo("make")
# todo: add specific coverage here </s> if ignore_nonexistent_tables:	reduce_columns raise except exc.NoReferencedTableError: continue else:
# todo: implement! </s> pass	_check def _check():
# todo: this check has the unfortunate side-effect that </s> filename_or_obj = stringio(filename_or_obj)	ScipyDataStore if (isinstance(filename_or_obj, basestring) and filename_or_obj.startswith('CDF')): self.ds = netcdf.netcdf_file(filename_or_obj, mode=mode, mmap=mmap, version=version)
# todo: support all tzinfo subclasses by calling utcoffset() </s> if date_time.tzinfo is not none and\	datetime_obj_to_dtstruct dt.time.offset = 0 dt.time.ok = '\1' date_time.tzinfo.__class__ is TZFixedOffset: dt.time.offset = date_time.tzinfo.offset
# todo tell it to some human operator </s> pass	__init__ self._store[ent[:-5]] = entity(data) except OSError: except IOError: pass
# todo: "wildcards" other than <any> </s> if type == "<any>" or type == term:	get_relations_by_arg1 continue for dummy, type in arg1s: rels.append(r) cache[directory] = rels
# todo deprecate? </s> def get_locations(*args, **kwargs) -> sequence[location]:	get_locations return list(locations(*args, **kwargs))
# todo: assert </s> profiles = self.remote.get_profiles(self.token)	test_create_subprofile def test_create_subprofile(self): Test: create/edit a subprofile object subprofile = self.remote.new_subprofile(self.token) self.assertTrue(self.remote.modify_profile(subprofile, "name", "testsubprofile0", self.token))
#todo: check the data! </s> count += 1	test_split for i in p:
# todo: needs to be fixed after series is converted into sqlalchemy </s> assert self.feed.find_entry(title='another.series.s01e20.720p.xvid-flexget'), 'another.series.s01e20.720p.xvid-flexget should have passed'	testSeries def testSeries(self):
pass # todo </s> def _rm(self, name):	_rm @register(r'^rm (?P<name>\S+)$')
# todo remove </s> args = parsing.array(parsing.array.tuple, none, values=[])	handle_iterators else: try: generators += \ it.execute_subscope_by_name('__iter__', args)
# todo 为适应私人收藏夹临时修改，正式版中必须改正 </s> try:	getInfoDict infoDict['commentCount'] = self.matchInt( self.getTagContent(self.content.find('div', {'id': 'zh-list-meta-wrap'}).find('a', {'name': 'addcomment'}))) infoDict['followerCount'] = self.matchInt( self.content.find_all('div', {'class': 'zm-side-section'})[2].find_all('div', {'class': 'zg-gray-normal'})[
# todo: probably should modify emulator driver to de-prioritize this </s> logging.getlogger("emulatordriver").setlevel(logging.critical)	ignore_floss_logs def ignore_floss_logs(): logging.getLogger("Monitor").setLevel(logging.ERROR) logging.getLogger("envi/codeflow.addCodeFlow").setLevel(logging.ERROR)
#todo: do we really need the history? </s> history = self.get_history( trans, history_id,	get_history_dataset_association_from_ids check_ownership=False, check_accessible=True ) else: check_ownership=True, check_accessible=True, deleted=False ) hda = self.get_history_dataset_association( trans, history, id,
# todo: normalize dict entries to make reverse lookup more reliable with </s> translation = event.getstring().strip()	on_translation_change def on_translation_change(self, event): self.listbox.Clear() if translation:
# todo(mriedem): make this smarter by keeping track of our bindings </s> return fake_requests.fakeresponse(200, content=jsonutils.dumps(data))	fake_create_port_binding @staticmethod def fake_create_port_binding(client, port_id, data):
# todo(dcramer): ideally we could just send the signal to the subprocess </s> self.task.status = taskstatus.failed	_timeout self._logreporter.save_chunk('>> Process exceeded time limit of %ds\n' % self.timeout) with lock(redis, 'task:{}'.format(self.task.id), timeout=5): self.task.date_finished = datetime.utcnow() db.session.add(self.task)
# compare filesizes todo print analysis of this :) </s> cmd = "ls -l '%s.ttf'*" % filename	ttx_process cmd = "ttx -i '%s.ttx'" % filename run(cmd, cwd=_out, log=log) run(cmd, cwd=_out, log=log) cmd = "rm  '%s.ttf.orig'" % filename
pass # todo </s> def parse_image(self, image):	parse_image
# todo(b/166479382): this cleanup-before-invocation pattern is a workaround </s> eager_cleanup_resources = []	_fn_to_return def _fn_to_return(arg, param_fns, wrapped_fn):  # pylint:disable=missing-docstring for op in wrapped_fn.graph.get_operations(): if op.type == 'HashTableV2':
# todo(stephenfin): remove cells_enabled parameter when we removed </s> num_instances = 2	test_multi_instance_display_name def test_multi_instance_display_name(self, cells_enabled=False): display and host name. refs, _ = self.compute_api.create(self.context, flavors.get_default_flavor(),
# todo: remove this test as soon as all old test methods are migrated </s> from sage.misc.sage_unittest import testsuite	test_old_testsuite def test_old_testsuite(self, backend: GenericBackend): TestSuite(backend).run(verbose=True, raise_on_failure=True, skip=("_test_pickling","_test_solve","_test_solve_trac_18572"))
# todo: handle `stream.close` and `stream.reset` </s> peer_id = stream.mplex_conn.peer_id	Node await stream.close() async def _handle_beacon_blocks(self, stream: INetStream) -> None: if peer_id not in self.handshaked_peers: self.logger.info(
# todo: assert </s> self.asserttrue(result)	test_rename_profile profile = self.remote.get_item_handle("profile", "testprofilecopy", self.token) result = self.remote.rename_profile(profile, "testprofile1", self.token) assert 0
# todo remove the day prefix of the file that was there prior to the crawl </s> file_name = filesystem.get_file_name_for_video(video)	main directory_name = filesystem.get_folder_name_by_date(metadata['date_taken']) dest_directory = '%s/%s' % (destination, directory_name) dest_path = '%s/%s' % (dest_directory, file_name) filesystem.create_directory(dest_directory)
''' </s> if inplace:	transposeBelowTarget >>> pitch.Pitch('f#2').transposeBelowTarget(pitch.Pitch('f#8'), minimize=True) <music21.pitch.Pitch F#8> src = self else:
# todo: remove in v8 </s> self._global_context['blog_desc'] = self.config.get('blog_description')	__init__ self._GLOBAL_CONTEXT['blog_title'] = self.config.get('BLOG_TITLE') self._GLOBAL_CONTEXT['blog_description'] = self.config.get('BLOG_DESCRIPTION') self._GLOBAL_CONTEXT['blog_url'] = self.config.get('SITE_URL', self.config.get('BLOG_URL')) self._GLOBAL_CONTEXT['body_end'] = self.config.get('BODY_END')
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_METADATA_style_value_matches_font_italicAngle_value_success def test_result_METADATA_style_value_matches_font_italicAngle_value_success(self): r = run_set(p, 'result') success_tests = r['success']
# todo: see get_scale_factor() to choose 72 px on hidpi </s> if icon and icon.bind(interface.factory):	pixbuf def pixbuf(clazz, interface, icon): pixbuf = None pixbuf = getattr(icon.bind(interface.factory), 'native_%i' % clazz.icon_size()).get_pixbuf() return pixbuf
except typeerror as e:          #todo: generalise for all api methods </s> raise exception(str(e))	do_method transaction_args, common_args, private_key_wif = split_params(**kwargs) return do_transaction(db, name=transaction, params=transaction_args, private_key_wif=private_key_wif, **common_args)
#todo: should use .append instead of writing directly to _rows? </s> table.identify_data_types(sample_size=none)	test_column_sizes [2, u'b', datetime.date(2000, 1, 1)], [3, u'c', datetime.date(1900, 1, 1)], ] expected = textwrap.dedent(u''' +----+----------+-----------------+
#todo replace dot with dotfunc? </s> def dotfunc(a, b):	dotfunc Computes the dot product of two variables. For two matrices, this is equivalent to matrix multiplication. For two vectors, this is the inner
# todo: call all server </s> self._servers.localserver().post("/projects/{project_id}/commit".format(project_id=self._uuid), none, body={})	commit def commit(self):
# todo - log details about the test </s> return result	check_mvip_connection test = self.sfe.test_connect_mvip(mvip=self.mvip) result = test.details.connected except: err = get_exception()
"""todo: doesn't remove unused nodes/renumber elements""" </s> y = self.xyz[:, 1]	slice_y def slice_y(self, yslice): self._slice_plane(y, yslice)
# todo(dcramer): ideally we could fire off jobs to sync test results </s> try:	_sync_job_from_active }) if should_finish: self._sync_test_results(job) except Exception:
# todo: call all server when we got uuid </s> log.info("project {} created".format(self._uuid))	_project_created self._uuid = params["uuid"]
# todo: error detection </s> __connect()	serialize_item def serialize_item(obj, item): collection = mongodb[obj.collection_type()] data = collection.find_one({'name':item.name})
# todo implement </s> return context	check return context elif name == 'TypedDict': elif name in ('no_type_check', 'no_type_check_decorator'): return context
# todo: remove this when domain decomposition is merged </s> warnings.warn('this feature is not yet implemented in a release ' \	set_dd_nodemap def set_dd_nodemap(self, nodemap): 'version of openmc') if not isinstance(nodemap, tuple) and \
# todo extend to nonbinary nodes </s> if i not in self._input_indices and i in self.context.node_indices:	__init__ else: self._dimension_labels.append(None) past_tpm_on = past_tpm_on.sum(i, keepdims=True) / 2 past_tpm_off = past_tpm_off.sum(i, keepdims=True) / 2
# todo: show the declaration info somewhere. </s> completions.append(	get_completions identifier = d['identifier'] declaration_info = d['info'] (identifier[:MAX_COMPLETION_LENGTH], identifier)) return completions
# todo: slow. </s> outputs = [output for output in outputs if is_scriptpubkey_spendable(output['scriptpubkey'], source)]	get_unspent_txouts outputs.append(coin) logger.debug('Pruning unspendable outputs.') logger.debug('Pruning spent outputs.') vins = {(vin['txid'], vin['vout']) for tx in raw_transactions for vin in tx['vin']}
# todo: make keys get passed through files or environment </s> inbuf = tempfile.namedtemporaryfile()	encrypt def encrypt(self, data, key): inbuf.write(data) inbuf.flush()
# todo: round values properly!: </s> duty_ns = int(value * (period_ns/resolution))	analogWrite freq = int(kernelFileIO(PWM_FILES[pwm_pin][PWM_FREQ])) period_ns = (1e9/freq) kernelFileIO(PWM_FILES[pwm_pin][PWM_DUTY], str(duty_ns)) if (kernelFileIO(PWM_FILES[pwm_pin][PWM_ENABLE]) == '0\n'):
# todo: does this import need to be delayed because </s> from pyomo.solvers.plugins.solvers.persistent_solver import \	solve exception_on_failure=True, io_options=None): PersistentSolver if self.instance is None:
raise retry(encode(reason))  #@todo: remove `encode` in 0.4.10 </s> self.retries[id] += 1	retry self.wait(wait_time, False)
# todo test with matlab </s> mat_answser = none	test_distanz def test_distanz(x, y): self.assertEqual(utils.distanz(x, y))
## todo: decode </s> folder_path = unicode(	BSA if self.bsa_header.has_names_for_folders(): name_size = struct.unpack_from('B', file_records_block)[0] file_records_block[1:name_size].tobytes(), encoding=_bsa_encoding)
# @todo: investigate why this isn't working </s> db = current.db	setup_run_playbook if hosts is None: hosts = ["127.0.0.1"] W2P_TASK = current.W2P_TASK table = current.s3db.scheduler_run
# todo: handle this </s> @param contact: the contact whose presence changed	on_contact_memberships_changed def on_contact_memberships_changed(self, contact): @type contact: L{Contact<papyon.profile.Contact>} @see: L{Memberships<papyon.profile.Membership>}"""
# todo: parent searching is not yet implemented </s> post = self.getelementsbyclass(clef.clef)	getClefs >>> len(c) == 1 True if len(post) == 0 and searchContext: post = Stream()
# todo: implement this rpc service </s> return empty_pb2.empty()	pull_variable def pull_variable(self, request, _):
# todo: fix this for mmcblk devices </s> device_path = row[col_device][:len("/dev/sdx")]	start_installation otherOS = row[COL_DETECTED_OS] fs_type = row[COL_FILESYSTEM] new_size = self.new_size res = fs.resize(partition_path, fs_type, new_size)
# todo: refactor, move to utils </s> dict = {}	get_form_data_context def get_form_data_context(self, form_data): if form_data: for field in form_data:
# todo: what does failing here look like? </s> else:	__call__ if name: repo.fetch(remote=name, refspec="git-annex") try: std_out, std_err = \
# todo: consider adding a parameter `ignore_padding=true` and when it's </s> start_row = start_row if start_row is not none else min_row	import_from_xls min_row, min_column = get_table_start(sheet) max_row, max_column = sheet.nrows - 1, sheet.ncols - 1 end_row = end_row if end_row is not None else max_row start_column = start_column if start_column is not None else min_column
# todo: @sbharadwajj implement and test </s> return true	hessian_is_zero def hessian_is_zero(self):
# todo: remove deprected flags in 1.2 </s> self._deprecated_fit, self._deprecated_partial_fit = true, false	fit self Fitted estimator. return self._fit(X, partial=False)
# todo: kept for backwards compatibility. </s> self._written_event_source_index = 0	__init__ self._storage_type = storage_type
# todo: move this function somewhere else </s> def get_raw_directory_stats(path_obj):	get_raw_directory_stats Example:: {'translated': {'units': 0, 'percentage': 0, 'words': 0},
# todo(justinsb): mock doesn't yet do this... </s> self.assertequal('active', found_server['status'])	test_deferred_delete_force created_server_id = created_server['id'] found_server = self._wait_for_state_change(created_server, 'BUILD') self.api.delete_server(created_server_id) found_server = self._wait_for_state_change(found_server, 'ACTIVE')
# todo: implement </s> return true	user_has_resource_permission def user_has_resource_permission(self, user_db, resource_db, permission_type):
# todo: [code] what to do to leave this? </s> if stop_system_importer_file_csv_run:	system_upload else: stop_system_importer_file_csv = run_check_config_attributes(model) pass model = SystemImporterFileCsvConfigModel.objects.get(system_importer_file_csv_config_name = 'SystemImporterFileCsvConfig')
# todo docstring </s> logging.info('begin extract')	extract def extract():
# todo: for multi-gpu, the final-loss will probably have to come from the optimizer. </s> if isinstance(memory, prioritizedreplay):	update_from_memory policy_vars = self_.call(policy._variables) step_op = self_.call(optimizer.step, policy_vars, loss) update_pr_step_op = self_.call(memory.update_records, sample_indices, loss_per_item) return step_op, loss, records, q_values_s, update_pr_step_op
"""todo: check this implementation""" </s> unit = (	checkerboard def checkerboard(shape, parity="even", dtype=tf.bool): tf.constant((True, False)) if parity == "even"
# todo(nakago): check why tolerance is high </s> def test_backward_cpu(model, data):	test_backward_cpu atom_data, adj_data, y_grad = data gradient_check.check_backward(model, (atom_data, adj_data), y_grad,
data.append(get_casedb_schema(app))  # todo use domain instead of app </s> data.extend(item_lists_by_domain(domain))	get_data_schema if app.domain != domain: raise Http404() kw = {} if "pretty" in request.GET:
# todo stub </s> pass	build_mysql_connection_string def build_mysql_connection_string(server, database, userid, password):
# todo: move this! this is generic code. </s> model = main_window.tree_model	_load self.filename = filename view.expand_root_nodes() try: iter = model.get_iter((0,))
# todo this requires fleshing out some more.. </s> field = string(name='name', required=true)	test_string_input_unicode def test_string_input_unicode(): output = {} field.marshal({'name': u'unicöde'}, output)
# todo: are we still using this? </s> lbrycrdd_path_conf = os.path.join(os.path.expanduser("~"), ".lbrycrddpath.conf")	set_wallet_attributes self.wallet_conf = os.path.join(self.wallet_dir, "lbrycrd.conf") if os.name != 'nt': if not os.path.isfile(lbrycrdd_path_conf): f = open(lbrycrdd_path_conf, "w")
# todo: handle `stream.close` and `stream.reset` </s> peer_id = stream.mplex_conn.peer_id	Node return beacon_blocks_response.blocks async def _handle_recent_beacon_blocks(self, stream: INetStream) -> None: if peer_id not in self.handshaked_peers: self.logger.info(
# todo refactor this to take an arbitrary number of models rather than just a linear and random forest </s> fpr_linear, tpr_linear, _ = sklearn.metrics.roc_curve(y_test, y_probab_linear)	display_roc_plot Returns: matplotlib.figure.Figure: The matplot figure roc_auc_linear = sklearn.metrics.auc(fpr_linear, tpr_linear) fpr_rf, tpr_rf, _ = sklearn.metrics.roc_curve(y_test, y_probab_rf)
# todo in python 2.7 and later, this should be a dict comprehension. </s> attributes = dict((column, getattr(instance, column))	DefaultSerializer foreign_key_columns = foreign_keys(model) columns = (c for c in columns if c not in foreign_key_columns) for column in columns) attributes = dict((k, (v() if callable(v) else v))
# todo implement .!{cmd} (ex shell out) test for windows </s> self.assertcontentmatches(r"\s*2\nbbb\nccc")	test_simple_filter_through_shell })
# matches sphinx-autodoc behaviour of comma separated values </s> members = set([x.strip() for x in text.split(",")])	create_filter OpenFilter() ) return CombinedFilter(OpenFilter(), NamedFilter(members))
# if intersection has changed, add sons to the todo list </s> if new_dom == dominators[node]:	compute_dominators new_dom = set() new_dom.update(set([node])) continue dominators[node] = new_dom
# todo: for the domain-allocation switch, this needs to return the shape </s> raise notimplementederror	shape_with_halo Shape of the domain plus the read-only stencil boundary associated with this :class:`Function`.
# todo discont: use offsets instead (note need for int conversion) </s> tb_ann.start = int(start)	_edit_span else: before = unicode(tb_ann) tb_ann.end = int(end) tb_ann.text = ann_obj._document_text[tb_ann.start:tb_ann.end]
from_block = 0  # todo: we can do better. get contract creation block. </s> if not to_block:	wrapper def wrapper(from_block=None, to_block=None, **argument_filters): if not from_block: to_block = 'latest' event_method = getattr(self.contract.events, event_name)
# todo @niklasrosenstein: handle metaclass arguments </s> pass	parse_classdef metaclass = value else: else: bases.append(Expression(str(index.current)))
# todo: rename int_to_str?  or str::from_int()? </s> if (callee_name not in ('str',) and	visit_call_expr if isinstance(callee_type, CallableType): ret_type = callee_type.ret_type isinstance(ret_type, Instance) and callee_name == ret_type.type.name()):
# todo: it is not needed </s> recent = [pretty_path(p) for p in recent]	move_recent_projects_to_top j = JsonFile(os.path.join(self.primary_dir, 'recent.json')) recent = j.load([]) return plist.sort( key=lambda p: recent.index(p[3]) if p[3] in recent else -1,
# todo set only zero order </s> vals = nm.repeat([fun], nods.shape[0] * dpn)	set_surface_dofs nods = nm.unique(nm.hstack(aux)) if nm.isscalar(fun): elif isinstance(fun, nm.ndarray): assert_(len(fun) == dpn)
# todo: we have to put [:100] on the end of the query due to issue #3431 </s> def test_search_all(self):	test_search_all results = self.backend.search(None, models.Book)[:100] self.assertEqual(set(results), set(models.Book.objects.all()))
pass # todo implement displaying this in the conversationwidget </s> if conversation_id == self.conversation_id:	on_typing_update def on_typing_update(self, conversation_id, user_ids, typing_status):
# todo: candidate for move to system/hdparm </s> out, err, rc = run_command(	get_disk_power_status :return: single word sting of state as indicated by hdparm -C /dev/<disk> and if we encounter an error line in the output we return unknown. [HDPARM, '-C', '-q', '/dev/disk/by-id/%s' % dev_byid], throw=False) if len(err) != 1:
lookup_view(self.window.active_view()).view_breakpoints() #todo fix view </s> def cansetscriptsource(self, command):	SwiDebugStartCommand pass save_breaks() during debugging assert_main_thread()
return self._plugin.content_assist_values(value) # todo: remove old functionality when no more needed </s> return self._controller.get_local_namespace_for_row(row).get_suggestions(value)	get_suggestions if self._controller and row:
# todo: investigate why results are sometimes 'nan' </s> return	draw_eyeball_outline ) except ValueError: draw_polyline( pts, 2, RGBA(0.0, 0.9, 0.1, pupil_detection_result_3d["model_confidence"])
# todo(developer): uncomment and set to a path to your audio file. </s> with io.open(speech_file, 'rb') as audio_file:	transcribe_file_with_auto_punctuation from google.cloud import speech_v1p1beta1 as speech client = speech.SpeechClient() content = audio_file.read() audio = speech.types.RecognitionAudio(content=content)
# todo(jflesch): i18n/l10n </s> txt = "updating label ..."	__cb_progress txt = "Sorting ..." elif step == DocSearch.LABEL_STEP_UPDATING: if txt != None and doc != None: txt += (" (%s)" % (str(doc)))
uploader.upload_file(file, container='export') # todo: right container folder?! </s> finally:	export_json memzip = make_onefile_memzip(datafile.name, '%s_task.json' % name) file = FileStorage(filename='%d_%s_task_json.zip' % (app.id, name), stream=memzip) datafile.close() json_task_run_generator = respond_json("task_run", app.id)
# todo: use invalidation time </s> params: userfull to dump, mediaid of the profile photo in the db	dump_user def dump_user(self, user_full, photo_id): Returns -, or False if not added""" timestamp = round(time.time())
# todo: cut this down somehow? </s> logger.warning('log from failed pod: %s', self._getlogforpod(pod))	_getUpdatedBatchJobImmediately pod.status.start_time).total_seconds() if chosenFor == 'failed': else: assert chosenFor == 'stuck'
# todo: remove check once pytorch avoids a copy for this case </s> if p.data_ptr() != p_data_fp32.data_ptr():	step p_data_fp32.add_(-(1 + momentum) * lr, d_p) buf.mul_(momentum * lr_correct).add_(-lr, d_p) p.data.copy_(p_data_fp32) group['lr_old'] = lr
# todo: change user for calling importer </s> csv_import()	system @login_required(login_url="/login") def system(request): return redirect(reverse('system_list'))
# todo: gan may yield unstable results; turning performance check off </s> self.x_train, self.y_train, self.x_test, self.y_test = generate_data(	setUp self.n_features = 10 self.contamination = 0.1 n_train=self.n_train, n_test=self.n_test, n_features=self.n_features, contamination=self.contamination,
# todo yield </s> raise runtimeerror("sibling '{0}' already exists with conflicting"	__call__ conflicting.append(repo_name) if not force and conflicting: " settings for {1} dataset(s). {2}".format( name, len(conflicting), conflicting))
output_zero_point = none  # todo non-zero zero point </s> output = quanttensor(	__truediv__ output_zero_point = self.zero_point / other.zero_point else: value=output_tensor, scale=output_scale,
# todo: handle 'narg' and 'append' options </s> cmd_str[0] += '"{}" '.format(self._args[action.dest])	traverse_parser if action.option_strings: cmd_str[0] += '{} '.format(action.option_strings[0])
# todo: description </s> def __ifaceattributes___ipv6_parse(ipv6, dct):	__ifaceAttributes___ipv6_parse global parse_ipv6_sourceguard global parse_ipv6_raguard
pass # todo: raise exception </s> else:	disconnect pass # TODO: raise exception
# todo: this will greedily query the same cases multiple times in a sharded </s> batch = formaccessorsql.get_forms_received_since(start_from, limit=chunk_size)	get_all_forms_received_since def get_all_forms_received_since(received_on_since=None, chunk_size=500): start_from = received_on_since or datetime.min while len(batch) == chunk_size: for form in batch:
tasks = ss("#todo-list>li") </s> visit("http://todomvc.com/examples/troopjs_require/#/")	test_create_task def test_create_task(): for task_text in ["1", "2", "3"]: s("#new-todo").set(task_text).press_enter()
# todo(kpy): this only works for subdomains that have a single fixed </s> if date:	from_local_time def from_local_time(self, date): the current subdomain. For convenience, returns None if date is None.""" if self.config.time_zone_offset: return date - timedelta(0, 3600*self.config.time_zone_offset)
# todo: remove </s> g.q = q	_read } q = request.params.get(u'q', u'') if g.group_dict.get(u'is_organization'): q += u' owner_org:"%s"' % g.group_dict.get(u'id')
# todo: replicate complete behaviour of urllib.urlopener.open </s> return filename, headers	__tuf_retrieve )
# todo(cutwater): replace `.decode('utf-8')` call with subprocess </s> tag_info = subprocess.check_output([	get_git_version :raises RuntimeError: If cannot determine git version string. try: 'git', 'describe', '--always', '--match', TAG_PREFIX + '*'] ).decode('utf-8').strip()
# todo: used to ignore eoferror. i hope things still work. </s> self.tracks.append(read_track(infile, debug=self.debug, clip=self.clip))	_load _dbg('Track {}:'.format(i))
'message': 'certainly <b> unfortunate </b>',  # todo: <- </s> }	get 'email': [(i, r) for i, r in enumerate(spam['author'].emails)], 'subject': 'Reports of spam', form = self.form_class(initial=initial) self.success_url = None  # TODO: <-
#todo: save the hotspot_x and y in the filename </s> for frame in self.get_frames():	save_pngs png_filename = "%06d_%03d_%02d.png" % (self.file_id, frame.frame_id, player_id) frame_path = os.path.join(destination_path, png_filename)
# todo handle 404 </s> return response(status=status.http_204_no_content)	partial_update def partial_update(self, request, minion_id): self._partial_update(minion_id, request.DATA)
# todo: check return value of attachthreadinput properly </s> if isinstance(keys, six.text_type):	TypeKeys window_thread_id = win32functions.GetWindowThreadProcessId(self, 0) win32functions.AttachThreadInput(win32functions.GetCurrentThreadId(), window_thread_id, win32defines.TRUE) aligned_keys = keys.encode(locale.getpreferredencoding(), 'ignore') elif isinstance(keys, six.binary_type):
#todo: milestones = get_milestones(destination_url) </s> milestones = []	import_all_open_issues labels = get_labels(source_url) import_labels(labels) labels = get_labels(destination_url) issues = get_issues(source_url)
# todo(sloria): test me </s> raise httperror(http.bad_request)	dropbox_upload return metadata_to_hgrid(metadata, node=node, permissions=permissions)
# todo: choose one from the following two </s> config.add_no_good_cuts = true	solve config.add_slack = False if config.strategy == "GOA": config.use_tabu_list = False config.add_slack = False
# todo(laigd): remove this check when 313682500 is in the release. </s> if use_tf_function and tf.compat.v1.__version__ < '2.3.0':	testScalarInput ) def testScalarInput(self, use_tf_function): return FLAGS.if_use_tf_function = use_tf_function
# todo: replace with "yield from" when dropping python 2. </s> for stream in streams.items():	_create_stream try: streams = HLSStream.parse_variant_playlist(self.session, stream_url) yield stream except IOError as err:
# todo: require tests </s> return true	hessian_is_zero def hessian_is_zero(self):
# todo: check if this different handling of none and '' has </s> if result == none:	run args['query'] = query result = self._query(vector, args) log.warn('%s %s' % (messages.module_sql_console.no_data, messages.module_sql_console.check_credentials)
# todo username </s> sudo = args.pushy('ssh+sudo:{hostname}'.format(hostname=hostname))	mds_create bootstrapped = set() for hostname, name in args.mds: if hostname not in bootstrapped: bootstrapped.add(hostname)
# todo implement other types </s> return	resample_time_series_frequency data_frame_r = data_frame.resample(data_resample_freq).last() else: if fill_empties == True: pass # TODO
# todo(nisanthan): revisit for supporting multiple codeowners. </s> codeowners = (	get_codeowners_cached codeowners = cache.get(cache_key) if codeowners is None: self.objects.filter(project_id=project_id).order_by("-date_added").first() or False )
# todo: the error raised here should be different </s> raise rpcerror("no schema results for table %s.%s" % (database_name, table_name))	get_table_schema results = self.fetchall() if len(results) == 0: tables = set() for col in results:
# todo: have a single list in place of two directional ones? </s> for direction in ['vertical', 'horizontal']:	_generate_table table = table.set_span() pos_errors = [] for t in self.t_bbox[direction]: indices, error = get_table_index(
check_result.syntax_type = 2  # todo 工单类型 0、其他 1、ddl，2、dml </s> for statement in sqlparse.split(sql):	execute_check critical_ddl_regex = config.get('critical_ddl_regex', '') p = re.compile(critical_ddl_regex) statement = sqlparse.format(statement, strip_comments=True) if re.match(r"^select", statement.lower()):
except exception:   # todo </s> raise exceptions.addresserror('invalid address:', addresses[0])	transaction try: base58_decode(addresses[0], config.ADDRESSVERSION) if not config.UNITTEST and encoding in ('multisig') and not public_key: if not rpc('validateaddress', [source])['ismine']:
raise mpdnotimplemented # todo </s> directory.	_stored_playlists_save Saves the current playlist to ``NAME.m3u`` in the playlist
# todo remove this constraint </s> finetuning = finetuning[finetuning['epoch'] <= 20]	df_from_results finetuning = pd.read_csv(exp / 'finetuning.csv') if eval(params['dataset']).startswith('ImageNet'): row += [finetuning['val_acc1'].max(), finetuning['val_acc5'].max()] else:
# todo(qos): register the callbacks to properly manage </s> pass	register_port_callbacks def register_port_callbacks(self):
# todo: find a better way to identify if it is a parameter schema </s> return 'in' in schema_object_spec	is_param_spec def is_param_spec(schema_object_spec):
# todo: implement shape inference for xladynamicslice </s> res.set_shape(tuple(slice_sizes))	_dynamic_slice res = tfxla.dynamic_slice(operand, tf.stack(start_indices), size_indices=slice_sizes) return res
# todo: clean up py3k support </s> if version_info < (3, 0):	get_encoding def get_encoding(page, contents=None): charset = page.headers.getparam("charset") or None else:
# todo: raise alert for connection failure </s> return	sync_zfs_keys config = self.middleware.call_sync('kmip.config') if self.test_connection()['error']: if config['enabled']: failed = self.sync_zfs_keys_from_db_to_server(ids)
# todo(stephenfin): remove this as it's related to nova-network </s> if 'security_groups' in expected_attrs:	_from_db_object objects.PciDevice, db_inst['pci_devices']) instance['pci_devices'] = pci_devices sec_groups = base.obj_make_list( context, objects.SecurityGroupList(context),
# todo: serialization of new version realtions is disabled </s> ]	test_related_identifiers_serialization 'relation': 'isPartOf' } assert rids == expected_v1 rids = serialize_related_identifiers(recid_v2)
# todo(mierdin): note that this will always return true if rbac is not enabled </s> if rbac_utils.user_has_role(requester_user, role):	put LOG.info("Checking user %s is in role %s" % (requester_user, role)) LOG.info(rbac_utils.user_has_role(requester_user, role)) break else:
).consume()  # todo see issue 170 </s> for instance in reservation["instances"]:	load_ec2_instances Region=region, aws_update_tag=aws_update_tag, instanceid = instance["InstanceId"] monitoring_state = instance.get("Monitoring", {}).get("State", "")
# todo: there's a race with the initial "output" event. </s> def handle_msg(msg):	handle_msg if msg.type != 'event': return False
# todo: windows git class integration </s> if git("rev-parse", "refs/remotes/origin/master").strip() != git("rev-parse", "master").strip():	add_to_blacklist pass git.remote.update() return (False, "HEAD isn't at tip of origin's master branch") if blacklist_file_name in git.status():  # Also ugly
# todo: rebalance if output distributions are 1d instead of 1d_var </s> df_vars = filter_node.df_vars	filter_distributed_run def filter_distributed_run(filter_node, typemap, calltypes): df_in_vars = df_vars[filter_node.df_in] df_out_vars = df_vars[filter_node.df_out]
# todo(b/150147476, b/150024785): fix tf.function in tf1 crash. </s> if not hasattr(tf.compat.v1, "executing_eagerly_outside_functions"	wrapped_func @functools.wraps(func) def wrapped_func(*args): ) or tf.compat.v1.executing_eagerly_outside_functions(): return tf.function(func=func, **self.func_kwargs)(*args)
# todo add verbose output </s> return self._domain	LocalizationModel @property def domain(self): @domain.setter def domain(self, new_domain):
# todo: переделать механизм pairs </s> pair = pairs.get(abbr[:2], none)	hayaku_extract return STATIC_ABBR[abbr] starts_properties = [] if pair is not None: starts_properties = [prop for prop in prop_iter if prop.startswith(pair)]
# todo verify permission type for the provided resource type </s> resolver = resolvers.get_resolver_for_permission_type(permission_type=permission_type)	user_has_permission if not cfg.CONF.rbac.enable: return True result = resolver.user_has_permission(user_db=user_db, permission_type=permission_type) return result
# todo - verify contents </s> self.client.logout()	testDashboard3 response = self.client.get('/dashboard/', {'view': 'to-me'}) self.assertEqual(response.status_code, 200)
# todo: remove this if/when we support rh mode. </s> dot_sens.options["dsdp_mode"] = ""	__init__ if dot_sens is None: dot_sens = SolverFactory("dot_sens") if k_aug.available(): self._k_aug = k_aug
# todo stub </s> def test_plot() -> none:	test_plot pass
# var_params = config.get_variable_params()  # todo: better way? </s> output_filename = get_filename(filename_template,	create_storage_unit measurements = list(stat.product.measurements.values()) datasets, sources = find_source_datasets(task, stat, geobox) task['tile_index'], task['start_time'])
# todo(ut) monkey patch </s> if encoding == u'cp65001':	decoder if isinstance(byte_str, unicode) or byte_str is None: return byte_str if encoding: encoding = u'utf-8' try: return unicode(byte_str, encoding)
# todo(stubexecutor): customize self.stubbed_component_ids to replace components </s> self.stubbed_component_ids = ['csvexamplegen', 'statisticsgen',	__init__ super(StubComponentLauncher, self).__init__(**kwargs) self.test_data_dir = "gs://{}/testdata".format(configs.GCS_BUCKET_NAME) 'SchemaGen', 'ExampleValidator', 'Trainer', 'Transform', 'Evaluator', 'Pusher']
# todo: should be injected </s> self.facade = awsfacade()	__init__ def __init__(self): self.service = 'iam'
# todo: check against plural_rules[lang]['nplurals'] </s> repl = variants.split('|')[plural_func(num)]	twntranslate except KeyError: plural_func = plural_rules['_default']['plural'] trans = re.sub(PATTERN, repl, trans) if param:
# todo: implement </s> if self.turn == white:	pop def pop(self): move = self.move_stack.pop() self.ply -= 1 self.half_moves = self.half_move_stack.pop()
# todo: enable admin tests </s> test_admin(ctx)	test if all: test_addons(ctx) karma(ctx, single=True, browsers='PhantomJS')
# todo remove in 4.0 </s> warnings.warn("message.id_type is deprecated, use is_extended_id", deprecationwarning)	Message @property def id_type(self): return self.is_extended_id @id_type.setter
raise notimplementederror # todo </s> def log_likelihood(self,data=none,changepoints=none,**kwargs):	_HMMPossibleChangepointsMixin data=data,changepoints=changepoints,**kwargs) def _get_mb_states_list(self,minibatch,changepoints=None,**kwargs): raise NotImplementedError # TODO
pass # todo </s> def add_operation(self, operation):	add_operation
#todo: python2 specific, remove </s> escaped_fn = filename	main ).decode() except UnicodeDecodeError: file_start = timeit.default_timer() try:
# todo: remove this hack asap </s> _load_skills()	_starting_up time.sleep(0.5)  # Allows system time to start the eyes spinning subprocess.call('systemctl reboot -i', shell=True)
# todo(huangtianhua): remove this method when bug #1479641 is fixed. </s> return six.text_type(getattr(vol, name))	_resolve_attribute return vol.description
# todo use properties here to infer mechanism and purview from </s> return mip(direction=direction,	_null_mip @staticmethod def _null_mip(direction, mechanism, purview): mechanism=mechanism, purview=purview,
# todo: remove when #980 has been merged </s> return info	_format info_dict['url'] = format_info['url']
# because it's being configured too late -- bad! todo refactor! </s> event_logger.format_json = true	set_log_format if flags.LOG_FORMAT == 'json': log_manager.format_json() else: log_manager.format_text()
# todo unordered float </s> if a is none and b is none:	fcomi def fcomi(ir, instr, a=None, b=None): a, b = float_st0, float_st1 elif b is None:
#  todo: resolve symlinks etc </s> sys.executable = abspath(pathjoin(getcwd(),argv[0]))	entry_point def entry_point(argv): exit_code = 0 sys.argv = argv try:
# todo: don't write them? is *much* slower on re-load (~3x) </s> try:	update_module fh.flush() os.fsync(fh.fileno())  # flush to disk os.unlink(self.module_path + 'c') except OSError:
# todo (jian): double check why typing is crashing </s> module = self.get_submodule(module_name)  # type: ignore	apply_to_input ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]: if module is None: if param is not None: assert module_name == param.name
# todo thread safe </s> return self.releaseme	testForRelease def testForRelease(self):
#todo make more easy n1, n2, n3 </s> def save(data):	slotNewAccount types = self.connector.proxy.getAccountTypes() self.accountEdit = AccountEdit.newAccount(types) if data["password"]: self.accountEdit.close()
# todo: write this method if possible </s> pass	address_transactions def address_transactions(self, addresslist):
# todo(dcramer): ideally we could just send the signal to the subprocess </s> self.task.status = taskstatus.failed	_read_timeout self._logreporter.terminate() self._logreporter.save_chunk('>> Process did not receive updates in %ds\n' % self.read_timeout) self.task.date_finished = datetime.utcnow() db.session.add(self.task)
# @todo: investigate why this isn't working </s> db = current.db	setup_run_playbook if hosts is None: hosts = ["127.0.0.1"] W2P_TASK = current.W2P_TASK table = current.s3db.scheduler_run
# todo: dump to file </s> if self._in == none:	write if "bistream" in self._modes: self._bistream.append(("in", data)) return data = data[12:]
# todo: migrate to new tilegrid format via library. </s> grid = {	run os.getenv("XRAY_DATABASE")), "r") as f: new_grid = json.load(f) 'tiles': new_grid, 'segments': {}
# todo: test coverage of this branch </s> raise http404(ugettext('no html template associated with the '	archive_detail EmailTemplate.get_templates('message', message.newsletter) if not html_template: 'newsletter this message belongs to.')) c = Context({
# todo this needs to be done on the content but seems to be a non-trival </s> xpath = util.get_value(conf["xpath"], _input, **kwargs)	pipe_xpathfetchpage content = unicode(request.read(), request.headers['content-type'].split('charset=')[-1]) html5 = False useAsString = False
# todo(mattjj): re-enable </s> update(opt_state, 0.9)  # doesn't crash	testTracedStepSize return update_fun(0, g, opt_state)
# todo: use the xmlrpc-c type indicators instead / additionally </s> if arg and arg[0] in "+-":	mainloop args = [] for arg in raw_args: try: arg = int(arg, 10)
# todo: prompt </s> pass	quit def quit():
# todo: provide more informative errors </s> try:	create_policy Character control endpoint for creating a policy and making arrangements with Ursulas. bob_pubkey = bytes.fromhex(request.args['bob_encrypting_key']) label = bytes.fromhex(request.args['label'])
# todo: remove this function </s> logger.warning((	__getitem__ def __getitem__(self, key): DEPRECATED: please use ``Inventory.get_host`` instead. 'Use of Inventory[<host_name>] is deprecated, ' 'please use `Inventory.get_host` instead.'
# todo: avoid use of _objects_by_key_id method </s> inp_keys, script_type, key = _objects_by_key_id(key_id)	transaction_create value = inp_utxo.value amount_total_input += value transaction.add_input(prev_hash, output_n, keys=inp_keys, script_type=script_type, sigs_required=self.multisig_n_required, sort=self.sort_keys,
# todo: add at least reflection tests before adding notimplemented version </s> def readcomments(context, uri):	readcomments *musicpd.org, music database section:* ``readcomments [URI]``
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: save additional models </s> return self.response(self.serialize_object(obj))	edit obj, models = self.deserialize_object(data, obj) obj = self.save_object(obj, data)
# todo: this check may hide a bug a should be removed. </s> if date is none:	datetime_to_ISO8601 def datetime_to_ISO8601(date): conver a datetime into ISO8601 date date = datetime_null() return date.isoformat() + "Z" # Z means that the date is in UTC
# todo: preallocate! </s> i, j, v = [], [], []	nodalGrad if getattr(self, '_nodalGrad', None) is None: self.number() E = self._edges N = self._nodes
# todo: unit test this </s> monitor_geom *= self.gdk_screen.get_monitor_scale_factor(monitor_id)	get_monitor monitor_geom = Rectangle.from_gdk( self.gdk_screen.get_monitor_geometry(monitor_id)) logging.debug(" Window is on monitor %s, which has geometry %s", monitor_id, Rectangle.from_gdk(monitor_geom))
# todo: funcname, funcbody </s> self.assertequal(16, p._pos)	testStatFunction self.assertIsNotNone(node)
# todo: consider returning an empty {} rather than raising </s> assert_raises(attributeerror, keyed_tuple._asdict)	test_values_but_no_labels eq_(keyed_tuple.__dict__, {}) assert_raises(AttributeError, keyed_tuple.keys) def should_raise(): keyed_tuple._fields
#todo redownload and verify against original? </s> self.api.change_song_metadata(orig_md)	upload_album_art self.api.upload_album_art(self.song.sid, test_utils.image_filename)
# todo test me ! </s> @register.filter	dict_get def dict_get(dictionary, key): try:
pass # todo </s> def handle_request(self, input):	handle_request
#todo: this is identical to regex in line 33 of subscribe.py! </s> regexword = "\\b" + word + "\\b"	words_in_text def words_in_text(word, text): return True if re.search(regexword, text, re.IGNORECASE) else False
# todo: only allow this if the task is still in error state </s> self.tiger._redis_move_task(self.queue, self.id, error,	delete else: remove_task = 'always' remove_task=remove_task)
# todo: add for morph targets data. </s> min_index = min(indices)	extract_primitive_floor process_bone = False bone_max = bone_index max_index = max(indices) for old_index in indices:
# todo: enable specificity beyond hostname (e.g. include scheme, port) </s> tuf_configuration = \	make_tuf_updater def make_tuf_updater( url ): parsed_url = urlparse.urlparse( url ) TUFUpdater._tuf_configurations.get( parsed_url.hostname ) if tuf_configuration is None:
# todo: this causes a blank window to be shown. </s> m = toga.window()	startup file_name = os.path.abspath(sys.argv[1]) except IndexError: file_name = m.select_folder_dialog(self.interface.name, None, False)[0] self.open_document(file_name)
# xxx todo </s> return address, size	input_address_range size    = None
# todo: handle timeout </s> return self.__socket.recv(1024)	read @keyword timeout: the maximum time in millisecond to wait before a message can be reached @type timeout: :class:`int`
# todo: matthewp, profile this transfer </s> sequence_lengths = mask.sum(dim=1).data.cpu().numpy()	remove_sentence_boundaries new_mask : ``torch.Tensor`` The new mask for the tensor of shape ``(batch_size, timesteps - 2)``. tensor_shape = list(tensor.data.shape) new_shape = list(tensor_shape)
# todo: in the future you can also add the possibility to synchronize from a chosen profile </s> is_account_owner = g.local_db.get_profile_config('isaccountowner', false)	_sync_library 'remove': 'remove_silent'}.get(operation) if operation and g.ADDON.getSettingBool('mylist_library_sync'): if not is_account_owner: return
# todo: remove this method in v2.5 </s> elif self._values['disabled'] in booleans_true:	disabled if self._values['state'] == 'disabled': return True return True elif self._values['disabled'] in BOOLEANS_FALSE:
# todo(ringw): fix barline detection here. </s> self.assertequal(len(page.system[1].staff), 2)	testIMSLP00823_008_mergeStandardAndBeginRepeatBars self.assertEqual(len(page.system), 6) self.assertEqual(len(page.system[0].staff), 2) self.assertEqual(len(page.system[2].staff), 2) self.assertEqual(len(page.system[2].bar), 7)
# todo: establish what's the increment of ctc decoder (how many new letters might be there) [rough estimate] </s> h, lpz = self._input_window_for_decoder()	advance_decoder def advance_decoder(self): self._last_recognition = self._e2e.dec.recognize_beam(h, lpz, self._recog_args, self._char_list, self._rnnlm)
# todo: this doesn't always work. for some scores where a part uses more than one clef, more </s> allpartns = allpartspresent(documentroot.findall('.//{mei}music//{mei}score//{mei}staffdef'.format(mei=_meins)))	convertFromString break environLocal.printDebug('*** finished tuplets') parsed = {n: stream.Part() for n in allPartNs} inNextMeasure = {n: stream.Part() for n in allPartNs}
# todo(shardy): remove when we no longer support essex </s> nova = client.client(username=con.username,	authenticate no_cache=True) except TypeError: api_key=con.password, project_id=con.tenant,
# todo: test me @jmcarp </s> logging.error('could not access github repo')	github_hgrid_data repo = connection.repo(node_settings.user, node_settings.repo) except NotFoundError: return None if repo.private:
# todo: this needs to be deferred but for now we hard code </s> d = augment.augmentservice.getaugmentrecord(record.guid)	collectResults memberGUIDs           = (), ) d.addCallback(lambda x:record.addAugmentInformation(x)) yield record
# todo: 验证 localport 有效性 </s> config = config._replace(localport=localport)	main if args.l: localPort = args.l if args.k: try:
# todo not portable, redo. </s> return	test_exploration_with_discrete_action_space def test_exploration_with_discrete_action_space(self): action_space = IntBox(5, shape=(2, 2), add_batch_rank=True) distribution = Categorical()
pass  # todo </s> def change_metadata(self):	change_metadata @song_test
# todo: skipped due to gh-4436 </s> yield known_failure_windows(skip_ssh(_test_initremote_alias)), \	test_initremote_alias def test_initremote_alias(): 'datalad-test' yield _test_initremote_alias, None
# todo: getattr doesn't return by default members </s> metaclass = owner.metaclass()	visit_getattr continue if isinstance(owner, astroid.Class): try: if metaclass and metaclass.getattr(node.attrname):
# todo: error detection </s> __connect()	serialize_item def serialize_item(obj, item): collection = mongodb[obj.collection_type()] data = collection.find_one({'name':item.name})
# todo: wrap backend call in error handling. </s> backend = self._get_backend(pending)	_on_about_to_finish pending = self.core.tracklist.eot_track(self._current_tl_track) while pending: if backend and backend.playback.change_track(pending.track).get(): self._pending_tl_track = pending
#todo delete backward compatibility check after some versions </s> last_column_table = self.cur.execute('pragma table_info(scan_history)').fetchall()[-1][1]	record_insert def record_insert(self, record): try: if last_column_table == self.table_columns[-1]: self.cur.execute('insert into scan_history({table_columns}) values (?,?,?,?,?,?,?,?,?,?)'.format(
# todo: launch visitor on node </s> return node	visit_Call def visit_Call(self, node): if not isinstance(node.func, ast.Name): fc_name = node.func.id new_name = fc_name
# todo: handle this case properly </s> continue	_add_paired_branches review_branch = abdt_naming.makeReviewBranchFromName(b) if review_branch is None: review_branch = abdt_gittypes.makeGitReviewBranch( review_branch, git.get_remote())
# todo results from p0f </s> return	_get_prev_ipv4_oses @staticmethod def _get_prev_ipv4_oses(endpoint):
raise notimplementederror  # todo ... </s> print("finishdiscard()")	finishDiscard def finishDiscard():
# todo: fix this assertionerror: eol while scanning string literal </s> self.assertcodeexecution("""	test_isspace def test_isspace(self): print(bytearray(b'testupper').isspace()) print(bytearray(b'test isspace').isspace())
# todo: also preserve __module__, __name__ and a few other important attrs </s> return new_fn	profile return fp(*args, **kw) new_fn.__doc__ = fn.__doc__
# todo: support multiple sourcestamps </s> sourcestamp = build_status.getsourcestamps()[0]	BuildJsonResource JsonResource.__init__(self, status) self.build_status = build_status self.putChild('source_stamp', SourceStampJsonResource(status, sourcestamp))
# forward all other methods. todo(l.zou): could use a proxy to automate these </s> return modified_grad	compute_gradients modified_grad.append((grad, var))
# todo: will be replaced with exception in future releases </s> logger.error('telegram does not support inline keyboard row width over %d.' % self.max_row_keys)	InlineKeyboardMarkup :return: if row_width > self.max_row_keys: row_width = self.max_row_keys self.row_width = row_width
#todo: why can't we read emboss's swiss output? </s> self.assert_(os.path.isfile(filename))	check_can_read_emboss_conversion def check_can_read_emboss_conversion(self, filename, old_format, new_formats=["genbank","fasta","pir","embl"]) : old_records = list(SeqIO.parse(open(filename), old_format)) for new_format in new_formats :
# todo(b/148082271): remove this line once tft 0.22 is used. </s> transformed_features.pop(	serve_tf_examples_fn parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec) transformed_features = model.tft_layer(parsed_features) features.transformed_name(features.LABEL_KEY), None) return model(transformed_features)
# todo: make test method </s> ssl             # missing ssl extension?	test_ssl def test_ssl(): import ssl return True
raise invalidpocketleveldbworldexception()  # todo maybe try convert/load old pe world? </s> if len(root_tag_buf) != nbt.tag_int.fmt.unpack(length)[0]:	_loadLevelDat if nbt.TAG_Int.fmt.unpack(magic)[0] < 3: logger.info("Found an old level.dat file. Aborting world load") raise nbt.NBTFormatError() self.root_tag = nbt.load(buf=root_tag_buf)
# todo: somehow caused by circular import under python3 refactor </s> if sys.version_info > (3, 0):	startServer web.helper.global_WorkerThread.stop() sys.exit(1) self.restart = web.py3_restart_Typ if self.restart == True:
"meta.deleted": false,  # todo(tsileo): retrieve deleted and expose tombstone </s> 'type': {'$in': [activitytype.create.value, activitytype.announce.value]},	nodeinfo q = { "box": Box.OUTBOX.value, } return Response(
# todo: total hack below. implement more principled formatting. </s> def process(line):	process if not line.startswith('#'): return '    ' + line
# todo implement project_out and uncomment this </s> mean_appearance = self.appearance_model.mean	_align def _align(self, max_iters=30): error = self.eps + 1 while self.n_iters < (max_iters - 1) and error > self.eps: IWxp = warp_image_onto_template_image(
# todo: floats </s> new_children.append(child)	block_container_layout document.fixed_boxes.append(child) else: continue if child.style.position == 'relative':
# todo: make these http requests asynchronous. not easy since we don't </s> url = api_retweets_url % tweet['id']	get_activities if (not tweet.get('retweeted') and        # this *is not* a retweet tweet.get('retweet_count', 0) >= 1):  # this *has* retweets
# todo: this is an ugly workaround. remove after refactoring the player and titlebar. </s> self._player._first_play = false	__on_rewind_clicked def __on_rewind_clicked(self, button): self._player.rewind() if self.progress_scale.get_value() > 30:
## todo: # fixme: remove me </s> try:	analyse tld = url_parsed['tld'] if tld is not None: tld = tld.decode() except:
# todo(blee): this should be derived from the dataset size. </s> decode_program_params.steps_per_loop = decode_steps_per_loop	SimpleProgramScheduleForTask decode_program_params = DecodeProgram.Params() decode_program_params.name = 'decode_tpu' decode_program_params.dataset_name = dataset_name program_schedule_params.eval_programs.append(decode_program_params)
# todo private access.. </s> argument_iterator = field_tree_instance._arguments.unpack()	_infer_field if field_tree_instance.py__name__() == 'ForeignKey': if isinstance(field_tree_instance, TreeInstance): key, lazy_values = next(argument_iterator, (None, None)) if key is None and lazy_values is not None:
# todo: we currently use the same data for test and validation. </s> return self.eval_dataset	test_dataset @property def test_dataset(self):
# todo: warn/error: check if this var has units: assigning </s> pass	_process_bound def _process_bound(self, val, bound_type): if type(val) in native_numeric_types or val is None: elif is_potentially_variable(val): raise ValueError(
@skipif('device-openmp')  # todo: still unsupported with openmp, but soon will be </s> assert np.all(usave.data[i, :, -3:] == 0)	test_save_w_subdims assert np.all(usave.data[i, :, :3] == 0)
# todo: rework resolver install system to log and report what has been done. </s> return tool.tool_requirements_status	install_dependencies if kwds.get('build_dependency_cache'): tool.build_dependency_cache()
# todo(zaneb): ensure parameters can be formatted for xml </s> return key, value	transform if key == engine_api.STACK_ID: return 'URL', stack_url(req, value)
# todo: temporary work around to issue #225 on github </s> clear_cache()	tti_operator def tti_operator(dse=False): problem = setup(dimensions=(50, 50, 50), time_order=2, space_order=4, tn=250.0)
# todo autoescape context </s> return nodelist.render(context(function()))	massaman t = get_template(file_name) nodelist = t.nodelist
pass # todo </s> else:	write_serialize write_serialize_builtin(s, field)
# todo: must be implemented </s> pass	range_from_chapters def range_from_chapters(crawler, times=0):
#todo print appropriate error message </s> raise notimplementederror	get_results Expression('Close', stream).evaluate(evaluation) else: tmp = tmp.get_leaves() assert all(expr.has_form('Rule', None) for expr in tmp)
# todo implement support for this </s> elemdict['pose'] = parsesdfpose(elem.find('pose'))	parseSDFLink elemdict = {'name': name} if objtype == 'collision': else:
# todo(sloria): test me </s> def list_dropbox_files(node, files=none, cursor=none, client=none):	list_dropbox_files node_settings = node.get_addon('dropbox') client = client or get_node_addon_client(node_settings)
federated_only=federated_only,  # todo: 289 </s> )	from_bytes rest_port=rest_info.port, certificate=certificate, return stranger_ursula_from_public_keys
raise notimplementederror  # todo </s> else:	forward posteriors = result[0] elif BackendEngine.is_tensorflow_selected(): raise NotImplementedError("unknown backend engine") if posteriors.ndim == 3:
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
# todo(benjaoming) for 0.15, remove this </s> writable_assessment_items = os.access(	handle call_command("syncdb", interactive=False, verbosity=options.get( "verbosity"), database="assessment_items") settings.KHAN_ASSESSMENT_ITEM_ROOT, os.W_OK) def _move_to_new_location(old, new):
# todo: with git <= 2.3 keep old mechanism: </s> with rm.repo.git.custom_environment(	fetch cnct = ssh_manager.get_connection(fetch_url) cnct.open() GIT_SSH_COMMAND="ssh -S %s" % cnct.ctrl_path): rm.fetch(refspec=refspec, progress=progress)  # TODO: progress +kwargs
data_source_name='case-sql',  # todo: this isn't really needed. </s> document_type='commcarecasesql',  # todo: should this be the same as the couch models?	publish_case_deleted document_id=case_id, data_source_type=data_sources.CASE_SQL, document_subtype=None,  # todo: does this need a value? domain=domain,
# todo: handle errors from _common_run? </s> lineage = acme.obtain_and_enroll_certificate(doms, authenticator,	run return "Configurator could not be determined" acme, doms = _common_run(args, config, acc, authenticator, installer) installer) if not lineage:
# todo: support complex </s> @testing.for_all_dtypes(no_float16=true, no_complex=true)	TestConvolveCorrelate in2 = testing.shaped_random((self.in2,)*in1.ndim, xp, dtype) return getattr(scp.signal, func)(in1, in2, self.mode, method='direct') @testing.numpy_cupy_allclose(atol=1e-5, rtol=1e-5, scipy_name='scp', accept_error=ValueError)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: template requires address otherwise it throws an exception </s> order_with_items_and_stock.shipping_address = billing_address	test_view_order_invoice user selects on extra menu PDF Invoice user downloads the invoice as PDF file order_with_items_and_stock.billing_address = billing_address order_with_items_and_stock.save()
# todo do a proper mro resolution. currently we are just listing </s> for lazy_cls in context.py__bases__():	py__mro__ mro.append(cls) mro = [context] for cls in lazy_cls.infer(): try:
# todo: log exception </s> return false	delete return True except Exception as e:
# todo: should assert that domain exists here but this breaks tests </s> case_db = casedbcache(domain=domain,	get_footprint if not initial_case_list: return {} strip_history=strip_history, deleted_ok=True)
# todo: additional treatment for "too many arguments"? although </s> file_chunks = generate_chunks(files, chunk_size)	_run_command_files_split ) // (maxl + 3)  # +3 for possible quotes and a space ) out, err = "", "" for file_chunk in file_chunks:
# todo: handle fancy-index copies by allocating a buffer and </s> next_index = self._subset_iterator.next()	next def next(self): return self._raw_data[next_index]
# todo: test training too </s> answers = example_reader(questions)	test_simple_mcqa SimpleMCOutputModule()) example_reader.setup_from_data(data_set) assert answers, "Should produce answers"
# todo: specific exception handling </s> term_hierarchy = default	__read_term_hierarchy_file f.close() except: return term_hierarchy
# todo: actually read newtablename. </s> try:	predict def predict(self, tablename, columnstring, newtablename, whereclause, numpredictions): conn = psycopg2.connect('dbname=sgeadmin user=sgeadmin') cur = conn.cursor()
# todo: remove logging </s> log.w(	handle_bounce content_type = msg.get_content_type().lower() if content_type != "multipart/report" or envelope.mail_from != "<>": "Handle auto reply %s %s. Msg:\n%s", content_type,
continue  # todo: 168 check version and update if required. </s> @crosstown_traffic()	node_metadata_exchange for node in nodes: if node.checksum_public_address in self._known_nodes: def learn_about_announced_nodes(): try:
# todo(dolph): can be uncommented pending bug 968519 </s> role)	test_create_null_role_name role['id'],
# todo add test </s> points = np.zeros((nb_steps + 1 + nb_steps + 2*(nb_steps**2), 2), dtype=np.float32)	generate_similar_points_manhattan the second axis contains the x- (first value) and y- (second value) coordinates. The center keypoint (the one on which this function was called) is always included. yy = np.linspace(self.y - nb_steps * step_size, self.y + nb_steps * step_size, nb_steps + 1 + nb_steps)
# todo: make a function and more generic (move to psutil) </s> get_bat_state = s.getoutput("cat /sys/class/power_supply/bat0/status")	autofreq def autofreq(): driver_check() bat_state = get_bat_state.split()[0] if bat_state == "Discharging":
# todo support multiple backends </s> return self.backends[0].stored_playlists.save(playlist).get()	save :param playlist: the playlist :type playlist: :class:`mopidy.models.Playlist`
# todo: boulder messes up content-type #56 </s> pass	_check_content_type def _check_content_type(self, response, content_type):
# todo: log exception </s> username = os.getenv('username')	_main username = os.getlogin() except Exception as e: results.append(( [],
if result.was_accepted:  # todo: here, we need to assess the result and see if we're actually good to go. </s> kfrag = self.assign_kfrag_to_contract(contract)	match_kfrags_to_found_ursulas def match_kfrags_to_found_ursulas(self, found_ursulas): for ursula, contract, result in found_ursulas: contract.activate(kfrag, ursula, result)
self.my_sender('text/cache-manifest', bytes(manifest, 'utf-8')) # todo: cache-control/last-modified headers </s> manifest += '\n# hash: {}'.format(hasher.hexdigest().upper())	generate_manifest manifest += '*\n'
# todo: semi-bounded -> exponential distribution. </s> else:	check_input_spaces elif action_space.low != float("-inf") and action_space.high != float("inf"): self.bounded_action_space = False raise RLGraphError( "Semi-bounded action spaces are not supported yet! You passed in low={} high={}.".\
# todo(b/160795287): deprecate estimator based executor. </s> eval_source = executor._eval_model_path(fn_args.model_run_dir)	run_fn io_utils.copy_dir(serving_source, serving_dest) absl.logging.info('Serving model copied to: %s.', serving_dest) eval_dest = fn_args.eval_model_dir io_utils.copy_dir(eval_source, eval_dest)
# todo ... </s> self.error("preprocessor: if-evaluation not yet implemented; cannot check '" + condstr + "'")	cpreprocess_evaluate_cond def cpreprocess_evaluate_cond(state, condstr): return True # may be more often what we want :P
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pcmpeqb res = 0x000000ff0000ff00ff00000000ffff00 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo(mordred): special casing auth_url here. we should </s> if 'auth' in config and 'auth_url' in config['auth']:	magic_fixes if type(config[key]) is not bool: config[key] = get_boolean(config[key]) config['auth']['auth_url'] = config['auth']['auth_url'].format( **config)
# todo make sure this works </s> log("creating chains...", 'info')	buildModelFromDictionary else: log("  No kinematic groups in model.", 'INFO') if 'chains' in model and model['chains']: for ch in model['chains']:
# todo: needs more testing </s> for (curref, curtyp, curval) in itertools.chain(*self.heaps[snapshot::-1]):	find_key_in_heap ) -> object: with NoTracing(): could_match = dynamic_typing.unify(curtyp, typ) if not could_match:
# todo(inf) commented out lines were only in fnv branch </s> importmenu.links.append(mod_fullnames_import())	InitModLinks importMenu.links.append(Mod_Prices_Import()) elif bush.game.fsName in (u'Fallout3', u'FalloutNV'): importMenu.links.append(Mod_Prices_Import()) importMenu.links.append(Mod_FactionRelations_Import())
raise notimplementederror # todo </s> def assert_populations_eq_komolgorofsmirnov(pop1, pop2, msg=none):	assert_populations_eq_komolgorofsmirnov
# todo: 判断返回结果，处理异常 </s> msg = task.del_user(get_object(permrole, id=role_id).name)	perm_role_recycle recycle_resource = gen_resource(recycle_assets) task = MyTask(recycle_resource) for asset_id in asset_ids: asset = get_object(Asset, id=asset_id)
pass  # todo </s> subparser to add other arguments related to wave_gen function.	add_wave_gen_args subparser : :class:`argparse._SubParsersAction`
# todo pseudo code: </s> pass	OpenUri @dbus.service.method(dbus_interface=player_interface) def OpenUri(self, uri):
except exception:  # todo - which exceptions? </s> start, stop = 1.0, self.sequence_id.index(end)	antiparallel try: start, stop = w.tag_ranges(SEL) seq = w.get(start, stop) seq = re.sub('[^A-Z]', '', seq)
# todo: can only set before calling go() </s> self.canedit = value	setCanEdit def setCanEdit(self, value=True):
# todo add read lock </s> try:	read_current_batch finally: pass self.storage_reader.read() finally:
raise exceptions.mpdnotimplemented  # todo </s> and "message:" lines.	readmessages Reads messages for this client. The response is a list of "channel:"
# todo should we instead pick the best result, instead of just the first? </s> self.store('watched', self.get_trakt_data(p_items[0], include_identifier))	watch if all([not x.seen for x in p_items]): return True
#todo: rewrite this without get_block_data() </s> blocks = get_block_data(df)	_count_grids header = self.dataset.parameters with open(self.dataset.parameter_filename, 'rb') as df: self.num_grids = len(blocks)
pass # todo: raise exception </s> else:	connect pass # TODO: raise exception
# todo: returning httpresponse seems dirty. see if it can be </s> return httpresponse(fobi_form_processor.rendered_output)	serve if response: return response
# todo: remove this </s> if not hasattr(self, '_relative'):	getTimeBefore def getTimeBefore(self): self._relative = None return self._relative
# todo instead of 3*t, use log_sf </s> self.durations[-1] += sample_discrete(dur_distn.pmf(np.arange(dur+1,3*self.t))) + 1	sample_forwards dur = self.durations[-1] dur_distn = self.dur_distns[self.stateseq_norep[-1]]
if util.pythonise(pipe['wires'][wire]['tgt']['moduleid']) == module_id and pipe['wires'][wire]['tgt']['id'] == '_input' and pipe['wires'][wire]['src']['id'].startswith('_output'): # todo? this equates the outputs </s> input_module = steps[util.pythonise(pipe['wires'][wire]['src']['moduleid'])]	build_pipe input_module = steps["forever"] for wire in pipe['wires']: if module_id in pipe['embed']: assert input_module == steps["forever"], "input_module of an embedded module was already set"
1  # todo: fill in identifier </s> )	profile_transfer amount, target, result.wait() profiling.print_all_threads()
1  # todo: fill in identifier </s> )	profile_transfer amount, target, result.wait() profiling.print_all_threads()
# todo: ensure the same callback logic as in set_settled </s> if self._closed_block != 0:	set_closed def set_closed(self, block_number): raise RuntimeError('channel is already closed') self._closed_block = block_number
# todo(tlashchova): remove this method when mistralclient>1.0.0 is used. </s> return trigger.remaining_executions	_resolve_attribute elif name == self.REMAINING_EXECUTIONS:
# todo: how do we know something has changed? </s> self.git_add([self._cfg_file, self._md_file])	CollectionRepo importer.set_graphs(graphs)  # necessary? importer.store_data(self.path) self.git_commit("Initialized config file.") def _get_cfg(self):
# todo: parse the field contents </s> self.index_fields.append(idx)	parse_index if len(field.instructions): idx = parse_func(field.instructions[0][1], log)
# todo fix the fold to allow any number of dispositions </s> base = (	build_sim_sir_w_date_chart if max_y_axis is not None: y_scale.domain = (0, max_y_axis) alt.Chart(sim_sir_w_date_df) .transform_fold(fold=["susceptible", "infected", "recovered"])
# todo error handling. </s> output = subprocess.check_output("git ls-files --unmerged".split())	merge parser = NotebookParser() if length == 0: output_array = [line.split() for line in output.splitlines()] hash_array = []
assert study_id == in_memory_storage_study_id  # todo(akiba) </s> with self._lock:	get_all_trials def get_all_trials(self, study_id): return copy.deepcopy(self.trials)
# todo support domain delegation, which will allow us to set a sub-account to execute as. we can then </s> else:	_authorize key, scope=self.scope) raise AirflowException('Scope undefined, or either key_path/service_account config was missing.') http = httplib2.Http()
# todo: implement this method </s> return self._ref	Handle @property def ref(self): @ref.setter def ref(self, value):
# todo check if for each column, all rows have equal-index series </s> if self.check_input:	fit self : object Returns self. X = check_ts_array(X) self.is_fitted_ = True
# todo(b/138406006): remove the narrower dependency for pyarrow </s> 'pyarrow>=0.14,<0.15',	make_required_install_packages 'tensorflow-model-analysis>=0.14,<0.15', 'tensorflow-transform>=0.14,<0.15',
content=content,  # todo(tsileo): handle markdown </s> tag=tags,	api_new_note cc=cc, to=[to if to else config.AS_PUBLIC], source={'mediaType': 'text/markdown', 'content': source}, inReplyTo=reply.id if reply else None
# todo: we should probably have a special folder just for header </s> import brian2.synapses as synapses	create_extension import numpy c_include_dirs.append(numpy.get_include()) synapses_dir = os.path.dirname(synapses.__file__) c_include_dirs.append(synapses_dir)
# todo: does it get closed properly after process gets killed? </s> self._client.connect(hostname=host, username=user, password=password)	__init__ self._client = SSHClient() self._client.load_system_host_keys() self._cwd = cwd super().__init__(mp_executable, api_stubs_path, cwd=cwd)
# todo: try /usr/bin/curl instead? </s> try:	inspect_sparkle_feed_url except HTTPError as err: if err.code == 403: raw_xml = useragent_urlopen(checked_url, "Mozilla/5.0") facts["user-agent"] = "Mozilla/5.0"
"""todo: document and test""" </s> return sum([hg.degree(edge) for edge in edges])	sum_deep_degree def sum_deep_degree(self, edges):
# todo make this smarter about more complex configurations (backup original values, etc) </s> del obj_doc['dps'][switch_found]['timeout']	config obj_doc['dps'][switch_found]['interfaces'][self.mirror_ports[switch_found]].remove( 'mirror') del obj_doc['dps'][switch_found]['arp_neighbor_timeout'] else:
# todo: determine proper template to use. </s> app_name + "." + recipe_format + ".recipe"] = "template tbd"	main if app_name + "." + recipe_format + ".recipe" not in existing_recipes: buildable_recipes[ print "\nExisting recipes: %s" % existing_recipes print "\nAvailable recipe formats: %s" % avail_recipe_formats
# todo: need a more specific colour here; conflict is wrong </s> rgba0 = self.fill_colors['conflict'] + (1.0,)	delete_chunk mark0 = b0.create_mark(None, it, True) mark1 = b0.create_mark(None, it, True) rgba1 = self.fill_colors['conflict'] + (0.0,) anim = TextviewLineAnimation(mark0, mark1, rgba0, rgba1, 0.5)
# todo(b/123952794): migrate to v2 function. </s> dataset = dataset.map_with_legacy_function(features_dict.decode_example)	features_encode_decode ) dataset = file_adapter.dataset_from_filename(tmp_filename) if not as_tensor:  # Evaluate to numpy array for el in dataset_utils.as_numpy(dataset):
# todo: filter according to what names the current file has in scope. </s> completions = []	get_completion_list def get_completion_list(self, current_file_name): "Get all the completions that apply to the current file." with self.info_lock: for file_name, file_info in self.info.items():
#add to favorites tag --> todo translated label for favorites ? </s> if userdata.get("favorite"):	addOrUpdateTvShowToKodiLibrary self.AddGenresToMedia(showid, genres, "tvshow", cursor) self.AddStudiosToMedia(showid, studios, "tvshow", cursor) self.AddTagToMedia(showid, "Favorite tvshows", "tvshow", cursor) else:
# todo(kan-bayashi): need to make more smart way </s> ys = [y[y != self.ignore_id] for y in ys_pad]  # parse padded ys	calculate_all_attentions def calculate_all_attentions(self, hpad, hlen, ys_pad): :return: numpy array format attentions hlen = list(map(int, hlen)) self.loss = None
# todo: write this in human </s> paths = ['/'.join(['..'] * (len(crumbs) - 1 - i)) for i in	gen_tasks crumbs = gallery_path.split(os.sep)[:-1] crumbs.append(os.path.basename(gallery_name)) range(len(crumbs[:-1]))] + ['#'] print paths
# todo: automate this (by lookup from nn). </s> internal_states_space=impalaagent.default_internal_states_space,	main state_space=env.state_space, action_space=env.action_space, execution_spec=execution_spec, summary_spec=dict(summary_regexp="time-step", directory="/tmp/impala_{}_{}/".format(agent_type, FLAGS.task))
# todo(dcramer): once this goes live in production, we can kill the pickle path </s> pipe.hsetnx(key, 'f', pickle.dumps(filters))	incr pipe = conn.pipeline() pipe.hsetnx(key, 'm', '%s.%s' % (model.__module__, model.__name__)) for column, amount in six.iteritems(columns): pipe.hincrby(key, 'i+' + column, amount)
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
# todo !!! </s> all_proxied_cols = util.set(chain(*[c.proxy_set for c in columns]))	reduce_columns def reduce_columns(columns, *clauses): raise NotImplementedError() columns = util.Set(columns) equivs = {}
# todo: handle base64 data another way? </s> return x == y	compare_base64_strict if len(x) != len(y): return False
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
# todo: better parallel sort test </s> def test_impl():	test_sort_parallel_single_col def test_sort_parallel_single_col(self): df = pq.read_table('kde.parquet').to_pandas() df.sort_values('points', inplace=True)
#todo: remove expressions </s> firstnotenachschlag.duration.quarterlength = self.quarterlength	Trill if nachschlag: firstNoteNachschlag = copy.deepcopy(srcObject) secondNoteNachschlag = copy.deepcopy(srcObject) secondNoteNachschlag.duration.quarterLength = self.quarterLength
# todo: simple_test_builtin should this as status=2. </s> return false	Eval st = posix.stat(s) except OSError as e: mode = st.st_mode if op_id in (Id.BoolUnary_e, Id.BoolUnary_a):  # -a is alias for -e
# todo merge sort on large dataset!!! </s> rows = list(it)	_iternocache flds = it.next() yield flds if key is not None: indices = asindices(flds, key)
# todo: strs->index() has a redundant check for (i < 0) </s> s = left.strs[rhs_int]	Eval rhs_int += n if 0 <= rhs_int and rhs_int < n: else: s = None
# todo(ntonci): add a check for small motion </s> j = i+1	prefilter_using_screw_axis screw_axis_W_E_i = screw_axis_W_E_i / np.linalg.norm(screw_axis_W_E_i) screw_axis_B_H_i = screw_axis_B_H_i / np.linalg.norm(screw_axis_B_H_i) while j < len(dq_W_E_vec): dq_W_E_j = dq_W_E_vec[j]
# todo: this is highly inefficient if more properties are accessed; </s> if self._spec is none:	spec @property def spec(self): if self._stft is not None: self._spec = np.abs(self._stft)
# todo: fix self.cursor_x >= w </s> line = self.output.lines[self.win_y + self.cursor_y]	main_k_end def main_k_end(self, h, w): if len(line) >= w: self.cursor_x = w - 1
# todo: log exception </s> continue	_rewrite_config conf = mod.DEFAULTCONF except Exception as e: Config.add_section(modname) for key in conf:
# xxx todo </s> content = mbi.content	__memory_get_row_values else: Type    = "Unknown" if not content: content = ''
# * todo heading 1 --> </s> vim.evalresults['g:org_todo_keywords'] = ['todo', 'started', 'done',	test_circle_through_todo_states_with_more_states def test_circle_through_todo_states_with_more_states(self): '|'] vim.current.window.cursor = (2, 0)
# todo, i would prefer to query if the language was found... </s> except:	metamodel_for_file_or_default_metamodel try: the_metamodel = metamodel_for_file(filename) if the_metamodel is None:  # no metamodel defined... raise
# todo: remove </s> self.inc_count('network-error-%s' % res['emsg'][:20])	execute_task_handler else: msg = res['emsg'] logger.error(msg) if self.network_try_limit > 0:
# todo: kwargs </s> def _impl(df, axis=none, skipna=none, level=none, ddof=1, numeric_only=none):	var_overload @overload_method(DataFrameType, 'var') def var_overload(df, axis=None, skipna=None, level=None, ddof=1, numeric_only=None): return hpat.hiframes.pd_dataframe_ext.var_dummy(df) return _impl
# todo check the dataset </s> _, response_data = run_on_dataset(	translate args = APP.config["args"] dataset = Dataset("request", data, {}) args.tf_manager, args.runners, dataset, args.postprocess, write_out=False)
# todo pass zk node version to make sure we still own this node </s> self._zookeeper.delete(self._path_from_partition(p))	_remove_partitions for p in partitions: try: except NoNodeException: pass
# todo: replace this with a simpler environment where we can actually test if it finds a solution </s> env = gym.make('pendulum-v0')	test_ddpg def test_ddpg(): np.random.seed(123) env.seed(123)
# todo tests for this </s> return the number of a pre-release.	get_pre_release def get_pre_release(version): :param bytes version: A pre-release version of Flocker. :return int: The number of the pre-release.
# todo: see collections </s> pass	__setitem__ def __setitem__(self, key, value):
# todo: tf2.0 not stable, cannot import tensorflow.contrib.eager.python.saver </s> pass	load_ckpt saver.restore(sess, ckpt_file) else: except Exception as e: logging.info(e)
# todo: add for morph targets data. </s> min_index = min(indices)	extract_primitive_floor process_bone = False bone_max = bone_index max_index = max(indices) for old_index in indices:
# todo(karita) use torch.no_grad here </s> if not torch_is_old:	evaluate state[key] = state[key].detach() data_count += 1 torch.set_grad_enabled(True) model.predictor.train()
# todo: bytes vs str </s> request_line = request_line.decode()	_handle def _handle(self, reader, writer): request_line = yield from reader.readline() method, path, proto = request_line.split() headers = {}
# todo: remove this older form of error handling. </s> if not node:	OshMain return 2 else: err = c_parser.Error() ui.PrintErrorStack(err, arena, sys.stderr)
# todo remove </s> supers = []	py__bases__ args = param.Arguments(self._evaluator, self.base.get_super_arglist() or ()) return list(chain.from_iterable(args.eval_args())) for s in self.base.supers: for cls in self._evaluator.eval_statement(s):
# todo: explicitly commit files by name </s> youngest_ancestor = os.path.commonprefix(files)	add raise IOError("[BZR] add in '%s' failed: %s" \ % (self.location_abs, error)) return output + type(self)(youngest_ancestor).commit(message, author)
# todo(hartikainen): once tfp.bijectors.chain supports conditioning, </s> return x	_forward for bijector in self.flow: x = bijector.forward(x, **conditions.get(bijector.name, {}))
# todo: disconnect </s> return	Node recent_beacon_blocks_response, ) self.logger.debug( "Processing recent beacon blocks request from %s is finished",
# todo: check whether the name is already used or not </s> material = bpy.data.materials.new(name)	add_material https://docs.blender.org/api/current/bpy.types.BlendDataMaterials.html https://docs.blender.org/api/current/bpy.types.Material.html material.use_nodes = use_nodes if use_nodes and make_node_tree_empty:
# todo(tr3buchet): fix function call after refactor </s> self._make_plugin_call('xenstore.py', 'write_record', instance,	inject_network_info self.write_to_param_xenstore(vm_ref, {location: info}) try: location, {'value': json.dumps(info)}, vm_ref)
# todo: make sure requesting user is owner of the build_id </s> current_image = request.form.get('current_image')	create_run release = Release.query.filter_by(id=release_id).first() assert release, 'release_id does not exist' current_log = request.form.get('current_log') current_config = request.form.get('current_config')
# todo: if build and the following command fails "podman inspect -t image <image_name>" then run build </s> if compose.global_args.no_cleanup == false:	compose_up def compose_up(compose, args): shared_vols = compose.shared_vols compose.commands['down'](compose, args) create_pods(compose, args)
# todo sp: updatable attributes ? </s> raise modificationnotallowed("cannot delete an attribute after "	_del_attr "DbAttribute {} does not exist".format(key)) else:
# todo imageio single frame seek seems slow. look into this </s> return image	load_video_frame self.vid_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_no)  # pylint: disable=no-member _, image = self.vid_reader.read()
# todo: move to config check </s> if not model.csv_import_username:	system def system(request=None): model = SystemImporterFileCsvCronbasedConfigModel.objects.get(system_importer_file_csv_cronbased_config_name = 'SystemImporterFileCsvCronbasedConfig') mainconfigmodel = MainConfigModel.objects.get(main_config_name = 'MainConfig') cron_username = mainconfigmodel.cron_username
# todo: boto3 call can fail with botocore.exceptions.clienterror, </s> response = client.change_resource_record_sets(	update_dns_zone_record dns_json = get_template(template_file='infrastructure/dns_upsert.json.j2', **kwargs) HostedZoneId=zone_id, ChangeBatch=json.loads(dns_json), )
# todo: move this to either settings.py or the sql configuration. </s> entries_per_page = 15	search_results def search_results(request): Shows an archive of news entries. search_form = SearchForm(request.GET) valid_search = search_form.is_valid()
#todo eval hook </s> return f	__genqa_reader __reader(f) genqa_readers.setdefault(f.__name__, f)
data_source_name='ledger-v2',  # todo: this isn't really needed. </s> domain=ledger_value.domain,	change_meta_from_ledger_v2 document_id=ledger_value.ledger_reference.as_id(), data_source_type=data_sources.LEDGER_V2, is_deletion=False,
# todo only do these thing if status is true </s> elif endpoint.state == 'unknown':	process status = Actions( endpoint, self.s.sdnc).mirror_endpoint() if self.s.investigations < self.controller['max_concurrent_reinvestigations']: self.s.investigations += 1
# todo -- parallelize this </s> for current_file in getattr(protocol, subset)():	objective metric = self.get_tune_metric() value, duration = [], [] reference = current_file['annotation'] uem = get_annotated(current_file)
# todo: remove this. </s> data_iterator_val = self._pair_iterator_v2(labeled_nodes_val, data)	TrainerAgreement logging.info('Skipping validation. No validation samples available.') break feed_dict_val = self._construct_feed_dict( data_iterator_val, is_train=False)
# todo: why the reverse order? </s> rotatef(rotz, 0, 0, 1)	transform def transform(x, y, z, rotx, roty, rotz, sx, sy, sz, cx, cy, cz): translatef(x - cx, y - cy, z - cz) rotatef(roty, 0, 1, 0) rotatef(rotx, 1, 0, 0)
# todo: migrate new article ids to articlecontents </s> self.alter_foreignkey_to_int('recipes_oldrecipearticleredirect', 'new_id')	alter_self_foreignkeys alter and migrate all tables that has foreign keys to this model self.alter_foreignkey_to_int('articles_articlecontents', 'article')
# todo: this size isn't calculating correctly currently, but it should be included in the json </s> json["version"] = self.version	ToJson json = {} json["hash"] = self.Hash.To0xString() json["previousblockhash"] = self.PrevHash.To0xString() json["merkleroot"] = self.MerkleRoot.To0xString()
# todo(b/131315065): remove the comment above when the csv decoder no longer </s> return (	BatchExamplesToArrowTables Returns: A PCollection of Arrow tables. examples | "BatchBeamExamples" >>
# todo: retry instead? </s> pass	handle_events_job_logs xp_logger.handlers = [] except OSError:
# todo: add support of tuple (row_offset, col_offset) </s> original_size = rows - offset, cols - offset	ms_image_deaugment for image, offset in zip(images, size_offsets): batch_size, channels, rows, cols = image.size() scaled_image = torch.nn.functional.interpolate( image, size=original_size, mode=mode, align_corners=align_corners
# todo: model only from our models </s> available_models = {**models.__dict__}	test_main ) print("Creating model") model = available_models[args.model](pretraining=args.pretrained) model.to(device)
# todo: axis should support 1 or 'columns' either at this moment </s> return dataframe(internal)	shift internal = self._internal.copy(sdf=sdf, data_columns=[c.name for c in applied])
# todo optimize </s> size check	CalcFiniteHorizonOptimalInput u""" Calc Finite Horizon Optimal Input search kron print("CalcFiniteHorizonOptimalInput start")
eprint('❗ ' + x) # todo yellow? </s> def warning(x: str):	warning
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
data = json.load(open(input_fn))  # todo do we support multiple arguments here? </s> y = numpy.array(data['y'])	line_plot def line_plot(input_fn, display=False, outfile='line_plot.png', max_n=20): if y.shape[0] > max_n: js = sorted(range(len(data['labels'])), key=lambda j: max(y[j]), reverse=True)
# todo: serialize the policy </s> import pudb; pudb.set_trace()	grant new_policy = drone_alice.grant(bob, label, m=m, n=n, expiration=expiration_time) return Response(bytes(new_policy.treasure_map), status=200)
# todo save the error to the plugin </s> plugin_db_setting.save(no_reload=true)	_init_plugins if (plugin.__name__ == disabled) or (plugin.__module__ == disabled): plugin_db_setting.active = False self.plugins_inactive[plug_key] = plugin_db_setting continue  # continue -> the plugin is not loaded
#todo: check if/where this is used; if not used externally - remove </s> return self.marker_detector.robust_detection	Surface_Tracker @property def robust_detection(self) -> bool: @robust_detection.setter def robust_detection(self, value: bool):
# todo - find a workaround for this </s> import time	test_process_path def test_process_path(self): proc = subprocess.Popen(sys.executable) time.sleep(0.5) try:
# todo find out if this is good because of sparcity... </s> 'prefers_data_normalized': false,	get_properties 'handles_numerical_features': True, 'prefers_data_scaled': False, 'is_deterministic': True, 'handles_sparse': False,
#todo: check the data! </s> count = 0	test_filtered_multiple_sources pipe_def = self._get_pipe_def("pipe_c1cfa58f96243cea6ff50a12fc50c984.json") p = pipe2py.compile.parse_and_build_pipe(pipe_def, verbose=True) for i in p: count += 1
# todo: remove the append lock once append like ops are thread safe </s> with self._append_lock:	task f() finally: self.finished.append(tid) while not self._can_exit:
# todo: skipped due to gh-4436 </s> yield skip_if_on_windows(skip_ssh(_test_create_store)), 'datalad-test'	test_create_simple def test_create_simple(): yield _test_create_store, None
# todo: non-numeric columns should be ignored automatically </s> def test_impl(n):	test_min1 def test_min1(self): df = pd.DataFrame({'A': np.arange(n)+1.0, 'B': np.arange(n)+1}) return df.min()
# todo error on missing levels </s> pass	parse_style dashes = dict(zip(style_levels, self.default_dashes)) elif dashes and isinstance(dashes, dict): elif dashes: dashes = dict(zip(style_levels, dashes))
## todo: # fixme: remove me </s> try:	download_domain faup.decode(domain) unpack_url = faup.get() domain = unpack_url['domain'].decode() except:
# todo(user): remove after 184 is out. </s> finalized_filenames = cls.get_filenames(mapreduce_state)	finalize_job state = cls._State.from_json(mapreduce_state.writer_state) files.finalize(state.filenames[0]) state = cls._State(finalized_filenames, []) mapreduce_state.writer_state = state.to_json()
# todo: fix this </s> return {	to_json_content def to_json_content(self): 'encoder': self._encoder.to_json(), 'encoder_config': self._encoder_config.to_json(),
# todo: reactivate after fixing alternative selection </s> self.assertequal(form.fields['serviceprovider'].label, 'serviceprovider')	test_system_importer_file_csv_form_based_serviceprovider_form_label form = SystemImporterFileCsvFormbasedForm()
#todo generate the labels for the dict automatically from labels </s> result = {'time': time_array, 'lc_3to6kev': y1,	parse_obssumm_file dim = np.array(y1).shape[0] time_array = [reference_time_ut + timedelta(0,time_interval_sec*a) for a in np.arange(dim)] 'lc_12to25keV': y2, 'lc_25to50keV': y3, 'lc_50to100keV': y4, 'lc_100to300keV': y5,
# todo: must be implemented </s> pass	range_from_chapters def range_from_chapters(crawler, times=0):
# todo manage tangent? </s> translation_keyframe = conversion.loc_gltf_to_blender(values[idx * 3 + 1])	parse_translation_channel for idx, key in enumerate(keys): if animation.samplers[channel.sampler].interpolation == "CUBICSPLINE": else: translation_keyframe = Conversion.loc_gltf_to_blender(values[idx])
# todo: find 1 or 2 utxo's with exact amount +/- self.network.dust_amount </s> one_utxo = utxo_query.filter(dbtransactionoutput.spent.op("is")(false),	select_inputs if not utxos: raise WalletError("Create transaction: No unspent transaction outputs found") DbTransactionOutput.value >= amount, DbTransactionOutput.value <= amount + variance).first()
if norm_groups[i] == -1:  # todo: early break </s> skip = true	step self._compute_grad_norm(grads_groups_flat[i]) ) if skip: self._update_scale(skip)
# todo generator </s> results.extend(cur_ds.repo.get(	_get lgr.info("Getting %i items of dataset %s ...", len(content), cur_ds) content, options=['--from=%s' % source] if source else [],
# todo: this is horrible </s> if len(setting) == 0 or setting[0] == '#':	load_floorc fd.close() for setting in default_settings: continue try:
# todo(b/155997704) clean this up once tfx_bsl makes a release. </s> if getattr(csv_decoder, "parse_csv_line_yields_raw_records", false):	convert_csv_to_tf_examples | "ParseCSVLine" >> beam.ParDo(csv_decoder.ParseCSVLine(delimiter=","))) parsed_csv_lines |= "ExtractParsedCSVLines" >> beam.Keys() column_infos = beam.pvalue.AsSingleton(
# todo: this doesn't seem necessary; test passes without it </s> time.sleep(1)	test_watch_file watcher = Watcher() watcher.count = 0 filepath = os.path.join(tmpdir, 'foo') with open(filepath, 'w') as f:
# todo what is the performance of this like? </s> extended = cls.get_parser().parse(file_name)	get_episodes continue file_name = os.path.splitext(os.path.basename(parts[0].get('file')))[0] identifier = cls.get_chain_identifier(extended.chains[0].info) if extended.chains else None season, episodes = cls.get_episode_identifier(video, identifier)
# todo add test for this </s> if order_samples[i] >= 3:	PiecewiseAffine ) arr_0to1_warped = arr_0to1_warped.astype(np.float32) arr_0to1_warped = np.clip(arr_0to1_warped, 0.0, 1.0, out=arr_0to1_warped) heatmaps_i.arr_0to1 = arr_0to1_warped
# todo: support multiple. </s> return res['mutate_response'][0]['id']	delete_podcast_series res = self._make_call(mutate_call, update_mutations)
# todo: don't assume that content is octetstring </s> return signers, signed['encap_content_info']['content']	_verify_cms_signers verifier.verify()  # Raises a SigningError if not valid signers.append(certificate)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: replace this with future on-the-fly-api-components. </s> return weights	get_weights weights = root._graph_fn_merge(weights, value_function_variables)
# todo: should be threaded </s> sni = self._test_sni()	test_supported_versions def test_supported_versions(self): non_sni = self._test_non_sni() self.supported_versions["SNI"].update(sni)
#exclude watched items --> currently hardcoded --> todo: add a setting for this ? </s> rule2 = subelement(root, "rule", {"field":"playcount","operator":"is"})	addVideoNodesForTag SubElement(root, "order", {"direction":"descending"}).text = "dateadded" SubElement(root, "limit").text = "25" SubElement(Rule2, "value").text = "0" try:
# todo: calculate mu-sigma for f1, accuracy, and roc_auc and make it selectable </s> cv_results['judgment_metric'] = np.mean(cv_results['mu_sigmas'])	atm_cross_val_small_multiclass rank_accuracies=rank_accuracies, mu_sigmas=mu_sigmas) cv_results['judgment_metric_std'] = np.std(cv_results['mu_sigmas']) return cv_results
# todo: should this be a glomerror of some form? probably </s> raise typeerror('failed to iterate on instance of type %r at %r (got %r)'	glomit iterator = iterate(target) except Exception as e: % (target.__class__.__name__, Path(*scope[Path]), e)) return self._fold(iterator)
# truffle todo(ls): revert this loop to "yield from" </s> for __x in self.__context__.format(chain=chain): yield __x	format elif (self.__context__ is not None and not self.__suppress_context__): yield _context_message if self.exc_traceback is not None:
assert study_id == 0  # todo(akiba) </s> trial_id = len(self.trials)	create_new_trial_id def create_new_trial_id(self, study_id): self.trials.append(trial.Trial(trial_id)) return trial_id
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
raise notimplementederror # todo </s> list every item in entry_point that match request	search def search(self, entry_point, request):
# todo: import that elsewhere </s> from . import _control	wrapper def wrapper(*args, **kwargs): _control.execQueue.socket.pumpInfoSocket() for key, value in kwargs.items():
# todo: check how those names are constructed and may be at least count the number of created object files in addition to that comparison </s> eq_(set([f for f in all_files_updated if not f.startswith('./.datalad/metadata/objects/')]), target_files)	test_openfmri_pipeline1 assert_not_equal(out[0]['datalad_stats'].get_total(), ActivityStats()) target_files.remove('./sub-1/beh/responses.tsv') commits_ = {b: list(repo.get_branch_commits(b)) for b in branches} commits_hexsha_ = {b: list(repo.get_branch_commits(b, value='hexsha')) for b in branches}
# todo: test this block </s> self.title = self.title or self.content_object.page_title	update_from_related_object self.heading = self.heading or self.content_object.meta_title elif hasattr(self.content_object, 'page_title'): self.heading = self.heading or self.content_object.page_title elif hasattr(self.content_object, 'title'):
# todo: implement logic for computing the host alias </s> playbook_host = self.client.post(	_get_or_create_host if host == playbook_host['name']: return playbook_host '/api/v1/hosts', name=host,
# todo: handle other resolve strategies </s> pass	read user_containers[0].deserialize(archive.open(instance_container_file).read().decode("utf-8")) else: user_instance_containers.append(instance_container) elif container_type == "quality_changes":
# todo: configurable timeout??? </s> syndic_dict['dead_until'] = time.time() + 60	_fire_master except SaltClientError: log.error('Unable to fire event to {0}, trying another...'.format(master)) log.critical('Unable to fire event on ANY master')
# todo(piyush): current api-site doesn't contain this api description. </s> uri = '/agents/%s/l3-routers' % agent_id	add_router_to_l3_agent def add_router_to_l3_agent(self, agent_id, **kwargs): return self.create_resource(uri, kwargs)
# todo(kevinbenton): remove after bug/1666493 is resolved </s> sub = pool_mock.get_instance.return_value.get_subnet.return_value	test_create_ipv6_pd_subnet_over_ipam def test_create_ipv6_pd_subnet_over_ipam(self, pool_mock): mocks = self._prepare_mocks_with_pool_mock(pool_mock) sub.subnet_manager.neutron_id = mock.ANY cfg.CONF.set_override('ipv6_pd_enabled', True)
# todo(patrick, suraj, anton) - it's surprising that "non-padded/non-numpified" padding </s> for max_length, padding, var_tol in zip(max_lengths, paddings, var_tolerances):	test_cepstral_mean_and_variance_normalization max_lengths = [None, 16, None] var_tolerances = [1e-3, 1e-3, 5e-1] inputs = feature_extractor( speech_inputs, padding=padding, max_length=max_length, return_attention_mask=True
# todo: remove these deprecated properties we are off es 1 </s> "merge.policy.merge_factor": 10,	get_reindex_es_settings "refresh_interval": "5s", "max_result_window": SIZE_LIMIT, "store.throttle.max_bytes_per_sec": "5mb", "store.throttle.type": "node",
n = 100 # todo: iir: more intelligent algorithm needed </s> else:	impz if N == 0: # set number of data points automatically if IIR: N = min(len(b),  100) # FIR: N = number of coefficients (max. 100) impulse = np.zeros(N)
#todo: get darknet class number from class file </s> num_classes_coco = 80	custom_yolo_body pre-trained weights from darknet and fit for our target classes.''' base_model = yolo_body(inputs, num_anchors, num_classes_coco) base_model.load_weights(weights_path, by_name=True)
# todo(ihrachys): replace with network.create() once we get an object </s> network = db_api.create_object(self.context, models_v2.network,	test_attach_network_get_network_policy obj = policy.QosPolicy(self.context, **self.db_obj) obj.create() {'name': 'test-network1'}) policy_obj = policy.QosPolicy.get_network_policy(self.context,
# todo: aio core is separate from transport </s> if self.config['transport'].lower() in ('zeromq', 'tcp'):	Master self.setup_logfile_logger() logger.info('Setting up the Salt Master') if not verify_socket(self.config['interface'], self.config['publish_port'],
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
st = manager.get_stat(path)  # todo: errors </s> if not stat.s_isreg(st.st_mode):	validate_meta return None path = os.path.abspath(path) manager.log('Metadata abandoned for {}: file {} does not exist'.format(id, path)) return None
# todo: maybe this should be under a lock? </s> if len(cls.__strong_cache) > cls.__strong_cache_size:	_TzOffsetFactory cls.instance(name, offset)) cls.__strong_cache[key] = cls.__strong_cache.pop(key, instance) cls.__strong_cache.popitem(last=False) return instance
logfile = open('logs/exceptions.log', 'a') #todo: make not hardcoded </s> logfile.write('from %s at %s:\n' % (origin.sender, str(datetime.now())))	error print trace except: logfile.write('Message was: <%s> %s\n' % (trigger.nick, trigger.group(0))) logfile.write(trace)
# todo the actual brodcast </s> return web.response()	ValidatorAPIHandler humanize_hash(attestation.hash_tree_root), )
# todo: remove when transition to python3 complete </s> return other / self	__rtruediv__ "FRD.__rtruediv__ is currently implemented only for SISO systems.")
# todo(laigd): remove this check when 312743821 is in the release. </s> if use_tf_function and tf.compat.v1.__version__ < '2.3.0':	testPreserveStaticShape ) def testPreserveStaticShape(self, use_tf_function): return FLAGS.call_defun_use_tf_function = use_tf_function
# todo: deprecation warning </s> return self.model._default_manager.all()	get_queryset return self.queryset._clone() if self.model is not None: raise ImproperlyConfigured("'%s' must define 'queryset' or 'model'" % self.__class__.__name__)
# todo: systemhistory_user_id </s> )	save systemhistory_old_value = self.previous_systemstatus.systemstatus_name, systemhistory_new_value = self.systemstatus.systemstatus_name, systemhistory.save() self.previous_systemstatus = self.systemstatus
# todo: figure out a way to set __name__ for partial object, update_wrapper </s> group_template = [(lambda a: a) if type(f) != partial and f.__name__ == 'dflt_f' else (lambda a: none)	_aggregate columns in project are either aggregation or group by columns vals = {} for f in ffuncs] for record, _ in inp:
# todo: test 2a: planilha total mais atualizada que deployed (total + deployed) </s> if date in cases:	get_state_data_from_db for spreadsheet in spreadsheets: date = spreadsheet.date continue report_data = reports.get(date, defaultdict(list))
# todo: end remove hosts/when block </s> for _, kwarg_configs in operation_kwargs.items():	pop_global_op_kwargs 'when': get_kwarg('when', True), } for key, config in kwarg_configs.items(): handler = None
self.ops_config = none  # todo </s> self.opscfg_revmap = none	Context self.amqs = {} self.fpti_client = None  # set from contrib ? self._serve_ufork = None self._serve_daemon = None
# todo: action value doesn't exist for beta </s> self.unittest(	test_early_horizon_estimate baseline_objective = 'policy_gradient' baseline_optimizer = 'adam' exclude_bounded_action=True, reward_estimation=reward_estimation, baseline_objective=baseline_objective, baseline_optimizer=baseline_optimizer
# todo: will be replaced with exception in future releases </s> logger.error('telegram does not support reply keyboard row width over %d.' % self.max_row_keys)	ReplyKeyboardMarkup row_width = self.row_width if row_width > self.max_row_keys: row_width = self.max_row_keys for row in util.chunks(args, row_width):
# todo: differentiate between tags assigned to the instance and a m2m field for tags (ex: configcontext) </s> if key == 'tags':	assertInstanceEqual model_dict = model_to_dict(instance, fields=data.keys()) for key in list(model_dict.keys()): model_dict[key] = ','.join(sorted([tag.name for tag in model_dict['tags']])) elif model_dict[key] and type(model_dict[key]) in (list, tuple) and hasattr(model_dict[key][0], 'pk'):
false)  # todo: check if this should be secure </s> feed = self.feed_type(title=self.title,	get_feed feed_url = add_domain(current_site.domain, reverse(feed_url_name) if feed_url_name else '', link=link, description='',
# @todo implement threading here. </s> ticket_data = self.call_api("/projects/" + str(task[u'project_id']) + "/tickets/" + str(task[u'ticket_id']))	find_issues assigned_task = dict() if task[u'type'] == 'Ticket': assignees = ticket_data[u'assignees'] for k, v in enumerate(assignees):
# todo: allow partial prase complete </s> if shouldeval and prase_input_complete(data):	_ data = data.replace('\r', '\n') shouldeval = data[-1] == "\n" and len(event.current_buffer.document.text_after_cursor) == 0 data = data.rstrip("\n") event.current_buffer.insert_text(data)
# todo: old requirement, remove in future versions? </s> self.signout()	TestPrivacyWebPublic res = self.app.get(url, follow_redirects=True) dom = BeautifulSoup(res.data) @with_context def test_02_account_index(self):
## todo : add layers </s> return g_1	generator g_1 = tf.reshape(g_1, [-1,4,4,4,512]) g_1 = batchNorm(g_1, 512, phase_train)
# todo see #682 detailedlist should have a _selection attribute + selection property like tree </s> self.interface.on_select(self.interface, list_box_row=row.toga_row)	_on_row_selected def _on_row_selected(self, list_box, list_box_row): if self.interface.on_select and list_box_row is not None:
# todo only do these thing if status is true </s> endpoint.unknown()	process status = Actions( endpoint, self.s.sdnc).unmirror_endpoint() self.s.investigations -= 1 endpoint.p_prev_states.append(
# todo verify results </s> self.assertsimpleerror("get request must contain a 'query' parameter", 400)	test_get_noquery b"/", )
# todo: parse automatically the 'swap' method </s> def _swap(args): a = list(args[0]); b = int(args[1]); c = a[0]; a[0] = a[b % len(a)]; a[b] = c; return "".join(a)	_interpreter def _interpreter(self, code): virtual_memory = {"a": "", "return": ""} methods = {
# todo make proper json-ld definition </s> 'parentds': 'path of the datasets that contains an entity',	_get_search_schema '@id': 'unique identifier of an entity', 'path': 'path name of an entity relative to the searched base dataset', 'type': { '@id': common_defs['type']['def'],
# todo: make sure this openssl command works everywhere, maybe we should use a text_base64_decode? </s> run('echo "%s" | openssl base64 -a -d >> %s' % (base64.b64encode(content), shell_safe(location)))	file_append def file_append(location, content, mode=None, owner=None, group=None): location, optionally updating its mode/owner/group.""" file_attribs(location, mode, owner, group)
# todo this pipeline may drop nulls in prediction rows if impute=false </s> pipeline = pipelines.full_pipeline(model_type, predicted_column, grain_column, impute=impute)	__init__ self.grain_column = grain_column, self.grain_column = grain_column, clean_dataframe = pipeline.fit_transform(dataframe) self._advanced_trainer = AdvancedSupervisedModelTrainer(clean_dataframe, model_type, predicted_column,
# todo: split name and email address </s> to_add = [to.strip() for to in self.__email['to'].split(',')]	generate_attributes self.add_attribute('message-id', value=self.__email['Message-ID']) if 'To' in self.__email: self.add_attributes('to', *to_add) if 'Cc' in self.__email:
self.assertequal(self.todolist.count(), 3)  # force won't delete subtasks </s> self.assertequal(self.output, "|  2| bar p:1\nremoved: foo id:1\n")	test_del3 command.execute() self.assertTrue(self.todolist.is_dirty()) self.assertEqual(self.errors, "")
# todo: this test requires manifold access, see: t88318502 </s> self._test_model("coco-instancesegmentation/mask_rcnn_r_50_fpn_3x.yaml", device="cuda")	testMaskRCNNGPU @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available") def testMaskRCNNGPU(self):
# todo: remove when postgres migration is done </s> config.add_request_method(	includeme name='legacy_es', reify=True) lambda r: _get_client(r.registry.settings), name='new_es',
# todo: @sbharadwajj implement and test </s> return true	hessian_is_zero def hessian_is_zero(self):
# todo: find out why pypy 1.8 with close_fds=true triggers </s> close_fds = platform.python_implementation() != 'pypy'	_spawn return [pid] os.close(elog_reader_fd) portage.process._setup_pipes(fd_pipes, close_fds=close_fds) signal.signal(signal.SIGINT, signal.SIG_DFL)
# todo: check error location </s> assert result.errors[0].message == sync_error.message	test_nulls_a_complex_tree_of_nullable_fields_that_throw result = execute(schema, ThrowingData(), ast, 'Q', {}) assert len(result.errors) == 2 assert result.errors[1].message == sync_error.message assert result.data == {
# todo keep playback.state unchanged </s> self.backend.playback.next().get()	Next def Next(self): logger.debug(u'%s.Next called', PLAYER_IFACE)
# todo: we lose the response code, so we can't check this </s> self.failunless("new.txt" in self._foo_node.children)	test_PUT_NEWFILEURL d = self.PUT("/vdrive/global/foo/new.txt", self.NEWFILE_CONTENTS) def _check(res): new_uri = self._foo_node.children["new.txt"] new_contents = self.files[new_uri]
# todo(dcramer): once we migrate to job plans we can remove this </s> current_app.logger.warning(	create_build try: if not job_plan: 'Got create_build task without job plan: %s', build_id) backend = JenkinsBuilder(
#@todo: remove in 0.4.10 </s> def error(self, reason=none, type="parse"):	error raise Fail("%s error%s | Plugin out of date" % (type.capitalize(), ':' + str(reason) if reason else "")) if self.core.debug:
