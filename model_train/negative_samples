# todo we need to add more tests </s> if layer_purpose == layer_type:	_add_id_column } has_id_column = False safe_id = field if layer.keywords.get(field['field_name']):
# todo use literal_eval instead of a simple eval. </s> self.nonterminal2number = {}	Grammar def __init__(self, bnf_grammar, start_nonterminal, rule_to_dfas, token_namespace): self._token_namespace = token_namespace self.number2nonterminal = {} self.states = []
#todo: add way to check if alt is pressed </s> self.debugstring += string	addDebugString def addDebugString(self, string):
# todo(ohta): convert `study` and `trial` to single objective versions before passing. </s> param_distribution: basedistribution,	RandomSampler study: "mo.study.MoStudy", trial: "mo.trial.FrozenMoTrial", ) -> Any: return self._sampler.sample_independent(study, trial, param_name, param_distribution)
# todo(mitmul): current cupy.random.choice doesn't support replace=false </s> anchors = cuda.to_cpu(anchors)	_calc_overlaps def _calc_overlaps(self, anchors, gt_boxes, inds_inside): gt_boxes = cuda.to_cpu(gt_boxes) overlaps = bbox_overlaps(
# todo: make the backend lazy and compute things when </s> polyhedra over `\zz` with normaliz.	Polyhedron_ZZ_normaliz class Polyhedron_ZZ_normaliz(Polyhedron_normaliz, Polyhedron_ZZ): INPUT: - ``Vrep`` -- a list ``[vertices, rays, lines]`` or ``None``
# todo: fix this! </s> self.assertraises(nap.napmissinginputerror, self.nap.add_schema, { 'name': 'crapson' })	test_schema_add_crap_input self.assertRaises(nap.NapMissingInputError, self.nap.add_schema, { })
# todo: add support for custom likelihoods </s> example:	model_list_to_batched type and have the shape (batch shape and number of training inputs). Returns: >>> list_gp = ModelListGP(gp1, gp2) >>> batch_gp = model_list_to_batched(list_gp)
data['type'] = 'query-%s' % editor_type  # todo: add handling for non-sql types </s> logs = db.get_log(notebook, snippet, startfrom=startfrom, size=size)	get_logs size = request.POST.get('size') size = int(size) if size else None jobs = json.loads(request.POST.get('jobs', '[]')) new_jobs = db.get_jobs(notebook, snippet, logs)
# todo: automate detection of max string length to set up numpy array accordingly </s> return self.app.activeworkbook.worksheets(s).activate()	set_sheet
# todo: remove this! compat w/<1 </s> returns a list of available operations.	get_operation_names def get_operation_names():
# todo: review this part one more time </s> parameters	extract_resnet_v1_101_mapping_without_logits@34 tasks. Last layer usually has different size, depending on the number of classes to be predicted. This is why we omit it from the dict and those variables will ---------- resnet_v1_101_variables_mapping : dict {string: variable}
# todo: handle case where the creation is rejected for some reason (should </s> results = self.client.list_server_logs(fqdn)	list_server_logs if not results: return Response(status=status.HTTP_503_SERVICE_UNAVAILABLE)
# todo: add also jsp backdoor/uploader support </s> if not cmd:	__webBackdoorRunCmd def __webBackdoorRunCmd(self, backdoorUrl, cmd): cmd = conf.osCmd cmdUrl  = "%s?cmd=%s" % (backdoorUrl, cmd)
# todo: move lifetime to syncpins </s> assert(newgraph is not none)	rawGraph def rawGraph(self, newGraph):
# todo: modifiers </s> def _pyvis_update(self):	_pyvis_update
pass # todo(denero) implement </s> self.assignment_name = 'assignment'	test_invalid_assignment_name
# todo addding a assertrvline to test reversed selections would make the calls to assertselection() below in this test # noqa: e501 </s> self.eq('{ab|}', 'v_%', '|{ab}|')	test_v_percent def test_v_percent(self): self.eq('a |{\nb\n}\nc', 'v_%', 'a |{\nb\n}|\nc') self.eq('a {\nb\n|}\nc', 'v_%', 'a |{\nb\n}|\nc')
# todo: this needs refactoring </s> for genus in genus_list:	in_genus_list if species.startswith(genus.capitalize()): return True
# todo project_id = 'your google cloud project id' </s> response = client.export_assets(parent, output_config)	export_assets@26 output_config.gcs_destination.uri = dump_file_path
# todo - this should be moved to the `finalize` method of the base resource, as it's not cross-service </s> self.parse_elb_policies_callback,	_parse_elb_policies self._go_to_and_do(self.services['elb'], ['regions'], {})
# todo: handle escape (0x1b) </s> supplied_crc = message[-2] * 256 + message[-1]	valid_crc calculated_crc = crc16.crc16xmodem(''.join([chr(item) for item in message[:-2]])) return supplied_crc == calculated_crc
# todo never test </s> output = torch.cat(heads, dim=2)	MultiHeadAtte heads.append(headi)
# todo: display correlation error better in graph! </s> ratio_progress_of_df_summary_vs_feature = 1.0	DataframeReport for skipped in fc.skip: if skipped not in all_source_names and skipped not in all_compare_names: number_features = len(filtered_series_names_in_source) exponential_checks = number_features * number_features
# todo: use parameterized tests to test all losses. </s> 'linear': 1. / rank,	_rank_discount def _rank_discount(rank_discount_form, rank): 'LOG': 1. / math.log(1. + rank), }
# todo: contains a self argument, should probably be a class method. </s> return from_polar_rad(math.radians(direction), magnitude)	from_polar
# todo: replace with copy and copy_file </s> origin.fetch()	create_remote origin = repo.create_remote('origin', remote_url)
# todo(brian): s/_obj/obj once other changes propogate </s> :class:`~openstack.object_store.v1.container.container`	get_account_metadata :rtype:
# todo: memoize? </s> pass	FloatColumn
# todo: should be 2.14 when released </s> except attributeerror:	safe_apply If 'object' has no method 'method' this is a no-op. try: pass else:
# todo: support other offsets types (time delta, etc.) </s> def roll_fixed_apply_seq(in_arr, win, center, kernel_func):  # pragma: no cover	roll_fixed_apply_seq N = len(in_arr) output = np.empty(N, dtype=np.float64)
# todo: arrange </s> self.remote.modify_repo(repo, "mirror_locally", "0", self.token)	createRepo repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "name", "testrepo0", self.token) self.remote.save_repo(repo, self.token)
# todo: documentation pending </s> elif is_train is not none and self.is_train is not none:	_check_mode def _check_mode(self, is_train): if is_train is None and self.is_train is None: if is_train == self.is_train: logging.warning("Training / inference mode redefined redundantly. Please EITHER use the argument `is_train` OR `Model.train()` / `Model.eval()` to define the mode.")
# todo - make this work on loop with more than two links </s> def make_fill_posts(bm, edges, **kwargs):	make_fill_posts
# todo(toshihikoyanase): remove catch_warnings after gridsampler becomes non-experimental. </s> if t.user_attrs.get("lightgbm_tuner:step_name") == self._step_name	_StepwiseStudy return [ t ] @property
#h = heading.parse_heading_from_data(text, self.allowed_todo_states) </s> return unittest.testloader().loadtestsfromtestcase(	suite TestHeadingRecognizeDatesInHeading)
# todo extend to nonbinary nodes </s> return self.index < other.index	__lt__
# todo: docstring </s> else:	on_disconnect@947 self._handle_disconnect()
#@todo: this chould be a new command api method. that gets automatically </s> @see: self._reuse_attr_of_parentcommand()	AtomsTool_Command such that it fits the new method names in command API. self._reuse_attr_of_parentCommand('flyoutToolbar') self._reuse_attr_of_parentCommand('propMgr') def _reuse_attr_of_parentCommand(self, attr_name = ''):
# todo: check error location </s> 'test': graphqlfield(test_type),	get_fields def get_fields(self): 'nest': GraphQLField(DataType(), resolver=lambda *_: Data())
# todo: split name and email address </s> to_return = []	attachments @property for attachment in self.__email.iter_attachments(): content = attachment.get_content()
# todo: enhancing highlighting.get_suspected_range_after_change </s> expected = []	test_no_highlighting from rope.highlight import NoHighlighting noHigh = NoHighlighting() for result in noHigh.highlights(text, None, None): self.assertEquals(expected[0], result)
#todo migrate to remove this hack </s> def auth_registerbeta():	auth_registerbeta
svg = apply_template(svg, {"maxpoints":maxpoints, "trdn": trdn, "msg":"", "valuemid":"0.5", "timemid":"10s", "datapoints":"","init_max_y": "false", "max_y": 0, "seconds_scale":0, "y_shift": 0, "zero": 0, "l_y":"", "nodata":"&#xf071;", "avg_upd": 0, "late": "no data stream"}) # todo templating engine </s> if request.headers.getlist("x-forwarded-for"):	feeder if not data: return plot(hashstr) ip = request.headers.getlist("X-Forwarded-For")[0] else:
# todo [cas-27]: remove access token from service validation </s> def __init__(self, message):	CasTokenError class CasTokenError(CasError):
# todo support intloguniformdistribution </s> "the parameter '{}' in trial#{} is sampled independently "	_log_independent_sampling def _log_independent_sampling(self, trial: FrozenTrial, param_name: str) -> None: "by using `{}` instead of `CmaEsSampler` " "(optimization performance may be degraded). "
# todo: should we remove the corners, whose normal derivative is not well defined? </s> if nz > 2:	Cuboid for v in [self.xmin[-1], self.xmax[-1]]: u = list(itertools.product(x, y)) for v in [self.xmin[1], self.xmax[1]]: u = np.array(list(itertools.product(x, z[1:-1])))
# todo : remove arg filename </s> result = self.php_code_exec_token("echo md5(file_get_contents('%s'));" % (path))	hash_remote_file if result[0]: content = result[1]
# todo: for backward compatibility only, remove if not used anymore </s> for v in vms['virtualmachine']:	get_vm args['projectid'] = self.get_project(key='id') vms = self.cs.listVirtualMachines(**args) if vm in [ v['name'], v['displayname'], v['id'] ]: self.vm = v
elevation_m = 0 # todo </s> vel = einsum('ij...,j...->i...', t.mt, vel)	_at pos, vel = terra(self.latitude.radians, self.longitude.radians, self.elevation.au, t.gast) if self.x: R = rot_y(self.x * ASEC2RAD)
# todo: require rewrite </s> def value_function(x):	value_function
# todo: process form submission </s> return redirect(url_for('index'))	col_delete_single source = get_source(sid) delete_collection(sid)
# todo: remove all elements of the list and remove the allowlist </s> ensure_api_is_typed(	test_api_typed "You can also take a look at the section about it in the CONTRIBUTING.md:\n" "https://github.com/tensorflow/addons/blob/master/CONTRIBUTING.md#about-type-hints" modules_list, exception_list, init_only=True, additional_message=help_message,
# todo: implement trough own logger? </s> self.entries.remove(entry)	_purge for entry in self.entries[:]: if entry in self.__filtered and not entry in self.__accepted: self.__purged += 1 self.__filtered = []
# todo remove else-branch after deprecating torch<1.9.0 </s> input shape ``[n, *, out_features]``.	_get_additional_dims module: A linear layer. Returns: return module.input0.shape[1:-1]
# todo: convert local to file uri / path </s> remainder = duration / self.count * (self.total - self.count)	increment self.count += 1 if self.count % 1000 == 0 or self.count == self.total: logging.info('Scanned %d of %d files in %ds, ~%ds left.', self.count, self.total, duration, remainder)
# todo: this relies on the gnu version of ps (need to fix macos support) </s> return stdout	get_stdout self.stdout_lock.acquire() stdout = self.stdout_file.getvalue()
# todo(dougalm): re-enable once we've tackled the less pendantic bugs </s> else:	gen_function arg_types = [v.vartype for v in arg_vars] fun, out_types = gen_function(size / size_reduction_factor, arg_types) arity = choice(primitive_generators.keys()) arg_vars = gen_sized_subset(cur_vars, arity)
# todo support for urls </s> output_contracts['evm'] = {'methodidentifiers': data['method_identifiers']}	format_to_output_dict if 'ir' in data: output_contracts['ir'] = data['ir'] evm_keys = ('bytecode', 'opcodes') if next((i for i in evm_keys if i in data), False):
# todo... do something with this annotation information </s> index = context.deleted_vars.pop()	ISTORE_name index = context.local_vars[name] except KeyError: except KeyError: index = len(context.local_vars)
# todo check if config was successfully updated </s> self._connect()	check_connection def check_connection(self): connected = False if self.ssh: connected = True
# todo: re-implement </s> base.metadata.drop_all(bind=engine)	init_sqlalchemy engine = create_engine('sqlite:///db-%s.sqlite' % self.configname, echo=self.options.debug_sql) Session.configure(bind=engine) Base.metadata.create_all(bind=engine)
# todo(qos) add agent extensions exception and catch them here </s> log.exception(	AgentExtensionsManager try: extension.obj.delete_port(context, data) _LE("Agent Extension '%(name)s' failed " "while handling port deletion"),
# todo - make sure to handle if there is no abi </s> transact_transaction.setdefault('to', self.address)	transact if 'data' in transact_transaction: raise ValueError("Cannot set data in transact transaction") if self.web3.eth.defaultAccount is not empty: transact_transaction.setdefault('from', self.web3.eth.defaultAccount)
# todo: remove this hack when dag info is stored in dir layout. </s> subparser.add_argument(	setup_parser 'spec', nargs=argparse.REMAINDER, help="spec of package extension to activate.")
# todo: specify a correct exception subclass. </s> tls_record += cert_msg	tls_generate_cert_msg tls_record += pack_2bytes(len(cert_msg))
# todo(haoyuzhang): remove this monkey patch when xla oom issue is fixed. </s> if not isinstance(lr, (float, np.float32, np.float64)):	on_batch_begin batch, self.batches_per_epoch, raise ValueError('The output of the "schedule" function should be float.') if lr != self.prev_lr:
# todo:  we might need additional logic comparing the state of git-annex </s> to, ds)	Publish if missing == 'skip': lgr.warning( ds_remote_info[ds_path] = None elif missing == 'inherit':
# todo symbolic frameworks? </s> message += '\n input tensor shape: {}. '.format(get_backend(tensor).shape(tensor))	reduce except EinopsError as e: message = ' Error while processing {}-reduction pattern "{}".'.format(reduction, pattern) else: message += '\n Input is list. '
# todo pre training and hard update before loop </s> action = a_tensor.cpu().data.numpy()[0]	get_episode_reward for _ in range(max_step): s_tensor = torch.tensor((state,), dtype=torch.float32, device=device) next_state, reward, done, _ = env.step(action) reward_item += reward
# todo: support ps fault-tolerance </s> self._ps_vars = ps_vars	init_ps_var_partition ps_vars[ps_id] = [v] else:
# todo(yanase): implement maximization. </s> return self.storage.get_study_system_attrs(self.study_id)	system_attrs @property
# todo: check for field </s> model instance	get_pk_value def get_pk_value(self, model): Return the primary key value from the model instance return model.pk
# todo(tobe): test other runtime containers. </s> container = basiccontainer.create_lambda_container(user_code_path, container_memory, container_cpu_shares)	POST@32 basicContainer = basic_container.BasicContainer() container_memory="1g" basicContainer.start_lambda_container(container) return "abc"
# todo: @sbharadwajj implement and test </s> def hessian_is_zero(self):	Conv1DDerivatives class Conv1DDerivatives(BaseParameterDerivatives): def get_module(self): return True def get_unfolded_input(self, module):
# todo: assert that set_step was called. </s> def split(self, data_inputs) -> list:	KFoldCrossValidation raise NotImplementedError("TODO")  # TODO.
# todo parse </s> return jsonify(result='render chunk sent to client')	worker_render_chunk 'start': 2, 'end': 8}
# @todo: we should move this to a shared method since filesystem.get_file_name() does it too. </s> }	parse_arguments 'location': None, 'album': None, config.update(args) return config
# todo need to pass on the session here </s> suggestion exists.	did_you_mean @property @serialized Will always return :class:`None` if the search isn't loaded. spotify.Error.maybe_raise(self.error)
# todo(shoyer): test fails on tpu </s> f = lambda pos, inc: (lax.add(pos, 1), lax.add(count, inc))	loop_body def loop_body(state): effect[0] = True return api.jit(f)(pos, inc)
# todo: alloc_shift </s> s = b[i]	_column_fillna_impl def _column_fillna_impl(A, B, fill):  # pragma: no cover if np.isnan(s): s = fill
# this is the dc dmdsec. @todo: account for other as well. </s> return fullfilecontent	generateFullFileEntry fullFileContent += "  <icon>" + filename + ".icon</icon>\n" fullFileContent += "  <update>0</update>\n  <info>nopdf</info>\n"
# todo: if py3k, override unpickler.find_class(). </s> return self.counter_tick_core(incrementing=1, mycpv=mycpv)	counter_tick def counter_tick(self, myroot=None, mycpv=None):
# todo blanket allows may be modified by subsequent denies... -- does policyuniverse handle? </s> merge (pnode:awspolicy{arn: {arn}})	load_policies def load_policies(session, policies, current_aws_account_id, aws_update_tag): ON CREATE SET pnode.policyid = {POLICY_ID}, pnode.firstseen = timestamp(), pnode.createdate = {CREATE_DATE} SET pnode.name = {POLICY_NAME}, pnode.path = {PATH}, pnode.defaultversionid = {DEFAULT_VERSION_ID},
# todo: check this out </s> return handle(handlerepobackend(self, branch))	get_handle
# todo(dcramer): ideally we could just send the signal to the subprocess </s> db.session.commit()	save_chunk offset=self.cur_offset, size=text_len, self.cur_offset += text_len
#todo: delete these following two lines when we default to geomean </s> json.dump(self.warning_history, open(tmp, "w"), indent=2, sort_keys=true)	saveWarningHistory tmp = fn + ".tmp"
# todo: do the computation without the 'sr' enforcement </s> freemoduleautomorphism._del_derived(self)	_del_derived sage: a._del_derived()
# todo: check rackspace file existence </s> filename='%s_%s_json.zip' % (name, ty)	download_name def download_name(self, app, ty): super(JsonExporter, self).download_name(app, ty) return filename
# todo: make these 3-d numpy arrays. </s> def evaluate_equality_constraints(self):	evaluate_equality_constraints
# todo: check that the performance measure is within some range </s> tests flow/benchmark/baselines/merge{0,1,2}.py	TestBaselines Tests flow/benchmark/baselines/grid1.py grid1_baseline(num_runs=1, sumo_binary="sumo") merge_baseline(num_runs=1, sumo_binary="sumo")
# todo: use parameterized tests to test all losses. </s> 'log': 1. / math.log(1. + rank),	_rank_discount def _rank_discount(rank_discount_form, rank): discount = { } return discount[rank_discount_form]
# todo: check that the birth date is not in the future </s> year += 100	_get_birth_date if year > 1953: raise ValueError('No 9 digit birth numbers after 1953.') return datetime.date(year, month, day)
# todo: remove in favor of a proper per-module selection </s> if sys.platform == 'win32':	Core if extra_args: args.extend(extra_args) args = ['"%s"' % arg for arg in args] os.chdir(os.getcwd())
# todo: set cookie </s> except keyerror:	api_view_settings del safe_settings['DNS_Port'] del safe_settings['HTTP_Interface_IP'] pass self.my_sender('application/json', bytes(json.dumps(safe_settings), 'utf-8'))
# todo add an option for preferred file descriptor here </s> chain_constraints.append(mem == cbvv)	_ip_overwrite_with_chain chain_constraints.append(state.regs.ip == stack_pivot.addr) mem = state.memory.load(chain_addr, chain.payload_len) if not state.satisfiable(extra_constraints=chain_constraints): continue
# todo: implement subdomains for slate tensors </s> def __init__(self, expression, tsfc_parameters=none):	LocalKernelBuilder expression which needs special handling. Instructions for assembling the full kernel AST of a Slate expression is :arg expression: a :class:`TensorBase` object. :arg tsfc_parameters: an optional `dict` of parameters to provide to
# todo(albert): this part of the test is broken because python </s> valid_src_files = ['hw1.py']	TestLoadTests VALID_ASSIGN = os.path.join(VALID, 'hw1', 'tests') VALID_NAME = 'hw1' def setUp(self): self.sample_test = mock.Mock(spec=core.Test)
# todo test </s> self.past_state.tostring(), self.network))	__hash__ def __hash__(self):
# todo: use shlex.quote as soon as a newer python version is available. </s> "line_regex": r"^.:\ *([0-9]*): (.*)"	RubocopCommand "shell": True, "working_dir": working_dir,
# todo, this is scratch/proto </s> pass	next_alternative
# todo: assert </s> def createrepo(self):	createRepo repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "name", "testrepo0", self.token)
# todo: probably not mutate these foreign attrs - ideally maybe move quite a bit of this method up to fleetstate (maybe in __setitem__). </s> self.remember_node(node)	read_nodes_from_storage def read_nodes_from_storage(self) -> set: stored_nodes = self.node_storage.all(federated_only=self.federated_only)  # TODO: 466
# todo: adjust dimension order for tf2 broadcasting </s> output.set_shape(shape)	sparsemax is_last_axis = (axis == -1) or (axis == rank - 1) if is_last_axis: return output rank_op = tf.rank(logits)
# todo: test for this error </s> this function is shorthand for match_tokens for a single token.	match_token def match_token(self, index, kind_expected):
# todo: warn if field has_choices but not in table.filtering </s> error = stderr	Command progress.update(process.stdin.write(data)) stdout, stderr = process.communicate() else: rows_imported = int(stdout.replace(b'COPY ', b'').strip())
# todo: see about implementing this via the popcnt instruction on </s> if equal(key, self._array[i]):	find_index def find_index(self, key): i = r_int(0) return i i += 2
# todo: remove this ``expectedfailure`` </s> self.assertlistequal(data2, [])	test_unsuccessful_read_transaction self.assertListEqual(data1, []) with self.assertNumQueries(1):
# todo: bytes vs str </s> return _route	route def _route(f): self.url_map.append((url, f, kwargs))
# todo(pep612): fix for paramspectype </s> ret_type=expand_type(callable.ret_type, id_to_type),	apply_generic_arguments arg_types=arg_types,
# todo: this is untested. </s> def raise_if_problem(self):	_VerifyHelper return 0 self.callback = _ffi.callback( if self._problems: try:
# todo: backwards compatibility; remove in favor of class method </s> ):	_load_recorded_calibrations if ( CalibrationResultNotification.calibration_format_version() logger.debug( f"Must update CalibrationResultNotification to match Calibration version"
# todo - this only allows using the default constructor </s> arg_names = code.co_varnames	extract_parameters def extract_parameters(code, annotations): keyword_only_count = code.co_kwonlyargcount parameters = []
# :todo: implement test. </s> self.skiptest('not implemented yet.')	FindTransactionsRequestFilterTestCase def test_fail_tags_wrong_type(self): self.skipTest('Not implemented yet.') def test_fail_approvees_wrong_type(self): self.skipTest('Not implemented yet.')
# todo increase precision </s> def create_monomial_exponents2(degree):	create_monomial_exponents2
# todo(boris-42): make it work through assertisinstance </s> def __init__(self, config):	EngineFake3 pass
# todo find out what is best used here! </s> self.max_iter = max_iter	RidgeRegression self.fit_intercept = fit_intercept self.normalize = normalize self.tol = tol self.solver = solver
creator = chain.web3.eth.accounts[0]  # todo: make it possible to override </s> chain.wait.for_receipt(tx, timeout=blockchain.timeout)	create_escrow@47 escrow, tx = chain.provider.get_or_deploy_contract( ESCROW_NAME, deploy_args=[token.address] + MINING_COEFF, return escrow
# todo: fix this </s> def num_languages(self):	num_languages return len(self._languages) if self._languages else None
# todo: return proper searchable iterator </s> data["action_type"] = "ma-type:user-generated-message"	wave Args: first: Whether to wave first or wave back data["lightweight_action_attachment[lwa_state]"] = ( "INITIATED" if first else "RECIPROCATED"
# xxx todo: rounding </s> instr, extra_ir = mnemo_func[instr.name.lower()](ir, instr, *args)	get_mnemo_expr def get_mnemo_expr(ir, instr, *args): if not instr.name.lower() in mnemo_func: return instr, extra_ir
raise notimplementederror # todo </s> return open(filename, 'rb')	file_opener def file_opener(filename): return _opener
# todo: migrate to glaziererror </s> args:	_LogFatal where to go for further assistance. - Log the user-facing failure string msg: The error message to accompany the failure. build_info: The active BuildInfo class.
# todo(john sirois): map target.resources in the same way </s> if len(analysis_cache_files) != 1:	analysis_cache_full_path if len(analysis_cache_product) != 1: raise TaskError('There can only be one analysis cache file per output directory') raise TaskError('There can only be one analysis cache file per output directory') return os.path.join(analysis_cache_dir, analysis_cache_files[0])
# todo gdef/lookup markfilteringsets </s> return l	sumLists l = [] for item in lst:
# todo: fix this </s> return len(self._languages) if self._languages else none	num_languages @property
uploader.upload_file(file, container='export') # todo: right container folder?! </s> apps_cached = []	warm_cache env_cache_disabled = os.environ.get('PYBOSSA_REDIS_CACHE_DISABLED') if not env_cache_disabled: pages = range(1, 4) import pybossa.cache.apps as cached_apps
# todo: deleted packages are currently being removed from the search </s> entity_types=default_entity_types):	_test_can def _test_can(self, action, users, entity_names, self._test_expectation(action, users, entity_names, interfaces=interfaces,
# todo partially update stored playlists? </s> u'callback called: message for track %d in playlist '	track_message_changed logger.debug(
pass  # todo: implement this </s> for line in range(self.cursor.y,	insert_lines count = count or 1 top, bottom = self.margins min(bottom + 1, self.cursor.y + count)): self.linebuf.pop(bottom)
# todo: make the min-max values a setting? </s> event = gst.event_new_tag(taglist)	set_metadata taglist[gst.TAG_TITLE] = track.name if track.album and track.album.name: self._playbin.send_event(event)
# todo: @sbharadwajj implement and test </s> def _jac_mat_prod(self, module, g_inp, g_out, mat):	Conv1DDerivatives raise NotImplementedError def ea_jac_t_mat_jac_prod(self, module, g_inp, g_out, mat): raise NotImplementedError def _jac_t_mat_prod(self, module, g_inp, g_out, mat):
# todo: force to download it </s> with open(file_name, "rb") as myfile:	get_md5 def get_md5(file_name): for line in myfile: md5_hash.update(line)
# todo: test me </s> return '(?p<{0}>[^/]+)'.format(kwarg_name)	get_lookup_allowed_symbols@17 if get_rest_framework_features()['use_dot_in_lookup_regex_by_default'] or force_dot: return '(?P<{0}>[^/.]+)'.format(kwarg_name)
# todo: unit-test this method. </s> path = locator.find_path_by_object(search_dirs, obj)	Loader obj: an instance of a user-defined class. search_dirs: the list of directories in which to search. return self.read(path)
# todo(yanase): update sklearn integration to support v0.22.1 or newer. </s> 'typing',	get_install_requires 'scipy<1.4.0', 'sqlalchemy>=1.1.0', 'joblib',
# todo(kan-bayashi): documentation and type hint </s> min_bin_height=default_min_bin_height,	piecewise_rational_quadratic_transform@18 tails=None, tail_bound=1.0, min_derivative=DEFAULT_MIN_DERIVATIVE, ):
# todo(b/161332815): make jax actor work with batched or unbatched inputs. </s> def observe_first(self, timestep: dm_env.timestep):	RecurrentActor self._prev_state = self._state  # Keep previous state to save in replay. self._state = new_state  # Keep new state for next policy call. if self._adder: self._adder.add_first(timestep)
if util.pythonise(pipe['wires'][wire]['tgt']['moduleid']) == module_id and pipe['wires'][wire]['tgt']['id'] != '_input' and pipe['wires'][wire]['src']['id'].startswith('_output'): # todo? this equates the outputs </s> pipe['modules'][util.pythonise(module['id'])] = module	_parse_pipe pipe['graph'] = {} pipe['wires'] = {} pipe['graph'][util.pythonise(module['id'])] = [] if module['type'] == 'loop':
# todo: use widgets.dialog </s> if self._editor_component and self.is_focused():	OnTreeSelection def OnTreeSelection(self, message):
# todo(ssbarnea): remove that deprecation fallback in 3.1+ </s> playbook = self._config.provisioner.abs_path(playbook)	_get_playbook except Exception: pass playbook = self._normalize_playbook(playbook) if os.path.exists(playbook):
# todo xxx graalvm change </s> ssl_sock.context = other_context	test_sni_callback def servername_cb(ssl_sock, server_name, initial_context): calls.append((server_name, initial_context)) server_context.set_servername_callback(servername_cb) stats = server_params_test(client_context, server_context,
# todo see https://github.com/healthcatalyst/healthcareai-py/issues/276 </s> self.grain_column = grain_column,	__init__@17 def __init__(self, dataframe, predicted_column, model_type, impute=True, grain_column=None, verbose=False): self.grain_column = grain_column, self.grain_column = grain_column, pipeline = pipelines.full_pipeline(model_type, predicted_column, grain_column, impute=impute)
# todo(stephenfin): use a helper </s> host.	_create_server server = super()._create_server(networks='none') self.addCleanup(self._delete_server, server)
# todo: make legacy detection non-reliant on side </s> with open(self.filepath, 'r') as config:	read_config def read_config(self): return config.read() except (OSError, IOError), e:
# todo this is a workaround since exceptions are currently not correctly stacked </s> return tregex_engine(pattern, self.jsflags)	__tregex_compile
# todo: documentation pending </s> if is_train == self.is_train:	_check_mode if is_train is None and self.is_train is None: raise ValueError("Training / inference mode not defined. Argument `is_train` should be set as True / False. Otherwise please use `Model.train()` / `Model.eval()` to switch the mode.") logging.warning("Training / inference mode redefined redundantly. Please EITHER use the argument `is_train` OR `Model.train()` / `Model.eval()` to define the mode.") else:
# todo: this is untested. </s> def raise_if_problem(self):	_VerifyHelper return 0 self.callback = _ffi.callback( if self._problems: try:
# todo: test size=var, with shape that change from call to call </s> dtype='float32'),	test_seed_fn fn1 = theano.function([], random.uniform((2, 2), dtype='float32'), mode=mode) mode=mode) fn3 = theano.function([idx],
# todo: logging belongs in on_tag_added hook </s> def addons(self):	addons return self.get_addons()
#todo - introduce an annotated alignment class? </s> return (name, start, end)	_identifier_split name = identifier.split("/",1)[0]
## todo : log error </s> return string_list	stringify_listvars string_list = map(lambda x: x is not None and str(x) or "", mylist) except IndexError:
# xxx todo: rounding </s> instr, extra_ir = mnemo_func[instr.name.lower()](ir, instr, *args)	get_mnemo_expr raise NotImplementedError('unknown mnemo %s' % instr)
# todo document </s> which the coefficients are stored. the coefficients can safely be	load_all_methods stored as attributes. This method also sets :obj:`Tmin`, :obj:`Tmax`, and :obj:`all_methods` as a set of methods for which the data exists for. altered once the class is initialized. This method can be called again to reset the parameters.
# short-circuit. todo: test it. </s> return tuple(execute(element, contexts) for element in self.value)	execute_tuple @register_executor(requests.TupleRequest)
# add newly added packages to the todownload list </s> print self.repos	doUtilYumSetup try: archlist = rpmUtils.arch.getArchList(sys.argv[2]) self._getSacks(archlist=archlist, thisrepo=sys.argv[3]) except yum.Errors.YumBaseError, msg:
"size": 50  # todo: support pagination. </s> kwargs = {}	search_file body = { "query": {
)  # todo ensure dictionaries stay dictionaries </s> if the input workflow or wfmodule is deleted, return ``none``.	_maybe_add_version `maybe_result` is non-``None`` and the result isn't the same as the previous one. Che caller may create a ``ChangeDataVersionCommand`` to set fields = {"is_busy": False, "last_update_check": timezone.now()} if maybe_result is not None:
# todo: elif gcpinstance().is_gcp_instance(): </s> env = azure	EnvironmentCollector if AwsInstance().is_aws_instance(): env = AWS else: env = ON_PREMISE
# todo: this is a temporal fix </s> info_str = "{}output image size {}w x {}h with {} channels.".format(	es info_str = "{}  - Input image is {}W x {}H with {} channels.\n".format( info_str, image_data.shape[2], image_data.shape[1], info_str, es_pixels.shape[2], es_pixels.shape[1], es_pixels.shape[0])
# todo 2.8 i want to change the way we handle unit scaling, see </s> key = make_key(dg_obj_instance.object.original)	make_key_from_instance key += "_" + make_key(dg_obj_instance.parent.original) key += "_".join([str(pid) for pid in dg_obj_instance.persistent_id]) return key
# todo: remove once elasticsearch v6.x is deprecated. </s> self._document_type = document_type	SetDocumentType document_type (str): document type.
# todo: this only works when orgmode is started with one orgfile </s> u':py orgmode.plugins[u"agenda"].list_all_todos()<cr>')))	Agenda leader = settings.get(u'org_leader', u',') self.keybindings.append(Keybinding(u'%scat' % leader, self.menu + ActionEntry(u'Agenda for all TODOs', self.keybindings[-1]) self.keybindings.append(Keybinding(u'%scaa' % leader,
# todo: consider adding some better error handling for bad/failed requests. </s> ),	_get_user_hubspot_id req = requests.get( u"https://api.hubapi.com/contacts/v1/contact/email/{}/profile".format( params={'hapikey': api_key}, )
# @todo: move to css </s> @param o: the original value	init_requires Initialize all IS_NOT_IN_DB to allow override of both original and duplicate value @param d: the duplicate value allowed_override = [str(o), str(d)]
# todo not tested </s> _api.openssl_free(hex_serial)	get_serial_number serial = int(hexstring_serial, 16) return serial finally: _api.BN_free(bignum_serial)
# todo: log exception </s> except exception as e:	get_note@443 return result
# todo: 判断返回结果，处理异常 </s> asset_groups = assetgroup.objects.all()	perm_role_push role_id = request.GET.get('id') role = get_object(PermRole, id=role_id) if request.method == "POST": asset_ids = request.POST.getlist("assets")
self.ws = 2  # todo check. </s> self.ir_emulator.write_memory(segment.header.p_vaddr + i, 1, b)	_load_binary_elf logger.info("Loading segment #{} ({:#x}-{:#x})".format(index, segment.header.p_vaddr, segment.header.p_vaddr + segment.header.p_filesz)) f.close()
# todo make this smarter about more complex configurations (backup original values, etc) </s> port_change = port_change[1].split()	Parser elif ' Port ' in line: port_change = line.split(' Port ') if port_change[1] == 'down': m_table = self.mac_table.copy()
# todo open file for writing </s> def set_metadata(self, metadata):	set_metadata @interfacedoc
# todo remove backwards compatability fix in a future version </s> os.rename(old_file, new_file)	_migrate_rcfile if os.path.exists(new_file): print('NeoVintageous: could not migrate "%s" to "%s": target already exists' % (old_file, new_file))
# todo: fix clone issue </s> assert (pred_proba.min() >= 0)	test_prediction_proba_linear pred_proba = self.clf.predict_proba(self.X_test, method='linear')
@retry()  # todo: what errors do we get for timeout, json parse failure, etc? </s> logger.warning("no available ami found in flatcar release feed; checking marketplace")	get_flatcar_ami logger.debug("No AMI found in TOIL_AWS_AMI; checking Flatcar release feed") ami = official_flatcar_ami_release(ec2_client=ec2_client) ami = aws_marketplace_flatcar_ami_search(ec2_client=ec2_client) if not ami:
# todo: move to a base class, and have test cases inherit from that. </s> self.user_settings.verify_oauth_access(	test_verify_false_metadata metadata={'foo': 'bar'} ) external_account=self.external_account, node=self.project,
# todo: adapt units_qs once we allow filtering </s> unitstates = request.get['unitstates'].split(',')	get_step_query if 'unit' in request.GET or 'page' in request.GET: return units_queryset if unitstates: state_queryset = units_queryset.none()
# todo(jay-lau-513) translate the contents to a json stdin </s> log.error("couldn't delete service %s due to error %s"	service_delete if err: return False % (uuid, e)) return False
# todo: copy resources once they're specified </s> install_kernel_spec(td, 'bash', user=user, replace=true)	install_my_kernel_spec@20 with open(os.path.join(td, 'kernel.json'), 'w') as f: json.dump(kernel_json, f, sort_keys=True)
# todo this behavior may change when eager mode is introduced </s> vb = np.random.randint(0, 10000, (100, 100))	testApi@159 with self.assertRaises(SystemError): sess.run(c) a = mt.array(va, chunks=30) b = mt.array(vb, chunks=30)
# todo :: move arbitray path construction to storagelayout object </s> bad_links = []	_verify_restore_paths def _verify_restore_paths(self, restor_spec): if not 'tablespaces' in restor_spec: return
# todo error reporting over the master event bus </s> log.warn('unaccounted for authentication failure')	ClearFuncs else: pass eload = {'result': False, 'id': load['id'],
# todo: try to remove when https://bugzilla.mozilla.org/show_bug.cgi?id=1508695 is fixed. </s> assert val in ['0', '1']	bool_str return True if val == '1' else False
# todo check if right </s> super(comet, self).__init__(**kwargs)	Comet class Comet(Graph): if k: self.k = k
recording_software_version = none  # todo </s> except keyerror:	is_pupil_invisible_recording info_csv = utils.read_info_csv_file(rec_dir) try: return False
# todo: the following reproduces the old behavior of </s> idx = s.ord(i)-1	_ctr_fun def _ctr_fun(i): if idx == 0:  # Needed since '-1' is considered a valid index in Python raise IndexError("list index out of range")
# todo: message depending on narrowing/float-conversion </s> x.dtype	castarray@13 def castarray(x, dtype): except AttributeError: msg = 'Implicit conversion from {} to {} (performance)'
# todo: might need some kind of diffing too? </s> elif list_a[uid] != status[uid][0]:  # item was updated in a	get_actions@80 if uid in list_a and uid in list_b: if list_a[uid] != status[uid][0] and list_b[uid] != status[uid][1]: prefetch_from_a.append(uid) actions.append(('update', uid, 'a', 'b'))
#todo: a single softmax'd vector?? </s> b = tensor.zeros_like(x[0,:])	crossentropy_softmax_1hot def crossentropy_softmax_1hot(x, y_idx, **kwargs):
invoice_data.pop("forgiven", none)  # todo remove </s> @patch(	test_invoice_without_plan "djstripe.models.Account.get_default_account", autospec=IS_STATICMETHOD_AUTOSPEC_SUPPORTED, "stripe.BalanceTransaction.retrieve", return_value=deepcopy(FAKE_BALANCE_TRANSACTION),
# todo: only allow classmethods on base/abstract classes </s> if not ifaces:	_create_adapter_test } def _test_adapter(self, adapter): self.fail("%s does not provide any interfaces" % adapter) methods = []
#todo change to native framework call, when plex allows token in header </s> @classmethod	playlistsV3 def init(self): self.getListsURL = misc.GetLoopBack() + '/playlists/all'
print("key error")  # todo </s> return local_functions	_module_functions for k,v in local_functions.items(): if not inspect.isfunction(v) or k.startswith('_'):
# :todo: implement test. </s> transactionid(self.trytes1),	GetInclusionStatesRequestFilterTestCase TransactionId(self.trytes2), ], TransactionId(self.trytes2), ],
error = errors[0]  # todo: handle multiple errors </s> class facebookerror(exception):	FacebookError All exceptions in the module inherit this. message = attr.ib(type=str)
# todo: verify validity of exchanging question and support. below: encode question, conditioned on support encoding. </s> output = boe_reader(question_embedded, question_lengths,	boe_reader_model varscope.reuse_variables() support_embedded = nvocab(support) support_embedded, support_lengths) print("INPUT SHAPE " + str(question_embedded.get_shape()))
# todo: there was a bug related to this </s> def test_relu_clipped_gradients_are_zero():	test_relu_clipped_gradients_are_zero@12 @pytest.mark.skip
# todo(guillermooo): implement other options. </s> def option(self):	option return self.params ['option']
# todo fix. </s> ctx_init = self.__init_context()	test_cmova def test_cmova(self): x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) cmp_result = self.__compare_contexts(ctx_init, x86_ctx_out, reil_ctx_out)
# todo: fix return type in generated code? </s> self._check_error('failed to get value')	_get_3floats al.alGetListener3f(key, x, y, z)
# todo: check types </s> getvalues = c.pyapi.object_getattr_string(val, "values")	unbox_timestamp_series @unbox(TimestampSeriesType) return unbox_array(types.Array(dtype=types.NPDatetime('ns'), ndim=1, layout='C'), getvalues, c)
e_die('divide by zero')  # todo: location </s> return i, lval	_EvalLhsAndLookupArith e_die("Can't use assignment like ++ or += on arrays") span_id = word_.SpanForLhsExpr(node)
# todo assert cls.__tablename__ == '' </s> s = model.galaxysession()	galaxy_session def galaxy_session(model, session, user):
# todo: call out to the ceph cluster to check the </s> fqdns = self.ceph_ctl.get_server_fqdns()	_get_version def _get_version(self): version = self.api.get("server/{0}".format(fqdns[0])).json()['ceph_version'] log.debug("version = %s" % version)
# todo - add team payroll and check receiving values </s> assert alice.is_free_rider is none	test_stt_resets_is_free_rider_to_null alice.set_tip_to(gratipay, '0.00')
annot.annotation_metadata.annotator.email = "todo"  # todo </s> fill_section_annotation(json_file, annot)	convert_JAMS fill_global_metadata(jam, json_files[0]) for json_file in json_files: f = open(out_file, "w") json.dump(jam, f, indent=2)
# todo self.update_repo should take this as a dictionary and create </s> '0.2.0', '0.dev.3.100.g3d644b2.dirty'),	test_good '0.2.0dev2-99-g3d644b1': rpm_version('0.2.0', '0.dev.2.99.g3d644b1'), } unexpected_results = []
return  #todo disabled for now, see #2151 for details </s> else:	sanity_checker SavegameManager.create_multiplayersave_filename(string) except RuntimeError: return True
# todo: can we assume reverse=false? </s> ).order_by('-num_times')	most_common def most_common(self, min_count=None): queryset = self.get_queryset().annotate( if min_count: queryset = queryset.filter(num_times__gte=min_count)
# todo: check pdf content? how? </s> assert h1.text == u'weasyprint test document (with ünicōde)'	test_html_parsing _head, body = html.root_element assert [child.tag for child in body] == ['h1', 'p', 'ul'] url = urlparse.urljoin(h1.base_url, 'pattern.png') assert url.startswith('file:')
# todo check if there is more than 1 device. </s> xxx_resp = requests.get(self.cerberus_url + '/v1/secret/' + path + '/?list=true',	get_sdb_list headers={'Content-Type' : 'application/json', 'X-Vault-Token': self.token})
# todo: consider parameterize this </s> regressor_log_scale: float	make_ts_multiplicative regressor_relevance: float 0 to 1; higher value indicates less number of useful regressors regressor_log_cov: 2-D array_like, of shape (num_of_regressors, num_of_regressors) covariance of regressors in log unit scale
# todo: confirm necessity of this session clearing and lay out mechanics. </s> url_with_unsupported_uri = self.url.replace('http', 'file')	test_download_url_to_tempfileobj_and_urls download_file, 'http://localhost:' + str(self.PORT+1) + '/' + self.random_string(), self.assertRaises(requests.exceptions.InvalidSchema, download_file, url_with_unsupported_uri, self.target_data_length)
# todo(termie): we should probably return not founds instead of none </s> token_id = self.options['admin_token']	IdentityApi data = json.loads(resp.body) self.assertDictEquals(self.tenant_bar, data[0]) c = client.TestClient(self.app, token=token_id) user_ref = models.User()
# todo: skipped due to gh-4436 </s> assert_raises(valueerror, ds.create_sibling_ria, 'ria+file:///some/where',	test_invalid_calls def test_invalid_calls(path): ds = Dataset(path).create() name='some', storage_name='some')
# todo could use a generator here </s> def get_operands(self, idx=-1):	Instruction23x buff = "" buff += "v%d, v%d, v%d" % (self.AA, self.BB, self.CC) return [(OPERAND_REGISTER, self.AA), (OPERAND_REGISTER, self.BB), (OPERAND_REGISTER, self.CC)]
# todo: these reductions could be delayed until _step is called. </s> gradient.assign(tf.zeros_like(gradient))	Trainer grads_and_vars.append((scaled_gradient, variable)) optimizer.apply_gradients(grads_and_vars) @dataset_util.function_on_next(dataset) def _forward(next_fn):
# todo: usefully interpret it & other non-public-number fields </s> key = serialization.load_der_private_key(	_decode_key def _decode_key(self, data): data, password=None, backend=default_backend() )
# todo: act </s> distro = self.remote.new_distro(self.token)	test_create_distro_positive @pytest.mark.usefixtures("removeTestdistro") def test_create_distro_positive(self): self.remote.modify_distro(distro, "name", "testdistro", self.token) for field in self.distro_fields:
# todo: for some reason on osx a unix socket cannot be </s> exe = p.exe()	_BaseFSAPIsTests create_exe(self.funky_name) subp = get_test_subprocess(cmd=[self.funky_name]) self.assertIsInstance(exe, str) if self.expect_exact_path_match():
# todo: support other reductions </s> equiv_set.insert_equiv(*all_shapes)	aggregate_array_analysis col_shape = equiv_set.get_shape(col_var) all_shapes.append(col_shape[0]) post = [] all_shapes = []
#  todo: move to le-utils package </s> new_msg['timestamp'] = msg.get('timestamp')	create_and_update_notifications new_msg['version_range'] = msg.get('version_range') new_msg['link_url'] = msg.get('link_url') new_msg['source'] = source PingbackNotification.objects.update_or_create(defaults=new_msg, id=new_msg['id'])
# todo: check that the group is public and we're being added publically </s> get_domain_from_id(group_id), group_id, requester_user_id,	get_group_summary ) defer.returnValue(res) ) chunk = res["users_section"]["users"]
# todo(mcgallaspy): get rid of old integration tests and refactor the mixin methods </s> self.browser = context.browser	ContextWithMixin class ContextWithMixin(CreateAdminMixin):
#todo: check cost line </s> producer.__init__(self)	StorageBuilding self.local_carriages = [] Building.__init__(self, x, y, owner, instance) Consumer.__init__(self) self.local_carriages.append(game.main.session.entities.units[2](self))
assert 'could not load target data' in res.stdout  # todo: stderr </s> return target_path	basic_target_path target_path = str(tmp_path) + '/basic_target.txt' with open(target_path, 'w') as f:
## todo: # fixme: remove me </s> paste_parent = self.r_serv_onion.hget('onion_metadata:{}'.format(self.domain), 'paste_parent')	get_last_crawled_pastes return self.get_all_pastes_domain(paste_parent)
# todo: url accepts post, need to refactor to use get+post </s> return test_cases.exclude(case_status=testcasestatus.get_confirmed()).count()	get_disabled_test_cases_count
pass  # todo... </s> array, start_idxs, batch_lens, beam_width = input_shapes	infer_shape def infer_shape(self, node, input_shapes):
# todo(b/161332815): make jax actor work with batched or unbatched inputs. </s> self._prev_state = self._state  # keep previous state to save in replay.	RecurrentActor key=next(self._rng), observation=observation, self._state = new_state  # Keep new state for next policy call. return utils.to_numpy(action)
# todo: support parameterization through full covariance matrix. </s> num_events (int): the number of events in the multivariate gaussian. default: bivariate (2).	MultivariateNormal def __init__(self, num_events=2, parameterize_via_diagonal=True, parameterize_via_covariance=False, scope="multivariate-normal", **kwargs): parameterize_via_diagonal (bool): Whether we are parameterizing via the diagonal stddev values. Note that
# todo: copy fixture types and data </s> new_report_perms.append(get_ucr_class_name(linked_id))	_convert_reports_permissions if master_report_perm in role_def['permissions']['view_report_list']:
# todo security </s> 'hue_core': hue_core,	admin_core_schema 'solr_schema': solr_schema,
# todo: dump content out for debugging in the future. </s> return driver_factory.get_local_driver(galaxy_test_selenium_browser)	get_local_driver
# todo: test stubs for other versions, especially python 2 stubs. </s> rv = rv[:-len('.__init__')]	file_to_module if rv.endswith('.__init__'):
# todo: must be the same if we merged/pushed before, if not -- skip </s> recursive=false,	Publish to=None, since=None, recursion_limit=None, git_opts=None,
# todo: this filtering condition is probably wrong </s> box, children=[child.copy() for child in box.children])	create_column_box def create_column_box(): resolve_percentages(column_box, containing_block) column_box.width = width
# todo: fix this! </s> return cmd.exe, cmd.arg, cmd.exe_options, set(comp)	cli_test def cli_test(self, cmd_tree, command): cmd = Command(cmd_tree, command)
# todo: add content disposition. </s> response["task"] = entry	tasks_view entry["errors"] = [] for error in task.errors: else: return HTTPError(404, "Task not found")
# todo: make truly async </s> self._resourcemanager_client = discovery.build('cloudresourcemanager', 'v1', cache_discovery=false, cache=memorycache())	__init__@10
# todo: this may block for 2 second if port is not used on windows </s> sock.close()	is_port_inuse@21 def is_port_inuse(port): sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) return rv == 0
# todo(sloria): test me </s> pass	dropbox_create_folder
# todo: add cwa logic </s> return self._create_owa_instance_object(instances=training_triples,	TriplesFactory training_triples, _ = self.map_triples(train_triples=self.train_triples, test_triples=self.test_triples) entity_to_id=self.entity_to_id, relation_to_id=self.relation_to_id)
#return qvariant()  # todo: is this right? </s> return	Model if row < len(self.items): return str(self.items[row])  # TODO: is this right? def flags(self, index): return QtCore.Qt.ItemIsEnabled | QtCore.Qt.ItemIsSelectable #| QtCore.Qt.ItemIsEditable
# todo: implement </s> raise notimplementederror	PytorchWrapper def to_gpu(self, device_num): raise NotImplementedError def resize_output(self): raise NotImplementedError
# todo: add at least reflection tests before adding notimplemented version </s> start, end = 0, none	playlistinfo - uses negative indexes, like ``playlistinfo "-1"``, to request the entire playlist else: tracklist_slice = protocol.RANGE(parameter)
# todo also handle method and request </s> protocol = dockerproxyclient	DockerProxyClientFactory
# todo: replace suite with testcases </s> _check_format(json_file, json_content)	load_json_file raise exceptions.FileFormatError(err_msg)
# todo: caching? </s> if not os.path.isdir(dir):	_list_kernels_in directories from it. if dir is None: os.makedirs(dir, mode=0o644) return {f.lower(): pjoin(dir, f) for f in os.listdir(dir)
# todo(mordred) fix that data path too </s> del cloud['cloud']	_get_base_cloud_config cloud['auth'] = dict() _auth_update(cloud, our_cloud) return cloud
# todo: add option for simple print without colors & machine readable format </s> if optional:	_get_project_arg If the first entry in args is a project name (e.g., '@project') then return the name of the project, or None. return None else:
# todo(iceboy): fix caller when udoc=none is passed in. </s> dt.astimezone(tz).strftime('%y-%m-%d %h:%m:%s')))	_datetime_span return markupsafe.Markup( '<span class="time" data-timestamp="{0}">{1}</span>'.format(
# todo you should put some extra protection on this, so a user can only </s> return jsonify({'msg': msg})	non_fresh_protected ip = jwt_claims['ip']  # Access data stored in custom claims on the JWT username = jwt_identity  # Access identity through jwt_identity proxy
""" todo: documentation </s> def _prepare(self, argv, input_file):	StreamingCommand super(StreamingCommand, self).__init__() def stream(self, records): configuration = type(self)._configuration argv = argv[2:]
# todo need data </s> data = response.json()	test_sub_projects_by_repo def test_sub_projects_by_repo(augur_db_routes): assert response.status_code == 200 assert len(data) >= 1
# todo, pass also best score </s> self.trainer.root_gpu = gpu_idx	ddp_train model = model.configure_sync_batchnorm(model) if self.trainer.on_gpu: torch.cuda.set_device(self.trainer.root_gpu) model.cuda(self.trainer.root_gpu)
#todo discont: actually use offsets instead of (start, end)! </s> s += '%s<span class="accesskey">%s</span>%s</label>' % (escape(dt[:key_offset]), escape(dt[key_offset:key_offset+1]), escape(dt[key_offset+1:]))	__generate_input_and_label if key_offset == -1: s += '%s</label>' % escape(dt) l.append(s) return l
raise  # todo: error reporting </s> self.history.insert(0, filename)	_remember index = self.history.index(filename) del self.history[index] self.filesCB.insertItem(0, os.path.basename(filename), userData=filename)
# todo: move succeed to papyon callbacks </s> print "error :", error_type, " ->", error	on_client_error
#todo: check if/where this is used; if not used externally - remove </s> self.gui.remove_surface(surface)	remove_surface self.surfaces.remove(surface) self._update_ui()
# todo does not work after multiprocessing branch merge </s> ``commands``	_reflection_commands @handle_pattern(r'^commands$') def _reflection_commands(self): Shows which commands the current user has access to. pass # TODO
# todo: support classification </s> self.hparams.static_reals	reals def reals(self) -> List[str]: return list( + self.hparams.time_varying_reals_encoder + self.hparams.time_varying_reals_decoder
# todo: does this type of numeric literal actually work? </s> if self.model is none:	Writer trees = self.trees if storage is None: self.model = RDF.Model(storage) if self.model is None:
# todo: call to _keys is bad </s> return the number of times a statement occurs in the database	get_occurrence_count return statement.get("occurrence", 1)
# todo: this is untested. </s> if self._problems:	_VerifyHelper self.callback = _ffi.callback( "int (*)(int, X509_STORE_CTX *)", wrapper) try: _raise_current_error()
# todo: does not handle pruning. </s> if not port_order:	get_lacp_dpid_nomination if valve.dp.is_stack_root(): root_dpid = valve.dp.dp_id return None, '' most_ports_dpid = port_order[0]
#todo: if the device is managed and user has nmcli installed, </s> .. note: this method will set the interface to managed mode.	set_interface_mac_random :type interface_name: str :return: None Also the first 3 octets are always 00:00:00 by default Only set the mac address when card is in down state
# todo: handle timeout </s> return self.__listeningip	TCPClient @property def listeningIP(self): @listeningIP.setter @typeCheck(str)
# todo: a way to store a channel's key in configuration </s> channel, _sep, part_msg = trigger.group(2).partition(' ')	part @sopel.module.priority('low') @sopel.module.example('.part #example') if part_msg: bot.part(channel, part_msg)
# todo incorporate weights -- currently handled by smoothing </s> cnarr : copynumarray	segment_hmm log-ratios and b-allele frequencies across a chromosome. Parameters The bin-level data to segment. method : string
):  # todo? this equates the outputs </s> pipe['modules'][util.pythonise(module['id'])] = module	_parse_pipe if not isinstance(modules, list): modules = [modules] pipe['graph'][util.pythonise(module['id'])] = [] if module['type'] == 'loop':
# todo: use a namedtuple? </s> and integral type information.	context_kernels @cached_property from firedrake.slate.slac.tsfc_driver import compile_terminal_form cxt_list = [compile_terminal_form(expr, prefix="subkernel%d_" % i,
# @todo: where is this in the yt api? </s> g.attrs["current_redshift"] = pf.current_redshift	write_to_gdf@87 g.attrs["unique_identifier"] = pf.unique_identifier g.attrs["cosmological_simulation"] = pf.cosmological_simulation g.attrs["omega_matter"] = pf.omega_matter g.attrs["omega_lambda"] = pf.omega_lambda
" # todo: i18n", </s> " ",	ToolTest "+++ {self.target}", "@@ -7,8 +7,8 @@", " # todo: i18n", '-print("Hello world!")',
# todo: consult best practices for python and twisted logging. </s> json.dump(top.state_to_json(), f)	noteDirty def noteDirty(): pass
# todo: remove this when domain decomposition is merged </s> for key in self._output:	create_output_subelement def create_output_subelement(self): if not self._output is None: subelement = ET.SubElement(element, key) subelement.text = str(self._output[key]).lower()
# todo: use different flag than .reentrant </s> state.sorting = colorsorter.sorting	_suspend state = _attrholder() if len(ColorSorter._gl_name_stack) > 1: state._sorted = ColorSorter._sorted state._immediate = ColorSorter._immediate
# :todo: implement test. </s> def test_fail_depth_too_small(self):	GetTransactionsToApproveRequestFilterTestCase self.skipTest('Not implemented yet.')
# todo: do i also need to test post/array? </s> when versioning is turned on.	test_data_relation_without_version def test_data_relation_without_version(self):
uploader.upload_file(file, container='export') # todo: right container folder?! </s> warm_app(a['id'], a['short_name'])	warm_cache page, app.config['APPS_PER_PAGE']) cached_users.get_leaderboard(app.config['LEADERBOARD'], 'anonymous') cached_users.get_top()
# todo: g+ has a multiply-valued 'urls' field. ignoring for now because </s> return self	get_or_save params={'response_key': self.key.urlsafe()},
# todo implement for all channels </s> payload = {'togglex':{'channel':channel, 'onoff':1}}	turn_on_channel return self._execute_cmd("SET", "Appliance.Control.ToggleX", payload)
# todo testing </s> async_call.send(none, self.i_am, address, device_id, max_apdu_len,	i_am_callback seg_supported, vendor_id):
# todo: these are required for consistency with couch representation, figure out how best to deal with it </s> def change_transform(self, doc_dict):	change_transform
# wait until the chunks have added, todo change this to a qtbot.waitsignal </s> screenshot[screen_offset, screen_offset], target_edge	test_tiled_rgb@107 screen_offset = 3  # Offset is needed as our screenshots have black borders np.testing.assert_allclose(screenshot[tuple(center_coord)], target_center) ) np.testing.assert_allclose(
# todo use the phobos settings for this information </s> hierarchy.	ind :return: str -- the current indentation (e.g. "  "). return "" + self.indentation * self.indent
# todo: test for last revision minus 50 on second page. </s> offset = url_for(controller='revision', action='purge', id=none)	test_purge res = self.app.get(offset) assert 'No revision id specified' in res
elif order <= 2147483647:   # todo: don't hard code </s> def is_finite(self):	is_finite
# todo?: if sha256 of the address is in deletedaddresses, </s> return profile.objects.get(api_token=api_token)	_get_user_profile def _get_user_profile(request, api_token): return request.user.profile_set.first()
# todo make comments clearer, see _viterbi_decode </s> last_tags = tags[seq_ends, torch.arange(batch_size)]	_compute_score score += self.transitions[tags[i - 1], tags[i]] * mask[i] score += emissions[i, torch.arange(batch_size), tags[i]] * mask[i] score += self.end_transitions[last_tags] return score
# todo: add docstring </s> 'seconds_remaining_to_warn' seconds, or less.	_log_warning_if_expires_soon def _log_warning_if_expires_soon(rolename, expires_iso8601_timestamp, seconds_remaining_to_warn): datetime_object = iso8601.parse_date(expires_iso8601_timestamp) expires_unix_timestamp = \
# todo extend to nonbinary nodes </s> n = d1.ndim	hamming_emd by state, one dimension per node). Singleton dimensions are sqeezed out. return emd(d1.ravel(), d2.ravel(), _hamming_matrix(N))
# todo(dcramer): ideally we could just send the signal to the subprocess </s> db.session.commit()	save_chunk ))
# todo: refactor this </s> if repo_filter in repo.full_name.lower() or \	repositories table = [] number = 0 repo_filter in repo.description.lower(): table.append([number,
# todo: deal with scroll position </s> def setupsidebargroup(self):	setupSidebarGroup group = Group((0, 0, 0, 0)) group.generalSettings = self.setupGeneralSettingsGroup()
# todo: find a better way to return errors... </s> def getpototals(self, pofilename):	getpototals
# todo: implement me </s> self.chatrooms.roomsctrl.userjoinedroom(msg)	UserJoinedRoom if self.chatrooms is not None:
# todo docstring </s> for unit in units:	discover_node_configuration d = self._gear_client.list() def applications_from_units(units): applications.append(Application(name=unit.name)) return applications
# todo i18n text entries </s> control.set_sensitive(false)	_write_async def _write_async(setting, value, sbox): failed.set_visible(False) spinner.set_visible(True)
# todo assert exit code != 0 </s> self.assertequal(commit.child("file.txt").getcontent(), "hello!")	test_commit_volume commit = volume.child("commits").child(commitId) self.assertTrue(commit.exists())
# @todo: test </s> wx.postevent(self.mainframe, ge.fitchanged(fitid=fitid))	FittingView fitID = self.getActiveFit() sFit = service.Fit.getInstance() event.Skip() def getActiveFit(self):
# todo(b/160795287): deprecate estimator based executor. </s> path_fn = serving_model_path	copy_model def copy_model(working_dir: Text, dest: Text, tag: Text) -> None: path_fn = None elif tag == 'eval': path_fn = eval_model_path
# todo legacy method to be removed/refactored </s> from corehq.apps.orgs.models import organization	get_organizations return filter(None, [Organization.get_by_name(org) for org in self.organizations])
# todo modification in place </s> wrapper for modifiers that work on both reads in a paired-end read	PairedModifier paired = True def __init__(self, modifier1, modifier2):
# todo(leofang): it seems as of rocm 3.5.0 hiprtc/hipcc can automatically </s> return os.environ.get('cupy_cache_dir', _default_cache_dir)	get_cache_dir
# todo reconsider with python 2 drop </s> if module_name in ('numpy', 'tensorflow', 'matplotlib', 'pandas'):	_complete_trailer if v.is_module(): if len(v.string_names) == 1: cached_name = module_name return cached_name, self._complete_trailer_for_values(values)
# todo: arrange </s> self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token)	createRepo def createRepo(self): repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "mirror_locally", "0", self.token) self.remote.save_repo(repo, self.token)
# check clock sync todo: alert on clockless origin server. </s> return meth(self, name, values)	GenericHeaderSyntax [[f.strip() for f in re.findall(r'((?:[^",]|%s)+)(?=%s|\s*$)' % (QUOTED_STRING, COMMA), v)] for v in values], [] return new
# todo ... </s> return arg in state.macros	cpreprocess_evaluate_ifdef if not is_valid_defname(arg): state.error("preprocessor: '" + arg + "' is not a valid macro name")
# todo: do something with loggers? </s> pass	PubsubNotifee@81 async def listen(self, network: INetwork, multiaddr: Multiaddr) -> None: pass
# todo: look this up in one query </s> return result.rowcount == 1	delete_playlist_by_mbid WHERE playlist.mbid = :playlist_mbid with ts.engine.connect() as connection:
# todo: figure out how to do this for multiple inputs </s> sqrt_ggn = einsum('bijc->bic', sqrt_ggn)	bias_diag_ggn num_classes)
# todo: fix appveyor - error comes up with bollu/vispy:cassowary-constaints </s> arrow.parent = none	test_arrow_draw connect="segments", parent=c.scene) assert_image_approved(c.render(), 'visuals/arrow_type_%s.png' %
# todo confirm we want floor division here </s> data, chosen = pmat(data, atype), pmat(chosen, atype)	mnl_estimate chosen = np.ones((numobs, numalts))  # used for latent classes data = np.transpose(data) if weights is not None: weights = PMAT(np.transpose(weights), atype)
# todo(hirofumi0810): remove this after supporting trasformer for the st task </s> dict[str, scorerinterface]: dict of `scorerinterface` objects	scorers def scorers(self): raise NotImplementedError("decoders method is not implemented")
# todo: expand to full set of info </s> print(task_status_table, 'table was created and had a task status update added')	DatabaseHandler task_status_table = create_task_status_table(info['task_id'], info['run_id'], self.meta) self.meta.create_all(self.eng) else: task_status_table = self.meta.tables[info['run_id'] + str(info['task_id'])]
#todo - is there a nice way to return an iterator and </s> self.assertequal(str(query_seq), str(alignment[0].seq).replace("-",""))	pairwise_alignment_check self.assertTrue(str(alignment[1].seq).replace("-","").upper() \ in str(target.seq).upper()) self.assertEqual(str(target.seq).upper(), \ str(alignment[1].seq).replace("-","").upper())
# :todo: implement test. </s> self.adapter = mockadapter()	SendTransferCommandTestCase class SendTransferCommandTestCase(TestCase): def setUp(self): def test_wireup(self): Verifies that the command is wired up correctly.
# todo 2.8 i want to change the way we handle unit scaling, see </s> if dg_obj_instance.is_instance:	make_key_from_instance key = make_key(dg_obj_instance.object.original) key += "_" + make_key(dg_obj_instance.parent.original)
# todo: refactor common tests for all models, e.g. shape checking </s> trans_h = transh(triples_factory=self.factory)	test_trans_h self.assertIsNotNone(trans_h)
#todo same issue with batch_size </s> else:	create_graph tf_graph.create_generator(graph)
#todo: check sequence id </s> return self.__packet_number	packet_number
# todo: add a .parse() method that includes boths steps? </s> def expecting_user_input(self, value):	expecting_user_input @expecting_user_input.setter
# todo(rakhmerov): actionspec should be used instead of dict. </s> (resp.status_code, resp.content))	HTTPAction except Exception as e: raise exc.ActionException("Failed to send HTTP request: %s" % e) if resp.status_code not in range(200, 307): raise exc.ActionException("Received error HTTP code: %s" %
# todo if standby ready, just swap and continue. </s> return self.apply(operator.sub, other)	__isub__
# @todo: split into smaller routines </s> return good	retry_interactive_command print(color_line('Command "{}" failed to execute.'.format( ' '.join(cmd_args)
#todo: we may want to deal with error nicely </s> response = httpresponse(mimetype='text/%s' % format_type)	export_rate get the prifix rates  from voip rate API according to search parameters & store into csv file response['Content-Disposition'] = 'attachment;filename=call_rate.%s' % format_type headers = ('prefix', 'destination', 'retail_rate')
# todo: optimization </s> while true:	BlockFilter async def _handle_new_blocks( self, send_channel: trio.abc.SendChannel[Eth1Block] for block in self._poll_new_blocks(): await send_channel.send(block)
# time.sleep(20)  # todo: should remove after polling get. </s> value_secret = data.send(clients[0])	test_secret_sharing def test_secret_sharing(get_clients) -> None: clients = get_clients(2) mpc_tensor = MPCTensor(secret=value_secret, shape=(2, 5), parties=clients) mpc_tensor.block_with_timeout(secs=10)
1/0  # conflict resolution todo </s> :param storage_b: the second storage	sync@28 def sync(storage_a, storage_b, status): :param status: {uid: (etag_a, etag_b)} list_a = dict(storage_a.list_items())
# :todo: implement test. </s> multisigaddress(	test_add_inputs_error_multiple This is not currently supported. with self.assertRaises(ValueError): trytes  = self.trytes_1, digests = [self.digest_1, self.digest_2],
# todo: dry things, should be one function for sa and mongo </s> '_id': objectid(),	create_document await mongo_collection.drop() for i in range(rows): 'title': 'mongo title {}'.format(i), 'category': 'category field {}'.format(i),
# todo security </s> return render('admin_core_schema.mako', request, {	admin_core_schema solr_schema = SolrApi(SOLR_URL.get()).schema(core) hue_core = Core.objects.get(name=core) 'solr_schema': solr_schema, 'hue_core': hue_core,
pass # todo(denero) re-enable when self.inst is actually valid. </s> self.assignment_name = 'assignment'	test_invalid_assignment_name
# todo assert cls.__tablename__ == '' </s> yield from dbcleanup_wrapper(session, s)	galaxy_session @pytest.fixture def galaxy_session(model, session, user):
# todo: temporarily use paddle.nonzero instead of paddle.max </s> is (n, c, d1, d2,..., dk), k >= 1.	smooth_l1_loss Parameters: input (Tensor): Input tensor, the data type is float32 or float64. Shape is label (Tensor): Label tensor, the data type is float32 or float64. The shape of label is the same as the shape of input.
# todo: for backward compatibility only, remove if not used anymore </s> if vms:	get_vm args = {} args['projectid'] = self.get_project(key='id') for v in vms['virtualmachine']: if vm in [ v['name'], v['displayname'], v['id'] ]:
# todo: smarter behavior </s> self.standarddetails.setdata(sub_data)	selectSubscription "User clicked on subscription. Has to be selected in detailPanel"
# todo(vek): need to pass context in for access to auth_token </s> self.name = name	InstanceInfo class InstanceInfo(object): assert state in power_state.valid_states(), "Bad state: %s" % state self.state = state
from _devbuild.gen.id_kind_asdl import id  # todo: fix circular dep </s> return self.msg % self.args	UserErrorString
# todo(sahid): direct call from domain should </s> stats, instance=instance)	get_all_volume_usage "Got volume usage stats for the volume=%(volume)s," " rd_req=%(rd_req)d, rd_bytes=%(rd_bytes)d, " vol_usage.append(stats) return vol_usage
# todo: calculate mu-sigma for f1, accuracy, and roc_auc and make it selectable </s> assert self.num_classes > 1	load_data_from_objects self.trainX = trainX self.trainY = trainY if self.judgment_metric is None: if self.num_classes == 2:
# todo: check u + v cubes for compatibility. </s> result = draw_method(*args, **kwargs)	_draw_2d_from_points draw_method = getattr(axes if axes else plt, draw_method_name) if arg_func is not None: else: result = draw_method(u, v, data, *args, **kwargs)
raise exception('lol') #todo fixme </s> def notify(self, name, old_value, new_value):	Settings self._config_obj.write() def add_change_listener(self, l): for l in self._listeners: l.setting_changed(name, old_value, new_value)
# todo: actual implementation </s> propagate_topic = parser._symbol_or_false()	make_logger except StopIteration: propagate_topic = [] return values.W_Logger(topic, parent, propagate_level, propagate_topic)
# todo: we may want to log this as soon as mobile ucr stops hitting this </s> def offset(self):	ChoiceQueryContext self.limit = limit self.page = page return self.page * self.limit
# todo: amortized flow parameters </s> if args.cuda:	CNFVAE super(CNFVAE, self).__init__(args) self.log_det_j = 0. self.cuda() def encode(self, x):
# todo(ihrachys): replace with network.create() once we get an object </s> _test_class = policy.qospolicy	QosPolicyBaseTestCase
# todo: set fileinfo to a valid object. </s> for directory, sub_directories, files in os.walk(targets_directory):	_get_list_of_target_paths rel_file_path = os.path.relpath(file_path, targets_directory) target_filepaths.append(rel_file_path) for _file in files: file_path = os.path.join(directory, _file)
# :todo: implement test. </s> ``transaction`` is not a trytescompatible value.	GetBundlesRequestFilterTestCase Request contains unexpected parameters. self.skipTest('Not implemented yet.') self.skipTest('Not implemented yet.') def test_fail_transaction_not_trytes(self):
# todo: this needs test coverage. </s> return h5py.file(self._filename, mode)	h5_file
# todo: assert len(args) == len(node.defn.type_vars) </s> sym.node.name(), t)	analyze_typevar_declaration tvars.append(tvar) else: return tvars return None
# todo: check arp reply is valid </s> self.logger = valve_util.get_logger('faucet', self.logfile, logging.debug, 0)	setup_valve self.faucet_event_sock = os.path.join(self.tmpdir, 'event.sock') self.logfile = os.path.join(self.tmpdir, 'faucet.log') self.registry = CollectorRegistry() self.metrics = faucet_metrics.FaucetMetrics(reg=self.registry) # pylint: disable=unexpected-keyword-arg
# todo check axises </s> yy = (y**y).sum()	distanz if rx != ry: raise("The sizes of x and y do not fit") xy = np.transpose(x)*y d = abs(sp.kron(sp.ones((1, cy)), xx) + sp.kron(sp.ones((cx, 1)), yy) - 2*xy )
# todo: try to find a better way to deal with local execution </s> def detail(worker: abstractworker, plan_tuple: tuple) -> "plan":	detail Args: worker: the worker doing the deserialization
# todo add tests when funcs are not set in lambda </s> rs2_copy = copy.deepcopy(rs2)	_same_rs def _same_rs(rs1, rs2): rnd1 = rs1_copy.randint(0, 10**6) rnd2 = rs2_copy.randint(0, 10**6)
# todo: get_style and word_to_glyphs may need proper implementations </s> return 0	y_offset
# todo: probably replace class_labels list with some custom object </s> be ordered in the same way as the class numbers were provided in the list.	get_labels_from_annotation@29 is derived from annotation tensor with sizes (width, height, 1) where value at each position represents a class. The functions requires a list with class Parameters ----------
# todo applicative log, database tracking of queue </s> continue	APSNotification if receiver_conf is None: print "Receiver", receiver_info['receiver_gus'], \ related_profile = yield profile_iface.get_single(receiver_conf['profile_gus']) settings_dict = { 'admin_settings' : related_profile['admin_settings'],
# todo: may test file contents </s> temp = tempfile.namedtemporaryfile(delete=false)	PluginCsvTestCase table = rows.import_from_csv(temp.name) self.assert_expected_table(table) self.files_to_delete.append(temp.name) rows.export_to_csv(utils.table, temp.file)
# todo: we can't disable this for normal, because folders don't </s> if not os.path.exists(os.path.join(location, self.vc_dir)):	check_repo_root raise ValueError return location
# todo: when this is changed to be a view, use loadselect2mixin instead of this </s> except integrityerror:	clone_problem problem.code = '%s%d' % (code, i) try: pass else:
#todo : include with the service cache. </s> query = u"select hostgroup_id from nagios_hostgroups where hostgroup_object_id='%s' and instance_id='%s'" % (hostgp_obj_id,instance_id)	get_hostgroup_id_by_id_sync self.db.execute_query(query) row = self.db.fetchone ()
# todo: get indexed value before transferring the variable (might be much smaller) </s> self._linear_solver = solver	linear_solver @linear_solver.setter def linear_solver(self, solver):
# todo: campid 939921818394902983238 </s> class mode(object):	Mode
# todo(ja): reclaiming space should be done lazy and low priority </s> volume_name = volume['name']	_do_iscsi_discovery def _do_iscsi_discovery(self, volume): (out, _err) = self._execute('sudo', 'iscsiadm', '-m', 'discovery', '-t', 'sendtargets', '-p', volume['host'])
# todo: remove in 1.3 </s> assert_array_almost_equal(kbd.bin_edges_[0], expected_bin_edges)	test_redundant_bins warning_message = "Consider decreasing the number of bins." with pytest.warns(UserWarning, match=warning_message):
# todo: refactor: this list of geotags is only used to determine if we need to show the geotag map or not </s> if request.user.is_authenticated():	username_reminder return HttpResponseRedirect(reverse("accounts-home")) if request.method == "POST":
"""todo doc me""" </s> >>> fields(table)	fields Return the header row for the given table. E.g.:: >>> from petl import fields ['foo', 'bar'] it = iter(table)
# todo(leofang): test newer rocm versions </s> a = testing.shaped_random(self.shape, cupy, dtype)	test_fftn_orders @testing.for_all_dtypes() def test_fftn_orders(self, dtype, enable_nd): if order == 'F': a = cupy.asfortranarray(a)
# todo maybe add the ability to disable this in settings ("identification" option, "basic" or "accurate") </s> if section is none:	get_directories @classmethod def get_directories(cls, key, cache_id=None): return [] return section.xpath('//Directory')
# todo: return errors in a universal way </s> source (str) - the c source code to compile.	compile_code return (str) - The asm output lexer = Lexer(token_kinds.symbol_kinds, token_kinds.keyword_kinds)
# todo: more error checking on the kernel to make sure it doesn't </s> for i in numba.parfor.internal_prange(len(a)):	_column_fillna_impl s = B[i] if hpat.hiframes_api.isna(B, i):
# todo: move it via south </s> "write your backwards migration here"	backwards def backwards(self, orm):
# todo results from ml </s> return endpoint.machine.name.strip()	_get_name @staticmethod
# :todo: implement test. </s> iota(self.adapter).sendtransfer,	SendTransferCommandTestCase self.assertIsInstance(
# todo: does this make sense? </s> root_nodes = hierarchy	__type_hierarchy_to_list types = [] for n in root_nodes:
# todo: issue #503 make pipx commands have proper exit codes </s> action="store_true",	add_pip_venv_args def add_pip_venv_args(parser): parser.add_argument( help="Give the virtual environment access to the system site-packages dir.", )
# todo: there must be a better way to do this. otherwise sometimes a file </s> return tf.reshape(tensor[:rows * columns], (1, rows, columns, 1))	reshape_tensor rows = tf.floor(element_count / columns) rows = tf.to_int32(rows)
# todo also check for motion codec parameter support </s> def is_motion_detected(camera_id):	is_motion_detected
# todo add tests </s> def xx_int(self):	xx_int @property
# todo: remove this method in v2.5 </s> resource.delete()	remove_from_device name=self.want.name, partition=self.want.partition
# todo: mock these so no network access is required </s> self.assertraises(importerror, lambda: get_repo("xyzzy"))	test_repo_registry self.assertEqual(l.url, "https://code.launchpad.net") self.assertTrue("github" in supported_repos())
# todo: add batch args </s> dataset = 'test'	run_treebank elif mode == Mode.SCORE_DEV: dataset = 'dev' if command_args.temp_output: with tempfile.TemporaryDirectory() as ete_dir:
# todo: implement me </s> self.logmessage("%s %s" % (msg.__class__, vars(msg)), 4)	UserJoinedRoom def UserJoinedRoom(self, msg): if self.chatrooms is not None:
# todo(#28) throw a configerror once it will be caught... </s> "linux":    "lin",	emulator_platform_prefix return { "Windows":  "Win", }[platform]
# todo: remove original subject and object from entity set </s> def split_list_in_batches(input_list, batch_size):	split_list_in_batches
# todo vary by supply point type? </s> if daily_rate is none:	monthly_consumption daily_rate = case.computed_.get('commtrack', {}).get('consumption_rate') if daily_rate is None: return None monthly_rate = daily_rate * 365.2425 / 12.
# todo - unittest this </s> return resolve_partition(path1).mountpoint == \	on_same_filesystem def on_same_filesystem(path1, path2):
# todo: we can't disable this for normal, because folders don't </s> state = self.state_1_map.get(state_string[0], none)	_lookup_tree_cache state_string, name = entry[:3], entry[4:].strip() if not re.match(self.valid_status_re, state_string): if state is None: state = self.state_2_map.get(state_string[1], _vc.STATE_NORMAL)
# todo: figure out the right thing to do here </s> a_1 = a_0 + sin_lat ** 2	haversine_grad sin_long = np.sin(0.5 * (x[1] - y[1])) cos_long = np.cos(0.5 * (x[1] - y[1])) d = 2.0 * np.arcsin(np.sqrt(min(max(abs(a_1), 0), 1))) denom = (np.sqrt(abs(a_1 - 1)) * np.sqrt(abs(a_1)))
# todo: preallocate! </s> def __gt__(self, other):	SortByX0 self.obj = obj def __lt__(self, other): return mycmp(self.obj, other.obj) > 0 def __eq__(self, other):
# todo(haoyuzhang): set config properly in tf2.0 when the config api is ready. </s> args:	parse_record_keras The input record is parsed into a label and image, and the image is passed through preprocessing steps (cropping, flipping, and so on). raw_record: scalar Tensor tf.string containing a serialized Example protocol buffer.
# todo implement w regex </s> return s	add_indent s = [(numSpaces * " ") + line for line in s] s = "\n".join(s)
while 1:  # todo: speed this up </s> if found_cases:	cycle_results_explicit def cycle_results_explicit(self, case=None, explicit=True): assert case is not False, case icase = self._set_case(case, self.icase, explicit=explicit, cycle=True) assert icase is not False, case
# todo, pass complete checkpoint as state dictionary </s> should_stop = stop == self.trainer.world_size	early_stopping_should_stop stop = torch.tensor(int(self.trainer.should_stop), device=pl_module.device) dist.all_reduce(stop, op=dist.reduce_op.SUM) return should_stop
# todo  the model predictions changed after update to posthoc sparsity. need to investigate. </s> loss4 = self.exp.compute_regularization_loss()	test_regularization_loss assert pytest.approx(loss4.data.detach().numpy(), abs=1e-4) == 0.2086 # regularization loss computed for given query instance and feature weights.
return none  # todo: mypy shouldn't require this </s> prompt_strategy=self.username_from_keyring_or_prompt,	username self.input.username, self.config,
# todo: log discarded bytes? </s> calculated_crc = crc16.crc16xmodem(''.join([chr(item) for item in message[:-2]]))	valid_crc supplied_crc = message[-2] * 256 + message[-1]
raise notimplementederror  # todo </s> :rtype: bool	have_more_data def have_more_data(self, session): :param tf.Session session: assert self.current_dataset_name return True
# todo don't use intermediate schematic... </s> if self.currentclone is not none:	mouseRelease self.doCloneOffsetCommand(self.dragStartClonePosition, self.clonePosition)
# todo: remove cache clearing once upstream issues regarding non-batch </s> acquisition_function.model.train()	gen_candidates@47 Returns: Tensor: The set of generated candidates acquisition_function.model.eval() options = options or {}
# todo, complete when the method becomes available, create proper response code </s> recording_uri = track["identifier"]	validate_playlist return for i, track in enumerate(jspf['track']): if not recording_uri: raise KeyError
# todo: add this to help output. </s> int: codereview issue number or none.	CreateIssue diffbase (str): diffbase. description (str): description. reviewers = list(self._REVIEWERS) reviewers_cc = list(self._REVIEWERS_CC)
# todo: non-int dict </s> typ = typemap[col_var.name]	aggregate_array_analysis col_shape = equiv_set.get_shape(aggregate_node.key_arr) all_shapes = [col_shape[0]] if typ == string_array_type: continue
# todo: skips header parsing </s> return model	read_abaqus def read_abaqus(abaqus_inp_filename, log=None, debug=False): model = Abaqus()
# todo(jfs): detect the case of a connection timeout </s> conf = global_conf.copy()	filter_factory conf.update(local_conf) def except_filter(app):
# todo fixme </s> cursor.close()	get_fee_fraction def get_fee_fraction (db, feed_address): cursor = db.cursor() if broadcasts: last_broadcast = broadcasts[-1]
uri=none,  # todo: </s> start_time=start_time.strftime(date_format)))	get_filename def get_filename(path_template, index, start_time): date_format = '%Y%m%d'
# todo: implement </s> return file_shown_in_view is not none and \	is_scanned_source def is_scanned_source(self, view): Utils.head_of(BackendManager.active_backend().module(file=file_shown_in_view)) is not None
# :todo: implement test. </s> def test_fail_unexpected_parameters(self):	FindTransactionsRequestFilterTestCase self.skipTest('Not implemented yet.') def test_fail_all_parameters_empty(self): self.skipTest('Not implemented yet.') def test_fail_bundles_wrong_type(self):
# todo: uncomment to enable claiming </s> 'user_id': user._primary_key if user else '',	get_globals return { 'user_name': user.username if user else '', 'user_url': user.url if user else '', 'user_api_url': user.api_url if user else '',
#todo: do something like </s> return 16 * player + self.base_color	get_pcolor_for_player if self.special_id == 1 or self.special_id == self.black_color: return 0 else: raise Exception("unknown special color")
# todo: support all tzinfo subclasses by calling utcoffset() </s> if self.offset < 0:	tzname def tzname(self, dt=None): sign = '-' return "%s%d:%d" % (sign, self.offset / 60, self.offset % 60)
except exception as exc:  # todo </s> uri = urlparse(rabbit_ctl_uri)	get_rabbit_manager host_port = '{0.hostname}:{0.port}'.format(uri) return Client2(host_port, uri.username, uri.password)
# todo - verify contents </s> self.assertequal(response.status_code, 301)	testReviewDetail0 def testReviewDetail0(self):
# todo replace w/ something smart (sun/earth special cased) </s> ) * u.deg	MarsFixed + 0.000031 * math.cos(K3.to("rad").value) + 0.000005 * math.cos(K4.to("rad").value) J1 = (129.071773 + 19140.0328244 * T) * u.deg J2 = (36.352167 + 38281.0473591 * T) * u.deg
# todo: log exception </s> with open(f, 'rb') as file_handle:	scan@45 if output is None: continue try: output = clamScanner.scan_stream(file_handle.read())
# todo: async </s> self.set_header('content-type', 'text/plain')	TornadioXHRPollingSocketHandler self.session.send('') @asynchronous data = self.get_argument('data') if not self.preflight():
# todo (pp): notifications don't have id; process all </s> v = sublime.active_window().active_view()	clear_ui def clear_ui(): v.erase_regions('dart.errors') v.erase_regions('dart.warnings')
# :todo: implement test. </s> account's total balance is not enough to cover spend amount.	PrepareTransfersCommandTestCase Preparing a bundle that finds inputs to use automatically. self.skipTest('Not implemented yet.') self.skipTest('Not implemented yet.') def test_pass_change_address_auto_generated(self):
pass # todo </s> def collect_incoming_data(self, data):	collect_incoming_data
# todo(mattjj, dougalm): make this work if we can, and delete subsequent test </s> assert len(effects) == 2	test_disable_jit with api.disable_jit(): f(2) f(2) f(2)
# todo: test more stuff! </s> cls.data = numpy.arange(3 * 192 * 192).astype(numpy.uint8).reshape((3, 1, 192, 192))	setup_class num_input_channels=1, num_output_channels=2 cls.tmp_dir = tempfile.mkdtemp() cls.h5_file = h5py.File(os.path.join(cls.tmp_dir, 'h5_file.h5'), mode='a')
# todo: factor this out, particularly if there are other places using </s> def boundary_distance(xy):	boundary_distance
pass # todo </s> def collect_incoming_data(self, data):	collect_incoming_data
if branch.commit == upstream.commit:  ### todo: a better check is possible </s> try:	_update_directory third, print an error. dir_type = "bookmark" if is_bookmark else "directory" repo = Repo(path) except exc.NoSuchPathError:
# todo(nnorwitz): store kind of inheritance...maybe. </s> fmt = '"%s"'	Include self.filename = filename self.system = system if self.system: fmt = '<%s>'
# todo: dns server is just a listener. we should allow the option of backwards communication. </s> dns += "\x01\x00"		# flags - standard query	build_dns res = host_to_resolve.split(".") dns = "" dns += "\x00\x01"		# Queries dns += "\x00\x00"		# Responses
'''todo: add docs''' </s> def discrete_columns(self):	discrete_columns return [x for x in self.columns if x['type'] == 'DiscreteColumn']
# todo(vek): need to pass context in for access to auth_token </s> assert state in power_state.valid_states(), "bad state: %s" % state	InstanceInfo class InstanceInfo(object): def __init__(self, name, state): self.state = state
# todo: config option? </s> across potential subdatasets, i.e. if a directory is provided, all files in	Get This command only operates on dataset content. To obtain a new independent dataset from some source use the `install` command. the directory are obtained. Recursion into subdatasets is supported too. If enabled, relevant subdatasets are detected and installed in order to
# todo: what about alpha (rgba)? </s> buffer = array.array(array_type, [0]) * (width * height * channel_count)	_import_aov try: buffer = self.aov_buffers[output_name] self.aov_buffers[output_name] = buffer if array_type == "I":
# todo: deprecate </s> @abstractmethod	reset_test_dataloader
# todo: separate this string to describe what each step is doing, then build it for the final command </s> print('  {}'.format(region))	display_all_regions def display_all_regions(self, command):
# todo (#2743, see also #2556): make a portable constant or remove completely </s> return retrievalresult(cfrags=new_cfrags)	with_result the one from `result` will be kept. new_cfrags = dict(self.cfrags)
# todo: add js compress </s> if self.res is none:	get_avatar_pair return ("", "") avt1 = self.res.get_avatar(self.parser.username)
# todo(dcramer): when we hit a notfound in the queue, maybe we should </s> try:	_get_response raise Exception('Invalid response. Status code was %s' % resp.status_code) data = resp.text return json.loads(data) except ValueError:
raise mpdnotimplemented # todo </s> result = self._find(type, what)	_findadd @register(r'^findadd "(?P<type>(album|artist|title))" "(?P<what>[^"]+)"$')
# todo: use pybossa uploader! only for debugging: </s> zipf.writestr(filename, memfile.getvalue())	make_onefile_memzip mode= zipfile.ZIP_STORED zipf = zipfile.ZipFile(memzip, 'w', mode) zipf.close() memzip.seek(0)
# todo -- can we do this without a subscription? </s> speech_text = render_template("play_library_text")	play_library stream_url = api.get_stream_url(first_song_id)
# todo: support multiple. </s> 'subscribed': subscribe,	edit_podcast_series update_mutations = mutate_call.build_podcast_updates([ { 'userPreferences': { 'subscribed': subscribe,
# todo: handle large files </s> return result	discover_hdfs_directory with sample(data, length=length) as fn: o = data.container(fn, **data.kwargs)
# todo pv 类型判断 </s> result = []	code_sort for k,v in data.items(): result.append(v['code'])
# todo resume admin log? </s> len(added), len(removed))	Downloader entity_downloader.extend_pending([p for p in participants if p.id in added or p.id in removed]) except ChatAdminRequiredError: __log__.info('Getting participants aborted (not admin).')
llen = len(' '.join(wordsplit[-2:])) #todo: what if 2 spaces between these words? </s> return "%s%s%s%s" % (ten[tens],	tenfn def tenfn(self, tens, units, mindex=0): '-' if tens and units else '', unit[units],
# todo: don't write them? is *much* slower on re-load (~3x) </s> if a module specifies __load__ we should only load/expose those modules	test__load__ self.update_module() with self.assertRaises(KeyError):
# todo: no testpath exercises this code... </s> def _mstime(self):	_msTime
annot.annotation_metadata.annotation_rules = "todo"  # todo </s> fill_section_annotation(lab_file, annot)	create_JAMS fill_global_metadata(jam, lab_file) logging.info("Parsing %s..." % lab_file) with open(out_file, "w") as f: json.dump(jam, f, indent=2)
# xxx todo </s> self["fileoutput"] = []	Configuration self["wait"] = 0 self['cookies'] = False self["text"] = { "filename": "linkchecker-out.txt",
#todo - find a better way to handle this </s> try:	_replaceExpressions remove = False for subscope in item.getScopes(): expr = item.expr except AttributeError:
# todo: test me. </s> if vi_cmd_data['_exit_mode'] == mode_insert:	eval_full_command self.view.run_command('vi_run', vi_cmd_data) self.reset() utils.blink() self.reset()
# :todo: implement test. </s> def test_pass_no_transactions(self):	GetTrytesResponseFilter self.skipTest('Not implemented yet.')
# todo: need to close computations on this node? </s> try:	scheduler_server sock = socket.socket(addrinfo.family, socket.SOCK_STREAM) sock = AsyncSocket(sock, keyfile=self.cluster_keyfile, certfile=self.cluster_certfile) sock.bind((addrinfo.ip, self.scheduler_port)) except Exception:
# todo: fails because of missing svg support </s> </thead>	test_image_repeat_block <table> <thead> <tbody> <tr><td></td></tr>
# todo: resolve possible conflicts with sysctl settings from other plugins </s> value = tuned.utils.commands.get_active_option(data, false)	_read_sysfs if len(data):
# todo unify </s> fh.write("$endnodes\n".encode("utf-8"))	_write_nodes )
# todo: log exception </s> print(module, " not a valid module...")	_start_module_threads@383 moddir = os.path.dirname(module) mod = load_module(os.path.basename(module.split('.')[0]), [moddir]) continue conf = None
# todo: remove: legacy function </s> verbose_name_plural = "buyers guide product categories"	Meta verbose_name = "Buyers Guide Product Category"
# todo: require tests </s> idx_top, idx_bottom = pad_top, out_x - pad_bottom	ZeroPad2dDerivatives result = mat.view(out_c, out_x, out_y, out_c, out_x, out_y) pad_left, pad_right, pad_top, pad_bottom = module.padding result = result[:, idx_top:idx_bottom, idx_left: idx_right, :, :, :].contiguous()
# todo: this is untested. </s> else:	_VerifyHelper if result: _lib.X509_STORE_CTX_set_error(store_ctx, _lib.X509_V_OK) return 0 self.callback = _ffi.callback(
# todo :: move arbitrary path construction to storagelayout object </s> return false	is_s3_response_error if not value.error_code == 'RequestTimeTooSkewed':
# todo: add support for more colors </s> cntr = 0	__repr__ else: box = axs.get_position() for name, legend in self.legend.iteritems(): if legend:
# todo: test for the _correct_ revision_id value. </s> assert activity.activity_type == 'new package', \	test_create_package assert activity.object_id == package_created['id'], \ str(activity.object_id) str(activity.activity_type) if not activity.id:
# todo: only handle it if the trigger refers to the current sensor </s> _, filename = os.path.split(self._sensor_file_path)	_get_sensor_instance def _get_sensor_instance(self): module_name, _ = os.path.splitext(filename) sensor_module = imp.load_source(module_name, self._sensor_file_path)
# todo: reconsider adding smth to data_ to be yielded" </s> cfg = safeconfigparserwithincludes()	_save_crawl_config makedirs(crawl_config_dir) crawl_config_repo_path = opj(CRAWLER_META_DIR, CRAWLER_META_CONFIG_FILENAME) cfg.add_section(CRAWLER_PIPELINE_SECTION) def secset(k, v):
# todo: put offset/data in main structure since immutable </s> class stringarraypayloadmodel(models.structmodel):	StringArrayPayloadModel def __init__(self, dmm, fe_type): members = [
# todo nice error recovery </s> if options.directory:	commandline_options self.file_from_cmd(options.filename)
# todo(b/142683826): beam type check error in </s> to.double_value.value = float(metric)	_copy_metric
# todo check if we can avoid that </s> out = numpy.fromfile( f, count=nquad, dtype=int, sep=" ")	read_buffer@69 out = numpy.fromfile( f, count=ntria , dtype=int, sep=" ") cell_data["triangle"] = {"ugrid:ref": out} cell_data["quad"] = {"ugrid:ref": out} if ntet > 0 :
# todo: disconnect </s> def listen_maddr_with_peer_id(self) -> multiaddr:	listen_maddr_with_peer_id return self.listen_maddr.encapsulate(Multiaddr(f"/p2p/{self.peer_id}"))
# todo: confirm necessity of this session clearing and lay out mechanics. </s> download_file,	test_download_url_to_tempfileobj_and_urls download_file, self.random_string(), self.target_data_length) 'http://localhost:' + str(self.PORT) + '/' + self.random_string(), self.target_data_length)
# todo: http://www.6502.buss.hk/6502-instruction-set/jmp says that there </s> def test_jsr_abs(self):	JsrTest tokens = lexical('JSR $1234') self.assertEquals(2, len(tokens))
# @todo: use real lightness from hsv or lab color model </s> gdk_color = gdk.rgba()	convert_theme_color_to_gdk gdk_color.parse("#" + theme_color) return gdk_color
# todo(ericbidelman) - memcache this calculation as part of models.py </s> feature_list = models.feature.get_chronological() # memcached	MainHandler order='-updated') return self.render_atom_feed('Features', feature_list) try: omaha_data = self.__get_omaha_data()
print "usage is not ready yet." # todo </s> print "usage is not ready yet." # todo	usage@6
# todo: remove cache clearing once upstream issues regarding non-batch </s> to generate from gen_function	q_batch_initialization@37 a Tensor when evaluated at a given q-batch q: The number of candidates in each q-batch torch_batches:  This factor determines how many q-batches in total will be returned by the function.  Must
# todo(developer): uncomment these lines and replace with your values. </s> import time	execute_workflow@29 def execute_workflow( project, location="us-central1", workflow="myFirstWorkflow" from google.cloud import workflows_v1beta from google.cloud.workflows import executions_v1beta
raise valueerror("bucket might not exist")  # todo: create custom exception for easier handling </s> raise e	S3Service if e.response['Error']['Code'] == "AccessDenied": read_acl_perm_allowed = False if self.aws_creds_configured: bucket.AllUsersReadACP = Permission.ALLOWED if read_acl_perm_allowed is True else Permission.DENIED
# todo should also include meta-chunk-hash </s> self._check_bad_headers(	test_bad_chunkhash 32, bad_headers={'x-oio-chunk-meta-chunk-hash': '0'}) self._check_bad_headers(
# todo: should we do check/modify it? wireshark shows the only msb to 0 </s> pass	Dot11WEPData self.set_long(0, nb) def get_keyid(self): def set_keyid(self): 'Set the \'WEP KEY ID\' field'
# todo: e_die with token </s> if 0:	EvalLHS print('EvalLHS()') node.PrettyPrint()
# todo fixme make sure we have exclusive write lock </s> r = tmap.get(cls, none)	_map_type float: sa.Float, datetime: IsoDateTime, if r is not None: return r
pass  # todo: implement this </s> if name in state['selected']:	set_state name = unicode(c.data(0, NAME_ROLE).toString())
# todo: use operators.translate() </s> ftrans = np.sqrt(g.n)*np.dot(g.u, (np.kron(np.ones((1, g.n)), ghat)*g.u.t))	_gwft_frame_matrix if G.N > 256: logger.warning("It will create a big matrix. You can use other methods.") F = utils.repmatline(Ftrans, 1, G.N)*np.kron(np.ones((G.N)), np.kron(np.ones((G.N)), 1./G.U[:, 0])) return F
raise notimplementederror  # todo... </s> def net_dict_post_proc(net_dict):	init_net def init_net(args, layers): :param args: return inject_retrieval_code(net_dict, rec_layer_name=args.rec_layer, layers=layers, dropout=args.dropout) rnn.engine.use_dynamic_train_flag = True  # will be set via Runner. maybe enabled if we want dropout
# todo: implement this method </s> callback(self, signum)	cb_wrapper
# todo: these reductions could be delayed until _step is called. </s> devices = [devices[0].name]	Trainer devices = tf.config.experimental.list_logical_devices(device_type="GPU") if not devices: self._checkpoint = checkpoint self._mixed_precision = mixed_precision
# todo(tobyboyd): remove when contrib.distribute is all in core. </s> return self._input_data	SyntheticDataset self._initializers.append(v.initializer) self._input_data = tf.nest.pack_sequence_as(tensor, variable_data) def initialize(self): if tf.executing_eagerly():
# todo: remove the following line when issue #71 (preserve the trajdataframe index during preprocessing operations) is solved. </s> return compressed_traj	_compress_trajectory compressed_traj = _compress_array(lat_lng_dtime_other, spatial_radius) compressed_traj = nparray_to_trajdataframe(compressed_traj, utils.get_columns(tdf), {})
# todo add installation logic for torch </s> if pkg.startswith('-e git+'):	DependencyPolicy def process(self, packages): result = [] result.append(pkg) elif '==' in pkg:
pass  # todo </s> @before_class	NoUpauthTests @test(groups=['server']) def login(self): self.api = test_utils.init(perform_upload_auth=False)
# todo/fixme: x and alpha should not contain observed values!! check that. </s> pass	cost_rotate_gmc
# todo ... </s> state.error("preprocessor: '" + arg + "' is not a valid macro name")	cpreprocess_evaluate_ifdef def cpreprocess_evaluate_ifdef(state, arg): arg = arg.strip() return False return arg in state.macros
raise notimplementederror #todo </s> pass #todo	Span
#todo: maybe it would be cleaner to have </s> def to_primitive(self, value):	PlayerIdType return unicode(value)
# todo: use a recursive memoized __eq__ if this ever shows up in profiles. </s> def level(self) -> loglevel:	FallibleBuiltGoPackage exit_code: int = 0 stdout: str | None = None return LogLevel.ERROR if self.exit_code != 0 else LogLevel.DEBUG def message(self) -> str:
# rfc822.header will give *all* of the headers, and is probably slower? // todo </s> log.info('selected folder %s with %d messages.' % (folder, select_info['exists']) )	select_folder def select_folder(folder): global server return select_info
# todo: read until ''' </s> def print(self, pnode):	Print
# :todo: implement test. </s> def test_pass_no_transactions(self):	GetTrytesResponseFilter skip_value_check = True def test_pass_transactions(self): self.skipTest('Not implemented yet.')
raise notimplementederror #todo </s> pass #todo	Span
# todo: make keys get passed through files or environment </s> h = sha512()	genkey def genkey(self, filename, magic): Generate keys for files based on host key and filename. h.update(":%s:%s:" % (filename, magic)) return h.hexdigest()
data.append(get_casedb_schema(app))  # todo use domain instead of app </s> if getattr(f, 'schedule', false) and f.schedule.enabled	form_designer scheduler_data_nodes.extend([ u"next_{}".format(f.schedule_form_id) ]) context = get_apps_base_context(request, domain, app)
# todo(lyarwood): remove the following in 16.0.0 pike </s> mock.call("use of the in tree encryptor class %(provider)s by "	VolumeEncryptorInitTestCase "directly referencing the implementation class will be " "blocked in the 16.0.0 Pike release of Nova.", "directly referencing the implementation class will be " "blocked in the 16.0.0 Pike release of Nova.",
# todo: enable custom config </s> logger.info('on_setup')	on_setup
# todo: instead of raising, we should do something (#957) </s> except character.invalidsignature:	orient payload message kit). try: raise self.InvalidSignature( "This TreasureMap does not contain the correct signature from Alice to Bob.")
# xxx todo </s> self["status"] = cfgparser.getboolean(section, "status")	Configuration except ConfigParser.Error, msg: linkcheck.log.debug(linkcheck.LOG_CHECK, msg) except ConfigParser.Error, msg: linkcheck.log.debug(linkcheck.LOG_CHECK, msg)
# todo: skips header parsing </s> model = abaqus()	read_abaqus model.read_abaqus_inp(abaqus_inp_filename) return model
# todo: seems like this just does everything that handle_exception does but not as well. </s> else:	FormRepeater url = super(FormRepeater, self).get_url(repeat_record) if not self.include_app_id_param: url_parts = list(urlparse.urlparse(url)) query = urlparse.parse_qsl(url_parts[4])
""" todo: better test here """ </s> @filter_params	test_filter_params def special_test_function(params, realm=None): return 'OAuth ' + ','.join(['='.join([k, v]) for k, v in params])
# todo: don't use getbrailletextforproperties directly. </s> try:	_get_name return self.UIAElement.currentName except COMError:
# todo(jd) add an exception in oslo.db to match foreign key </s> __tablename__ = 'resource_entity'	ResourceEntity resource_id = sqlalchemy.Column(GUID, sqlalchemy.ForeignKey('resource.id',
# todo: pass expect parameters from above? </s> ctrl_path += ":%s" % parsed_target.port	SSHManager raise ValueError("Malformed URL (missing host): %s" % url) ctrl_path = "%s/%s" % (self.socket_dir, parsed_target.netloc) if ctrl_path in self._connections: return self._connections[ctrl_path]
#todo: increase timeout based on number of plugins </s> if isinstance(_models[form._meta.model], dict):	prepare_modelforms continue if form.__module__ == modname and subclass: _models[form._meta.model][form.__name__] = form else:
# todo :: move arbitray path construction to storagelayout object </s> detail=standard_detail_message(	PartitionUploader socketmsg = value[1] if isinstance(value, tuple) else value logger.info( "The socket error's message is '{0}'." .format(socketmsg)))
# todo: redo as json </s> pasturls.add(url)	push_malware_url url = url.strip() if url not in pasturls: q.put(url) else:
# todo: only unescape \n \t and \\? </s> raise valueerror('must be a resolveable hostname or valid ip')	Hostname try: socket.getaddrinfo(value, None) return value
# todo: delete the orphaned flairtemplate row </s> def insert(self, ft_id, position=none):	FlairTemplateBySubredditIndex return '%s%08d' % (cls._key_prefix, position) def __iter__(self): A position value of None means to simply append. keys = self._index_keys()
# todo handle error </s> return self.values[valuename]	get_value
# todo: this should take a vector </s> self._me = self.mesh.getedgeinnerproduct()	Me def Me(self): Edge inner product matrix return self._Me
pass # todo: pass link problem upstream? </s> e(red.uri[m-1]),	format_droid cl, e(red.uri[:m-2]), e(red.uri[m]), )
pass # todo </s> def collect_incoming_data(self, data):	collect_incoming_data
# todo put this in a .extra w/a subselect </s> ctx = {	ContractHour method = 'https' if utils.get_setting('TIMEPIECE_EMAILS_USE_HTTPS')\ else 'http' 'deleted': True, 'new': False,
# todo: handle this </s> yy = row_number(y, horizontal=not vertical)	half_max_width if vertical else (0 <= y <= grid_height and 0 <= x < grid_width) _, (_, width, _) = border_list[yy][x] result = max(result, width)
deck.notes = note.get_notes_from_collection(collection, deck.anki_deck["id"])  # todo ugly </s> def update_db(self):	update_db
# todo multi-level import non-breakable </s> if n.start_pos[0] is not none and n.start_pos < position:	_get_defined_names_for_position return names names_new = [] names_new.append(n) return names_new
# todo protocol is incorrect (refer to article) </s> y_sh = 2 * a_sh	msb bob, alice, crypto_provider=crypto_provider ).child  # least -> greatest from left -> right r_sh = y_sh + x_sh r = r_sh.get()  # .send(bob, alice) #TODO: make this secure by exchanging shares remotely
# todo: if the procpool has been exhausted this will block. </s> if self._consumers:	kill if self.greenlet is not None and not self.greenlet.dead: self.should_stop = True for c in self._consumers: c.cancel()
return broadcast_best(samples_b, llik_b, llik_a)[0]  # todo </s> return np.asarray([])	linear_forgetting_weights assert N >= 0 assert LF > 0 elif N < LF: return np.ones(N)
@raises(sshexception) # todo: more granular </s> public_blob=publicblob.from_file(	test_certs_implicitly_loaded_alongside_key_filename_keys key_path = _support(os.path.join('cert_support', key_name)) self._test_connection( '{}-cert.pub'.format(key_path)
# todo: make it cleaner </s> ----------	FCN_32s@135 is fixed in this model definition, because it didn't give significant improvements according to aforementioned paper. image_batch_tensor : [batch_size, height, width, depth] Tensor Tensor specifying input image batch
# todo: add docstring </s> return is_hydrogen	_is_hydrogen is_hydrogen = True else:
# todo manage tangent? </s> kf.interpolation = 'linear'	set_interpolation @staticmethod def set_interpolation(interpolation, kf): elif interpolation == "STEP": kf.interpolation = 'CONSTANT'
# todo - del just my poll, not the entire list ! </s> else:	get_next_url * HTTP_REFERER if 'next' in request.POST and request.POST['next'].startswith('/'): return request.META.get('HTTP_REFERER', '/')
# todo(msb) remove this hack </s> filtered_locations: list[optional[location]] = [loc for loc, _ in itertools.groupby(locations)]	instantiate_partition if len(current_module) > 0: append_module(current_module) filtered_locations.append(None) for i in range(len(filtered_locations) - 1):
# todo: with only non-mandatory model attributes, it is not possible to get an invalid form </s> login = self.client.login(username='testuser_system_exporter_spreadsheet_csv_config', password='octjgndjzmafypl2fr43')	test_system_exporter_spreadsheet_csv_config_post_redirect@80 data_dict = { 'spread_csv_system_id': 'on',
# todo: small gains expected, when views are pre-calculated in main. </s> return f < f	should_continue
# todo for now, baked in assumption that the number of trials is the </s> all_scores = []	score_benchmark for env_id, scores in episode_scores.items(): all_scores += scores
# todo - use smarter weights (e.g. hamming window) </s> step : float, optional	SequenceLabelingAggregation normalize : boolean, optional Set to True to z-score normalize Sliding window duration and step (in seconds). Defaults to 3 seconds window with 750ms step.
#todo: check system is stable, perhaps a utility in ctrlutil.py </s> -------	hsvd ------ sys : a state space system H : a list of Hankel singular values Wc = gram(sys,'c')
pass # todo </s> element.text.split(none, 1)[0])	Proto def __init__(self, element): self.name = element.find('name').text
raise notimplementederror # todo </s> for s in self.states_list:	resample_states s.resample()
# todo: use actual html decode </s> url = 'http://torcache.net/torrent/{:x}.torrent'	get_torrent req = request.Request(url.format(info_hash), headers=pirate.data.default_headers)
# todo: remove parameters dictionary look up when we can confirm each trigger dictionary </s> trigger_instance.status = status	update_trigger_instance_status return TriggerInstance.add_or_update(trigger_instance)
# todo: se a data já estiver no cases mas for só total e essa </s> deaths = f"mortes_{day}_{month}"	row_with_sorted_columns for date_str in sorted(row_dates, reverse=True): year, month, day = date_str.split("-") new[confirmed] = row[confirmed] new[deaths] = row[deaths]
# todo: is this safe? </s> line = '\niface %s inet manual\n' % intf	NAT self.cmd( 'sysctl net.ipv4.ip_forward=1' ) intf = self.localIntf config = open( cfile ).read() if ( line ) not in config:
# todo this assertion can occasionally fail </s> assert not g	test_send_rpc args={'foo': 'bar', }, timeout=3)
# todo: re-implement </s> engine = create_engine('sqlite:///db-%s.sqlite' % self.configname, echo=self.options.debug_sql)	init_sqlalchemy old.close() import shutil Session.configure(bind=engine) if self.options.reset:
# todo: check that the performance measure is within some range </s> def test_merge(self):	TestBaselines def test_grid1(self): Tests flow/benchmark/baselines/grid1.py Tests flow/benchmark/baselines/merge{0,1,2}.py merge_baseline(num_runs=1, sumo_binary="sumo")
# todo: implement auto-dtype method in general parameters </s> examples	add_elements include_pre_edges : bool If True, the ionization edges with an onset below the lower -------- >>> s = signals.EELSSpectrum({'data' : np.arange(1024)})
# todo: implement retrying. api requests 5 seconds between retries. </s> def register_plugin():	register_plugin @event('plugin.register')
# todo: we lose the response code, so we can't check this </s> "403 forbidden",	test_GET_FILEURL_localfile_nonlocal d = self.GET("/vdrive/global/foo/bar.txt?t=download&localfile=%s" % localfile) "localfile= or localdir= requires a local connection") def _check(res):
# todo(b/163904067): mimic defun' behavior and reset the step seed to </s> raise valueerror('unexpected axis: %d (rank = %d)' % (axis, rank))	CumSum return tf.cumsum(x, axis=axis, exclusive=exclusive) if axis < -1: axis += rank length = GetShape(x)[axis]
# todo: safe labels </s> if not cell:	condition_for_cell return None condition = and_(*self.conditions_for_cuts(self, cell.cuts))
# todo factor out attribute name usage in cell so this restriction is moot for non-settable cells </s> del not_yet_set_state[key]	unserialize_exported_state this_kwargs = getter.state_to_kwargs(value) if this_kwargs is None: continue obj = ctor(**all_kwargs) if len(not_yet_set_state) > 0:
# todo(mriedem): remove this stub once we're only using fakelibvirt. </s> mock_find_secret.return_value = secret	test_delete_secret @mock.patch('nova.virt.libvirt.host.Host.find_secret') def test_delete_secret(self, mock_find_secret): expected = [mock.call('rbd', 'rbdvol'), mock.call().undefine()]
assert oldsize == self.size, '%s: %d, %d' % (self.type, oldsize, self.size) # todo: remove </s> return struct.unpack('>i', stream.read(4))[0]	read_uint
#todo raise error or others </s> column.pack_start(renderer, false)	KeyDirView def __add_columns(self): column = gtk.TreeViewColumn(_('KeyDir')) column.set_attributes(renderer, pixbuf=self.COLUMN_ICON) renderer = gtk.CellRendererText()
# todo: move update_spec to worker. agent should not hold these execution details. </s> [1] human-level control through deep reinforcement learning. mnih, kavukcuoglu, silver et al. - 2015	DQNAgent class DQNAgent(Agent): [2] Deep Reinforcement Learning with Double Q-learning. v. Hasselt, Guez, Silver - 2015 [3] Dueling Network Architectures for Deep Reinforcement Learning, Wang et al. - 2016
# if the test has the todo flag set, then our failures and errors are </s> else:	getRunningTime def getRunningTime(self): if hasattr(self, 'stopTime'): return time.time() - self.startTime
# todo: rename intervalstodiatonic </s> >>> getwrittenhighernote(anote, bnote)	getWrittenHigherNote >>> from music21 import note >>> aNote = note.Note('c#3') <music21.note.Note D-> >>> aNote = note.Note('c#3')
#todo: this should never happen, dep on equiv </s> j_dic['equivs'] = []	enrich_json_with_base j_dic['events'] = [] j_dic['triggers'] = [] j_dic['infos'] = []
elif isinstance(other, containeroperand): # todo 0.7: use iterable to array force user to provide values </s> offset_apply = not offset is none	LocMap partial_selection: if True and key is an iterable of labels that includes lables not in the mapping, available matches will be returned rather than raising. Returns: if isinstance(key, slice): if offset_apply and key == NULL_SLICE:
# todo: let the globe return the semimajor axis always. </s> def boundary_distance(xy):	boundary_distance
# todo{jihongju} the mask branch </s> y = keras_resnet.block.temporal.bottleneck(512, (1, 1), first=true)(y)	residual channel_axis = 1 y = keras.layers.TimeDistributed(keras.layers.Conv2D(1024, (1, 1)))(x) y = keras.layers.TimeDistributed(keras.layers.BatchNormalization(axis=channel_axis))(y) y = keras.layers.TimeDistributed(keras.layers.Activation("relu"))(y)
# todo confirm we want floor division here </s> data : 2d array	mnl_estimate Calculate coefficients of the MNL model. Parameters The data are expected to be in "long" form where each row is for one alternative. Alternatives are in groups of `numalts` rows per
n)  # todo: access alice's private key inside this method. </s> def packed_payload(self):	packed_payload
## todo check exception type </s> args[1] = 'num'	_visit_call_helper_instanceof args = map(self.visit, node.args) if len(args) == 2: return '%s is %s' %tuple(args) else:
# todo log here </s> links = data.get('_links')	auth_okta@109 else: return False if not links: return False
# todo: confirm necessity of this session clearing and lay out mechanics. </s> download_file,	test_download_url_to_tempfileobj_and_urls self.assertRaises(securesystemslib.exceptions.FormatError, download_file, None, self.target_data_length) self.random_string(), self.target_data_length) self.assertRaises(requests.exceptions.HTTPError,
# todo maybe have the api level loading in the __init__ method and pass the apk as well? </s> return self.exception_analysis	get_exception_analysis
# todo: replace the blockerdb with a depgraph of installed packages </s> def _deallocate_config(self, settings):	_deallocate_config
# todo: speedup by allocating the denominator directly instead of constructing it by sum </s> masked = tf.exp(-abs_dif) * mask	get_minibatch_features big = tf.cast(big,dtype=dtype) abs_dif = tf.reduce_sum(tf.abs(tf.expand_dims(activation,3) - tf.expand_dims(tf.transpose(activation, [1, 2, 0]), 0)), 2) def half(tens, second): m, n, _ = tens.get_shape()
# todo: support iat/iloc differences </s> def resolve_argsort(self, ary, args, kws):	resolve_argsort resolver = ArrayAttribute.resolve_argsort.__wrapped__ sig = resolver(self, ary, args, kws)
# todo: change logic to c_leq based on benchmarking </s> else:	handle_NLP_subproblem_other_termination var_values = list(v.value for v in fix_nlp.MindtPy_utils.variable_list) if config.add_integer_cuts: raise ValueError( 'MindtPy unable to handle NLP subproblem termination '
# todo: make locking works for mssql </s> session_args_idx = find_session_idx(func)	provide_session If you want to reuse a session or run the function as part of a database transaction, you pass it to the function, if not this wrapper @wraps(func) def wrapper(*args, **kwargs) -> RT:
# todo log here </s> return false	auth_okta@155 }, ) if response.status_code != 200: return False
# todo: don't write them? is *much* slower on re-load (~3x) </s> self.update_module()	test__load__ def test__load__(self): with self.assertRaises(KeyError): self.loader[self.module_key + '2']
# todo: neil-san, you should keep this </s> gm.get_op_type_matches(p, matches)	pass_quantize_convolutions gm = GraphMatcher(graph) matches = list() for m in matches: conv_node = m.node
# todo(developer): uncomment and set to a path to your audio file. </s> print('first alternative of result {}: {}'	transcribe_file_with_diarization@147 for i, result in enumerate(response.results): alternative = result.alternatives[0] .format(i, alternative.transcript)) print('Speaker Tag for the first word: {}'
# todo: +kwargs </s> raise	get_last_commit_hash except CommandError as e: if 'does not have any commits' in e.stderr:
# todo: convert [n, c, v] to  new convention [v, n, c] </s> def sqrt_hessian_sampled(self, module, g_inp, g_out):	sqrt_hessian_sampled M = self.MC_SAMPLES C = module.input0.shape[1]
# todo check how we define frames regarding action repeat. </s> while executed < num_timesteps:	SingleThreadedWorker episode_durations = [] episode_steps = [] episode_reward = 0 episode_step = 0
# todo(b/141131288): enable complex-valued sorts on tpu. </s> [(7,), (4,), (7,), none],	testSliceGrad "limits": limit_indices, "strides": strides, "rng_factory": rng_factory} for shape, start_indices, limit_indices, strides in [ [(5,), (1,), (5,), (2,)], [(8,), (1,), (6,), (2,)],
# todo: this may return a shortname or a longname, with no way </s> ('pointer', lpvoid),	OVERLAPPED ('OffsetHigh', ctypes.wintypes.DWORD),
# todo this paragraph is necessary, but not sure it works. </s> if names is not none:	Completion names, level, only_modules, unfinished_dotted = \ helpers.check_error_statements(module, self._pos) imp_names = tuple(str(n) for n in names if n.end_pos < self._pos) i = imports.Importer(self._evaluator, imp_names, module, level)
# todo hardwired magic numbers! bad api smell. </s> self.assertequal(m[k], v)	assert_map_key_val def assert_map_key_val(self, m, k, v): except KeyError: self.assertTrue(False, list(m.items()))
# todo: endianness support </s> begin=variable, end=variable))	set_watchpoint begin=variable, end=variable)) if read is True: self._hooks.append(hooks) return len(self._hooks) - 1
# todo(mattjj,levskaya): re-enable when test failure is sorted out </s> args_maker = lambda: [rng(shape, dtype)]	testCountNonzero rng = jtu.rand_some_zero() onp_fun = lambda x: onp.count_nonzero(x, axis) self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True) self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
val = '"%s"' % val  # todo: handle correctly escaping input strings. </s> if not parameters:	_parameterize_notebook return param_content = _python27_param_content(parameters)
# todo: change retrieval mode </s> @contestrequired	MessageHandler def post(self, user_name): BusinessLayer.update_users(self.c)
# todo lib </s> return self._items_renamed	items_renamed @property
# todo: is this really ok? </s> raise w3afexception('plugin "'+self.getname()+'" is not implementing required method getoptions' )	getOptions def getOptions(self):
# todo: implement non-zero js </s> multiply the matrix \\\(\\\hat{a}\\\) by a fields vector where	ProblemTDEM_b :param simpegEM.TDEM.FieldsTDEM vec: Fields object :rtype: simpegEM.TDEM.FieldsTDEM .. math:: \mathbf{\hat{A}} = \left[
# :todo: implement test. </s> def test_wireup(self):	GetTransactionsToApproveTestCase def setUp(self): super(GetTransactionsToApproveTestCase, self).setUp() Verify that the command is wired up correctly. self.assertIsInstance(
# todo stop guessing </s> lint, app_dir, lint_result, ignore_layouts = parse_args()	run_lint_command def run_lint_command(): if not lint_result: lint_result = os.path.join(app_dir, 'lint-result.xml')
# todo(stephenfin): use a helper </s> target_host = 'host2' if source_host == 'host1' else 'host1'	_create_server server = super()._create_server(networks='none') self.addCleanup(self._delete_server, server) return server, source_host, target_host
encoding = "utf-8"  # todo: receive as a parameter </s> except programmingerror:  # does not exist	ImportDataCommand with transaction.atomic(): try: pass finally:
# todo: if the type for struct-type is not controlled by the current inspector, the exn:fail:contract exception should be raised </s> def do(a):	make_pred_eq @expose(name, [values.W_Object], simple=True)
# todo use self.rester when in prod </s> "mp-540081"), "mp-19017")	test_get_materials_id_from_task_id def test_get_materials_id_from_task_id(self):
"""todo doc me""" </s> it = iter(table)	fields >>> table = [['foo', 'bar'], ['a', 1], ['b', 2]] >>> fields(table) return it.next()
# todo message </s> 'availableelements': 'none',	RegisterImport 'FunctionChannels': '{"FileNames"}', 'Sources': 'None', 'Options': '{}', 'OriginalChannel': 'False',
# todo: check that ambient makes sense </s> sage: realset(-oo, oo).closure()	closure def closure(self): Return the topological closure of ``self``. (-oo, +oo) sage: RealSet((1, 2), (2, 3)).closure()
pass  # todo... </s> return [tuple(beam_width if i == self.idx_dim else array[i] for i in range(len(array)))]	infer_shape def infer_shape(self, node, input_shapes):
# todo: add type and value checkings </s> flow = torch.matmul(self._i_hs_ref, point)	DepthWarper self._i_Hs_ref = homography_i_H_ref(pinhole_i, pinhole_ref) def _compute_projection(self, x, y, invd): z = 1. / flow[:, :, 2] x = (flow[:, :, 0] * z)
# todo: replace column names and remove the appliance name from </s> mains_column_names = [x for x in df.columns if "mains" in x]	add_mains building.utility.electric.mains = df[mains_column_names] return building
# todo test </s> return hash((frozenset(self.nodes), self.current_state.tostring(),	__hash__ def __hash__(self):
# :todo: implement test. </s> def test_pass_no_transactions(self):	GetTrytesResponseFilter skip_value_check = True def test_pass_transactions(self): self.skipTest('Not implemented yet.')
if not real: self.assertequal(exita, 1) # todo: real = 0 </s> if not m:	test_2027_show_unit_for_oneshot_service self.assertGreater(len(lines(out)), num_lines) for line in lines(out): self.assertEqual("word=value", line) self.rm_testdir()
"""todo doc me""" </s> return it.next()	fields >>> fields(table) ['foo', 'bar']
"""todo: not implemented""" </s> return x.shape[0]	n if isinstance(x, pd.DataFrame):
maxtime = 10445238000  # todo: change after 31 dec 2300 lol </s> def file_exists(self):	file_exists @property
# todo this swallows status message, yield properly </s> commit,	Save ds = Dataset(res['path']) commit = ds.repo.get_hexsha() ds))
# todo: remove </s> return lookup_group_plugin(group_type).form_to_db_schema()	_form_to_db_schema
#todo: remove convert_bare true and deprecate this in with_ </s> conn_type = "paramiko"	_get_connection cmd = subprocess.Popen(['ssh','-o','ControlPersist'], stdout=subprocess.PIPE, stderr=subprocess.PIPE) (out, err) = cmd.communicate() except OSError: conn_type = "paramiko"
# todo: assuming the register is written in its number format </s> self._translate_branch(tb, instruction, link=false)	_translate_bge
# todo; handle multiple doc-types </s> auto_repopulate_tables=false,	get_kafka_ucr_pillow return ConfigurableReportKafkaPillow( processor=ConfigurableReportPillowProcessor( ucr_division=ucr_division, include_ucrs=include_ucrs,
# todo manage tangent? </s> if interpolation == "linear":	set_interpolation @staticmethod kf.interpolation = 'LINEAR' elif interpolation == "STEP":
#todo change to native framework call, when plex allows token in header </s> @classmethod	playlistsV3 @classmethod def init(self): def LIST(self, req, *args): try:
# todo: fix </s> return self.alloc((d0,), dtype=dtype)	alloc_i1d
# todo: write me </s> key = tile_key(layer, coord, format, self.revision)	unlock def unlock(self, layer, coord, format): mem.delete(key+'-lock') mem.disconnect_all()
# todo: this might be a rather nasty hack to fix the circular dependency </s> return datetime_index	_tz_to_naive datetimeindicies with a daylight saving transition in them. See: http://stackoverflow.com/q/16628819/732596 timestamp = datetime_index[0] tz_offset = (timestamp.replace(tzinfo=None) -
# todo: mock these so no network access is required </s> g = get_repo("github")	test_repo_registry def test_repo_registry(self): self.assertEqual(g.title, "Github") self.assertEqual(g.url, "https://github.com")
# todo(hub-cap): turn this into middleware </s> path = "/{tenant_id}/instances"	API self._instance_router(mapper) def _instance_router(self, mapper): mapper.resource("instance", path, controller=instance_resource)
# todo: only standard themes supported right now </s> def switch_to_dark():	switch_to_dark
# todo: add some kind of 'failed to open' notification </s> pass	set_labels
# steps = 0 # todo </s> def __init__(self, exported_obj, matrix):	Duplis self.exported_obj = exported_obj self.matrices = matrix
# todo: append to current tree </s> return var	create_variable else: var = vtype()
# todo: direct use of json['error'] is deprecated; use display_message('...', 'error') instead. </s> j_dic['triggers'] = []	enrich_json_with_base j_dic['offset'] = 0 j_dic['entities'] = [] j_dic['modifications'] = [] j_dic['equivs'] = []
# todo: multithread optimization, one thread per ps communication. </s> results[index].append(name)	hash_partition results = [[] for i in range(ps_size)] for name in v: else: raise TypeError('Illegal v type %s, only dict or '
# todo(kevinbenton): bulk? </s> @expose(generic=true)	RootController def _lookup(self, version, *remainder): if version == 'v2.0': def index(self): return dict(message='A neutron server')
# todo: accept these via quirks? </s> blocking=true, data_packet_pid=0):	send_on_endpoint Sends a block of data on the provided endpoints. endpoint_number -- The endpoint number on which to send.
# todo: optimize db call </s> return summary.jobs, summary.warnings	summarize supplied history - needed for building workflow from a history. Formerly call get_job_dict in workflow web controller.
# xxx todo: real error handling, as this is probably going to </s> /etc/modprobe.d/anaconda-blacklist.conf so that modules will	write_module_blacklist def write_module_blacklist(): continue to be blacklisted when the system boots. if "modprobe.blacklist" not in flags.cmdline:
# todo: ... </s> return os.path.join(conf['dump_path'], os.path.basename(file_name))	get_dump_path
).consume()  # todo see issue 170 </s> return {'loadbalancerdescriptions': elbs}	get_loadbalancer_data elbs = [] for page in paginator.paginate():
# todo: empty env? </s> workdir='workdir',	TestP4 self.patch_getCommand('p4', 'path/to/p4') self.clean_environ() mode='copy', revision=None,
return none # todo: probably need a universal failure code </s> return the the disassembler platform this script is executing in.	get_disassembler_platform def get_disassembler_platform():
# todo: test pdb files with dna and rna too: </s> record id 1a8o:a, chain a	CifAtomIterator ...     for record in CifAtomIterator(handle): ...         print("Record id %s, chain %s" % (record.id, record.annotations["chain"])) from Bio.PDB.MMCIFParser import MMCIFParser structure = MMCIFParser().get_structure(None, source)
# wait until the chunks have added, todo change this to a qtbot.waitsignal </s> qtbot.wait(short_loading_delay)	test_tiled_changing_contrast_limits@111 ) visual = viewer.window.qt_viewer.layer_to_visual[layer] screenshot = viewer.screenshot(canvas_only=True) center_coord = np.round(np.array(screenshot.shape[:2]) / 2).astype(np.int)
# todo: change nonce </s> return true	handle_timeout_sustain
selection = page # todo api selection objects </s> else:	load_plugin if isinstance(plugin, basestring): import zim.plugins klass = plugin plugin = klass(self)
# todo (shea): extract method(s) to get_source_processor() </s> if (not os.path.exists( robo_join(prefs["recipecreatelocation"],	generate_jss_recipe keys["Input"]["POLICY_CATEGORY"] = "Testing" keys["Input"]["POLICY_TEMPLATE"] = "PolicyTemplate.xml" "%s.png" % facts["app_name"]))): facts["reminders"].append(
raise exceptions.mpdnotimplemented  # todo </s> unsubscribe from a channel.	unsubscribe@31 ``unsubscribe {NAME}``
# todo: check the data! </s> self.asserttrue(i == {u'familynumofjourneys': u'0', u'member': u'lancaster', u'mpothereuropean': u'0',	test_csv count = 0 for i in pipe: u'FamilyTotal': u'0', u'OfficeRunningCosts': u'19848', u'MPOtherRail': u'233', u'CostofStayingAwayFromMainHome': u'22541', u'StationeryAssocdPostageCosts': u'3471',
# @todo: deployment_setting </s> )	newsfeed (T("Description"), "body"), ], s3.dl_pagelength = 6  # 5 forces an AJAX call def prep(r):
# todo(lbragstad): currently, fernet tokens don't support bind in the </s> token=token_dict,	validate_v3_token trust=trust_ref,
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> self.fail("not implemented")	Test_AcquireTokenWithUsernamePassword context = AuthenticationContext(authorityUrl, cache) def callback(err, tokenResponse): context.acquire_token_with_username_password(resource, sampleParameters['username'], sampleParameters['password'], sampleParameters['clientId'], callback)
# todo: replicate complete behaviour of urllib.urlopener.open </s> tuf_configuration.hostname	map global _tuf_configurations assert isinstance( tuf_configuration, TUFConfiguration ) ] = tuf_configuration
return runtime.strarray(strs)  # todo: reuse this object too? </s> def push(self, entry):	Push
# todo: aggregate serializable errors into errors dict </s> func = func.__get__(object).__func__	prepare_validator def prepare_validator(func, argcount): if len(inspect.getargspec(func).args) < argcount: @functools.wraps(func)
# todo; to remove after full rollout of https://github.com/dimagi/commcare-hq/pull/21329/ </s> update_unknown_user_from_form_if_necessary(self._es, change.get_document())	UnknownUsersProcessor def __init__(self): self._es = get_es_new()
#todo: make the comparaison without transfert. </s> def __hash__(self):	__hash__
if lang is none:  # todo: remove in v8 </s> path[-1] += extension	_add_extension return path
async_pub['jid'],  # todo: fix </s> is passed in as kwargs, will re-use the jid passed in	functions_dict It creates a wrapper around the function allowing **kwargs, and if pub_data
# todo: implement proper mutexes instead of these stubs </s> return time_ns()	ql_syscall_clock_cycles
# todo: dont hardcode </s> for info in self.in_info + self.out_info:	TorchWrapperOp self.lua_file = lua_file self.lua_fw_func = lua_fw_func assert "ndim" in info assert "shape" in info
#todo: make more general (if possible) </s> measurement_type_tag,	get_ndarray_config ndarray_type_tag, ndarray_type_id, measurement_metatype_id, measurement_type_id,
# todo: i don't think these should be pushed back on. </s> else:	byte_END_FINALLY def byte_END_FINALLY(self): if self._lastException[0]: return 1
# todo: does this whole section disappear with proper headers from requesthandler? </s> config = parseconfigfile(config)	requestLayer if key in _previous_configs: config = _previous_configs[key] if key: _previous_configs[key] = config
# todo: remove patch and update test once calculation_magic is implemented </s> _update_lpq_eligibility(project_id=17, cutoff=10)	test_not_eligible_not_lpq def test_not_eligible_not_lpq(self, store) -> None:
# todo generator </s> check=true,	Drop dataset=None, recursive=False, if_dirty='save-before'): if not dataset and not path:
for people in annos:  # todo : speed up with affine transform </s> if thread_count is none:	threading_data threads.append(t) for t in threads: try: return np.asarray(results)
# todo(b/141131288): enable test once complex sort is supported on tpu. </s> "return_index": return_index, "return_inverse": return_inverse,	testUnique jtu.format_shape_dtype_string(shape, dtype), return_index, return_inverse, return_counts), "return_counts": return_counts} for dtype in default_dtypes
# todo: different codec to be used </s> or not all([isinstance(v, str_types + (nonetype,)) for v in val]):	_safechk_val if not isinstance(val, tuple) or len(val) != 8 \
@persistent # todo: not sure if i should be using @persistent </s> rd = context.scene.render	draw_header self.layout.prop(context.scene.my_addon, "save_gt_data", text="")
raise notimplementederror #todo </s> class span(object):	Span
# @todo: return only packages for the current architecture </s> return https_client_task(loop, self.host, self.uri)	get_task def get_task(self, loop):
assert study_id == 0  # todo </s> for t in self.trials	collect_param_result_pairs (t.params[param_name], t.result)
# todo: check for expected warnings. </s> m = buildmaster(self.basedir, self.configfile)	_run_master def _run_master(self, config_str): with open(self.configfile, "w") as f: m.config = config.MasterConfig.loadConfig( self.basedir, self.configfile)
# todo: make pull request to get this custom vgg feature accepted </s> upsampled_logits = tf.nn.conv2d_transpose(logits, upsample_filter_tensor,	FCN_32s@33 downsampled_logits_shape[2] * upsample_factor, downsampled_logits_shape[3] output_shape=upsampled_logits_shape, strides=[1, upsample_factor, upsample_factor, 1])
# todo: can the instance lock be downgraded here? take the optional disk </s> if name in os_defs and os_defs[name] == self.op.osparams[name]:	_RevertToDefaults del nic[name] os_defs = cluster.SimpleFillOS(self.op.os_type, {}) del self.op.osparams[name]
annot.annotation_metadata.annotation_rules = "todo" #todo </s> jam = jams.jams()	create_JAMS if not os.path.exists(path): logging.warning("Path not found %s", path) fill_global_metadata(jam, metadata) for possible_annot in xrange(3):
# todo test </s> return hash((frozenset(self.nodes), self.current_state.tostring(),	__hash__ def __hash__(self):
# todo: does this need to be made more efficient? </s> f.write('\n')	Print self.Print(entry, f, indent=indent+1) elif isinstance(tu, int): elif isinstance(tu, str): f.write(str(tu))
# todo: remove this when hftransformersnlp is removed for good </s> weights = component_meta.get("model_weights") or {}	cache_key def cache_key( cls, component_meta: Dict[Text, Any], model_metadata: Metadata return f"{cls.name}-{component_meta.get('model_name')}-{get_dict_hash(weights)}"
#[todo]: softmax is not supported yet </s> args[2 + l], args[2 + self.num_layers + l])	loss_and_derivative for l in xrange(self.num_layers): if l < (self.num_layers - 1): else: last_layer_output, _ = affine_forward(last_layer_output,
# todo: avoid dummy and generate func here when inlining is possible </s> def df_len_overload(df):	df_len_overload if len(df.columns) == 0:  # empty df return lambda df: 0
# todo: external summary for enum values </s> return publish_rst(state, source, translator_class=_saneinlinehtmltranslator).writer.parts.get('body').rstrip()	render_inline_rst
# todo: if this is the initial load of logging config we might not have </s> return (section, key, value)	parse_override def parse_override(override): section, remainder = override.split('/', 1)
# todo(b/179510447): align these parameters with schulman 17. </s> ndims = action_tensor_spec.shape.num_elements()	create_dist return tfp.distributions.MultivariateNormalDiag( loc=loc_and_scale[..., :ndims],
# todo: mark this error on the eq relation, not the entities </s> return a1.start >= a2.start and a1.end <= a2.end	contained_in_span def contained_in_span(a1, a2):
+ ansi.ansi_normal  # todo: why does it keep it? </s> parser = text2html.html_parser	test_convert_linebreaks self.assertEqual("foo", parser.convert_linebreaks("foo")) self.assertEqual(
# todo sensible lookup table (doesn't matter for now because small n) </s> def set_mode(mode):	set_mode
# todo: use pybossa uploader! only for debugging: </s> app.config['apps_per_page'])	warm_cache warm_app(a['id'], a['short_name']) for page in pages: for a in apps: warm_app(a['id'], a['short_name'], featured=True)
from pyomo.core.kernel.component_variable import ivariable # todo </s> return _absexpression(_self)	generate_expression elif _self.is_constant(): return _Constant_AbsExpression(_self) else: return _NPV_AbsExpression(_self)
# todo: do something more than simply selecting the last match? </s> else:	_shallow_copy return obj.copy()
# todo: move this into the operations code for its caller </s> val = ""	readmmp_info_chunk_setitem elif key == ['save_as_pam']: if val not in ("", MODEL_PAM3, MODEL_PAM5): self.save_as_pam = val else:
# todo: compute bezier curve. </s> return none	get_anchors vmsk = blocks.get(TaggedBlock.VECTOR_MASK_SETTING1, blocks.get(TaggedBlock.VECTOR_MASK_SETTING2)) width, height = self._psd.header.width, self._psd.header.height knot_types = (
gre = [1, 1, 1]  # todo calculate based on mesh </s> return false	_is_valid return False if "1.0." in f.attrs["openPMD"]: f.close() return True
# todo: unfortunately, mongoengine contains bug which </s> return model.pk	get_pk_value
# todo: edge dps could use a different forwarding algorithm </s> ofmsgs.extend(self._delete_all_valve_flows())	_add_default_flows def _add_default_flows(self): ofmsgs.extend(self._add_default_drop_flows()) ofmsgs.extend(self._add_vlan_flood_flow())
# todo: assert </s> def createrepo(self):	createRepo repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "name", "testrepo0", self.token)
# floc-1828 todo - use archive_bucket rather than clusterhq-archive-testing  # noqa </s> for service in ['flocker-control-api', 'flocker-control-agent']	task_open_control_firewall raise NotImplementedError() return sequence([
# todo: all the expand stuff! </s> if collection is provided, it is used to set shortcuts to the action.	action def action(name, parent=None, collection=None): title = template.title(name) if not title:
# todo: 判断返回结果，处理异常 </s> password_push = true if request.post.get("use_password") else false	perm_role_push calc_assets = list(set(assets_obj) | set(group_assets_obj)) push_resource = gen_resource(calc_assets) key_push = True if request.POST.get("use_publicKey") else False task = Tasks(push_resource)
# todo(twd2): do more visibility check eg. contest </s> self.response.content_type = 'text/markdown'	DiscussionTailReplyRawHandler drrid: objectid.ObjectId): ddoc = await discussion.get(self.domain_id, did) self.response.text = drrdoc['content']
# todo: finish this </s> if template is not none:	View the corresponding template as a string, preferably as a unicode string.  The method should return None if there is self.template = template context = Context.create(self, context, **kwargs)
# todo save the error to the plugin </s> plugins = self.plugins.items()	_activate_plugins def _activate_plugins(self, force_reload=False): :param force_reload: force reload base apps, defaults to False logger.info(f'Found {len(plugins)} active plugins') self.activate_integration_globalsettings(plugins)
# todo also test these! </s> for name, e in estimators:	test_all_estimators def test_all_estimators(): estimators = all_estimators() if E in dont_test: continue
# todo(b/158741360): add type annotations once pytype checks across modules. </s> "each other.")	compile_model a compiled keras.Model. if bool(loss is None) == bool(train_step is None): model.compile(optimizer=optimizer, loss=loss, **kwargs) if train_step:
irregular_dim_names = ['time', 't']  # todo: use irregular flag from database instead </s> return reponse	_create_response 'size': sample_xray.shape, 'coordinate_reference_systems': []  # TODO: CRS
## todo: # fixme: remove me </s> if tld is not none:	analyse@68 tld = url_parsed['tld'].decode() except: date = datetime.datetime.now().strftime("%Y%m") server_statistics.hincrby('SQLInjection_by_tld:'+date, tld, 1)
# todo: determine proper template to use. </s> out, err = proc.communicate()	get_exitcode_stdout_stderr def get_exitcode_stdout_stderr(cmd): args = shlex.split(cmd) exitcode = proc.returncode return exitcode, out, err
# todo: make an ascii-art bar </s> return ""	render_time def render_time(self, ctx, data): s = float(data) if s >= 1.0:
# todo: check num strings and support nan </s> self.calltypes	_get_series_name self.typingctx, (self.typemap[series_var.name],), ).blocks.popitem()[1] replace_arg_nodes(f_block, [series_var])
time.sleep(40)  # todo: should remove after polling get. </s> mpc_tensor_1 = mpctensor(parties=clients, secret=remote_value_1, shape=(1, 5))	test_mpc_private_private_op@57 value_2 = Tensor(child=np.array([42], dtype=np.int32)) remote_value_1 = value_1.send(clients[0]) mpc_tensor_2 = MPCTensor(parties=clients, secret=remote_value_2, shape=(1,)) op = getattr(operator, op_str)
pass # todo </s> class artist(testmodel):	Artist
# todo: remove temporary workaround once https://github.com/python-babel/babel/issues/415 has been resolved. </s> self.ui.logout.hide()	GeneralOptionsPage else: self.ui.logged_in.hide() def login(self): self.tagger.mb_login(self.on_login_finished, self)
# todo: get a better way for this. the 'downloadsize' key in logicpro_plist </s> shutil.copyfileobj(req, fp)	download_package_as try: req = urllib2.urlopen(url) except HTTPError, e: print "HTTP Error:", e.code, url
# todo: remove compatability hooks </s> pkg_resources does this using a regexp: (\d+ | [a-z]+ | \.| -)	_split_version_components Unfortunately the 're' module isn't in the bootstrap, so we have to do an equivalent parse by hand.  Forunately, that's pretty easy.
pass  # todo </s> overrides = {}	CLIApplicationBase error(str(e)) sys.exit(1) for opt, value in opts: if opt == "-a":
# todo: test bytearray </s> get uncompressed address from public key	address_uncompressed :param prefix: Specify versionbyte prefix in hexstring or bytes. Normally doesn't need to be specified, method uses default prefix from network settings :type prefix: str, bytes
# todo: improve the unicode checking </s> else:	_batch return results_dict
#todo classes broken </s> def sample(self, type='batch', c=none, features=none, z_iterate=none, target_value=none, seed=none,should_send_file=true):	GANWebServer plot(self.config, np.vstack(stacks), sample_file)
# this is the dc dmdsec. @todo: account for other as well. </s> return fullfilecontent	generateFullFileEntry fullFileContent += "  <icon>" + filename + ".icon</icon>\n" fullFileContent += "  <update>0</update>\n  <info>nopdf</info>\n"
# todo: fix output type </s> s = fill	_column_fillna_impl for i in numba.parfor.internal_prange(len(A)): s = B[i] A[i] = s
# todo: read back actual frequency and store </s> def get_agc(self):	get_agc
# todo(cvan): uncomment when bug 719512 gets fixed. </s> {'contributor': 999, 'application_id': 1, 'type': 1},	test_delete_link url = reverse('collections.edit_contributors', args=['admin', self.slug]) follow=True) url = reverse('collections.edit', args=['admin', self.slug])
# todo is this check necessary; this was an assertion which are disabled in <4000 which is good </s> def registers_op_yank(view, register: str = none, linewise=false) -> none:	registers_op_yank
# todo presto drops the union and decimal fields </s> ('map', 'varchar', none, none, none, none, true),	TestPresto ('timestamp', 'bigint', None, None, None, None, True), ('binary', 'varchar', None, None, None, None, True), ('struct', 'varchar', None, None, None, None, True), ])
# todo: finish this. </s> raise fuseoserror(eio)	_GDriveFS except: logging.exception("Could not register _OpenedFile for created " logging.debug("File created, opened, and completely registered.") return fh
# todo: may test file contents </s> table = rows.import_from_csv(fobj, encoding=self.encoding)	PluginCsvTestCase self.assert_expected_table(table) def test_import_from_csv_fobj(self): self.assert_expected_table(table) def test_export_to_csv_filename(self):
# todo: wrap this with a try and handle exceptions gracefully </s> opts['location']	get_location def get_location(opts, provider=None): provider['location'] DEFAULT_LOCATION
# todo: impala attempt to speed up final pass after lstm. </s> )	get_action_layer_output def get_action_layer_output(self, nn_input, internal_states=None): nn_output, last_internals = unify_nn_and_rnn_api_output( action_layer_output = self.call(self.action_adapter.get_action_layer_output, nn_output) return (action_layer_output, last_internals) if last_internals is not None else action_layer_output
# todo: choose one from the following two </s> todo: for now, it is always available. however, sub-solvers may not	available always be available, and so this should reflect that possibility. return True
# todo: sync with server </s> log.info(f"rejecting user {member}: account is too old and defcon is enabled")	Defcon@54 if self.days.days > 0: now = datetime.utcnow() try: await member.send(REJECTION_MESSAGE.format(user=member.mention))
# todo: use `summary`, `severity` and `description` </s> message = attr.ib(type=str)	FacebookError @attrs_exception class FacebookError(Exception):
# todo: can be done faster by custom code </s> for i in tcfg.autocomplist:	getMatchingText matches[upstr] = None if i < self.line: upstr = util.upper(i) if upstr.startswith(text):
#todo remove str() -- http://github.com/fifengine/fifengine/issues/701 </s> def _init_playing(self):	AmbientSoundComponent self.__emitter = horizons.globals.fife.sound.soundmanager.createEmitter() self.__emitter.setGain(horizons.globals.fife.get_uh_setting("VolumeEffects")*10) if hasattr(self.instance.owner, "is_local_player") and self.instance.owner.is_local_player: play_every = self.__class__.AMBIENT_SOUND_INTERVAL + \
def _get_response(self):  # todo: add timeout </s> res = urlopen(self._prepare_request(), timeout=1)	_get_response def _get_response(self):  # TODO: add timeout except (ValueError, URLError): self.log.warning('Wrong Graphite server')
# todo:  needs to be moved into rasterdata level api </s> self._configs[kernel_id].layers[layer_name].provider._generate_vrt()	KtileConfigManager def add_layer(self, kernel_id, layer_name, layer_dict, dirpath=''): self._configs[kernel_id].layers[layer_name] = \
# todo(john sirois): map target.resources in the same way </s> if len(analysis_cache_product) != 1:	analysis_cache_full_path raise TaskError('There can only be one analysis cache file per output directory') analysis_cache_dir, analysis_cache_files = analysis_cache_product.iteritems().next()
# todo check the error message here. </s> filters = [dict(name='name', op='is_null')]	test_is_null person2 = self.Person(id=2, name='foo') self.session.add_all([person1, person2]) response = self.search('/api/person', filters) document = loads(response.data)
# todo: not sure if this is pg only or standard </s> context.assert_(	test_alter_column_type def test_alter_column_type(): context = _op_fixture() 'ALTER TABLE t ALTER COLUMN c TYPE VARCHAR(50)'
# todo: set the following parameter </s> return self.request.host	request_host def request_host(self):
# todo add shape check </s> return self._sqrt_hessian_sampled(module, g_inp, g_out)	sqrt_hessian_sampled
# todo: support data with shape </s> saver.restore(self.sess, ckpt.model_checkpoint_path)	init_session_handler logging.info("Use the model: {}".format( ckpt.model_checkpoint_path)) self.inputs = json.loads(tf.get_collection('inputs')[0]) self.outputs = json.loads(tf.get_collection('outputs')[0])
# todo: only if successful </s> return os.path.join(submission_root(instance), "pred", "run", filename)	submission_prediction_output_filename
pattern = '[aeiou]{2}' # todo: change this to '/[aeiou]{2}/' </s> self.assertisinstance(filtered_result, output.seeresult)	TestSeeResultClass pattern = re.compile('[aeiou]{2}') expected = ('.clear()', '.count()') if PY3 else ('.count()',) self.assertEqual(expected, filtered_result) @unittest.skip('Not implemented')
# todo: remove when #980 has been merged </s> })	MetacriticIE 'ext': 'mp4', 'format_id': rate_str, formats.sort(key=operator.itemgetter('rate')) description = self._html_search_regex(r'<b>Description:</b>(.*?)</p>',
# truffle todo: revert </s> def _ishidden(path):	_ishidden
# todo: handle this error </s> :param context: context variables for email template	email_transactional def email_transactional(subscribed_users, **context): :return: for user in subscribed_users:
# todo: use separate index type instead of just storing array </s> data_tup = context.make_tuple(builder, types.tuple(data_typs), data_arrs)	init_dataframe context, builder, string_type, c) for c in column_names] dataframe = cgutils.create_struct_proxy( column_tup = context.make_tuple(builder, types.UniTuple(string_type, n_cols), column_strs) dataframe.data = data_tup
# todo: if `sqlalchemy` interface, delete key in engines </s> instance = get_connector_by_type(dialect, interface)	new_connector instance['id'] = None return JsonResponse({'connector': instance})
# todo - need to parameterize this into generate_match_filters, </s> group is the cn of the group to be removed from	remove_user_from_group def remove_user_from_group(self, user, group, opts=None): old_group = self.get_group_by_cn(group, None, opts) if old_group is None:
# todo: is this safe? </s> self.cmd( 'iptables -a forward -i', self.localintf, '-s', self.subnet, '-j accept' )	NAT self.cmd( 'iptables -P OUTPUT ACCEPT' ) self.cmd( 'iptables -P FORWARD DROP' ) self.cmd( 'iptables -A FORWARD -i', self.inetIntf, '-d', self.subnet, '-j ACCEPT' ) self.cmd( 'iptables -t nat -A POSTROUTING -o ', self.inetIntf, '-j MASQUERADE' )
# todo check for collision with user filter </s> audio_rate = self.audio_rate	AMReceiver demod_rate = 64000 SimpleAudioReceiver.__init__(self, name=name, demod_rate=demod_rate, band_filter=5000, band_filter_transition=5000, **kwargs) self.agc_block = gr.feedforward_agc_cc(1024, 0.1) self.demod_block = gr.complex_to_mag(1)
# todo use trads with %s </s> except usageerror as err:	Vhost params = {'paas_id': paas_id, 'vhost': vhost} try: if err.code == 580142: params['--dry-run'] = True
# todo does a real upgrade (instead of uninstall/install) work? </s> open_firewall = open_ufw	task_open_control_firewall if distribution in ('centos-7', 'fedora-20'): open_firewall = open_firewalld else: raise DistributionNotSupported(distribution=distribution)
#todo generate the labels for the dict automatically from labels </s> --------	get_obssumm_file See Also -------- >>> import sunpy.instr.rhessi as rhessi >>> rhessi.get_obssumm_file(('2011/04/04', '2011/04/05'))
# todo make this faster either in c++ or python </s> pass_name = output_name	_import_aov elif output_name.startswith("RADIANCE_GROUP"): pass_name = lightgroup_name blender_pass = render_layer.passes[pass_name] convert_func(width, height, buffer, blender_pass.as_pointer(), aov.normalize)
# todo: when repo.subscribe(observer) </s> def on_next(self, value: tuple[hyperparamsrepository, trial]):	SomeObserver class SomeObserver(_Observer[Tuple[HyperparamsRepository, Trial]]): def __init__(self): self.events.append(value)
# todo: test dropping "collision_lines_map" and replacing with "enter/exit" tiles </s> condition = mapcondition(cond_type, args, x, y, w, h, operator, key)	load_event for key, value in natsorted(obj.properties.items()): if key.startswith("cond"): conds.append(condition) elif key.startswith("act"):
# todo: configurable rsync options? </s> files_to_copy.append(full_filepath)	_list_files_to_copy continue if copy_it: return files_to_copy
# todo: and results </s> def __call__(self, args):	Value class Value(object): def __init__(self, name): return args[self._name]
# todo: give a vanilla example </s> states[power > on_power_threshold] = 1	powers_to_states@180 states = pd.DataFrame(np.zeros(power.shape))
#execute 'autodone' on the current command </s> if hasattr( self.propmgr, 'view_changed'):	view_changed Called whenever the glpane's view (view center, direction, projection) may have changed, so that self can do UI updates in response to that. self.propMgr.view_changed() return
# todo: call 'status_callback'? </s> yield self._cluster._sched_event.set()	_terminate_scheduler
# todo: this is a jump. </s> return vi_cmd_data	vi_big_m vi_cmd_data['is_jump'] = True vi_cmd_data['motion']['command'] = 'vi_big_m'
# todo(mottodora): add reduce option </s> error of two inputs.	mean_squared_error A variable holding an array representing the mean squared
# todo: log exception </s> result = result[-1]	scan@71 file = file + ' ' + result.pop(0) if file not in filelist or not result: results.append((file, result)) metadata = {}
# todo(evonide) define field </s> def num_files(self):	num_files @property
reorder_attributes(element)  # todo: remove when support is python 3.8+ </s> if self._num_instances is none:	num_instances @property raise ValueError( 'Number of material instances have not been determined. Call '
if not version_2_79_or_older():  # todo </s> row = col.row(align=true)	CustomPanel col = box.column(align=True) if not version_2_79_or_older():  # TODO row.scale_y = 0.75 row.label(text='Not yet compatible with 2.8!', icon='INFO')
# todo: do something more than simply selecting the last match? </s> if isinstance(obj, schemabase):	_shallow_copy return obj.copy(deep=False) elif isinstance(obj, list):
# todo: deprecation warning </s> return self.destroy(request, *args, **kwargs)	DestroyAPIView GenericAPIView): Concrete view for deleting a model instance.
# todo(brett.cannon) implement </s> def mock_implicit_hooks():	mock_implicit_hooks
# todo(mhickey): get rid of it once we switch the db model to using </s> if 'ip_address' in fields:	AllowedAddressPair@38 @classmethod def modify_fields_from_db(cls, db_obj): fields['ip_address'] = netaddr.IPAddress(fields['ip_address']) if 'mac_address' in fields:
# todo(b/34288791): this needs to be exactly the same as in impl.py </s> returns:	_apply_operation_on_fine_grained_view fine_grained_view: A `_OptimizationView.fine_grained_view`. next_hashed_path: The hashed path for the currently processed The resulting list of `_OptimizationView.fine_grained_view`s. result_fine_grained_view = collections.OrderedDict()
## todo : log error </s> string_list = map(lambda x: x is not none and str(x) or "", mylist)	stringify_listvars for row in mylist: string_list.append(map(lambda x: x is not None and str(x) or "", row)) except IndexError: pass
# todo: remove this after we create the contents web service and directories are </s> def save_notebook_model(self, model, name, path=''):	save_notebook_model
@unittest.skipif(not settings.dev_mode, 'dev_mode disabled, osf.users.create unavailable')  # todo: remove when available outside of dev_mode </s> assert_not_in(self.user_two._id, ids)	test_find_single_user_in_users user_json = res.json['data'] ids = [each['id'] for each in user_json]
# todo: i am relying on order preservation here... </s> :meth:`block` of the eigen tensor is provided. otherwise, no block	eigen_tensor def eigen_tensor(expr, temporary, index): information is needed and the tensor is returned as is. :arg expr: a `slate.Tensor` node.
# todo: this should be solved via plugins </s> migrate_foreignkey(self.app_label, self.model, 'recipes_oldrecipearticleredirect', 'new_id', self.orm)	Migration super(Migration, self).move_self_foreignkeys(orm) migrate_foreignkey(self.app_label, self.model, 'articles_articlecontents', self.model, self.orm)
# todo: in #5022 </s> "children__category",	get_menu_as_json "category", "page", "children__page", "children__collection",
# todo - take this out of the menu </s> editmenu.add_command(label="increase font size",      command=self.root.zoom_in, accelerator="ctrl+=")	MenuBar editmenu.add_command(label="Paste",      command=self.root.edit_paste, accelerator="Ctrl+V") editmenu.add_command(label="Select All", command=self.root.selectall,  accelerator="Ctrl+A") editmenu.add_command(label="Decrease Font Size",      command=self.root.zoom_out, accelerator="Ctrl+-") editmenu.add_separator()
# :todo: implement test. </s> def test_fail_approvees_wrong_type(self):	FindTransactionsRequestFilterTestCase self.skipTest('Not implemented yet.') def test_fail_tags_contents_invalid(self): self.skipTest('Not implemented yet.') def test_fail_approvees_contents_invalid(self):
pass # todo </s> xmlreader.xmlreader.seterrorhandler(self, err_handler)	setErrorHandler def setErrorHandler(self, err_handler):
# todo: 289 </s> such that we don't have enough kfrags to give to each ursula.	MoreKFragsThanArrangements class MoreKFragsThanArrangements(TypeError):
# todo: remove once elasticsearch v6.x is deprecated. </s> search_index = timesketch_sketch.searchindex.query.filter_by(	Close the Timesketch search index object. super(TimesketchOutputModule, self).Close() index_name=self._index_name).first() search_index.set_status('ready')
# todo: find the operation that does not properly close the oblivion\data dir. </s> values in settings dictionary can be either actual values or	IniFile ini_settings = {section:{key:value}} self.saveSettings(ini_settings) full key=value line ending in newline char.""" if not self.path.exists() or not self.path.isfile():
assert study_id == 0  # todo(akiba) </s> self.param_distribution[param_name] = distribution	set_study_param_distribution def set_study_param_distribution(self, study_id, param_name, distribution):
# todo: need token </s> raise notimplementederror(arg_type)	_Eval if op_id == Id.BoolUnary_n: return bool(s) if node.tag == bool_expr_e.BoolBinary: op_id = node.op_id
pass # todo: explain </s> self.setmessage('header-location', rs.redirect_without_location)	status303 def status303(self):        # See Other
# todo: when dropping python 3.6, use </s> "please use '--allow-chromium-download' to allow downloading one.")	_check_launch_reqs raise RuntimeError("No suitable chromium executable found on the system. "
# todo: replace this root with tree hash root </s> block_without_signature_root	validate_proposer_signature@33 proposal_root = ProposalSignedData( state.slot, ).root beacon_proposer_index = get_beacon_proposer_index(state, state.slot, epoch_length)
# todo: handle agg_columns. </s> def idxmin(self, skipna=true):	idxmin
# todo: remove from self.workers (except that detached() should get </s> if not can_start:	canStartBuild if worker.builds_may_be_incompatible: props = setupPropsIfNeeded(props) return False if IRenderable.providedBy(locks):
# todo(mattjj): support argument pattern-matching </s> handle_arg = partial(shard_arg, compiled._device_ordinals, axis_size)	parallel_callable out = compile_replicated(jaxpr, axis_name, axis_size, consts, *avals) compiled, nrep, shard_result_shape = out handle_replica_result = xla.result_handler(shard_result_shape) handle_full_result = sharded_result_handler(axis_size, merged_aval(pval))
logfile = open('logs/exceptions.log', 'a') #todo: make not hardcoded </s> try: os.kill(self.child, signal.sigkill)	kill def kill(self):
# todo instead of indexing and filtering later </s> if not had_things or stop == limit:	page_query for thing in things: had_things = True break bar.update(start)
# todo do a proper mro resolution. currently we are just listing </s> and we need to check for that, e.g. a signature can be:	define_generics from jedi.inference.gradual.base import GenericClass def remap_type_vars(): def iter(iterable: Iterable[_T]) -> Iterator[_T]: ... However, the iterator is defined as Iterator[_T_co], which means it has
pass  # todo </s> xl_workbook.sheets(sheet_name_or_index).cells.clear()	clear_worksheet
# todo: change href="$help:command" to href="help.html#command" </s> return ''	_AttrsToString@473 def _AttrsToString(attrs): return ''.join(' %s="%s"' % (k, v) for (k, v) in attrs)
# todo return the property set too. see #1086 </s> basis_gates (list[str]): list of basis gate names supported by the	_transpilation pass_manager=None): Args: target. Default: ['u1','u2','u3','cx','id'] coupling_map (CouplingMap): coupling map (perhaps custom) to target in mapping
"""todo doc me""" </s> [u'b', u'3', u'7.8', true],	test_profile_distinct_values table = [['foo', 'bar', 'baz'], ['A', 1, 2], ['D', 'xyz', 9.0], ['E', None]]
# todo: remove the coverage limitation with further mandatory model attributes </s> )	system_exporter_spreadsheet_csv_config_view@33 { 'form': form, else: model = SystemExporterSpreadsheetCsvConfigModel.objects.get(system_exporter_spreadsheet_csv_config_name = 'SystemExporterSpreadsheetCsvConfig')
# todo: add highlighting line </s> self.functions_list.show()	update_functions item.setData(function_addr, Qt.UserRole + 2) self.functions_list_model.appendRow([item]) else: self.functions_list.hide()
# todo(b/184055743): once tensorflow is released with cl/342914534, </s> inputs['x_center'] = inputs['x'] - tft.mean(inputs['x'])	preprocessing_fn def preprocessing_fn(inputs):
expr,  # todo rethink this circular import </s> _label_counter += 1	_generate_label def _generate_label(name: str) -> str: return f"label{_label_counter}"
# todo: empty env? </s> self.basedir)	TestP4 + { 'stdout' : 'Change 28147 on 2008/04/07 by p4user@hostname\n' } + 0, + 0, ]
# todo(shoyer): test fails on tpu </s> f = lambda pos, inc: (lax.add(pos, 1), lax.add(count, inc))	loop_body def loop_body(state): effect[0] = True return api.jit(f)(pos, inc)
# todo: bindings should be done in collection class: </s> query_string = """select ?g ?r {graph ?g {?r rdf:type dlns:handle .	SearchHandle for remote in local_master.git_get_remotes()] + [local_master.get_backend_from_branch()]) ?s ?p ?o . FILTER regex(?o, "%s")}}""" % \
# todo: use the kinetic scroller if implemented </s> return _outside	edge def edge(self, point): rect = self.geometry() edge = 0 if point.x() <= rect.left() + 4:
# todo(b/141131288): enable test once complex sort is supported on tpu. </s> jnp_fun = lambda x: jnp.unique(x, return_index, return_inverse, return_counts)	testUnique np_fun = lambda x: np.unique(x, return_index, return_inverse, return_counts)
# todo: same code as for batch gradient, but with sum_batch = true </s> req_output=false):	GradBase derivatives, params=None, super().__init__( module,
# todo: https://github.com/python/mypy/issues/3004 </s> def labels(self) -> list[str]:	labels return list(self.patterns)  # type: ignore
# todo: kwargs </s> def df_len_overload(df):	df_len_overload if len(df.columns) == 0:  # empty df return lambda df: 0
our_balance = our_balance - have_amount  #todo i think this line is unnecessary here </s> parser.add_argument("--price-feed", type=str,	args def args(self, parser: argparse.ArgumentParser): parser.add_argument("--config", type=str, required=True, help="Source of price feed. Tub price feed will be used if not specified") parser.add_argument("--order-expiry", type=int, required=True,
# todo: this should maybe go somewhere else </s> def translate_expression(self, expr):	translate_expression
# todo: remove </s> *musicpd.org, reflection section:*	urlhandlers @protocol.handle_request(r'urlhandlers$') ``urlhandlers`` Gets a list of available URL handlers.
# todo(elliot): how to determine bundle id </s> for line in out.split("\n"):	create_existing_recipe_list if exitcode == 0: for i in range(0, len(recipes)): if search_term in line: recipes[i]["existing"] = True
#todo can the fetchparser code be adapted for use here? </s> @param flags: sequence of flags to remove	remove_flags def remove_flags(self, messages, flags): @return: As for get_flags. return self._store('-FLAGS', messages, flags)
# todo: handle na as 1st value </s> "pyexc_typeerror",	check_element_type with c.builder.if_then(type_mismatch, likely=False): c.builder.store(cgutils.true_bit, errorptr) "can't unbox heterogeneous list: %S != %S", expected_typobj, typobj,
#todo: raise an exception: unexpected date format </s> date = datetime.today().timetuple()	pipe_datebuilder@45 count = int(date.split(' ')[0]) date = datetime.today().replace(year = datetime.today().year + count).timetuple() elif date == 'tomorrow': date = (datetime.today() + timedelta(days=1)).timetuple()
# todo: verify </s> binds = manager.find_by_repo(self.repo_id)	test_remove_repo_cleanup manager = factory.consumer_bind_manager()
raise notimplementederror  # todo(mattjj,frostig) </s> papply_primitive_rules[prim] = partial(reducer_papply, prim, collective_prim)	defreducer
log("dispersy.log", "handled-barter-record") # todo: maybe move to barter.log </s> currently i always sign for testing proposes	allow_signature_request assert message.name == u"barter-record" assert not message.authentication.is_signed
# todo is this check necessary; this was an assertion which are disabled in <4000 which is good </s> def registers_op_yank(view, register: str = none, linewise=false) -> none:	registers_op_yank
# todo read2 is silently discarded </s> untrimmed = xopen(output_path, 'w')	trimmed_and_untrimmed_files elif untrimmed_path is not None: untrimmed = xopen(untrimmed_path, 'w') else: untrimmed = default_output
# todo %b, %(fmt)t, plus "the standard ones in printf(1)" </s> return 0  # all options are true	Shopt return 2  # bash gives 1 for invalid option; 2 is better if not getattr(exec_opts, opt_name): b = None if arg.s:
# todo disjunct can be fathomed? </s> return disjunctive_bound(var, scope)[1]	disjunctive_ub
with open(filename, 'r', encoding='iso-8859-1') as f:  # todo: solve iso encoding pb ! </s> def loadconversations(self, filename, fields):	CornellData lineObj[field] = values[i] lines[lineObj['lineID']] = lineObj Args: fileName (str): file to load
# todo: fix this 405 error </s> def post_add_redirect(self):	post_add_redirect
# todo move to function </s> self.backup_name	cleanup_and_exit if self.notify_nsca: self.notify_nsca.notify(self.notify_nsca.critical, "%s: backup '%s' failed!" % ( )) if self.db:
# todo: test this case </s> c = cos(theta)	rot_y s = sin(theta) zero = theta * 0.0
# @todo: deprecate </s> if rheader:	irs_dispatch output = msg.compose(**opts) if attr.get("rheader"): output["rheader"] = rheader output["title"] = T("Send Dispatch Update")
# todo: supports blocking queries and all consistency modes </s> return self.agent.http.get(	Check self.agent = agent def ttl_pass(self, check_id): lambda x: x.code == 200, '/v1/agent/check/pass/%s' % check_id)
asynchronous=false, # todo: (true) when jconnor fixes </s> expected body: {units:[], options:<dict>}	Content return result def uninstall(self, id): where unit is: {type_id:<str>, unit_key={}} and the options is a dict of uninstall options.
# @todo this needs to be using domain fronting to defeat censorship </s> fp = os.path.join(dirpath, f)	dir_size total_size = 0 for dirpath, dirnames, filenames in os.walk(start_path): if not os.path.islink(fp): total_size += os.path.getsize(fp)
# todo: maybe a chardet integration </s> self._eol = self._eol or eol	_set_eol
# todo: implement these buttons </s> if row > -1 and row < self.rowcount():	title def title(self, index): return self.recipe_model.custom_recipe_collection[row].get('title', '')
# todo: check whether it's already installed?. see yum notes  yum list installed "$@" >/dev/null 2>&1 </s> shutit.log('checking exit value.',level=logging.debug)	get_exit_value def get_exit_value(self, shutit): success_check = self.expect(['SHUTIT_RESULT:0','SHUTIT_RESULT:1']) if success_check == 0:
# todo: make this query also check over the alias table. </s> f"special rate limit of {self.special_rate_limit} reached! sleeping for thirty seconds.")	request_dict_from_endpoint if self.recent_requests_made == self.special_rate_limit: self.recent_requests_made = 0 time.sleep(20) try:
# todo: this is not the correct setting. set hyperparameters correctly </s> loaded, all_dataset, transformers = deepchem.utils.save.load_dataset_from_disk(	load_pdbbind_grid subset + "_smiles_labels.csv") tasks = ["-logKd/Ki"] save_dir) if loaded:
#todo: dry [1] </s> def has_end(self):	has_end
candidates.sort() # todo: this sort has side effects </s> if id(e) == objid:	_hasElementByObjectId if id(e) == objId: return True return True return False
cursor.execute('''delete from mempool_messages''')  # todo </s> logging.debug('status: {} has been conserved ({} {} both issued and held)'.format(asset, util.devise(db, issued, asset, 'output'), asset))	check_conservation held = sum([holder['address_quantity'] for holder in util.holders(db, asset)]) if held != issued:
# todo: using get_fft_info is digging into the implementation </s> persists=false))	_RetuningTestDevice value=freq, type=RangeT([(-1e9, 1e9)]),  # TODO kludge magic numbers
# todo: implement this </s> def pushbutton_close_onclick(self):	pushbutton_close_onclick
#todo:  raise error </s> def edit(self):	B2Bucket print(response.json()) else: pass
# todo: overwrite step's self return value on fitting. </s> return hash((self.i, self.x))	__hash__
# todo(mordred) when this changes to rest, force interface=admin </s> return self.manager.submit_task(	get_nic_by_mac def get_nic_by_mac(self, mac): _tasks.MachineNodePortGet(port_id=mac)) except ironic_exceptions.ClientException:
# todo behind </s> indent(f, indent_level)	print_atl f.write(u"TODO atl\n")
# todo: we could assert here that latest_version matches x.y.z. </s> release_tags = [tag for tag in git_tags if tag.startswith('ckan-')]	latest_release_tag e.g.: "ckan-2.1.1" This requires git to be installed. release_tags.sort() return release_tags[-1]
# todo detect for typeerror: duplicate base class str, </s> given_types=tuple(remap_type_vars())	define_generics if type_var_dict: return AnnotatedSubClass( ) return self
# todo: currently not used by pyrep. </s> return handle	simRMLVel handle = lib.simRMLVel(dofs, smallestTimeStep, flags, currentPosVelAccel, maxAccelJerk, selection, targetVel, ffi.NULL)
# todo make private methods private </s> class getchecksumtests(synchronoustestcase):	GetChecksumTests
# todo: can we assume reverse=false? </s> queryset = self.get_queryset().annotate(	most_common num_times=models.Count(self.through.tag_relname()) ).order_by('-num_times')
# todo: check against cygwin </s> skipped=false):	_challenge_done congratulations=None, follow_on_context={}, cfg = shutit_global.shutit.cfg if result == 'ok':
# todo - unittest this </s> resolve_partition(path2).mountpoint	on_same_filesystem def on_same_filesystem(path1, path2):
# todo: write this </s> shape = polygon(points) + control_points(controls, extrude_height=0)	bezier_points_variants ]
# todo -- validate other options </s> if service not in services:	validate_config service = config.get(target, 'service') if not service: die("'%s' in [%s] is not a valid service." % (service, target)) validate_section[service](config, target)
# todo delete me </s> tagger.attrib('diffuse', '{0} {1} {2} {3}'.format(	exportSDFMaterial tagger.attrib('ambient', '{0} {1} {2} {3}'.format( ambient['r'], ambient['g'], ambient['g'], alpha)) diffuse['r'], diffuse['g'], diffuse['b'], alpha)) specular = materialdata['specularColor']
#todo: consider factoring out: some duplication between xliff and tmx </s> unit.target = translation	addtranslation def addtranslation(self, source, srclang, translation, translang): tuvs = unit.xmlelement.findall('.//%s' % self.namespaced('tuv')) lisa.setXMLlang(tuvs[0], srclang)
if self.source_file:  # todo: should we error here or something if the source_file doesn't exist? </s> self._require_file()	_get_path return self.storage.path(self.name)
# todo: implement </s> return hash(repr(self))	Value def __init__(self, *args): vars(self).update(zip(self._fields, args)) def __eq__(self, other): return other and vars(self) == vars(other)
#obj_url = url_for(self.get_endpoint(), **params) # doesn't work :(, todo : why? </s> sqla_encode	SAFRSJSONEncoder result = str(object) return result fields = {} for field in [x for x in dir(obj) if not x.startswith('_') and x != 'metadata']:
@unittest.skipif(not settings.dev_mode, 'dev_mode disabled, osf.users.create unavailable')  # todo: remove when available outside of dev_mode </s> self.user_one.save()	test_find_single_user_in_users def test_find_single_user_in_users(self): url = "/{}users/?filter[full_name]=my".format(API_BASE) res = self.app.get(url) user_json = res.json['data']
# todo(shoyer): test fails on tpu </s> f = lambda pos, inc: (lax.add(pos, 1), lax.add(count, inc))	loop_body pos, count = state
pass  # todo </s> def test_picture(self):	TestSheet pass  # TODO
# todo should this be null cut? </s> this is the distance from the subsystem's constellation to the null	conceptual_information concept.""" return constellation_distance(subsystem.constellation(), ())
# todo: underscore </s> return m	kaug cloneModel=True, tee=False, keepfiles=False, solver_options=None): m = sensitivity_calculation('kaug', instance, paramSubList, perturbList,
#todo: gst.numberoftableentries = len(confs) </s> ql.mem.write(addr, struct.pack('<q', val))	ptr_write64
# todo model? </s> threshold: float = 0.95,	one_step def one_step( video_path: str, frame_count: int = 5, compress_rate: float = 0.2,
# todo pseudo code: </s> def seeked(self, position):	Seeked pass
sleep(1)    # todo </s> 'status': status,	cancel_order def cancel_order (db, order, status, block_index): cursor = db.cursor() 'tx_hash': order['tx_hash'] }
# todo wait_for_unit_state? why (not)? </s> d = self.client.add(node_1_name, image)	DeploymentTests node_1_name = random_name() node_2_name = random_name() d.addCallback(lambda _: self.client.add(node_2_name, image)) d.addCallback(lambda _: self.client.list())
# todo: write tests </s> "when there is an otherwise-unspecified validity error that prevents parsing."	MeiValidityError class MeiValidityError(exceptions21.Music21Exception):
# todo: return false to disable selection. </s> data_row = self.interface.data[row]	tableView_objectValueForTableColumn_row_ @objc_method try: data = data_row._impls
#todo: manage this </s> self._set_raw()	generate_docs self._set_return()
# todo(stephenfin): the mock of 'migrate_disk_and_power_off' should </s> networks = [	test_create_server_with_physnet_and_tunneled_net requested networks share at least one NUMA node. extra_spec = {'hw:numa_nodes': '1'} {'uuid': base.LibvirtNeutronFixture.network_1['id']}, {'uuid': base.LibvirtNeutronFixture.network_3['id']},
if self.idx_dim != 0: raise notimplementederror  # todo... </s> return [input_shapes[0]]	infer_shape
# todo: handle hiframes filter etc. </s> if func_name == 'predict' and isinstance(func_mod, ir.var):	_analyze_call if self.typemap[func_mod.name] == hpat.ml.naive_bayes.mnb_type: self._meet_array_dists(args[0].name, args[1].name, array_dists) if self.typemap[func_mod.name] == hpat.ml.svc.svc_type: self._meet_array_dists(
# todo: is there any way to move this to serializer? </s> return get_table_model("covid19", "boletim")	Boletim @cached_property
# todo: throw a useful error message </s> if dep is not op.tensor:	side_effects side_effects = OrderedSet() for op in self.ops[0]: side_effects.add(dep) return side_effects
assert study_id == 0  # todo </s> def set_trial_system_attr(self, study_id, trial_id, attr_name, attr_value):	InMemoryStorage def set_trial_intermediate_value(self, study_id, trial_id, step, intermediate_value): assert study_id == 0  # TODO assert study_id == 0  # TODO self.trials[trial_id].system_attrs[attr_name] = attr_value
#todo load this from somewhere </s> "batch %i" % self.batch_idx,	print_process if log.verbose[5]: mem_usage = self.device_mem_usage_str(self.alloc_devices) "score %f" % self.score if self.score is not None else None, "elapsed %s" % hms(start_elapsed),
#ack = self.serial_port.read() # todo: use ack </s> except serial.serialexception:	Disconnect self.serial_port.close()
response.set_cookie(cookie_name, cookie, 31536000, path='/') # todo: move cookie max_age to settings </s> 'score': score,	AddRatingView return self.invalid_field_response(request, context) context.update({ }) had_voted = bool(field.get_rating_for_user(request.user, request.META['REMOTE_ADDR'], request.COOKIES))
# todo remove this sleep to reproduce http://tracker.ceph.com/issues/8107 </s> r.raise_for_status()	_assert_visible def _assert_visible(self, cluster_id, pool_name, visible=True): pool = self._filter_pool(r.json(), pool_name) if visible:
#@todo: 1) perform some sort of check to test the export works </s> def test_export_staff(self):	ExportStaff print "\n" self.login(account="admin", nexturl="hrm/staff/search")
recording_software_name = none  # todo </s> return info_csv["capture software"] == "pupil mobile" and "data format version" not in info_csv	is_pupil_mobile_recording def is_pupil_mobile_recording(rec_dir: str) -> bool: info_csv = utils.read_info_csv_file(rec_dir) except KeyError: return False
# todo: use idc.nexthead(ea) instead... </s> pass	OnClose
# todo fixme is that utc??? </s> import pandas as pd # type: ignore	dataframe %matplotlib gtk from my.bluemaestro import get_dataframe return pd.DataFrame(p._asdict() for p in measurements()).set_index('dt')
# todo: add rotary inertia </s> for nid in omit.ids:	get_omit_set for omit in model.omits: if omit.type == 'OMIT1': for compi in comp: omit_set_map.add((nid, int(compi)))
# todo: replace with positional arguments when we drop python 2 support. </s> return [o if isinstance(o, string_types) else o._meta.db_table	_get_tables for o in tables_or_models]
# todo : accept pay and keywords as parameters too </s> return {}	location_geodata if isinstance(result, (list, tuple)): result = {r['geonameid']: r for r in result}
size = ()  # todo(kataoka): remove this after #4615 is merged </s> _kernels.standard_gamma_kernel(alpha, self._rk_seed, y)	dirichlet else: size += alpha.shape y /= y.sum(axis=-1, keepdims=True) self._update_seed(y.size)
# todo: create dicts as we go, for now we can only assign into existing namespaces </s> super(metafilechanger, self).add_options()	add_options self.add_bool_option("-n", "--dry-run", help="don't write changes to disk, just tell what would happen")
@unittest.skip('not written')  # todo: finish! </s> @py3_only	test_SimpleNamespace def test_SimpleNamespace(self): raise NotImplementedError
# todo: add supported sources when implemented </s> def setup_kickstart(self, data):	setup_kickstart
# todo - needs tests </s> "payment_plans": settings.payments_plans  # possibly nuke	payments_settings@8 return { "STRIPE_PUBLIC_KEY": settings.STRIPE_PUBLIC_KEY,
# todo(yanase): change `task` in storages to `direction`. </s> return self.storage.get_study_system_attrs(self.study_id)	system_attrs @property
# todo: may use sys.stdout.encoding if output_file = '-' </s> sys.exit(1)	_get_field_names if diff: missing = ', '.join(['"{}"'.format(field) for field in diff]) else: result = []
# todo: change download_list to list </s> data = open(path, 'rb').read()	get_md5 def get_md5(path):
# todo: 评估对所有订阅都只取title作为id的效果和影响，考虑基于内容相似度的算法 </s> feed.feed.get("subtitle") or \	_get_feed_title def _get_feed_title(self, feed: feedparser.FeedParserDict) -> str: feed.feed.get("description")
# todo: this is an important operation that should be controlled </s> self._run.write_attr("cmd", self.cmd_args)	_init_attrs assert self._run is not None for name, val in self.attrs.items(): self._run.write_attr("env", self.cmd_env) self._run.write_attr("started", self._started)
# todo: it's a stub before we implement compute_per_cycle_transition </s> private_key,	attest_proposed_block ) sig = bls.sign( ) return cls.get_attestation_record_class()(
# todo: add more complicated testcases </s> random_state=42)	TestAOM [0.74, 0.85, 0.38, 0.47, 0.27, 0.69]]) def test_aom_static_norepeat(self): assert_equal(score.shape, (4,)) shuffled_list = shuffle(list(range(0, 6, 1)), random_state=42)
# todo(sbiswas7): remove this check once we move to glance v2 </s> _images.append(_translate_from_glance(image))	detail _images = [] for image in images: return _images
# todo(pdmars): this should probably be changed to a more generic </s> self.accounts = accounts	AccountsSummary class AccountsSummary(object): @classmethod def load(cls):
# todo(shafey, yonghui): fetch axis_name from globals. </s> p = self.params	_get_beta_gamma if p.use_moving_avg_in_training: beta = 0.0
# todo: use parameter names for run_in_executor() once python 3.4.4 is released. </s> curl --head --silent --output /dev/null \	Spark while [ "$master_ui_response_code" -ne 200 ]; do sleep 1 --write-out "%{{http_code}}" {m}:8080 )"
raise notimplementederror #todo </s> class span(object):	Span
# todo: don't mess with the user's cursor. </s> client = client_for_view(self.view)	request_rename params["newName"] = new_name client.send_request(Request.rename(params), self.handle_response)
pass  # todo </s> called from traintaskthread at the end of an epoch.	train_finish_epoch assert self.train_started assert len(self.forward_data_queue) == 0, "Not all forwardings were used?"
# todo: add tests </s> for fd in required_fds[3:]:	_fixup_fds self.statusfd, self.status_stream.fileno() old_flags = fcntl.fcntl(fd, fcntl.F_GETFD) fcntl.fcntl(fd, fcntl.F_SETFD, old_flags | fcntl.FD_CLOEXEC)
# todo - this isn't actually the correct way to set the vary header, </s> {'detail': 'method \'%s\' not allowed on this resource.' % self.method})	http_method_not_allowed def http_method_not_allowed(self, request, *args, **kwargs):
## todo - fix - this breaks easily </s> f = ast.call( ast.name('get', none), args, [], none, none )	visit_Attribute args = [ ast.Str(node.attr) ]
# todo: does it occur when opening a file with line numbers in it? </s> self.started_undo_blocks -= 1	undo_block_stop if self.started_undo_blocks == 0: self.add_undo_separator()
# todo yield str </s> state.error("preprocessor: '" + arg + "' is not a valid macro name")	cpreprocess_evaluate_ifdef def cpreprocess_evaluate_ifdef(state, arg): arg = arg.strip() return False return arg in state.macros
return 0 # todo </s> return [u'spotify:', u'http://open.spotify.com/']	url_handlers
# todo(halldor): not tested... </s> def unlock(self):	unlock
# todo: config option? </s> content_by_ds = {}	Get path = refds_path refds = require_dataset( for sdsres in Subdatasets.__call__( contains=path,
# todo: remove this method in v2.5 </s> name=self.want.name,	remove_from_device def remove_from_device(self): partition=self.want.partition )
# todo: we should run multithreaded at some point </s> "written several times during a single write() or read() operation.")	parse_args parser.add_option("--cachesize", type="int", default=10, help="Cache size in kb (default: %default). Should be at least 10 times " (options, pps) = parser.parse_args(args) if not len(pps) == 1:
# todo make sure we can still read an unconstrained successor </s> :param state: an optionally state to work off of	_ip_overwrite_with_chain def _ip_overwrite_with_chain(self, chain, state=None): exploit an ip overwrite using rop if state is None: state = self.crash.state
# todo: remove when #980 has been merged </s> http_template = re.sub(qualities_re, r'%s', http_path)	GameSpotIE QUALITIES_RE = r'((,\d+)+,?)' qualities = self._search_regex(QUALITIES_RE, f4m_path, u'qualities').strip(',').split(',') http_template = http_template.replace('.csmil/manifest.f4m', '') http_template = compat_urlparse.urljoin('http://video.gamespotcdn.com/', http_template)
# todo: finish this. </s> logging.debug("file opened.")	_GDriveFS logging.exception("Could not register _OpenedFile for opened " "file.") return fh def release(self, filepath, fh):
# todo: https://twistedmatrix.com/trac/ticket/10137 </s> pdict = {x: y.encode("charmap") for x, y in pdict.items()}	_parseHeader def _parseHeader(line): key, pdict = cgi.parse_header(line.decode("charmap")) return (key, pdict)
# todo: fixup when moving to python 3.3 </s> s.config['danger_mode'] = true	test_danger_mode_redirects s = requests.session()
# todo: this is a jump. </s> vi_cmd_data['motion']['command'] = 'vi_big_m'	vi_big_m def vi_big_m(vi_cmd_data): vi_cmd_data['motion']['args'] = {'mode': vi_cmd_data['mode']} return vi_cmd_data
'alexnet'       : [testmodels.caffeemit, testmodels.cntkemit, testmodels.coremlemit, testmodels.mxnetemit, testmodels.pytorchemit, testmodels.tensorflowemit], # todo: testmodels.kerasemit('tensor' object has no attribute '_keras_history') </s> test_table = get_test_table()	test_caffe tester = TestModels(test_table) tester._test_function('caffe', tester.CaffeParse)
# todo: python3 </s> return index_value	_format_index_value def _format_index_value(self, index_value): Hash these bytes, or don't. else: return self.hash_function(bytes(index_value)).hexdigest()
# todo use an interface here and move the check inside </s> if len(not_yet_set_state) > 0:	unserialize_exported_state all_kwargs.update(this_kwargs) del not_yet_set_state[key] obj.state_from_json(not_yet_set_state) return obj
# todo: re-enable when python3 is available on darwin </s> vmargs += ['-cp', mx.classpath(["com.oracle.graal.python"]), "com.oracle.graal.python.shell.graalpythonmain", name, str(iterations)]	_gate_python_benchmarks_tests def _gate_python_benchmarks_tests(name, iterations): success_pattern = re.compile(r"^(?P<benchmark>[a-zA-Z0-9.\-]+): (?P<score>[0-9]+(\.[0-9]+)?$)") out = mx.OutputCapture()
# todo: add savedmodel support for sparse inputs. </s> ranker = model_lib.create_keras_model(	test_create_keras_model_without_padding network = _DummyUnivariateRankingNetwork( context_feature_columns=self.context_feature_columns, network=network, loss=self.loss,
# todo(kgriffs): this decorator does not work on callable </s> action, responder, resource, true)	after if callable(responder): def let(responder=responder): setattr(resource, responder_name, do_after_all) let()
# todo: what does constructor of gitconfigparser, in case file doesn't exist? </s> based on the current working directory""",	ModifySubhandleURLs args=("--dataset", "-d",), doc=""""specify the dataset to update. If constraints=EnsureDataset() | EnsureNone()), recursive=Parameter(
# todo incorporate inter-bin distances </s> observations = as_observation_matrix(cnarr)	segment_hmm logging.info("Building model from observations") model = hmm_get_model(cnarr, method, processes) states = np.concatenate([np.array(model.predict(obs, algorithm='map')) for obs in observations])
# todo: make sure the image is present or pull it </s> letters = string.ascii_lowercase	random_word return ''.join(random.choice(letters) for _ in range(length))
# todo remove it with inspector frontend </s> return "null"	none_parser
pass  # todo finish me </s> def has_auth(self):	AddonDropboxNodeSettings def is_registration(self): pass pass
# todo: no testpath exercises this code... </s> render_duration = int(now - mid)	report_framerate def report_framerate(self, start, mid, now): totalTime = stepTime + render_duration fps = int(1000.0 / max(totalTime, 1))
# todo: require tests </s> def jac_t_mat_prod(self, module, g_inp, g_out, mat):	ZeroPad2dDerivatives idx_right].contiguous() return result batch, out_features, num_cols = mat.size() _, out_channels, out_x, out_y = module.output_shape
# todo: other code-paths: </s> return [ttfont(path) for path in mada_fonts]	mada_ttFonts @pytest.fixture
# todo: make out_key_var an index column </s> if len(df_typ.columns) == 0:	_run_call_len def _run_call_len(self, lhs, df_var): return [ir.Assign(ir.Const(0, lhs.loc), lhs, lhs.loc)] nodes = []
#todo: be sure to test _version==2 here </s> when versioning is turned on.	test_data_relation_without_version def test_data_relation_without_version(self):
# todo(qos) add agent extensions exception and catch them here </s> extension.obj.handle_port(context, data)	AgentExtensionsManager def handle_port(self, context, data): for extension in self: except AttributeError: LOG.exception(
# todo: should change to 'bytes' on python3 </s> self.assertnotequal(before, after)	test_table_order_by before = [row.birthdate for row in self.table] self.table.order_by('birthdate') self.assertEqual(sorted(before), after) self.table.order_by('-birthdate')
#todo: check the data! e.g. pubdate etc. </s> count = 0	test_feed TODO: have these tests iterate over a number of test pipelines pipe_def = self._get_pipe_def("testpipe1.json") for i in p: count += 1
# todo: losing precision on double types </s> return self.operation.build_parameters(**params)	CLIDriver args = self.create_service_parser(remaining) params = {} def main(self): self.create_main_parser()
# todo: test filter functionality; this only tests that the operations work </s> self.assertequal(1000, filt.get_transition_width())	TestMultistageChannelFilter filt.set_transition_width(1000) filt.set_center_freq(10000) self.assertEqual(10000, filt.get_center_freq())
#todo(wuzewu): download file in tmp directory </s> return none	Downloader for index in module_index_list: if self.module_list_file['version'][index] == version:
# todo: remove </s> tmpfolder, git_mock):	test_verify_project_dir_when_project_doesnt_exist_and_updating with pytest.raises(DirectoryDoesNotExist): verify_project_dir({}, dict(project="my-project", update=True))
# todo again, why this rather than just a dict? </s> return filecontents	payload_encode_file filecontents = f.read() hue = binascii.hexlify(filecontents)
# todo: replace with generic function to generate random sequence of floats </s> record["test_results"], _ = \	_test_python get_times(pyfunc, *args, **kwargs)
# todo untested </s> finally:	get_serial_number _api.OPENSSL_free(hex_serial)
#todo(jk0): finish this later </s> return dict(flavors=items)	detail def detail(self, req):
# todo: the following skipped suite and fixtures should be enabled </s> provider = provider	VultrProviderTests provider_name = 'vultr' domain = 'capsulecd.com'
# @todo: representation? </s> crud_strings[tablename] = storage(	DataCollectionModel self.add_components(tablename, dc_answer = "collection_id", label_create = T("Create Data Collection"), title_display = T("Data Collection Details"),
# todo: use re.error here mayhaps, also: should we log? </s> positions = []	split_args_in_optional_and_positional for i, arg in enumerate(args): previous = None
# todo: test. </s> except twittererror as e:	UsersLookup resp = self._RequestUrl(url, 'GET', data=parameters) try: _, e, _ = sys.exc_info() t = e.args[0]
# todo: specialist error </s> .. seealso::	schema @property :py:func:`Schema <py2neo.neo4j.Schema>` if self.__schema is None:
# todo: check syntax, values? </s> self.setmessage('header-location', rs.redirect_without_location)	status303 def status303(self):        # See Other
# todo: implement it </s> @login_required	GoogleLoginHandler def get(self): user = users.get_current_user()
" # todo: i18n", </s> self.asserttrue(os.path.isfile(self.rej_file))	ToolTest mock_log.assert_called_with( f"hunks failed to apply, rejects saved to {self.target}.rej" self.assertTrue(os.path.isfile(self.orig_file)) @mock.patch.object(log, "exception")
# todo: raise the error instead, and make the user do the refresh manually </s> url = "https://m.facebook.com/login.php?login_attempt=1"	is_logged_in r = self._session.get(url, allow_redirects=False) return "Location" in r.headers and is_home(r.headers["Location"])
# todo do something with reccomendation </s> out_flag = false	print_endpoint_state def print_endpoint_state(self, endpoint_states): def same_old(logger, state, letter, endpoint_states): for my_hash in endpoint_states.keys(): my_dict = endpoint_states[my_hash]
# todo: make this a hard error, instead of a silent overwrite </s> acpi = false	StopInstance name = instance.name acpi = instance.hvparams[constants.HV_ACPI] _, pid, alive = self._InstancePidAlive(name) if pid > 0 and alive:
# todo: remove getattr when https://github.com/rtfd/readthedocs.org/pull/3339 got merged </s> def venv_path(self):	venv_path
# todo: this needs auth. </s> except exception as e:	battle return True, win_percent
# todo: arrange </s> self.remote.modify_repo(repo, "name", "testrepo0", self.token)	createRepo @pytest.fixture def createRepo(self): self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token) self.remote.modify_repo(repo, "mirror_locally", "0", self.token)
# todo move me to unicodedata module and autogenerate. </s> 0xe0002 <= u <= 0xe001f or	# cn  [30] <reserved-e0002>..<reserved-e001f>	is_Default_Ignorable@6 0x1D173 <= u <= 0x1D17A or	# Cf   [8] MUSICAL SYMBOL BEGIN BEAM..MUSICAL SYMBOL END PHRASE u == 0xE0000 or				# Cn       <reserved-E0000> 0xE0020 <= u <= 0xE007F or	# Cf  [96] TAG SPACE..CANCEL TAG 0xE0080 <= u <= 0xE00FF or	# Cn [128] <reserved-E0080>..<reserved-E00FF>
# todo(mordred) when this changes to rest, force interface=admin </s> try:	get_nic_by_mac return self.manager.submit_task( _tasks.MachineNodePortGet(port_id=mac))
# todo: kill this </s> return os.path.join(cache_dir(), filename)	cache_single
#@todo: move to utils in 0.4.10 </s> see `set_config`	setConf return self.set_config(*args, **kwargs)
# todo remove with https://github.com/rtfd/readthedocs-build/issues/30 </s> return code == 0	ref_exists def ref_exists(self, ref):
# todo - здесь можно поставить порог, ниже которого сигнал не пройдёт </s> len_outa = len(outa)	layerA def layerA(self, list_in, prop): if len_outa < prop['InA']: ext_list_in = prop['InA'] - len_outa
continue  #ignore if the source doesn't have our (dereferenced) target field (todo: issue a warning if debugging?) </s> if "subkey" not in key:	pipe_itembuilder@53 dkv = [] for attr in attrs: key['value'] = util.get_value(key, kwargs) value = attr['value']
annot.annotation_metadata.annotator.email = "todo" #todo </s> fill_annotation(path, annot, possible_annot, metadata)	create_JAMS if os.path.isfile(os.path.join(path, "textfile" + str(possible_annot+1) + ".txt")): f = open(out_file, "w") json.dump(jam, f, indent=2)
# todo progress_meter will be changed to a class </s> if dist != 0.0 and dist < cutoff:	contact_matrix@72 else: diff[2] -= box[2] sparse_contacts[i,j] = True else:
# todo: also matches //foo/bar.txt and http://host.tld/foo/bar.txt </s> @return: a list of forms.	getForms def getForms( self ):
#todo: handle ipv6 </s> return none	get_device_instance if device_id not in instance_dict: log.debug("device ID {} doesn't exist".format(device_id), exc_info=1) return instance_dict[device_id]
# todo: allow other formats? </s> @validate(validators={'file_id': validators.int(),	reorder_file 'budge_infront_id': validators.Int()}) def reorder_file(self, id, file_id, budge_infront_id, **kwargs):
# todo: split into a function + context manager </s> else:	teardown_package lgr.setLevel(_test_states['loglevel']) if _test_states['DATALAD_LOG_LEVEL'] is None: os.environ['DATALAD_LOG_LEVEL'] = _test_states['DATALAD_LOG_LEVEL'] from datalad.tests import _TEMP_PATHS_GENERATED
# todo write summaries </s> :return:	get_global_step Returns global step to coordinator.
# todo: interim solution to avoid problems with this step. many commands don't need </s> self.settings.vi['expecting_user_input'] = value	expecting_user_input @expecting_user_input.setter
# todo: also search in the path </s> if adjust_internal_cursor:	cursor_back write(ansi.CUB()) else: self.cursor_position += 1
# todo: this should definitely be fixed to allow auto decompression via that api. </s> })	test_composite_datatype "files_1|type": "upload_dataset", "files_2|url_paste": "log content", roadmaps_content = self._get_roadmaps_content(history_id, dataset) assert roadmaps_content.strip() == "roadmaps content", roadmaps_content
# todo: this function fails for an empty file. better use self.header and self.chunk_headers </s> length = length[0] # unpack always returns a tuple, even unpacking one element	parse_chunk_headers elif status == self.STATUS_CHUNK_OK: self.file.seek(offset*4096) # offset comes in sectors of 4096 bytes compression = unpack(">B",self.file.read(1)) compression = compression[0]
raise  # todo </s> file exists, it will create it and write the data. if a file does exist	_write_registrar_file def _write_registrar_file(registrar_data: dict, registrar_filepath: str) -> None: and contains JSON data, it will _overwrite_ everything in it. with open(registrar_filepath, 'w+') as registrar_file:
# todo: choose something better </s> determine if this remediation script is applicable for this product.	fix_is_applicable_for_product Return 'True' if so, 'False' otherwise""" product, product_version = parse_product_name(product)
# todo extend to nonbinary nodes </s> self.network = nodes[0].network	Purview class Purview: self.nodes = nodes def max_entropy_distribution(self):
ret = plot_func(*args, **merge_kwargs(kwargs_call, fig_ax_kwargs))  # todo conflict?? </s> return _plot(fig_axesplot_func, list(args), name=_name,	_wrapped_factory_fn def _wrapped_factory_fn(*args, **kwargs_call): _plot = plot_many if batch else plot **kwargs_call)
#todo - use a context manager here once we drop python 2.6 </s> [ 5.7, 5.9 ],	test_pca [ 2.0, 1.5 ], [ 1.7, 1.9 ], [ 5.7, 5.9 ], [ 3.1, 3.3 ],
# todo: implement </s> assert p_ginzame.returncode == 0	test_ginzame def test_ginzame(self, input_file): p_ginzame = run_cmd(["ginzame", input_file]) assert p_ginzame.stdout == p_ginza.stdout
# todo(rbharath): modify the featurization so that it matches desired shaped. </s> train, test = train_test_random_split(dataset, seed=seed)	process_3D_convolutions List of paths to convolution datasets. dataset = load_and_transform_dataset(paths, task_transforms, datatype="pdbbind") elif splittype == "scaffold": train, test = train_test_scaffold_split(dataset)
# todo: should use debug </s> todo: figure out why "didn't work" exactly, and adjust description	MeanFeatureMeasure class MeanFeatureMeasure(Measure): and possibly name above is_trained = True
# todo: mock out sleep or use version-ed digests </s> self.assertequal(egg_file.read(), 'egg file content')	test_copy_file_between_directories path = os.path.join(DOC_DIR, 'egg.txt') self.assertTrue(os.path.exists(path))
# todo test this </s> raise notimplementederror	max_likelihood
return self._diameter  # todo: best value? </s> the rotated vector.	_rotate Returns ------- return (vec*np.cos(angle) - np.cross(axis, vec)*np.sin(angle) +
return none  # todo better error handling here </s> "access_type" : "offline",  # to get a refresh token	authorize_redirect_url "client_id": GOOGLE_CONSUMER_KEY, "response_type": "code", } if email_address:
# todo: fix test </s> assert any([x.levelname == "error" for x in caplog.records]), \	test_print_rec_hypothesis if index < 0 or (0 <= index <= db_len and not is_range): check_print = True '\n'.join(err_msg) assert any([x.getMessage() == "Negative range boundary" for x in caplog.records]), \
logfile = open('logs/exceptions.log', 'a') #todo: make not hardcoded </s> print >> sys.stderr, 'please fix this and then run jenni again.'	create_logdir print >> sys.stderr, e.__class__, str(e)
# todo: this is a debug level log </s> args = [arg for i, arg in enumerate(args) if i not in positions]	split_args_in_optional_and_positional positionals = [arg for i, arg in enumerate(args) if i in positions]
except exception:  # todo - which exceptions? </s> w.tag_add(self.current_tag, '1.%d' % start, '1.%s' % stop)	do_search if other_strand: w.tag_add(self.current_tag + 'R', '1.%d' % start, '1.%s' % stop) w.see('1.%d' % start)
# todo: duplicate + replace with psutil.getloadavg()? (available in 5.6.2) </s> for remaining in range(s, 0, -1):	countdown sys.stdout.write("\r") sys.stdout.write("\t\t\t\"auto-cpufreq\" refresh in:{:2d}".format(remaining))
# todo: logging </s> if os.path.islink(yamldir):	Database def __init__(self, yamldir): self.data = self.generate(yamldir) sys.exit('Location for YAML-files can not be a symlink: %s' % yamldir) if not os.path.isdir(yamldir):
# todo: this is a jump. </s> return vi_cmd_data	vi_big_m vi_cmd_data['is_jump'] = True vi_cmd_data['motion']['command'] = 'vi_big_m'
# todo type checks </s> regex_parts = []	highlight_content query_regex = '({0})'.format(re.escape(query)) content = re.sub(query_regex, '<b>\\1</b>', content, flags=re.I) for chunk in query.split(): if len(chunk) == 1:
# todo(vladikr): this code can be removed once the minimum version of </s> conf.target_dev = vif.vif_name	_set_config_VIFOpenVSwitch def _set_config_VIFOpenVSwitch(self, instance, vif, conf, host=None): conf.net_type = "bridge" self._set_config_VIFPortProfile(instance, vif, conf)
context.nrt.incref(builder, arr_typ, arr)  # todo required? </s> c.pyapi.decref(typobj)	check_element_type ) c.pyapi.decref(typobj)
# todo make the delay configurable </s> def kick(self, nick, date, message=true, extra_tags=[]):	kick
# todo debug </s> ruleelement.element = rulemonthdaynew	parseRuleRecursively + "attribute in monthday tag.") ruleElement = RuleElement() currentRule.elements.append(ruleElement) elif not hourItem is None:
# todo use renorm </s> inputs=input_tensor,	upsample_conv input_tensor = tf.concat([upsampled_layer, previous_layer], 3) for i, (nb_filters, filter_size) in enumerate(layer_params): num_outputs=nb_filters, kernel_size=[filter_size, filter_size],
# todo xxx graalvm change </s> sni_name='supermessage')	test_sni_callback server_context.set_servername_callback(servername_cb) stats = server_params_test(client_context, server_context, self.assertEqual(calls, [("supermessage", server_context)]) self.check_common_name(stats, 'fakehostname')
# todo: test coverage of this branch </s> env = {	subscribe_user if already_subscribed: messages.info( 'newsletter': my_newsletter, 'action': 'subscribe'
# todo(vek): need to pass context in for access to auth_token </s> self.state = state	InstanceInfo def __init__(self, name, state): self.name = name
continue # todo: what encoding does gst give us? </s> returns the raw gstreamer state	_get_gst_state raise NotImplementedError
super(syncedmodel, self).save(*args, **kwargs) # todo(jamalex): can we get rid of this? </s> return "%s (organization: %s)" % (self.user, self.organization)	OrganizationUser def get_zones(self): return self.organization.get_zones()
content=content,  # todo(tsileo): handle markdown </s> def clean_html(html):	clean_html
# todo: assert len(args) == len(node.defn.type_vars) </s> tvars = []  # type: typevarlist	analyze_typevar_declaration if sym is None or sym.node is None: return None for arg in unbound.args: tvar = self.analyze_unbound_tvar(arg)
# todo this should recurse down the entire deps tree </s> name="machine").value}	XhrProject try: data["machine"] = {"name": except ProjectVariable.DoesNotExist: data["machine"] = None
# todo: hook this up to something </s> self.initui()	AIGameWindow actionCb ): self.missionEditCb = missionEditCb self.adviceEditCb = adviceEditCb
# todo: how to check it? meybe we can omit this test </s> importer = c2importer()	test_constant net = core.Net("net") net.ConstantFill([], ["Y"], shape=shape, value=val, run_once=0, name="Y") importer.parse_net_def(net.Proto(), verbose=False) f_ng = importer.get_op_handle("Y")
# todo: instead of discarding pending jobs, maintain them </s> yield self._sched_event.set()	_terminate_scheduler
# todo handle join for non public rooms </s> return w.weechat_rc_ok	room_close_cb def room_close_cb(data, buffer): W.prnt("", "Buffer '%s' will be closed!" %
# todo remove the files after loading, make sure that no writer uses theses files anymore </s> :param output_data_dict: the container, which keeps the different images, which should be saved to disc.	save_to_hdf5 append_to_existing_output: bool = False, stereo_separate_keys: bool = False): Saves the information provided inside of the output_data_dict into a .hdf5 container Each key will be saved as its own key in the .hdf5 container. :param append_to_existing_output: If this is True, the output_dir_path folder will be scanned for pre-existing
''' # todo filter in the database by the columns i will actually use? </s> sumsq += item * item	data_suff_stats for item in cursor: count += 1 return (count, xsum, sumsq)
#todo: make this an async task; so client wouldnt wait </s> link_path = os.path.join(self.distro_path, repo["relative_path"])	_delete_ks_link log.info("Unlinking %s" % link_path) if os.path.lexists(link_path):
# todo: this doesn't work because local_rev has already been set </s> return tuple(self.sync.queued_for_download.queue)	queued_for_download @property
# todo: in 0.6.0 change this to "disabled": false </s> "dh": "dh.pem",	test_enabled_missing "cert": "cert.pem", "dev": "tap0", "key": "key.pem", "mode": "server",
# todo: 逻辑应该有问题, 但不确定 </s> try:	decorate return func(*args, **kwargs) except Exception as e:
# :todo: implement test. </s> def test_pass_no_transactions(self):	GetTrytesResponseFilter skip_value_check = True def test_pass_transactions(self): self.skipTest('Not implemented yet.')
# todo: this should be a separate test </s> self.assertequals(event['message'], 'valueerror: hello world')	MiddlewareTestCase self.assertEquals(exc['type'], 'ValueError') self.assertEquals(exc['value'], 'hello world') self.assertTrue('sentry.interfaces.Http' in event) http = event['sentry.interfaces.Http']
# todo: need to close computations on this node? </s> sock.listen(32)	scheduler_server addrinfo.ip, self.scheduler_port) raise StopIteration while 1: conn, addr = yield sock.accept()
# @todo: deployment_setting </s> multiple = true	org_sector_represent if isinstance(opt, (list, tuple)): opts = opt elif isinstance(opt, int): opts = [opt]
#@todo: remove in 0.4.10 </s> for p in xrange(2, pages + 1):	handleMultiPages pages = int(m.group(1)) except: self.html = self.loadPage(p) self.package_links += self.getLinks()
# todo: this relies on the gnu version of ps (need to fix macos support) </s> get the full stdout of the subprocess so far.	get_stdout :return: Standard output of the process. :rtype: str
def aaaaa(self, bbbbb, cccccccccccccc=none):  # todo(who): pylint: disable=unused-argument </s> xxxxxxxxxxxxxxxxxxx = {	testB26868213 } expected_formatted_code = textwrap.dedent("""\ 'ssssss': { 'ddddd': 'qqqqq',
# todo: verify behavior </s> threads=[	test_few received = self.vsc.received self.assert_vsc_received(received, [ {'id': 1, 'name': 'spam'}, {'id': 3, 'name': ''},
# todo: figure out what's going on </s> ar = annexrepo(dst, src)	test_AnnexRepo_instance_from_clone @with_testrepos('.*annex.*') @with_tempfile assert_is_instance(ar, AnnexRepo, "AnnexRepo was not created.") assert_true(os.path.exists(os.path.join(dst, '.git', 'annex')))
# todo(jblespiau): we can simply use buf.xla_shape() when version 0.1.58 is </s> if not isinstance(x, xla.devicearray):	to_dlpack it were deleted. If ``False``, JAX retains ownership of the buffer; it is undefined behavior if the DLPack consumer writes to a buffer that JAX raise TypeError("Argument to to_dlpack must be a DeviceArray, got {}" .format(type(x)))
# todo: when we reset migrations the following need only check </s> def _blink_drive(cls, dname, request):	_blink_drive disk = cls._validate_disk(dname, request) total_time = int(request.data.get('total_time', 90))
# todo fix bug for not identifying unused variables in nested exceptions see issue #4391 </s> _ = 24	test_unused_with_prepended_underscore def test_unused_with_prepended_underscore(): __a = 24 dummy = 24
# todo(mattjj): remove this special case, used for debugging on cpu </s> def parallel_callable(fun, axis_name, axis_size, *avals):	parallel_callable pvals = [PartialVal((aval, core.unit)) for aval in avals] with core.new_master(JaxprTrace, True) as master:
# todo: make it pass </s> finder = self.get_range_finder('a = (10 +', 1)	test_get_last_open_parens2 self.assertEquals((1, 4), finder.last_open_parens())
# todo: replace with a hook?  just like setting lang= can have a hook. </s> run functions using the $0 pattern.	OshCommandMain oshc deps --path: the $PATH to use to find executables.  What about libraries? --chained-command sudo try:
# todo(is) we can set the min and max depends on the unit, later </s> clear_layout(item.layout())	clear_layout if isinstance(item, QWidgetItem): item.widget().close() elif isinstance(item, QSpacerItem): pass
# todo: this should also test python 2, and pass pyversion accordingly. </s> return rv	file_to_module rv = os.path.splitext(file)[0].replace(os.sep, '.') if rv.endswith('.__init__'):
"""todo: document me.""" </s> avg=0., std=1., dtype=config.floatx)	random_design_matrix Z = self.s_rng.normal(size=(m, self.mu.shape[0]),
# todo: interpolated p-values </s> return self.err_func(prediction, y)	calc_nc def calc_nc(self, x, y):
# todo: handle index </s> arg_typs = (slice_type, types.intp,)	_get_ind_sub_slice return slice(old_slice.start - offset, old_slice.stop - offset) args = [slice_var, offset_var] _globals = self.func_ir.func_id.func.__globals__ f_ir = compile_to_numba_ir(f, _globals, self.typingctx, arg_typs,
# todo: get pytest's coverage plugin working, iirc it has issues? </s> runner = "pytest"	test@52 modstr = "" if module is not None: if coverage: runner = "coverage run --source=paramiko -m pytest"
# todo: replace suite with testcases </s> json_content = json.load(data_file)	load_json_file def load_json_file(json_file): with io.open(json_file, encoding='utf-8') as data_file: except exceptions.JSONDecodeError: err_msg = u"JSONDecodeError: JSON file format error: {}".format(json_file)
# todo have no idea if is cdecl or stdcall </s> self.ql.mem.write(params['dest'], data)	hook_memcpy def hook_memcpy(self, address, params): try: except Exception as e: import traceback
# todo(epot): should be moved inside `features.decode_example` </s> del self, args, kwargs  # unused	mock_download_and_prepare
# todo: handle other method types? other content types? </s> args:	register_request_handler @log_function handler (TransportRequestHandler) self.request_handler = handler
# todo: implement node label semantics </s> return (lambda i: lambda n, m=none: (hasattr(n, 'treeposition') and	_tgrep_nltk_tree_pos_action node_tree_position = tuple(int(x) for x in tokens if x.isdigit())
# todo: handle arrays </s> else: # assume an array type	inputCType return tf.placeholder(ctypes2TF[ctype], shape, name) elif issubclass(ctype, Structure): base_type = ctype._type_ return [inputCType(base_type, shape, name + "/" + str(i)) for i in range(ctype._length_)]
pass # todo </s> write_u32(data, self.offset+0x20, self.duplicate_id)	save_changes def save_changes(self): data = self.file_entry.data write_u32(data, self.offset+0x24, self.action_index) write_u32(data, self.offset+0x34, self.flag_id_to_set)
# todo: change the frontend to pass seconds instead. </s> return {'sub': 'email', 'email': email, 'exp': id_token_expiration_timestamp}	userinfo_mock
# todo: how do i make the __iter__ thread safe? </s> obj = cpickle.loads(r[0])	__iter__@158 def __iter__(self): cursor = self._conn.execute('SELECT information FROM data') yield obj
return 0 # todo </s> def url_handlers(self):	url_handlers
# todo(hirfumi): support for chainer backend </s> parser.add_argument('--eps-decay', default=0.01, type=float,	main@232 help='Optimizer') parser.add_argument('--eps', default=1e-8, type=float, help='Decaying ratio of epsilon') parser.add_argument('--weight-decay', default=0.0, type=float,
# todo check if this always works? </s> else:	triangulate bm = bmesh.new() bm.from_mesh(me) me = obj.data if obj.mode == 'EDIT':
# todo: 搜索和分页 </s> print ret	perm_role_push ret["key_push"] = task.push_multi_key(**role_key) if ret["key_push"].get("status") != "success": if ret_failed: return HttpResponse(u"推送失败")
# xxx todo [ ] should consider concurrent open files of differing modes </s> def ftruncate(self, path, len):	ftruncate self.log("ftruncate(%r, %r)" % (path, len,)) return self._get_file(path).ftruncate(len)
#! todo: this needs to be made more intelligent </s> plt.semilogx(omega, mag)	bode phase = unwrap(phase*180/sp.pi, 360) plt.subplot(211); else: plt.loglog(omega, mag)
# todo: why is this separate? </s> assert ncols == 2, ncols	_read_matrix if matrix_num in [101, 102]: assert form == 2, form assert tout == 1, tout assert nvalues == 3, nvalues
# todo(py3.7): add required=true </s> datalogger.add_subparsers(p_log)	SensorSCD30Applet "measure", help="read measured values") p_log = p_operation.add_parser( async def interact(self, device, args, scd30): major, minor = await scd30.firmware_version()
#todo _rule_for_states needs to made into a generator </s> >>> student.add_states('intel', ('smart', 'avg', 'dumb'))	active_trail_nodes >>> student.add_nodes('diff', 'intel', 'grades') >>> student.add_edges(('diff', 'intel'), ('grades',)) >>> student.add_states('grades', ('A', 'B', 'C')) >>> student.add_observations({'grades': 'A'})
#todo: cast input with np.asanyarray() </s> dtype = getval(anp.array(x)).dtype   # np.full() has a bug for complex numbers	repeat_to_match_shape if axis is None: dtype=None if keepdims: return lambda g : anp.full(shape, anp.sum(g), dtype=dtype), anp.prod(shape)
# todo: this is a temporal fix </s> image_data.shape[1], image_data.shape[2]))	es grad_abs = np.abs(grad[::2, ...] + 1j * grad[1::2, ...]) grad_abs = grad_abs + np.median(grad_abs) es_pixels[::feat_channels, ...] = grad[::2, ...] / grad_abs es_pixels[1::feat_channels, ...] = grad[1::2, ...] / grad_abs
# todo: add for morph targets data. </s> if context == 'node':	decompose_transition def decompose_transition(matrix, context, export_settings): translation = convert_swizzle_location(translation) rotation = convert_swizzle_rotation(rotation)
# todo remove this assertion and test </s> print_stats([gather_stats(mesh)])	flip_delaunay print('\nstep: {}'.format(flip_delaunay.step))
#todo: check the data! </s> todo: have these tests iterate over a number of test pipelines	test_feed pipe_def = self._get_pipe_def("testpipe1.json") p = pipe2py.compile.parse_and_build_pipe(pipe_def, verbose=True)
# todo new message here </s> factory.setsubmitsmrespcallback(self.submit_sm_resp)	startWorker factory.setLastSequenceNumber(last_sequence_number) factory.setConnectCallback(self.esme_connected) factory.setDeliveryReportCallback(self.delivery_report) factory.setDeliverSMCallback(self.deliver_sm)
# '%5d' doesn't work yet.  todo: fix this. </s> to_test = examplestotest()	ExamplesToBenchmark def ExamplesToBenchmark():
'''todo: add docs''' </s> def discrete_columns(self):	discrete_columns @property
# todo: generate extraction warning. </s> handler_value = subkey.getvaluebyname(trigger)	WinlogonPlugin notify_subkey (dfwinreg.WinRegistryKey): Notify Windows Registry subkey. for subkey in notify_subkey.GetSubkeys(): if not handler_value: continue
# todo: figure out if this is 1 or 2 positions ahead. </s> print '- '*39	parse_stream print '-'*79 print 'VBA MACRO %s ' % vba_filename m = None if vba_code.strip() == '':
# todo switch to using the db to determine next duplicate number to use </s> return query_results[0]["id"]	get_photos_folder_id query_results = self.googleDrive.ListFile( {"q": GooglePhotosSync.GOOGLE_PHOTO_FOLDER_QUERY}).GetList() except: raise NoGooglePhotosFolderError()
# todo: require an api key on the basic auth header </s> - for a build, find me the active release with this name.	Release class Release(db.Model): - Mark this release as abandoned. - Show me all active releases for this build by unique name in order
pass # todo: explain </s> if not self.response.parsed_hdrs.has_key('location'):	status303 self.setMessage('header-location', rs.REDIRECT_WITHOUT_LOCATION)
# todo: convert to casetransaction object </s> )	_create_model_for_stock_transaction product_id=transaction_helper.product_id, type=transaction_helper.action, def lazy_original_balance(): previous_transaction = txn.get_previous_transaction()
# todo: implement this rpc service </s> return empty_pb2.empty()	PserverServicer def pull_embedding_vector(self, request, _): return empty_pb2.Empty() def push_gradient(self, request, _): return empty_pb2.Empty()
# todo support multiple backends </s> results = pykka.get_all(futures)	StoredPlaylistsController Read/write. List of :class:`mopidy.models.Playlist`. futures = [backend.stored_playlists.playlists return [playlist for result in results for playlist in result] @playlists.setter  # noqa
# todo: improve the unicode checking </s> results_dict = self._results_list_to_dict(results_list)	_batch response, content = Request().post(self.url, data=self.operations) if response.status == 200: return results_dict else:
# @todo: check to see if they disappeared, etc </s> if callable(standard_postp):	custom_postp output = standard_postp(r, output) if r.method == "check-in":
# todo: implement </s> jsonschema.validate(result, cls.output_schema)	validate_result return with open(cls.output_schema_path, "r") as f:
# todo: also use backward pass </s> cell_dropout = cell	get_selective_model@200 cell_dropout = \ tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=1.0-dropout) with tf.variable_scope("rnn") as rnn_varscope: rnn_result = []
# todo: what if we fail?  error-handling should be recorded someplace, </s> yield txn.commit()	sketch def sampleFunction(txn): for x in range(10):
# todo(pulkitb): parameterize and add more model/runtime options. </s> if dims[0] is none:	_batch batch_size: batch_size Returns: dims[0] = batch_size return dims
# todo: check if format matches </s> 'database': {	_create_database use_default_location = destination['useDefaultLocation'] external_path = destination['nonDefaultLocation'] 'name': database, 'comment': comment,
# todo: refactor to catch errors smartly in get_screenshot_as_file </s> return timeoutexception(	_save_and_log_screenshot def _save_and_log_screenshot(error: TimeoutException) -> Exception: error.msg + f''' Screenshot: file://{path}''',
# todo dont convert to uint8 </s> * ``bool``: no	AddToHueAndSaturation * ``float32``: no * ``float64``: no Parameters ----------
# todo(mitmul): when cupy.random.choice becomes available, remove it </s> gt_argmax_overlaps_inds, xp.arange(overlaps.shape[1])]	_calc_overlaps max_overlaps = overlaps[ xp.arange(len(inds_inside)), argmax_overlaps_inds] gt_argmax_overlaps_inds = np.where(overlaps == gt_max_overlaps)[0] return argmax_overlaps_inds, max_overlaps, gt_argmax_overlaps_inds
# todo consider removing this test entirely, or hardcoding column names </s> print 'side', side_html	test_read_revision3 assert 'title3' in res assert 'key2' in pkg_html assert 'tag3' in side_html assert 'tag2' in side_html
# todo(dcramer): ideally member:write could approve </s> 'ref': request.get.get('ref'),	OrganizationMembersView 'org_has_sso': auth_provider is not None, 'member_list': member_list, 'can_add_members': can_add_members, 'can_remove_members': can_remove_members,
# todo add test for this </s> segments+1, intensity_image=image[..., c])	_replace_segments nb_channels = image.shape[2] for c in sm.xrange(nb_channels): for ridx, region in enumerate(regions): if replace_samples[ridx % len(replace_samples)] > 0.5:
# xxx todo after #2156 datasets may not necessarily carry all </s> if k.startswith('@') or k == 'datalad_unique_content_properties':	_deep_kv composition dct must be a dict continue key = u'{}{}'.format(
# todo: direct use of json['error'] is deprecated; use display_message('...', 'error') instead. </s> j_dic['modifications'] = []	enrich_json_with_base j_dic['entities'] = [] j_dic['events'] = [] j_dic['equivs'] = [] j_dic['infos'] = []
pass # todo </s> return self['desktop entry']['x-zim-readonly']	isreadonly @property
# todo: implement </s> def test_migration_skew(self):	TestMigrations class TestMigrations(SelinonTestCase): def test_migration(self): pass
#todo change back </s> tunnels.append(answer)	find_tunnel try: answer, address = sock.recvfrom(BUFFER_READ) except socket.timeout: break
# todo: remove verify ssl config when working without it. </s> return la_palma_data	fetch_island_data@26 if not la_palma_data: raise ParserException(zone_key, "LaPalma not response") elif zone_key == 'ES-CN-TE': tenerife_data = Tenerife(session, verify=False).get_all()
# todo(mierdin): note that this will always return true if rbac is not enabled </s> existing_inquiry = self._get_one_by_id(	InquiriesController (reject if invalid) 4. Update inquiry's execution result with a successful status and the validated response id=iid, requester_user=requester_user,
# todo: make this nicer! </s> call = ["ffmpeg", "-v", "quiet", "-y", "-i", infile, "-f", str(fmt)]	_assemble_ffmpeg_call infile = infile.encode(sys.getfilesystemencoding()) else: if num_channels is not None: call.extend(["-ac", str(num_channels)])
# todo: may test file contents </s> self.files_to_delete.append(temp.name)	PluginCsvTestCase self.assert_expected_table(table) def test_export_to_csv_filename(self): rows.export_to_csv(utils.table, temp.name) table = rows.import_from_csv(temp.name)
tag = tagger(dt, point) # todo take accuracy into account?? </s> try:	Window self.end = 0 def load_to(self, to): ii = next(self.it) self.storage.append(ii)
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
# @todo: is this always true? </s> g.attrs["omega_lambda"] = pf.omega_lambda	write_to_gdf@123 if pf.cosmological_simulation: g.attrs["current_redshift"] = pf.current_redshift g.attrs["hubble_constant"] = pf.hubble_constant g = f.create_group("field_types")
# test for uwsgi -- todo save this somewhere so we only have to do it once. </s> log.debug( "enabling 'recursive' middleware" )	wrap_in_middleware if asbool(conf.get('use_recursive', True)): from paste import recursive if debug: if asbool( conf.get( 'use_lint', True ) ):
else: # todo: deprecated </s> load_library('armory pbr')	reload_blend_data def reload_blend_data(): armory_pbr = bpy.data.node_groups.get('Armory PBR')
# todo check if solve successful </s> possible_disjunct = possible_disjunct.parent_block()	disjunctive_ub if disj_bnd is not None else disj_ub except AttributeError: return disj_ub
# todo: another solution should be used here. this is a hack for compatibility reasons. to resolve the gadget address calculation of segments of elf files have a different base address if calculated segment.virtualaddress - segment.offset </s> offset_tmp += match.start()	__gatherGadgetsByEnding to_return = [] match = re.search(ending[0], tmp_code) index = match.start() if offset_tmp % arch.align == 0:
# todo(jaypipes): log the error? </s> if res.status == 200:	get_images c = self.connection_type(self.netloc, self.port) c.request("GET", "images") data = json.loads(res.read())['images'] return data
# todo: look at word_spid </s> else:	PrintAst def PrintAst(nodes, opts): if len(nodes) == 1: node = command.CommandList(nodes) if opts.ast_format == 'none':
# todo: move this hard-coded mixin/manager injections to maybe a model </s> "text": rows_fields.textfield,	schema "integer": rows_fields.IntegerField, "json": rows_fields.JSONField, } return OrderedDict(
# todo: move to base class </s> else:	getNodesRect continue n_rect = QtCore.QRectF(n.scenePos(), QtCore.QPointF(n.scenePos().x() + float(n.w), n.scenePos().y() + float(n.h))) for n in self.getAllNodes(): if activeGraphOnly:
g.configure_new(config) # todo: test for emitted warning </s> def test_fs_neat_hidden(self):	TestCreateNew print(g) self.assertEqual(set(iterkeys(g.nodes)), {0, 1, 2}) gid = 42 config = self.config.genome_config
# todo: for the domain-allocation switch, this needs to be turned </s> return tuple([self.coordinates.indexify((p_dim, i))	coordinate_symbols p_dim = self.indices[-1]
# todo(developer): uncomment and set to a path to your audio file. </s> print('first alternative of result {}'.format(i))	transcribe_file_with_multichannel@182 for i, result in enumerate(response.results): alternative = result.alternatives[0] print(u'Transcript: {}'.format(alternative.transcript)) print(u'Channel Tag: {}'.format(result.channel_tag))
# todo: check arp reply is valid </s> self.metrics = faucet_metrics.faucetmetrics(reg=self.registry) # pylint: disable=unexpected-keyword-arg	setup_valve self.table = FakeOFTable(self.NUM_TABLES) self.logger = valve_util.get_logger('faucet', self.logfile, logging.DEBUG, 0) self.notifier = faucet_experimental_event.FaucetExperimentalEventNotifier( self.faucet_event_sock, self.metrics, self.logger)
# todo: remove dependency on legacy_examples </s> runner = clirunner()	test_build_dags@21 DAG parsing, DagBag() will add an entry to its import_errors property. By exercising this path, we ensure that our codegen continues to generate valid Airflow DAGs, cli_args_to_test = [ ['--module-name', 'dagster_examples.toys.log_spew', '--pipeline-name', 'log_spew'],
# todo: sphinx stores created and updated as seconds since the </s> return (self._allows_public_posting() or	allows_posting_by def allows_posting_by(self, user):
# todo: cache this result so multiple failing calls don't keep hitting the db </s> last_modified_form_id=stock_state.last_modified_form_id,	from_stock_state entry_id=stock_state.product_id, balance=stock_state.stock_on_hand, daily_consumption=stock_state.daily_consumption, location_id=stock_state.location_id,
#todo tuplet: add variables for if it's the start of a tuplet </s> beaming is a boolean value determining if the voice should be beamed	getBeaming Note: So far only VexFlow's automatic beaming is supported Cannot manually modify beams
# todo: support default period argument </s> for i in numba.parfor.internal_prange(len(a)):	_column_fillna_impl s = B[i] if np.isnan(s):
# todo: documentation pending </s> if is_train is none and self.is_train is none:	_check_mode raise ValueError("Training / inference mode not defined. Argument `is_train` should be set as True / False. Otherwise please use `Model.train()` / `Model.eval()` to switch the mode.") elif is_train is not None and self.is_train is not None:
# todo this has been messed around with. confirm that </s> self.routing_rules[intent.bucket][intent.prefix] = intent.target_prefix	_perform_update_s3_routing_rule def _perform_update_s3_routing_rule(self, dispatcher, intent): See :class:`UpdateS3RoutingRule`. return old_target
raise exceptions.mpdnotimplemented  # todo </s> subscribe to a channel. the channel is created if it does not exist	subscribe@18 def subscribe(context, channel): *musicpd.org, client to client section:* already. The name may consist of alphanumeric ASCII characters plus underscore, dash, dot and colon.
# todo in python 2.7 or later, this should be a set literal. </s> pass	DeserializationException class DeserializationException(Exception):
# todo: raise an alert please </s> class config:	Config
await self._stream.reset()  # todo: specify error code </s> return self	__aiter__
assert r.status == 200  # todo: other codes </s> self.transactions.remove(tx)	HTTP return tx def commit(self, tx): r = self._post("/db/data/transaction/%s/commit" % tx) assert r.status == 200  # TODO: other codes
# todo(iftenney): compare performance of these implementations. </s> for i in range(self.t.shape[0]):	pprint output = StringIO() output.write("{:s}({:d}, {:d}):\n".format(self.__class__.__name__, targs = sorted(list(self.project_tokens(i))) output.write("  {:d} -> {:s}".format(i, str(targs)))
# todo: for backward compatibility only, remove if not used anymore </s> return self._get_by_key(key, self.vm)	get_vm def get_vm(self, key=None): vm = self.module.params.get('vm') if not vm:
# todo if update_variable_bounds = false, this will not work as intended. </s> pass	FBBTException
return cursor_offset, line #todo not implemented </s> if cursor_offset == 0:	backspace @on('\x7f') @on('KEY_BACKSPACE') return cursor_offset, line if not line[:cursor_offset].strip(): #if just whitespace left of cursor
pass # todo </s> self.add_deltas(event.delta_x, event.delta_y, 10)	on_scroll_on_area
#todo(ziad): use a more sophisticated proxy </s> return authprotocol(app, conf)	auth_filter
# todo: bug, until startup_nodes is refactored "master" must be default because some nodes do not have "server_type" </s> return conn	get_connection_from_node_obj self.connections[node["name"]] = conn else: except RedisClusterException as rce: print(" RedisClusterException: {}".format(rce))
# todo: replace with specific error when exceptions are refactored </s> def left(self, value):	left @left.setter
# todo: remove this once dask.array automatically aligns chunks </s> and 'axis' arguments can be supplied. if neither are supplied, then	reduce Dimension(s) over which to apply `func`. axis : int or sequence of int, optional the reduction is calculated over the flattened array (by calling `func(x)` without an axis argument).
# todo(ytknzw): add more specific assertion with the test case. </s> def fail_objective(_: trial) -> float:	fail_objective
#@todo: remove in 0.4.10 </s> cnlport = self.getconfig('port')	coreReady return ip      = socket.gethostbyname(socket.gethostname()) if self.getConfig("extern") else "127.0.0.1" self.proxy(ip, webport, cnlport)
# todo: handle rtl </s> class flexline(list):	FlexLine
msg="your scan id is not valid!"  # todo: add to message() </s> def session_kill():	session_kill unset session on the browser Returns:
# todo use trads with %s </s> return cls.call('paas.vhost.info', name)	Vhost return cls.call('paas.vhost.list', options) @classmethod @classmethod def create(cls, paas, vhost, background):
# todo(mattrobenolt): deal with conflict resolution if </s> return culprit	title if culprit:
# rbarlow_todo: convert this call into a celery task </s> bind['distributor_id'],	distributor_update_itinerary bind_requests = bind_itinerary( bind['consumer_id'], bind['notify_agent'], bind['binding_config'],
# todo: make test method </s> finally:	test_gps return event_loop()
# todo: kill this debug print. </s> self.assertequal(reply['body'], "foo")	test_handle_patch reply = yield self.dispatch_command('patch', url='http://www.example.com') self.assert_http_request('http://www.example.com', method='PATCH')
#todo: milestones = get_milestones(dst_url) </s> req.add_header("accept", "application/json")	send_post_request req = urllib.request.Request(url, json.dumps(data).encode("utf-8")) req.add_header("Authorization", b"Basic " + base64.urlsafe_b64encode(username.encode("utf-8") + b":" + password.encode("utf-8"))) response = urllib.request.urlopen(req) json_data = response.read()
# todo(b/116308354): frequency_threshold is misleading since this threshold can </s> define a set of segments, e.g. [0, 0, 1, 2, 2, 2] defines 3 segments of length	segment_indices def segment_indices(segment_ids, name=None): 2, 1 and 3.  The return value is a `Tensor` containing the indices within each segment.
if lang is none:  # todo: remove in v8 </s> return path	_add_extension def _add_extension(self, path, extension):
# todo test needed to enter here. </s> data = {	action_by_next_token "Please refer to documentation." ).format(action)) 'Action': action, 'NextToken': next_token,
# todo: reproduce and submit traceback to issue 41 </s> :param request: django request object	MyIpView def MyIpView(request): :return: HttpResponse object return HttpResponse(request.META['REMOTE_ADDR'], content_type="text/plain")
def normalised(self):    # todo: mark as deprecated </s> def is_identity(self):	is_identity return quaternion.is_identity(self)
new_todo = selenium_browser.find_element_by_css_selector("#new-todo") </s> selenium_time = time_spent(create_tasks_with_raw_selenium)	test_cashed_selene_is_almost_as_fast_raw_selenium def test_cashed_selene_is_almost_as_fast_raw_selenium(): print("%s vs %s" % (selene_time, selenium_time)) assert selene_time < 1.15 * selenium_time
# todo(leofang): test float16 ('e') once cupy/cupy#5346 is resolved </s> arr2[x, y, z] = arr1[x, y, z]	test_raw_grid_3D def f(arr1, arr2, k, m, n): x, y, z = jit.grid(3) l, m, n = (2, 3, 4) x = cupy.arange(24).reshape(l, m, n)
# todo: replace with sys-ctrl command </s> message_id = random.randint(1, 1e5)	save_work msg_factory = message_factory.SwirlMessageFactory() req_msg = msg_factory.create_submit_share_request(message_id=message_id,
# todo: this is seriously intensive and takes a _long_ time to run. </s> es.index(doc, index, doc_type=document.elasticmeta.type,	index_doc es.index(doc, index, doc_type=Document.ElasticMeta.type, id=doc['id'], bulk=bulk, force_insert=force_insert) id=doc['id'], bulk=bulk, force_insert=force_insert)
# todo: remove after migration to osf storage </s> if auth and user:	can_edit :returns: Whether user has permission to edit this node. if not auth and not user: raise ValueError('Cannot pass both `auth` and `user`') user = user or auth.user
# todo(toshihikoyanase): remove catch_warnings after gridsampler becomes non-experimental. </s> best_trial = max(trials, key=lambda t: t.value)	_StepwiseStudy if self.direction == optuna.study.StudyDirection.MINIMIZE: best_trial = min(trials, key=lambda t: t.value) return copy.deepcopy(best_trial)
# todo: try iso format first? </s> if char.islower():	format_year value = self.value.year else:
# todo scope = 'scope of the search' </s> for policy in page:	search_all_iam_policies@25 response = client.search_all_iam_policies( scope, query=query, page_size=page_size) print(policy) break
# :todo: implement test. </s> )	GetInclusionStatesRequestFilterTestCase TransactionId(self.trytes2), ], def test_fail_empty(self): self.assertFilterErrors(
# todo: add axis parameter </s> def idxmin(self, skipna=true):	idxmin
# todo data alignment stuff </s> sparse = sparse(component_type, type, json, gltf)	importer @staticmethod SparseImporter.read(sparse) return sparse
if compute_stats:  # todo(b/119906277): remove </s> "dataset class must inherit from `dataset_builder.datasetbuilder`.")	test_baseclass def test_baseclass(self): self.assertIsInstance(
# todo: preprocessing=[resample(sample_period)]) </s> \\left | \\sum_t y^{(n)}_t - \\sum_t \\hat{y}^{(n)}_t \\right |	error_in_assigned_energy def error_in_assigned_energy(predictions, ground_truth): .. math:: Parameters ----------
# todo: needs input cleansing and validation </s> federated_only=true)	make_alice_control federated_only = True # const for now bob = Bob.from_public_keys({DecryptingPower: bob_pubkey, except KeyError as e: return Response(str(e), status=500)
# todo(phawkins): remove this after a jaxlib release. </s> self.asserttrue(onp.all(norm(onp.eye(k) - onp.matmul(t(lq), lq)) < 5))	testQr self.assertTrue(onp.all(norm(q1 - q2) < 30)) self.assertTrue(onp.all(norm(a - onp.matmul(lq, lr)) < 30)) if not full_matrices and m >= n: jtu.check_jvp(np.linalg.qr, partial(jvp, np.linalg.qr), (a,))
# todo: the import name wouldn't have to be an object really, could do with a </s> emit(	generateImportNameCode } ) "%s = IMPORT_NAME(%s, %s);" % (
# todo: warn/error: check if this var has units: assigning </s> for vardata in self.values():	setlb Set the lower bound for this variable.
# todo: drop non-callable keys in dramatiq v2. </s> while true:	decr def decr(self, key, amount, minimum, ttl): ttl = int(ttl / 1000) value, cid = client.gets(key) if cid is None:
# todo: check that the performance measure is within some range </s> tests flow/benchmark/baselines/figureeight{0,1,2}.py	TestBaselines Tests flow/benchmark/baselines/bottleneck2.py bottleneck2_baseline(num_runs=1, sumo_binary="sumo") figure_eight_baseline(num_runs=1, sumo_binary="sumo") def test_grid0(self):
#todo: kvick we should rename 'short_circuit' to something like 'disable_service_start' </s> def _store_package_metadata(self):	_store_package_metadata
# todo: why does resourcefilecache return none in some cases? </s> def new_variables(self, value):	new_variables
#   todo:   2012-11-07 14:05:42 by brian mcfee <brm2132@columbia.edu> </s> window[half_f:(half_f-acthalflen):-1]   = half_window[:acthalflen]	hann_window window[half_f:(half_f+acthalflen)]      = half_window[:acthalflen]
raise fail(encode(e))  #@todo: remove `encode` in 0.4.10 </s> raise notimplementederror	handle_free def handle_free(self, pyfile): else: self.fail(_("Required premium account not found"))
# todo: most of this is copied from resolve flats </s> fdir[fdir == k] = v	_flatten_fdir for k, v in gotomap.items():
def normalise(self):    # todo: mark as deprecated </s> def is_identity(self):	is_identity return quaternion.is_identity(self)
#todo context.active_object = obj </s> if not self.demonmesh:	listSubdivisionModes def listSubdivisionModes(self, context): items.append(('mesh', 'Mesh', "Create vertices at each pixels")) return items
#todo: hardcoded to sigma as the model </s> c = self.mesh.edgecurl	ProblemFDEM_b SRCz = src(tx.loc, self.mesh.gridEz, 'z') rhs[i] = np.concatenate((SRCx, SRCy, SRCz)) b_0 = C*a return -1j*omega(freq)*self.MfMui*b_0
# todo: make test method </s> finally:	test_gps return event_loop()
browser.set_driver(webdriver.chrome(chromedrivermanager().install()))  # todo: was firefox here... should it be here? </s> def teardown_module(m):	teardown_module
args.learner_gpus = (gpu_id,)  # todo </s> "step: 2e5, reward: -200, usedtime: 200s modsac"	demo_discrete_action_off_policy@42 args.reward_scale = 2 ** -1 args.target_step = args.env.max_step * 4 args.env = build_env(env=env_name) args.max_memo = 2 ** 19
# todo check if it exists </s> return cls.usable_ids(id, false)	usable_id @classmethod
# todo(rbharath): modify the featurization so that it matches desired shaped. </s> x_test, y_test, w_test = tensor_dataset_to_numpy(test)	process_3D_convolutions elif splittype == "scaffold": train, test = train_test_scaffold_split(dataset) return (X_train, y_train, W_train, train), (X_test, y_test, W_test, test)
# todo: send the user an email </s> bytes_email_message	_get_text_html_attachments message_json['content'].encode('utf-8'), policy=policy.default ) ) return text_content, html_content, attachments
# todo: check syntax </s> self.setmessage('header-location', rs.redirect_without_location)	status303 def status303(self):        # See Other
# todo: replace with stream-changed </s> return self.current_tl_track	get_current_tl_track
# todo(cp16net): need to set the return code correctly </s> def _instance_router(self, mapper):	API mapper = routes.Mapper() super(API, self).__init__(mapper) instance_resource = InstanceController().create_resource() path = "/{tenant_id}/instances"
# todo: resizing on incoming config to make batching more efficient, predict </s> )	classify@165 channel_swap=channel_swap,
# todo: make pull request to get this custom vgg feature accepted </s> is_training=is_training_placeholder,	FCN_32s@33 with slim.arg_scope(vgg.vgg_arg_scope()): logits, end_points = vgg.vgg_16(processed_images, spatial_squeeze=False, fc_conv_padding='SAME')
# todo results from ml </s> return endpoint.machine.name.strip()	_get_name @staticmethod
# todo: error handling </s> self.__backend.remove(triple)	__isub__ for triple in other:
def downloadlink(self, link, disposition=false):  #@todo: set `disposition=true` in 0.4.10 </s> if m is none:	checkErrors def checkErrors(self): self.errmsg = None else:
tol = 0.15  # todo(skye): can we be more precise? </s> valueerror, lambda: func(rng([2, 3], dtype=onp.float64), axis=[-3]))	testFftErrors self.assertRaises( ValueError, lambda: func(rng([2, 3], dtype=onp.float64), axis=[2])) pass
# todo use seed_max </s> * if number, then that threshold will be used for all images.	SimplexNoiseAlpha (e.g. 5.0) will move the saddle point towards the right, leading to more values close to 0.0. * If tuple of two numbers ``(a, b)``, then a random value will be sampled per image from the range ``[a, b]``.
# todo: found a way to force parsing crash or simulate it. </s> tr = template("{% load emarkdown %}{{ content | emarkdown_inline}}").render(self.context)	EMarkdownTest "<p>test</p>\n" "</blockquote>", tr) self.assertEqual(u"<p># Titre 1\n\n" "## Titre <strong>2</strong>\n\n"
# todo: restrict to 'value' type nodes </s> for child in ob.children:	recurse if depth > levels: return recurse(child, copy, depth + 1) return copy
# todo: test that package didn't install, anything else necessary? </s> result.assert_installed('source', editable=false)	test_install_from_wheel_installs_deps result = script.pip( 'install', '--no-index', '--find-links', data.find_links, package,
## todo: # fixme: remove me </s> result.append(line[temp_res:temp_res+len(word)])	is_sql_injection for word in word_list: temp_res = str.find(line, str.upper(word)) for word in word_injection_suspect: temp_res = str.find(line, str.upper(word))
# todo: add test for failures due to missing children </s> },	test_lifecycle_with_nvt "ID": "wholebucket", "Filter": { "Status": "Enabled" }
# todo this eventually needs to be upgraded to support ipv6 </s> if self.pendingrescans == [] or self.dispatchedrescans == []:	getIncompleteScans from app.models import RescanTask self.pendingRescans = RescanTask.getPendingTasks()
# todo: don't write them? is *much* slower on re-load (~3x) </s> self.loader[self.module_key + '2']	test__load__ If a module specifies __load__ we should only load/expose those modules self.update_module()
media=none  # todo select from the database </s> forward.from_id,	dump_forward Returns: ID of inserted row""" values = (None, # Database will handle this forward.channel_post, forward.post_author)
# todo: exceptions </s> if sum >= i and sum < i + 8:	LedCtrlChar for i in range(0, 8*16, 16): for j in range(8): if CHARTAB[char]  &  0x80 >> j: self.LedCtrlRaw( sum, red, green )
# todo(guillermooo): we cannot access the ouput panel used by exec. </s> project = dartproject.from_path(view.file_name())	get_target_path target_path = project.path_to_web if project.is_path_under(project.path_to_web, view.file_name()):
# todo use shlex.quote in python 3.3. </s> options (dict): mapping from parameter to textual json argument.	configure_cms def configure_cms(options): The parameters are substituted in textually, and thus this may be with io.open("%(TEST_DIR)s/config/cms.conf.sample" % CONFIG, "rt", encoding="utf-8") as in_f:
# todo(pl): https://github.com/pantsbuild/pants/issues/206 </s> if agent.can_redefine:	write_agent_manifest manifest.addentry('Premain-Class', agent.premain) if agent.agent_class: manifest.addentry('Can-Redefine-Classes', 'true') if agent.can_retransform:
# todo: support out argument </s> def __isub__(self, other):	__isub__
# todo not sure if this is necesasry anymore? </s> assert uf.text == 'unfavorited'	test_unfav ff = uev[0] assert ff.text == 'favorited [initial]'
# todo(vek): need to pass context in for access to auth_token </s> self.name = name	InstanceInfo class InstanceInfo(object): assert state in power_state.valid_states(), "Bad state: %s" % state self.state = state
# todo - verify contents </s> response = self.client.get('/r/1')	testReviewDetail0 self.assertEqual(response.status_code, 301)
# todo(kevinbenton): remove after bug/1668958 is resolved </s> remote_address = req.headers.get('x-forwarded-for')	_handle_instance_id_request_from_lb if remote_address is None: msg = _('X-Forwarded-For is missing from request.')
# todo fix expected variance </s> self.assertequal(self.estimator1.gettotalindividualcounts(), [15])	test_getTotalIndividualCounts
# todo: remove in hyperspy 1.0 </s> return "%s, " * (len(lst) - 2) % lst[:-2] + "%s and %s" % lst[-2:]	strlist2enumeration elif len(lst) == 2: return "%s and %s" % lst
# todo: infer kernel arguments </s> new_kernel = kernel.copy(schedule=new_schedule)	map_schedule_onto_host_or_device [ReturnFromKernel(kernel_name=kernel.name)])
# todo: get file name from user. </s> self._set_menu_items_sensitive(false)	load_store self.cursor = None
# todo(haoyuzhang): set size to 128 per gpu when multi-gpu xla oom is fixed </s> self._run_and_report_benchmark()	benchmark_8_gpu FLAGS.distribution_strategy = 'default' FLAGS.model_dir = self._get_model_dir('benchmark_8_gpu')
# todo: exit codes (not only for this, but for other exceptions) </s> @click.option('--m', help="m-threshold kfrags", type=click.int)	alice@232 @click.option('--bob-encrypting-key', help="Bob's encrypting key as a hexadecimal string", type=click.STRING) @click.option('--bob-verifying-key', help="Bob's verifying key as a hexadecimal string", type=click.STRING) @click.option('--n', help="N-Total KFrags", type=click.INT) @click.option('--value', help="Total policy value (in Wei)", type=types.WEI)
# todo: replace this with a common get_cluster_info_ec2() method. </s> memory=spark_ui_info['memory'] / 1024)))	Spark status=spark_ui_info['status'], workers=len(spark_ui_info['workers']), def configure_slave(self): pass
# todo: stderr=_stderr_file, </s> os.environ["authfile"] = os.environ["xauthority"] = filename	_setup_xauth self._old_xauth = {} self._old_xauth["AUTHFILE"] = os.getenv("AUTHFILE") cookie = xauth.generate_mcookie() xauth.call("add", self.new_display_var, ".", cookie)
# todo(hrybacki): move to framework.utils.rapply once @sam's pr#4027 is merged. </s> def draft_before_register_page(auth, node, draft, *args, **kwargs):	draft_before_register_page @autoload_draft @must_have_permission(ADMIN) ret = serialize_node(node, auth, primary=True) ret['draft'] = serialize_draft_registration(draft, auth)
# todo: preallocate! </s> def __ge__(self, other):	SortByX0 return mycmp(self.obj, other.obj) == 0 def __le__(self, other): return mycmp(self.obj, other.obj) >= 0 def __ne__(self, other):
# todo: document those attributes </s> self.out_occlusion: floatstr = '1.0'	ParserState self.out_basecol: vec3str = 'vec3(0.8)' self.out_roughness: floatstr = '0.0' self.out_specular: floatstr = '1.0' self.out_opacity: floatstr = '1.0'
# todo: get extra kwargs to serializer (by parsing output_format key)? </s> type=str, #default=default_input_format,	make_option_parser oparser = OptionParser( "%prog [-h] [-i INPUT_FORMAT] [-o OUTPUT_FORMAT] [--ns=PFX=NS ...] [-] [FILE ...]") help="Format of the input document(s). One of: %s." % parser_names, metavar="INPUT_FORMAT")
# todo: test this function </s> if not super(seo_content_type_choices, self).__len__():	_populate_content_types self['id__in'] = [ct.id for ct in get_seo_content_types()]
if (counter >= self.dict_analysis[last]['min_tokens']-1 and counter <= self.dict_analysis[last]['max_tokens']+1): # todo: x-check </s> pluginpath = os.path.join(path.__plugindestination__, plugin)	load_plugins pluginsfound = os.listdir(path.__plugindestination__) for plugin in pluginsfound: if (self.debug): print ('loading and initialzing '+pluginpath)
data_source_name='ledger',  # todo: this isn't really needed. </s> document_type='commcarecasesql',  # todo: should this be the same as the couch models?	change_meta_from_sql_case document_id=case.case_id, data_source_type=data_sources.CASE_SQL, document_subtype=case.type, domain=case.domain,
# todo: may test file contents </s> def test_import_from_csv_filename(self):	PluginCsvTestCase def test_imports(self): self.assertIs(rows.import_from_csv, rows.plugins.csv.import_from_csv) table = rows.import_from_csv(self.filename, encoding=self.encoding) self.assert_expected_table(table)
# todo: add --quiet </s> if output in ("-", none):	schema export_fields = _get_export_fields(table.field_names, fields_exclude) if export_fields is None: output = sys.stdout.buffer else:
# todo: raise forbidden exceptions after adding protection check in the ui </s> session.	_get_config_session_key def _get_config_session_key(self): return '{}_config_{}'.format(self.report_link_type, self.entry_parent.id)
#todo: geli detach -l </s> _passphrase = "-p"	geli_attach_single def geli_attach_single(self, prov, key, passphrase=None): else: _passphrase = "-j %s" % passphrase
# todo: this linebox should use the 'main' color. </s> if self.edit is none:	current_line return '' return self.edit.get_edit_text()
# todo: wait for an event instead of spinning. </s> class wavehdr(structure):	WAVEHDR
# todo: change this so that it unselects by pressing left and then right rather than pasting over the top </s> offset = len(new_text)	copypaste_remove_phrase_from_text if direction == "right":
# todo: handle multiple skip stacks </s> replaced_box_width.without_min_max(box, containing_block)	block_replaced_width @handle_min_max_width block_level_width.without_min_max(box, containing_block)
# todo - are there any other checks that need to be performed at this stage? </s> for child in instance.children.all():	before_delete_stock_location for item in instance.stock_items.all(): item.location = instance.parent child.parent = instance.parent child.save()
# wait for upload processing to finish (todo: this should be done in each test case instead) </s> folder_id=self.security.encode_id( folder1.id ),	test_045_add_dataset_to_folder1 ldda_message = '2.bed message' self.upload_library_dataset( cntrller='library_admin', filename=filename, file_type='bed',
# :todo: implement test. </s> skip_value_check = true	GetBundlesRequestFilterTestCase class GetBundlesRequestFilterTestCase(BaseFilterTestCase): def test_pass_happy_path(self): Request is valid.
pass  # todo </s> playlist = playlist(uri='gmusic:' + i, name=name, tracks=tracks)	GMusicPlaylistsProvider for i in ids: tracks = [translator.to_mopidy_track(track) for track in playlists.append(playlist) self.playlists = playlists
# todo(ochuprykov): remove this method when bug #1485619 will be fixed </s> ]),	Secret constraints=[ constraints.AllowedValues([ ], ),
# todo: preallocate! </s> def mycmp(c1,c2):	SortByX0 def SortByX0(): if c1.x0.size == 2: if np.abs(c1.x0[1] - c2.x0[1]) < eps:
# todo: print should really go to logger, this print goes </s> sample_size = self.task.grad_denom(	valid_step logging_output = self.task.aggregate_logging_outputs( logging_output, self.criterion sample_size, self.criterion )
# todo: get rid of load_module in favor of </s> pack={'__runner__': runner},	roster opts, tag='roster',
# todo: remove this skip after fixing </s> raise skiptest	test_circle_draw@20 @requires_application() def test_circle_draw(): with TestingCanvas() as c: ellipse = visuals.Ellipse(pos=(75, 35, 0), radius=20,
#todo parse via xml output </s> print 'outputs: %s' % [output[0] for output in fn.outputs]	detect_nan print 'Inputs : %s' % [input[0] for input in fn.inputs]
# todo xxx here, force update success </s> instr, extra_ir = mnemo_func[instr.name.lower()](ir, instr, *args)	get_mnemo_expr raise NotImplementedError('unknown mnemo %s' % instr)
# todo: emit for hidden key/values? </s> item = self.itemfromindex(index)	parent def parent(self, index): if index is None: parent = item.parent() if parent is None:
# todo: if py3k, override unpickler.find_class(). </s> if group_name in traversed_groups:	_expandLicenseToken if not traversed_groups: traversed_groups = set() writemsg(("Circular license group reference" + \ " detected in '%s'\n") % group_name, noiselevel=-1)
# todo: which encoding? </s> else:	decode_signed_byte raise ValueError('signed byte must be in range -128..127') if byte > 127: return byte
'location_type': loc.location_type.name,  # todo: remove when types aren't optional </s> return map_reduce(lambda (k, v): [(p, k) for p in v],	parent_child child types
#todo - raise notimplementederror and handle via subclass? </s> self.letters = none	AlphabetEncoder if alphabet.letters is not None: self.letters = alphabet.letters + new_letters def __getattr__(self, key): if key[:2] == "__" and key[-2:] == "__":
# todo: remove this later </s> (os.path.altsep and	egg_info_path assert filenames, "No files/directories in %s (from %s)" % (base, filename) if len(filenames) > 1: x.count(os.path.altsep) or 0)) self._egg_info_path = os.path.join(base, filenames[0])
# todo: this logic does not prevent duplicate test cases, need to address this in the future. </s> self.logger.error(	restart_target return False time.sleep(3) "no vmcontrol or procmon channel available ... sleeping for %d seconds" % self.restart_sleep_time )
# todo(paul): work out why because i really don't think it should </s> self.keys = keys	RoomConstraint class RoomConstraint(object): def __init__(self, search_type, keys, value): self.value = value @classmethod
# todo: remove in v.0.6 </s> self.assertalmostequal(csep, 0.72981476)	TestCovariance cov = Covariance() cov.fit(self.iris_points)
# todo: try/except these calls </s> if(format == "png"):	image_tag Format for returned or referenced images. Subclasses supporting image display should override this try: image = self._get_image(match.group("name"))
# todo make this configurable </s> self._leave(nick, date, message, "kick", extra_tags)	kick
# todo(lyarwood): test drivervolumeblockdevice.detach in </s> instance, self.requested_networks, self.security_groups)	test_build_networks_if_not_allocated system_metadata={}, expected_attrs=['system_metadata']) mock_allocate.assert_called_once_with(self.context, instance, self.requested_networks, None, self.security_groups, None)
raise mpdnotimplemented # todo </s> def _findadd(self, type, what):	_findadd @register(r'^findadd "(?P<type>(album|artist|title))" "(?P<what>[^"]+)"$')
pass  # todo </s> prereg_questions = ()	get_prereg_questions def get_prereg_questions(prereg_schema=None): for page in prereg_schema.schema['pages']: prereg_questions = prereg_questions + tuple(page['questions'])
pass  # todo </s> end = vector(x, y)	tap state['start'] = vector(x, y) else: shape(start, end) state['start'] = None
svg = apply_template(svg, {"maxpoints":maxpoints, "trdn": trdn, "msg":"", "valuemid":"0.5", "timemid":"10s", "datapoints":"","init_max_y": "false", "max_y": 0, "seconds_scale":0, "y_shift": 0, "zero": 0}) # todo templating engine </s> return ""	feeder try: if hashstr in lhosts and ip != lhosts[hashstr]: except KeyError: return ""
# todo: build this into a spnego_negtokenresp() </s> if conndata['openedfiles'].has_key(fileid) is false:	smb2QueryDirectory fileID = str(queryDirectoryRequest['FileID']) else: return [smb2.SMB2Error()], None, STATUS_FILE_CLOSED if os.path.isdir(connData['OpenedFiles'][fileID]['FileName']) is False:
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
pass # todo </s> self._parser.errorhandler = jyerrorhandlerwrapper(err_handler)	setErrorHandler xmlreader.XMLReader.setErrorHandler(self, err_handler)
# todo ideally this happens a layer higher, but this is a bad </s> dataset_source = dataproviders.dataset.datasetdataprovider( dataset )	chunk_dataprovider @dataproviders.decorators.dataprovider_factory( 'chunk', dataproviders.chunk.ChunkDataProvider.settings ) return dataproviders.chunk.ChunkDataProvider( dataset_source, **settings )
# todo remove this sleep to reproduce http://tracker.ceph.com/issues/8107 </s> r = self.api.get("cluster/%s/pool" % cluster_id)	_assert_visible r.raise_for_status() pool = self._filter_pool(r.json(), pool_name)
# todo: this may be wrong, see ticket #1115 comment:27 and ticket #1784. </s> return true	is_allowed_in_immutable_directory
# todo(vish): do we have to use sql here? </s> fixed_ip_ref.instance = none	fixed_ip_instance_disassociate fixed_ip_ref = models.FixedIp.find_by_str(address, session=session)
# todo: implement me </s> def fset(self, value):	fset
pass # todo: explain </s> if not self.response.parsed_hdrs.has_key('location'):	status303 self.setMessage('header-location', rs.REDIRECT_WITHOUT_LOCATION)
# todo: support steps and times (motion blur) </s> self.count += 1	Duplis self.count = 1 def add(self, matrix):
raise pathaccesserror()  # todo path </s> return '%s(%s)' % (cn, ', '.join([repr(p) for p in self.path_parts]))	Path self.path_parts.append(part) def __repr__(self):
# todo: act </s> result = self.remote.modify_distro(distro, fname, fg, self.token)	test_create_distro_positive (fname, fgood, fbad) = field for fg in fgood: self.assertTrue(result) except Exception as e:
# todo: change this back to a factory in the instance trait some day </s> super(plotgrid, self).do_layout(*args, **kw)	do_layout else:
# todo test </s> return hash((frozenset(self.nodes), self.current_state.tostring(),	__hash__ def __hash__(self):
# todo send the key to the master for approval </s> if 'img.mnt_{0}'.format(mnt) in __context__:	umount_image CLI Example:: salt '*' img.umount_image /mnt/foo __salt__['qemu_nbd.clear'](__context__['img.mnt_{0}'.format(mnt)]) return
# todo: 判断返回结果，处理异常 </s> if ret_failed.get(asset.hostname):	perm_role_push else: def func(**kwargs): failed_asset.append(asset) func(is_password=password_push, is_public_key=key_push, role=role, asset=asset, success=False,
capacity_in: tokenamount,  # todo: rename </s> self.flat	_fee def _fee(self, balance: Balance, amount: float) -> float: assert self._penalty_func + self.proportional / 1e6 * abs(amount) + self._penalty_func(balance + amount)
# todo(b/160795287): deprecate estimator based executor. </s> absl.logging.info('%s model copied to: %s.', tag.capitalize(), dest)	copy_model raise ValueError('Invalid input tag: {}.'.format(tag)) source = path_fn(working_dir)
raise notimplementederror  # todo </s> suggested_command = 'nucypher alice run'	alice@235 click.secho("Generated keyring {}".format(alice_config.keyring_dir), fg='green') click.secho("Saved configuration file {}".format(alice_config.config_file_location), fg='green') if config_root is not None: config_file_location = os.path.join(config_root, config_file or AliceConfiguration.CONFIG_FILENAME)
msg="your scan id is not valid!"  # todo: add to message() </s> res.set_cookie("key", "", expires=0)	session_kill ) ) return res
# todo: abstract into function to avoid repetition with `import_`. </s> already_downloaded = [	merge_index checkpoint['status'] = 'NOT_DOWNLOADED' to_add.append(checkpoint) c for c in remotes_in_local.values() if c['id'] in missing_ids and c['status'] == 'DOWNLOADED'
# todo: avoid dummy and generate func here when inlining is possible </s> return lambda df: 0	df_len_overload @overload(len)  # TODO: avoid lowering? def df_len_overload(df): return lambda df: len(df._data[0])
# todo fix. </s> self.__save_failing_context(ctx_init)	test_cmova if not cmp_result:
geth_process.start()  # todo: graceful shutdown </s> elif action == "allocations":	deploy@88 click.secho("Block #{} | {}\n".format(receipt['blockNumber'], receipt['blockHash'].hex())) click.secho("Cumulative Gas Consumption: {} gas\n".format(total_gas_used), bold=True, fg='blue') if not allocation_infile: allocation_infile = click.prompt("Enter allocation data filepath")
# todo temporary, try to stay compatible with rest of the code </s> else:	show_popup return self.show_dialog(popup, {OkButton.DEFAULT_NAME : True, CancelButton.DEFAULT_NAME : False}, return self.show_dialog(popup, {OkButton.DEFAULT_NAME : True}, modal=modal)
assert r.status == 200  # todo: other codes </s> headers=dict(self.headers))	HTTP def hello(self): r = self.http_pool.request(method="GET", metadata = json_loads(r.data.decode("utf-8")) if "neo4j_version" in metadata:     # Neo4j 4.x
# todo: the zip contains actors.xml and banners.xml, which are currently ignored [gh-20] </s> tvrage_api-myuser)	_getTempDir if hasattr(os, 'getuid'): uid = "u%d" % (os.getuid())
# todo: parlist, dots, block </s> p = get_parser('foo')	testFieldExp node = p._field() self.assertIsNotNone(node)
# todo(jflesch): instantiate an actionxxx to do that, so </s> total_time=self.__config.scan_time['ocr'])	__on_single_scan_ocr self.__scan_start = time.time() self.workers['progress_updater'].start(
# todo: figure out if we are clobbering the tests by this </s> def to_dt(msg):	to_dt
# todo: check entity headers </s> if not self.response.parsed_hdrs.has_key('location'):	status303 self.setMessage('header-location', rs.REDIRECT_WITHOUT_LOCATION)
# todo: find a more robust way of checking that the coefficients are indeed </s> return polyhedra_qq_normaliz(base_ring, ambient_dim)	Polyhedra@92 elif backend == 'ppl' and base_ring is ZZ: return Polyhedra_ZZ_ppl(base_ring, ambient_dim) elif backend == 'normaliz' and base_ring is ZZ: return Polyhedra_ZZ_normaliz(base_ring, ambient_dim)
# todo: should changes to draft versions of studies be logged? </s> cache_file = '{0}.html'.format(file_id)	dataverse_get_rendered_file file_id = kwargs['path']
irregular_dim_names = ['time', 't']  # todo: use irregular flag from database instead </s> variables[variable_name] = variable	get_variables for storage_unit in self._storage_units: for variable_name, variable in storage_unit.variables.items(): return variables
# todo : pytest.mark.parametrise once nose is gone. </s> class sb(sa):	SB
# todo: this should be handled by command_start </s> dest='version',	add_legacy_options dest='version_legacy', help="Show version number and exit") help='Show version number and exit')
# todo(lyarwood): merge this into _build_server </s> image_uuid=uuids.cinder_encrypted_image_uuid,	test_nonbootable_metadata_image_metadata def test_nonbootable_metadata_image_metadata(self): server = self._build_server( networks='none' )
# todo: the following skipped suite and fixtures should be enabled </s> if request.node.get_marker('ext_suite_1'):	VultrProviderTests return ['API-Key'] @pytest.fixture(autouse=True) pytest.skip('Skipping extended suite')
"""todo doc me""" </s> >>> import zarr	save_array Examples -------- >>> import numpy as np >>> arr = np.arange(10000)
# todo: if the main thread receives a signal and we have no timeout, we </s> return openssl.crypto.load_certificate(openssl.crypto.filetype_pem,	GetCertificate self.ssl_cert_pem)
#todo finish me </s> for k in keylist:	checkFolders if k.parentFolder is not None and k.parentFolder not in [x.name for x in keyList]: raise Exception
#todo: cascades need handling. </s> return row[column]	proc
# todo: perhaps support parallel effects, as long as all the child effects </s> cb = errback if is_error else callback	resolve_effect a sequence, and if they're returned from another effect's callback they will be returned just like any other effect. if cb is None: continue
# todo: does this work/handle already being logged out/logged in deep ok? </s> conn_module.finalize(self)	finalize_target if mod.module_id == self.build['conn_module']: conn_module = mod
# todo: make sure we do not pass ngons with more than 4 versices to zbrush </s> else:	restore_object_mode def restore_object_mode(mode='OBJECT'): if mode == 'EDIT': bpy.ops.object.mode_set(mode='OBJECT')
# todo: check for field </s> :param fields:	QueryAjaxModelLoader class QueryAjaxModelLoader(AjaxModelLoader): def __init__(self, name, model, fields): Fields to run query against super(QueryAjaxModelLoader, self).__init__(name)
# todo: log exception </s> result = result.split(' ')	scan@71 for (file, result) in virusresults[:]: if result.endswith(' '): if file not in filelist: file = file.split(':')[0]
# todo not portable, redo. </s> sample_stochastic = self_.call(distribution.sample_stochastic, parameters)	TestExplorations exploration_pipeline.add_components(action_adapter, distribution, exploration, scope="exploration-pipeline") def get_action(self_, nn_output): sample_deterministic = self_.call(distribution.sample_deterministic, parameters) action = self_.call(exploration.get_action, sample_stochastic, sample_deterministic)
# todo check width & height is in self.available modes </s> logger.debug("pipeline started for device " + device_id)	_initialize_device s.stream_type(): s.as_video_stream_profile() for s in self.pipeline_profile.get_streams() logger.debug("Stream profiles: " + str(self.stream_profiles)) self._intrinsics = load_intrinsics(
# todo(solitude): remove this. </s> def activity_log(request, userid):	activity_log all_apps = request.amo_user.addons.filter(type=amo.ADDON_WEBAPP) return jingo.render(request, 'account/activity.html', {'log': _get_items(None, all_app)})
# todo add negative sampling? </s> compute cosine similarity between two words.	similarity Example:: >>> trained_model.similarity('woman', 'man')
return none  # todo better error handling here </s> return url_concat(oauth_authenticate_url, args)	authorize_redirect_url if email_address: args['login_hint'] = email_address
# todo: need to change for 2.0 </s> :raises valueerror: unsupported types of authorizations	SwaggerAuth :param str name: name of the authorization to be updated :param auth_info: the real authorization data, token, ...etc. auth = self.__app.root.authorizations.get(name, None) if auth == None:
# todo: this can be removed for cartopy > 0.14.3 </s> 'process locations to get a consistent set of tuples for location'	_handle_location if is_string_like(location): location = self.location_names[location]
# todo fuck. do i really need to split myself?? </s> def make_schema(obj):	make_schema
# todo: support steps and times (motion blur) </s> requires_view_update = config	Change VISIBILITY = 1 << 4 WORLD = 1 << 5
# hack to support saving/loading pytorch models. todo: improve </s> raise notimplementederror	Model cls.ops = curr_ops @property @property def output_shape(self):
#ack = self.serialport.read() # todo: use ack </s> - degrees: motor step in degrees (00.00 - 99.99)	performHandshake - delay: motor pulse delay (0 - 99999) Receives ack ("bq")
#@todo: remove in 0.4.10 </s> self.html = self.loadpage(p)	handleMultiPages for p in xrange(2, pages + 1):
# todo check output </s> f.flush()	test_obj with tempfile.NamedTemporaryFile() as f: test_content = 'test content' self._test_obj(f.name, test_content)
# todo: xmlhttp.onreadystatechange = self.onreadystatechange </s> print xmlhttp.readystate	onReadyStateChange xmlHttp = get_main_frame().gobject_wrap(xmlHttp) # HACK! except: if xmlHttp.readyState != 4: return
# todo: axis, skipna, and many arguments should be implemented. </s> def schema(self) -> structtype:	schema return self.to_dataframe()._sdf.schema
# todo: check ping response </s> self.faucet_event_sock = os.path.join(self.tmpdir, 'event.sock')	setup_valve def setup_valve(self, config): self.tmpdir = tempfile.mkdtemp() self.logfile = os.path.join(self.tmpdir, 'faucet.log') self.table = FakeOFTable(self.NUM_TABLES)
# todo implement </s> return hash((frozenset(self.nodes), self.current_state.tostring(),	__hash__ def __hash__(self):
# todo: yeah, parametrize the user here -w. werner, 2020-07-07 </s> use_vt=false,	test_install_multiple_requirements_arguments_in_resulting_command expected, saltenv="base", python_shell=False,
#todo: find a better name... </s> if hasattr(tmp_p, 'rec_tag'):	Realms tmp_p.rec_tag = False p.get_realms_by_explosion(self) del tmp_p.rec_tag del tmp_p.already_explode
# todo: remove when #980 has been merged </s> qidx = ['low', 'med', 'high', 'veryhigh'].index(format['3sat_qualityname'])	_sortkey prefer_http = 1 if 'rtmp' in format['url'] else 0 return (qidx, prefer_http, format['video_bitrate'])
# todo: tree should listen to chief controller </s> sizer.add(wx.statictext(self, label=self._controller.label,	SettingEditor def _create_controls(self): sizer = wx.BoxSizer(wx.HORIZONTAL) size=(context.SETTING_LABEL_WIDTH, context.SETTING_ROW_HEIGTH)))
# todo: do not duplicate seqno here </s> data = attrib_raw_data_serializer.deserialize(attr)	parse_attr_txn def parse_attr_txn(txn_data): attr_type, attr = _extract_attr_typed_value(txn_data) re_raw = attrib_raw_data_serializer.serialize(data, toBytes=False)
# and the port number made available for re-use. todo: examine the </s> self.failunless(res.check(download.notenoughpeerserror),	_baduri_should_fail self.failUnless(isinstance(res, Failure))
# todo: deprecated - remove in version 0.10 </s> assert loaded.domain.intents == agent.domain.intents	test_agent_train loaded = Agent.load(tmpdir.strpath) assert [a.name() for a in loaded.domain.actions] == \ assert loaded.domain.entities == agent.domain.entities assert loaded.domain.templates == agent.domain.templates
# todo find a better way to set up these defaults </s> _get_core(cm, 'mor1kx-generic')	_run_test_util def _run_test_util(cm, args): _get_core(cm, 'atlys')
# todo: abstract and require implementation? </s> blob = self.bucket.get_blob(bytes(jobstorefileid), encryption_key=self.ssekey)	readFile with open(localFilePath, 'w') as writeable:
# todo: how to check it? meybe we can omit this test </s> f_result = ngt.make_transformer().computation(f_ng)()	test_constant f_ng = importer.get_op_handle("Y")
# todo: remove in v1.2 </s> 'minkowski' and `p` parameter set to 2.	KNeighborsClassifier effective_metric_ : str or callble The distance metric used. It will be same as the `metric` parameter effective_metric_params_ : dict Additional keyword arguments for the metric function. For most metrics
# todo: remove all elements of the list and remove the allowlist </s> tfa.activations,	test_api_typed def test_api_typed(): modules_list = [ tfa.callbacks, tfa.image,
# todo: cache this result so multiple failing calls don't keep hitting the db </s> balance=stock_state.stock_on_hand,	from_stock_state case_id=stock_state.case_id, section_id=stock_state.section_id, last_modified=stock_state.last_modified_date, last_modified_form_id=stock_state.last_modified_form_id,
# todo: may test file contents </s> temp = tempfile.namedtemporaryfile(delete=false)	PluginCsvTestCase table = rows.import_from_csv(temp.name) self.assert_expected_table(table) self.files_to_delete.append(temp.name) rows.export_to_csv(utils.table, temp.file)
# todo: implement auto-dtype method in general parameters </s> if self.elements:	add_elements if not hasattr(self.mapped_parameters, 'Sample'): self.mapped_parameters.add_node('Sample') self.generate_subshells(include_pre_edges)
# todo: figure out how to parametrize this on formats, too </s> def test_year_month(dt, fmt):	test_year_month @pytest.mark.parametrize('dt', tuple(DATES)) @pytest.mark.parametrize('fmt', dtstr = dt.strftime(fmt) assert isoparse(dtstr) == dt
# note this may download autodock vina... </s> current_dir = os.path.dirname(os.path.realpath(__file__))	TestPoseGeneration vpg = dc.dock.VinaPoseGenerator( detect_pockets=True, exhaustiveness=1) protein_file = os.path.join(current_dir, "1jld_protein.pdb") ligand_file = os.path.join(current_dir, "1jld_ligand.sdf")
# todo ... </s> state.error("preprocessor: '" + arg + "' is not a valid macro name")	cpreprocess_evaluate_ifdef def cpreprocess_evaluate_ifdef(state, arg): arg = arg.strip() return False return arg in state.macros
# todo: probaby need to keep priority in callee kernel </s> elif isinstance(callee_insn, (cinstruction,	_match_caller_callee_argument_dimension_for_single_kernel new_callee_insns.append(callee_insn.copy(expression=dim_changer( callee_insn.expression), _DataObliviousInstruction)): pass
# todo: add examples </s> sage: m = manifold(2, 'm') # the 2-dimensional sphere s^2	add_comp_by_continuation EXAMPLES: Mixed form defined by differential forms with components on different sage: U = M.open_subset('U') # complement of the North pole sage: c_xy.<x,y> = U.chart() # stereographic coordinates from the North pole
#todo raise error or others </s> log.error(sterror)	KeyDirView process = Popen(['gconftool-2', '--all-dirs', dir], stdout=PIPE) stdout, sterror = process.communicate() return dirlist = stdout.split()
annot.annotation_metadata.annotator.email = "todo" #todo </s> logging.warning("path not found %s", path)	create_JAMS def create_JAMS(in_dir, metadata, out_file): path = os.path.join(in_dir, "data", metadata[0]) return jam = jams.Jams()
# todo: check if "class" in current line, add class name </s> count += 1	countspaces def countspaces(txt): count = 0 return count
# todo(emilon): torrentmigrator64 shouldn't upgrade the database </s> class databaseupgradeerror(exception):	DatabaseUpgradeError
# todo: cache the list of relations up front, and then we </s> msg = "{} is not supported in bigquery and will be ignored"	warning_on_hooks @classmethod dbt.ui.printer.print_timestamped_line(msg.format(hook_type), dbt.ui.printer.COLOR_FG_YELLOW)
1  # todo: fill in identifier </s> line[7::7] = len(line[7::7]) * '.'	print_stats return stack def _line(recursion): return ''.join(line) highest_time = 0
# todo: handle timeout </s> raise valueerror("listeningport must be > 0 and <= 65535")	TCPClient if listeningPort <= 0 or listeningPort > 65535:
# todo: switch to: </s> return self.current_tl_track	get_current_tl_track
pass # todo(denero) implement </s> def test_invalid_assignment_name(self):	test_invalid_assignment_name
# todo use left / right constants instead of bitmasks </s> rpio.output(self.pins[i + 7], bit)	_write4bits def _write4bits(self, value): for i in xrange(4): self._pulse_enable()
# todo eval hook </s> readers.setdefault(f.__name__, f)	__reader return f
# todo: remove all elements of the list and remove the allowlist </s> tfa.metrics,	test_api_typed tfa.callbacks, tfa.image, tfa.optimizers, tfa.rnn,
return self.__parameters.copy()  # todo deep copy, test </s> return self.__parameters.get(self._distance_measures_key)	get_distance_measures
# todo: catch exepction and return true/false on success/error </s> if sum >= i and sum < i + 8:	LedCtrlChar for i in range(0, 8*16, 16): for j in range(8): if CHARTAB[char]  &  0x80 >> j: self.LedCtrlRaw( sum, red, green )
# todo(cutwater): replace `.decode('utf-8')` call with subprocess </s> daft punk songs.	get_version_name def get_version_name(): return "Doin' it Right"
# todo : pytest.mark.parametrise once nose is gone. </s> class sb(sa):	SB
# todo manage tangent? </s> elif interpolation == "cubicspline":	set_interpolation kf.interpolation = 'LINEAR' elif interpolation == "STEP": kf.interpolation = 'BEZIER' else:
# todo: cleanup cache files? </s> def _get_cache_file(cached_pkg):	_get_cache_file
# todo: remove this logging statement when all other todos </s> try:	BattleshipTransaction self._name = minfo['Name'] if 'Name' in minfo else None self._action = minfo['Action'] if 'Action' in minfo else None oid = self.OriginatorID except AssertionError:
# todo issue #163: remove call to skip_ongoing_request() if it doesn't help. </s> self._debug_info['error_cbs'][-1], self._debug_info['request'])	on_error def on_error(self, callback, autoremove=True): self._debug_info['error_cbs'].append(_func_call_str(callback)) self._on_error.connect(callback, weak=autoremove)
# todo: check entity headers </s> if not self.response.parsed_hdrs.has_key('location'):	status303 self.setMessage('header-location', rs.REDIRECT_WITHOUT_LOCATION)
#todo a tester </s> shosterurl = shosterurl.decode("iso-8859-1", 'ignore')	showResult if (aResult[0] == True): for aEntry in aResult[1]: sHosterUrl = sHosterUrl.encode("utf-8", 'ignore') oHoster = cHosterGui().checkHoster(sHosterUrl)
# todo - check and if we don't have category, take the only placement that exists in current site </s> absolute_url = self.get_absolute_url()	full_url if absolute_url: return mark_safe('<a href="%s">url</a>' % absolute_url)
self._make_zip(app, ty)     # todo: make this with rq? </s> filename='%s_%s_json.zip' % (name, ty)	download_name def download_name(self, app, ty): super(JsonExporter, self).download_name(app, ty) return filename
# todo: require an api key on the basic auth header </s> name = db.column(db.string(500))	Release of creation date descending. id = db.Column(db.Integer, primary_key=True) status = db.Column(db.Enum('live', 'dead'), default='live') build_id = db.Column(db.Integer, db.ForeignKey('build.id'))
# todo: remove when unicode vlens implemented </s> attrlist.append(self._d(name))	iter_cb
# todo: revise exception taxonomy </s> position = 0	parse_pubkey_bundle PACKET_TYPE_USER_ATTR: collections.OrderedDict(), PACKET_TYPE_SUB_KEY: collections.OrderedDict() while position < len(data): try:
# todo: keep a childrenbyname dict so we won't have to loop </s> assert self.children[inode].bookmarkdata == bookmarkdata	addChildObserver if inode in self.children: assert self.children[inode].name == name else: self.children[inode] = DirectoryEntry(name, modTime, bookmarkData)
epsilon = 10**(-bpy.data.worlds[0].decimalplaces)  # todo: implement this separately </s> bpy.ops.export_mesh.stl(filepath=outpath, use_selection=true, use_mesh_modifiers=true)	exportStl tmpobject = bUtils.createPrimitive(objname, 'box', (1.0, 1.0, 1.0)) tmpobject.data = obj.data  # copy the mesh here bpy.ops.object.select_all(action='DESELECT') tmpobject.select = True
# todo: in #5022 </s> return {"@type": "organization", "name": site.name}	get_organization def get_organization():
# no todo item selected </s> def selectable(self):	selectable
# todo: add exception handling </s> "longer than %d characters." % max_tag_size, listen)	validate_listen for tag in tags: if len(tag) > MAX_TAG_SIZE: single_mbid_keys = ['release_mbid', 'recording_mbid', 'release_group_mbid', 'track_mbid'] for key in single_mbid_keys:
# todo alert? </s> dist_info = os.path.join(	agent_name def agent_name(self, agent_uuid): agent_path = os.path.join(self.install_dir, agent_uuid) agent_path, agent_name, agent_name + '.dist-info') if os.path.exists(dist_info):
# todo: test this block </s> note:	html @property def html(self): * 'heading' and 'subheading' should not be included. * Be careful not to try to get the full html inside this template.
# todo: implement </s> for i in range(1, 6):	filenames for k in range(j, 6): for l in range(j if i == k else k, 6): for j in range(i, 6): for k in range(j, 6):
# todo: are the utf-8 decodes really necessary? </s> indent = none	dumpjson if pretty: indent = 4 return self.serialize(models, indent=indent)
# todo check </s> def set_metadata(self, metadata):	set_metadata pass
# todo: this might be too slow because of the addition </s> return a count of the number of datapoints for a time interval.	Timeseries return reduce(operator.add, rval.values()) return rval Returns an ordered dictionary like get(), but the values are integers rather than lists.
# todo: make sure limits are deterministic then update this </s> def test_double_add_ignored(self):	test_double_add_ignored pass
# todo: add logger here </s> f.write(resp.headers.get('www-authenticate', 'none'))	make_auth_screenshot def make_auth_screenshot(resp): print(url + " Requires HTTP Basic Auth") f.write('<title>Basic Auth</title>') f.close()
# # todo discuss </s> return {	get_node_permission 'is_contributor' : node.is_contributor(user), 'can_edit' : node.is_contributor(user) and not node.is_registration,
# todo: how to test the file was added in direct mode? </s> ar = annexrepo(dst, src)	test_AnnexRepo_instance_from_clone @with_testrepos @with_tempfile assert_is_instance(ar, AnnexRepo, "AnnexRepo was not created.") assert_true(os.path.exists(os.path.join(dst, '.git', 'annex')))
# todo(jay-lau-513) translate the contents to a json stdin </s> % (uuid, e))	service_delete return False except Exception as e: return False return False
print "returning", len(datapoints), "datapoints"  # todo make sure this is always correct </s> u'name': u'stats.timers.dfvimeoplayproxy3.varnish.miss.410.count_ps',	fix_datapoints_multi out = {} data looks like: u'points': [[1402928319, 1, 0.133333], ....
# todo: attributes should be freed </s> layout.set_text(first_line_text.replace('\u00ad', ''))	first_line_metrics first_line_text = text.encode('utf-8')[:length].decode('utf-8') if space_collapse: first_line, _ = layout.get_first_line() length = first_line.length if first_line is not None else 0
# todo: parse the field contents </s> ae(r'name \b \i', {'text':'name', 'bold':none, 'italic':none})	test_xe ae(r'"some name"', {'text':'some name'})
# todo will need to go through this for each disjunct, since it does </s> if v2 in v1_pairs:	_bilinear_expressions for pair in repn.quadratic_vars: v1, v2 = pair v1_pairs[v2].add(constr) else:
#todo remove confirmed, and maybe token, from object </s> return redirect('/account')	auth_registerbeta
# todo: redundant of advanced panel implementation, very inaccessible here </s> self.startcrawlbutton.bind(wx.evt_button, self.crawlurislisted)	setupNewCrawl self.startCrawlButton.SetDefault()
"""todo doc me""" </s> return the header row for the given table. e.g.::	fields >>> from petl import fields >>> table = [['foo', 'bar'], ['a', 1], ['b', 2]]
# todo this module is not the module of the param in case of a function </s> match = p.search(code)	search_return_in_docstr def search_return_in_docstr(code): if match: return match.group(1)
# todo: don't produce an expression when used in conditional context </s> return translate_call(builder, expr, callee)	translate_super_method_call or not isinstance(self_arg.node, Var) or not self_arg.node.is_self typ_arg = callee.call.args[0] if (
## todo: deinitializer </s> timeout = len(descriptors) > 0 and timeout or 0.01	ResultsFetcher def results(workers, timeout): descriptors = [w.channel for w in workers if not w.expired] ready, _, _ = select(descriptors, [], [], timeout) return [w for w in workers if w.channel in ready]
self.current_width = self.current_width * 2 #todo </s> "tanh": nn.tanh(),	activations "softshrink": nn.Softshrink(), "softsign": nn.Softsign(), "tanhshrink": nn.Tanhshrink()
#todo: check the data! </s> self.asserttrue("the" in i.get('description'))	test_feed count = 0 for i in p: self.assertEqual(count, 4)
# todo - retrieve from config </s> raise ipaerror.gen_exception(ipaerror.ldap_not_found)	remove_user_from_group if user is None:
) # todo: translate </s> if resp.ok:	google_login if not google.authorized: return redirect(url_for("google.login")) account_info_json = resp.json() return bind_oauth_or_register(google_blueprint.name, account_info_json['id'], 'google.login')
# todo deviations of around 0.5 here from expected values, why? </s> aug = iaa.affine(scale={"x": 1.0, "y": 1.75},	test_image_scale_zoom_in_only_y_axis__deterministic_and_list translate_px=0, rotate=0, shear=0) aug_det = aug.to_deterministic()
# todo: check type via docstring </s> return []	parse_table def parse_table(h4): table = find_next_sibling_until(h4, 'table', h4.find_next_sibling('h4')) head = [td.text for td in table.tr.find_all('td')] row = namedtuple('{}TableRow'.format(h4.text), ','.join(head))
# todo: disconnect </s> return self.listen_maddr.encapsulate(multiaddr(f"/p2p/{self.peer_id}"))	listen_maddr_with_peer_id @property
# todo: what to do if not enough shares, or invalid? </s> decrypts an ecies encrypted symmetric key.	EncryptingKeypair symm_key, enc_symm_key = self.pre.encapsulate(self.pub_key) return (symm_key, enc_symm_key) :rtype: bytes :return: Bytestring of the decrypted symmetric key
# todo: remove </s> return lookup_group_plugin(group_type).form_to_db_schema()	_form_to_db_schema
# todo(eric_k): unicorn@778171fc9546c1fc3d1341ff1151eab379848ea0 doesn't like writing to </s> if unicorn.__version__ <= '1.0.0' and reg_name == 'apsr':	_to_unicorn_id reg_name = 'CPSR' if self._cpu.arch == CS_ARCH_ARM:
# todo complete this method </s> if not self._made_shared:	make_ip self._make_shared() cput0 = (time.clock(), time.time())
# todo only do these things if status is true </s> the message should be item = (routing_key,msg)	format_rabbit_message ret_val = {} routing_key, my_obj = item
# todo: replace xrange (could fail with 32-bit python 2.x). </s> n = min(n, self.len)	__irshift__ return self
#todo(bcwaldon): accomplish this without a type-check </s> del args['controller']	get_action_args except Exception: return {} except KeyError: pass
# todo currently needed for bcf </s> if 'controller_mirror_ports' in self.mod_configuration:	Update_Switch_State if 'controller_log_file' in self.mod_configuration: self.controller['LOG_FILE'] = str( self.controller['MIRROR_PORTS'] = ast.literal_eval( self.mod_configuration['controller_mirror_ports'])
# todo convert to fit transform </s> if hyperparameter_grid is none:	random_forest_classifier def random_forest_classifier(self, trees=200, scoring_metric='roc_auc', hyperparameter_grid=None, max_features = helpers.calculate_random_forest_mtry_hyperparameter(len(self.X_test.columns), self.model_type)
#ack = self.serial_port.read() # todo: use ack </s> sys.stderr.write("error closing the port {0}".format(self.serial_name))	Disconnect if self.serial_port.isOpen(): self.serial_port.close()
# todo if nothing is found ['rows'] will raise a keyerror </s> if not self.request.user.has_perm('tutorialv2.change_contentreaction') and \	HideReaction if 'text_hidden' in self.request.POST: text = self.request.POST['text_hidden'][:80]  # TODO: Make it less static not self.request.user.pk == reaction.author.pk: raise PermissionDenied
# todo: logging </s> :return: true if successful	is_closed def is_closed(self) -> bool: raise NotImplementedError()
#todo: only rebuild static files that changed </s> logger.info('this does not look like a (complete) cactus project (missing "%s" subfolder)', p)	verify_path required_subfolders.append('locale') for p in required_subfolders: sys.exit(1)
#todo: manage different in/out styles </s> self.generated_docs = true	generate_docs self._set_params() self._set_return()
# todo: for dev, store hash of .cpp and .h files on extension build inside version_dev, then when </s> focused = false	open_sim hwnds = [] win32gui.EnumWindows(callback, hwnds) while not focused: time.sleep(1)
## \todo these dialogue methods should be available publicly. </s> self.__textwidget.settext( node.filename() )	__referenceLoaded
# todo - del just my poll, not the entire list ! </s> * http_referer	get_next_url Return URL for redirection on success Try to get it from: if 'next' in request.POST and request.POST['next'].startswith('/'): return request.POST['next']
# todo(rossella_s): get rid of it once we switch the db model to using </s> in_(device_owner)).first())	get_alloc_by_subnet_id alloc_db = (context.session.query(models_v2.IPAllocation). filter_by(subnet_id=subnet_id).join(models_v2.Port). if exclude and alloc_db: return super(IPAllocation, cls)._load_object(context, alloc_db)
#todo: add type hints </s> self.previous_raw_markers = markers	detect_markers aperture=aperture, min_marker_perimeter=self.marker_min_perimeter, markers = [ Square_Marker_Detection(
# todo: i should make sure to escape single quotes here </s> def send_null(self, key, safe=false):	send_null
# todo handle this special case for discord bridge users and </s> self._leave(nick, date, message, "kick", extra_tags)	kick
# todo(andreaf) there is a fair amount of code that could me moved from </s> 'image': conf.service_available.glance,	get_service_list def get_service_list(): service_list = { 'baremetal': CONF.service_available.ironic, 'volume': CONF.service_available.cinder,
# todo: use different flag than .reentrant </s> save our state so we can reentrantly start.	_suspend class _attrholder: pass
# todo: is this the right str? </s> received = self.vsc.received	test_few (12, ''), ) self.assert_vsc_received(received, [ self.new_event('thread', threadId=1, reason='started'),
# todo: import engines dynamically </s> driver_class = cls.get_provider_class(driver_name)	ProviderFactory driver_name = instance.engine_name.lower()
# todo: remove in last pr for bug 1215587 </s> 3. build a new list of jobs in ``new_data`` that are not already in	_remove_existing_jobs 1. split the incoming jobs into pending, running and complete. 2. fetch the ``job_guids`` from the db that are in the same state as they the db and pass that back.  It could end up empty at that point. new_data = []
# todo(johnp): remove once bug 952618 is fixed in the glance client. </s> with self.assertraises(attributeerror):	test_get_properties_missing image = api.Image(self.WITHOUT_PROPERTIES)
# todo: add error handling if multiscanner_process </s> def delete_task(task_id):	delete_task Delete the specified task. Return deleted message. result = db.delete_task(task_id)
# todo: remove safe_unescape_html when mako html safe comes in </s> def delete_all():	delete_all delete_index(INDEX)
# todo: think how to resolve landscape.io warning: </s> self.mouse_x = mouse_x	MouseEvent self.event_type = event_type
# todo(mordred) add this back wnen ksa releases </s> self.addcleanup(self._cleanup_server)	TestInventory if self.flavor is None: self.assertTrue(False, 'no sensible flavor available') server = self.operator_cloud.create_server( name=self.server_name, image=self.image, flavor=self.flavor,
# todo update timeout </s> if self._upstream_status == status_wait_writing:	update_stream event = eventloop.POLL_ERR if self._downstream_status == STATUS_WAIT_READING: event |= eventloop.POLL_OUT self._loop.modify(self._remote_sock, event)
# todo: should this be limited for users with many projects / components? </s> return {	user_profile @must_be_logged_in def user_profile(auth, **kwargs): 'user_id': user._id, 'user_api_url': user.api_url,
# todo implement </s> self.past_state.tostring(), self.network))	__hash__ def __hash__(self):
# @todo: use real lightness from hsv or lab color model </s> return shutil.get_terminal_size((80, 80)).columns	get_term_width def get_term_width():
""" todo """ </s> def __init__(self):	CFG raise NotImplementedError def visualize(self):
# todo: more tests </s> pass	MacGuffin
#todo: check system is stable, perhaps a utility in ctrlutil.py </s> hsv = np.matrix(hsv)	hsvd hsv = np.sqrt(w)
file_obj = data_path(data_files['spectra']) # todo: add images option </s> def test_sdss_template(patch_get, patch_get_readable_fileobj):	test_sdss_template
# todo(tamaranorman) enable when these can be run on tpu </s> for nth_replica in per_replica[1:]:	TPUStrategyTest for per_replica in variables_per_replica: per_replica = strategy.experimental_local_results(per_replica) self.assertAllEqual(first_replica, nth_replica)
# todo: +kwargs </s> if 'does not have any commits' in e.stderr:	get_last_commit_hash commit = stdout.strip() return commit return None raise
pass # todo </s> self.image.window.prompt_message(true, "error: no tool " + tool_id)	_get_tool if tool_id in all_tools: return all_tools[tool_id]
# object. todo: change this to minimize subreddit get sizes. </s> except valueerror:	WikiController oldpermlevel = page.permlevel try: self.handle_error(403, 'INVALID_PERMLEVEL') description = 'Page: %s, Changed from %s to %s' % (page.name, oldpermlevel, permlevel)
# todo: provide more informative errors </s> return response(bytes(new_policy.treasure_map), status=200)	make_alice_control new_policy = drone_alice.grant(bob, label, m=m, n=n, expiration=expiration_time) return alice_control
# todo: return eigenvectors of modified fock matrix </s> h+= numpy.einsum('ckdl->kcld', eri_mo[nocc:,:nocc,nocc:,:nocc]) * 2	rhf_internal for a in range(nvir): for i in range(nocc): h-= numpy.einsum('cldk->kcld', eri_mo[nocc:,:nocc,nocc:,:nocc]) nov = nocc * nvir
# todo: overhaul this method. </s> raise exception( "unknown shell file action %s" % env_var_action )	create_or_update_env_shell_file elif env_var_action == "source": line = "if [ -f %s ] ; then . %s ; fi" % ( env_var_value, env_var_value ) env_shell_file_path = os.path.join( install_dir, 'env.sh' ) return line, env_shell_file_path
# todo:  make these lists permanent attributes of self, so they don't need to be created </s> "unscaling non-fp32 grads may indicate an error. "	unscale_grads_python logger = logging.getLogger("apex.amp") logger.warning( "When using Amp, you don't need to call .half() on your model.") LossScaler.warned_unscaling_non_fp32_grad = True
# todo some kind of non-zero check to make sure that this passes. </s> def run_full_test_suite(c):	run_full_test_suite print('run all tests in collection')
# todo also check for motion codec parameter support </s> return _motion_detected.get(camera_id, false)	is_motion_detected
# todo in python 2.7 or later, this should be </s> instance of a sqlalchemy model.	DeserializationException pass
# @todo: remove this if in 0.6 </s> i = nodeimage(id=el.get('id'),	_to_image name=el.get('name'), driver=self.connection.driver,
# todo: replace this with something, that notifies the user but </s> (horizons/world/__init__.py). it's where the magic happens and all buildings and units are loaded.	Session assert hasattr(horizons.main.session.world, "player"), 'Error: there is no human player' TUTORIAL: def generate_map(self): horizons.main.db("attach ':memory:' as map")
# todo: the following check is not optimal yet. when a </s> checks if the current node is up and running in the cloud	is_alive running = False if not self.instance_id:
# todo(mordred) when this changes to rest, force interface=admin </s> except ironic_exceptions.clientexception:	get_nic_by_mac try: return self.manager.submit_task( return None
# todo: actually we should register for all compile time constant values, and know </s> else:	computeConstantSlice ], description = "Slicing of constant with constant indexes." if lower.isCompileTimeConstant(): return getComputationResult(
# # todo: add to the operation history that this happened </s> form_ids = this_form_accessor.get_form_ids_for_user(user_id)	Command self.handle("0a286c0eb864a382a85974336f9dad09", "test-proj-2") def handle(self, user_id, domain, **options): new_username = "Deleted username success - UPDATED" for form_data in this_form_accessor.iter_forms(form_ids):
# todo: icon by mime-type </s> basename = file.basename	async_callback def async_callback(self, file, size, pixbuf): if not file.dir == self.dir: def update(model, path, iter): if model[iter][BASENAME_COL] == basename:
#todo: add method </s> try:	onPlayBackStarted result = utilities.kodiJsonRequest({'jsonrpc': '2.0', 'method': 'Player.GetItem', 'params': {'playerid': 1}, 'id': 1}) logger.debug("[traktPlayer] onPlayBackStarted() - %s" % result) _filename = self.getPlayingFile() except:
# todo add test for reversed endianning </s> assert [] == the_group.signals	test_signalgroup_empty
# todo: find a better random value </s> must have three attributes:	AccessPoint class AccessPoint(object): :attr:`properties` is a dict where keys are property names as strings, and value are :class:`kalamar.property.Property`
# todo: drape topography? </s> self.nsrc = nsrc	genLocs_2D self.SrcLoc = SrcLoc self.RxLoc = RxLoc
# todo - remove the following once biosql bug 2839 is fixed. </s> return self.execute_and_fetch_col0(	list_biodatabase_names def list_biodatabase_names(self):
# todo: test for last revision on first page. </s> res = self.app.get(offset)	test_purge def test_purge(self): assert 'No revision id specified' in res
# todo: import from `py-evm` if possible?.. </s> if not should_run_slow_tests():	blockchain_fixture_mark_fn def blockchain_fixture_mark_fn(fixture_path, fixture_name, fixture_fork): for slow_test in SLOW_TESTS: return pytest.mark.skip("skipping slow test on a quick run") break
# todo make sure this works </s> def creategroup(group):	createGroup
# todo - needs tests </s> "stripe_public_key": settings.stripe_public_key,	payments_settings@8 def payments_settings(request): "PLAN_CHOICES": settings.PLAN_CHOICES,  # possibly nuke "PAYMENT_PLANS": settings.PAYMENTS_PLANS  # possibly nuke
# todo check the op returned a view </s> print >> file, '  total time spent in calling the vm %es (%.3f%%)' % (	summary_function val = 0 if self.call_time > 0: self.vm_call_time, val) val = 100
# todo(mattjj,levskaya): re-enable when test failure is sorted out </s> args_maker = lambda: [rng(shape, dtype)]	testCountNonzero rng = jtu.rand_some_zero() onp_fun = lambda x: onp.count_nonzero(x, axis) self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True) self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
# todo find out what is best used here! </s> name="weights", choices=["uniform", "distance"], default="uniform"))	KNearestNeighborsRegressor n_neighbors = cs.add_hyperparameter(UniformIntegerHyperparameter( name="n_neighbors", lower=1, upper=100, default=1)) p = cs.add_hyperparameter(CategoricalHyperparameter( name="p", choices=[1, 2], default=2))
# todo: switch _ignore_connection_aborted for _ignore_transmission_error, or provide retry mechanism </s> return 100  # todo select count(*) from cases -- but watch out for partially finished case	SessionInfo return 100  # TODO upgrade database format to store this info @property def test_case_data(self, index): Args:
# todo: sublime text seems to ignore the 'extend' param to ctrl+d, so disable it. </s> vi_cmd_data['motion']['args'] = {'mode': vi_cmd_data['mode']}	vi_big_m vi_cmd_data['motion']['command'] = 'vi_big_m'
# todo implement. </s> self.dataset_rdd = rdd	SparkModel super(SparkModel, self).__init__(keras_model, data, optimizer, master_port) self.num_workers = num_workers def train(self, parameters):
##todo : even/odd </s> def parent_position(self, pos):	parent_position
# todo: remove when we stop supporting python < 3.5 </s> 3d array of quadruplets to predict, with each row corresponding to four	_QuadrupletsClassifierMixin ---------- quadruplets : array-like, shape=(n_quadruplets, 4, n_features) or \ points, or 2D array of indices of quadruplets if the metric learner uses a preprocessor.
# todo: use ids in db, instead of objects! </s> self._notifymodification()	setAffiliation def setAffiliation(self, af):
# todo get from cache </s> font_size = int(view.settings().get('font_size', 11))	calc_cursor_position offset[1].append(self.get_setting('imesupport_offset_y')) p = add(p, (sum(offset[0]), sum(offset[1]))) sublime.status_message('IMESupport: ' + str(p) + repr(offset)) return (int(p[0]), int(p[1]), font_face, font_size)
# todo: split is int </s> self.session.query(feature).delete(synchronize_session="fetch")	clear_all logger.info("Clearing ALL Features and FeatureKeys.")
# todo(devcamcar): this assert should be more specific. </s> return self._client(username='foo',	foo_client password='foo2', tenant_name='BAR')
# todo equip `unique()` with a tolerance </s> assert len(parts) == 4	_read_facet flt = numpy.vectorize(float) for k in range(3): assert parts[0] == 'vertex' facet[k] = flt(parts[1:])
# todo: test coverage for this branch. </s> 'action': 'update',	update_activate_url 'email': self.email,
pass  # todo </s> registry_infile=registry_infile,	inspect _ensure_config_root(config_root) _initialize_blockchain(poa, provider_uri, emitter, ignore_solidity_check) download_registry=not bool(registry_infile)) paint_deployer_contract_inspection(emitter=emitter,
{# todo do not show this. #}''') </s> assert '{}:\n  * [  1] fixme this is a comment.'.format(template_path) in out	test_with_template_sub_dirs out, err = capsys.readouterr()
# todo handle numpy repr differences </s> return pen	colortuple if isinstance(pen, (QPen, QBrush)): color = pen.color()
# todo delete? we should search for valid parser </s> is very useful for refactoring (renaming), or to show all usages of a	usages def usages(self, additional_module_paths=()): Return :class:`classes.Definition` objects, which contain all variable. .. todo:: Implement additional_module_paths
# todo: check </s> dhis2_api = dhis2api(settings.dhis2_host, settings.dhis2_username, settings.dhis2_password)	get_children_only_theirs Returns a list of child entities that don't have cchq_case_id set
# todo: second loop can be removed with using segment_axis. no large gain. </s> return f < f	should_continue
# todo: weather data source changed, temporarily disable, will enable it later when new data source is available. </s> continue	_on_action_received from_station_idx: int = action.from_station_idx to_station_idx: int = action.to_station_idx station: Station = self._stations[from_station_idx] station_bikes = station.bikes
# todo wonder if old takeouts could contribute as well?? </s> self.start = 0	Window def __init__(self, it): self.it = it self.end = 0 def load_to(self, to):
# todo yield </s> repo_props['url'] = url	__call__@197 repo_name.replace("/", "-")) if pushurl: repo_props['pushurl'] = pushurl
# todo: docstring </s> normalize_rows=self.normalize_users)	_create_user_feed_dict if not sp.issparse(user_features_matrix): raise Exception('User features must be a scipy sparse matrix') feed_dict = {self.tf_n_users: n_users, self.tf_user_feature_indices: user_feature_indices,
# todo: remove in 21.08 </s> log.debug("completed generating cache")	generate_cache_text@57 if os.path.exists(each_path): write_cache_text(each_path, text_file) else: LOG.debug("Cache file 'cache_text.txt' already exists")
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> sampleparameters = {	Test_AcquireTokenWithUsernamePassword class Test_AcquireTokenWithUsernamePassword(unittest.TestCase): "tenant" : "common", "authorityHostUrl" : "https://login.windows.net",
# todo implement this function </s> set a system setting to a certain value	set_system_setting :param setting_name: name of setting, could be volume, brightness :param setting_value: value of setting
# todo: remove in 0.24 </s> def hess(x, p):	hess
# todo: give a vanilla example </s> ))	feca@26 fraction = np.append(fraction, np.min( appliance_energy_predicted/total_energy_predicted, return fraction
# todo:  check this </s> account_id = self.default_account_id	_get_account_defaults if network is None: network = self.network.network_name qr = self._session.query(DbKey).\ filter_by(wallet_id=self.wallet_id, purpose=self.purpose, depth=3, network_name=network)
# todo check that it actually does something useful </s> assert_almost_equal(reconstruction_error,	test_isomap_reconstruction_error mode='distance').toarray() K_iso = centerer.fit_transform(-0.5 * G_iso ** 2) clf.reconstruction_error())
# todo: batch this up properly once we care about multi-project rules. </s> updates a subscription to a snuba query.	update_snuba_subscription :param query: An event search query that we can parse and convert into a set of Snuba conditions
# todo: this algorithm has lots of room for improvement </s> 'connection': 'close'	Referer@51 gen_headers = { 'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 6.1; rv:2.2) Gecko/20110201', } verbout(O,'Making request with tampered headers...')
# todo: add here additional variants for other reprog_controls </s> activity_text = ', '.join(('%d=%d' % (d, a)) for d, a in activity if a > 0)	_print_receiver activity = [(d, ord(activity[d - 1:d])) for d in range(1, receiver.max_devices)]
z_t = gan.encoder.z #todo </s> return {	BatchSampler inputs_t = gan.inputs[0]
# self.todolist was loaded with old identifier settings </s> self.assertequal(self.errors, "")	test_list17 self.assertFalse(self.todolist.dirty) self.assertEqual(self.output,
# todo: wrap backend call in error handling. </s> returns a :class:`mopidy.models.tltrack` or :class:`none`.	get_current_tl_track def get_current_tl_track(self):
# todo: set globals to user module </s> thisfile.close()	writeFile thisFile.write(self.data)
pass  # todo - should this do something </s> radiomgmtdnsmanual = self.builder.get_object("radiomgmtdnsmanual")	on_acceptmgmtinterface_clicked change = True radiomgmtipmanual = self.builder.get_object("radiomgmtipmanual") radiomgmtdnsdhcp = self.builder.get_object("radiomgmtdnsdhcp") ip = self.builder.get_object("txtmgmtip").get_text()
# todo: incref? </s> ('data_offsets', types.cpointer(offset_typ)),	StringArrayPayloadModel def __init__(self, dmm, fe_type): members = [ ] models.StructModel.__init__(self, dmm, fe_type, members)
# :todo: implement test. </s> self.assertequal(tryte[2].value, -1)	test_init_mixed_types self.assertEqual(tryte[0], Trit(1)) self.assertEqual(tryte[1], Trit(1))
# todo add test for this </s> arrs_aug = np.array(arrs_aug, dtype=input_dtype)	_augment_arrays_by_samples if n_shapes == 1:
err = str(e)  #@todo: recheck in 0.4.10 </s> raise notimplementederror	handleFree def handleFree(self, pyfile): else: self.fail(_("Required premium account not found"))
# todo: this should be a separate test </s> return webob.request.blank('/an-error?foo=bar')	MiddlewareTestCase return TempStoreClient() @fixture def test_captures_error_in_iteration(self): iterable = ErroringIterable()
verifying_key = bytes.fromhex(verifying_key)  # todo: move / validate </s> emitter.message(f'successfully imported card.')	import_card def import_card(filepath): emitter = StdoutEmitter()
# todo check error message here. </s> relationship in the url yields an error.	test_missing_relation person = self.Person(id=1) article = self.Article(id=1)
# todo: ... </s> amount_paid: float, stripe_token: str):	StoreService class StoreService: @staticmethod pass @staticmethod
if self._ndim == 3: # todo: use hasz </s> return self.array	coordinates @property
# todo legacy method to be removed/refactored </s> from corehq.apps.orgs.models import organization	get_organizations return filter(None, [Organization.get_by_name(org) for org in self.organizations])
# todo counts as yes if vhnd was not available </s> growth_monitoring_session_12 = format_percent(1, 0)	HealthStatus growth_monitoring_session_9 = format_percent(1, 0) growth_monitoring_session_10 = format_percent(1, 0) nutritional_status_normal = format_percent(1, 0) nutritional_status_mam = format_percent(1, 0)
# todo policyuniverse can't expand resource wildcards so further thought is needed here </s> attachment_count=policy["attachmentcount"],	load_policies CREATE_DATE=str(policy["CreateDate"]), POLICY_UPDATE=str(policy["UpdateDate"]), AWS_ACCOUNT_ID=current_aws_account_id, aws_update_tag=aws_update_tag
# todo fix. </s> cmp_result = self.__compare_contexts(ctx_init, x86_ctx_out, reil_ctx_out)	test_cmova asm = ["cmova eax, ebx"] ctx_init = self.__init_context() if not cmp_result: self.__save_failing_context(ctx_init)
# todo: accept multiple trade_ids (just extend list below (+ extend params?)) </s> if min_price:   params['micr'] = min_price	searchAuctions } if level:       params['lev'] = level if max_price:   params['macr'] = max_price if min_buy:     params['minb'] = min_buy
# todo: is this the original intent of this test? </s> target_filepaths.append(file_path)	_get_list_of_target_paths for directory, sub_directories, files in os.walk(targets_directory): for _file in files: return target_filepaths
# todo: normal gl requires these lines, es 2.0 does not </s> prog.uniforms['u_color'] = 0.0, 1.0, 1.0	on_paint@112 gl.glEnable(GL.GL_POINT_SPRITE) with self._program as prog: prog.attributes['position'] = boids['position'] prog.draw_arrays(gl.GL_POINTS)
self.reads_matched += 1  # todo move to filter class </s> def __call__(self, read):	DoubleEncoder Double-encode colorspace reads, using characters ACGTN to represent colors. def __init__(self): read = read[:] read.sequence = read.sequence.translate(self.double_encode_trans)
# todo: check if optional or required </s> for tr in table.find_all('tr')[1:]:	parse_table head = [td.text for td in table.tr.find_all('td')] row = namedtuple('{}TableRow'.format(h4.text), ','.join(head)) t.append(row(*[td.text for td in tr.find_all('td')])) return t
# todo: remove this </s> return self.bounds[1]	max @property
# todo: this should now raise an exception </s> result_before = model.predict(np.zeros((1, 3)))	test_model_predict_different model = dqn.ModelWrapper( state_axes=1, action_size=2, batch_size=3, model=small_model result_before = np.copy(result_before) result_after = model.predict(np.ones((1, 3)))
# todo: algorithm to collect missing target counters </s> else:	flex_children anonymous = boxes.BlockBox.anonymous_from( box, child.children) anonymous = boxes.BlockBox.anonymous_from(box, [child]) anonymous.is_flex_item = True
# todo when there is already data in this packet </s> self._timeouts.append(handler)	update_activity length = len(self._timeouts)
# todo: log exception </s> except exception as e:	teardown@51 os.remove('tmp_report.json')
# todo: non standard, expects 299 and aux outputs </s> clf_module.is_classifier = true	mark_classifier clf_module = get_classifier_module(model)
# todo: clear last object inspector requests dictionary </s> def to_remote_path(path):	to_remote_path
# todo(tdurakov): remove dict to object conversion once rpc api version </s> instance_bdms=instance_bdms))	_get_host_volume_bdms context, instance.uuid, use_slave=use_slave) instance_bdms = [bdm for bdm in bdms if bdm.is_volume] return compute_host_bdms
# todo: implement this rpc service </s> def push_gradient(self, request, _):	PserverServicer return empty_pb2.Empty()
# todo: update the associated revision if there is one </s> with phlsys_fs.chdircontext("phab"):	_phabUpdateWithExpectations runCommands("git fetch origin -p") processUpdatedRepo(self.conduit, "phab", "origin")
# todo(a_d) event is unused </s> this.system_link = parent.children['system']  # system label in main window	plugin_app this.station_link = parent.children['station']  # station label in main window this.system_link.bind_all('<<InaraLocation>>', update_location)
return # todo raise error </s> logger.debug(u'%s.seeked signaled', player_iface)	Seeked def Seeked(self, position):
#todo - use sql for this, much more efficient! </s> return self.execute_and_fetch_col0(	list_biodatabase_names def list_biodatabase_names(self):
# todo implement .!{cmd} (ex shell out) test for windows </s> def test_command_output(self):	TestExShellOutNoInput def tearDown(self): self.view.window().run_command('show_panel', {'panel': 'output.UnitTesting'}) output_panel = self.view.window().get_output_panel('vi_out') self.view.run_command('ex_shell_out', {
# todo: min() is to support mono sources with default channel mapping. handle this better, and give a warning if an explicit mapping is too big. </s> else:	_at_most_one return items[0] elif len(items) == 0: raise ValueError(u'Exactly one %s must be provided, not %i' % (name, len(items)))
# todo: remove in 1.2 </s> list of the leaf nodes.	_get_leaves Returns ------- leaf_ptr = self.dummy_leaf_.next_leaf_ leaves = []
# todo(ytknzw): add more specific assertion with the test case. </s> raise valueerror	fail_objective
# todo(shoyer): test fails on tpu </s> return api.jit(f)(pos, inc)	loop_body effect[0] = True pos, count = state
# todo: merge the logic to map_partitions </s> def set_index(self, other, **kwargs):	set_index from .shuffle import set_index return set_index(self, other, **kwargs)
# todo add code </s> p.save()	test_creation def test_creation(self): self.assertEqual(Poll.objects.count(), 2) # Cause setup created one already
# todo: zglobs.to_filespecs does not match files in the current directory when a pattern </s> return {s.split(' ')[0].replace(self.path_prefix, '')	_extract_exclude_output def _extract_exclude_output(self, test_name): for s in stdout_data.split('\n') if s.startswith(self.path_prefix)}
#todo(jogo): make the following doctests pass: </s> r"""check multi line docstring end.	hacking_docstring_multiline_end @flake8ext OpenStack HACKING guide recommendation for docstring: Docstring should end on a new line
# todo: abstract py 2/3 check out to utils </s> return (	root_down (not endpoint.live) or endpoint.https_bad_hostname or
# todo: remove when old stats are removed </s> def test_data_tool_store(store0):	test_data_tool_store assert data_tool.get() is None assert data_tool.get(store0.__class__) is StoreDataTool
# todo(rakhmerov): implement. </s> self._await(lambda: self.is_execution_success(exec_db.id))	test_succeed def test_succeed(self): exec_db = db_api.get_execution(exec_db.id) self.assertEqual(1, len(exec_db.tasks))
node_list = ursula.batch_from_bytes(nodes, federated_only=self.federated_only)  # todo: 466 </s> load_seed_nodes: bool = true,	NodeConfiguration return node_list def load_bootnodes(self, retry_attempts: int = 3, retry_rate: int = 2,
#todo: manage this </s> self._set_params()	generate_docs def generate_docs(self): self._set_return() self._set_raw()
# todo: proper pluralization </s> for f in list(self.fields.keys()):	InvoiceNameForm class InvoiceNameForm(InvoiceAddressForm): def __init__(self, *args, **kwargs): if f != 'name_parts': del self.fields[f]
# todo: test. </s> bb_lower_half = self._disassemble_bb(address, bb.end_address, symbols, [])	_split_bb def _split_bb(self, bb, address, symbols): bb_upper_half.direct_branch = address return bb_lower_half, bb_upper_half
# todo(stephenfin): there are some issues here that may </s> self, context, instance, guest, network_info=none):	_attach_direct_passthrough_ports if network_info is None: network_info = instance.info_cache.network_info
# todo: add exception handling </s> if 'listened_at' not in listen:	validate_listen def validate_listen(listen, listen_type): log_raise_400("JSON document must contain the key listened_at at the top level.", listen) try:
# todo: this property is only used by the mvpformindicatorpillow </s> def get_failure_response(doc):	get_failure_response return OpenRosaResponse( message=doc.problem,
# todo ... </s> if not is_valid_defname(arg):	cpreprocess_evaluate_ifdef def cpreprocess_evaluate_ifdef(state, arg): state.error("preprocessor: '" + arg + "' is not a valid macro name") return False
# todo: should we add a test case for an empty context stack? </s> query the stack for a non-dotted name.	_get_simple result = _NOT_FOUND for item in reversed(self._stack):
'''todo: add docs''' </s> def discrete_columns(self):	discrete_columns @property
# todo(morgan): rework this to not need an explicit token render as </s> self.project_tag_name = kwargs.pop('project_tag_name', none)	RequestContext class RequestContext(oslo_context.RequestContext): def __init__(self, **kwargs): self.is_delegated_auth = kwargs.pop('is_delegated_auth', False) self.trust_id = kwargs.pop('trust_id', None)
# todo generator </s> in subdirectories within a dataset as always done automatically. an	Drop a common (super)dataset is given explicitly, the given paths are interpreted relative to this dataset. optional recursion limit is applied relative to each given input path. By default, the availability of at least one remote copy is verified,
# todo: wells if display config has more than one column </s> bar = ('username', 'userid')	mycmp def mycmp(x, y): if x in foo and y in foo: return -1 if foo.index(x) == 0 else 1
def __init__(self, p_args, p_todolist, #pragma: no branch </s> the -e flag. use -x to also process todo items that are normally invisible	help It is also possible to deprioritize items as complete with an expression using
# todo: write this method if possible </s> return res	getrawtransaction def getrawtransaction(self, txid):
# todo: truffle change begin </s> return ''.join(['%.2x' % i for i in self.digest()])	hexdigest
# todo: test </s> def get_type_enum_overload(arr_typ):	get_type_enum_overload typ_val = _h5_typ_table[arr_typ.dtype] return lambda a: np.int32(typ_val)
# todo: remove "get_" from the name </s> index = none	edge_index print("Warning: Edge key '{}' not found.".format(edge_type))
# todo: make this configurable </s> :return auth_item: a list of hashed otp values	MachineApplication :param serial:     the serial number of the token. :param challenge:  n/a ret = {} if token_type.lower() == "hotp":
# todo: using json.dumps because node_to_use.registered_meta's values are </s> def node_register_page(*args, **kwargs):	node_register_page @must_be_valid_project @must_be_contributor # returns user, project user = kwargs['user'] node_to_use = kwargs['node'] or kwargs['project']
# todo this is a bit jankey to be honest </s> settings.integration_plugin_setting[slug] = plugin_setting	PluginConfig for slug, plugin in settings.INTEGRATION_PLUGIN_LIST.items(): if plugin.mixin_enabled('settings'): settings.INTEGRATION_PLUGIN_SETTINGS.update(plugin_setting) if (not settings.INTEGRATION_APPS_LOADED) and InvenTreeSetting.get_setting('ENABLE_PLUGINS_APP'):
# todo: proper java error? </s> def call_static_float_method(self, mu, env):	call_static_float_method raise NotImplementedError()
# todo: still hardcoded. </s> containers = containerregistry.getinstance().findinstancecontainers(id=variant_id)	setActiveVariant @pyqtSlot(str) old_variant = Application.getInstance().getGlobalContainerStack().findContainer({"type": "variant"}) if old_variant:
# todo remove in v8 </s> def emit(self, record: logging.logrecord) -> none:	StrictModeExceptionHandler if record.levelno >= logging.WARNING: raise ApplicationWarning(self.format(record))
# todo: remove debug statements after fixing in-toto/in-toto#171 </s> verifies the passed signature against the passed content using the	gpg_verify_signature def gpg_verify_signature(signature_object, pubkey_info, content): passed public key, or one of its subkeys, associated by the signature's keyid.
# todo only return these if there have been changes </s> @sync_performer	FakeYum class FakeYum(object): Enough of a fake implementation of yum utilities to test def _perform_download_packages_from_repository(self, dispatcher, intent): See :class:`DownloadPackagesFromRepository`.
# todo generator </s> --------	Drop before file content is dropped. As these checks could lead to slow operation (network latencies, etc), they can be disabled. Drop all file content in a dataset:: ~/some/dataset$ datalad drop
f.write(self.pastie_content.encode('utf8'))  # todo error checking </s> return none	getRandomUserAgent def getRandomUserAgent(): if yamlconfig['user-agent']['random'] and yamlconfig['user-agent']['list']:
# todo(wb): it doesn't sense for this to be here if varianttypenode </s> return "processinginstructiondatanode(offset=%s, length=%s, token=%s)" % \	ProcessingInstructionDataNode return "ProcessingInstructionDataNode(buf=%r, offset=%r, chunk=%r, parent=%r)" % \ (self._buf, self.offset(), self._chunk, self._parent) (hex(self.offset()), hex(self.length()), hex(0x0B)) def xml(self, substitutions, cleanup=False):
# todo in python 2.7 or later, this should be a set comprehension. </s> instance of a sqlalchemy model.	DeserializationException pass
# todo: this should be abstracted into a property/method or something </s> def __init__(self, item):	ContentProxy >> page = Page.objects.all()[0] >> page.content.main self.item = item def __getattr__(self, attr):
log("dispersy.log", "handled-barter-record") # todo: maybe move to barter.log </s> if __debug__: dprint(message)	allow_signature_request Currently I always sign for testing proposes assert message.name == u"barter-record" is_signed, other_member = message.authentication.signed_members[0] if other_member == self._my_member:
# todo if update_variable_bounds = false, this will not work as intended. </s> pass	FBBTException
# todo: revise this function and try to speed it up!!! </s> pass	from_vector def from_vector(self, vectorized_instance):
# temporary hack to get pf api working. todo - remove </s> return httpresponsebadrequest("no schema matches 'resolution'")	daily_report def daily_report(request): formdefs = FormDefModel.objects.filter(target_namespace__icontains='resolution') response = read(request=request, ids=[ formdefs[0].pk ], \ index='day', value=['count'])
# match apache formatting. todo: when we move to dsa dirnodes and </s> {	TahoeLAFSRequest if self.method == 'POST': self.fields = FieldStorage( name.lower(): value[-1] for (name, value)
# todo: fast initialization of alpha terms, preferably with fugacities </s> excess heat capacity of the liquid phase, [j/mol/k]	CpE_l Returns ------- Notes -----
# todo discont: use offsets instead (note need for int conversion) </s> if t in keymap:	__generate_input_and_label dstr = ' disabled="disabled"' s  = indent+'    <input id="%s%s" type="radio" name="%stype" value="%s" %s/>' % (prefix, t, prefix, t, dstr) key_offset= dt.lower().find(keymap[t].lower()) else:
# todo (b/162341937) remove once it's fixed. </s> this is needed because	create_dataset_from_data_producer def create_dataset_from_data_producer(producer, params): - The label needs to be extended to be used in the loss fn - We need the same inputs for training and eval so adding fake inputs
# todo(nakago): check why tolerance is high </s> return nfp(out_dim=out_dim)	model @pytest.fixture
# todo: must be implemented </s> def should_fetch_kindlegen():	should_fetch_kindlegen
# todo manage tangent? </s> kf.interpolation = 'constant'	set_interpolation if interpolation == "LINEAR": kf.interpolation = 'LINEAR' elif interpolation == "CUBICSPLINE": kf.interpolation = 'BEZIER'
#     todo </s> cache_path = self.cache.get_path_or_dummy()	chmod os.chmod(cache_path, mode) return
# todo: support indexhierarchy </s> own_data=own_data,	set_index return self.__class__(blocks, columns=columns, own_columns=own_columns, own_index=True,
# todo: > 16.04: remove these </s> else:	__update_dataset if 'visible' in payload: anon_allowed_payload[ 'visible' ] = payload[ 'visible' ] hda = self.hda_manager.get_owned( self.decode_id( id ), trans.user, current_history=trans.history ) check_state = not payload.get( 'deleted', False )
# todo: do we need to multiply the offset by sizeof wchar? </s> str2 = params["psz2"].lower()	hook_StrCmpIW }) def hook_StrCmpIW(ql: Qiling, address: int, params): return __cmp__(str1, str2)
# todo: implement </s> yield "k%c%c%c%cvk" % (pchr[i], pchr[j], pchr[k], pchr[l])	filenames for j in range(i, 6): for k in range(j, 6):
except exception:  # todo: refactor this... </s> def test_init(self):	test_init
# todo: refactor </s> df_typ = self.typemap[df_var.name]	_run_call_len if len(df_typ.columns) == 0: return [ir.Assign(ir.Const(0, lhs.loc), lhs, lhs.loc)]
# todo: this decompose is used because of cache </s> def __isub__(self, other):	__isub__
# todo uncomment when gr-10346 will be fixed </s> return 1 + count_set_bits(n & n - 1) if n else 0	count_set_bits
# todo: may test file contents </s> temp = tempfile.namedtemporaryfile(delete=false)	PluginCsvTestCase table = rows.import_from_csv(fobj, encoding=self.encoding) self.assert_expected_table(table) self.files_to_delete.append(temp.name) rows.export_to_csv(utils.table, temp.name)
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> self.fail("not implemented")	Test_AcquireTokenWithUsernamePassword print(tokenResponse)
#for f in fields_manfcat:#todo </s> pass	IdenticalComponents
# todo: has some issues with datetime and sqlite </s> def test_sponsor_api(self):	test_sponsor_api
# todo fix bug for not identifying unused variables in nested exceptions see issue #4391 </s> __a__ = 24 # [unused-variable]	test_unused_with_prepended_underscore _a_ = 42 # [unused-variable]
# todo: what about normalize? </s> return 1 # file type "csv"	_get_file_type_index_from_filename idx = ALLOWED_EXTENSIONS.index(extension) if idx < 3:
# todo files listed here may not belong to the given camera </s> target_dir = camera_config.get('target_dir')	cleanup_movies if preserve_movies == 0: return # preserve forever _remove_older_files(target_dir, preserve_moment, exts=_MOVIE_EXTS)
#todo classes broken </s> plot(self.config, np.vstack(stacks), sample_file)	GANWebServer print("sample is ", sample) print(sample.shape) def sample_grid(self, sample_file): grid_sampler.sample(sample_file, self.sess, self.config)
# todo(gibi): remove this when live migration is fully supported and </s> resource provider that also has that trait in placement.	test_flavor_image_traits_based_scheduling def test_flavor_image_traits_based_scheduling(self): rp_uuid = self._get_provider_uuid_by_host(self.compute2.host) self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX',
# todo only update transform </s> self.changed_mesh.append(obj)	ObjectCache if obj.is_updated: if obj.type in ["MESH", "CURVE", "SURFACE", "META", "FONT", "EMPTY"]: else: self.changed_transform.append(obj)
#todo?# self.asserttrue(greps(err, "unit zzz.service not for --user mode")) </s> cmd = "docker rm --force {testname}"	test_6133_run_default_services_from_single_service_saved_container ExecStart=/usr/bin/testsleep 111 [Install] sx____(cmd.format(**locals())) cmd = "docker run --detach --name={testname} {image} sleep {sometime}"
# todo: without the lstrip, we get an extra empty line at the beginning. is </s> pangocairo.update_layout(cairo_context, pango_layout)	show_first_line def show_first_line(cairo_context, pango_layout, hinting): lines = pango_layout.get_lines_readonly() PangoCairo.show_layout_line(cairo_context, lines[0])
# todo: do we need to add a slicer. does the data always come as numpy array with time series in axis=1. </s> splitted_expected_outputs = self._validation_split(expected_outputs)	validation_split :return: validation_data_inputs, validation_expected_outputs splitted_data_inputs = self._validation_split(data_inputs) return splitted_data_inputs, splitted_expected_outputs return splitted_data_inputs
# todo: could just build up array of class/kind </s> def _on_window_deactivated(self, next_handler, ev, data):	_on_window_deactivated self.dispatch_event('on_deactivate') carbon.CallNextEventHandler(next_handler, ev)
# todo(ihrachys) adopt to v3 </s> self.addcleanup(self.admin_client.delete_floatingip,	test_create_floatingip_with_specified_ip_address else: self.fail("Could not get an unused IP after 100 attempts") created_floating_ip['id']) self.assertIsNotNone(created_floating_ip['id'])
# todo: verify behavior </s> self.send_request()	test_few (11, 'pydevd.eggs'), (12, ''), received = self.vsc.received self.assert_vsc_received(received, [
# todo: symlink or whatever annex does, since annexes beneath </s> return	result_renderer_cmdline from datalad.ui import ui if not res: msg = "{n} {obj} uninstalled:\n".format( obj='items were' if len(res) > 1 else 'item was',
# todo: do we still need to support ../shed_tools when running_from_source? </s> container_types_to_destinations = collections.defaultdict(list)	_set_enabled_container_types for destinations in self.job_config.destinations.values(): for destination in destinations:
# :todo: implement test. </s> filter_type = findtransactionsrequestfilter	FindTransactionsRequestFilterTestCase skip_value_check = True def test_pass_all_parameters(self):
# todo (aron): add i18n by varying the language of the topic tree here </s> def obj_delete(self, request):	obj_delete
# todo(henry-nash): the test above uses list_projects_for_user </s> {'params': {'user': 0, 'project': 0, 'effective': true},	test_inherited_role_grants_for_user {'user': 0, 'role': 2, 'domain': 0, 'inherited_to_projects': True}], 'results': [{'user': 0, 'role': 0, 'project': 0}, {'user': 0, 'role': 2, 'project': 0,
# todo: make this backend dependent. </s> conf = msg.train_config	from_proto def from_proto(self, msg): task = rv.TaskConfig.from_proto(conf.task) backend = rv.BackendConfig.from_proto(conf.backend)
# todo placeholder; implement </s> self.name = name	RoleInfo "worker_infos",  # List[WorkerInfo] ] self.role_world_size = role_world_size self.local_world_size = local_world_size
# todo: email request_user_id </s> curation[request_user_id] = user.get('_id')	CuratedFolder if enabled and not oldEnabled: curation[ENABLE_USER_ID] = user.get('_id') for doc in folder['access']['users'] + folder['access']['groups']: if doc['level'] == AccessType.WRITE:
# todo: this is seriously intensive and takes a _long_ time to run. </s> if es is none:	index_post def index_post(post, bulk=False, force_insert=False, es=None): from forums.models import Post es = elasticutils.get_es() index = settings.ES_INDEXES['default']
# todo docstring args </s> def none_found(s):	none_found
# todo: need to add counter </s> return [user_ids]	_get_user_ids user_ids = self.convert_to_user_id(user_ids)
# todo: could cache the results of this for speed </s> for loc in data:	import_locations def import_locations(domain, f): data = list(csv.DictReader(f)) messages.extend(import_location(domain, loc)) return messages
# todo improve precision </s> return	__init__@38 self.weights, self.points = concat( zero(A), pm([B, u, -v], [B, v, u], [C, r, -s], [C, r, s])
# forward all other methods. todo(l.zou): could use a proxy to automate these </s> def apply_gradients(self, *args, **kwargs):	apply_gradients
#set limit to 25 --> currently hardcoded --> todo: add a setting for this ? </s> currentsetartsql =  "select type, url from art where media_type = ? and media_id = ? and url != ''"	addBoxsetToKodiLibrary result = cursor.fetchone() if result != None: cursor.execute(currentsetartsql, ("set", setid)) existing_type_map = {}
# todo: cleaning of facts should eventually become part of taskresults instead of vars </s> variables[i] = delegated_vars[i]	_get_connection if isinstance(delegated_vars, dict): for i in delegated_vars: conn_type = self._play_context.connection connection = self._shared_loader_obj.connection_loader.get(
# todo: let's see if we can find sane versioning for `latest` from upstream </s> f'k8s.{resource_type}.query', [	parse_k8s_resource_tag if resource_type not in ('configmap', 'secret'): return ['metadata.namespace', '=', release_data['namespace']], ['metadata.name', '=', resource_name] ]
try: #todo: fix brodcast issue if different </s> return false	is_none def is_none(val): else: return val is None
# todo i/o </s> raise notimplementederror	score_po
codecs.decode('5050827d08000000d00745b2cf451b00', 'hex'), # tcp random cmd_ack_ok todo: generate proper sequenced response </s> socket.return_value.sendto.assert_called_with(codecs.decode('e80317fc00000000', 'hex'), ('192.168.1.201', 4370))	test_udp_connect socket.return_value.recv.return_value = codecs.decode('d007fffc2ffb0000','hex') # tcp CMD_ACK_OK zk = ZK('192.168.1.201') conn.disconnect() socket.return_value.sendto.assert_called_with(codecs.decode('e903e6002ffb0100', 'hex'), ('192.168.1.201', 4370))
# todo: mv this to network; cache directly on network. </s> return self.expand_repertoire(directions[past], purview, repertoire,	expand_cause_repertoire def expand_cause_repertoire(self, purview, repertoire, new_purview=None): new_purview)
# todo: does path make sense? or a separate table? </s> engine = create_engine('postgresql:///agdc', echo=true)	_ingest_one connection = engine.connect() _model.ensure_db(connection, engine)
# todo in python 2.7 and later, this should be a dict comprehension. </s> instance of a sqlalchemy model.	DeserializationException class DeserializationException(Exception):
# todo: make this cache preparation configurable </s> text = re.sub(r'>\s*\n\s*<', newline_between_tags, text)	_carefully_strip_whitespace text = re.sub(r'>\s{2,}<', SPACE_BETWEEN_TAGS, text) return text
# todo: untested </s> def to_cpu(self):	PytorchWrapper self._model.load_state_dict(torch.load(filelike)) def to_gpu(self, device_num): raise NotImplementedError def resize_output(self):
@pytest.mark.skip()  # todo: fix this </s> response = await client.conversations_list(exclude_archived=true)	TestWebClient client = self.async_client
#todo: another idea: </s> raise exception("unknown special color")	get_pcolor_for_player elif self.special_id == 2 or self.special_id == self.player_color: return 16 * player + self.base_color
# todo: make test method </s> droid.stoplocating()	test_gps try: return event_loop()
# todo: make it optional </s> def run_with_slack(runner, test_at_the_end: bool = false):	run_with_slack runner.run(test_at_the_end=test_at_the_end)
# todo: make it really async. </s> def parse_resource(self, encryptions):	TransparentDataEncryptions self.resource_group_name, self.server_name, self.database_name)
# todo: determine test dependencies from plaso.dependencies. </s> u'# one per line.',	GIFTInstallScriptWriter u'DEBUG_DEPENDENCIES="python-guppy";', u'', u'DEVELOPMENT_DEPENDENCIES="python-sphinx', u'                          pylint";',
# todo(tr3buchet) - remove comment in multi-nic </s> def get_public(self):	get_public
# todo: figure out how extend stuff works, a bit confused again. </s> if self.leftpanel:	_sharey_panels self._sharey_setup(self.leftpanel, 3) left = self.leftpanel or self
# todo: handle parser errors </s> return typefind.peek(0, 11) == b'[playlist]\n'	detect_pls_header def detect_pls_header(typefind):
# todo: for backward compatibility only, remove if not used anymore </s> return self._get_by_key(key, self.vm)	get_vm self.vm = v
# todo: support pairwise arg </s> super(rollingtype, self).__init__(	RollingType self.on = on self.selection = selection name="RollingType({}, {}, {}, {})".format( df_type, on, selection, explicit_select))
kwargs.get('crypt', 'aes'),  # todo: use the same channel for crypt </s> def master_pub(self):	master_pub Return the master publish port return 'tcp://{ip}:{port}'.format(ip=self.opts['master_ip'],
# todo fix bug for not identifying unused variables in nested exceptions see issue #4391 </s> __never_used = 42	test_unused_with_prepended_underscore dummy = 24 _a_ = 42 # [unused-variable]
# todo: implement this </s> return self._get_usages()[1]	get_max_memory
# todo:liberate - move this to a more generalized tag enhancement package? </s> if not scaled_file:	ReplacingImageWithThumbField data = super(ReplacingImageWithThumbField, self).clean(*args, **kwargs) scaled_file = scale_image(data.file, raise ValidationError(_('Cannot process image')) data.file = scaled_file
# todo remove this custom equality testing code when </s> 'fasta_id_whitespace_replacement_empty_str')	FASTAWriterTests 'fasta_id_whitespace_replacement_multi_char'), (whitespace_id_gen(), ]) self.blank_seq = BiologicalSequence('')
# todo: this needs rethinking, as currently we can only express </s> for d in self._find_deps(self._dependencies, deptype)]	dependencies def dependencies(self, deptype='all'):
# todo(yanase): implement maximization. </s> return self.storage.get_study_system_attrs(self.study_id)	system_attrs @property
# todo: remove in 1.3 </s> with pytest.warns(userwarning, match=warning_message):	test_redundant_bins X = [[0], [0], [0], [0], [3], [3]] kbd = KBinsDiscretizer(n_bins=3, strategy=strategy) kbd.fit(X) assert_array_almost_equal(kbd.bin_edges_[0], expected_bin_edges)
# todo: i don't like this interface. is there a standard python one? pubsub? </s> source file.	ImageSpec self.image_cache_strategy = StrategyWrapper(image_cache_strategy or self.image_cache_strategy) def apply(self, source_file): return ImageSpecFile(self, source_file) def _handle_source_event(self, event_name, source_file):
if sys.version_info[:2] < (3, 5): # todo </s> def test10(self):	test10 @raises_exception(TypeError, "Incomparable types '%s' and 'float' in expression: s.name < s.gpa" % unicode.__name__)
# todo test cases </s> self._new_sensor_alert = sensor_alert	set_new_sensor_alert
# todo: the absolute and fixed boxes in the floats must be </s> box.pango_layout = layout	add_word_spacing extra_space = justification_spacing * nb_spaces x_advance += extra_space elif isinstance(box, (boxes.LineBox, boxes.InlineBox)): box.position_x += x_advance
# todo: remove in cacheops 3.0 </s> get = handle_connection_failure(redis.strictredis.get)	SafeRedis
# todo not implemented yet </s> rows.append(n)	set_current_group_height_to_n for i in range(row_len - 2): if i < current_group_num: start = n for i in range(row_len - 2):
# todo: remove this deprecated function in release 1.0 </s> job_state = djangojob.objects.get(id=job_id).job_state	lookup_job def lookup_job(self, job_id: str) -> Union[None, AppSchedulerJob]: return self._reconstitute_job(job_state) if job_state else None except DjangoJob.DoesNotExist:
# todo: find a better way to return errors... </s> django_file = request.files['file']	process_upload overwrite   = upload_form['overwrite'].data == 'checked' if django_file.name.endswith('.zip'):
# todo ask for the correct attributes e.g state and private_ips </s> in most cases, it will return a true or false value.	__virtual__ Most often, it uses get_configured_provider() to determine if the necessary configuration has been set up. If the name of the driver used does not match the filename, then that name should be returned instead of True.
# todo make this check if writeable </s> set in the minion config file as ``pip.pip_bin``.	freeze The name (and optionally path) of the pip command to call. This option will be ignored if the ``env`` argument is given since it will default cmd = '{0} freeze'.format(_get_pip_bin(pip_bin, env)) return __salt__['cmd.run'](cmd).split('\n')
# todo: add information on optional / default components </s> tlv = envelope('tlv', gen=(asn1codecber.encode_tag_ws(t[0], 1, t[1]),	_to_ber_ws if ASN1CodecBER.ENC_LUNDEF: for t in reversed(self._tagc[:-1]): ASN1CodecBER.encode_len_ws(-1), TLV,
return {}  # todo return none, somehow </s> @property	HTTP @property def broken(self): def local_port(self): raise NotImplementedError
# todo remove below workaround for double actions </s> self.selectrow(row, addtoselected=true)	_row_label_left_click start, end = (cursor_row, event_row) \ if cursor_row < event_row else (event_row, cursor_row) else: self.SelectRow(event.Row, addToSelected=False)
# todo: print line number.  pass a span_id into this function? </s> def evalwordtostring(self, w, do_fnmatch=false, do_ere=false):	EvalWordToString
# todo: use valid_episodes.mask for mean </s> create_graph=true)	_product def _product(vector): kl = self.mean_kl(episodes) flat_grad_kl = torch.cat([grad.view(-1) for grad in grads]) grad_kl_v = torch.dot(flat_grad_kl, vector)
# todo(dtantsur): backwards compability hack, remove in the v release </s> boot_device = boot_devices.pxe	prepare_instance pxe_utils.build_service_pxe_config(task, instance_image_info, root_uuid_or_disk_id, else: pxe_utils.clean_up_pxe_config(task, ipxe_enabled=True)
#todo: show examples </s> return self.__class__ is op.__class__ and \	QueryOperator except KeyError: raise QueryOperatorException("{} doesn't map to a QueryOperator".format(symbol)) self.column.db_field_name == op.column.db_field_name and \ self.value == op.value
# todo are there more exceptions besides timeout? </s> if self.empty() or isinstance(result.request, messagecontainer):	_ContainerQueue a list won't be returned in said case. async def get(self): return result result = [result]
# todo: skipped due to gh-4436 </s> name='some', storage_name='some')	test_invalid_calls ds = Dataset(path).create() assert_raises(TypeError, ds.create_sibling_ria)
# todo: losing precision on double types </s> sub_types = [detect_shape_structure(p)	detect_shape_structure if param.type in SCALAR_TYPES: return 'scalar' for p in param.members] if all(p in SCALAR_TYPES for p in sub_types):
# todo chceck correct shapes for integration </s> arg_types = ('material', 'virtual', 'state')	AdvFluxDGTerm1D self.setup() name = "dw_dg_advect_flux" arg_shapes = {'material': 'D, 1', 'virtual': ('D', 'state'),
engine.execute(alala.table_data.insert().values(datas)) # todo chunks?? </s> r = tmap.get(cls, none)	_map_type float: sa.Float, datetime: IsoDateTime, if r is not None: return r
# todo: move to profile callbacks </s> def on_client_error(self, error_type, error):	on_client_error
## todo: decode </s> if isinstance(other, self.__class__): return self.hash > other.hash	__gt__ def __gt__(self, other):
# todo remove? </s> pass	AlreadyEvaluated
# todo: unify handling of text objects in one function. perhaps add state.args to merge with vi_cmd_data['motion']['args'] </s> return vi_cmd_data	vi_big_m vi_cmd_data['motion']['args'] = {} if vi_cmd_data['mode'] == MODE_VISUAL:
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
# todo: non-numeric columns should be ignored automatically </s> n = 11	test_iloc4 def test_impl(df, n): return df.iloc[[1,4,9]].B.values df = pd.DataFrame({'A': np.arange(n), 'B': np.arange(n)**2}) np.testing.assert_array_equal(hpat_func(df, n), test_impl(df, n))
# todo(ls): revert this loop to "yield from" </s> yield '  file "{}", line {}\n'.format(filename, lineno)	format_exception_only return filename = self.filename or "<string>" badline = self.text offset = self.offset
# todo(vek): need to pass context in for access to auth_token </s> assert state in power_state.valid_states(), "bad state: %s" % state	InstanceInfo self.name = name
#todo (nmakhotkin) this should be refactored later </s> executions = [execution]	Executions
# todo - use new error message api! ts </s> returns:	connectLayerListener ..seealso:: disconnectLayerListener Args: None Raises:
# todo: make sure package names can't be changed to look like package ids? </s> if c.remote_addr == 'localhost' or c.remote_addr == '127.0.0.1':	__before__ c.__version__ = ckan.__version__ c.user = request.environ.get('REMOTE_USER', None) c.remote_addr = request.environ.get('HTTP_X_FORWARDED_FOR', '127.0.0.1')
# todo ... </s> def addchild(self, child):	addChild
#todo: the following functions are used sage.combinat.combination and </s> w -= 1	_comb_largest 4 w = a - 1 return w
## todo: fix the unicode issue mentioned in </s> if synonyms_list:	synonym except KeyError: return False return json.dumps(synonyms_list) else:
# todo: probably better to work out why they are occurring, but imo the </s> version_info=(3, 6, 5),	test_compatible dist = make_fake_dist('== 3.6.5') _check_dist_requires_python( ignore_requires_python=False, )
except attributeerror:  # todo: this logic belongs somewhere - anywhere - else. </s> raise valueerror("either pass filepath or checksum_address; not both.")	__read_tls_public_certificate @validate_checksum_address def __read_tls_public_certificate(self, filepath: str = None, checksum_address: str = None) -> Certificate: if not filepath and checksum_address is not None: filepath = self.generate_certificate_filepath(checksum_address)
report_config = {}  # todo port to fooddata.from_request </s> def data_providers(self):	data_providers @property return [ ConvFactorGapsSummaryData(config=self.report_config),
# todo: perhaps some code belongs here to enforce rules about which </s> def cert(self):	cert
# todo: no testpath exercises this code... </s> fps = int(1.0 / max(totaltime, 0.001))	report_framerate stepTime = mid - start render_duration = now - mid log.debug("%sms/%sfps / Frame: %sms / Update: %sms", round(1000 * totalTime), fps, round(1000 * stepTime),
1  # todo: fill in identifier </s> line[7::7] = len(line[7::7]) * '.'	print_stats return stack def _line(recursion): return ''.join(line) highest_time = 0
# todo: this loop is pretty slow .. (parellize) </s> grad = self.mesh.nodalgrad	getADeriv def getADeriv(self, ky, u, v, adjoint=False): sigma = self.sigma vol = self.mesh.vol
# @todo: use real lightness from hsv or lab color model </s> )	read_colorscheme_from_path image_palette = self.generate_terminal_palette( THEME_MODEL_BY_KEY.get('_PIL_PALETTE_STYLE', {}).get('fallback_value'), theme_template = THEME_MODEL_BY_KEY.get('_PIL_THEME_TEMPLATE', {}).get('fallback_value') oomox_theme = {}
# todo write bin data </s> _response = dict(	response def response(self, val): self._response = val code=self._response.status_code, headers={k:v for (k,v) in self._response.headers}
if len(words) + len(tokens) <= self.args.maxlength:  # todo: filter don't happen here </s> for i in range(len(batch.encoderseqs[0])):  # batch size	printBatch Args: batch (Batch): a batch object print('Encoder: {}'.format(self.batchSeq2str(batch.encoderSeqs, seqId=i))) print('Decoder: {}'.format(self.batchSeq2str(batch.decoderSeqs, seqId=i)))
# todo(nnorwitz): other warnings to add: </s> def _getlinenumber(self, start):	_GetLineNumber
pass  # todo </s> def delete(self, uri):	SpotifyPlaylistsProvider self._backend = backend def create(self, name): pass  # TODO def lookup(self, uri):
# todo for pytorch 2.0.4, need to set dim=1 for log_softmax or use softmax then take log </s> char = self.char_embedd(input_char)	_get_rnn_output length = mask.data.sum(dim=1).long() word = self.word_embedd(input_word) char_size = char.size() char = char.view(char_size[0] * char_size[1], char_size[2], char_size[3]).transpose(1, 2)
form_data.history.append(operation)  # todo: should this show in form history tab? it doesn't </s> form_attachment_dict["data"]["n0:meta"]["n0:username"] = new_username	Command def parse_form_data(form_data, new_username): form_attachment_xml = form_data.get_attachment("form.xml") form_attachment_xml_new = xmltodict.unparse(form_attachment_dict) return form_attachment_xml_new
# todo: activate this code if we have limits at webmail level </s> self.subnet = ipaddress.ip_network(app.config["subnet"])	Limiter self.storage = limits.storage.storage_from_string(app.config["RATELIMIT_STORAGE_URL"]) self.limiter = limits.strategies.MovingWindowRateLimiter(self.storage) def check(self,clientip): if not self.limiter.test(self.rate,"client-ip",clientip):
# todo: compare this with compress_code_broken and fix the latter. </s> else:	from_filename if filename.endswith('.p8'): with open(filename, 'r', encoding='utf-8') as fh: with open(filename, 'rb') as fh: g = Game.from_p8png_file(fh, filename=filename)
#todo : multi parent intelligence </s> def has_child(self):	has_child
# todo: arrange </s> repo = self.remote.new_repo(self.token)	createRepo @pytest.fixture self.remote.modify_repo(repo, "name", "testrepo0", self.token) self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token)
# ---- todo: the following should be removed in milestone:0.11 </s> return util.embedded_numbers(a['name'].lower())	file_order
# todo: find the common ground of these, and make it an expression method. </s> return self.operator	getOperator
# todo fixme result type? </s> for m in t.iter_messages(order_by='-timestamp'):	_dump_helper name = t.name.replace('/', '_') # meh.. path = tdir / (name + '.txt') dts = m.dt.strftime('%Y-%m-%d %a %H:%M') msg = f"{dts}: {m.text}"
# todo fix. </s> cmp_result = self.__compare_contexts(ctx_init, x86_ctx_out, reil_ctx_out)	test_cmova asm = ["cmova eax, ebx"] ctx_init = self.__init_context() if not cmp_result: self.__save_failing_context(ctx_init)
# todo: may be check whether it fits to tracking branch </s> def git_get_active_branch(self):	git_get_active_branch
# todo(slaweq): self.has_neutron = true should be added, the mock of </s> ])	test_wait_for_server mock_get_server.assert_has_calls([ mock.call(building_server['id']), self.assertEqual(2, mock_get_active_server.call_count) mock_get_active_server.assert_has_calls([
#create and insert todo on remote site </s> master = self.get_client()	TestNodeConfiguration class TestNodeConfiguration(unittest.TestCase): master_doc = self.insert_into_master(master, 'test creation 1 sync') pull_master_data()
# todo: verify </s> omit_set_map = set()	get_omit_set for omit in model.omits: if omit.type == 'OMIT1':
# todo: other types than can have series inside: list, set, etc. </s> s = fill	_column_fillna_impl for i in numba.parfor.internal_prange(len(A)): s = B[i] A[i] = s
# todo: ... </s> return eval_metric	wrap_xgboost_metric target = target.get_label() metric_value = metric(target, prediction)
raise exceptions.mpdnotimplemented  # todo </s> raise exceptions.mpdnotimplemented  # todo	subscribe@18 Subscribe to a channel. The channel is created if it does not exist already. The name may consist of alphanumeric ASCII characters plus
# todo: repackage timeout? </s> return self.application_key	get_application_key
# remove all ads (todo: ad specific div classes probably change over time, look into a more generic method) </s> if 'start' in args:	gen_query if 'tbm' in args: tbm = '&tbm=' + args.get('tbm') start = '&start=' + args.get('start') near = ''
# todo: write tests </s> pass	MeiValidityError class MeiValidityError(exceptions21.Music21Exception):
self.current_height = self.current_height * 2 #todo </s> def _phase_shift(i, r):	layer_phase_shift def _squeeze(x): single_batch = (int(x.get_shape()[0]) == 1)
# todo: add asserts and type checkings </s> if len(tensor.shape) == 2:	tensor_to_image@18 def tensor_to_image(tensor): tensor = torch.unsqueeze(tensor, dim=0) tensor = tensor.permute(1, 2, 0).contiguous()
# fix: https://github.com/certtools/intelmq/issues/1720 # todo: find better fix </s> return feedname, function	get_feed_by_filename def get_feed_by_filename(given_filename): for feedname, filename, function in mapping: else: return None
## \todo there should really be a method to map from plug to parameter. </s> else :	__parameterNoduleCreator return GafferUI.StandardNodule( plug )
# todo: in case of recursive object, this will break python </s> else:	_to_ber_ws TLV, ASN1CodecBER.encode_tag_ws(0, 0, 0), lval += (TLV[0].get_bl() + TLV[1].get_bl()) >> 3 for t in reversed(self._tagc[:-1]):
# todo: check that tag key matches section start tag key. </s> self._delimiters = delimiters	_change_delimiters self.compile_template_re()
# todo handle 4 types of transition exceptions </s> logger.get('payment').error("invalid manual action {} on successful status".format(action))	_next_from_successful elif action == TransactionAction.cancel: return TransactionStatus.cancelled elif action == TransactionAction.complete: return TransactionStatus.successful
# todo(b/171992041): remove the string-typed metric key branch once v1 </s> single_slice_key_dicts.append({kind: getattr(single_slice_key, kind)})	convert_slice_key_to_parquet_dict kind = single_slice_key.WhichOneof('kind') if not kind: return {_SINGLE_SLICE_KEYS_PARQUET_FIELD_NAME: single_slice_key_dicts}
# todo: this is highly inefficient if other properties depending on the </s> def ratio(self):	ratio @property
pass  # todo </s> listener.backendlistener.send('playlists_loaded')	GMusicPlaylistsProvider playlist = Playlist(uri='gmusic:' + i, name=name, tracks=tracks) playlists.append(playlist) def save(self, playlist): pass  # TODO
# todo: use different flag than .reentrant </s> return	_suspend state.sphereLevel = ColorSorter.sphereLevel ColorSorter._suspended_states += [state]
# c2 = (219, 220, 200, 255)  # todo: not used? </s> x, y = pos	unset_jungle_mask def unset_jungle_mask(pos):
# todo(hrybacki): move to framework.utils.rapply once @sam's pr#4027 is merged. </s> return ret	draft_before_register_page def draft_before_register_page(auth, node, draft, *args, **kwargs): ret = serialize_node(node, auth, primary=True)
# todo: raise more specific exception </s> destination = self._trigger.sender	notice if destination is None:
# todo extend it to output into multidimensional space </s> fd = {}  # type: feeddict	feed_dict sentences = cast(Iterable[List[str]], dataset.get_series(self.data_id, allow_none=True)) if sentences_list is not None: fd[self.gt_inputs] = list(zip(*sentences_list))[0]
# todo: this is ambiguous: it's not clear whether we correctly </s> self.refresh()	TestPostUpdate author=user) eq_(elasticutils.S(Thread).count(), original_count + 1) eq_(elasticutils.S(Thread).count(), original_count + 1)
# todo: use denormalized site_domain field </s> verbose_name = _('article config')	META class META:
# todo: estimate fees </s> return hdwalletkey(self.parent_id, session=session)	parent
# todo in the future, use http://schacon.github.io/git/git-ls-remote.html to validate the url string </s> flash(_("repository succesfuly deleted"))	delclone return redirect(url_for('settings.repos') + "#tab_owngit") db.session.delete(project) return redirect(url_for('settings.repos') + "#tab_owngit")
# todo: provide some feedback on what happens in git </s> def packages_for_sha(self, sha):	packages_for_sha repo = get_repo() repo.checkout(sha)
# todo: arrange </s> self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token)	createRepo def createRepo(self): repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "mirror_locally", "0", self.token) self.remote.save_repo(repo, self.token)
# todo: with success_re etc </s> def myopen(*args, **kwargs):	fake_open write_(*args, **kwargs) def close(self): return myfile return myopen
# todo: verify learning rule contents </s> tfm_flows = [flow for flow in features_flows if isinstance(flow, valve_of.parser.ofptablefeaturesstatsrequest)]	test_switch_features def test_switch_features(self): self.assertTrue(isinstance(self.valve, valve.TfmValve)) self.assertTrue(tfm_flows)
# todo: write tests </s> pass	MeiValidityError class MeiValidityError(exceptions21.Music21Exception):
# todo: need to delete all queues </s> return start_service	pytest_funcarg__start_service
# todo better logging </s> scp.get(self.config_file, local_path='/tmp/faucet.yaml')	Connection try: scp = SCPClient(self.ssh.get_transport()) elif f_type == 'log': scp.get(self.log_file, local_path='/tmp/faucet.log')
# todo(py3.7): add required=true </s> def test_build(self):	test_build self.assertBuilds()
# todo xxx </s> min_op = optimizer.minimize(loss)	KBPReader for grad, var in gradients] min_op = optimizer.apply_gradients(gradients) self.sess.run([v.initializer for v in tf.global_variables() if v not in self.model_module.variables]) logger.info("Start training...")
# todo(dcramer): this needs to be a get_or_create pattern </s> db.session.add(app)	AppDetailsApiView db.session.add(repo) db.session.flush() db.session.commit() return self.respond(serialize(app))
# todo(mordred): this needs to be mutex protected </s> raise openstackcloudexception(	delete_keypair except OpenStackCloudException: raise "Unable to delete keypair %s: %s" % (name, e)) return True
#todo?# self.asserttrue(greps(err, "unit zzz.service not for --user mode")) </s> out3 = output(cmd.format(**locals()))	test_6133_run_default_services_from_single_service_saved_container self.assertFalse(greps(top, "testsleep 99")) # <<<<<<<<<< difference to 5033 self.assertTrue(greps(top, "testsleep 111")) logg.info("\n>\n%s", out3) top_container = "docker exec {testname} ps -eo pid,ppid,user,args"
# todo(mattjj): remove this special case, used for debugging on cpu </s> with core.new_master(jaxprtrace, true) as master:	parallel_callable @lu.memoize def parallel_callable(fun, axis_name, axis_size, *avals): jaxpr, (pval, consts, env) = trace_to_subjaxpr(fun, master).call_wrapped(pvals) assert not env
# todo: use mock to patch dict </s> def testcaninitializeclass(self):	testCanInitializeClass
# todo: refactor accordingly when v3 websocket api is released </s> return bittrexorderbook	order_book_class @property
# todo do something with temp </s> s.resample(temp=temp)	resample_states def resample_states(self,temp=None):
# todo: remove in 0.26 when grid_scores_ is deprecated </s> emp_cov = empirical_covariance(x)	test_graphical_lasso_iris_singular [0.0, -12.499999143, 0.0, 462.5] ]) for method in ('cd', 'lars'): cov, icov = graphical_lasso(emp_cov, alpha=0.01, return_costs=False,
# todo: add this back </s> log.info(	register_secret 'registerSecret called', node=pex(self.node_address),
# todo debug </s> self.timetriggeredfor = 0.0	RuleElement self.type = None self.triggered = False self.element = None
# todo(dspasovski): fix this. </s> type=amo.addon_webapp)	get_new_cat def get_new_cat(self):
# todo - might need to click on transportation mode if url doesn't work </s> raise timeouterror("your request has been timed out! try overriding timeout!")	get_last_seen if elapsed_time > 10: return "None" except NoSuchElementException: return "None"
# todo this was already broken, just making it obvious </s> if 'confidences' in classification:	get_roles_confidences classification = metadata.get('classification', {}) if 'labels' in classification: top_conf, second_conf, third_conf = classification['confidences'][:3] return (top_role, second_role, third_role), (top_conf, second_conf, third_conf)
# todo: check that the birth date is not in the future </s> return datetime.date(year, month, day)	_get_birth_date raise ValueError('No 9 digit birth numbers after 1953.') elif year < 1954:
pass  # todo </s> def _device_open(self):	RivalMouse pass  # TODO def _find_device(self): self._device = open(self._device_path, "wb") def _device_write(self, *bytes_):
# todo: add a better throttling mechanism </s> dict_writer.writerows(list_of_posts)	write_posts_to_csv with open(filename, 'w', encoding=encoding) as output_file: dict_writer = csv.DictWriter(output_file, keys)
1  # todo: fill in identifier </s> cumulative_depth = float('inf')	print_stats for line in _stack(stat): highest_time = max(highest_time, line.average) formated_stack = [] print(' total   cumm single')
# todo: add this back </s> if not is_binary_address(token_address):	add_token raise InvalidAddress('Expected binary address format for token') log.info(
# todo: only if successful </s> def submission_prediction_output_filename(instance, filename="output.zip"):	submission_prediction_output_filename
# todo: docstrings! </s> return pd.dataframe(appliance_dict)	get_dataframe_of_appliances if dual_supply_columns: appliance_dict[appliance_name] = (dual_supply_columns[0] +
# todo(b/132329316) remove when `xla.compile` allows tf.device(tpu). </s> per_replica = replicator.experimental_local_results(per_replica)	assertSameValuePerReplica first_replica = per_replica[0] for nth_replica in per_replica[1:]:
# todo: find a better solution than this: </s> pass	NoMoreVideoError
# todo: unit test </s> if template is not none:	View the corresponding template as a string, preferably as a unicode string.  The method should return None if there is self.template = template context = Context.create(self, context, **kwargs)
# todo: clocksignal, resetsignal, memory </s> try:	Simulator for generator in self.generators[cd]: reply = None request = generator.send(reply) if request is None:
transaction_manager.retry_attempt_count = 3  # todo: hardcoded for now </s> dbsession.close()	terminate_session
with prepare_file(["#todo this is todo"], none) as (lines, filename): </s> coala_json.main, 'coala-json', '--text-logs', '-c', 'nonex')	test_text_logs def test_text_logs(self): self.assertRegex( output,
# todo(jogo): make the following doctests pass: </s> yield(0, 'h305: imports not grouped correctly '	hacking_import_groups if normalized_previous and normalized_previous[0] == 'import': previous_type = _get_import_type(normalized_previous[1]) '(%s: %s, %s: %s)' % (normalized_previous[1], previous_type,
# todo: extend to string variables </s> lir.inttype(64).as_pointer(), lir.inttype(64).as_pointer(),	h5_read dset_name_arg = sig.args[1] val2 = context.insert_const_string(builder.module, dset_name_arg.value) lir.IntType(64), lir.IntType(8).as_pointer(), lir.IntType(32)] fnty = lir.FunctionType(lir.IntType(32), arg_typs)
# todo implement extra options </s> break	reader last_word = word word = "" if accepted is not None and tmp not in accepted: last_word = word
# todo: askr, undocumented! </s> min( blue, 63 )	LedCtrlChar max( red,  0 ) min( green, 63 ) max( blue,  0 ) for i in range(81, 1, -10):
# todo: finish this. </s> entry = entry_clause[0]	_GDriveFS raise FuseOSError(ENOENT) effective_permission = 0444 is_folder = get_utility().is_directory(entry) and not just_info if entry.editable:
# todo: config option? </s> return source	_get_git_url_from_source source = source_ri.as_git_url() else:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	ReplayBundleRequestFilterTestCase self.skipTest('Not implemented yet.') def test_fail_depth_null(self): def test_fail_depth_string(self): ``depth`` is a string.
self.setup() # todo: perhaps, remove this to pass path in context </s> data_container = step.read_checkpoint(data_container)	_load_pipeline_checkpoint if isinstance(step, BaseCheckpointStep):
# todo: askr, undocumented! </s> blue   =  0	LedCtrlChar if blue is None: red   *= 21 min( red, 63 ) max( red,  0 )
# todo: add permissions </s> return discord.utils.find(lambda m: m.id == self.config.owner_id, self.get_all_members())	_get_owner if m.id == self.config.owner_id: return m
# todo: handle sigchld to avoid wasting time in polling </s> flags = fcntl.fcntl(fd, fcntl.f_getfd)	set_cloexec_flag cloexec_flag = fcntl.FD_CLOEXEC except AttributeError: fcntl.fcntl(fd, fcntl.F_SETFD, flags | cloexec_flag)
# todo: switch to: </s> return self.current_tl_track	get_current_tl_track
# todo: only handle events that are new. </s> self.rtm_thread.start()	bootstrap def bootstrap(self): self.client = SlackClient(settings.SLACK_API_TOKEN)
# todo(shoyer): test fails on tpu </s> f = lambda pos, inc: (lax.add(pos, 1), lax.add(count, inc))	loop_body pos, count = state
# todo.hartikainen: remove this </s> def template_function(self):	ConvnetPreprocessor super(ConvnetPreprocessor, self).__init__( *args, name=name, **kwargs) return convnet_preprocessor_template
time.sleep(1.0)  # todo properly wait until sequencer is online </s> port = match.group(1)	list_alsa_sequencer_ports continue match = port_pattern.match(line) desc = match.group(2) flags = match.group(3)
# todo: once we allow filtering, unit.store.units has to be a qs </s> qualitycheck__false_positive=false, qualitycheck__name__in=matchnames)	get_step_query matchnames.remove('hassuggestion') if matchnames: units_queryset = match_queryset return units_queryset
##todo     temp = x </s> for chunkx, chunky in mcr.get_chunks():	iterate_chunks of integers (x,z,mtime) for each chunk.  Other chunk data is not returned here. Old name: world.iterate_chunk_metadata yield chunkx+32*regionx, chunky+32*regiony, mcr.get_chunk_timestamp(chunkx, chunky)
if testname == "tests5": continue # todo </s> expected=expected, errors=errors, treecls=treecls):	testFunc return self.runParserTest(innerHTML, input, expected, errors, treeCls)
# todo/rsi for when have nonlocal </s> return super().f() + 'd'	D class D(C, B): def cm(cls): return (cls, super().cm(), 'D')
# todo: add axis parameter </s> def idxmin(self, skipna=true):	idxmin
# todo(b/160795287): deprecate estimator based executor. </s> keras_model=_keras_model_builder(), config=run_config)	trainer_fn save_checkpoints_steps=999, keep_checkpoint_max=1) run_config = run_config.replace(model_dir=trainer_fn_args.serving_model_dir) eval_receiver_fn = lambda: _eval_input_receiver_fn(schema) return {
# todo: move install_time away from app_setting </s> ssowat_conf['redirected_urls'] = {}	app_makedefault except IOError: ssowat_conf = {} ssowat_conf['redirected_urls'][domain +'/'] = app_domain + app_path with open('/etc/ssowat/conf.json.persistent', 'w+') as f:
# todo scope the cleanup to the current project - https://github.com/lyft/cartography/issues/381 </s> response_objects.append(res)	get_gcp_instance_responses res = req.execute()
# todo: import that elsewhere </s> from . import _control	getConstant@50 _control.execQueue.socket.pumpInfoSocket() return elements.get(key)
#todo : multi parent intelligence </s> def has_child(self):	has_child
# todo: fix with stubber / before send event </s> sent_request.headers.get('accept'), b'application/yaml')	TestApiGateway sent_request = _send.call_args[0][0] self.assertEqual(sent_request.method, 'GET')
"""the arguments given to a command.""" #todo elaborate </s> def __init__(self, jenni):	JenniWrapper self.bot = jenni def __getattr__(self, attr):
# todo: should we enable auto-retry, </s> self.queue = queue	ConsumerConfig class ConsumerConfig(object): Stores information about a consumer-decorated method. self.include_raw_message = include_raw_message self.prefetch_count = 1
# todo: set language preference from config </s> _readsettings()	accessmanager if _accessmanager: return _accessmanager app.settingsChanged.connect(_readSettings) return _accessmanager
pass # todo </s> def collect_incoming_data(self, data):	collect_incoming_data
# todo is this serious enough to raise a canerror exception? </s> log.debug('binding socket with id %d to channel %s', socketid, channel_name)	bindSocket +----+----------------------------+ | -1 |socket creation unsuccessful| socketID = ctypes.c_int(socketID) ifr = IFREQ()
# todo: convert to a python xml thing </s> if b is not none: return	on_actionSave_Text_triggered if self.text_fname is not None: f=open(self.text_fname,'w+')
# todo(hartikainen): this should get the logdir some other way than </s> def _do_training(self, iteration, batch):	_do_training feed_dict = self._get_feed_dict(iteration, batch) self._sess.run(self._training_ops, feed_dict)
# xxx todo </s> e= []	jno e.append(ExprAff(eip, ExprCond(of, a, b))) return e
pass  # todo </s> print('invalid key. using key: 0.')	get_key key = int(text) return key return 0
# todo pydocs </s> :py:meth:`execute` did not produce any result set or no call was issued yet.	BigQueryCursor fetch as many rows as indicated by the size parameter. If this is not possible due to the specified number of rows not being available, fewer rows may be returned. if size is None: size = self.arraysize
# todo: create a remote print interface for objects which displays them in a </s> return getreprmessage(	GetReprMessage ) @staticmethod id_at_location=_deserialize(blob=proto.id_at_location), msg_id=_deserialize(blob=proto.msg_id),
# todo: batch </s> return self.type == type	is_type :py:const:`False` otherwise. .. deprecated:: 1.5
# todo(nakago): check why tolerance is high </s> def model():	model @pytest.fixture
# todo: must be implemented </s> def should_fetch_kindlegen():	should_fetch_kindlegen
# todo: https://github.com/turicas/brasil.io/issues/210 </s> return urls	clean_boletim_urls url_validator = URLValidator(message="Uma ou mais das URLs não são válidas.") for url in urls:
# todo: [debug] change this </s> return redirect(reverse('system_list'))	system_instant model = SystemImporterFileCsvConfigModel.objects.get(system_importer_file_csv_config_name = 'SystemImporterFileCsvConfig') stop_system_importer_file_csv_run = run_check_content_file_system(model, request) stop_system_importer_file_csv = run_check_config_attributes(model) if stop_system_importer_file_csv_run:
# todo: use triple factory </s> trans_h = transh(triples_factory=self.factory)	test_trans_h self.assertIsNotNone(trans_h)
# todo: there's a race with the initial "output" event. </s> attach_args = {'options': 'windows_client=true'}	test_attach_from_windows_os self.attach(expected_os_id='WINDOWS', attach_args=attach_args)
# todo: how do we handle this? </s> self._reset()	Parser data = tuple(self._data[1:]) msg = opcode2msg[0xf0](manifacturer=manifacturer, data=data) else: self._opcode = opcode
# todo(b/135607227): add device scope automatically in keras training loop </s> label = tf.compat.v1.sparse_to_dense(label, (cifar_main.num_classes,), 1)	parse_record_keras Returns: Tuple with processed image tensor and one-hot-encoded label tensor. return image, label
# todo: finnish luks+zfs </s> logging.warning(err_output)	set_zfs_mountpoint output = subprocess.check_output(cmd, stderr=subprocess.STDOUT) except subprocess.CalledProcessError as err:
# todo: replace with below line when numba supports np.isin in nopython mode </s> result = pandas.series(self._data[idx])	hpat_pandas_series_getitem_idx_slice_impl def hpat_pandas_series_getitem_idx_slice_impl(self, idx): return result
# todo "specify timestamp precision when writing to influxdb."? </s> time=dt,	dit yield dict( measurement=measurement, fields=d,
engine = db.connect() # todo do i need to tear anything down?? </s> def make_schema(obj):	make_schema
# todo add </s> for line in f:	read_history_file expanded = os.path.expanduser(filename) new_history = reader.getHistory().getClass()() new_history.addToHistory(line) reader.history = new_history
# @todo: pheonix </s> self.populateskilltree()	delaySearch def delaySearch(self, evt): else: self.searchTimer.Stop()
# todo: componentize! </s> from twisted.python import rebuild	on_reload_self_activate rebuild.rebuild(inspect.getmodule(self.__class__))
#ack = self.serial_port.read() # todo: use ack </s> if self.serial_port.isopen():	Disconnect def Disconnect(self): self.serial_port.close() except serial.SerialException:
# todo: implement </s> if cls.output_schema is none:	validate_result @classmethod if cls.output_schema_path is None: return
match = re.search(pattern, ret) # todo: more strict </s> dest="userdefaults",	generate_option_parser parser.add_option("-u", "--UserDefaults", action="store_true", help="show UserDefaults info") return parser
# todo ideally this happens a layer higher, but this is a bad </s> dataset_source = dataproviders.dataset.datasetdataprovider( dataset )	chunk_dataprovider @dataproviders.decorators.dataprovider_factory( 'chunk', dataproviders.chunk.ChunkDataProvider.settings ) return dataproviders.chunk.ChunkDataProvider( dataset_source, **settings )
# todo: stub function, add actual logic </s> if user.is_staff:	private_user_permissions user = account_models.User.objects.filter(pk=user_pk).first() if not user: return [AccountPermissions.MANAGE_STAFF] return [AccountPermissions.MANAGE_USERS]
full_payload)  # todo: parse response for confirmation. </s> return msgpack.dumps(self.ids)	packed_payload
# todo(rakhmerov): implement. </s> lambda: self.is_execution_success(exec_db.id),	test_full_join wf_service.create_workflows(WF) exec_db = self.engine.start_workflow('wf', {}) ) exec_db = db_api.get_execution(exec_db.id)
# todo: fix with stubber / before send event </s> 'accepts': 'application/yaml'	TestApiGateway 'restApiId': 'foo', 'stageName': 'bar', } with mock.patch('botocore.endpoint.Endpoint._send') as _send:
# todo: consider use float32 type from the beginning of this function </s> return bboxes_pred	get_box xy0 = (xy_ctr[:, :, :] - offsets[:, :, :2]) xy1 = (xy_ctr[:, :, :] + offsets[:, :, 2:])
# todo files listed here may not belong to the given camera </s> _remove_older_files(target_dir, preserve_moment, exts=_movie_exts)	cleanup_movies return # preserve forever preserve_moment = datetime.datetime.now() - datetime.timedelta(days=preserve_movies)
# todo: update consumer </s> raise missingresource('/'.join((repo_id, distributor_id)))	__distributor if dist is None:
#todo: issue warning that this is an unsafe operation, but doing it cause user insists </s> return context	selinux_initial_context context = [None, None, None] if self.selinux_mls_enabled():
# todo: remove when support is python 3.6+ only </s> path : str, optional	DataLibrary def from_xml(cls, path=None): Parameters Path to XML file to read. If not provided, the `OPENMC_CROSS_SECTIONS` environment variable will be  used.
""" todo: better test here """ </s> filtered_params = special_test_function(self.sample_params_list)	test_filter_params def special_test_function(params, realm=None): return 'OAuth ' + ','.join(['='.join([k, v]) for k, v in params]) self.assertFalse("notoauth" in filtered_params) self.assertTrue("oauth_consumer_key" in filtered_params)
# todo: start looking in current group. </s> except keyerror:	ExUnmap mappings.remove(modes.VISUAL, cmd)
# todo: proper java error? </s> def call_static_float_method(self, mu, env):	call_static_float_method @native_method
# todo: capture stdout for both the test assert and docs embedding </s> newton = prob.model.nl_solver = newtonsolver()	test_feature_iprint_neg1@33 def test_feature_iprint_neg1(self): prob = Problem() ln_scipy = prob.model.ln_solver = ScipyIterativeSolver() newton.options['maxiter'] = 2
# todo: bindings should be done in collection class: </s> filter regex(?o, "%s")}}""" % \	SearchHandle metacollection.conjunctive_graph.bind('dlns', DLNS) query_string = """SELECT ?g ?r {GRAPH ?g {?r rdf:type dlns:Handle . search results = metacollection.conjunctive_graph.query(query_string)
# todo: support axel, prozilla </s> exit_code = subprocess.call(curl_opts)	curl_download if resuming: curl_opts += ['--continue-at', '-'] if exit_code != 0: raise Exception('curl exited abnormaly')
# todo check if lus can be more than one token </s> processed[sentence_id]['sentence'] = sentence	read_full_results sentence_id = row['id'] sentence = h.unescape(row['sentence'].decode('utf-8')) processed[sentence_id]['frame'] = row['frame'] processed[sentence_id]['lu'] = row['lu']
#todo: support unicode, greek, and special latin characters </s> character,	PyKeyboardEvent else: character = self.lookup_char_from_keycode(keycode) press=press_bool) def lookup_char_from_keycode(self, keycode):
# todo: add option for attentive reader </s> if answers - ans_alts:	model raise TypeError('invalid candidates argument(s): {}'.format( ', '.join(candidates - can_alts))) raise TypeError('invalid answers argument(s): {}'.format( ', '.join(answers - ans_alts)))
# todo: this will incorporated in the future, if needed. </s> ],	I210SubNetwork "119257908#1-AddedOffRampEdge", "119257908#2", "119257908#1-AddedOffRampEdge": [ (["119257908#1-AddedOffRampEdge",
accept_federated_only=self.federated_only)  # todo: 466 </s> def canonical_public_address(self, address_bytes):	canonical_public_address @canonical_public_address.setter
# todo, make broadcasting </s> params = msg["params"]	register_handlers async def _cb_handle_tasks_create(sio, msg): task_type = msg["task_type"] project_uuid = msg["project_uuid"] tasks = self.task_manager.create_task(
# todo: other types like boolean </s> typ_val = _h5_typ_table[arr_typ.dtype]	get_type_enum_overload def get_type_enum_overload(arr_typ):
# todo - implement fully and test </s> win32defines.miim_id | \	_GetMenuItems menu_info.cbSize = ctypes.sizeof (menu_info) menu_info.fMask = \ win32defines.MIIM_STATE | \ win32defines.MIIM_SUBMENU | \
# todo untested </s> hexstring_serial = _api.string(hex_serial)	get_serial_number try: hex_serial = _api.BN_bn2hex(bignum_serial) serial = int(hexstring_serial, 16) return serial
# todo semantic validation </s> results = self.client.list_server_logs(fqdn)	list_server_logs if not results: return Response(status=status.HTTP_503_SERVICE_UNAVAILABLE)
# todo stub </s> def metrics_identity(evaluator_identity: detectionevaluator) -> dataframe:	metrics_identity @pytest.fixture  # type: ignore
# :todo: implement test. </s> self.skiptest('not implemented yet.')	GetTransactionsToApproveRequestFilterTestCase Request is empty, so default values are used for all parameters. self.skipTest('Not implemented yet.') def test_fail_depth_float(self): self.skipTest('Not implemented yet.')
# todo(b/160795287): deprecate estimator based executor. </s> else:	copy_model path_fn = serving_model_path elif tag == 'eval': raise ValueError('Invalid input tag: {}.'.format(tag)) source = path_fn(working_dir)
# todo: support minp arg end_range etc. </s> return arr	rolling_fixed
# todo: has some issues with datetime and sqlite </s> def test_sponsor_api(self):	test_sponsor_api
# todo: replace xrange (could fail with 32-bit python 2.x). </s> if n == 0:	__irshift__ raise ValueError("Cannot shift by a negative amount.") if not self.len: return self n = min(n, self.len)
# todo pseudo code: </s> pass	Seeked @dbus.service.signal(dbus_interface=player_interface, signature='x')
# todo(slaweq): change this to neutron floating ips and turn neutron </s> ret_active_server = fakes.fakeserver('1234', '', 'active',	TestRebuildServer rebuild_server = fakes.FakeServer('1234', '', 'REBUILD', adminPass='ooBootheiX0edoh') adminPass='ooBootheiX0edoh') fake_floating_ip = fakes.FakeFloatingIP('1234', 'ippool',
# todo: this can unnecessarily suspend the starting of a build, in </s> def ping(self):	BuilderControl for r in buildrequests ] def getBuild(self, number): if not self.original.slaves: self.original.builder_status.addPointEvent(["ping", "no slave"])
# todo: this is a hack to make a rule know </s> return self.story_graph.story_end_checkpoints.get(end_name, end_name)	_find_start_checkpoint_name
# # todo: </s> def get_feeder_name(self):	get_feeder_name
# todo: memoize? </s> pass	FloatColumn
# todo: kill this </s> def cache_single(filename):	cache_single
# todo: implement model fitting based on the legacy code bellow </s> self.__eye_translations = eye_translations	GazerHMD3D_v1x label = "HMD 3D (v1)" def __init__(self, g_pool, *, eye_translations, calib_data=None, params=None):
# todo: calculate the checksum if not given </s> diff id'''	get_layer_object for layer in self.layers: if layer.diff_id == diff_id:
'auto_link': m.case_type == module.case_type,   # todo: which menus need manual linking? some child menus probably </s> module_name = trans(form.get_module().name, langs)	qualified_form_name form_name = trans(form.name, langs) star = '* ' if auto_link else '  '
# todo: avoid dummy and generate func here when inlining is possible </s> return lambda df: 0	df_len_overload if len(df.columns) == 0:  # empty df
# end todo </s> version_parser.add_argument('--dummy', help=argparse.suppress)	add_version_parser def add_version_parser(subparsers):
# todo make bit depth and sample rate into parameters. </s> if wet_only:	reverb def reverb(self, wet_only, reverberance, hf_damping, room_scale, stereo_depth, pre_delay, wet_gain): self.command += ' -w ' self.command += ' '.join(map(str, [reverberance, hf_damping, room_scale, stereo_depth, pre_delay, wet_gain]))
# todo: figure out the right thing to do here </s> ]) / (denom + 1e-6)	haversine_grad (np.cos(x[0] + np.pi / 2) * np.cos(y[0] + np.pi / 2) * sin_long * cos_long),
# todo: improve this code. </s> def _translate_hlt(self, tb, instruction):	_translate_hlt
# todo: remove warning check once deprecated </s> def test_sindex_rebuild_on_set_geometry(self):	TestFrameSindex assert self.df._sindex is None assert self.df.sindex.size == 5 assert self.df.sindex is not None self.df.set_geometry(
# todo: support unicode </s> a[i] = s	_column_fillna_impl s = B[i] if hpat.hiframes.api.isna(B, i):
# todo: try removing the none checks after https://github.com/mozilla/rust-code-analysis/issues/528 is fixed. </s> path_to_component = get_component_mapping(false)	download_component_mapping utils.download_check_etag( "https://firefox-ci-tc.services.mozilla.com/api/index/v1/task/gecko.v2.mozilla-central.latest.source.source-bugzilla-info/artifacts/public/components.json",
oldsize = self.size # todo: remove </s> return struct.unpack('>i', stream.read(4))[0]	read_uint
# todo: use the following when reddit pr #631 is added </s> subreddit = self.r.get_subreddit(self.sr)	test_create_link_through_subreddit unique = uuid.uuid4() title = 'Test Link: %s' % unique submission = subreddit.submit(title, url=url) self.assertEqual(submission.title, title)
# :todo: implement test. </s> b'zijgaj9aadlrpwncynnhuhrrac9qoudatedqumtn'	GetInclusionStatesRequestFilterTestCase b'D9YRJMXFXBDPFDTRAHHHP9YPDUVTNOFWZGFGWMYHE' ) b'OTABUVRPTSTFQDGZKFYUUIE9ZEBIVCCXXXLKX9999' )
# todo(gabriel-bezerra): simplify this after mitaka </s> }	test_deprecated_spt_in_driver_info_and_in_capabilites 'server_hardware_type_uri': 'fake_sht_uri', 'enclosure_group_uri': 'fake_eg_uri', self.assertEqual( expected_node_info,
#     todo </s> return	chmod def chmod(self, path, mode): cache_path = self.cache.get_path_or_dummy()
#todo: actually check for change </s> 'okay', '1', 'allow', 'allowed']:	str_to_bool Given a string, parses out whether it is meant to be True or not s = str(s).lower() # Make sure return True try:
# todo(msb) all-to-all </s> def all_to_all_combine(self, combine_weights: tensor, input: tensor) -> tensor:	MOELayer dispatched_input = torch.einsum("gsec,gsm->egcm", dispatch_mask.float(), input) dispatched_input = torch.squeeze(dispatched_input, 0)  # drop E dimension expert_output = torch.unsqueeze(input, 1)  # add E dimension output = torch.einsum("gsec,gecm->gsm", combine_weights, expert_output)
# todo: remove all elements of the list and remove the blacklist </s> tfa.image,	test_api_typed tfa, tfa.activations, tfa.losses, tfa.metrics,
# todo: optimise this </s> return self._camera_rotate_speed	EditCanvas self._camera_move_speed = val @property @camera_rotate_speed.setter def camera_rotate_speed(self, val: float):
# todo: uncomment to enable claiming </s> 'user_url': user.url if user else '',	get_globals 'user_name': user.username if user else '', 'user_full_name': user.fullname if user else '', 'user_api_url': user.api_url if user else '', 'display_name': framework.auth.get_display_name(user.username) if user else '',
# todo: remove the need for using dbobject.qualified_name </s> all_reads.update(reads)	determine_all_nonschema_privileges continue writes, reads = determine_nonschema_privileges_for_schema(role, objkind, schema, dbcontext) return all_writes, all_reads
# todo: reevaluate how to deal with different types of errors; soft </s> monitor         if the prefix should be monitored or not	XhrController customer_id     Customer identifier vlan            VLAN ID from-prefix     A prefix the prefix is to be allocated from from-pool       A pool (ID) the prefix is to be allocated from
# todo: move to base class </s> return qtcore.qrect(qtcore.qpoint(min_x, min_y), qtcore.qpoint(max_x, max_y))	getNodesRect max_x = max(arr2) min_y = min(arr3)
# todo: kwargs </s> if len(df.columns) == 0:  # empty df	df_len_overload @overload(len)  # TODO: avoid lowering? return lambda df: 0 return lambda df: len(df._data[0])
break  # we are done. todo: should we continue? ;) </s> (gitrepo, {}, {})	repo@370 valid = False for cls, ckw, kw in ( ): if cls.is_valid_repo(self._path, **ckw):
# todo: reflection formula </s> result = mpci_mul(result, x, wp)	mpci_pow_int result = (mpi_one, mpi_zero) while n: n -= 1 x = mpci_square(x, wp)
# todo: test/handle object not found. </s> post /actionexecutions/1?_method=delete	StactionExecutionsController def delete(self, id): Delete a actionexecution. DELETE /actionexecutions/1 actionexec = ActionExecution.get_by_id(id)
# todo: handle direct payments </s> if output["script"] == output_script:	on_tx_received output_script = 'a914' + digest(unhexlify( self.contract["buyer_order"]["order"]["payment"]["redeem_script"])).encode("hex") + '87' self.amount_funded += output["value"] if tx not in self.received_txs:
# todo: test me @jmcarp </s> def routes(self):	routes from . import routes return [routes.api_routes]
# todo: should use response.json instead </s> async def clean_title(self, value):	HackerNewsItem url = AttrField(css_select='a.storylink', attr='href')
# todo: add metadata support when it is merged from develop </s> jid = re.sub(r"'*", "", jid)	_escape_jid jid = str(jid)
""" todo: documentation </s> _generates_timeorder = false	GeneratingCommand time order Default: False @property def local(self):
# todo : factorize this in utils/packages.py ? </s> number=number, name=name))	_load_migration migration_id = migration_file[:-len(".py")] number, name = migration_id.split("_", 1) try: module = import_module("yunohost.data_migrations.{}".format(migration_id))
# todo(vmiura): parse these in a more readable way. </s> if self.stack_id:	ToDict ret['weight'] = self.weight  # Sample weight ret['name'] = self.type  # Sample type ret['sf'] = self.stack_id  # Stack frame id return ret
# todo: remove when signals land </s> store_data["critical_checks"]	test_data_tp_qc_stats check_count = qc_qs.count() store_data = tp0.data_tool.updater.get_store_data() == tp0.data.critical_checks == check_count)
# todo: maybe the output dataplaceholders should be replaced too </s> for target in parent_step.targets:	collect_steps_from collect_steps_from(input)
# todo: require an api key on the basic auth header </s> - for a build, find me the active release with this name.	Release class Release(db.Model): - Mark this release as abandoned. - Show me all active releases for this build by unique name in order
# todo: this one doesn't work quite well, handle it </s> source = re.sub(r'(^|\n)' + spaces, '\n', source)	get_ast if spaces:
# todo: find a better way to enforce this. </s> self._mem = mem_new	_translate_stm mem_new = self._solver.mkArray(self._address_size, "MEM_" + str(self._mem_instance))
# todo: a better solution might be to use locking in this code </s> ).one()	get_or_create@35 include_docs=True, reduce=False, if existing: return existing
# todo: add theano function. </s> else:	gradients elif backend == "tensorflow": import tensorflow raise NotImplementedError() pass
# todo: this is a jump. </s> vi_cmd_data['is_jump'] = true	vi_big_m vi_cmd_data['motion']['command'] = 'vi_big_m' vi_cmd_data['motion']['args'] = {'mode': vi_cmd_data['mode']}
#todo : what if multiple projects are selected ? </s> self.tid_todelete = none	on_delete_confirm pr.delete_task(self.tid_todelete[1])
# todo addding a assertrvline to test reversed selections would make calls to assertselection() obsolete in this test  # noqa: e501 </s> self.eq('|{ab}', 'v_%', '|{ab}|')	test_v_percent self.eq('{ab|}', 'v_%', '|{ab}|') self.eq('a |{\nb\n}\nc', 'v_%', 'a |{\nb\n}|\nc')
# todo: think of something more sensible to do than sum(). on one </s> regularization = 0	CrossEntropy and prediction. Use for binary-valued features (but not for, e.g., one-hot codes). if isinstance(self.model, Autoencoder): regularization = self.model.compute_penalty_value()
# todo implement. </s> self.label_column = label_col	Trainer def __init__(self, keras_model, features_col="features", label_col="label"): self.master_model = keras_model.to_json() def train(self, data): raise NotImplementedError
## todo: add only the optimizations needed? </s> _approx_eq.debug = 1	test10 self.cmp(self.rand(3,4), -1.0, self.rand(3,5), self.rand(5,4), 0.0)
# todo: copy icons into directory </s> return {f.lower(): pjoin(dir, f) for f in os.listdir(dir)	_list_kernels_in os.makedirs(dir, mode=0o644)
# todo this paragraph is necessary, but not sure it works. </s> if p._definition.stars == 0:  # no *args/**kwargs	Completion module = call_sig._name.get_parent_until() if not isinstance(module, compiled.CompiledObject): completion_names.append(p._name) needs_dot = not completion_parts.has_dot and completion_parts.path
# todo (bev) exception or log? </s> return session.curplot()	curplot session = session() if not isinstance(session, Session):
# todo: fix the fact the we have more keys in settings.yaml </s> assert not result	test_update_settings_file_emtpy_dict settings_data = {} settings_path = os.path.join(tmpdir, "settings.yaml")
# todo: deprecation 3.1 </s> return complex_fun(x)	wrapped_fun if iscomplex(x): return jax_fun(x)
except exception:  # todo - which exceptions? </s> [2.0, 1.5],	test_pca data = numpy.array([[3.1, 1.2], [1.4, 1.3], [1.7, 1.9], [1.7, 1.9],
#todo same issue with batch_size </s> tf_graph.create(graph)	create_graph graph.y=tf.cast(graph.y,tf.int64) graph.y=tf.one_hot(graph.y, self.config['y_dims'], 1.0, 0.0) elif graph_type == 'generator': tf_graph.create_generator(graph)
# todo: /data/local/tmp might not be execuable and atx-agent can be somewhere else </s> return int(text)	_pidof_app Return pid of package name text = self._reqsess.get(self.path2url('/pidof/' + pkg_name)).text
# todo consider launching a second search if results.total_tracks() </s> return hash(self._proxy.uri)	__hash__
# todo: remove this test as soon as all old test methods are migrated </s> def backend(self) -> genericbackend:	backend @pytest.fixture
# todo: explicitly commit files by name </s> return exitcode == 0	is_available def is_available():
# todo: kernels need some sort of structured form </s> def transforms(self):	transforms return [tps.transform for tps in self.tps]
#todo this method is so weird, find a unused address to inject code not the base address </s> idaapi.step_into()	get_dbg_brk_linux32@62 for i in xrange(len(code)): idc.patch_dbg_byte(inj +i, ord(code[i])) idc.GetDebuggerEvent(WFNE_SUSP, -1) idaapi.step_into()
# todo: this is an important operation that should be controlled </s> self._run.write_attr(name, val)	_init_attrs def _init_attrs(self): assert self._run is not None self._run.write_attr("cmd", self.cmd_args) self._run.write_attr("env", self.cmd_env)
# todo: implement the shit herein instead of collectionrepo </s> this transformation of a filename to a handle's key may change.	_filename2key def _filename2key(self, fname, branch):
# todo: remove "--feature=2020-resolver" when pip 20.3 is released </s> assert sys_exit.value.code == assert_exit	run_pipx_cli_exit run_pipx_cli(pipx_cmd_list) if assert_exit is not None:
'''todo: add docs''' </s> def discrete_columns(self):	discrete_columns @property
# todo: self._line_structures is a work-around and this needs </s> date = structure.get('date', none)	_ParseLogLine time = structure.get('time', None) if not (date and time):
# todo consolidate with other drivers </s> self._work_queue.put((delay + time.time(), job))	post_job
# todo(cdent): make this something other than a stub </s> except exception.servicetooold as exc:	init_application logging.setup(CONF, "nova") try: return error_application(exc) conf = conf_files[0]
# todo: comment this </s> if extension.endswith('.' + engine):	Site continue extension = filename[len(basename) + 1:] return (u'/'.join(path_parts + [filename]), extension[:-(len(engine) + 1)],
# todo: enable this.  this is more like cpython.  note that i worked </s> def islocalname(self, name):	isLocalName
# todo: set numpy setflags </s> def halo(self):	halo @property
#todo: fix this better </s> except:	_try_execute_and_wait with open(os.path.join(outs_dir, cmd[0]+'_stdout.txt'), 'w') as stdout: with open(os.path.join(outs_dir,cmd[0]+'_stderr.txt'), 'w') as stderr: print(sys.exc_info()[0]) _ALL_ELAPSED.append(elapsed)
#todo - code itself tolerates ambiguous bases (as nan). </s> counts[letter] = [float(pseudocounts[letter])] * self.length	FrequencyPositionMatrix counts[letter] = [0.0] * self.length elif isinstance(pseudocounts, dict): else: for letter in self.alphabet.letters:
# todo: this is untested. </s> self._problems.append(e)	_VerifyHelper try: result = callback(connection, cert, error_number, error_depth, ok) return 0 else:
# todo support subvars (@sub.var1) </s> _kwargs = {key: value for key, value in obj.items() if key not in ("data", "from_base64")}	from_path try: with open(ref_full_path) as fp: kwargs.update(_kwargs) return cls(obj["data"], from_base64=True, **kwargs)
# todo(b/179510447): align these parameters with schulman 17. </s> sample_batch_size=num_environments,	normalization_dataset_fn return reverb_replay_normalization.as_dataset(
# todo: this test requires manifold access, see: t88318502 </s> @unittest.skipif(not torch.cuda.is_available(), "cuda not available")	TestCaffe2Export ts_model.save(os.path.join(d, "model.ts")) def testMaskRCNN(self): def testMaskRCNNGPU(self): self._test_model("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml", device="cuda")
# # todo: </s> return 'twitter'	get_feeder_name
# todo: get this from cache. </s> now = datetime.datetime.now()	get_live_notifications@76 @classmethod return Notification.objects.filter( start_date__lte=now, end_date__gte=now)
# todo threading </s> elif col_type == "date":	to_sql_type_mappings return 'BIGINT' elif col_type == 'boolean': return 'DATE' elif col_type == 'bytes':
# todo use smart_open again when https://github.com/rare-technologies/smart_open/issues/207 will be fixed </s> return struct.unpack(fmt, file_handle.read(num_bytes))	struct_unpack def struct_unpack(self, file_handle, fmt):
# todo: comment as to why this is </s> _to_string = self._to_string	__lookup if context is not None: context = _to_string(context)
# todo: add logging to indicate the failure </s> self.router = router	add_router
# todo: add invocation wrapper #1353 </s> assert temporary_domain in result.output	test_bob_view_with_preexisting_configuration assert "domains" in result.output
# todo: make this work for sql </s> datespan=datespan,	export_all_form_metadata_async domain, format=format, user_ids=user_ids, ))
# todo: ensure collisions can't happen by verifying the config_prefix is empty </s> preferred_name = context.memory.free_layer_name(prefix = preferred_name)	GenericIntelProcess prefix = self.vol.layer_name + "_Process_") else: parent_layer = context.memory[self.vol.layer_name] parent_config = parent_layer.build_configuration()
raise notimplementederror # the below does most probably not work anymore todo </s> return "it does not differ from `{0}`.".format(b)	detail def detail(a, b, a_to_b, b_to_a): elif not a_to_b: return "`{0}` is ahead of `{1}` by {2} commits:\n{3}".format(a,b,len(b_to_a),"\n".join(b_to_a))
"bpm"        : float(self.bpm), # todo: serialise timevar etc </s> def get_elapsed_sec(self):	get_elapsed_sec
# todo subject.cn from cert? </s> last = ret[key]	TestMac ret[key].append(val) else: return ret def assert_common_signed_properties(self, info):
todo: factorize once #1457 is merged. </s> acq_time = get_cds_time(days.values, msecs.values)	_add_scanline_acq_time days, msecs = self._get_acq_time_visir(dataset_id)
# todo: find the id that's suitable for this extruder </s> logger.log("d", "converting containerstack {stack} to {type}", stack = container.getid(), type = container_type)	_convertContainerStack container_type = container.getMetaDataEntry("type") if container_type not in ("extruder_train", "machine"): new_stack = None if container_type == "extruder_train":
stats_to_calculate = ['mean', 'std', 'min', 'max']  # todo: get input from user </s> return os.path.exists(self.infile)	collect_local
#todo: detect the size of gpu pointeur and c int. </s> if type(node.op) == tensor.incsubtensor:	local_gpu_incsubtensor gpu_from_host(x), gpu_from_host(y), x, y = node.inputs[0:2] assert isinstance(x.type, tensor.TensorType)
pass  # todo </s> def _value_type_rgbcolor(self, *args):	RivalMouse self._device = None def _value_type_choice(self, value): pass  # TODO def _value_type_none(self):
# todo: this scrolling is lame and centers text :/ </s> msg = "[{time}] {msg}\n"	MSG else:
# todo: add and store preprocessing errors. </s> args:	LinuxDistributionPlugin class LinuxDistributionPlugin(interface.FileArtifactPreprocessorPlugin): ARTIFACT_DEFINITION_NAME = 'LinuxDistributionRelease' knowledge_base (KnowledgeBase): to fill with preprocessing information. file_object (dfvfs.FileIO): file-like object that contains the artifact
# todo test this logic with effect </s> prefix=stable_prefix,	publish_docs keys=existing_version_keys - new_version_keys)) old_prefix = yield Effect( target_prefix=version_prefix)) if old_prefix:
# todo this might have the same bug the r package had </s> save (bool): true to save the plot, false to display it in a blocking thread	plot_random_forest_feature_importance trained_rf_classifier (sklearn.ensemble.RandomForestClassifier): x_train (numpy.array): A 2D numpy array that was used for training best_rf = trained_rf_classifier if not isinstance(best_rf, sklearn.ensemble.RandomForestClassifier):
# todo: uris as bytes </s> if 'date' in data:	_convert_mpd_data album_kwargs[b'name'] = data['album'] if 'title' in data: track_kwargs[b'date'] = data['date'] if 'musicbrainz_trackid' in data:
# todo(ericbidelman): support more than one filter. </s> d['id'] = self.key().id()	AppUser d = self.to_dict()
# todo caching? prebuilt mode table? </s> per can_set_mode.	set_mode
# todo debug </s> self.triggered = false	RuleElement class RuleElement: def __init__(self): self.timeWhenTriggered = 0.0 self.timeTriggeredFor = 0.0
# temporary for testing. todo: remove </s> rate=buy_rate,	add_buy_to_events BuyEvent( amount=bought_amount, fee_rate=fee_cost / bought_amount, cost=cost
# todo: backend tensorflow </s> losses = outputs_losses(true, inputs, targets)[1]	Model ) def train_step(inputs, targets): total_loss = torch.sum(losses) self.opt.zero_grad()
#todo: set secure automatically if being accessed by https. </s> (splitrequesthandler, handlerclass, object), {})	wrapRequestHandler def wrapRequestHandler (handlerClass):
# todo: description </s> def sourceguard_iface(iface_params, vlanmap_type, allinterf, enabled):	sourceguard_iface
# todo: implement this (maybe change answer type) </s> return center	center_generate if ((center - c) ** 2).sum() < ((size * 2) ** 2): pas = False
# todo only do these thing if status is true </s> ret_val = {}	format_rabbit_message def format_rabbit_message(self, item): routing_key, my_obj = item self.logger.debug('rabbit_message:{0}'.format(my_obj))
#ack = self.serial_port.read() # todo: use ack </s> self.serial_port.close()	Disconnect def Disconnect(self): try: except serial.SerialException: sys.stderr.write("Error closing the port {0}".format(self.serial_name))
# todo: can this be optimized to avoid duplicating the anchors? </s> a = utils.generate_pyramid_anchors(	get_anchors if not hasattr(self, "_anchor_cache"): self._anchor_cache = {} self.config.RPN_ANCHOR_SCALES, self.config.RPN_ANCHOR_RATIOS,
# todo: all the expand stuff! </s> view.textcursor().inserttext(text)	insert@34 def insert(name, view):
# todo: send back error </s> self._send_packet(session, packet)	_send_data_packet packet = {'opcode': OPCODE_DATA, 'block_number': block_number,
# todo yield </s> info = get_status_dict(	_query_remotes known_remotes = ds.repo.get_remotes() remotes = [name] if name else ['here'] + known_remotes action='query-sibling', path=ds.path,
# todo: convert non uris to file uris. </s> return typefind.peek(0, 11) == b'[playlist]\n'	detect_pls_header def detect_pls_header(typefind):
# todo: move to base class </s> arr2 = [i[2] for i in rectangles]	getNodesRect n_rect = QtCore.QRectF(n.scenePos(), QtCore.QPointF(n.scenePos().x() + float(n.w), n.scenePos().y() + float(n.h))) rectangles.append([n_rect.x(), n_rect.y(), n_rect.bottomRight().x(), n_rect.bottomRight().y()]) arr3 = [i[1] for i in rectangles] arr4 = [i[3] for i in rectangles]
assert returned == 0  # todo return different value? </s> out, err = capsys.readouterr()	test_help_cmd def test_help_cmd(self, capsys): returned = cmd_main(["help", "list"], {'dep_file': 'foo.db'}) assert "Purpose: list tasks from dodo file" in out assert "file used to save successful runs [default: foo.db]" in out
# todo: should this be filtered? </s> if len(l): d.label = l	set_labels def set_labels(self, lst): assert len(lst) <= len(self.bufferdata)
# todo: kill this debug print. </s> url='http://www.example.com')	test_handle_patch def test_handle_patch(self): self.http_request_succeed("foo") self.assertTrue(reply['success']) self.assertEqual(reply['body'], "foo")
# todo: css pointer-events have been set to none for the nested anchor tag </s> def row(*args, **kwargs):	Row @component
# todo: rate should not have to be inversed </s> arbitrage_loop.insert(0, next_node)	retrace_negative_loop if next_node not in arbitrage_loop: arbitrage_loop.insert(0, next_node) arbitrage_loop = arbitrage_loop[arbitrage_loop.index(next_node):] return arbitrage_loop
# todo: consider using eafp here instead. </s> return hasattr(obj, '__call__')	_is_callable
# todo : move this control elsewhere </s> out += int_to_bytes(self.packet_id, 2)	PacketIdVariableHeader self.packet_id = packet_id def to_bytes(self): return out @classmethod
## todo : log error </s> @return: a tuple of string values or a tuple of string value tuples	stringify_listvars Convert each item in the list, into a string (replace None with the empty string ""). string_list = [] try:
# todo: do something clever with originselectionrange and targetrange. </s> def handle_response(self, response: 'optional[any]', position: int) -> none:	LspGotoCommand request = request_type(document_position) client.send_request( window = sublime.active_window() if response:
# end todo </s> raise notimplementederror('{} by {} must implemented in '	init_config def init_config(self, config): Args: 'subclass.'.format(self.name, self.author))
"size": 50  # todo: support pagination. </s> self.user = get_object_or_404(user, username=self.kwargs.get('username'))	ProjectIndex def get_queryset(self): queryset = Project.objects.public(self.request.user) queryset = queryset.filter(user=self.user) else:
pass  # todo - should this do something </s> self.builder.get_object("dialogreconfigure").show()	on_acceptmgmtinterface_clicked if dns1 + "," + dns2 != pif['DNS']: change = True else: self.builder.get_object("mgmtinterface").hide()
#todo: add deployment/replicaset? </s> statuses = ('schedulable', 'unschedulable')	kube_node_spec_unschedulable def kube_node_spec_unschedulable(self, message, **kwargs): for metric in message.metric: tags = ['{}:{}'.format(label.name, label.value) for label in metric.label]
# todo set only zero order </s> what is shape used for and what it really means.	_setup_shape Does it represent shape of the problem? :return:
# todo: not fool proof yet, but captures the most obvious cases. </s> self.ok_button.setenabled(true)	AddCardTypeDlg if not self.name.text(): self.OK_button.setEnabled(False) def accept(self): parent_instance = self.card_types[self.parent_type.currentIndex()]
# todo: self.assertfalse(prop.is_valid(np.bool8(false))) </s> self.assertfalse(prop.is_valid(baz()))	test_Instance self.assertFalse(prop.is_valid({})) self.assertTrue(prop.is_valid(Foo()))
# todo: investigate why this fails </s> to=first_model_def.model_ct)	test_simple_foreign_key_between_mutable_models first = FirstModel.objects.create(second=second) ForeignKeyDefinition.objects.create(model_def=second_model_def, second.first = first self.assertRaisesMessage(ValidationError,
#todo: check if/where this is used; if not used externally - remove </s> self._update_ui()	remove_surface self.surfaces.remove(surface)
# todo(frostig): finalize api. for now, return the underlying </s> dtypes.issubdtype(aval.dtype, np.complexfloating)):	_check_input_dtype_revderiv elif not allow_int and not (dtypes.issubdtype(aval.dtype, np.floating) or
# todo: in the future you can also add the possibility to synchronize from a chosen profile </s> nfo_settings = nfo.nfosettings()	LibraryActionExecutor self.params = params @common.inject_video_id(path_offset=1) nfo_settings.show_export_dialog(videoid.mediatype) library.execute_library_tasks(videoid,
# todo: wrap backend call in error handling. </s> return self._current_tl_track	get_current_tl_track def get_current_tl_track(self):
# todo g.ind_edges = sub2ind(size(g.w), g.v_in, g.v_out) </s> ev = np.sort(eigenvalues)	full_eigen def full_eigen(L):
loader = imageloader(32) #todo crop=true? </s> with self.test_session():	test_load_fixture loader = ImageLoader(32) #TODO crop=true? x, y = loader.load(fixture_path(), width=2, height=2)
# todo: improve this with a better resampling algorithm </s> self._check_finalized()	values_numpy try:  # This can fail in the case values are an iterable collection of non-numeric types (strings, etc.) return torch.stack(self.get_values()).cpu().numpy()
# todo: enable specificity beyond hostname (e.g. include scheme, port) </s> raise notimplementederror	go_away
#todo(wuzewu): version sort method </s> if total_length is none:	Downloader "Cache file %s not found, downloading %s" % (file_name, url)) r = requests.get(url, stream=True) with open(file_name, 'wb') as f: shutil.copyfileobj(r.raw, f)
# todo: move to base class </s> max_y = max(arr4)	getNodesRect min_y = min(arr3)
# todo: why with bookkeeping=false? </s> else:	BulkUpdate attr = self.mapper._columntoproperty[key] except orm_exc.UnmappedColumnError: return attr.key else:
spack.do_checksum = false        # todo: remove this global. </s> 'packages', nargs=argparse.remainder, help="specs of packages to install")	setup_parser '--fake', action='store_true', dest='fake', help="Fake install.  Just remove the prefix and touch a fake file in it.")
# todo: using variables (aka xccdf values) in ocil content </s> if "," in stig_id:	_verify_stigid_format return
status = 'published'  # todo: find a way for draft posts </s> base64string = base64.encodestring(("%s:%s" % (email, password)).encode('utf-8')).replace(b'\n', b'')	get_posterous_posts url = "http://posterous.com/api/v2/users/me/sites/primary/posts?api_token=%s&page=%d" % (api_token, page) request = urllib_request.Request(url)
except typeerror as e:          #todo: generalise for all api methods </s> cursor.close()	get_messages messages = cursor.fetchall()
# todo add verbose output </s> @localedir.setter	LocalizationModel@8 @property def localedir(self): def localedir(self, new_localedir): self._localedir = new_localedir
# todo: create a new, better, doc interface to remove it </s> triggered. if triggered from a private message, ``channel`` is	kick :param str channel: optional channel to kick ``nick`` from :param str message: optional message for the kick required. .. seealso::
# todo(user): remove after 184 is out. </s> self.assertequals(1000, sum(file_lengths))	FileOutputWriterEndToEndTest with files.open(filename, "r") as f: data = f.read(10000000) def testMultipleShards183Compat(self): entity_count = 1000
# todo: clean this api when tensorflow requirement is updated to >=2.6. </s> return constraints	_length_constraints constraints.append(tf.greater(length, 0)) if maximum_length is not None:
# todo(b/80125832): enable nccl in tests </s> text_format.merge(contents, step_stats)	testTraceFileStepStatsProto with open(trace_file) as f: step_stats = step_stats_pb2.StepStats()
# todo: should assert that the tag values are all strings </s> return ["%s %d" % (self.name, self.counts[()])]	CounterMetric return ",".join(["%s=%s" % kv for kv in zip(self.keys, values)]) def render(self): return ["%s{%s} %d" % (self.name, self._render_key(k), self.counts[k]) for k in sorted(self.counts.keys())]
# todo: confirm necessity of this session clearing and lay out mechanics. </s> self.assertraises(securesystemslib.exceptions.formaterror,	test_download_url_to_tempfileobj_and_urls def test_download_url_to_tempfileobj_and_urls(self): download_file = download.safe_download download_file, None, self.target_data_length) self.assertRaises(tuf.exceptions.URLParsingError,
# todo return, catch exception in main() </s> print("offset   string")	print_static_strings print("%s" % (s.s)) elif os.path.getsize(path) > MAX_FILE_SIZE: print("------   ------") has_string = False
# todo: match line and arrowbox </s> matches = re.match(asy_wrapper_pat, asy)	extract_asy_body body = asy[len(matches.group(0)) :] assert matches
# todo: legacy behavior, should remove after new case processing </s> def has_case_id(case_block):	has_case_id
# todo: should be none for undef instead?  or ''? </s> span_id = word.spanidfromerror(e)	_ValToArithOrError raise else: self.errfmt.PrettyPrintError(e, prefix='warning: ') return i
# todo: check the data! </s> count += 1	test_csv pipe = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def) count = 0 self.assertTrue(i == {u'FamilyNumOfJourneys': u'0', u'Member': u'Lancaster', u'MPOtherEuropean': u'0', u'FamilyTotal': u'0', u'OfficeRunningCosts': u'19848', u'MPOtherRail': u'233',
# todo(vek): need to pass context in for access to auth_token </s> def __init__(self, name, state):	InstanceInfo self.name = name assert state in power_state.valid_states(), "Bad state: %s" % state
#@todo: replace lambdas </s> return item[1]["order"]	get_sort_key
# todo !!! remove all this </s> return [self._get_state_attr_by_column(state, column) for column in self.primary_key]	_primary_key_from_state
# todo indexerror? </s> return order	normalize_order order = str(order).upper() if order not in ['C', 'F']:
# todo: remove </s> *musicpd.org, reflection section:*	urlhandlers @protocol.handle_request(r'urlhandlers$') ``urlhandlers`` Gets a list of available URL handlers.
# todo: fill with last value. </s> stdv = 1. / math.sqrt(n)	SplineGCN n = self.in_channels for k in self.kernel_size: self.weight.data.uniform_(-stdv, stdv) if self.bias is not None:
# todo: handle output diffing with plugins? i.e. image diff, svg diff, json diff, etc. </s> "compare source and outputs of cells x,y exactly."	compare_cell_source_and_outputs if x["cell_type"] != y["cell_type"]: return False
# todo: log exception </s> virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.multiline)	scan@71 except Exception as e: return None results = [] for (file, result) in virusresults[:]:
# todo: currently the functional tests only pass when run in a specific </s> if len(args._get_kwargs()) > 1:	_run_from_commandline def _run_from_commandline(): # pragma: no cover try: rc = args.func(args) else:
# todo: remove this global variable hacks after refactoring process. </s> response = self.client.get('/load_file',	TestViews def test_load_file(self): test_arg = 'dlopen/test.c' query_string=dict(path=test_arg)) deps = json.loads(response.json['deps'])
# todo: should wait for the end of the ongoing test case, and stop gracefully netmon and procmon </s> self.logger.error(	restart_target return False time.sleep(3) "no vmcontrol or procmon channel available ... sleeping for %d seconds" % self.restart_sleep_time )
# todo: in 0.6.0 change this to "disabled": false </s> option key 'key.pem'	test_enabled_missing option dev_type 'tap' option dh 'dh.pem' option mode 'server' option proto 'udp'
# todo: think how to resolve landscape.io warning: </s> self.event_type = event_type	MouseEvent class MouseEvent(object): def __init__(self, current_key=None, event_type=None, mouse_x=0, mouse_y=0): self.mouse_x = mouse_x self.mouse_y = mouse_y
# todo: deprecated - remove in version 0.10 </s> "of type '{}', but should be policy, an array of "	_create_ensemble "Invalid param `policies`. Passed object is "
# todo better check would be if the node is linked to the output and actually used </s> obj_count += get_total_particle_count(psys, false)	get_obj_count_estimate for obj in depsgraph.objects: try: except AttributeError: pass
# todo: kill this after dictionaries build correctly </s> 'beam_min_length': 8,	_test_batchsize 'gpt2_size': 'small', 'text_truncate': 16, 'inference': 'beam', 'beam_size': 1,
# todo: sessions and not only dates/days should be considered </s> bardt = self.data.datetime.date(index)	_barisover_months dt = self.lines.datetime.date(index)
# todo: fix this! </s> if x < min_value:	clamp x = min_value elif x > max_value:
# todo: validate </s> handler.setformatter(logging.formatter('%(levelname)s : %(message)s'))	formatted_logger @pytest.yield_fixture def formatted_logger(handler): logger.addHandler(handler) yield logger
# todo: supports blocking queries and all consistency modes </s> def __init__(self, agent):	Check self.agent = agent def ttl_pass(self, check_id):
# todo: parse flags, error checking, etc. </s> self.var_stack.append(self.top)	Push self.top = {}
# todo: rate should not have to be inversed </s> distance_to[quote_currency] = distance_to[base_currency] + graph[base_currency][quote_currency]	relax except KeyError:
# todo: keep this as a dict through the whole path to simplify this code </s> if module.startswith("/"):	_copy_module raise errors.AnsibleFileNotFound("%s is not a module" % module) in_path = os.path.expanduser(os.path.join(self.module_path, module))
# todo: do properly with .predict() </s> ax.plot(x, y, alpha=0.03, c='red')	plot_regression_from_trace if chains is not None: for chain in range(100, len(trace) * trace.nchains, chains): H2D, bins1, bins2 = np.histogram2d(trace_slope, trace['inter'], bins=50)
# todo: @sbharadwajj implement and test </s> def _weight_jac_t_mat_prod(self, module, g_inp, g_out, mat, sum_batch=true):	Conv1DDerivatives raise NotImplementedError def _weight_jac_mat_prod(self, module, g_inp, g_out, mat): raise NotImplementedError
# todo(ddeja): those 2 options should be gathered from config. </s> try:	KombuRPCListener :param message: the plain amqp kombu.message with additional information message.ack() except Exception as e:
#todo - add checks in case we don't have a well-formatted xmlns </s> table_name = sanitize(name)	get_table_name return TABLE_PREFIX + table_name
# todo guard on system (provides_system_bus) </s> device = self.nm_client.get_device_by_iface(iface)	_is_device_activated return device and device.get_state() == NM.DeviceState.ACTIVATED
# todo: - torch.abs(h_emb + r_emb - t_emb) </s> neg_score = self.calc_score(h_emb=neg_h_emb, r_emb=neg_r_emb, t_emb=neg_t_emb)	TransE neg_r_emb = self.relation_embeddings(neg_r) neg_t_emb = self.entities_embeddings(neg_t) loss = self.loss_fct(pos_score=pos_score, neg_score=neg_score) return loss
if colorsorter._parent_csdl and colorsorter._parent_csdl.reentrant: ##### todo: use different flag </s> state.sorted_by_color = colorsorter.sorted_by_color	_suspend state._parent_csdl = ColorSorter._parent_csdl state.glpane = ColorSorter.glpane state._cur_shapelist = ColorSorter._cur_shapelist state.sphereLevel = ColorSorter.sphereLevel
# todo: this here always returns empty response. if/when we want to </s> expected_set = set(expected_tags) if expected_tags else none	compare_account_data expected_tags = expected_entry.get('tags', None) expected_tags_str = 'no tags' if not expected_tags else ','.join(expected_tags) msg = ( f'Comparing account data for {got_address} got tags [{got_tags_str}] '
# todo: could cache the results of this for speed </s> messages.extend(import_location(domain, loc))	import_locations for loc in data:
# todo(stephenfin): use a helper </s> source_host = server['os-ext-srv-attr:host']	_create_server host. server = super()._create_server(networks='none') target_host = 'host2' if source_host == 'host1' else 'host1' return server, source_host, target_host
w, h = tiledsurface.n, tiledsurface.n # todo: support for other sizes </s> fn = helpers.uri2filename(uri)	open_last_cb uri = self.recent_items.pop().get_uri()
# todo: same code as for batch gradient, but with sum_batch = true </s> super().__init__(linearconcat, grad, params=["weight"])	GradLinearConcat class GradLinearConcat(FirstOrderExtension, LinearConcatDerivatives): def weight(self, module, grad_input, grad_output): shape = module.weight.shape
# todo: is incref required? </s> typobj = nth.typeof(itemobj)	check_element_type with c.builder.if_then( cgutils.is_null(c.builder, typobj),
# todo this is ideal for threading. </s> d['password'] = d['password'].replace('\\@','@')	parse_host_connect_string p = re.compile (r'(?P<username>)(?P<password>)(?P<hostname>[^:]*):?(?P<port>[0-9]*)') m = p.search (hcs) return d
return [] # todo: look at htmlify.py </s> get_needle('typedef', 'qualname'),	needles_by_line get_needle('function', 'name'), get_needle('variable', 'qualname'), get_needle('typedef', 'name'), get_needle('macro', 'qualname'),
# todo(dcramer): this doesnt handle concurrency </s> self.logger.info('fetching %r', url)	_get_response if params is None: params = {} resp = getattr(requests, method.lower())(url, params=params, **kwargs) if resp.status_code == 404:
# todo: mac os handling </s> if text is none:	TextEditor return text.replace('\r\n', '\n') return text return None if sys.platform=='win32':
# todo: revision 5 </s> key = cycle(key)	xor @return: The xored bytes
# :todo: implement test. </s> 'transaction': text_type(self.transaction, 'ascii'),	GetBundlesRequestFilterTestCase ``transaction`` is not a TrytesCompatible value. self.assertFilterErrors( }, {
# todo: manage exceptions when parameters cannot be decoded. </s> if submission is not none:	get_task_type@57 for x in [submission, file_cacher]].count(True) not in [0, 2]: raise ValueError("Need file cacher to grade a submission.") task = submission.task if task is not None:
lambda responses: responses,  # todo </s> if parameter.tan_process != '2':	_process_response for security_function, parameter in self.get_tan_mechanisms().items(): if security_function == '999': continue try:
# todo: candidate for move to system/hdparm </s> mnt_pt = mnt_prefix + sname	is_share_mounted return is_mounted(mnt_pt)
# todo: use initializer_list<k> and initializer_list<v> perhaps?  do </s> self.write(' : ')	visit_conditional_expr self.accept(o.cond) self.write(' ? ') self.accept(o.else_expr)
# todo: test this </s> values = style[name]	get_value Return the value of a property as a string, defaulting to 'initial'. if name not in style: if hasattr(values, 'value'): return values.value
# todo candidate for move to system/osi as not btrfs related </s> return the id	share_id def share_id(pool, share_name): returns the subvolume id, becomes the share's uuid. root_pool_mnt = mount_root(pool) out, err, rc = subvol_list_helper(root_pool_mnt)
# todo:  we might need additional logic comparing the state of git-annex </s> remote_info = ds_remote_info[ds_path]	Publish lgr.debug("Attempt to publish %i datasets", len(content_by_ds)) published, skipped = [], [] if not remote_info: lgr.debug("Skipping dataset at '%s'", ds_path)
# todo: /data/local/tmp might not be execuable and atx-agent can be somewhere else </s> return pid of package name	_pidof_app text = self._reqsess.get(self.path2url('/pidof/' + pkg_name)).text if text.isdigit():
# todo: create xxx_failure test </s> tests = exclude_from_resultlist(r, 'failure')	test_result_space_failure p = op.join(app.config['ROOT'], 'tests/fixtures/ttf/Font-Light!.ttf') r = run_set(p, 'result') self.assertTrue(check('test_space', failure_tests), lookup('test_space', tests))
# todo: this is not thread-safe! </s> reorg_cursor = db.cursor()	reorg reorg_cursor.execute('''SELECT * FROM blocks WHERE block_index = (SELECT MAX(block_index) from blocks)''') last_block_index = util.last_block(db)['block_index']
# todo: fix this, this is one of the few cases where using the config </s> prepares a test suite to be used for running tests	_make_test_suite_loader :param references: List of tests references to be resolved and transformed into test factories
# todo(mitmul): remove this when cupy.random.choice becomes available </s> assert proposals.shape[1] == 4	_check_data_type_forward def _check_data_type_forward(self, proposals, gt_boxes): assert proposals.dtype.kind == 'f' assert isinstance(proposals, (np.ndarray, cuda.cupy.ndarray))
# todo implement this function </s> set a system setting to a certain value	set_system_setting :param setting_name: name of setting, could be volume, brightness :param setting_value: value of setting
# todo: put this into timeframegroup. #316 </s> if not intersect.empty:	check_for_overlap def check_for_overlap(self, other): raise ValueError("Periods overlap: " + str(self) + " " + str(other))
# todo: write tests </s> "when there is an otherwise-unspecified validity error that prevents parsing."	MeiValidityError pass
# todo: fix </s> fnty = lir.functiontype(lir.inttype(32),	string_gt_impl @lower_builtin(operator.gt, std_str_type, std_str_type) @lower_builtin('>', std_str_type, std_str_type) [lir.IntType(8).as_pointer(), lir.IntType(8).as_pointer()]) fn = builder.module.get_or_insert_function(fnty, name="str_compare")
except exception:  # todo: what could happen here? </s> except exception:  # todo: what could happen here?	fix_extension_on_pickles@19 try: if os.path.isfile(txt): raise RuntimeError("Could not migrate Pickle file from .txt extension to .p extension.") from None
""" todo(datapipe-1525): this fixture override the `mock_source_cluster_name` </s> def producer(self):	producer producer = mock.Mock(autospect=Producer) return producer
except exception as e:  # todo: do not use bare except </s> def create_payload_buffer(self, entry):	create_payload_buffer
# todo: maybe could be simplified using contexts </s> else:	__isub__ return self._add_sub(other, operator.sub)
# todo: save the new xml to the database </s> self.handle("username")	Command help = "Scrubs the username from all forms associated with the given user" def add_arguments(self, parser): def handle(self, username, **options): DOMAIN = 'test-proj-2'
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> self.fail("not implemented")	Test_AcquireTokenWithUsernamePassword context = AuthenticationContext(authorityUrl, cache) def callback(err, tokenResponse): context.acquire_token_with_username_password(resource, sampleParameters['username'], sampleParameters['password'], sampleParameters['clientId'], callback)
# todo: look at the model to see which revision is last. </s> res = self.app.get(offset)	test_purge def test_purge(self): assert 'No revision id specified' in res
# todo: remove this (ssh_user is a legacy arg) </s> if ssh_user:	_user_or_ssh_user logger.warning(( 'The `ssh_user` argument is deprecated in `ssh.*` operations, '
# todo: don't add output_type in dry_run mode? </s> def get_namemap(config):	get_namemap
# todo(ahundt) add weight loading </s> x = activation('relu')(x)	AtrousFCN_Resnet50_16s bn_axis = 3 x = Convolution2D(64, 7, 7, subsample=(2, 2), border_mode='same', name='conv1', W_regularizer=l2(weight_decay))(img_input) x = MaxPooling2D((3, 3), strides=(2, 2))(x) x = conv_block(3, [64, 64, 256], stage=2, block='a', weight_decay=weight_decay, strides=(1, 1), batch_momentum=batch_momentum)(x)
# todo some complication with -1 label </s> " from %s to %s during fit."	test_estimators_overwrite_params for k, v in params.items(): assert_false(np.any(new_params[k] != v), % (name, k, v, new_params[k]))
"""todo: add documentation""" </s> @staticmethod	Filecontents contexts = (None, "repositories") value_class = api.filecontent.Filecontent def json(value, parameters): def part_as_dict(part):
# todo: reformat or delete </s> camera.lookat[1] = 0.85	sawyer_pusher_camera_upright camera.trackbodyid = 0 camera.distance = .45 camera.lookat[2] = 0.45 camera.elevation = -50
#todo: get darknet class number from class file </s> x = depthwise_separable_resblock_body(x, 1024, 4)	darknetlite_body x = depthwise_separable_resblock_body(x, 512, 8)
# todo: enable custom config </s> def on_setup():	on_setup
# todo: surface user-facing error here </s> for key, value in tags.items()	DauphinPartition return [] return [ ] def resolve_runs(self, graphene_info):
# todo handle bad type </s> return tokens[1].strip()	__trimgit def __trimgit(status_line): if status_line.find('->') >= 0: tokens = status_line.split(':') return tokens[1].strip()
# todo refactor this to use one code path for all lists </s> else:	download_nvd_dbs download_archives('https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-' + str(currentyear) + '.json.gz', 'nvd/cve-items-' + str(currentyear) + '.json.gz')
# todo find a better way to set up these defaults </s> from fusesoc.main import _get_core	_run_test_util _get_core(cm, 'mor1kx-generic') _get_core(cm, 'atlys')
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
#todo: check the data! e.g. pubdate etc. </s> todo: have these tests iterate over a number of test pipelines	test_feed pipe_def = self._get_pipe_def("testpipe1.json") p = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def)
# todo: remove all elements of the list and remove the blacklist </s> tfa,	test_api_typed def test_api_typed(): tfa.activations, tfa.callbacks,
# todo: add permissions </s> if voice:	_get_owner for server in self.servers: for channel in server.channels:
@pytest.mark.skip()  # todo: fix this </s> @async_test	TestWebClient response = client.conversations_list(exclude_archived=True) self.assertIsNotNone(response) async def test_issue_560_failure_async(self): client = self.async_client
# todo change this to a class. </s> sys.stderr.write(s)	echo@142 def echo(s=''):
# todo check magic class methods and return them also </s> def get_parent_until(self, *args):	Exec self.base = base
# todo(nnorwitz): enable test. </s> result = ast._convertbasetokenstoast(list(tokens))	testTemplateWithMultipleTemplateArgsMid def testTemplateWithMultipleTemplateArgsMid(self): self.assertEqual(1, len(result)) types = [Class('Foo'),
# todo: remove print </s> class error(exception):	Error
# no todo item selected </s> def selectable(self):	selectable
# todo: really dirty. figure out a better way. </s> limit = limits_mapping[action_type]	_apply_actions_limit Returns the modified (or not) all_actions actions_mapping = self.actions_per_location[action_type] if current_num_actions >= limit: return all_actions
# todo(b/141243467) remove these workarounds. </s> except:  # pylint: disable=bare-except	_is_eager_tensor return False
# todo: use triple factory </s> trans_h = transh(triples_factory=self.factory)	test_trans_h def test_trans_h(self):
# :todo: implement test. </s> ],	GetInclusionStatesRequestFilterTestCase 'tips': [ TransactionId(self.trytes1), } filter_ = self._filter(request)
# todo: confirm necessity of this session clearing and lay out mechanics. </s> url_with_unsupported_uri = self.url.replace('http', 'file')	test_download_url_to_tempfileobj_and_urls download_file, 'http://localhost:' + str(self.PORT+1) + '/' + self.random_string(), self.assertRaises(requests.exceptions.InvalidSchema, download_file, url_with_unsupported_uri, self.target_data_length)
# todo: initialize config in a unified place </s> default sender configs are in base_session_config['sender']	ExpSenderWrapperSSARNStep class ExpSenderWrapperSSARNStep(ExpSenderWrapperSSAR): super().__init__(env, learner_config, session_config) self.n_step = self.learner_config.algo.n_step
assert study_id == 0  # todo </s> self.trials[trial_id].state = state	InMemoryStorage return trial_id def set_trial_state(self, study_id, trial_id, state): def set_trial_param(self, study_id, trial_id, param_name, param_value): assert study_id == 0  # TODO
# todo: store this data </s> value = value * 91 + ord(ch) - 33	_parse_base91 def _parse_base91(text): value = 0 return value
# todo: this can permit a failing program to run by eliminating </s> if y.dtype != 'float32':	local_gpu_incsubtensor go_gpu = True gpu_y, = y.owner.inputs y = tensor.cast(y, 'float32') gpu_y = as_cuda_ndarray_variable(y)
# todo: return annexrepo instead if there is one </s> raise notimplementederror("todo")	set_state Parameters ----------
# todo: use an "event" subcase of subject </s> 'timestamp': form.received_on,	edit_item 'user_id': oc_user.user_id, 'username': oc_user.username, 'audit_type': 'Item data value updated', 'old_value': item_dict['value'],
# todo: handle agg_columns. </s> def idxmin(self, skipna=true):	idxmin
# todo: support for 'file' type </s> model_name = model_spec[model_marker]	marshal_model :type model_value: Model instance :rtype: dict model_type = swagger_spec.definitions.get(model_name, None) if model_type is None:
# todo: series support is not implemented yet. </s> raise notimplementederror()	agg
# todo: python-components: for now, we call each preprocessor's graph_fn directly. </s> apply(input[, input2, ...]?): sends one (or more, depending on the 1st sub-component's `apply` method)	Stack sub-component's api-metehod's number of return values has to match the forth sub-component's api-method's number of input parameters. DataOpRecord(s) through the stack and returns one (or more, depending on the last sub-Component's `apply` method) DataOpRecords.
# todo expand the policy before checking if the sts:assumerole action is allowed </s> set r.lastupdated = {aws_update_tag}	load_policies MATCH (aa:AWSAccount{id: {AWS_ACCOUNT_ID}}) MERGE (aa)-[r:AWS_POLICY]->(pnode) for policy in policies: session.run(
#todo wrap the lp api or use library </s> self.name = pkg	AbsPkg class AbsPkg: self.des = des
# todo: make it optional </s> def run_with_slack(runner, test_at_the_end: bool = false):	run_with_slack @slack_sender(webhook_url=slack_webhook_url, channel=slack_channel)
# todo: use dictfield and listfield to do validation </s> validate[operation](values)	BaseNestedFieldSerializerFactory } if self.update_data_is_valid(data): return data else:
# todo: check output </s> model.nonlinear_solver.options['max_sub_solves'] = 0	test_hierarchy_iprint model.nonlinear_solver = NewtonSolver() model.linear_solver = ScipyIterativeSolver() g1.nonlinear_solver = NewtonSolver() g1.linear_solver = LinearBlockGS()
# :todo: implement test. </s> self.assertequal(tryte[1], trit(1))	test_init_mixed_types As a convenience, you are allowed to initialize a Tryte using ints. tryte = Tryte([Trit(1), 1, -1]) self.assertNotEqual(tryte[2], Trit(-1)) self.assertEqual(tryte[2].value, -1)
# todo better to write a separate function for this </s> return false	reference_invariant for simplex in self.simplices: if any(simplex not in self.vertex_to_simplices[pt] return True
# todo(pep612): fix for paramspectype </s> bound or constraints, instead of giving an error.	apply_generic_arguments 'def (int) -> int'. Note that each type can be None; in this case, it will not be applied. tvars = callable.variables assert len(tvars) == len(orig_types)
# todo it seems that yahoo! converts relative links to absolute </s> elif from_location > 0:	pipe_fetchpage@46 to_location = content.find(to_delimiter, from_location) if from_location > 0 and to_location > 0: content = content[from_location:] elif to_location > 0:
# todo(mordred) when this changes to rest, force interface=admin </s> _tasks.machinenodeportget(port_id=mac))	get_nic_by_mac def get_nic_by_mac(self, mac): try: except ironic_exceptions.ClientException: return None
# todo: deprecation warning </s> def delete(self, request, *args, **kwargs):	DestroyAPIView class DestroyAPIView(mixins.DestroyModelMixin, GenericAPIView): return self.destroy(request, *args, **kwargs)
# todo: in the future we will log the trace </s> return self.primary_endpoint	_get_host
# todo: write this in human </s> in_name = os.path.join(root, f)	Listings if ext in ignored_extensions: continue out_name = os.path.join( kw['output_folder'],
# todo: remove this patching when the `content` property is supported. </s> def parse_css(filename):	parse_css
# todo retry connects </s> c_socket, addr = s.accept()	TCPStore s.settimeout(0.5) while self._run.value == 1: except socket.timeout: continue
# todo: ensure that if multiple flags are provided, the *last* one overrides </s> arg, i = shopt_spec.parse(argv)	Shopt if arg.p:  # print values if arg.o:  # use set -o names
except exception:  # todo: be specific </s> r = get(url, headers=headers)	get_insta_json def get_insta_json(url): headers = {"Accept-Language": "en"} json_start = r.text.find("window._sharedData") + 21 json_stops = r.text.find("</script>", json_start) - 1
#todo: check the data! </s> for i in p:	test_feed pipe_def = self._get_pipe_def("testpipe1.json") p = pipe2py.compile.parse_and_build_pipe(pipe_def, verbose=True) count += 1 self.assertTrue("the" in i.get('description'))
# todo: try/except these calls </s> try:	image_tag return '<img src="data:image/png;base64,\n%s\n" />' % ( re.sub(r'(.{60})',r'\1\n',str(ba.toBase64()))) svg = str(self._name_to_svg[match.group("name")]) except KeyError:
# todo: need to deterministically set to polynomial fitter </s> assert_allclose(self.profile_tools.rng_mode.state.x_range, (1, 15))	TestProfileTools self.viewer.axes.figure.canvas.button_press_event(x, y, 1) x, y = self.viewer.axes.transData.transform([[15, 4]])[0] self.profile_tools.ui.button_fit.click() self.profile_tools.wait_for_fit()
#ack = self.serial_port.read() # todo: use ack </s> if self.serial_port.isopen():	Disconnect def Disconnect(self): self.serial_port.close() except serial.SerialException:
# todo: implement me </s> except exception, e:	get_current_time try: position, format = self.playbin.query_position(gst.FORMAT_TIME) logging.warn("get_current_time: caught exception: %s" % e) return None
assert study_id == 0  # todo(akiba) </s> self.param_distribution[param_name] = distribution	set_study_param_distribution def set_study_param_distribution(self, study_id, param_name, distribution):
# todo: compute bezier curve. </s> return [(int(p['anchor'][1] * self._psd.header.width),	anchors blocks.get(TaggedBlock.VECTOR_MASK_SETTING2)) if not vmsk: int(p['anchor'][0] * self._psd.header.height)) for p in vmsk.path if p.get('selector') in (1, 2, 4, 5)]
'ms_ssim': false,  # todo: enable fixed_ms_ssim </s> self.assertalmostequal(results_rc[1]['vmafrc_float_ssim_score'], results[1]['vmafossexec_ssim_score'], places=5)	test_run_vmafrc_compare_directly_with_ossexec_422_10bit self.assertAlmostEqual(results_rc[1]['VMAFRC_motion2_score'], results[1]['VMAFOSSEXEC_motion2_score'], places=5) self.assertAlmostEqual(results_rc[1]['VMAFRC_adm2_score'], results[1]['VMAFOSSEXEC_adm2_score'], places=5) self.assertAlmostEqual(results_rc[1]['VMAFRC_float_ms_ssim_score'], results[1]['VMAFOSSEXEC_ms_ssim_score'], places=5) self.assertAlmostEqual(results_rc[0]['VMAFRC_score'], results[0]['VMAFOSSEXEC_score'], places=4)
# todo: also preserve __module__, __name__ and a few other important attrs </s> try:	FuncProfile if FuncProfile.in_profiler: return self.fn(*args, **kw) FuncProfile.in_profiler = True return profiler.runcall(self.fn, *args, **kw)
# todo(mottodora): add reduce option </s> error of two inputs.	mean_squared_error Returns: ~chainer.Variable: return MeanSquaredError(ignore_nan).apply((x0, x1))[0]
pass  # todo... </s> return [tuple(beam_width if i == self.idx_dim else array[i] for i in range(len(array)))]	infer_shape def infer_shape(self, node, input_shapes):
# todo: we could find a way to handle this. </s> module: the module object needing the binaries	considerExtraDlls def considerExtraDlls(self, dist_dir, module): Args: Returns: tuple
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
#todo: info page, test other objs </s> def _init_fuzzer(self, aggr):	_init_fuzzer
time.sleep(1)  # todo: avoid race conditions in other way </s> def test_csr_no_sans(self):	test_csr_no_sans
# todo: handle multiple skip stacks </s> block_level_width.without_min_max(box, containing_block)	block_replaced_width @handle_min_max_width def block_replaced_width(box, containing_block):
raise notimplementederror # todo </s> return self._id	id @property
# todo: process form submission </s> def col_delete_single(sid):	col_delete_single @app.route('/col/delete/<sid>', methods=('POST',)) source = get_source(sid) delete_collection(sid)
if isinstance(err, asyncio.cancellederror):  # todo: remove when updated to 3.8 </s> def start(self, batch_size: typing.optional[int] = 10):	BlobAnnouncer@27 if announced: await self.storage.update_last_announced_blobs(announced) assert not self.announce_task or self.announce_task.done(), "already running" self.announce_task = self.loop.create_task(self._announce(batch_size))
# todo(shoyer): test fails on tpu </s> effect[0] = true	loop_body pos, count = state f = lambda pos, inc: (lax.add(pos, 1), lax.add(count, inc))
# todo is this still needed? </s> returns:	calcBoundingBoxCenter def calcBoundingBoxCenter(boxcorners): Args: mathutils.Vector -- centerpoint of the bounding box center = sum((mathutils.Vector(point) for point in boxcorners), mathutils.Vector())
# todo consider more type conversions? </s> return (x + self.width // 2 + self.width // 4 + self.width // 8,	get_midbottomright def get_midbottomright(self): y + self.height // 4)
# todo counts as yes if vhnd was not available </s> growth_monitoring_session_10 = format_percent(1, 0)	HealthStatus growth_monitoring_session_7 = format_percent(1, 0) growth_monitoring_session_8 = format_percent(1, 0) growth_monitoring_session_11 = format_percent(1, 0) growth_monitoring_session_12 = format_percent(1, 0)
pass  # todo... </s> array, start_idxs, batch_lens, beam_width = input_shapes	infer_shape def infer_shape(self, node, input_shapes):
# todo: improve performance, get rid of unfold </s> num_classes = bmat.size(2)	__reshape_for_conv_in def __reshape_for_conv_in(self, bmat, module): bmat = einsum('boc->cbo', (bmat, )).contiguous() bmat = bmat.view(num_classes * batch, in_channels, in_x, in_y)
# todo: this output will break output formats such as json </s> if message.code == 'suppressed-message':	_parse_pylint_informational ignore_messages = defaultdict(lambda: defaultdict(list)) for message in messages: match = _PYLINT_SUPPRESSED_MESSAGE.match(message.message) suppressed_code = match.group(1)
# todo: this should take a vector </s> return self._me	Me Edge inner product matrix if getattr(self, '_Me', None) is None:
# todo: handle requests with cookies (e.g. s3 pre-signed urls). </s> return the_string	html_decode ) for char in html_chars:
#todo - turn that into a working doctest </s> cri       corrected and republished in	Record CIN       Comment in EIN       Erratum in CRF       Corrected and Republished from PRIN      Partial retraction in
pass # todo </s> def collect_incoming_data(self, data):	collect_incoming_data
# todo: assert </s> self.remote.modify_repo(repo, "mirror_locally", "0", self.token)	createRepo repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "name", "testrepo0", self.token) self.remote.save_repo(repo, self.token)
#todo: dont unfold all, but allow enum_all() to work </s> self.generate_context_menu()	tree_on_menu menu_proc(self.h_menu, MENU_SHOW, command='')
# todo: kwargs </s> return lambda df: 0	df_len_overload if len(df.columns) == 0:  # empty df
# todo: verify the length of blockhashes. </s> def str2l(s):	str2l
# todo: improve this algorithm - maybe fortran/c </s> isubs = [isub]	BaseLoadBalancing isub = color[iproc] iproc1 = proc_range[0] + numpy.sum(num_procs[:isub]) sub_comm = comm.Split(isub) sub_proc_range = [iproc1, iproc2]
# todo: __prepare_scriptable__ was reverted from pytorch: d25061862 </s> f.write(f"could not found code for {name} (type={mod.original_name})\n")	dump_code code = get_code(mod) name = prefix or "root model" f.write("\n") else:
# todo refactoring remove </s> return self.get_line(self.position[0])[:self.position[1]]	get_position_line
# todo: +kwargs </s> files,	get_last_commit_hash paths""" try: ['git', 'log', '-n', '1', '--pretty=format:%H'], expect_fail=True)
#todo: dataset/hda by id (from history) or check_ownership for anon user </s> return {	_summary_hda_dict } api_type = "file" 'id'    : encoded_id, 'name'  : hda.name,
# todo: delet this when all preprintproviders have a mapping </s> def banner_path(self):	banner_path if self.logo_name: return '/static/img/preprint_providers/{}'.format(self.logo_name)
# todo: move this scopes conversion from and to string into a utils function </s> return self.error_response(e, uri, **kwargs)	PreAuthorizationMixin self.oauth2_data = kwargs return super(PreAuthorizationMixin, self).get(request, *args, **kwargs)
# todo: implement me </s> parent = self.get_parent_conn()	distrib_branch_level if i.conntimer is not None: i.conntimer.cancel() if parent is not None: self.queue.put(slskmessages.HaveNoParent(0))
return none  # todo better error handling here </s> if email_address:	authorize_redirect_url "scope": OAUTH_SCOPE, "access_type" : "offline",  # to get a refresh token args['login_hint'] = email_address args["approval_prompt"] = "force"
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
# todo: implement! </s> raise notimplementederror()	ListenerMixin :meth:`_handle`. _EVENTS = tuple() def _stop_platform(self): raise NotImplementedError()
# todo: test for the _correct_ revision_id value. </s> assert detail.object_type == "package", str(detail.object_type)	test_create_package assert detail.activity_id == activity.id, str(detail.activity_id) assert detail.activity_type == "new", str(detail.activity_type) elif detail.object_id == package_created['resources'][0]['id']: assert detail.object_type == "Resource", \
# todo remove </s> def parent(self):	parent Return the builtin scope as parent, because the arrays are builtins return builtin.Builtin.scope
# todo do something with temp </s> s.resample(temp=temp)	resample_states def resample_states(self,temp=None):
if not self.hoster_url:  #@todo: remove in 0.4.10 </s> else:	XFSAccount else: premium    = False self.logDebug("VALID_UNTIL_PATTERN not found") m = re.search(self.TRAFFIC_LEFT_PATTERN, html)
# todo: implement </s> text = self.mainwindow().currentview().textcursor().selection().toplaintext()	copy_dehyphenated def copy_dehyphenated(self):
raise mpdnotimplemented # todo </s> def _findadd(self, type, what):	_findadd result = self._find(type, what)
# todo(ntonci): add a check for small motion </s> dq_w_e_vec.append(dq_w_e)	compute_dual_quaternions_with_offset dq_W_E = dq_W_B * dq_B_H * dq_H_E dq_W_E.normalize() return dq_W_E_vec
#@todo: move to utils in 0.4.10 </s> return self._log("debug", args)	log_debug def log_debug(self, *args):
# xxx todo </s> for self.ns in server:	sendTCPRequest def sendTCPRequest (self, server): " do the work of sending a TCP request " try: self.socketInit(socket.AF_INET, socket.SOCK_STREAM)
# todo(nate): temporarily disabled </s> self.step.job.build_id.hex, self.step.id.hex, contents['job_step_id'])	ManifestJsonHandler contents = json.load(fp) if contents['job_step_id'] != self.step.id.hex: except Exception: self.logger.exception('Failed to parse manifest.json; (build=%s, step=%s)',
# todo: here we should check for the leverage based on the config value </s> break	on_order_cancellation for index, item in enumerate(self.buy_orders[base_asset]): if item[0] == order.qty and item[1] == order.price: else: for index, item in enumerate(self.sell_orders[base_asset]):
# todo: candidate for move to system/hdparm </s> mnt_pt = mnt_prefix + sname	is_share_mounted def is_share_mounted(sname, mnt_prefix=DEFAULT_MNT_DIR):
# todo: implement </s> try:	can_print_pdf except ImportError: logging.info('Importing interwibble failed') printer = interwibble.UrlPrinter() except TypeError, err:
# todo - move or delete </s> with iso15765_2.isotp(self.arb_id_request, self.arb_id_response) as tp:	test_create_iso_tp def test_create_iso_tp(self):
#todo: check if/where this is used; if not used externally - remove </s> self._update_ui()	remove_surface def remove_surface(self, surface): self.gui.remove_surface(surface) self.save_surface_definitions_to_file()
# todo use the faster method </s> super(command, self).handle_noargs(**options)	handle_noargs def handle_noargs(self, **options):
pass  # todo - should this do something </s> if pif['dns'] != "" and radiomgmtdnsdhcp.get_active():	on_acceptmgmtinterface_clicked change = True if pif['DNS'] == "" and radiomgmtdnsmanual.get_active(): change = True if dns1 + "," + dns2 != pif['DNS']:
# todo: i18n plurals </s> 'event': self.request.event.slug,	get_next_url return self.request.GET.get('next') else: 'organizer': self.request.event.organizer.slug,
# todo before moving to pyart.io </s> if np.isscalar(d['data']):	_ncvar_to_dict d = dict((k, getattr(ncvar, k)) for k in ncvar.ncattrs() if k not in ['scale_factor', 'add_offset']) d['data'] = np.array(d['data']) d['data'].shape = (1, )
# todo uncomment label and description when they are </s> model = twofieldmodel	DynamicSerializer class DynamicSerializer(serializers.ModelSerializer): fields = ('field_b',)
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> "password" : "super secret password"	Test_AcquireTokenWithUsernamePassword "authorityHostUrl" : "https://login.windows.net", "clientId" : "04b07795-8ddb-461a-bbee-02f9e1bf7b46", # xplat's which is supposed to be in every tenant } authorityUrl = sampleParameters['authorityHostUrl'] + '/' + sampleParameters['tenant']
# todo(dcramer): it's a terrible api and realistically we should just be </s> elif source.repository.backend == repositorybackend.git:	get_release_id@39 if source.repository.backend == RepositoryBackend.hg: return vcs.run( counter = vcs.run(['rev-list', source.revision_sha, '--count']) return '%d:%s' % (counter, source.revision_sha)
# todo(pygeos) does not support empty strings, np.nan, or pd.na </s> y = np.arange(10).astype(np.float64) ** 2	test_points def test_points(): points = points_from_xy(x, y) assert isinstance(points, GeometryArray)
# todo: post operations should only add to db. </s> list all actionexecutions.	StactionExecutionsController return ActionExectuionAPI.from_model(actionexec_db) @expose('json') Handles requests: GET /actionexecutions/
# todo: make this pretty </s> client = s.getsockname()[0]	get_my_ip s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) try: except socket.error: client = "1.1.1.1"
# todo implement this method </s> add one state listener to the listeners list	AppStateMonitor self.package_name = package_name self.listeners = set() :param state_listener: :return:
# todo: documentation pending </s> "but the mode is currently set as %s. " % ('training by model.train()' if self.is_train else 'inference by model.eval()') +	_check_mode raise AttributeError("Training / inference mode mismatch. The argument `is_train` is set as %s, " % is_train +
# todo: untested </s> raise notimplementederror	PytorchWrapper def resize_output(self): raise NotImplementedError @contextlib.contextmanager def use_params(self, params): # pragma: no cover
# todo: not all values have exact matches in flexget, need to update flexget qualities </s> config.get('port'), parsedurl.path, config.get('api_key'))	CouchPotato if config.get('include_data'): profile_url = '%s://%s:%s%s/api/%s/profile.list' \ try: profile_json = task.requests.get(profile_url).json()
n = 1000  # todo: should scale with radius, dr </s> refx_all.append(x)	_points_ring3D ref /= np.linalg.norm(ref, axis=1).repeat(3).reshape((len(ref), 3)) ref *= dr*np.random.random(size=(len(ref), 3))+ r refy_all.append(y) refz_all.append(z)
pass  # todo - should this do something </s> change = true	on_acceptmgmtinterface_clicked if pif['DNS'] == "" and radiomgmtdnsmanual.get_active(): change = True if dns1 + "," + dns2 != pif['DNS']: change = True
# todo remove the unstable prefix when servers have updated. </s> logger.info("registering federation query handler for %r", query_type)	register_query_handler raise KeyError("Already have a Query handler for %s" % (query_type,))
# todo(py3.7): add required=true </s> def test_build(self):	test_build self.assertBuilds()
# todo: test this beam stuff (after you figure out wheter it's sufficient) </s> :class:`slur` in the ``slurbundle``.	addSlurs Because of how the MEI format specifies slurs, the strategy required for proper import to music21 is not obvious. There are two ways to specify a slur: must already exist in the ``slurBundle``, and special attributes must be added to the affected elements (``@m21SlurStart`` to the element at the start of the slur and
# todo: assert </s> def createrepo(self):	createRepo repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "name", "testrepo0", self.token)
except exception:  # todo - which exceptions? </s> if organism_element.tag == ns + 'name':	_parse_organismHost for organism_element in element:
# todo: take namespace into account, currently doesn't matter since </s> nses = {'private': [], 'shared': [] }	top_level_namespaces def top_level_namespaces(self, user_id): well as all the shared folder rows. with session_scope() as db_session: user = db_session.query(User).join(ImapAccount)\
# todo find out if this is good because of sparcity... </s> probs = np.zeros([len(x), len(self.estimators)])	GPyClassifier def predict(self, X): if self.estimators is None: for i, model in enumerate(self.estimators): probs[:,i] = model.predict(X)[0].flatten()
# todo(blk-u): the following should be </s> auth_json['trust_id'] = trust_id	_build_user_auth if tenant_id is not None: auth_json['tenantId'] = tenant_id return auth_json
# todo: distributed search messages need to implemented </s> self.logmessage("%s %s" %(msg.__class__, vars(msg)), 4)	UserJoinedRoom if self.chatrooms is not None: self.chatrooms.roomsctrl.UserJoinedRoom(msg)
"""todo: document me.""" </s> z = self.s_rng.normal(size=(m, self.mu.shape[0]),	random_design_matrix avg=0., std=1., dtype=config.floatX) return self.mu + T.dot(Z, self.L.T)
# todo(mriedem): perform some version discovery at some point. </s> resp = self.get(url)	_get_allocations @safe_connect def _get_allocations(self, rp_uuid): if not resp: return {}
# todo some kind of non-zero check to make sure that this passes. </s> print('run all tests in collection')	run_full_test_suite @task
# todo: it's a bit weird that to_dict modifies the object. </s> def mark_area(self, **kwargs):	mark_area self.mark = 'area' return self.configure_mark(**kwargs)
# todo(petef): support hard/soft limits </s> for child in self._worker.get_children()])	send_signal_child @debuglog def send_signal_child(self, pid, signum): children[pid].send_signal(signum)
raise mpdnotimplemented # todo </s> result = self._find(type, what)	_findadd @register(r'^findadd "(?P<type>(album|artist|title))" "(?P<what>[^"]+)"$')
# todo(brett.cannon) implement </s> return []	mock_implicit_hooks
# todo: localization support </s> skill_reload_thread	_load_watch_skills def _load_watch_skills(): ws.on('skill_manager', skills_manager) ws.emit(Message("skill_manager", {}))
# todo(eric_k): unicorn@778171fc9546c1fc3d1341ff1151eab379848ea0 doesn't like writing to </s> catches did_map_memory and copies the mapping into manticore	map_memory_callback logger.info(' '.join(("Mapping Memory @", hex(address) if type(address) is int else "0x??",
# todo(hvy): make percentile computation faster for gpus </s> x (array): target array for which sparsity is computed.	sparsity def sparsity(x): Returns: int: Number of zeros.
# todo: handle this </s> value = str(exception)	from_exception name = type(exception).__name__
# todo(mitmul): use cupy.random.choice if it becomes available </s> return argmax_overlaps_inds, max_overlaps, gt_argmax_overlaps_inds	_calc_overlaps gt_max_overlaps = overlaps[ gt_argmax_overlaps_inds, xp.arange(overlaps.shape[1])]
# todo subject.cn from cert? </s> while len(buf) > 0:	get_hash_hex_md5 hasher = hashlib.md5() with open(path, 'rb') as afile: hasher.update(buf) buf = afile.read(HASH_BLOCKSIZE)
# todo: reuse the utils method for service restarts </s> if not manager:	get_manager :param collection_mgr: The collection manager which holds all information in the current Cobbler instance. :return: The object to manage the server with. MANAGER = _IscManager(collection_mgr) return MANAGER
# todo parameters </s> f.write(u" with todo")	print_Say if stmt.with_ is not None:
# todo: use complete list of `order_by` fields </s> else:	_get_field_names permit_not=permit_not) if not permit_not: diff = set(field_name.replace('^', '') for field_name in new_field_names) - table_field_names
# todo(dcramer): implement timing for tsdb </s> rate=settings.sentry_metrics_sample_rate)	timing@30 def timing(key, value):
# todo: also validate that subsystem is a </s> arguments to the cached function must be hashable.	cache If *typed* is True, arguments of different types will be cached separately. For example, f(3.0) and f(3) will be treated as distinct calls with View the cache statistics named tuple (hits, misses, currsize) with f.cache_info(). Clear the cache and statistics with f.cache_clear().
# todo: remove conditional for v4 (always do the following) </s> old_value = (old_value, none)	_merge_file_leaf Returns: tuple or str: resulting value for the merged leaf if not isinstance(new_value, (list, tuple)): new_value = (new_value, None)
# todo(soren): we need this until we can stop polling in the rpc code </s> rv = self.cloud._format_describe_instances(self.context)	test_instance_update_state 'state': 0x01, 'user_data': '' self.assert_(len(rv['reservationSet']) == 0)
# todo: check values? </s> if not self.response.parsed_hdrs.has_key('location'):	status303 def status303(self):        # See Other
# todo: add metadata support when it is merged from develop </s> jid = str(jid)	_escape_jid def _escape_jid(jid): jid = re.sub(r"'*", "", jid) return jid
# todo: we should store a storage version number in later releases. </s> def __getitem__(self, key):	HybridDict self._keys.update(obj.keys()) def keys(self): for obj in self._dict_like_list: if key in obj:
# todo: have to assume trans.user here... </s> self.deserializers[ 'annotation' ] = self.deserialize_annotation	add_deserializers
# @todo: filter dropdown to just those who are accepting volunteers </s> page_name = "donate",	customise_req_home def customise_req_home(): current.menu.options = None view = "VM/CCC/views/donate.html")
# todo: properly get whether there is a body from the response </s> self.data += chunk	dataReceived
self.asserttrue(greps(err, "unit zz-unknown.service does not exist, proceeding anyway.")) #todo </s> [service]	test_6133_run_default_services_from_single_service_saved_container text_file(os_path(testdir, "zzb.service"),""" [Unit] Type=simple ExecStart=/usr/bin/testsleep 99
# todo: change this -- func def changed </s> save_message(body, message_data, workflow_execution_id, false)	test_save_message_no_valid_users 'requires_reauth': False} workflow_execution_id = 'workflow_uid4' messages = Message.query.all() self.assertEqual(len(messages), 0)
# todo: implement me </s> image = self.image.clone()	_test_gradcheck depth = self.depth.clone() depth = utils.tensor_to_gradcheck_var(depth)  # to var
# todo: delete this method when no longer needed </s> graph = pydigraph()	get_graph for sentence in sentences: graph.add_node(sentence)
assert false # todo </s> datasetpath = inputslot(stype='string') # the path to the original the dataset we're saving	OpBatchIo ExportDirectory = InputSlot(stype='filestring') # A separate directory to export to.  If '', then exports to the input data's directory Format = InputSlot(stype='int')                 # The export format ImageToExport = InputSlot()             # The image that needs to be saved Dirty = OutputSlot(stype='bool')            # Whether or not the result currently matches what's on disk
@jtu.skip_on_devices("gpu", "tpu")  # todo(b/145608614): svd crashes on gpu. </s> "_shape={}".format(jtu.format_shape_dtype_string(shape, dtype)),	testSlogdet @parameterized.named_parameters(jtu.cases_from_list( "shape": shape, "dtype": dtype, "rng_factory": rng_factory} for shape in [(0, 0), (1, 1), (3, 3), (4, 4), (10, 10), (200, 200),
# todo: for now, circumnavigate the detached head issue. </s> eq_(list(target.git_get_branch_commits("master")),	test_publish_simple@42 publish(dataset=source, dest="target") ok_clean_git(src_path, annex=False) list(source.repo.git_get_branch_commits("master"))) eq_(list(target.git_get_branch_commits("git-annex")),
# todo: check syntax, values? </s> self.setmessage('header-location', rs.redirect_without_location)	status303 def status303(self):        # See Other
# todo: test coverage of this branch </s> return render_to_response(	subscribe_user 'newsletter': my_newsletter, 'action': 'subscribe' "newsletter/subscription_subscribe_user.html", env, context_instance=RequestContext(request))
# todo(b/147296819): delete this line. </s> if p.returncode:	flush_and_unmount out, err = p.communicate() if mount._DEBUG:  # pylint:disable=protected-access raise ValueError('flush_and_unmount failed')
# todo add locales </s> logger.warning("%s" % e, exc_info=1)	domain_main_domain write_to_file("/etc/yunohost/current_host", new_main_domain) _set_hostname(new_main_domain) raise YunohostError("main_domain_change_failed") app_ssowatconf()
# todo(garyk): update the volumeops to read the state form the </s> resource = self._get_resource_for_node(nodename)	_get_volumeops_for_compute_node return resource['volumeops']
# todo: until we get it working. </s> "app.")	generate_jss_recipe else: facts["warnings"].append( recipe.append_processor({ "Processor": "JSSImporter",
pass # todo </s> def collect_incoming_data(self, data):	collect_incoming_data
# todo: probably warn about this. </s> instances = group.objects.filter(project=project).in_bulk(groups.keys())	associate_with_instance def associate_with_instance(project, groups): for key, records in groups.iteritems(): try:
# @todo: what else? (uninitialized variables!) </s> opts.append(tr(tds))	S3ACLWidget requires=attr.get("requires", None), hideerror=True, _value=k, if opts: opts[-1][0][0]["hideerror"] = False
# todo: handle this more gracefully? </s> invalid or port is not an integer or out of range.	parse_address def parse_address(address): words = address.split(':') if len(words) != 2:
# todo: open in new tab </s> messages.error(request, "csv import path does not exist. check config or file system!")	config_check_cron@139 stop_system_importer_file_csv_cronbased = True csv_import_file = model.csv_import_path + '/' + model.csv_import_filename stop_system_importer_file_csv_cronbased = True else:
# todo: pandas supports timedelta std, otherwise dask raises: </s> def test_map():	test_map
# todo update this code once keras > 2.0.4 is released </s> func = value_and_multigrad(self.loss, argnums=[0, 1])	loss_and_grad X = batch['X'] y = batch['y'] loss, (fX_grad, fC_grad) = func(fX, self.fC_, y) return {'loss': loss,
# todo don't know how to get right return value </s> rtn = ql.clipboard.format_available(params['uformat'])	hook_IsClipboardFormatAvailable def hook_IsClipboardFormatAvailable(ql, address, params):
#todo: may be this should go in a decorator for use in every command. </s> note: this is intended for private use in this class by now.	_annex_init If you have an object of this class already, there shouldn't be a need to 'init' again.
# logger ..." todo: this should be done before plugins discovery </s> parser_or_group = self.add_group(name, description=plugin_ep.description)	add_plugin_args for name, plugin_ep in plugins.iteritems():
# todo: really need crs specified properly in agdc-metadata.yaml </s> dst_nodata=dst_nodata,	reproject src_nodata=source.nodata, dst_transform=dst_transform, resampling=resampling, NUM_THREADS=4)
#todo: investigate why the flask provided request.json returns none. </s> destination_ip=data['server_host'],	feeder_data@33 protocol=data['protocol'], username=data['login'], destination_port=data['server_port'], source_ip=source_ip,
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
# todo: fill some sane numbers here </s> def getsearchid(self):	getSearchId
#todo change to native framework call, when plex allows token in header </s> request = urllib2.request(self.getlistsurl, headers=myheader)	playlistsV3 users = plexTV().getUserList() myHeader = {} playlists = XML.ElementFromString(urllib2.urlopen(request).read()) result = {}
"""todo: document me.""" </s> return self.mu + t.dot(z, self.l.t)	random_design_matrix def random_design_matrix(self, m): Z = self.s_rng.normal(size=(m, self.mu.shape[0]),
#todo - use sql for this, much more efficient! </s> return self.execute_and_fetch_col0(	list_biodatabase_names "SELECT name FROM biodatabase")
# todo g.ind_edges = sub2ind(size(g.w), g.v_in, g.v_out) </s> eigenvalues, eigenvectors = np.linalg.svd(l)	full_eigen def full_eigen(L):
# todo: i dont like in place updates. change this to somthing else. </s> frozenset(subdomain.get_var_names(isl.dim_type.param))	_insert_subdomain_into_domain_tree def _insert_subdomain_into_domain_tree(kernel, domains, subdomain): & kernel.all_inames()) idx, = kernel.get_leaf_domain_indices(dependent_inames)
#todo: update this to try fsfindfolder and fallback to this </s> import _winreg	_get_win_folder_from_registry def _get_win_folder_from_registry(csidl_name): registry for this guarantees us the correct answer for all CSIDL_* shell_folder_name = { "CSIDL_APPDATA": "AppData",
# todo: logging, or warning </s> if recurse_depth > 9:	exponential_backoff print ('Max number of tries exceeded while trying to open ' + filepath)
# todo: datetime.date, datetimeindex? </s> with c.builder.if_then(type_mismatch, likely=false):	check_element_type c.builder.store(cgutils.true_bit, errorptr) loop.do_break() c.builder.store(cgutils.true_bit, errorptr) c.pyapi.err_format(
# todo: error checking </s> if not self._cookie:	list_boards@62 nActionsSinceLastView, idOrganization @todo: the JSON response contains the values needed to dereference idFoo; be nice and expose raise AuthenticationRequired() headers = {'Cookie': self._cookie, 'Accept': 'application/json'}
# todo(dspasovski): fix this. </s> return category.objects.create(name='slap tickling', slug='booping',	get_new_cat type=amo.ADDON_WEBAPP)
# todo remove this line? i think it's not needed. (dave) </s> bnf_text = f.read()	load_grammar except KeyError: try: grammar = generate_grammar(bnf_text) return _loaded_grammars.setdefault(path, grammar)
# todo for now, simply ack msg_new.answer_msg_id </s> return self.connection.is_connected()	is_connected
# todo for each sampled sub epoch, validate number of segments </s> height_to_hash: dict[uint32, bytes32]	load_blocks_dont_validate header_cache: Dict[bytes32, HeaderBlock] = {} height_to_hash: Dict[uint32, bytes32] = {} prev_block = None difficulty = test_constants.DIFFICULTY_STARTING
pass  # todo... </s> return [tuple(beam_width if i == self.idx_dim else array[i] for i in range(len(array)))]	infer_shape def infer_shape(self, node, input_shapes):
# todo: improve error handling </s> server = none	get_server_and_client else:
# todo: it has only one element in current state. handle rest of elements. </s> def builtins_type(objects, bases, dicts):	builtins_type if bases or dicts: return NO_VALUES
# todo: move this config validation to acl object. </s> ('vlan', ('eth_dst', 'eth_type', 'in_port', 'vlan_vid')),	_configure_tables self.groups = ValveGroupTable() for table_id, table_config in enumerate(( ('vlan_acl', None), ('eth_src', ('eth_dst', 'eth_src', 'eth_type', 'in_port', 'vlan_vid')),
# todo danger error log: unable to save in fs </s> models.internaltip.id == unicode(id)).one()	get_tip_by_internaltip @transact def get_tip_by_internaltip(store, id): if not itip: raise errors.SubmissionGusNotFound
# todo: handle tuple assignment? </s> if not isinstance(grandchildren[0], assname):	ModelChecker return if isinstance(child, Assign): continue name = grandchildren[0].name
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
# todo this should be more modular </s> for element in itertools.product(*d.values()):	_dict_product def _dict_product(self, d): yield dict(zip(keys, element))
# todo increase precision </s> def create_monomial_exponents2(degree):	create_monomial_exponents2
# todo debug </s> self.timetriggeredfor = 0.0	RuleElement self.type = None self.triggered = False self.element = None
# todo: test me </s> return func(*args, **kwargs)	wrapped session.data['visited'] = visited d['$inc']['total'] = 1
# todo resource arns may contain wildcards, e.g. arn:aws:iam::*:role/admin -- </s> ingest_policy,	load_policies SET r.lastupdated = {aws_update_tag} for policy in policies: ARN=policy["Arn"], POLICY_ID=policy["PolicyId"],
# todo: fix this </s> return 0	Shopt b = (argv[1] == '-s') for opt_name in cmd_val.argv[2:]:
# todo: unit tests </s> def dashboard(auth):	dashboard user = auth.user dashboard_folder = find_dashboard(user)
# todo: non-int dict </s> for _, col_var in aggregate_node.df_out_vars.items():	aggregate_array_analysis equiv_set.insert_equiv(*all_shapes) post = [] typ = typemap[col_var.name] if typ == string_array_type:
# todo implement for all channels </s> return self._execute_cmd("set", "appliance.control.togglex", payload)	turn_on_channel def turn_on_channel(self, channel):
#web directories to use in case todo file is empty </s> return 0	get_avg def get_avg(state1, state2): Takes two values and returns the percentage of state1 that is state2. else: return (state2 / state1) * 100
# todo: test me </s> return{}	AddonFilesNodeSettings class AddonFilesNodeSettings(AddonNodeSettingsBase):
# todo: take care of theano to keras port: </s> shape = [1, 3, 1, 1]	vgg16_preprocess def vgg16_preprocess(X): else: shape = [1, 1, 1, 3]
#todo: assert that layer inputs are always >= 0 </s> epsilon=epsilon,	EpsilonProxyRule the LRP analyzer class to the decopmosition rules. def __init__(self, *args, **kwargs): bias=bias, **kwargs)
# truffle todo: revert </s> def _ishidden(path):	_ishidden
# todo: provide loghandler, which gives access to recent records or return stderr </s> assert_true(os.path.exists(os.path.join(dst, '.git', 'annex')))	test_AnnexRepo_instance_from_clone def test_AnnexRepo_instance_from_clone(src, dst): ar = AnnexRepo(dst, src) assert_raises(GitCommandError, AnnexRepo, dst, src)
# todo: remove in 0.24 when none is removed </s> ----------	score_samples @if_delegate_has_method(delegate='_final_estimator') def score_samples(self, X): X : iterable Data to predict on. Must fulfill input requirements of first step
# todo: replace with a call to int(_, 16) </s> if (argstr[i:i + len(tok)] == tok	tok else: if tok not in self.keywords: and (argstr[i + len(tok)] in _notKeywordsChars)): i = i + len(tok)
# todo review this </s> self.ctl.output("restart <gname>:*\trestart all processes in a group")	help_restart def help_restart(self): self.ctl.output("restart <name> <name>\tRestart multiple processes or " "groups")
# todo(twd2): send ac mail </s> @base.require_priv(builtin.priv_read_record_code | builtin.priv_write_record)	JudgePlaygroundHandler @app.route('/judge/playground', 'judge_playground') async def get(self): self.render('judge_playground.html')
# todo: waitlist </s> except oserror:	killMonitor try: self.wireshark_process.terminate() log.warn("Error during wireshark process termination") self.wireshark_process = None
# todo test that non 9.[01].x errors out </s> def test_execute_postgres(self):	TestSetupDB class TestSetupDB(unittest.TestCase): config_manager = self._setup_config_manager() klass = setupdb_app.PostgreSQLManager
# todo: turn this into an id lookup on a keystore. </s> return api.ecdsa_bytes2pub(self._power_ups[	CryptoPower raise NoSigningPower def pubkey_sig_tuple(self): keypairs.SigningKeypair].pub_key) except KeyError:
# todo: only serialize parent object, use: </s> utils.run_triggers(self.api, obj, "/var/lib/cobbler/triggers/delete/profile/post/*", [])	Profiles self.collection_mgr.serialize_delete(self, obj) if with_delete: utils.run_triggers(self.api, obj, "/var/lib/cobbler/triggers/change/*", []) if with_sync:
if self._ndim == 3: # todo: use hasz </s> return {	__array_interface__ @property 'version': 3, 'shape': (len(self), self._ndim),
# todo: make sure this is indeed stdcall </s> s = qlosutils.read_string(self.ql, address, '\x00\x00')	read_wstring s = s.replace("\x00", "") self.string_appearance(s)
# todo: tried to reuse the process, but something seems to be buffering </s> stdin=subprocess.pipe,	Generator def __call__(self, title, n=8, timeout=50.0): preprocessor = subprocess.Popen([self.preprocessor], stderr=subprocess.DEVNULL) title_pp = preprocessor.communicate((title.strip() + '\n').encode())[0].decode('utf-8')
# todo: specify page range </s> groups[found_key].append(obj)	group_objects found_key = attr
# todo: remove </s> abort(404, _('incorrect group type'))	_ensure_controller_matches_group_type if group.type not in self.group_types:
# todo: enable specificity beyond hostname (e.g. include scheme, port) </s> raise notimplementederror	go_away
# todo implement and test this </s> synchronously.	receive_message def receive_message(self, operation, request_id): Returns the message body. Asserts that the message uses the given opcode Arguments: - `operation`: the opcode of the message
# todo! the offset is a bit off. </s> drw = pdb.gimp_image_active_drawable(img)	CoaExport sprite.frame_index = int(frames) return sprite pdb.file_png_save2(img, drw,
# todo handle error situations </s> '%d tracks, %d albums, %d artists, and %d playlists found.',	do_search self.logger.warning(e) return result.track_total, result.album_total, result.artist_total, result.playlist_total)
# todo find out what is best used here! </s> return cs	get_hyperparameter_search_space default="linear")) max_depth = cs.add_hyperparameter(UniformIntegerHyperparameter(
# todo: add more checks here? </s> return false	SWFParser magic = body[:3] if magic in ('FWS', 'CWS'): def _is_compressed(self, swf_document): :param swf_content: The SWF file.
# todo: make sure package names can't be changed to look like package ids? </s> else:	_make_unicode for key, val in entity.items(): new_dict[key] = self._make_unicode(val) return entity
# todo bring back? </s> *gather_stats(mesh),	laplace@15 if verbosity > 1: print("\nStep {}:".format(k + 1)) extra_cols=["  maximum move: {:.5e}".format(max_move)] )
# todo: add theano function. </s> backend = k.backend()	gradients def gradients(Xs, Ys, known_Ys): if backend == "theano": assert len(Ys) == 1
# todo: remove when #980 has been merged </s> prefer_http = 1 if 'rtmp' in format['url'] else 0	_sortkey qidx = ['low', 'med', 'high', 'veryhigh'].index(format['3sat_qualityname'])
# todo: timestamp as datetime in payload must die. </s> return self.cls.load(manager, result.get_key())	map_lookup_result
# todo: this won't work for classes with __slots__ </s> yield delim.join(parts[:i])	all_ancestors parts = pathname.split(delim) yield parts[0]
# todo: comment as to why this is </s> context = _to_string(context)	__lookup def __lookup(self, (subject, predicate, object), context): _to_string = self._to_string i = 0 if subject is not None:
# todo: must be implemented </s> def should_fetch_kindlegen():	should_fetch_kindlegen
# todo implement properties for proteins </s> protein_mask = [true if p in protein_list else false for p in all_proteins]	get_protein_foreground_probability protein_mask = slice(None) else: if n_samples > 1 and return_mean is False: if return_numpy is False:
# todo test this </s> def max_likelihood(self,*args,**kwargs):	max_likelihood
# todo: add .data and .grad to syft tensors </s> if p.grad is none:	module_is_missing_grad def module_is_missing_grad(model): return True return False
# todo(blk-u): this doesn't look like it works as expected. </s> attrs = {'tenant_name': 'tenant', 'project_name': 'project'}	testProjectTenantCollision attrs = {'tenant_id': 'tenant', 'project_id': 'project'} self.assertRaises( self.assertRaises( exceptions.InvalidCredentials, auth.KeystoneV3Credentials, **attrs)
annot.annotation_metadata.validation_and_reliability = "todo"  # todo </s> fill_global_metadata(jam, lab_file)	create_JAMS def create_JAMS(lab_file, out_file, parse_beats=False): logging.info("Parsing %s..." % lab_file) annot = jam.sections.create_annotation()
return response(status=400)  # todo </s> message = ["nodes", self.known_nodes.abridged_nodes_dict()]	send_nodes def send_nodes(subscriber):
box = self._canvas._render_world._selection_box  # todo: make a way to publicly access this </s> evt.skip()	_box_click def _box_click(self, evt):
# todo(sbauza): remove the service_id filter in a later release </s> result = _ec2_instance_get_query(context).\	ec2_instance_get_by_id @require_context @pick_context_manager_reader filter_by(id=instance_id).\ first()
# todo: optimize me </s> is_api_node = auth.api_node == self	can_edit raise ValueError('Cannot pass both `auth` and `user`') user = user or auth.user else: is_api_node = False
def generate(self): # todo </s> return np.sum(self.abl[start:stop],axis=0)	likelihood_block
# todo this is a workaround since exceptions are currently not correctly stacked </s> return tregex_engine(pattern, self.jsflags)	__tregex_compile
# todo: remove dependency on legacy_examples </s> operator = customoperator	test_my_custom_operator@19 ):  # pylint: disable=redefined-outer-name caplog.set_level(logging.INFO, logger='CustomOperatorLogger') environments_path = test_project_environments_path() results = dagster_airflow_custom_operator_pipeline(
# todo add test for this </s> >>> aug = iaa.simplexnoisealpha(iaa.edgedetect(1.0), sigmoid_thresh=iap.normal(10.0, 5.0))	SimplexNoiseAlpha scale the simplex noise masks to the final image sizes, i.e. no nearest neighbour upsampling is used, which would result in rectangles with hard Same as the first example, but uses a threshold for the sigmoid function that is further to the right. This is more conservative, i.e. the generated
# todo: add comment here to explain these numbers </s> c.pump(repeat(1, 6))	ReadRequestLoadScenarioTest c = Clock() s = ReadRequestLoadScenario(c, FakeFlockerClient([node1, node2]), 5, interval=1) s.maintained().addBoth(lambda x: self.fail()) d.addCallback(lambda ignored: s.stop())
# todo: try to find a better way to deal with local execution </s> returns:	detail Args: worker: the worker doing the deserialization plan: a Plan object readable_plan, id, arg_ids, result_ids, name, tags, description, is_built = plan_tuple
# todo: give a vanilla example </s> re: float representing fraction of energy correctly assigned	feca@26 [array of ground truth power]} Returns fraction = np.array([]) for appliance in predicted_power:
# todo switch to pytest.skip() if verified to be expected failure </s> links = []	makelinks dimsizes = np.empty(ndims, dtype='int32') for dim in range(ndims): for ofmdim in np.ndindex(ofmshape): src = ofmdim[-1]
# todo: probabilistic with each route </s> def max_speed(self):	max_speed
# todo: look for a better implementation handling "hold[expression]". </s> if word == "":	reader yield last_word break continue if stream.io.seekable():
# todo: fancier. </s> req.raise_for_status()	http_fetch_release releases = [] status = req.status_code == requests.codes.ok if not self.release: for rel in req.content.split():
# todo: probably warn about this. </s> try:	associate_with_instance groups = dict(groups) instances = Group.objects.filter(project=project).in_bulk(groups.keys()) yield instances[key], records except KeyError:
""" todo """ </s> try:	read_file_from_install def read_file_from_install(self, path): lines = [] with open(path) as grub_cfg: lines = grub_cfg.readlines()
# todo: reuse the utils method for service restarts </s> creates a manager object to manage a dnsmasq server.	get_manager :param collection_mgr: The collection manager to resolve all information with. :return: The object generated from the class.
# todo: determine if this is object store safe and what needs to be </s> return check_complete_response	raw_check_complete def raw_check_complete(self): Get check_complete response from the remote server.
name: optional[str] = none) -> common_types.consistenttensortype:  # todo(b/64987151): remove # pytype: disable=annotation-type-mismatch </s> raise valueerror(	segment_indices A `Tensor` containing the indices within each segment. ndims = segment_ids.get_shape().ndims 'segment_indices requires a 1-dimensional input. ' 'segment_indices has {} dimensions.'.format(ndims))
if isinstance(err, asyncio.cancellederror):  # todo: remove when updated to 3.8 </s> while batch_size:	BlobAnnouncer@27 raise err log.warning("error announcing %s: %s", blob_hash[:8], str(err)) if not self.node.joined.is_set(): await self.node.joined.wait()
# :todo: implement test. </s> self.assertisinstance(	SendTransferCommandTestCase self.adapter = MockAdapter() def test_wireup(self): Iota(self.adapter).sendTransfer, SendTransferCommand,
# todo: instead of attributing it to the word part, it would be </s> "$pat") echo 'equal to glob string' ;;  // must be glob escaped	EvalWordToString pat="*.py" case $x in esac part_vals = []
# todo: #1823 - workaround for new nickname every restart </s> if final_word in ("symbol", "suit", "sign"):	nicename def nicename(symbol): unicode_name = unicodedata.name(symbol) final_word = unicode_name.split()[-2] return final_word.capitalize()
# todo: this is processor specific </s> if not idc.makecode(ea):	analyze_code_area @return: True if area successfully analyzed, otherwise False. if not idc.AnalyzeArea(start_ea, end_ea): return False idaapi.invalidate_dbgmem_config()
# todo check transformation to the reference element </s> u2 = nm.ones((2, len(self.mesh.coors) + 1, 1), dtype=nm.float64)	RK3Solver b = nm.zeros((2, len(self.mesh.coors) - 1, 1), dtype=nm.float64) u  = nm.ones((2, len(self.mesh.coors) + 1, tsteps, 1), dtype=nm.float64) u3 = nm.ones((2, len(self.mesh.coors) + 1, 1), dtype=nm.float64) u[:, 0, 0] = self.boundary_cond["left"]
ret = os.system("cd " + path + " && " + gradlew + " assembledebug")  # todo: change to release </s> def set_package_name(path, package_name):	set_package_name
return skiptest("test doesn't pass yet")  # todo(frostig) </s> ans = serial_pmap(pfun, axis_name)(x)	testTransposeAndAddRank3 pfun, axis_name = papply(fun, 2)
# todo: consider adding some error handling for bad/failed requests. </s> )	_get_user_hubspot_id urllib.quote(webuser.username) ), if req.status_code == 404: return None
# todo this paragraph is necessary, but not sure it works. </s> variable.	usages Return :class:`classes.Definition` objects, which contain all names that point to the definition of the name under the cursor. This .. todo:: Implement additional_module_paths :rtype: list of :class:`classes.Definition`
# todo investigate different results between mac and linux/win platforms </s> func.stack_adjustment = func.stack_adjustment	test_function_stack def test_function_stack(self): funcinfo = [] func.reg_stack_adjustments = func.reg_stack_adjustments func.create_user_stack_var(0, binja.Type.int(4), "testuservar")
# todo: arrange </s> self.remote.modify_repo(repo, "mirror_locally", "0", self.token)	createRepo repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "name", "testrepo0", self.token) self.remote.save_repo(repo, self.token)
# todo(user): remove after 184 is out. </s> ctx=none,	RecordsPool _RECORD_OVERHEAD_BYTES = 10 def __init__(self, filename, exclusive=False): Args:
# todo: common crud method </s> nthash = hashlib.new('md4', cleartext.encode('utf-16le')).digest()	nt_password def nt_password(cleartext):
# todo: replace usages of strictredis (redis-py 2.x) with redis in dramatiq 2.0. </s> except redis.watcherror:	decr return True
# todo: internal configuration conflicts within one package. </s> return [d.spec	dependencies def dependencies(self, deptype='all'):
# todo: use other libraries. </s> return 4 if get_bits() == 32 else 8	get_size_of_pointer
# todo(vek): need to pass context in for access to auth_token </s> assert state in power_state.valid_states(), "bad state: %s" % state	InstanceInfo self.name = name
# todo: time to say goodbye to the old mount namespace, see "man 2 unshare" to get some help </s> def _get_container_path(container_id, container_dir, *subdir_names):	_get_container_path
# todo pydocs </s> else:	BigQueryCursor self.job_id = None self.page_token = None self.page_token = query_results.get('pageToken') fields = query_results['schema']['fields']
# todo: more efficient copy </s> def __init__(self, dmm, fe_type):	StringArrayPayloadModel @register_model(StringArrayPayloadType) members = [ ('offsets', types.CPointer(offset_typ)),
# todo: remove this in v1.8. </s> something_cool_i_want_to_save: anything you define through model.on_save_checkpoint	dump_checkpoint CHECKPOINT_HYPER_PARAMS_NAME: CHECKPOINT_HYPER_PARAMS_KEY: LightningDataModule.__class__.__name__: pl DataModule's state }
# todo replace all that with ssh-copy-id </s> {'title': 'setting up ssh keys',	initSequences def initSequences(controller): 'functions':[installKeys]} ]
# @todo: make drop-down list of options </s> elif root_org == vnrc:	customise_hrm_programme_controller elif root_org in (CVTL, PMI, PRC): settings.hrm.vol_active = vol_active settings.pr.name_format = "%(last_name)s %(middle_name)s %(first_name)s" field = current.s3db.hrm_programme_hours.job_title_id
writeable=is_writeable,      # everything is false for now, todo later </s> id=row[1],	read_channel_data for row in result.fetchall(): data = KolibriExportedChannelData( path=channeldbpath, )
# todo: check </s> nn.init.xavier_uniform(self.entities_embeddings.weight.data)	TransR def _project_entities(self, entity_embs, projection_embs): return torch.matmul(projection_embs, entity_embs) nn.init.xavier_uniform(self.relation_embeddings.weight.data) nn.init.xavier_uniform(self.projection_matrices.weight.data)
status = 'published'  # todo: find a way for draft posts </s> request.add_header("authorization", "basic %s" % base64string.decode())	get_posterous_posts base64string = base64.encodestring(("%s:%s" % (email, password)).encode('utf-8')).replace(b'\n', b'') url = "http://posterous.com/api/v2/users/me/sites/primary/posts?api_token=%s&page=%d" % (api_token, page) handle = urllib_request.urlopen(request) posts = json.loads(handle.read().decode('utf-8'))
# todo: try removing the none checks after https://github.com/mozilla/rust-code-analysis/issues/528 is fixed. </s> "data/component_mapping.json",	download_component_mapping path_to_component = get_component_mapping(False) utils.download_check_etag( ) with open("data/component_mapping.json", "r") as f:
# todo: implement me </s> if self.chatrooms is not none:	UserJoinedRoom self.chatrooms.roomsctrl.UserJoinedRoom(msg) self.logMessage("%s %s" % (msg.__class__, vars(msg)), 4)
raise notimplementederror  # todo(mattjj) </s> def defreducer(prim, collective_prim):	defreducer
# todo: unify with instagram, maybe in source.get_comment() </s> else:	Flickr result = {'items': []} if activity_id: photos = photos_resp.get('photos', {}).get('photo', []) for photo in photos:
# if pycryptodomex is not available, we are done </s> self.assertequal(payload, enc)	test_aes_deencode_without_password def test_aes_deencode_without_password(self): payload = u'\u66f4\u7a33\u5b9a\u7684\u4ea4\u6613\u5e73\u53f0'
gc.collect()  # todo: see first comment above </s> assert_raises(valueerror, c, ['a', 'list'])	test_EnsureDataset def test_EnsureDataset(): c = EnsureDataset() assert_raises(ValueError, c, (1, 2, 3)) assert_raises(ValueError, c, {"what": "ever"})
#todo: code this </s> start, stop = slicetoroi(key, self.shape)	allocateStorage storage = numpy.ndarray(stop - start, dtype=self.dtype) return storage
# todo - fix meta.submission to point to real submission </s> return cursor	_get_cursor sql = sql + (" ORDER BY s.%s %s " % (sort_column, desc)) cursor = connection.cursor()
# todo: house ad </s> else:	show_to_geo if filter.filter_type == INCLUDE: if country_code in filter.codes: return False if filter.filter_type == EXCLUDE:
# todo: make this more efficient </s> if seek < 0:	seek @_raise_if_closed def seek(self, seek: int, whence: int = 0) -> int: raise ValueError(f'negative seek value {seek}') self._seek = seek
# todo: log discarded bytes? </s> supplied_crc = message[-2] * 256 + message[-1]	valid_crc calculated_crc = crc16.crc16xmodem(''.join([chr(item) for item in message[:-2]])) return supplied_crc == calculated_crc
# todo store account here </s> pickle = self.account.pickle()	to_session_dir path = os.path.join(session_path, account_file_name) try: f.write(pickle) except OlmAccountError as error:
# todo: should we check the arity? </s> return s	_make_get_literal s = self.literal(s)
oldsize = self.size # todo: remove </s> def read_uint(stream):	read_uint
_, g_loss = self.trainable_gan.forward_loss()#todo targets=['d'] </s> return "".split()	required
# todo(b/181866850): enable tokenize_with_offsets when it works and test. </s> model_prefix=model_prefix,	_make_sp_model_file control_symbols += ",[MASK]" full_vocab_size += 1 model_type="word", input=input_file,
# todo remove this paragraph, it's ugly and shouldn't be needed </s> def to_reverse():	_path name = self._name if name.api_type == 'module':
# todo: requires special treatment? </s> cls._building_line_to_game_entity(building_line)	_process_game_entities for unit_line in full_data_set.unit_lines.values(): cls._unit_line_to_game_entity(unit_line)
train_critic = tf.train.adamoptimizer(1e-4).minimize(vloss) # todo: parameterize </s> def getpolicy(self, state, **unused):	getPolicy
# todo(b/141575627): we handle complex-dtype sum-reduction directly as a </s> (v,), (d,) = vals_in, dims_in	_ppermute_batcher def _ppermute_batcher(frame, vals_in, dims_in, axis_name, perm): assert len(perm) == frame.size, "Permutation doesn't match the axis size!" assert d is not batching.not_mapped perm_indices = [None] * frame.size
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> context = authenticationcontext(authorityurl, cache)	Test_AcquireTokenWithUsernamePassword authorityUrl = sampleParameters['authorityHostUrl'] + '/' + sampleParameters['tenant'] resource = '00000002-0000-0000-c000-000000000000' # or 'https://management.core.windows.net/' def callback(err, tokenResponse): print(tokenResponse)
# todo: remove this after 2.4 as the switch to using underscores is a breaking change </s> serializer_class = serializers.interfaceconnectionserializer	InterfaceConnectionViewSet queryset = InterfaceConnection.objects.select_related('interface_a__device', 'interface_b__device')
# todo - this isn't actually the correct way to set the vary header, </s> raise errorresponse(status.http_405_method_not_allowed,	http_method_not_allowed {'detail': 'Method \'%s\' not allowed on this resource.' % self.method})
# todo: this is a hack -- document can't be converted to string </s> def _write(self, text, path):  # pragma: no cover, integration test	BaseFileObject pass  # pylint: disable=W0107 self._loaded = False if not self._exists: raise DoorstopError("cannot save to deleted: {}".format(self))
#todo: handle this exception </s> @return:	dbg_process_start def dbg_process_start(self, pid, tid, ea, name, base, size): return True
# todo: comment as to why this is </s> i += 2	__lookup i += 1 subject = _to_string(subject) predicate = _to_string(predicate) if object is not None:
# todo find the correct hbin </s> return self._parent	parent
# todo: remove? </s> raise smserror(_(	StockReportParser RequisitionActions.FULFILL, RequisitionActions.RECEIPTS "You can no longer use requisitions! Please contact your project supervisor for help" ))
pass # todo </s> self.family_filter = family_filter	SetFF self._SetLabels()
# todo(jk0): this will eventually need to take ssl into consideration </s> images.append(base_image_meta)	detail base_image_meta = self._translate_from_glance(image_meta)
# todo: can convert to @abstractmethod once subclasses handle it </s> raise notimplementederror('this document store is read only!')	ReadOnlyDocumentStore class ReadOnlyDocumentStore(DocumentStore): def delete_document(self, doc_id): raise NotImplementedError('This document store is read only!')
# todo: re-enable for hardware </s> self.one_stack_port_down(self.port_map['port_3'])	test_tunnel_path_rerouted def test_tunnel_path_rerouted(self): src_host, other_host, dst_host = self.net.hosts[:3] self.verify_tunnel_established(src_host, dst_host, other_host, packets=10)
# todo: fix this </s> [43, 46, ('c2', 'o')], [46, 47, ('o', 'c3')], [47, 48, ('c3', 'h3')],	TestLammpsForceFieldData [36, 37, ('C2', 'H2')], [36, 38, ('C2', 'H2')], [36, 39, ('C2', 'O')], [39, 40, ('O', 'C2')], [40, 41, ('C2', 'H2')], [40, 42, ('C2', 'H2')], [47, 49, ('C3', 'H3')], [47, 50, ('C3', 'H3')]] tangles = [[1, 0, 2, ('H3', 'C3', 'H3')], [1, 0, 3, ('H3', 'C3', 'H3')],
#todo: add transaction cost here also </s> episode = episode_data[i]	show_trader_path portfolio_value = portfolio_value_list[index] i += 1 last_price = episodic_data.data_average_price(data_dict, episode) reward = (portfolio_value_list[-1] + portfolio_list[-1]*last_price)
# todo: accept ifs as a named arg?  split('a b', ifs=' ') </s> note: we're leaving out su -c, find, xargs, etc.?  those should generally	OshCommandMain - oshc --help oshc deps run functions using the $0 pattern. --chained-command sudo
# # fixme: # todo: remove me </s> with open(os.environ['ail_bin']+'/torcrawler/blacklist_{}.txt'.format(service_type), 'r') as f:	load_blacklist def load_blacklist(service_type): redis_crawler.delete('blacklist_{}'.format(service_type)) lines = f.read().splitlines()
# todo: test me </s> def to_json(self, user):	AddonFilesNodeSettings class AddonFilesNodeSettings(AddonNodeSettingsBase):
# todo check that location is associated with this pipeline </s> return false	create_location location = api.location.post(new_location) except slumber.exceptions.HttpClientError as e: return location
# todo: show in display_problems() </s> pkg.root, dep_str, self._pkg_use_enabled(pkg),	_greedy_slots dep_str = " ".join(pkg.metadata[k] for k in blocker_dep_keys) try: parent=pkg, strict=True) except portage.exception.InvalidDependString:
raise typeerror(msg.format(dtype))  # todo(mattjj, dougalm): handle complex </s> if eqn.destructure:	jaxpr_to_graphviz fragment.extend(map(freevar_node, jaxpr.freevars, jaxpr.freevars)) fragment.extend(map(constant_node, jaxpr.constvars, consts)) id_name = next(id_names) fragment.append(function_node(id_name, eqn.primitive.name))
# todo: change this file format to be plain yaml and use safeloader </s> color palette for diverging palette types.	set_option 'style.color_palette_sequential': (str) Color palette for sequential palette types. 'style.color_palette_accent': (str) Color palette for assigning color to specific values.
# todo: hacked values for now </s> d = dist(data[i], data[indices[j]], *dist_args)	nn_descent for i in range(data.shape[0]): indices = rejection_sample(n_neighbors, data.shape[0], rng_state) heap_push(current_graph, i, d, indices[j], 1) heap_push(current_graph, indices[j], d, i, 1)
# :todo: implement test. </s> def test_fail_min_weight_magnitude_string(self):	ReplayBundleRequestFilterTestCase def test_fail_depth_too_small(self): ``depth`` is < 1. ``min_weight_magnitude`` is a string. self.skipTest('Not implemented yet.')
# todo: find a better solution for this? </s> run_commands(commands[subcommand]["script"], variables)	project_exec variables = config.get("variables", {}) commands = {cmd["name"]: cmd for cmd in config_commands}
# todo add write unlock </s> log.error("{} the specified path is not referred to the directory!".format(settings.data_folder_path))	init_data_files else:
# todo need copy? </s> rhs[2 * i_boundary + 1] = 0.0	newton_update matrix[2 * i + 1, 2 * i + 1] = 1.0 rhs = -jac_uniform(mesh) out = numpy.linalg.solve(matrix, rhs) return out.reshape(-1, 2)
print('warning: driver init returned none, {}'.format(ep.name))  # todo: use proper logger </s> except:	load_drivers def safe_load(ep): try: print('WARNING: failed to resolve {}'.format(ep.name))  # TODO: use proper logger return None
# todo: タイミング情報がとれるかをテストする </s> self.cap = cv2.videocapture(source)	init_capture try: if not self.cap is None: self.last_t = 0 finally:
# todo: test timezone-aware datetime.date </s> integerfield.type)	test_IntegerField from rows.fields import IntegerField self.assertEqual(IntegerField.TYPE, int) self.assertEqual(IntegerField.deserialize('42'), 42) self.assertEqual(IntegerField.serialize(42), '42')
# todo: entropy loss </s> eps = 1e-32	_decay_learning_rate for param_group in optimiser.param_groups: param_group['lr'] = max(param_group['lr'] - param_group['lr'] / steps, eps)
# todo(liuyuhui) only xpu broadcast parameters here. </s> hook(layer, input, output) -> none or modified output	register_forward_post_hook def register_forward_post_hook(self, hook): It should have the following form, `input` and `output` of the `hook` is `input` and `output` of the `Layer` respectively. Parameters: hook(function): a function registered as a forward post-hook
# todo: put action definitions in a dict and loop here </s> help="stop torrent")	add_options self.add_bool_option("-S", "--start", help="start torrent") self.add_bool_option("-H", "--hash-check", help="hash-check torrent")
# todo: handle multiple skip stacks </s> replaced_box_width.without_min_max(box, containing_block)	block_replaced_width def block_replaced_width(box, containing_block):
# todo deprecated? </s> parts = urlparse(iri)	iri_to_uri parts_seq = list( part.encode('idna')
# todo: move unset and set output in messages </s> self._assert_exec(':unset asd', messages.sessions.error_session_s_not_modified % 'asd', log_captured)	test_unset self._assert_exec(':unset', messages.terminal.unset_usage, log_captured)
# no todo item selected </s> def selectable(self):	selectable
# todo: fix singlemachinebatchsystem to support this call </s> os.remove(name)	robust_remove @classmethod def robust_remove(cls, name): except OSError: pass
# todo(b/163904067): mimic defun' behavior and reset the step seed to </s> raise valueerror('unexpected axis: %d (rank = %d)' % (axis, rank))	CumSum return tf.cumsum(x, axis=axis, exclusive=exclusive) if axis < -1: axis += rank length = GetShape(x)[axis]
# todo: where does this value come from? </s> return event_manager	get_environment event_manager = EventCore() plugin_manager = pycam.Plugins.PluginManager(core=event_manager)
# todo: remove when botfactory can force everything to be unthreaded </s> def test_get_site_url(site, expected):	test_get_site_url assert isup.get_site_url(site) == expected
# todo: this needs refactoring </s> for genus in genus_list:	in_genus_list if species.startswith(genus.capitalize()): return True
# todo: look in other supported bumpversion config locations </s> return all([	Project return len(self.labels) > 0 @property 'description' in properties and len(properties['description']) > 0 for label, properties in self.labels.items()
# todo: don't write them? is *much* slower on re-load (~3x) </s> self.loader[self.module_key + '2']	test__load__ If a module specifies __load__ we should only load/expose those modules self.update_module()
# todo remove? </s> def parent(self):	parent return builtin.Builtin.scope
# todo untested </s> _api.bn_free(bignum_serial)	get_serial_number finally: _api.OPENSSL_free(hex_serial)
# todo: use complete list of `order_by` fields </s> sys.exit(1)	_get_field_names if diff: missing = ', '.join(['"{}"'.format(field) for field in diff]) else: return new_field_names
# todo: remove when materialized paths are fixed in the payload returned from waterbutler </s> if guid_obj is not none:	_update_comments_timestamp enqueue_postcommit_task(partial(ban_url, node, [])) if root_id is not None: enqueue_postcommit_task(partial(ban_url, guid_obj.referent, [])) if page == Comment.OVERVIEW:
# todo(andi) comment types should be unified, see related issue38 </s> def _stripws_identifierlist(self, tlist):	StripWhitespaceFilter token.value = ' ' last_was_ws = token.is_whitespace() last_nl = None for token in tlist.tokens[:]:
# todo: use all of the followed objects as input to documentation. </s> par = self._definition	_path if x: path.insert(0, x) while par is not None: if isinstance(par, pr.Import):
# todo: requires special treatment? </s> cls._building_line_to_game_entity(building_line)	_process_game_entities for unit_line in full_data_set.unit_lines.values(): cls._unit_line_to_game_entity(unit_line)
# todo: serialize properly </s> parameters = self.params()	RepoResource return self.not_found(serialized) @auth_required(UPDATE) delta = parameters.get('delta', None) if delta is None:
#todo: consider factoring out: some duplication between xliff and tmx </s> unit = self.addsourceunit(source)	addtranslation unit.target = translation tuvs = unit.xmlelement.findall('.//%s' % self.namespaced('tuv'))
# todo(jheek): re-introduce this test when the tracer check is revived. </s> return x + self.param('bias', x.shape, initializers.ones)	MultiMethod class MultiMethod(nn.Module): @nn.module_method def l2(self):
# todo postremora replace the above with this line when remora goes away </s> u = get_object_or_404(userprofile, email=email)	ajax email = request.GET.get('q', '').strip()
# todo: make this regex verbose </s> def save(self):  # pragma: no cover, abstract method	BaseFileObject raise DoorstopError(msg) return data pass  # pylint: disable=W0107 self._loaded = False
# todo only if handle </s> def absolute_image_url(profile, image_name):	absolute_image_url url = safe_text(profile.image_urls[image_name]) if url.startswith("/"):
# todo: deprecate `extra_info` in favor of `options` </s> if not list_of_posts:	write_posts_to_csv :param credentials: Credentials for login - username and password, tuple :return:            CSV written in the same location with <<account_name>>_posts.csv print("Couldn't get any posts.", file=sys.stderr) return
# todo: see https://github.com/mozilla/openwpm/issues/867 for </s> f.write(browser_params.to_json())	start_webdriver@142 js_request_as_string = jsi.clean_js_instrumentation_settings(js_request) browser_params.js_instrument_settings = js_request_as_string if with_extension: xpi()
# todo perform error checking if the optimized clause is used. </s> return 'normal', {}	_default_numerical
# todo: return should cause an 'exit 0' reply? </s> up_result = result	Interactive prompt_plugin.Run() try: with tagswitch(result) as case: if case(parse_result_e.EmptyLine):
# todo: move conversion to string to enhanced json handler and rather pass objects in trace.log() </s> if schema_path:	validate_result :param task_name: name of task :param result: result of task with open(schema_path, "r") as input_file: schema = json.load(input_file)
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
# todo: run this loop in parallel ! </s> return self.alpha	learning_rate @property
# todo(dcramer): this doesnt handle concurrency </s> if params is none:	_get_response def _get_response(self, path, method='GET', params=None, **kwargs): params = {} params.setdefault('token', self.token)
# todo get a fallback consumption rate based on product/facility type </s> return none	monthly_consumption if daily_rate is None: daily_rate = default_consumption(case) monthly_rate = daily_rate * 365.2425 / 12. return monthly_rate
x0 = np.zeros(self.cum_states[-1])  # todo: pre-allocate? </s> if copy:	last_result def last_result(self, n=1, copy=False): return (np.copy(self.t[self.res_idx-n]), np.copy(self.x[self.res_idx-n, :]),
# todo: get pytest's coverage plugin working, iirc it has issues? </s> runner = "pytest"	test@52 modstr = "" if module is not None: if coverage: runner = "coverage run --source=paramiko -m pytest"
# todo: check that the birth date is not in the future </s> raise valueerror('no 9 digit birth numbers after 1953.')	_get_birth_date if year >= 1980: year -= 100 elif year < 1954: year += 100
# todo (in luxcore): </s> return luxcore_name, props	black props.SetFromString(""" scene.materials.{mat_name}.type = matte
# todo(mattjj): remove this special case, used for debugging on cpu </s> def parallel_callable(fun, axis_name, axis_size, *avals):	parallel_callable pvals = [PartialVal((aval, core.unit)) for aval in avals] with core.new_master(JaxprTrace, True) as master:
# todo: set numpy setflags </s> def halo(self):	halo @property
#todo - should the default be gapped(single_letter_alphabet) instead? </s> raise valueerror("must have at least one sequence")	ClustalWriter alignment.add_sequence(record.id.replace(" ","_"), record.seq.tostring()) self.handle.write(str(alignment))
# todo not tested </s> asn1_serial = _api.x509_get_serialnumber(self._x509)	get_serial_number def get_serial_number(self): Return serial number of the certificate bignum_serial = _api.ASN1_INTEGER_to_BN(asn1_serial, _api.NULL) try:
# todo: is there any way to divide up a single document into paragraphs? </s> document = []	read_document@45 sentence = [] for line in lines:
# todo: consider returning an empty () rather than raising </s> eq_(twin1.issuperset(twin2), true)	test_issuperset super_, sub_, twin1, twin2, unique1, unique2 = self._create_sets() eq_(sub_.issuperset(super_), False) eq_(twin2.issuperset(twin1), True) eq_(unique1.issuperset(unique2), False)
# todo: support aa </s> lh: array = xp.vstack([x.lh for x in sub_acts])	Activations @classmethod def join(cls, sub_acts: List["Activations"]) -> "Activations": po: Array = xp.vstack([x.po for x in sub_acts]) ah = list(map(xp.vstack, zip(*[x.ah for x in sub_acts])))
# todo: merge these </s> tags = {'tags': url_tag.netloc + ',maltrieve'}	upload_viper if response: url_tag = urlparse(response.url) url = "{srv}/file/add".format(srv=cfg.viper) headers = {'User-agent': 'Maltrieve'}
# todo present hw freq range </s> return bool(self.osmosdr_source_block.get_gain_mode(ch))	get_agc
# todo: do more stuff! </s> data = json.loads(req.body)	_parseRequestBody def _parseRequestBody(self, req): Raises an HTTPBadRequest exception if the request isn't valid JSON.""" except JSONDecodeError, e: logging.error(req.path+': Unable to parse JSON: '+str(e), exc_info=True)
# :todo: implement test. </s> def test_pass_addresses_only(self):	FindTransactionsRequestFilterTestCase self.skipTest('Not implemented yet.') def test_pass_bundles_only(self): self.skipTest('Not implemented yet.') def test_pass_tags_only(self):
# todo: multi dp route resolver needs to flood out stack ports </s> self.one_stack_port_down(first_stack_port)	test_tunnel_path_rerouted def test_tunnel_path_rerouted(self): self.verify_stack_up() src_host, other_host, dst_host = self.hosts_name_ordered()[:3] self.verify_tunnel_established(src_host, dst_host, other_host, packets=10)
# todo: check the data! </s> u'costofstayingawayfrommainhome': u'22541', u'stationeryassocdpostagecosts': u'3471',	test_csv count += 1 self.assertTrue(i == {u'FamilyNumOfJourneys': u'0', u'Member': u'Lancaster', u'MPOtherEuropean': u'0', u'CommsAllowance': u'9767', u'Mileage': u'3358', u'MPMisc': u'20', u'title': u'Mr Mark Lancaster',
self.assertequals(self.todolist.count(), 1) # force won't delete subtasks </s> command = deletecommand.deletecommand(["99"], self.todolist, self.out, self.error)	DeleteCommandTest self.assertEquals(self.output, "Removed: Bar p:1\n") self.assertEquals(self.errors, "") command.execute() self.assertFalse(self.todolist.is_dirty())
# todo: let the globe return the semimajor axis always. </s> return boundary.project(sgeom.point(*xy))	boundary_distance
# todo: take care of nested resources </s> _wsme_attributes = []	Resource def to_dict(self): d = {}
# todo: value check </s> note:	_get_y_viewport def _get_y_viewport(self): 1 = in visible viewport 0 = above viewport
svg = apply_template(svg, {"maxpoints":maxpoints, "trdn": trdn, "msg":"", "valuemid":"0.5", "timemid":"10s", "datapoints":"","init_max_y": "false", "max_y": 0, "seconds_scale":0, "y_shift": 0}) # todo templating engine </s> del tokenhashes[hashstr]	feeder for ptoken in lClean: tokenHashes[hashstr].remove(ptoken) gevent.spawn(notify) updates_received += 1
# todo: rethink default remote/tracking branch. see above. </s> if knows_annex(repo.path):	Update cmd_list.append(repo.get_active_branch()) std_out, std_err = repo._git_custom_command('', cmd_list) lgr.info("Updating annex ...") std_out, std_err = repo._git_custom_command(
#todo: add type hints </s> aperture=aperture,	detect_markers markers = square_marker_detect.detect_markers_robust( gray_img=gray_img, prev_markers=self.previous_raw_markers, true_detect_every_frame=3,
# todo(leofang): how about ptds? </s> def test_add_scalar(self):	test_add_scalar
# todo: get rid of the wrapping dict, see #24568 </s> template for writing list functions	_item_list Return a list of available items (glance items-list) CLI Example:
# todo: to be implemented </s> + aws_region	create_ami + args.base_ami_os + " --partition region" + " --custom" )
assert study_id == 0  # todo </s> def set_trial_system_attr(self, study_id, trial_id, attr_name, attr_value):	InMemoryStorage def set_trial_intermediate_value(self, study_id, trial_id, step, intermediate_value): assert study_id == 0  # TODO assert study_id == 0  # TODO self.trials[trial_id].system_attrs[attr_name] = attr_value
1  # todo: fill in identifier </s> elif cumulative_depth <= line.recursion:	print_stats cumulative_depth = float('inf') if highest_time * 0.85 <= line.average: formated_line = colored(formated_line, 'blue') formated_stack.append(formated_line)
assert self.restart_seq is none #todo: better handling of this situation </s> l.warning("unknown program died (pid={0}, status={1})".format(pid, status))	on_terminate_program if pid != p.pid: continue return p.on_terminate(status)
# todo: warn/error: check if this var has units: assigning </s> for vardata in self.values():	setlb def setlb(self, val): vardata.setlb(val)
# todo: decide wtf to do here </s> except httperror:	get_user return user except User.DoesNotExist: return None
# todo account for all packages - this ignores the ones with different </s> from admin.packaging import package_filename, packagetypes	FakeYum def _perform_download_packages_from_repository(self, dispatcher, intent): See :class:`DownloadPackagesFromRepository`. rpm_version = make_rpm_version(intent.version) versioned_packages = [
# todo: remove when we stop supporting python < 3.5 </s> check_is_fitted(self)	_QuadrupletsClassifierMixin if sys.version_info.major < 3 or sys.version_info.minor < 5: check_is_fitted(self, 'preprocessor_') quadruplets = check_input(quadruplets, type_of_inputs='tuples', preprocessor=self.preprocessor_,
# todo: make keys get passed through files or environment </s> generate keys for files based on host key and filename.	genkey Nice utility, Bob! h = sha512()
# todo: remove hardcoded http </s> with get_connection() as connection:	send_campaign def send_campaign(campaign): site = get_current_site(request=None)  # get site based on SITE_ID for subscriber in campaign.mailing_list.get_active_subscribers(): sent = send_campaign_email_subscriber(campaign.email, subscriber, site, connection)
# todo: check if we can get values for "importados/indefinidos" </s> "http://www.saude.pr.gov.br/modules/conteudo/conteudo.php?conteudo=3507"	Covid19PRSpider class Covid19PRSpider(BaseCovid19Spider): name = "PR" ] def parse(self, response):
# todo: constants file for "broadcast" </s> return 'tcp://{ip}:{port}'.format(ip=self.opts['master_ip'],	master_pub @property def master_pub(self): port=self.publish_port)
# todo: also preserve __module__, __name__ and a few other important attrs </s> self.stats = pstats.stats(profile())	FuncProfile if isinstance(self.sort, str): self.sort = (self.sort, ) self.ncalls = 0 self.skip = skip
#todo: implement extra options </s> = openappend[mathicsnonexamplefile]	OpenAppend >> OpenAppend[] = OutputStream[...] mode = 'a' stream_type = 'OutputStream'
# todo: an ir for ${} might simplify these lengthy conditions </s> strs.append(s)	_PartValsToString else: tmp = [s for s in part_val.strs if s is not None] return ''.join(strs)
# todo data should be bytes only </s> except ioerror as ex:	from_path _kwargs = {key: value for key, value in obj.items() if key not in ('data', 'from_base64')} kwargs.update(_kwargs) if ex.errno == errno.ENOENT: return None
# todo: python 2.4, 2.5 don't have io.bufferedreader!!! </s> return false	Axt except: return False else: return True
# xxx todo </s> def input_integer(self, token):	input_integer
#todo: io plugins should assign default image formats </s> filename = str(qtgui.qfiledialog.getopenfilename())	open_file_dialog if len(filename) == 0: return None
#todo: this is just for backwards compatibility. it should be removed in v0.98 with p2.6 </s> def filter(self, record):	ExtraIndicoFilter if record.name.split('.')[0] == 'indico': return 0
# todo: what if we fail?  error-handling should be recorded someplace, </s> class myworkitem(workitem):	sketch def sketch(): @inlineCallbacks def doWork(self):
# todo: verify learning rule contents </s> self.asserttrue(tfm_flows)	test_switch_features self.assertTrue(isinstance(self.valve, valve.TfmValve)) features_flows = self.valve.switch_features(None)
# todo: need to make sure the `rootvars dict` </s> pass	SubstitutionError
# todo: make test method </s> droid.stoplocating()	test_gps try: return event_loop()
# '-rs',  # @todo: manually remove dependencies of conflicting packages, </s> if answer:	cli_upgrade_packages while True: if answer is None: letter = answer.lower()[0] if letter == 'y':
# todo: waiting for a fix: https://developer.blender.org/t53509 </s> if ob:	LuxCoreMaterialHeader ob = context.object slot = context.material_slot row = layout.row() row.template_list("MATERIAL_UL_matslots", "", ob, "material_slots", ob, "active_material_index", rows=2)
# todo remove arch dependent code </s> address = target_oprnd.immediate	_extract_branch_target@571 isinstance(target_oprnd, ArmImmediateOperand):
if self._ndim == 3: # todo: use hasz </s> d = c_double()	getX def getX(self): lgeos.GEOSCoordSeq_getX(cs, 0, byref(d)) return d.value
# todo remove arch dependent code </s> bb_lower_half = self._disassemble_bb(address, bb.end_address, symbols)	_split_bb def _split_bb(self, bb, address, symbols): bb_upper_half.direct_branch = address return bb_lower_half, bb_upper_half
# todo: add keep parameter </s> def agg(self, func_or_funcs, *args, **kwargs):	agg
# todo use properties here to infer mechanism and purview from </s> return self._expand_repertoire('past', mechanism, purview, repertoire,	expand_cause_repertoire over the entire subsystem's state space."""
# steps = 0 # todo </s> self.matrices = matrix	Duplis class Duplis: def __init__(self, exported_obj, matrix): self.count = 1 def add(self, matrix):
accept_federated_only=self.federated_only)  # todo: 466 </s> def canonical_public_address(self, address_bytes):	canonical_public_address self._checksum_address = to_checksum_address(address_bytes)
# todo: implement me </s> self.chatrooms.roomsctrl.userjoinedroom(msg)	UserJoinedRoom if self.chatrooms is not None:
# todo: other "expected" error types to catch? </s> def wrapper(self, *args, **kwargs):	allow_asynchronous raise pywikibot.OtherPageSaveError(self, err) if callback: if kwargs.get('asynchronous'): pywikibot.async_request(handle, func, self, *args, **kwargs)
# todo: rewise once row_limit is working </s> assert hasattr(response, 'request')	test_query_bibcode_async assert hasattr(response, 'connection') assert hasattr(response, 'close') assert not isinstance(response, MockResponse) assert not issubclass(response.__class__, MockResponse)
# todo - this should be replaced by official repo when </s> raise notimplementederror()	task_open_control_firewall elif distribution == 'ubuntu-14.04': return sequence([])
# todo(py3.7): add required=true </s> def test_build(self):	test_build @synthesis_test
# todo: handle boolean overrides </s> @click.option('--version', help="echo the cli version", is_flag=true, callback=echo_version, expose_value=false, is_eager=true)	nucypher_cli @click.option('-v', '--verbose', help="Specify verbosity level", count=True) @nucypher_click_config
# todo: update once lakshmi's pr is merged </s> sanitized['id'] = str(sanitized['id'])	sanitize_trigger def sanitize_trigger(trigger): sanitized = trigger._data return sanitized
# todo: move this scopes conversion from and to string into a utils function </s> def get(self, request, *args, **kwargs):	PreAuthorizationMixin uri, http_method, body, headers = self._extract_params(request) try:
# todo: need to fix this test </s> output = run_node(node_def, [x])	test_floor def test_floor(self): node_def = helper.make_node("Floor", ["X"], ["Y"]) np.testing.assert_almost_equal(output["Y"], np.floor(x))
#todo fix cache </s> def on_client_error(self, error_type, error):	on_client_error
# todo: configurable timeout??? </s> differently.	_handle_decoded_payload Override this method if you wish to handle the decoded data
# xxx todo </s> elif errortype == sle_minorerror:	rip msg = "RIP error at thread %d, code %x" elif errorType == SLE_ERROR: msg = "RIP minor error at thread %d, code %x" elif errorType == SLE_WARNING:
# todo: handle errors in future </s> def schedule_load_statuses(self, *args):	schedule_load_statuses
# todo test </s> def parser_keys(self):	parser_keys
# todo - verify contents </s> response = self.client.get('/r/1')	testReviewDetail0 def testReviewDetail0(self):
# todo(luotao): use clone() method to flush the program.desc in force, </s> current_op.rename_input(input_arg,	_adjust_input if input_arg in self.input_map:
# todo not implemented yet </s> rows.append(0.0)	set_current_group_height_to_n start = 0.0 end = 1.0 for i in range(row_len - 2): if i < current_group_num:
# todo: add ratio and percentage </s> {% endfor %}	make_tabs <div class="tab-content" id="myTabContent"> {% for name, content in zip(names, contents) %} </div> return HTML(html)
# todo support intloguniformdistribution </s> "if this independent sampling is intended behavior.".format(	_log_independent_sampling "to `False` in the constructor of `CmaEsSampler`, "
# todo: may test with codecs.open passing an encoding </s> encoding = 'utf-8'	PluginCsvTestCase class PluginCsvTestCase(utils.RowsTestMixIn, unittest.TestCase): def test_imports(self): self.assertIs(rows.import_from_csv, rows.plugins.csv.import_from_csv)
# todo use the faster method </s> super(command, self).handle_noargs(**options)	handle_noargs def handle_noargs(self, **options):
# todo add options to modify the columns </s> self.close()	do_quit print('Thank you for using Poseidon')
# todo: fix </s> self.remote_op.command_blocking('/bin/su mobile -c /usr/bin/uicache', internal=true)	list_iOS_89 def list_iOS_89(applist): pl = self.remote_op.parse_plist(applist) self._applist = pl["User"]
pass  # todo </s> assert self.is_forwarding_finished, "forwarding not finished?"	train_finish_epoch assert len(self.forward_data_queue) == 0, "Not all forwardings were used?"
# todo: other types than can have series inside: list, set, etc. </s> if np.isnan(s):	_column_fillna_impl def _column_fillna_impl(A, B, fill):  # pragma: no cover for i in numba.parfor.internal_prange(len(A)): s = fill A[i] = s
# todo(rbharath): how does distance need to be modified here to </s> for z_ind in range(len(z_bins)):	put_atoms_in_cells atom_to_cell = {} for x_ind in range(len(x_bins)): cell_to_atoms[(x_ind, y_ind, z_ind)] = [] for atom in range(N):
# todo this closure is ugly. it also doesn't work with </s> else:	Completion if next(context) == 'from': if unfinished_dotted: return set([keywords.keyword(self._evaluator, 'import').name]) if isinstance(user_stmt, tree.Import):
# todo: remove this </s> def max(self) -> numpy.ndarray:	max @property
# todo: leaf label is broken </s> displacement = sibling.n	disp sibling = parent.r else: return displacement
return runtime.strarray(strs)  # todo: reuse this object too? </s> self.stack.append(entry)	Push
pass # todo </s> def collect_incoming_data(self, data):	collect_incoming_data
# todo: handle poster </s> should run on the main thread to ensure we avoid vlc plugins' reentrency problems.	do_set_time Args: t (`float`): the timestamp, in s
# todo: what decorator should we put here? in scipy.fftpack there is no planning, </s> return scp.fftpack.ifft(x, n=self.n, axis=self.axis,	test_ifft_overwrite contiguous_check=False, scipy_name='scp') def test_ifft_overwrite(self, xp, scp, dtype): overwrite_x=True)
# todo: find another method to test this behavior without patching. </s> self.copy_object_response(),	test_request_payer self.parsed_responses = [ self.list_objects_response(['mykey']), ] self.run_cmd(cmdline, expected_rc=0)
# todo: assert metrics. </s> }""",	test_get_touched_functions } void func2() { unit=False, )
# todo sk: select standby db if necessary </s> :return: dict of ``doc_id -> django db alias``	get_database_for_docs @staticmethod def get_database_for_docs(doc_ids): return ShardAccessor._get_doc_database_map(doc_ids, by_doc=True)
raise notimplementederror # todo </s> return d.__hash__()	phash 'hash based on object address in memory, not data values' assert isinstance(d,np.ndarray) or isinstance(d,tuple) else: return hash(tuple(map(phash,d)))
# todo: add type and value checkings </s> szeliski, richard, and daniel scharstein. "symmetric sub-pixel stereo matching." european conference on computer vision. springer berlin heidelberg, 2002.	DepthWarper return torch.stack([x, y], 1) def compute_subpixel_step(self): delta_d = 0.01 xy_m1 = self._compute_projection(self.width / 2, self.height / 2,
# todo _why_? the user already logged in </s> b.fill_in("login", "user@example.org", wait=true)	test_signup self.assertIsNotNone(url) b.visit(url) b.fill_in("password", "?P455W0rd!") b.click_button("Sign In")
# todo replace with collections.sequence subclass </s> def did_you_mean(self):	did_you_mean suggestion exists. Will always return :class:`None` if the search isn't loaded.
#todo implement transformation for corner nodes </s> sigma_xx, sigma_yy, sigma_xy stresses.	thetadeg_to_principal Parameters ---------- Returns -------
fp = open("%s.py" % name, "w")   #todo confirm file overwrite </s> p = pipe2py.compile.parse_and_build_pipe(pipe_def, verbose=true)	test_feed def test_feed(self): TODO: have these tests iterate over a number of test pipelines count = 0 for i in p:
# todo: check if scheme can be 'http' for http/2 ? </s> self._response_headers[name] = value	receive_headers def receive_headers(self, headers):
pass # todo </s> def collect_incoming_data(self, data):	collect_incoming_data
# todo could keep_trailing_newline fix this better? </s> backslash = false	parse_value else:
# todo: timeout should be removed when the engine is fixed so that it never hangs. -saul </s> def _nh_systemipaddressdidchange(self, notification):	_NH_SystemIPAddressDidChange
pass # todo </s> self._parser.errorhandler = jyerrorhandlerwrapper(err_handler)	setErrorHandler def setErrorHandler(self, err_handler):
# todo: allow to specify arguments </s> keyword argument:	app_makedefault def app_makedefault(auth, app, domain=None): app domain
+ ansi.ansi_normal  # todo: why does it keep it? </s> parser = text2html.html_parser	test_convert_linebreaks self.assertEqual("foo", parser.convert_linebreaks("foo")) self.assertEqual(
# todo(kurts): extract all this pinject-decorated fn stuff to a </s> return fn(*pargs, **kwargs)	pinject_decorated_fn
# todo: migrate to where-in subqueries? </s> def add_output_dataset_collection(self, name, dataset_collection_instance):	add_output_dataset_collection
rm.fetch(refspec=refspec, progress=progress)  # todo: progress +kwargs </s> def git_get_active_branch(self):	git_get_active_branch
# todo: do this resampling in the pipeline? </s> errors[meter.instance()] = np.abs(predicted_energy - ground_truth_energy)	error_in_assigned_energy sections = meter.good_sections() ground_truth_energy = ground_truth_meter.total_energy(periods=sections) return pd.Series(errors)
# todo cache? </s> latest_dt = last.dt	check_backed_up last = list(model.iter_entries())[-1]
# todo: fix this </s> models (list): the models to evaluate	eval_phone@41 length_penalty=0, coverage_penalty=0, progressbar=False): dataset: An instance of a `Dataset' class eval_batch_size (int): the batch size when evaluating the model
# todo: make sure this has test coverage </s> })	subscribe_user _('User %(rs)s subscribed to %(my_newsletter)s.'), { "rs": request.user, if already_subscribed: messages.info(
# todo: kkrampa, shouldn't we wait to save the checkpoint until after we've processed all the data? </s> def ils_clear_stock_data_task():	ils_clear_stock_data_task StockTransaction.objects.filter(report__domain='ilsgateway-test-1').delete() StockReport.objects.filter(domain='ilsgateway-test-1').delete()
except exception:  # todo: what could happen here? </s> try:	fix_extension_on_pickles@19 'autoIgnoredPosts.txt', 'users.txt', 'notifications.txt', 'whyData.txt', 'whyDataAllspam.txt', 'latestMessages.txt', 'apiCalls.txt', 'bodyfetcherQueue.txt', 'bodyfetcherMaxIds.txt'] if os.path.isfile(txt): os.rename(txt, (txt[:-4] + '.p'))
# :todo: implement test. </s> super(sendtransfercommandtestcase, self).setup()	SendTransferCommandTestCase class SendTransferCommandTestCase(TestCase): self.adapter = MockAdapter() def test_wireup(self):
# todo uncomment when gr-10346 will be fixed </s> return 1 + count_set_bits(n & n - 1) if n else 0	count_set_bits
# todo remove when below works </s> :return: str -- the current indentation (e.g. "  ").	ind hierarchy.
# todo: kerberos login </s> if queried_displayname:	get_netgpo def get_netgpo(self, queried_gponame='*', queried_displayname=str(), queried_domain=str(), ads_path=str()): gpo_search_filter += '(displayname={})'.format(queried_displayname) else:
# todo: should we enable auto-retry, </s> stores information about a consumer-decorated method.	ConsumerConfig def __init__(self, queue): self.queue = queue
# todo: use domain and port </s> return {}	AddonWikiNodeSettings class AddonWikiNodeSettings(AddonNodeSettingsBase):
# todo: requires same key or none key </s> def num_node_features(self) -> int:	num_node_features for value in self.values('x'): return 1 if value.dim() == 1 else value.size(-1)
# todo: this is horrible </s> old_data = utils.get_persistent_data(old_path)	migrate_symlinks old_path = os.path.join(G.COLAB_DIR, 'persistent.json') if not os.path.exists(old_path): data['workspaces'] = get_legacy_projects() data['recent_workspaces'] = old_data.get('recent_workspaces')
# todo: remember to delete </s> return response_content	open_alert def open_alert(alert_id: int): url_suffix: str = f'/alerts/{alert_id}/open/'
raise exception('not valid monitor')  # todo: custom exception </s> raise exception('not valid monitor')  # todo: custom exception	MonitorSuite return self.add_monitor(monitor=monitor, name=name) def _add_monitor_from_tuple(self, monitor_tuple): monitor, name = monitor_tuple return self.add_monitor(monitor=monitor, name=name)
# todo(ytknzw): add more specific assertion with the test case. </s> raise valueerror	fail_objective
# todo is the processor the correct place to set this? </s> parameters	SevenPlaneFileProcessor num_planes=num_planes, consolidate=consolidate) keras.backend.set_image_dim_ordering('th') ---------- color: color of the next person to move
# todo: do something more than simply selecting the last match? </s> if isinstance(obj, schemabase):	_shallow_copy return obj.copy(deep=False) elif isinstance(obj, list):
# todo(b/142683826): beam type check error in </s> return merged_data	_calculate_t_distribution for index in range(len(merged_data)): merged_data[index] = _calculate_t_distribution( else: sampling_data_list = [
# todo: the contents of <question_text> element used to be broken in </s> stig_id = self.references.get("stigid", none)	_verify_stigid_format if not stig_id: return
# todo: loop or recurse here since a select may have multiple </s> if len(select_info.table_aliases) == 1:	_get_wildcard_info if '.' in seg.raw: table =seg.raw.rsplit('.', 1)[0] table = select_info.table_aliases[0].ref_str else:
# todo: define specific exception types (instead of general type) </s> except keyerror:	removeMachineAction del self._machine_actions[action.getKey()]
# todo - remove the -word_size argument as per blast+ 2.2.30 </s> requires: in_msa	NcbipsiblastCommandline "E-value inclusion threshold for pairwise alignments (float, default 0.002).", equate=False), Incompatible with: msa_master_idx, in_pssm, query, query_loc, phi_pattern _Option(["-phi_pattern", "phi_pattern"],
'inception_v4'  : [testmodels.cntkemit, testmodels.coremlemit, testmodels.kerasemit, testmodels.pytorchemit, testmodels.tensorflowemit], # todo testmodels.mxnetemit(small error), testmodels.caffeemit(crash for shape) </s> tester = testmodels(test_table)	test_caffe def test_caffe(): tester._test_function('caffe', tester.CaffeParse)
# todo: perform this part concurrently. </s> return signing_power.generate_self_signed_cert(self.stamp.fingerprint().decode())	generate_self_signed_certificate def generate_self_signed_certificate(self):
#todo trap and ignore attributeerror here? </s> attrs = conf['attrs']	pipe_itembuilder@39 attrs -- key, value pairs Yields (_OUTPUT): dkv = [] for attr in attrs:
# todo: support for multiple message versions </s> self.hash_keyword.update(	HashJob def update_url(self, url): self.hash_url.update(url.encode()) json.dumps(kwdict, sort_keys=True).encode() )
# todo: don't mess with the user's cursor. </s> params["newname"] = new_name	request_rename def request_rename(self, params, new_name): client.send_request(Request.rename(params), self.handle_response)
# todo eh. traverse all of filesystem?? or only specific dirs for now? </s> return true	by_me if actor.email in ('***REMOVED***', '***REMOVED***@gmail.com'): return True for thing in THINGS: if thing in aa:
# todo: try mlp rather than bilinear </s> def get_variables(self):	get_variables
# todo some of these types could be filled in better </s> self.assertisinstance(one_row_complex.c.smallint.type, types.integer)	TestSqlAlchemyPresto from sqlalchemy.databases.mysql import MSBigInteger as BigInteger self.assertIsInstance(one_row_complex.c.boolean.type, types.Boolean) self.assertIsInstance(one_row_complex.c.int.type, types.Integer) self.assertIsInstance(one_row_complex.c.bigint.type, BigInteger)
# todo remove .as_posix when requiring python 3.6 </s> if not any(c.type == "tetra" for c in mesh.cells):	write@89 fmt = "{} " + " ".join(3 * ["{:" + float_fmt + "}"]) + "\n" for k, pt in enumerate(mesh.points): raise WriteError("TegGen only supports tetrahedra") if any(c.type != "tetra" for c in mesh.cells):
# todo: need to replace the five "get" functions </s> try:	check_name def check_name(args, sign): return [sign, args[idx+1]] except IndexError:
# todo: remove when migration plan / state is passed (#24100). </s> pass	get_default_username try: auth_app.User._default_manager.get(username=default_username) else: return ''
# todo torches, redstone torches, crops, ladders, stairs, </s> texture_resolution = terrain_width / 16	_split_terrain def _split_terrain(terrain): textures = [] for y in xrange(16): for x in xrange(16):
# todo: allow numerical indices, and "+" for append </s> help="when using --reannounce-all, do not add a non-standard field to the info dict ensuring unique info hashes")	add_options self.add_bool_option("--no-ssl", help="force announce URL to 'http'") self.add_value_option("--comment", "TEXT", help="set a new comment (an empty value deletes it)")
# todo: universal? </s> self.set_setting(self.power_setting, [0x03, 0x00, 0x2b, 0x2b, 0x13])	EPD4in2 self.full_set_lut() def init_gray(self): self.set_setting(self.BOOSTER_SOFT_START, [0x17, 0x17, 0x17]) self.send_command(self.POWER_ON)
# todo xxx graalvm change </s> self.assertequal(calls, [])	test_sni_callback chatty=True, sni_name='notfunny')
# todo maybe not here but in an own function? </s> if 'price' in col_guess.lower():	MatchItem elif col_guess in file_manager.OPTIONAL_HEADERS: field_name = col_guess.lower() + '-' + str(row['index']) self.fields[field_name] = MoneyField( label=_(col_guess),
# todo: refactor </s> return strictversion("0.0")	_get_version with open(join_path(dirname(__file__), "VERSION")) as fp: return StrictVersion(fp.read().strip())
# todo: send alert </s> return false	chk_prefix_in_whitelist@129 if flag: blacklist_whitelist_notification.delay(4) # notice_type = 4 whitelist
# todo: are we following this in the spec? </s> return s	_make_get_literal Returns: a string of type unicode. s = self._get_string_value(context, name) return get_literal
# todo test </s> return hash((frozenset(self.nodes), self.current_state.tostring(),	__hash__ self.past_state.tostring(), self.network))
# todo username </s> 'ceph-mon',	create_mon args=[ 'initctl', 'cluster={cluster}'.format(cluster=cluster), 'id={hostname}'.format(hostname=hostname),
# todo: no-op component? </s> self.define_api_methods_actor(*sub_components)	define_api_methods if self.type == "single": self.define_api_methods_single(*sub_components) else: self.define_api_methods_learner(*sub_components)
# todo this is a bit jankey to be honest </s> plugin_setting = plugin.settingspatterns	PluginConfig if InvenTreeSetting.get_setting('ENABLE_PLUGINS_SETTING'): for slug, plugin in settings.INTEGRATION_PLUGIN_LIST.items(): settings.INTEGRATION_PLUGIN_SETTING[slug] = plugin_setting settings.INTEGRATION_PLUGIN_SETTINGS.update(plugin_setting)
raise exceptions.mpdnotimplemented  # todo </s> name: foo (samba 4.1.11-debian)	listneighbors@78 Example:: listneighbors OK .. versionadded:: MPD protocol 0.19
# '-rs',  # @todo: manually remove dependencies of conflicting packages, </s> elif letter == 'm':	cli_upgrade_packages answer = print_sysupgrade( repo_packages_updates, aur_updates, verbose=True raise NotImplementedError() else:
# todo: raise something more specific or catch earlier </s> "column_id": property_name,	_make_indicator def _make_indicator(property_name): return { "datatype": default_case_property_datatypes.get(property_name, "string"), 'property_name': property_name,
# todo: logging </s> eventpermission.objects.filter(event__slug=event, user__id=pk).update(is_orga=false)	EventTeamDelete def dispatch(self, request, event, pk):
#todo - handle start/end coordinates properly. short term hack for now: </s> start = display_start \	_extract_alignment_region - display_start \ + align_stripped.count("-") + 1 - int(annotation['al_start']) end   = display_start \
# todo: remove in 1.2 release </s> warnings.warn(	run_deprecated_edit_bird_hook def run_deprecated_edit_bird_hook(request, items): for fn in hooks.get_hooks('construct_wagtail_edit_bird'): "The 'construct_wagtail_edit_bird' hook has been renamed to 'construct_wagtail_userbar'." "Please update function '%s' in '%s'." % (fn.__name__, fn.__module__), RemovedInWagtail12Warning
#    # todo(soren): this is what the compute manager does when you </s> if not os.path.exists(tmp_file):	check_shared_storage_test_file :param context: security context :param filename: confirm existence of FLAGS.instances_path/thisfile raise exception.NotFound(_('%s not found') % tmp_file)
# todo lookup tracks in batch for better performance </s> try:	searchadd them to current playlist. Parameters have the same meaning as for ``find``, except that search is query = _query_from_mpd_search_format(mpd_query) except ValueError:
# todo: if needed allow other handling (like adding values) </s> return self._num_bands	num_bands @property
# todo: this self.size enforces a 2**64 limit to array size </s> >>> s = coo.from_numpy(np.random.rand(2, 3, 4))	maybe_densify Examples -------- >>> x = s.maybe_densify() >>> np.allclose(x, s.todense())
""" todo: documentation </s> you can change the default behavior by configuring your generating command	GeneratingCommand Generating commands receive no input and must be the first command on a pipeline. By default Splunk will run your command locally on a search head: for event streaming: :<source lang=python>@Configuration(streaming=True)</source>
# todo: this test requires manifold access, see: t88318502 </s> cfg.model.device = device	TestCaffe2Export from detectron2.export import Caffe2Model, add_export_config, Caffe2Tracer cfg = model_zoo.get_config(config_path) model = model_zoo.get(config_path, trained=True, device=device) inputs = [{"image": get_sample_coco_image()}]
# todo don't know how to get return value </s> def hook_isclipboardformatavailable(ql, address, params):	hook_IsClipboardFormatAvailable @winapi(cc=STDCALL, params={ "uFormat": UINT rtn = ql.clipboard.format_available(params['uFormat']) return rtn
# todo: verify/modify these lists </s> setattr(ccopy, attr, 99)	CompatibilityTestCase for attr in version_changing_attrs: fcopy = copy.deepcopy(filled) fcopy.child_elements = [ccopy] self._do_two_way_compatibility_check(filled, fcopy, False)
#todo - fix the trailing space! </s> cline.set_parameter("input", temp_large_fasta_file)	test_long SeqIO.write(records, handle, "fasta") handle.close() cline.set_parameter("stable") cline.set_parameter("maxiters", 1)
annot.annotation_metadata.validation_and_reliability = "todo"  # todo </s> json.dump(jam, f, indent=2)	convert_JAMS f = open(out_file, "w")
'''todo: add docs''' </s> def discrete_columns(self):	discrete_columns @property
# todo: require tests </s> mat_unpad = mat[:, :, idx_top:idx_bottom, idx_left:	ZeroPad2dDerivatives pad_left, pad_right, pad_top, pad_bottom = module.padding idx_left, idx_right = pad_left, out_y - pad_right idx_right, :].contiguous() _, in_channels, in_x, in_y = module.input0_shape
#todo implement qgis2.0 variant </s> theqgisvectorlayer.savedefaultstyle()	setVectorStyle myRenderer.setMode(QgsGraduatedSymbolRendererV2.EqualInterval) myRenderer.setClassAttribute(myTargetField)
# todo: use triple factory </s> self.assertisnotnone(trans_h)	test_trans_h def test_trans_h(self):
(timestamp - relativedelta(days=(90 if histogram_type == 'active_cases' else 30))).isoformat(),  # todo - add to configs </s> datefield='created_on'):	get_mobile_workers_data Returns mobile workers that have used SMS. Returned based on date mobile worker is created
# todo treat boundary conditions </s> :return:	_setup_shape def _setup_shape(self): What is shape used for and what it really means. self.n_components = nm.prod(self.shape) self.val_shape = self.shape
# todo: exclude tests which fail to import: e.g. on windows </s> def run(self, test):	TextTestRunnerPyMVPA class TextTestRunnerPyMVPA(unittest.TextTestRunner): result = super(TextTestRunnerPyMVPA, self).run(test) if not result.wasSuccessful():
# todo; to include configurablereportkafkapillow features </s> doc_provider = couchdocumentprovider(iteration_key, doc_type_tuples=[	CouchCaseReindexerFactory ] def build(self): CommCareCase, ("CommCareCase-Deleted", CommCareCase)
# todo: validate grant </s> query = "create user '%s'@'%s'" % (user, host,)	user_create return False db = connect() if password is not None: query = query + " IDENTIFIED BY '%s'" % password
# todo(nnorwitz): need to ignore friend method declarations. </s> print node.name	PrintFunctions@22 try: for node in builder.Generate(): except: pass
# :todo: implement test. </s> preparing a bundle that does not transfer any iotas.	PrepareTransfersCommandTestCase PrepareTransfersCommand, ) self.skipTest('Not implemented yet.') def test_pass_inputs_explicit(self):
# todo(adu): delete when it will no longer be used </s> return volumeindexfilter(app, conf)	filter_factory conf = global_conf.copy() conf.update(local_conf) return except_filter
# todo: should not we see here actual texts in log too? </s> <li>alex</li>	test_have_text_exception GivenPage(browser.driver).opened_with_body( <ul>Hello: </ul> with pytest.raises(TimeoutException) as error:
# todo: add hook to be called after each song </s> return await func(self, *args, **kwargs)	owner_only async def wrapper(self, *args, **kwargs): orig_msg = self._get_variable('message') else: return Response("only the owner can use this command", reply=True, delete_after=20)
# todo(shardy): may be able to remove when the bug above is fixed </s> return client	nova@257 client = nc.Client(**args) client.authenticate()
# todo: get fee </s> self.ev_loop.run_until_complete(asyncio.sleep(1))	test_filled_orders_recorded order_id = self._place_order(True, amount, OrderType.LIMIT, price, 1, None, fixture.WS_TRADE) self.event_logger.clear() price = self.connector.get_price(self.trading_pair, True) * Decimal("0.95")
print('warning: failed to resolve {}'.format(ep.name))  # todo: use proper logger </s> try:	load_drivers except: print('WARNING: failed to resolve {}'.format(ep.name))  # TODO: use proper logger driver = driver_init() except:
# todo: check that the birth date is not in the future </s> year = 1900 + int(number[0:2])	_get_birth_date month = int(number[2:4]) % 50 % 20 day = int(number[4:6])
# todo:  urlencode file names in the url while adding/decode upon retrieval </s> self.debug("not claiming url %s" % url)	req_CLAIMURL else:
pass # todo </s> self._parser.errorhandler = jyerrorhandlerwrapper(err_handler)	setErrorHandler xmlreader.XMLReader.setErrorHandler(self, err_handler)
# todo expression </s> f.write(u"%s " % (stmt.who, ))	print_Say def print_Say(f, stmt, indent_level): f.write(u"\"%s\"" % (escape_string(stmt.what), )) if stmt.with_ is not None:
# todo: differentiate between tags assigned to the instance and a m2m field for tags (ex: configcontext) </s> user = user.objects.create_user(username=username)	create_test_user def create_test_user(username='testuser', permissions=list()): for perm_name in permissions: app, codename = perm_name.split('.')
# todo: implement this in c. </s> def gi_yieldfrom(self):	gi_yieldfrom @property
# todo xxx no memory lock implemented </s> instr, extra_ir = mnemo_func[instr.name.lower()](ir, instr, *args)	get_mnemo_expr def get_mnemo_expr(ir, instr, *args): if not instr.name.lower() in mnemo_func: return instr, extra_ir
# todo: verify this is always none. e.g. run with runtime input input </s> destination_params.update(job.destination_params)	summarize_destination_params 'Handler': job.handler}
# todo: all filtered scanlines start with a byte indicating the filter </s> assert list(out) == [8, 10, 9, 108, 111, 113]  # paeth	test_unfilter_scanline_paeth scanprev = [2, 0, 0, 0, 9, 11] scanline = [6, 10, 9, 100, 101, 102]
# todo: localize </s> def lang(self):	lang return self.config_core.get('lang')
# todo: actually implement feature importance visualization for multiclass problems. </s> argmax = np.argmax(y_pred_probas[i])	Model y_test_filter = [] y_pred_filter = [] if y_pred_probas[i][argmax] < confidence_threshold: continue
#todo: am i supposed to be adding the namespace like this? </s> },	Command 'task_due': '/data/task_due', 'owner_id': '/data/owner_id', close_condition=FormActionCondition( answer=None,
# todo: this should be method on bound object </s> 'exception during toggle action: %s(%s)' % (self.name, active))	do_activate self.__get__(instance, instance.__class__)() except Exception as error:
# todo: remove when 36lts is discontinued </s> if recorded_cmdline is none:	retrieve_cmdline def retrieve_cmdline(resultsdir): Retrieves the job command line from the results directory. return None with open(recorded_cmdline, 'r') as cmdline_file:
pass  # todo </s> pass  # todo	SpotifyPlaylistsProvider def __init__(self, backend): self._backend = backend def delete(self, uri): pass  # TODO
## todo: fix the unicode issue mentioned in </s> if synonyms_list:	synonym except KeyError: return False return json.dumps(synonyms_list) else:
#todo - introduce an annotated alignment class? </s> start, end = map(int, start_end.split("-"))	_identifier_split if identifier.find("/")<>-1 : start_end = identifier.split("/",1)[1] name = identifier.split("/",1)[0] return (name, start, end)
# todo: generalize </s> s = fill	_column_fillna_impl if hpat.hiframes.api.isna(B, i):
# todo: raise error in 0.9 or 0.10. </s> --------	rename_geometry inplace : boolean, default False Modify the GeoDataFrame in place (do not create a new object) >>> from shapely.geometry import Point >>> d = {'col1': ['name1', 'name2'], 'geometry': [Point(1, 2), Point(2, 1)]}
step = 0.1  # todo </s> return topos_at(t).observe(sun).apparent().altaz()[0].degrees > -0.8333	sunrise sun = ephemeris['sun'] topos_at = (ephemeris['earth'] + topos).at return is_sun_above_the_horizon
#todo: make more general (if possible) </s> }	get_ndarray_config 'measurement_types': {}, 'domains': {}, db_dict['ndarray_types'][record_dict['ndarray_type_tag']] = ndarray_type_dict measurement_type_dict = ndarray_type_dict['measurement_types'].get(record_dict['measurement_type_tag'])
# todo should be resp.raise_from_status </s> raise pepperexception('unable to parse the server response.')	req logger.debug('Error converting response from JSON', exc_info=True)
# todo: take care of theano to keras port: </s> x -= offset	vgg16_preprocess else: shape = [1, 1, 1, 3] return X
""" todo(datapipe-1525): this fixture override the </s> def producer(self):	producer return mock.Mock(autospect=Producer)
# todo check the actual transformation matrix. </s> mss = (np.linalg.norm(estimate.to_array()	TestICP max_iter=100) self.assertTrue(converged is True) - self.source.to_array(), axis=1) ** 2).mean() self.assertLess(mss, 1)
# todo: create a validate for class name </s> def get_valid_name(self, name: str, excludes: optional[set[str]] = none,) -> str:	FieldNameResolver @classmethod def _validate_field_name(cls, field_name: str) -> bool: if not name: name = self.empty_field_name
# todo implement this method </s> return	set_system_setting :param setting_name: name of setting, could be volume, brightness :param setting_value: value of setting
# todo: remove compatability hook </s> yield exe	get_executables new_exe.__dict__.update(exe.__dict__) new_exe.script = script
# todo: create xxx_failure test </s> self.asserttrue(check('test_space', failure_tests),	test_result_space_failure tests = exclude_from_resultlist(r, 'failure')
raise pathaccesserror()  # todo: path </s> def __init__(self, *path_parts):	Path 3 >>> glom(target, Path('a', 'd.e')) self.path_parts = list(path_parts) def append(self, part):
# todo since </s> logger.debug(f"[parsed comamnd args] {args}")	split_command_args logger.debug(f"[Parsed comamnd name] {input_command}")
min_stake=0,  # todo: where to get this? </s> if not hw_wallet:	deployer_pre_launch_warnings emitter.echo(NO_HARDWARE_WALLET_WARNING, color='yellow') if etherscan:
# todo use read_package_list_from_file when #618 is merged </s> pass	AbstractPluginInstaller def install_other_packages(self): Install packages with package managers other than pip/dnf/apt. def install_files(self): Download and install files.
# todo: eliminate asap, for backwards compatibility only </s> params["headers"] = headers	build_headers self.add_authorization(headers)
# todo(vek): need to pass context in for access to auth_token </s> self.state = state	InstanceInfo def __init__(self, name, state): self.name = name
# todo: refactor </s> hpat.timsort.sort(l_key_arrs, 0, n_out, l_data)	local_sort l_key_arrs = to_string_list(key_arrs) l_data = to_string_list(data) if not ascending: hpat.timsort.reverseRange(l_key_arrs, 0, n_out, l_data)
self.assertequals(status, 200) # todo: 202 when asynchronous </s> self.assertequals(body['distributor_id'], self.distributor_id)	test_get_bind self.assertTrue(body is not None) self.assertEquals(body['consumer_id'], self.CONSUMER_ID) self.assertTrue('_href' in body)
# todo disabled since it breaks existing configurations </s> start:end	_get_collate_fun size = end - start label_coords[current_index : (current_index + size), 0] = batch_index ] current_index += size
# todo: use message request - not orm access! </s> if os.path.isfile(config_file):	populate_bait database_actor.stop()
# todo: use regex matching instead of this </s> inmemorycontractregistry.download_latest_publication = lambda: registry_filepath	temp_registry @pytest.fixture(scope='module', autouse=True) test_registry.commit(filepath=registry_filepath)
# todo fix bug for not identifying unused variables in nested exceptions see issue #4391 </s> __never_used = 42	test_unused_with_prepended_underscore dummy = 24 _a_ = 42 # [unused-variable]
# todo: this assumes that we're always signing proxy retargetting. for the moment is true. </s> @option_registry_infile	inspect @group_general_config @option_provider_uri(required=True) @option_deployer_address @option_poa
#@todo: move to utils in 0.4.10 </s> if self.pyload.debug:	log_debug return self._log("debug", self.__type__, self.__name__, args)
# todo log here </s> if not poll_url:	auth_okta@99 if not poll: return False return False time.sleep(0.2)
# todo use unused info </s> if address_int not in self.__aircraft:	__ensure_aircraft self.__aircraft[address_int] = Aircraft(address_int) return self.__aircraft[address_int]
# todo: replace with "yield from" when dropping python 2. </s> if not playlist:	Euronews def _get_streams(self): res = http.get(self.url) return for item in playlist:
# todo: add checks for broken paddings/encrypted values and malformed enc_data </s> self.asserttrue(1 in hsm.secrets, hsm.secrets)	test_07_encrypted_key_file hsm.setup_module({"password": "test1234!"}) self.assertTrue(hsm.is_ready) self.assertTrue(2 in hsm.secrets, hsm.secrets) self.assertRaises(Exception, hsm._get_secret, 4)
# todo default_init_configs </s> def forward(self, x, *args):	BatchReshape def __init__(self, shape): super(Reshape, self).__init__() return minpy.numpy.reshape(X, X.shape[:1] + self._shape)
'''todo: add docs''' </s> def discrete_columns(self):	discrete_columns return [x for x in self.columns if x['type'] == 'DiscreteColumn']
# todo: use logging module </s> raise exception("no unit-test modules found--\n  in %s" % package_dir)	_discover_test_modules return file_name.startswith(UNITTEST_FILE_PREFIX) names = get_module_names(package_dir=package_dir, should_include=is_unittest_module) return names
# todo: only works for diagonal tensors. getedgeinnerproductderiv, </s> if getattr(self, '_me', none) is none:	Me @property def Me(self): self._Me = self.mesh.getEdgeInnerProduct() return self._Me
# todo better way of making sure tznames are unique </s> required = ('action', 'trigger',)	Alarm class Alarm(Component): singletons = ( 'ATTACH', 'ACTION', 'DESCRIPTION', 'SUMMARY', 'TRIGGER',
# todo: after reasonable amount of time replace with string option </s> self.args = args	unset_arg assert isinstance(name, str) args = self.args
# todo: do not include measures!!! </s> attrib = attribs[name]	localize_attributes keys as attribute names, values are dictionaries with localizable attribute metadata, such as ``label`` or ``description``.""" localize_common(attrib, atrans)
#todo trap and ignore attributeerror here? </s> reduce(lambda i,k:i.setdefault(k, {}), [d] + key.split('.')[:-1])[key.split('.')[-1]] = value	pipe_rssitembuilder@32 if key == 'title': d['y:%s' % key] = value except AttributeError: continue  #ignore if the source doesn't have our (dereferenced) target field (todo: issue a warning if debugging?)
# todo: fix figleaf traceback with doctests </s> to_address="wwwsearch-general@lists.sourceforge.net",	send_email body = body.lstrip() assert len(body) > 0, body subject=subject, body=body)
# todo this should be more modular </s> yield dict(zip(keys, element))	_dict_product def _dict_product(self, d): keys = d.keys()
d="'d.${def1}.${def2}'"             #todo </s> cmd = "docker rmi {images}:{testname}"	test_6133_run_default_services_from_single_service_saved_container sx____(cmd.format(**locals())) cmd = "docker rm --force {testname}x" sx____(cmd.format(**locals())) self.rm_testdir()
# todo remove this custom equality testing code when </s> (_nucleotide_sequence_to_fasta,	FASTAWriterTests BiologicalSequence('ACGT', id=id_, description=desc), ('fasta_single_bio_seq_defaults', NucleotideSequence('ACGTU', id=id_, description=desc), ('fasta_single_nuc_seq_defaults',
raise notimplementederror #todo </s> self.tokenexprs.append(tokenexpr)	Query i += 1 else: def __len__(self): return len(self.tokenexprs)
# todo: normal gl requires these lines, es 2.0 does not </s> gl.glblendfunc(gl.gl_src_alpha, gl.gl_one)	on_initialize@113 def on_initialize(self, event): gl.glClearColor(0,0,0,1); from OpenGL import GL gl.glEnable(GL.GL_VERTEX_PROGRAM_POINT_SIZE)
# todo: add logging. </s> except keyerror:	ExUnmap mappings.remove(modes.NORMAL, cmd) mappings.remove(modes.OPERATOR_PENDING, cmd) sublime.status_message('Vintageous: Mapping not found.')
# todo it seems that yahoo! converts relative links to absolute </s> to_location = content.find(to_delimiter)	pipe_fetchpage@39 from_location = content.find(from_delimiter) to_location = 0 if from_location > 0 and to_location > 0: content = content[from_location:to_location]
# todo: askr, undocumented! </s> if chartab[char]  &  0x80 >> j:	LedCtrlChar for j in range(8): sum = i + j + offsx self.LedCtrlRaw( sum, red, green, blue ) else:
assert oldsize == self.size, '%s: %d, %d' % (self.type, oldsize, self.size) # todo: remove </s> def read_uint(stream):	read_uint
# todo(billdodd): change to self.systems_uri after pr 62921 merged </s> response = self.get_request(uri)	get_volume_inventory volume_list.append(volume[u'@odata.id']) for v in volume_list: if response['ret'] is False: return response
# todo: print 's' </s> pass	DuplicateDeclarationError
# todo: make it handle more components </s> _on_web_admin = _on_lb	CheckConfig self.logger.info('Redis connection OK') def _on_lb(self, *ignored_args, **ignored_kwargs):
# todo: get rid of this feature one day (v8?; warning added in v7.3.0.) </s> self.wrap_encrypt(dest, post.meta('password'))	PostEncryption utils.LOGGER.warn("The post {0} is using the `password` attribute, which may stop working in the future.") utils.LOGGER.warn("Please consider switching to a more secure method of encryption.") def set_site(self, site): super(PostEncryption, self).set_site(site)
# todo: clean up this event print out. we probably want something </s> for fun in sorted(docs):	print_docs Print out the documentation! arg = self.opts.get('fun', None) display_output('{0}:'.format(fun), 'text', self.opts) print(docs[fun])
#     # todo: add an exception message </s> parts["microsecond"] = int(value[:6]) + rounding	_parse_token rounding = 1 else: elif token == "X": parts["timestamp"] = int(value)
# todo test for final </s> def canonical_dir(self, ignore_args = false):	canonical_dir
# todo: store only successful commands. </s> def run(self, command_line=''):	ExUnmap class ExUnmap(ViWindowCommandBase): Command: :unm[ap]  {lhs} assert command_line, 'expected non-empty command line' unmap = parse_ex_command(command_line)
# todo: agree on protocol here </s> (a for a, _ in node.arguments)):	_get_subtypes_for_type pass arcs = {} arc_labels = get_labels_by_storage_form(directory, arc) if arc_labels is not None:
# todo: skipped due to gh-4436 </s> assert_raises(typeerror, ds.create_sibling_ria)	test_invalid_calls @with_tempfile def test_invalid_calls(path): assert_raises(ValueError, ds.create_sibling_ria, 'ria+file:///some/where', name='some', storage_name='some')
# todo: each dp learns independently. an edge dp could </s> return none	edge_learn_port if edge_dp is None:
# todo : outsource to cython </s> corner_mask = np.zeros(image.shape, dtype=bool)	corner_fast@19 if image.ndim != 2: raise ValueError("Only 2-D gray-scale images supported.") test_pixels = np.asarray([[-3, 0], [-3, 1], [-2, 2], [-1, 3], [0, 3], [1, 3], [2, 2], [3, 1], [3, 0], [3, -1],
# todo: waiting for a fix: https://developer.blender.org/t53509 </s> if ob:	LuxCoreMaterialHeader row.operator("object.material_slot_select", text="Select") row.operator("object.material_slot_deselect", text="Deselect") split.template_ID(ob, "active_material", new="luxcore.material_new") if slot:
# todo: impala attempt to speed up final pass after lstm. </s> return (action_layer_output, last_internals) if last_internals is not none else action_layer_output	get_action_layer_output self.call(self.neural_network.apply, nn_input, internal_states) )
# todo send the key to the master for approval </s> return ''	bootstrap salt '*' qemu_nbd.bootstrap /srv/salt-images/host.qcow 4096 qcow2 location = __salt__['qemu_img.make_image'](location, size, fmt) nbd = connect(location) __salt__['partition.mklabel'](nbd, 'msdos')
pass # todo </s> error("invalid todo number given.")	pri self.todolist.todo(number).set_priority(priority) self.dirty = True except ValueError: error("Invalid todo number given.")
# todo - del just my poll, not the entire list ! </s> * http_referer	get_next_url Return URL for redirection on success Try to get it from: if 'next' in request.POST and request.POST['next'].startswith('/'): return request.POST['next']
# migration todo: remove ? </s> self.enriched_events_counter = counter(	EnrichWorker self.enrich_event = enrich_event self.name = "enrich worker" "enriched_events", "Enriched events",
if attribute == 'position':   # todo: clean up </s> else:	parse_value value = parse_string(chars) elif first_char.isnumeric() or first_char in '+-': value = None return value
# todo: must be implemented </s> def should_fetch_kindlegen():	should_fetch_kindlegen
# todo: this is untested. </s> pass	_VerifyHelper try: _raise_current_error() raise self._problems.pop(0)
pass  # todo </s> else:	_finditer yield match if this_match_is_empty: pos = match.end()
# todo: log </s> self._verify_client(client)	RunSaltAPIHandler client = self.get_arguments('client')[0]
# todo handling when g is a list of graphs </s> raise notimplementederror('todo')	plot_graph@9 if G.coords.shape[1] == 2: raise NotImplementedError('TODO') else: if G.coords.shape[1] == 2:
self.assertequals(status, 200) # todo: 202 when asynchronous </s> self.repo_id,	test_get_bind bind = manager.bind(self.CONSUMER_ID, self.REPO_ID, self.DISTRIBUTOR_ID) path = '/v2/consumers/%s/bindings/%s/%s/' % \ self.DISTRIBUTOR_ID) status, body = self.get(path)
# todo: 1. try all parameters </s> amount = models.decimalfield(default=0.0, max_digits=7, decimal_places=4)	SmsUsageFee Currency is always in USD since this is something we control. Once an SmsUsageFee is created, it cannot be modified. date_created = models.DateField(auto_now_add=True) @classmethod
except exception:  # todo: log error once </s> with active_threads_lock:	apply_function_actor ------- msg: dictionary with status, result/error, timings, etc.. active_threads[ident] = key thread_state.execution_state = execution_state
# todo: duplicate checking </s> pass	VPPLicenseCursor
# todo(ytknzw): add more specific assertion with the test case. </s> raise valueerror	fail_objective
# todo - unit test </s> def match(node):	_function_matcher try: return matcher_func(node)
origin_args = {}  # todo: gas management </s> linker_args = (self.contract_name, self.target_contract.address, secret_hash)	LibraryLinkerDeployer self._contract = self.blockchain.get_proxy(target_address=self.target_contract.address, proxy_name=self.contract_name) linker_contract, linker_deployment_txhash = self.blockchain.deploy_contract(gas_limit=gas_limit, *linker_args) self._contract = linker_contract
# todo: figure out a way to actually log this information without </s> if f['length'] > 3:	decode_hybi if f['opcode'] == 0x08: if f['length'] >= 2: f['close_reason'] = f['payload'][2:] return f
def __init__(self, p_args, p_todolist, #pragma: no branch </s> def help(self):	help
# todo(b/201683262) replace all calls to this method </s> return_stderr=true,	test_xml_fails self.assertIn('command can only be with the Cloud Storage JSON API', stderr) expected_status=1) self.assertIn('command can only be with the Cloud Storage JSON API',
# todo: there seems to be a bug in gitpython, which leads </s> def git_get_active_branch(self):	git_get_active_branch
# todo: assert </s> self.remote.modify_repo(repo, "mirror_locally", "0", self.token)	createRepo repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "name", "testrepo0", self.token) self.remote.save_repo(repo, self.token)
# todo: remove in v.0.6 </s> self.assertalmostequal(csep, 0.72981476)	TestCovariance cov = Covariance() cov.fit(self.iris_points)
# todo parameters </s> if stmt.with_ is not none:	print_Say if stmt.who is not None: f.write(u"%s " % (stmt.who, )) f.write(u" with TODO") f.write(u'\n')
# todo implement through browser </s> driver.delete_cookie(name)	delete_cookie if not cookie: raise Exception('Cookie "{}" was not found'.format(name))
# todo: autodetect size from passed-in file object? </s> issue_types = [issuetype(self._options, self._session, raw_type_json) for raw_type_json in r_json]	issue_types r_json = self._get_json('issuetype')
# todo: write </s> def string_keys_to_dict(key_strings, callback):	string_keys_to_dict@9
# todo: update the tolerance after the ci moves to torch 1.10 </s> ds = ds.filter(lambda x: x["id"] in ids).sort("id").map(map_to_array)	_load_datasamples batch["speech"] = speech return batch return ds["speech"][:num_samples]
# todo this context is probably not right. </s> def get_code(self, normalized=false):	AlreadyEvaluated class AlreadyEvaluated(frozenset): def __init__(self, *args, **kwargs): return str(self)
# todo should be: </s> 'pname': 'raid10pool',	test_raid10_crud def test_raid10_crud(self): raid10 post, put & delete api requests 'raid_level': 'raid10', } e_msg = ('A minimum of Four drives are required for the raid level: raid10')
# todo(yamahata): replace n_events with neutron_lib.callback.events </s> rpc_worker = service.rpcworker(_plugin)	TestRpcWorker _plugin = mock.Mock()
# todo: automate this (by lookup from nn). </s> action_space=env.action_space,	TestIMPALAAgentFunctionality architecture="large", environment_spec=environment_spec, internal_states_space=IMPALAAgent.standard_internal_states_space, execution_spec=dict(
# todo block until metadata_updated callback is called. </s> def __hash__(self):	__hash__
# todo we should make sure that trivial equality relations are </s> pass	_bilinear_expressions
#todo: move to filter </s> "read and parse notebook into notebooknode called self.nb"	from_filename with io.open(filename) as f: return self.convert(nbformat.read(f, 'json'))
# todo: test coverage </s> 'action': 'update',	update_activate_url return ('newsletter_update_activate', (), { 'newsletter_slug': self.newsletter.slug, 'activation_code': self.activation_code
# todo - move 'brac' to db (eventschedule.callback_args) </s> elif forms_submitted >= 26:	send_activity_to_reporter greeting = _t( _("Congratulations for the work done %(username)s. "), recipient.language) greeting = _t( _("Thank you %(username)s. "), recipient.language) instructions = _t( _("Please do your best to complete and send all the forms. "),
# todo support multiple backends </s> @playlists.setter  # noqa	StoredPlaylistsController for backend in self.backends] results = pykka.get_all(futures) def playlists(self, playlists): self.backends[0].stored_playlists.playlists = playlists
session.add(job)  # todo review this after remapping job (required to lazy-load `container` attr) </s> with dbcleanup(session, obj) as obj_id:	TestVisualizationRatingAssociation assert stored_obj.rating == rating def test_relationships(self, session, cls_, visualization, user): stored_obj = get_stored_obj(session, cls_, obj_id) assert stored_obj.visualization.id == visualization.id
# ---- todo: the following should be removed in milestone:0.11 </s> def file_order(a):	file_order
# todo result type? </s> msg = f"{dts}: {m.text}"	_dump_helper dts = m.dt.strftime('%Y-%m-%d %a %H:%M')
# todo - fix meta.submission to point to real submission </s> to_append = " and s.%s %s '%s' " % tuple( filter )	_get_cursor raise TypeError("_get_cursor expects column_filters of length 3 " + \ "e.g.['pk','=','3'] (only %s given)" % len(filter)) else: filter[2] = unicode(filter[2])
# todo: use model fit loss instead of a random loss </s> )	Tuner trials=trials
# todo: fails because of missing svg support </s> </table>''')	test_image_repeat_block <tr><td></td></tr> <tr><td></td></tr>
return  # todo [review] should an exception be raised, and if yes, what type of exception e.g. package, module, plugin, generic?  # noqa: e501 </s> return state.settings.view['vintageous_enable_surround']	is_enabled
# todo - unit test </s> try:	_function_matcher def _function_matcher(matcher_func): return matcher_func(node) except (LookupError, AttributeError, ValueError):
# todo: also improve 'crash-start' detection (to reduce lag when server fails to start) </s> s = cnsapp.connect()	main@25 launch_server_daemonized() for _ in range(100): # Check server availability for next 10 seconds if s is not None: break if s is None:
# todo - check and if we don't have category, take the only placement that exists in current site </s> return obj.box_class(obj, *args, **kwargs)	ListingBox " Delegate the boxing to the target's Box class. " obj = listing.placement.publishable
return # todo: flag bad gzip </s> self.setmessage('header-location', rs.redirect_without_location)	status303 def status303(self):        # See Other
# todo: manage next letter "t" </s> return width, height, viewbox	node_format viewbox = tuple(size(pos) for pos in viewbox.split()) width = width or viewbox[2]
# todo: documentation pending </s> "but the mode is currently set as %s. " % ('training by model.train()' if self.is_train else 'inference by model.eval()') +	_check_mode raise AttributeError("Training / inference mode mismatch. The argument `is_train` is set as %s, " % is_train +
# todo: improve performance by using conv instead of unfold </s> def _weight_jac_t_mat_prod(self, module, g_inp, g_out, mat, sum_batch=true):	Conv3DDerivatives raise NotImplementedError def _weight_jac_mat_prod(self, module, g_inp, g_out, mat): raise NotImplementedError
# todo: scale and translation could be merged into a single network </s> assert mask.get_shape() == shape[1:]	get_mask def get_mask(self, x, dtype): shape = x.get_shape() return mask
# beginning of block, preventing fusion. todo: fix pa </s> s = fill	_column_fillna_impl if hpat.hiframes_api.isna(B, i):
# todo / fixme : use functions in utils/network.py to manage this </s> m18n.n('monitor_stats_file_not_found'))	monitor_show_stats result = _retrieve_stats(period, date) if result is False: elif result is None: raise MoulinetteError(errno.EINVAL,
#todo replace when better connection mechanism is available </s> service_instance	vsan_supported def vsan_supported(service_instance): Returns whether vsan is supported on the vCenter: Service instance to the host or vCenter try:
#todo: introduce constants for readability </s> self.lock = threading.lock()	FakeGetItemRequestObject class FakeGetItemRequestObject(object): def __init__(self,gr): self.thread = threading.current_thread() self.requestID = self.thread.runningRequestID
# todo(thejulia): once we have power/state callbacks to nova, </s> task.node.uuid)	delete_configuration :param task: a TaskManager instance. :returns: states.CLEANWAIT if operation was successfully invoked step = task.node.clean_step return deploy_utils.agent_execute_clean_step(task, step)
# todo: remove when #980 has been merged </s> return (qidx, prefer_http, format['video_bitrate'])	_sortkey def _sortkey(format): qidx = ['low', 'med', 'high', 'veryhigh'].index(format['3sat_qualityname'])
# todo: remove anytime in 2016 </s> (including remote apps)	FormsByApplicationFilter @memoized def _nonmatching_app_forms(self): all_forms = set(self._all_forms) std_app_forms = set(self._application_forms)
# todo: parameterize and check for wrong argument </s> test_api = cobblerapi()	test_set_virt_bridge test_manager = CollectionManager(test_api) test_system = System(test_manager)
# todo: shouldn't it to be in the statefu module? </s> else:	is_control_channel return True
# todo: remove </s> def _form_to_db_schema(group_type=none):	_form_to_db_schema
# todo: catch unquoting errors, range of chars, charset </s> r'%s(?=%s|\s*$)' % (item, split), instr	split_string if not instr: return []
# todo: automate this (by lookup from nn). </s> update_times = list()	TestIMPALAAgentFunctionality ) print("Items in queue: {}".format(agent.call_api_method("get_queue_size"))) print("Updating from queue ...") for _ in range(updates):
# todo xxx graalvm change </s> client_context.check_hostname = false	test_sni_callback def test_sni_callback(self): calls = [] def servername_cb(ssl_sock, server_name, initial_context): calls.append((server_name, initial_context))
pass  # todo </s> assert_not_equal(sht.range('a1:d4').column_width, 40)	TestSheet assert_equal(sht.range('A1:D4').column_width, 40) sht.autofit() sht.autofit('r') sht.autofit('c')
# todo subject.cn from cert? </s> for pair in pairs:	TestMac key = props_match.group(1) val = {} pairmatch = re.match('(\w+)=(\S+)', pair) pairkey = pairmatch.group(1)
# todo handle exception </s> return ps	get_all_plugins ps.append(plugins.function_meta_data_plugin.FunctionRecursivePlugin()) ps.append(plugins.library_function_plugin.FunctionIsLibraryPlugin())
# todo: what about '_type'? </s> def _postprocess(self, obj, fossil, iface):	_postprocess
# todo: why do we have `has` below, should not it be `have`? </s> assert "has texts ('alex',)" in error.value.msg	test_have_texts_throws_exception </ul> with pytest.raises(TimeoutException) as error: assert "AssertionError: actual visible_texts: ['Alex', 'Yakov']" in error.value.msg
# todo(aron): move these client test cases to their own test class </s> self.assertfalse(device_log_csv_resp.get("objects"), "authorization error")	test_api_auth_not_logged_in exercise_log_csv = self.client.get(self.api_exercise_log_csv_url + "?group_id=" + self.group.id) self.assertFalse(exercise_log_csv.content, "Authorization error") store_log_csv_resp = self.client.get(self.api_store_log_csv_url + "?zone_id=" + self.zone.id) self.assertFalse(store_log_csv_resp.get("objects"), "Authorization error")
# todo: for now, circumnavigate the detached head issue. </s> raise skiptest("todo")	test_publish_default_target
# todo: write manifest and bagit metadata </s> f.write(packed.encode("utf8"))	packed_workflow f = open(path, "w")
# truffle todo: revert </s> def _ishidden(path):	_ishidden
# todo: bytes vs str </s> return f	route self.url_map.append((url, f, kwargs))
from gi.repository import glib  # todo: to fix </s> for original_indice in table.keys():	updateConversionAddressingTableWithTable self.conversionAddressingTable[original_indice] = table.get(original_indice)
# todo: test me </s> else:	add_trailing_slash_if_needed@9 return regexp_string[:-2] + '{trailing_slash}$'
todo = atomlist # list of atoms we must still mark and explore (recurse on all unmarked neighbors) </s> todo = newtodo	ops_connected_Mixin if id(at2) not in marked: marked[id(at2)] = at2 for atom in marked.itervalues(): atom.pick()
# todo: multi dp routing requires learning from directly attached switch first. </s> all_stacked_valves = {valve}.union(stacked_other_valves)	get_lacp_dpid_nomination if not other_valves: return None, '' ports = {} root_dpid = None
# todo: implement </s> :param value: value used for patching the member.	terrain_defense_upgrade :type converter_group: ...dataformat.converter_object.ConverterObjectGroup :param line: Unit/Building line that has the ability. :type value: MemberOperator :param operator: Operator used for patching the member.
# todo: remove when materialized paths are fixed in the payload returned from waterbutler </s> if root_id is not none:	_update_comments_timestamp def _update_comments_timestamp(auth, node, page=Comment.OVERVIEW, root_id=None): if node.is_contributor(auth.user): guid_obj = Guid.load(root_id) if guid_obj is not None:
principled.inputs[5].default_value = pypbr.metallic_factor #todo : currently set metallic & specular in same way </s> node_tree.nodes.remove(node)	BlenderPbr node_tree = material.node_tree for node in list(node_tree.nodes): output_node = node_tree.nodes[0] output_node.location = 1000,0
pass  # todo </s> assert self.is_forwarding_finished, "forwarding not finished?"	train_finish_epoch assert len(self.forward_data_queue) == 0, "Not all forwardings were used?"
# @todo: bulk lookups </s> _href = url(c="event", f=f,	org_name return A(org_represent(organisation_id),
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
# todo: 搜索和分页 </s> password = setting.default_password	perm_role_push for asset in calc_assets: if asset.use_default_auth: port = Setting.default_port else:
# todo set() is used for uniqueing results in case a checker name is </s> checker labels are string lists. in case of any format error a	__check_json_format def __check_json_format(self, data: dict) -> bool: Check the format of checker labels' JSON config file, i.e. this file ValueError exception is thrown with the description of the wrong format.
# todo: check botocore version </s> except waitererror as we:	copy_image Tags=[{'Key' : k, 'Value': v} for k,v in module.params.get('tags').items()] ) module.fail_json(msg='An error occured waiting for the image to become available. (%s)' %  we.reason) except ClientError as ce:
# todo: configurable timeout??? </s> differently.	_handle_decoded_payload Override this method if you wish to handle the decoded data
"""create todo database and tables""" </s> retrieves a task for the given task id.  if no task id is given, it	todo will return a list of task ids that that user has added to their todo list.
# todo: write this method if possible </s> res = self.proxy.getrawtransaction(txid)	getrawtransaction return res
# todo parse units as: units='oe' </s> self.refresh_time = refresh_time	Plotter def __init__(self, results, refresh_time=0.1): super(Plotter, self).__init__() def run(self): app = QtGui.QApplication(sys.argv)
# todo: add logger </s> destroy_session(session)	save_screenshot_data@18 scan = scans[0] scan.screenshot_path = screenshotter
#todo - do we still use the betweenposition class? </s> arguments:	BetweenPosition o position - The start position of the boundary. o extension - The range to the other position of a boundary.
# todo: this is all just debugging stuff and can be removed </s> axis -= ndim	right_rotate_covariance if axis < -ndim or axis >= ndim: raise ValueError("Axis out of range") axes_R = [Ellipsis, abs(axis)+1, abs(axis)] axes_Cov = [Ellipsis] + list(range(abs(axis),
# todo(mlavalle) a follow up patch in the address groups implementation </s> devices = self.plugin_rpc.security_group_rules_for_devices(	_apply_port_filter security_groups.update(devices_info['security_groups']) security_group_member_ips.update(devices_info['sg_member_ips']) self.context, list(device_ids)) trusted_devices = self._get_trusted_devices(device_ids, devices)
# todo: move 'hardcoded' coordinate specs (name, units, etc) into tile_spec </s> storage_units = create_storage_units(datasets, storage_type, executor=executor)	store_datasets _LOG.info('Using %s to store %s datasets', storage_type, datasets)
# todo: displacement. </s> export_settings['filtered_objects'] = filtered_objects	filter_apply@107 if current_parent not in filtered_objects and current_parent not in implicit_filtered_objects: implicit_filtered_objects.append(current_parent) filtered_meshes = {} filtered_vertex_groups = {}
oldsize = self.size # todo: remove </s> return struct.unpack('>i', stream.read(4))[0]	read_uint
# todo(t2r_contributors): switch to using gin config for all saver params. </s> from_dtype=tf.float32,	get_feature_specification self._t2r_model.get_feature_specification(mode),
# todo maybe add multichart class? </s> args:	load_dataset name (str, optional): Name of dataset to load. Either 'covid19' or 'urban_pop'. Defaults to 'covid19'. Returns:
# todo: test this </s> random_state : none, int or numpy.random.randomstate	sample_khatri_rao number of samples to be taken from the Khatri-Rao product skip_matrix : None or int, optional, default is None if int, used to set the seed of the random number generator if numpy.random.RandomState, used to generate random_samples
# todo cleanup/merge with above `call` once we have `subprocess.run` after dropping python 2 support? </s> if python_package_identifier not in installed_system_packages:	install_cpython def install_cpython(version, url): installed_system_packages = subprocess.check_output(['pkgutil', '--pkgs'], universal_newlines=True).splitlines() download(url, '/tmp/Python.pkg') call(['sudo', 'installer', '-pkg', '/tmp/Python.pkg', '-target', '/'])
# todo: temporary hack until they fix </s> )	test_extra_image_separated if report.when == 'call': from pytest_html import extras testdir.makepyfile("def test_pass(): pass") result, html = run(testdir)
oldsize = self.size # todo: remove </s> def read_uint(stream):	read_uint
# todo: different codec to be used </s> or not all([isinstance(v, str_types + (nonetype,)) for v in val]):	_safechk_val def _safechk_val(self, val): raise(ASN1ObjErr('{0}: invalid value, {1!r}'.format(self.fullname(), val)))
# todo: this function can replace repeated code in raw_format_table() in the future </s> pile.append(urwid.columns(resp, dividechars=1))	raw_format_list )
# todo make this a user explicit choice </s> if not self.in_store:	_get_store_channels def _get_store_channels(self): return dict() return snap_store_info["channels"]
# todo: openssl 1.1.0 has ssl_get0_verified_chain() to do this directly </s> self._certificate_dict = self._extract_certificate_dict()	get_certificate_with_subject if self._certificate_dict is None:
# todo implement support for this </s> if 'emissioncolor' in materialdata:	exportSDFMaterial specular = materialdata['specularColor'] tagger.attrib('specular', '{0} {1} {2} {3}'.format( emission = materialdata['emissionColor'] tagger.attrib('emissive', '{0} {1} {2} {3}'.format(
# todo: remove this monkeypatch once upstream class is fixed. </s> if next_serial <= self.soa.rdata.serial:	_increment_serial def _increment_serial(self): next_serial = self.soa.rdata.serial + 1 if hasattr(self.soa.rdata, 'replace'):
'one, two, three, four, five') #todo: group being ignored? </s> ('sth', 'an sth'),	test_A ('houri', 'a houri'), ('nth', 'an nth'), ('xth', 'an xth'), ('ant', 'an ant'),
## todo : log error </s> @param mylist: a list/tuple of values, or a list/tuple of value list/tuples.	stringify_listvars def stringify_listvars(mylist): Convert each item in the list, into a string (replace None with the empty @return: a tuple of string values or a tuple of string value tuples string_list = []
#todo: manage multiple foreign key </s> def save(self, item):	Alchemy else: item_dict[name] = item[name] connection = self._table.bind.connect() transaction = connection.begin()
# todo(vek): need to pass context in for access to auth_token </s> assert state in power_state.valid_states(), "bad state: %s" % state	InstanceInfo self.name = name
raise exceptions.mpdnotimplemented  # todo </s> def prio(context, priority, position):	prio@341 @protocol.commands.add( *musicpd.org, current playlist section:* ``prio {PRIORITY} {START:END...}``
# todo: remove logging </s> user: user,	handle_spam contact: Contact, alias: Alias, mailbox: Mailbox, email_log: EmailLog,
annot.annotation_metadata.annotation_rules = "todo"  # todo </s> fill_global_metadata(jam, json_files[0])	convert_JAMS def convert_JAMS(jams2_file, jams_file): for json_file in json_files: annot = jam.sections.create_annotation()
# todo get annexed obj file </s> for k in spec:	merge_purge if k in self: del self[k]
# todo: remove this if-block.  this is a hack </s> tmp = {}	_repn_ def _repn_(self, option): if not option.schema and not self._active and not self._required: for key in self._sections: rep = dict.__getitem__(self, key)._repn_(option)
# time.sleep(40)  # todo: should remove after polling get. </s> assert (mpc_1_2_3.reconstruct() == exp_res.child).all()	test_tensor_abstraction_pointer@32 exp_res = op(data_1, data_2) assert (mpc_1_2.reconstruct() == exp_res.child).all()
# todo: think about the most useful class api here </s> return self._add_transform(core.timeunittransform(*args, **kwargs))	transform_timeunit @use_signature(core.TimeUnitTransform)
self.kernel_quantizer_internal._set_trainable_parameter() # todo recurrent as well? </s> **cell_kwargs)	QSimpleRNN recurrent_dropout=recurrent_dropout, dtype=kwargs.get('dtype'), super(QSimpleRNN, self).__init__( cell,
#todo: does not keep case </s> self.assertraises(badnumvalueerror, p.num, 'text')	test_num ret = p.num(count=3, show=0) self.assertEqual(p.persistent_count, 3)
# todo: expand to full set of info </s> ins = workflow.insert().values(**{k: v for k, v in info.items() if k in workflow.c})	DatabaseHandler if len(self.eng.execute(self.meta.tables[run_id].select(self.meta.tables[run_id].c.task_id == info['task_id'])).fetchall()) == 0: with self.eng.begin() as con: con.execute(ins) print('Task ' + str(info['task_id']) + " was added to the workflow table")
# todo fix. </s> asm = ["cmova eax, ebx"]	test_cmova ctx_init = self.__init_context() x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init)
# todo: animation change </s> grazingtime = 2	Animal def __init__(self, home_building): self.__home_building = home_building
# todo: improve error message(add error code) </s> return (	BaseNestedFieldSerializerFactory return self.validate_pk_based_nested(data) return self.validate_data_based_nested(data) "BaseNestedField(%s, many=False)" % (serializer_class.__name__, )
# todo rename to "_to_reach_aspect_ratio"? matches other methods </s> img_0to1 = 1 - img_0to1  # depth map was saved as 0 being furthest away	quokka_heatmap img = imresize_single_image(img, shape_resized[0:2]) img_0to1 = img[..., 0]  # depth map was saved as 3-channel RGB return HeatmapsOnImage(img_0to1, shape=img_0to1.shape[0:2] + (3,))
# todo(rbharath): there should be some automatic check to ensure that all </s> "task7", "task8", "task9", "task10", "task11", "task12",	TestHyperparamOptAPI "nb_layers": 1, "batchnorm": False} input_file = os.path.join(self.current_dir, "multitask_example.csv") "task13", "task14", "task15", "task16"] task_types = {task: task_type for task in tasks}
except exception: # todo: what exception? </s> return 0	get_rating try: rating = float(self.get_tag_raw('__rating')) maximum = settings.get_option("rating/maximum", 5) rating = int(round(rating*float(maximum)/100.0))
# todo: find a better way to do this. </s> commcarecase(domain=self.domain, name=case_name).save()	test_case_reindexer def test_case_reindexer(self): case_name = 'reindexer-test-case-{}'.format(uuid.uuid4().hex) call_command('ptop_fast_reindex_cases', noinput=True, bulk=True) results = CaseES().run()
# todo ... </s> self.grid.addwidget(child.nativeguiobject, 0, 0)	addChild
# todo: may test with codecs.open passing an encoding </s> self.assert_expected_table(table)	PluginCsvTestCase def test_import_from_csv_fobj(self): with open(self.filename) as fobj: def test_export_to_csv_filename(self): temp = tempfile.NamedTemporaryFile(delete=False)
# todo(mattjj,phawkins): use 'yield from' when py2 is dropped </s> ans = yield py_args, {}	pytree_fun_to_flatjaxtuple_fun @transformation_with_aux def pytree_fun_to_flatjaxtuple_fun(in_trees, *args): yield pytree_to_flatjaxtuple(ans)
gc.collect()  # todo: see first comment above </s> assert_raises(valueerror, c, ['a', 'list'])	test_EnsureDataset def test_EnsureDataset(): c = EnsureDataset() assert_raises(ValueError, c, (1, 2, 3)) assert_raises(ValueError, c, {"what": "ever"})
# todo - implement searching google, bing, yahoo, baidu, and ask </s> headers = {'pragma':          'no-cache', 'dnt': '1',	search_crtsh@555 def search_crtsh(self): 'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'en-US,en;q=0.9,it;q=0.8',
# todo: why would this throw an exception?  we should handle </s> question_data['has_helpful'] = false	extract_question if not question.answers.count() == 0: question_data['id'] = question.id * ID_FACTOR question_data['answer_creator'] = u'' question_data['answer_votes'] = 0
# todo remove </s> else:	_process_alert_system_active_option self._internal_sensor.set_state(0)
# todo: check if integerctype, not just ctype.arith (floats, etc.) </s> return type1_promo	_promo_type type1_promo = ctypes.integer if type1.size < 4 else type1 type2_promo = ctypes.integer if type2.size < 4 else type2 elif type1_promo.signed == type2_promo.signed: return max([type1_promo, type2_promo], key=lambda t: t.size)
# todo: until we get it working. </s> facts["warnings"].append(	generate_jss_recipe facts["app_name"] + ".png") extract_app_icon(facts, extracted_icon) "I don't have enough information to create a PNG icon for this " "app.")
# todo: wells if display config has more than one column </s> bar = ('username', 'userid')	mycmp def mycmp(x, y): if x in foo and y in foo: return -1 if foo.index(x) == 0 else 1
# todo: merge with backup in helpers </s> output, err, exit_status = self._run_git(cur_git, test_cmd)	_find_working_git logger.log(u"Trying known alternative git locations", logger.DEBUG) for cur_git in alternative_git: if exit_status == 0: logger.log(u"Using: " + cur_git, logger.DEBUG)
# todo: add test for this </s> intermediates = [key for key in dsk	delete_intermediates deps = dependents[key] yield [completed[dep].wait() for dep in deps] if key not in out_keys] for key in intermediates:
# todo: remove constraint of batch_size == 1 </s> regression_diff - 0.5 / sigma_squared	smooth_l1 regression_loss = keras_retinanet.backend.where( keras.backend.less(regression_diff, 1.0 / sigma_squared), ) regression_loss = keras.backend.sum(regression_loss)
# todo enforce uniqueness on arrange panel? </s> response = filesystem_ajax_helpers.directory_to_dict(path)	contents path = request.GET.get('path', '/home')
#todo: it'd be nice to log and then ignore it if not data_is_complete. </s> s = ''	binstr while True: s = ('1' if n & 1 else '0') + s
# todo: use other libraries. </s> return 4 if get_bits() == 32 else 8	get_size_of_pointer
return none  # todo better error handling here </s> "response_type": "code",	authorize_redirect_url args = { "redirect_uri": redirect_uri, "scope": OAUTH_SCOPE, "access_type" : "offline",  # to get a refresh token
# todo(pachristopher): remove this once tfdv 0.14 is released. </s> stats = tfdv.load_statistics(stats_path)	validate_stats anomalies_path: Location where the detected anomalies are materialized. print('Validating schema against the computed statistics.') anomalies = tfdv.validate_statistics(stats, schema) print('Detected following anomalies:')
# todo is pexpect thread safe, e.g. could we be blocked on this </s> handle = int(hxstr[0], 16)	BluetoothLeDevice if pnum == 0: after = self.con.after except pexpect.TIMEOUT: pass
# todo: check that the performance measure is within some range </s> tests flow/benchmark/baselines/merge{0,1,2}.py	TestBaselines def test_merge(self):
pass # todo: raise exception </s> date = date[:-6]	strdate_to_time def strdate_to_time(date): c = date[-5:-4] c = date[-3:] if c in ["GMT","CST","EST","PST","EDT","PDT"]:
loader = imageloader(32) #todo crop=true? </s> self.assertequal(y.get_shape(), [])	test_load_fixture with self.test_session(): loader = ImageLoader(32) #TODO crop=true? self.assertEqual(int(x.get_shape()[1]), 2) self.assertEqual(int(x.get_shape()[2]), 2)
# todo: ... </s> @staticmethod	StoreService def purchase_album(user: Account, album: Album, amount_paid: float, stripe_token: str):
if not isinstance(self.name_str, (str, unicode)):  # todo remove </s> names_new.append(n)	_get_defined_names_for_position if n.start_pos[0] is not None and n.start_pos < position:
pass # todo: some magic with getstate to find out if it's the </s> pass	Win32Exception
# todo(ralexstokes) look at better way to handle once we have fork choice in place </s> def is_sync_peer_selected(self) -> bool:	is_sync_peer_selected return self.sync_peer is not None
else: # xxx todo: convert warning into a bqlerror. </s> if nu <= count_cutoff:	numerical_p def numerical_p(column, count_cutoff, ratio_cutoff): return False if float(nu) / float(len(column)) <= ratio_cutoff:
# todo: use regex matching instead of this </s> def temp_registry(testerchain, test_registry, agency):	temp_registry InMemoryContractRegistry.download_latest_publication = lambda: registry_filepath test_registry.commit(filepath=registry_filepath)
# xxx todo: investigate what the meaning of dominsert is about </s> self.subpanel.setwidgetposition(w, left, top)	setWidgetPosition
# todo: figure out how to re-use the same return function in base.py </s> log.error("certificate uid matched.  continue")	check_consumer_id else:
else: # todo: deprecated </s> verts = []	add_hook_mod trait = {} trait['type'] = 'Script' if group_name != '': group = bobject.vertex_groups[group_name].index
# todo: add function names </s> myfunction()	skip_hash_check Examples -------- try: self._skip_hash_check = True
if jaxpr.constvars: raise notimplementederror  # todo(mattjj) </s> in_tracers: sequence[jaxprtracer],	tracers_to_jaxpr out_tracers: Sequence[JaxprTracer] ) -> Tuple[Jaxpr, Tuple[Any, ...], Tuple[Any, ...]]:
# @todo: move to css </s> for r in requires:	init_requires else: if not isinstance(requires, (list, tuple)): if hasattr(r, "allowed_override"): r.allowed_override = allowed_override
# todo: serialize the policy </s> payment_details = request.args['payment']	make_alice_control bob_pubkey = bytes.fromhex(request.args['bob_encrypting_key']) label = bytes.fromhex(request.args['label']) federated_only = True # const for now bob = Bob.from_public_keys({DecryptingPower: bob_pubkey,
# todo(bcipolli): bulk saving of logs </s> return [int(ceil(random.random()/other_weight - (common_weight/other_weight))) for ni in range(n)]	generate_user_type n_types = len(user_types) common_weight = user_types[0]["weight"]
# todo: configurable timeout??? </s> self.syndic_cmd(data)	_handle_decoded_payload def _handle_decoded_payload(self, data): Override this method if you wish to handle the decoded data
# todo: this is lazy, we should only reconfigure the drone(s) who are actually </s> self.config_commands.send(messages.ok + ' ' + json.dumps({'public_key': publickey,	_handle_command_genkeys private_key, publickey = self._get_zmq_keys(name)
# todo: remove this - a leftover from an earlier version, needed for old </s> - all parts of the ui that need to be updated when specs have been	update_UI def update_UI(self): changed outside this class (e.g. coefficient wordlength). - coefficients
# todo: the following skipped suite and fixtures should be enabled </s> def _filter_headers(self):	VultrProviderTests Provider = Provider provider_name = 'vultr' return ['API-Key'] @pytest.fixture(autouse=True)
# todo: refactor </s> return '%s (step %d)' % (self.parent.name,	logical_name def logical_name(self):
# todo don't change get_code, the whole thing should be the same. </s> b = a + '\nimport os'	test_class_in_docstr jedi.Script(a, 1, 0)._parser
# todo: remove this when the checks in `before_run` have been moved to the template </s> 'once during a time step.' % (str(self.dt),	before_run if (np.logical_and(np.diff(timebins)==0, np.diff(self._neuron_index)==0)).any(): raise ValueError('Using a dt of %s, some neurons of ' self.name)) self._previous_dt = dt
# todo: test filter functionality more </s> self.assertapproximates(0, delta_3, 100, '%f fewer output samples than expected' % delta_3)	__run delta_1 = self.__run1(block, in_size, ratio) delta_2 = self.__run1(block, in_size * 2, ratio) msg = 'varying delta: %i %i %i' % (delta_1, delta_2, delta_3) self.assertApproximates(delta_1, delta_2, 200, msg)
# pattern for a markdown todo-list () </s> db.session.commit()	generate_fake confirmed=True) db.session.add(u) except IntegrityError: db.session.rollback()
# todo: remove getattr when https://github.com/rtfd/readthedocs.org/pull/3339 got merged </s> def venv_path(self):	venv_path
self.assertequal(out.strip(), "inactive") # todo real "failed" </s> out, end = output2(cmd.format(**locals()))	test_2027_show_unit_for_oneshot_service self.assertTrue(greps(out, r"^UnitFileState=")) num_lines = len(lines(out)) logg.info(" %s =>%s\n%s", cmd, end, out) self.assertEqual(end, 0)
# todo: sessions and not only dates/days should be considered </s> bardt = self.data.datetime.date(index)	_barisover_months dt = self.lines.datetime.date(index)
# todo: until we get it working. </s> information we know so far about the app associated with the	generate_jss_recipe def generate_jss_recipe(facts, prefs, recipe): Args: input path. prefs: The dictionary containing a key/value pair for each
# todo: unique constraint </s> level = len(self.path)	path_add_level def path_add_level(self, value='?'): self.path.append(value) return level
# todo: check arp reply is valid </s> self.faucet_event_sock = os.path.join(self.tmpdir, 'event.sock')	setup_valve def setup_valve(self, config): self.tmpdir = tempfile.mkdtemp() self.logfile = os.path.join(self.tmpdir, 'faucet.log') self.table = FakeOFTable(self.NUM_TABLES)
assert ne.test("frexp(z3)", xr.dataarray(xr.ufuncs.frexp(z3)))  # fixme todo fails with xarray >= 0.8 </s> assert ne.test("real(z1)", xr.ufuncs.real(z1))	test_2@59 assert ne.test("log2(z1)", xr.ufuncs.log2(z1)) assert ne.test("rad2deg(z1)", xr.ufuncs.rad2deg(z1)) assert ne.test("rint(z1)", xr.ufuncs.rint(z1)) assert ne.test("sign(z1)", xr.ufuncs.sign(z1))
# todo test that citext.sql gets loaded with 9.0.x </s> with klass('postgres', config.logger) as db:	TestSetupDB with klass('postgres', config.logger) as db: self.assertRaises(ProgrammingError, db.execute, db.execute('CREATE DATABASE blah', ['bad things']) config.logger.warning.assert_called_with('bad things happened')
# todo(rbharath): this is wrong!! </s> n_cells = int(n_cells)	get_cells_for_atoms cells_for_atoms: tf.Tensor Shape (N_atoms, 1) tiled_cells = tf.tile(cells, (N_atoms, 1)) tiled_coords = tf.reshape(
# todo: better to use an inotify method that doesn't conflict with eventlets. </s> self.send_flows_to_dp_by_id(dp_id, ofmsgs)	valve_flow_services for dp_id, valve in list(self.valves.items()): ofmsgs = getattr(valve, valve_service)()
# todo: send `goodbye` req then disconnect </s> def listen_maddr_with_peer_id(self) -> multiaddr:	listen_maddr_with_peer_id return self.listen_maddr.encapsulate(Multiaddr(f"/p2p/{self.peer_id}"))
# todo(developer): uncomment and set the following variables </s> from google.cloud import dataproc_v1 as dataproc	create_cluster@22 client = dataproc.ClusterControllerClient(client_options={
# todo this creates an identical dataframe for every hour. we only need one for all hours. </s> corresponding_row.append(i_row)	webparser for tag in rows[i_row].find_all("string"): if not tag.get_text().isdigit(): else: hours.append(int(tag.get_text()))
# todo: implement me </s> except exception, e:	get_current_time try: position, format = self.playbin.query_position(gst.FORMAT_TIME) logging.warn("get_current_time: caught exception: %s" % e) return None
for joint in annos:  # todo : speed up with affine transform </s> for i in range(thread_count):	threading_data divs = np.round(divs).astype(int) results = [None] * thread_count t = threading.Thread( name='threading_and_return', target=apply_fn, args=(results, i, data[divs[i]:divs[i + 1]], kwargs)
# todo: this loop is pretty slow .. (parellize) </s> return (self.mesigmaderiv(grad*u).t * (grad*v) +	getADeriv sigma = self.sigma vol = self.mesh.vol ky**2*self.MnSigmaDeriv(u).T*v) return (Grad.T*(self.MeSigmaDeriv(Grad*u)*v) +
# todo warn the user? </s> self.font.face_size,	FreeTypeGlyphRenderer face = self.font.face error = FT_Set_Char_Size(face, self.font.dpi, self.font.dpi)
# todo(somebody): make this a literal type. </s> expected_formatted_code = textwrap.dedent("""\	testHangingIndentCollision pass for connection in itertools.chain(branch.contact, branch.address, morestuff.andmore.andmore.andmore.andmore.andmore.andmore.andmore): if (aaaaaaaaaaaaaa + bbbbbbbbbbbbbbbb == ccccccccccccccccc and xxxxxxxxxxxxx or yyyyyyyyyyyyyyyyy):
# todo(twd2): do more visibility check eg. contest </s> self.response.content_type = 'text/markdown'	DiscussionTailReplyRawHandler drrid: objectid.ObjectId): ddoc = await discussion.get(self.domain_id, did) self.response.text = drrdoc['content']
#todo(sbaker) check for a non-default router for this network </s> return self.properties.get(key, '')	FnGetAtt if key == 'AvailabilityZone':
# todo debug </s> self.triggered = false	RuleElement class RuleElement: def __init__(self): self.timeWhenTriggered = 0.0 self.timeTriggeredFor = 0.0
# todo: check for ip subnet/range and break it out to individuals </s> scr = script_metadata()	list_scripts def list_scripts(): Lists nmap scripts return dict(scripts=scr)
# todo(ytknzw): add more specific assertion with the test case. </s> raise valueerror	fail_objective
# todo: pico-8 doesn't allow multiline strings, so this probably </s> name = 'comment'	TokComment
# @todo: follow global settings: </s> def alert():	alert
#todo link to some protocol for reporting this </s> source = 'gmusicapi-' + gmusicapi.__version__	ClientLogin if logintoken is not None or logincaptcha is not None: raise ValueError('ClientLogin captcha handling is not yet implemented.') return locals()  # dict of {'varname': val} @classmethod
# todo: rewrite in python. </s> ")",	folderify@141 TEMP_MASK_IMAGE, "(", "-compose", "dst-out", "-composite", "+repage", "-negate", "-geometry", ("+0+%d" % offset_white), ")",
# todo check the op returned a view </s> print >> file, '=================='	summary_function print >> file, 'Scan Op profiling (', self.name, ')' else: print >> file, '  Message: %s' % self.message print >> file, ('  Time in %i calls of the op (for a total of %i '
""" todo: documentation </s> def overrides_timeorder(self):	StreamingCommand return type(self)._local _local = False event records Default: False
continue ### todo </s> parsefragment(expected).childnodes])	runSanitizerTest def runSanitizerTest(name, expected, input): expected = json.loads(json.dumps(expected)) assert expected == sanitize_html(input)
""" todo: documentation </s> def reduce(self, records):	reduce
# :todo: implement test. </s> def test_fail_transaction_null(self):	ReplayBundleRequestFilterTestCase def test_fail_unexpected_parameters(self): Request contains unexpected parameters. ``transaction`` is null. self.skipTest('Not implemented yet.')
# todo: spaces should depend on the horizontal </s> self.adjust_space()	get_figure_and_axs figure._themeable = {'legend_title': [], 'legend_text': []} try: axs = axs.flatten()
# todo raise expected values once tests complete successfully </s> end_date = ['2013-10-13', none, '']	test_datetimes_to_durations_censor def test_datetimes_to_durations_censor(self): T, C = datetimes_to_durations(start_date, end_date, freq='Y') npt.assert_almost_equal(C, np.array([1, 0, 0], dtype=bool))
#todo(bcwaldon): implement optional kwargs such as limit, sort_dir </s> def detail(self, context, **kwargs):	detail
# todo: kept for backwards compatibility. </s> returns:	CopyToDict dict[str, object]: attribute values per name. dictionary = {}
# todo check for if-modified-since and if-none-match </s> route = self.router.bind_to_environ(environ)	RoutingHandler def __call__(self, environ, start_response): return self.wsgi_app(environ, start_response) try: endpoint, args = route.match()
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
# todo: reconsider this. </s> for nested_key in nested_keys[:-1]:	parse_override key, value = key_value nested_keys = key.split('.') if nested_key not in local_override_dict: local_override_dict[nested_key] = {}
# todo: ugly n^2 </s> example usage::	get_comment_form Syntax:: {% get_comment_form for APP_LABEL.MODEL_NAME with FIELD VALUE [with OPTIONS_STRING] as VARNAME %} {% get_comment_form for testapp.apple with id 1 with 'LO' as comment_form %} {% get_comment_form for object with 'LO' as comment_form %}
# todo: specific exception handling </s> return pc_get_event_types(self.directory)	get_event_types
# todo: handle return value from sandbox </s> event.matched_pattern = "php_cgi_rce"	TestEmulatorIntegration Notes:""" event = attack.AttackEvent() request_handler = RequestHandler(self.data_dir) emulator = request_handler.get_handler(event.matched_pattern)
# todo: remove this hack asap </s> if not is_paired():	check_connection def check_connection(): if connected(): payload = { 'utterances': ["pair my device"],
# todo constrain value to be in the directory </s> self._cell = cell	CellResource class CellResource(Resource): isLeaf = True self._noteDirty = noteDirty def grparse(self, value):
# todo: add bot's signature if needed (bug: t131517) </s> if code == 1:	fix_double_or_delete_broken_redirects count = 0 for (redir_name, code, target, final)\ continue elif code == 0:
# todo: scale and translation could be merged into a single network </s> assert mask.get_shape() == shape[1:]	get_mask def get_mask(self, x, dtype): shape = x.get_shape() return mask
# todo(nate): temporarily disabled </s> reason='malformed_manifest_json'	ManifestJsonHandler job_id=self.step.job_id, build_id=self.step.job.build_id, )) self.step.result = Result.infra_failed
# todo: write tests </s> "when there is an otherwise-unspecified validity error that prevents parsing."	MeiValidityError pass
#todo - what is bio.popgen using this for? </s> raise notimplementederror	_AbstractParameter def __init__(self): raise NotImplementedError
#todo: review this carefully, as we are now deleting stuff </s> path_parts[1] = translation_project.language.code	get_translated_name path_parts = store.parent.get_real_path().split(os.sep) path_parts.append(store.name) pootle_path_parts[1] = translation_project.language.code path_parts[-1] = name + '.' + translation_project.project.localfiletype
# todo: i18n/l10n: spaces aren't always the correct word separator </s> for suggestion in suggestions:	_update_results documents = self.docsearch.get_documents(txt.split(" ")) print "Got %d documents" % len(documents) self.suggestionList.append([ suggestion ]) self.matchList.clear()
# todo: must be implemented </s> pass	should_fetch_kindlegen
# todo: hacked @ hackathon, fix the comments </s> for block in self.blocks:	parent def parent(self): if block.parent: parent = self.runtime.get_block(block.parent)
# todo: i can't manage the import issue, can you? </s> response and replace a logtensor on top of all tensors found in	LogTensor instruction content, replace in the args all the LogTensors with their child attribute, forward the command instruction to the the response. :param command: instruction of a method command: (command name,
# todo packages for cloudera not available on lucid yet, using karmic for the moment (beta 1) </s> sudo("easy_install -u %s" % pname)	_python_library_installer def _python_library_installer(config):
# todo candidate for move to system/osi as not btrfs related </s> root_pool_mnt = mount_root(pool)	share_id returns the subvolume id, becomes the share's uuid. @todo: this should be part of add_share -- btrfs create should atomically out, err, rc = subvol_list_helper(root_pool_mnt) subvol_id = None
# # todo: add error handler </s> config.set('misp_modules', 'port', '6666')	init_config else: config.add_section('misp_modules') return config
# todo: remove </s> :returns: list of :class:`pykka.actor` subclasses	get_frontend_classes Mopidy will take care of starting the actors.
def getconfig(self, option, default=''):  #@todo: remove in 0.4.10 </s> self.pluginmodule = self.core.pluginmanager.loadmodule(type, self.__name__)	_initPlugin self.plugintype   = type
#todo: consider factoring out: some duplication between xliff and tmx </s> unit.target = translation	addtranslation def addtranslation(self, source, srclang, translation, translang): tuvs = unit.xmlelement.findall('.//%s' % self.namespaced('tuv')) lisa.setXMLlang(tuvs[0], srclang)
# todo: allow other formats? </s> media.reposition_file(file_id, budge_infront_id)	reorder_file 'budge_infront_id': validators.Int()}) def reorder_file(self, id, file_id, budge_infront_id, **kwargs): DBSession.add(media) DBSession.flush()
# todo: remove in 0.9.0 </s> delattr(model, trigger)	remove_transition self.events[trigger].transitions = defaultdict(list, **tmp) else: del self.events[trigger]
# todo action required that updates the endpoint </s> return eps	_get_endpoints mac_endpoints = self.sdnc.endpoints_by_mac(device) if len(mac_endpoints) > 0:
# todo implement </s> pass	dict def __init__(self, **elements): self.__elements = elements def get(self, k, d=None): try:
# todo extend to nonbinary nodes </s> return self.index < other.index	__lt__
# todo: using html formating tags eg. <pre> in ocil content </s> if "," in stig_id:	_verify_stigid_format stig_id = self.references.get("stigid", None) if not stig_id: raise ValueError("Rules can not have multiple STIG IDs.")
# todo: use lazylist </s> responsecls = zerigodnsresponse	ZerigoDNSConnection class ZerigoDNSConnection(ConnectionUserAndKey): host = API_HOST def add_default_headers(self, headers): auth_b64 = base64.b64encode('%s:%s' % (self.user_id, self.key))
else:  # todo assuming 720 webcam for now </s> return vid1	_filein def _filein(P:dict) -> list: fn = Path(P['filein']).expanduser()
# todo(vek): need to pass context in for access to auth_token </s> assert state in power_state.valid_states(), "bad state: %s" % state	InstanceInfo self.name = name
# todo make this a private api </s> else:	_parse_json return None if exactly_one: return [self.parse_code(feature) for feature in features]
# todo handle bad type </s> caller = inspect.getouterframes(inspect.currentframe())[1][3]	__abstract import inspect
# todo: fatal, how should this be handled? </s> for item in items:	compute_album_gain if len(self._file_tags) != len(items): raise Exception() ret.append(Gain(self._file_tags[item]["TRACK_GAIN"], self._file_tags[item]["TRACK_PEAK"]))
# todo: make remoteapp.create_all_files not return media files </s> def render_to_response(self, context, **response_kwargs):	BaseMultimediaTemplateView return context
# todo: only move interfaces attached to self.wlan, or all nodenum in script? </s> for model in models:	handleevent ) return cls = self.models.get(model) if not cls:
# todo: skip anything that's not marathon </s> return agent['hostname']	find_agent_hostname if 'slaves' in state: for agent in state['slaves']:
# todo this is a bit jankey to be honest </s> apps.apps_ready = apps.models_ready = apps.loading = apps.ready = false	PluginConfig apps_changed = True if apps_changed: apps.clear_cache() apps.populate(settings.INSTALLED_APPS)
# todo: can just leave this in superclass </s> return output	_get_version output = self.test(version_check) if output is None:
# todo - molecule type - see issue 363 / pull request #1005 </s> seqio.read(path.join("genbank", "negative_location.gb"), "genbank")	test_001_negative_location_warning with warnings.catch_warnings(): warnings.simplefilter("error", BiopythonParserWarning) except BiopythonParserWarning as e: self.assertEqual(str(e), "Couldn't parse feature location: '-2..492'")
# todo get stacktrace </s> for i in range(len(parameters)):	_set_stmt_parms prep_stmt.setObject(i + 1, parameters[i])
# todo check if config was successfully updated </s> if self.host:	check_connection def check_connection(self): self._connect() if self.ssh:
# todo check if obsolete </s> logger.info("send_add_object_to_scene %s <- %s", scene_name, obj_name)	send_add_object_to_scene buffer = common.encode_string(scene_name) + common.encode_string(obj_name) client.add_command(common.Command(common.MessageType.ADD_OBJECT_TO_SCENE, buffer, 0))
# todo: check the result </s> expected = matrix33.inverse(matrix33.create_from_quaternion(q))	test_inverse_equivalence result = matrix33.create_from_inverse_of_quaternion(q)
# todo: do we really need to populate the unnamed reg when we're populating the small </s> self.assertequal(fname, [testsstate.view.file_name()])	testCanGetFileNameRegister def testCanGetFileNameRegister(self):
# todo: log exception </s> shutil.rmtree(path)	main@38 for path in walk: path = path[0]
# arno, 2010-02-10: todo: convert infohash to binary </s> for p in res:	findPeers if not res: return [] ret.append({'permid':str2bin(p[0])}) return ret
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
# todo(jamalex): burn it all down! </s> node["description"] = _(node.get("description", "")) if node.get("description") else ""	recurse_nodes node["title"] = _(node.get("title", ""))
# todo: py2 does not support multiple * unpacking expressions in a single call. </s> tty.msg("modified files:", *[filename.strip() for filename in file_list])	print_style_header tty.msg("style: tools selected: " + ", ".join(tools))
# todo: check that the performance measure is within some range </s> def test_grid1(self):	TestBaselines def test_grid0(self): Tests flow/benchmark/baselines/grid0.py Tests flow/benchmark/baselines/grid1.py grid1_baseline(num_runs=1, sumo_binary="sumo")
# todo: /data/local/tmp might not be execuable and atx-agent can be somewhere else </s> text = self._reqsess.get(self.path2url('/pidof/' + pkg_name)).text	_pidof_app def _pidof_app(self, pkg_name): if text.isdigit(): return int(text)
# todo check for collision with user filter </s> def get_transition_width(self):	get_transition_width
f, en, nm = getcnns(level=1) # todo more flexible load needed. </s> data = getdatafromtxt(txt)	E error = np.zeros((len(data), 3)) for i in range(len(data)):
# todo: improve this code. </s> return [rawgadget(n) for n in node_list]	_build_gadgets def _build_gadgets(self, gadget_tree_root):
# todo: this add_metadata call should be removed once we are </s> return httpresponsebadrequest(json.dumps({'error': unicode(no_tag)}),	remove_tag_async return HttpResponse('{}', mimetype='application/json')
# todo: fix self.cursor_x >= w </s> self.comm = self.comm[:i]	inline_k_ctrl_k return i
# todo handle file does not exist </s> validated against a schema.	secret_metadata Missing secrets are not included in the returned dict. Secrets are not
# todo: list is incomplete, to be completed for missing languages. </s> 'lv',	Family 'ks', 'lb', 'mi', 'mn',
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> }	Test_AcquireTokenWithUsernamePassword "clientId" : "04b07795-8ddb-461a-bbee-02f9e1bf7b46", # xplat's which is supposed to be in every tenant "username" : "crwilcox@microsoft.com", authorityUrl = sampleParameters['authorityHostUrl'] + '/' + sampleParameters['tenant'] resource = '00000002-0000-0000-c000-000000000000' # or 'https://management.core.windows.net/'
# todo debug </s> self.triggered = false	RuleElement class RuleElement: def __init__(self): self.timeWhenTriggered = 0.0 self.timeTriggeredFor = 0.0
self.select(1)  # todo this should work without havng to use select() i.e. if cursor is at point on target. </s> self.assertnormal('|<q>hello world!</q>')	TestSurroundDocumentationExamples self.assertNormal("|'Hello world!'") self.select(1)  # TODO This should work without havng to use select() i.e. if cursor is at point on target. self.feed('cst"') self.assertNormal('|"Hello world!"')
# todo: disconnect </s> def listen_maddr_with_peer_id(self) -> multiaddr:	listen_maddr_with_peer_id @property
# todo: remove need for this </s> self._clone, self._review_branch.remote_base) is none:	get_any_author_emails def get_any_author_emails(self): hashes = phlgit_log.get_last_n_commit_hashes_from_ref( self._clone, 1, self._review_branch.remote_branch)
# todo: compute the weighted average instead of using the first solid </s> returns a copy of the ojbect, transformed by the provided matrix,	transformShape def transformShape(self, tMatrix): with all objects keeping their type tmp = self.wrapped.copy()
# todo problem - if there are no valid indices, we cannot return anything </s> return records	ReplayMemory next_states = self.read_variable(self.record_registry[state_name], next_indices) next_state_name = re.sub(r'^/states\b', "/next_states", state_name) def _graph_fn_get_records(self, num_records): size = self.read_variable(self.size)
# todo candidate for move to system/osi as not btrfs related </s> subvol_id = none	share_id return the id root_pool_mnt = mount_root(pool) for line in out: if (re.search(share_name + '$', line) is not None):
# todo: remnants from rllab -> gym conversion </s> goal_dists = [p['env_infos'][-1]['goal_distance'] for p in paths]	Pusher2dEnv ]).reshape(-1) def log_diagnostics(self, paths): logger.record_tabular('FinalArmDistanceAvg', np.mean(arm_dists)) logger.record_tabular('FinalArmDistanceMax', np.max(arm_dists))
#todo: this isn't actually most_recently_used (as defined in histories) </s> encoded_id = trans.security.encode_id( hda.id )	_summary_hda_dict 'url'   : < api url to retrieve this datasets full data >, } return { 'id'    : encoded_id,
# todo: distributed search messages need to implemented </s> else:	UserJoinedRoom def UserJoinedRoom(self, msg): if self.chatrooms is not None: self.logMessage("%s %s" %(msg.__class__, vars(msg)), 4)
# todo: figure out how to get multiscanner to report </s> delete the specified task. return deleted message.	delete_task @app.route('/api/v1/tasks/delete/<int:task_id>', methods=['GET']) result = db.delete_task(task_id) if not result:
# todo use deepcopy() here </s> vec_down = np.float32([0, 1])	_is_polygon_line @classmethod p1 = exterior[0] angles = set()
# todo: remove all elements of the list and remove the blacklist </s> tfa.seq2seq,	test_api_typed tfa.metrics, tfa.optimizers, tfa.text, ]
# todo the following line should use lazy lookup name-to-index mapping </s> for line in lines:	readUntil if line[0] == what: raise StopIteration
# now we can kill it. todo: on a slow machine, the node might kill </s> argv = ["--quiet", "stop", "--basedir", basedir]	test_baddir rc = runner.runner(argv, stdout=out, stderr=err) self.failUnlessEqual(rc, 1) out,err = StringIO(), StringIO() rc = runner.runner(argv, stdout=out, stderr=err)
# todo refactor set position cursor after operation into reusable api. </s> mappings_remove(normal, parsed.command.keys)	ExUnmap assert command_line, 'expected non-empty command line' parsed = parse_command_line(command_line) mappings_remove(OPERATOR_PENDING, parsed.command.keys) mappings_remove(VISUAL, parsed.command.keys)
# todo: then observer.events[0] == trial </s> self.events: list[trial] = []	SomeObserver class SomeObserver(_Observer[Tuple[HyperparamsRepository, Trial]]): def on_next(self, value: Tuple[HyperparamsRepository, Trial]): self.events.append(value)
# todo: cancel the previous calllater? </s> service_name, e	resolve_service if cache_entry: logger.warn( ) servers = list(cache_entry)
# todo test that labels in individual blocks are indeed different </s> seg, max_id = parallel_watershed(x, max_workers=4)	TestCarvingTools from ilastik.workflows.carving.carvingTools import parallel_watershed shape = (400,) * 2 max_expected = seg.max() assert max_id == max_expected, f"Expect {max_expected} but got {max_id}"
# todo: use lt and rt in profile as well </s> self.button_widgets[b] = controllertrigger(self, b, use_icons, w)	create_binding_buttons for b in TRIGGERS: w = self.builder.get_object("bt" + b) for b in PADS: w = self.builder.get_object("bt" + b)
# todo: end remove hosts/when block </s> stdout_queue,	shell read_buffer( 'stdout', print_output=print_output, print_func=lambda line: '{0}{1}'.format(print_prefix, line),
# todo: headervalueerror does not belong here </s> def none_or_unicode(val):	none_or_unicode
# todo use a proper category instead </s> "--toc"] + [i["file"] for i in config["markdown_files"]]	html "-t", "html5", "-o", "{}.html".format(CONFIG["FULL_PROJECT_NAME"]), local(" ".join(args)) local("open {}.html".format(CONFIG["FULL_PROJECT_NAME"]))
# todo: uncomment when adding support for literal hex bytes </s> except attributeerror as err:	test_setattr x.attr = 42
# todo add test </s> return [recursive_negate(v_) for v_ in v]	recursive_negate elif isinstance(v, tuple): return tuple([recursive_negate(v_) for v_ in v]) else: raise Exception("Expected None or int or float or StochasticParameter or list or tuple, got %s." % (type(v),))
self.assertequal(r[1], 0)  # todo: that is success? </s> _set_next_blob(data)	mock_success def mock_success(data, params, password, challenge):
# todo: add hook to be called after each song </s> orig_msg = self._get_variable('message')	owner_only def owner_only(func): @wraps(func) if not orig_msg or orig_msg.author.id == self.config.owner_id: return await func(self, *args, **kwargs)
# todo: verify </s> raise notimplementederror(omit)	get_omit_set else:
# @todo: bulk lookup </s> return div(div(label,	filter_formstyle_profile comment.add_class("inline-tooltip") controls_col.append(comment) controls, _class="small-12 columns",
# todo generator </s> doc="path/name of the component to be dropped",	Drop path=Parameter( args=("path",), nargs="*", constraints=EnsureStr() | EnsureNone()),
# todo: cache this result so multiple failing calls don't keep hitting the db </s> def from_stock_state(cls, stock_state):	from_stock_state return cls( case_id=stock_state.case_id,
# todo: add ssl verification </s> ],	create_job_object AUTH_TOKEN_ENV, EVALAI_API_SERVER_ENV, ) template = client.V1PodTemplateSpec(
# todo: is that right? </s> es.index(doc, index, doc_type=document._meta.db_table,	index_doc except pyes.urllib3.TimeoutError:
# todo placeholder; implement </s> self.worker_infos = worker_infos	RoleInfo self.name = name self.role_world_size = role_world_size
raise notimplementederror  # todo </s> else:	build_axis_spec_tree return tuple(map(build_axis_spec_tree, spec, in_tree.child_specs)) else: raise TypeError(type(in_tree))
## todo : log error </s> pass	stringify_listvars except IndexError:
# todo: verify </s> manager = factory.consumer_bind_manager()	test_remove_repo_cleanup self.assertEquals(len(binds), 1) manager = factory.repo_manager() binds = manager.find_by_repo(self.REPO_ID) self.assertEquals(len(binds), 0)
# todo: move this code into renderengine. </s> parse the contents of a template section.	_parse_section Arguments: template: a unicode template string.
# todo: this would be a good candidate for refactoring into a testcase subclass shared across backends </s> def prepare_text(self, obj):	SolrQuotingMockSearchIndex return MockModel
# todo(jakevdp): in rare cases, this fails python_should_be_executing check. why? </s> self.assertarraysequal(m, msparse.todense())	test_dense_round_trip rng = rand_sparse(self.rng()) M = rng(shape, dtype)
# todo: use xml config to get mac address and then parse ips </s> domain = self._get_domain_for_node(node=node)	ex_start Start a stopped node.
# todo: refactor </s> ' pixels per channel asked to convert design'	design_mat_to_topo_view if self.shape[-1] * self.pixels_per_channel != X.shape[1]: raise ValueError('View converter with ' + str(self.shape[-1]) + ' matrix with ' + str(X.shape[1]) + ' columns.') start = lambda i: self.pixels_per_channel * i
#todo - negative indices, see bug 2411 </s> % (short, seguid(record.seq), len(record.seq))	checksum_summary short = record.seq.tostring()[:19] \ + "..." + record.seq.tostring()[-3:]
# todo: 'annex.backends' actually is a space separated list. </s> size_str = key.split('-', 2)[1].lstrip('s')	get_size_from_key @staticmethod def get_size_from_key(key): except IndexError: return None
# todo: we should probably have a special folder just for header </s> except attributeerror:	CythonExtensionManager def so_ext(self): try: self._so_ext = self._get_build_extension().get_ext_filename('') return self._so_ext
# todo: attributes </s> dstr = ""	__generate_input_and_label l = [] assert " " not in t, "INTERNAL ERROR: space in storage form" else: dstr = ' disabled="disabled"'
# todo: remove compatability hook </s> if rewrite:	get_executables exe = s else: name = exe.name if sys.platform == "win32" and name.endswith(".exe"):
# todo: cast the value? </s> cursor = self.idb.id0.find_prefix(key)	get_tag_entries Yields: Entry: an entry (with key and value) under the given tag in this netnode. while bytes(cursor.key).startswith(key): parsed_key = parse_key(cursor.key, wordsize=self.idb.wordsize)
# todo pseudo code: </s> def seeked(self, position):	Seeked pass
# todo: displacement. </s> export_settings['temporary_meshes'] = temporary_meshes	filter_apply@118 filtered_vertex_groups[blender_mesh.name] = blender_object.vertex_groups export_settings['filtered_meshes'] = filtered_meshes filtered_materials = [] for blender_material in get_used_materials():
# todo: conflict detection/resolution </s> self._meta[name] = kwargs	meta if name in self._meta: del self._meta[name] return self
# todo make "master" not hard-coded, fetch it from some metadata </s> ]	VoluminousOptions subCommands = [ ["init", None, InitOptions, "Create a volume and its default master branch"], def postOptions(self):
# todo(piyush): current api-site doesn't contain this api description. </s> resp, body = self.get('tenants')	list_tenants self.expected_success(200, resp.status) body = json.loads(body)
federated_only=federated_only  # todo: 289 </s> return signing_power.generate_self_signed_cert(self.stamp.fingerprint().decode())	generate_self_signed_certificate def generate_self_signed_certificate(self):
# todo: stop at minimum scale </s> if index != self.active_scene_view:	activateSceneView for scene_view in self.scene_views: scene_view.deactivate()
# todo: and results </s> def __call__(self, args):	Value self._name = name
# todo(b/179510447): align these parameters with schulman 17. </s> return tfp.distributions.multivariatenormaldiag(	create_dist def create_dist(loc_and_scale): loc=loc_and_scale[..., :ndims], scale_diag=tf.math.softplus(loc_and_scale[..., ndims:]),
# todo: make this pretty </s> client = "1.1.1.1"	get_my_ip s.connect((server_addr, 9)) client = s.getsockname()[0] finally: del s
# todo: another solution should be used here. this is a hack for compatibility reasons. to resolve the gadget address calculation of segments of elf files have a different base address if calculated segment.virtualaddress - segment.offset </s> offset_tmp += match.start()	__gatherGadgetsByEnding to_return = [] match = re.search(ending[0], tmp_code) index = match.start() if offset_tmp % arch.align == 0:
# todo: repeating timers </s> return timer_id	AddTimer timer_id = self._timer_id_last self._timer_add.append(Timer(owner, timer_id, time.time(),
# todo: in 0.6.0 change this to "disabled": false </s> "name": "test-properties",	test_enabled_missing "dh": "dh.pem", "key": "key.pem", "proto": "udp", "tls_server": True
# xxx - 'todo' should just be a string </s> second = newframes[1]	_trimFrames if len(frames) < 2: return newFrames if (first[0] == "maybeDeferred" and os.path.splitext(os.path.basename(first[1]))[0] == 'defer'
# todo: support delete by name </s> def put(self, id, **kwargs):	StactionExecutionsController actionexec_db = ActionExecution.add_or_update(actionexec_db) return ActionExecutionAPI.from_model(actionexec_db) Update a actionexecution. Handles requests:
#todo(#212): use a map construct instead of unrolling. </s> return new_dtype	convert_element_type_dtype_rule
#todo: check cost line </s> if 'coastline' in classes:	BoatBuilder for xx,yy in [ (xx,yy) for xx in xrange(x, x + cls.size[0]) for yy in xrange(y, y + cls.size[1]) ]: tile = island.get_tile(Point(xx,yy)) coast_tile_found = True elif 'constructible' not in classes:
# todo: remove when migration plan / state is passed (#24100). </s> content_type__in=ctypes,	create_permissions@62 for perm in _get_all_permissions(klass._meta, ctype): searched_perms.append((ctype, perm)) ).values_list( "content_type", "codename"
# todo: what is this here for? we should really be catching something </s> from corehq.apps.orgs.models import organization	get_organizations def get_organizations(self):
# todo add sp_playlist_* methods </s> pass	PlaylistContainer
# todo(slaweq): change this to neutron floating ips and turn neutron </s> self.cloud._normalize_server(	TestRebuildServer mock_nova.floating_ips.list.return_value = [fake_floating_ip] self.cloud.name = 'cloud-name' meta.obj_to_dict(ret_active_server)), self.cloud.rebuild_server(
# todo: update hits@k </s> kg_embedding_model=kg_embedding_model,	compute_mean_rank corrupt_suject=True, compute_hits_at_k=False) triples=triples, corrupt_suject=False,
# todo: distributed search messages need to implemented </s> else:	UserJoinedRoom self.chatrooms.roomsctrl.UserJoinedRoom(msg)
# todo: check proactive neighbor resolution </s> self.sock.connect(self.faucet_event_sock)	setup_valve self.valve = valve_factory(dp)(dp, 'test_valve', self.notifier) self.valve.update_metrics(self.metrics)
# todo: this should be written to a log </s> print(ti.current_state())	task_state dag = get_dag(args.subdir, args.dag_id) task = dag.get_task(task_id=args.task_id)
# todo: sysex messages do not arrive here. </s> (message, delta_time) = self._rt.get_message()	Input if self._has_callback: raise IOError('a callback is currently set for this port') if message is None: break
# todo pydocs </s> size = self.arraysize	BigQueryCursor An :py:class:`~pyhive.exc.Error` (or subclass) exception is raised if the previous call to :py:meth:`execute` did not produce any result set or no call was issued yet. result = [] for _ in xrange(size):
entry['meta']['type'] = 'padding'  # todo handle padding, summarize and transfer </s> for c in set(include_currencies) & set(result.keys())}	serialize_inventory result = {p.lot.currency: p.number for p in inventory if p.number != ZERO} if include_currencies: return result
# todo: test. </s> url = '%s/users/lookup.json' % self.base_url	UsersLookup A list of twitter.User objects for the requested users if not user_id and not screen_name and not users: parameters = {} uids = list()
# todo: remove patch and update test once calculation_magic is implemented </s> _update_lpq_eligibility(project_id=17, cutoff=10)	test_not_eligible_not_lpq assert store.get_lpq_projects() == set()
# todo be more fussy with prefix checking: validate strings </s> obj (contest or task): an object that has the token_* attributes.	_get_token_status @staticmethod return (int): one of 0 (disabled), 1 (enabled/finite) and 2 (enabled/infinite).
# todo: division of mimo transfer function objects is quite difficult. </s> data[p][i][j] = data[p][i][j][k:]	_truncatecoeff k += 1
# todo(b/141131288): enable test once complex sort is supported on tpu. </s> for return_inverse in [false, true]	testUnique for dtype in default_dtypes for shape in all_shapes for return_counts in [False, True])) def testUnique(self, shape, dtype, return_index, return_inverse, return_counts):
# todo: support ipv6 addresses as well. </s> event_bus=self.event_bus,	_make_peer_pool max_peers=self.max_peers,
# todo: optimizer state gets cast to fp16 and back to fp32 for </s> betas (tuple[float, float], optional): coefficients used for computing	Adam params (iterable): iterable of parameters to optimize or dicts defining parameter groups. running averages of gradient and its square. (default: (0.9, 0.999)) eps (float, optional): term added to the denominator to improve
# todo: this ought to be a separate test of block-resources </s> def set_rw(self, value):	StateSpecimen @setter
if not config.testnet:  # todo </s> data = struct.pack(config.txtype_format, id)	compose@17 raise processblock.ContractError('negative startgas') if gasprice < 0: curr_format = FORMAT + '{}s'.format(len(payload)) data += struct.pack(curr_format, binascii.unhexlify(contract_id), gasprice, startgas, value, payload)
return  # todo: update </s> if section is none:	create_personal_data_fields def create_personal_data_fields(regform): section = RegistrationFormPersonalDataSection(registration_form=regform, title='Personal Data') missing = set(PersonalDataType)
# todo: check arp reply is valid </s> self.metrics = faucet_metrics.faucetmetrics(reg=self.registry) # pylint: disable=unexpected-keyword-arg	setup_valve self.table = FakeOFTable(self.NUM_TABLES) self.logger = valve_util.get_logger('faucet', self.logfile, logging.DEBUG, 0) self.notifier = faucet_experimental_event.FaucetExperimentalEventNotifier( self.faucet_event_sock, self.metrics, self.logger)
# todo stub </s> def metrics_identity(evaluator_identity: detectionevaluator) -> dataframe:	metrics_identity @pytest.fixture  # type: ignore
# todo: may test with codecs.open passing an encoding </s> self.assertis(rows.import_from_csv, rows.plugins.csv.import_from_csv)	PluginCsvTestCase filename = 'tests/data/all-field-types.csv' encoding = 'utf-8' self.assertIs(rows.export_to_csv, rows.plugins.csv.export_to_csv) def test_import_from_csv_filename(self):
# todo: argument constraints </s> searches for inconsistent annotations in given annotations	check_consistency objects.  Returns a list of SearchMatchSet objects, one for each checked criterion that generated matches for the search.
# todo check </s> pass	set_metadata @interfacedoc
# todo doc </s> owner_id = owner_id or user_id	SupplyPointCase def create_from_location(cls, domain, location, owner_id=None): id = uuid.uuid4().hex kwargs = {'external_id': location.external_id} if location.external_id else {} caseblock = CaseBlock(
# todo: replace usages of strictredis (redis-py 2.x) with redis in dramatiq 2.0. </s> pipe.lpush(message_key, self.encoder.encode(result))	RedisBackend def _store(self, message_key, result, ttl): with self.client.pipeline() as pipe: pipe.pexpire(message_key, ttl) pipe.execute()
# todo: macos/win? </s> class imagemagickbackenderror(exception):	ImagemagickBackendError
# todo: handle /dev/null (windows equivalent?) for new or deleted files </s> nbdiffapp.main([before, after])	show_diff@45
#todo: manage different in/out styles </s> self._set_raw()	generate_docs self._set_desc() self._set_params() self.generated_docs = True
# @todo: extend entity_types within the template </s> height, width = current.deployment_settings.get_ui_thumbnail()	doc_image_represent args = filename, ), if height: div["_style"] = "height:%spx;width:%spx;" % (height, width)
# todo: skip port_acl table if not configured. </s> for vlan in list(vlans.values()):	reset_refs if vlans is None: vlans = self.vlans vlan.reset_ports(list(self.ports.values())) if vlan.get_ports():
# @todo: replace with plot from get /api/v2/show/{id} </s> return self._genericmessage('error', 'unable to find the specified show')	subtitleShow return self._genericMessage('Error', 'Invalid show ID') show_obj = Show.find(app.showList, int(show)) app.showQueueScheduler.action.download_subtitles(show_obj) time.sleep(cpu_presets[app.CPU_PRESET])
# todo: this could be done using single query, but how? </s> id = column(integer, primary_key=true)	Quality class Quality(Base): quality = Column(String) entry = Column(PickleType)
# todo: handle incorrect or invalid certificate connection... </s> pass	test_issue114 client = SoapClient(wsdl=WSDL, soap_server="axis") ret = client.CREATEREQUEST(LOGIN="hello", REQUESTTYPE=1, REQUESTCONTENT="test") except SoapFault as sf: xml = SimpleXMLElement(client.xml_request)
# todo: description </s> def sourceguard_iface(iface_params, vlanmap_type, allinterf, enabled):	sourceguard_iface
# todo: add test option fro datasets that support that </s> batch_size=args.batch_size,	test_main@125 ) data_loader_test = torch.utils.data.DataLoader( sampler=test_sampler, num_workers=args.workers,
# todo xxx graalvm change </s> chatty=true,	test_sni_callback ssl_sock.context = other_context server_context.set_servername_callback(servername_cb) sni_name='supermessage') self.assertEqual(calls, [("supermessage", server_context)])
# todo: fix later </s> def select_batch_size(self, batch_size, min_frame_num_batch):	select_batch_size
pass # todo(denero) implement </s> self.assignment_name = 'assignment'	test_invalid_assignment_name
# todo: some regressors have extra options in their predict method, and they return a tuple of arrays. </s> if len(input_data) != len(self.inputs):	Model def predict(self, input_data): cache = dict()  # keys: Data instances, values: actual data (e.g. numpy arrays) raise ValueError('The number of training data arrays does not match the number of inputs!') cache.update(zip(self.inputs, input_data))
# todo:  google only takes the first 180 characters, so maybe we find a logical </s> if 'from' not in request.get or 'to' not in request.get:	compare_revisions The ids are passed as query string parameters (to and from). doc = get_object_or_404( raise Http404 from_id = smart_int(request.GET.get('from'))
# todo: remove this skip after fixing </s> assert_image_equal("screenshot", 'visuals/circle2.png')	test_circle_draw@20 color=(1, 0, 0, 1), border_color=(0, 1, 1, 1)) gloo.clear() ellipse = visuals.Ellipse(pos=(75, 35, 0), radius=20,
# todo: consider whether this is going to cause a crash on amd cards. it's possible it should be commented out. </s> glmatrixmode(gl_modelview)	cameraPosition glLoadMatrixd(np.ascontiguousarray(proj.T))
# todo: move following logic under util.filenaming </s> else:	_rmdir shutil.rmtree(dir, False)
# todo: write me! </s> def show_all(self):	show_all return self.value.value == SHOW_ALL_CHOICE
# todo: move this import to toplevel if possible [bruce 071029 comment] </s> use none in their place if necessary).	copy_nodes_in_order MT order, i.e. their native order in their Part) -- in fact, with a precise 1-1 correspondence between originals and copies See also copied_nodes_for_DND, which uses the nodes' native order instead. copies = copied_nodes_for_DND(nodes, assy = assy, _sort = True)
# todo: this is a magic fudge factor... </s> size = text_string.size()	Font textAttributes = NSMutableDictionary.alloc().init() textAttributes[NSFontAttributeName] = self.native size.width += 3 return size.width, size.height
# todo: tor might mark these dirs as setgid </s> * hiddenserviceauthorizeclient	test_tor_services_config * HiddenServiceDir * HiddenServicePort Check for each as appropriate. f = File("/etc/tor/torrc")
# todo(sdague): enforce license in init file if it's not empty of content </s> r"""check multi line docstring end.	hacking_docstring_multiline_end @flake8ext OpenStack HACKING guide recommendation for docstring: Docstring should end on a new line
# todo(boto-2.49.0): remove when we pull in the next version of boto. </s> kwargs['timeout'] = ssl_timeout_sec	GetNewHttp proxy_info.proxy_rdns = config.get('Boto', 'proxy_rdns', True) break http = http_class(proxy_info=proxy_info, **kwargs) http.disable_ssl_certificate_validation = (not config.getbool(
# todo sync protocol </s> def on_window(i3, e):	on_window
# todo: find out how to get global usernames </s> mastodon.status_favourite(rest)	fav @command faved = mastodon.status(rest) print("  Favorited: " + re.sub('<[^<]+?>', '', faved['content']))
# todo: collect extra data </s> import os	handle_connect_error def handle_connect_error(self, host, port, err):
# todo: get rid of prints, left over from refactoring </s> filter(series.name == parser.name.lower()).filter(episode.identifier == parser.identifier).first()	get_first_seen def get_first_seen(self, session, parser): if not episode: log.log(5, '%s not seen, return current time' % parser)
#todo: add metadata support when it is merged from develop </s> jid = "%s" % jid	_escape_jid def _escape_jid(jid): jid = re.sub(r"'*", "", jid) return jid
# todo: write custom scope generator for devices (in case none, etc..). </s> if isinstance(device, dict):	get_device device = self.default_device if isinstance(device, dict): device = device.get("variables", None) if variables is True else device.get("ops", None) return device
# todo map the alias to its target </s> if not os.path.isfile(dst_filename):	need_conversion blob = f.read(4096) version = int(re.search(r'version number="\$Revision: (\d+)', blob).group(1)) return True with open(dst_filename, 'rb') as f:
# todo: figure out a way to actually log this information without </s> if f['length'] == 126:	decode_hybi f['fin'] = (b1 & 0x80) >> 7 f['masked'] = (b2 & 0x80) >> 7 f['hlen'] = 4 if blen < f['hlen']:
# todo: should we do this? </s> width = size(node.get("width"))	node_format height = size(node.get("height")) viewbox = node.get("viewBox")
# todo: to be removed in v2.8.0 </s> def flat_menu_model_class(self):	FLAT_MENU_MODEL_CLASS return self.model_from_path_setting('FLAT_MENU_MODEL')
# todo: actually implement feature importance visualization for multiclass problems. </s> print(f'x_test: {x_test.shape}, y_test: {y_test.shape}')	Model if self.sampler is not None: X_train, y_train = self.sampler.fit_resample(X_train, y_train) self.clf.fit(X_train, y_train) feature_names = self.get_feature_names()
# todo: can we make use of existing property maps for this? </s> if idx != -1:	get_struct_info def get_struct_info(self, env, cont): from pycket.interpreter import return_multi_vals cont = call_cont(self.handlers[idx], env, cont) return self.inner.get_struct_info(env, cont)
#todo - complete implementation of these apis </s> except exception.networknotfound as e:	_items result = [builder.build(port, is_detail)['port'] for port in ports] return faults.Fault(faults.NetworkNotFound(e))
# todo: merge with config_check_pre_system_cron </s> mainconfigmodel = mainconfigmodel.objects.get(main_config_name = 'mainconfig')	config_check_run@158 stop_system_importer_file_csv_run = False cron_username = mainconfigmodel.cron_username error_logger(cron_username, " SYSTEM_IMPORTER_FILE_CSV_CRON_NO_USER_DEFINED")
# todo: remove v1compatibility when v1 migration is complete </s> if config.scheduling.options.v1compatibility:	_prepareData if principal is None: return (None, None, None) return (principal.record.fullName.decode("utf-8"), principal.record.guid,
# todo ... </s> [c.get_cc_outfilename(fn)],	testCpp binfile = c.get_cc_outfilename(fn) + ".bin" link_exec( options=["-g"] )
# todo setup attrs </s> json_meta['cname'] = str(meta['cname'], 'ascii')	write_array_metadata def write_array_metadata(path, meta): json_meta = dict(meta) json_meta['dtype'] = encode_dtype(meta['dtype']) meta_path = os.path.join(path, METAPATH)
errordialog(error[0], error[1]) # todo no-parent </s> (this docstring seems very legacy/historical, not accurate.)	_display_welcome_message def _display_welcome_message(parent=None): _display_generic_message("master", 'behavior.betawarn', parent=parent)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	GetTrytesResponseFilter def test_pass_transactions(self): self.skipTest('Not implemented yet.')
# header. todo for aron: fix polib.py </s> yield os.path.join(current_dir, po_file)	all_po_files po files in the given directory. for current_dir, _, filenames in os.walk(dir):
# todo: make grouper in query </s> tiles.append(tile_query)	list_tiles tile_query.update({dim: dataset.__getattribute__(dim) for dim in dataset.type.dimensions return tiles
# todo: clean up </s> def is_gossipsub():	is_gossipsub return True
# todo: log errors to log file </s> return self.user	get_user
# todo: add support of tuple (row_offset, col_offset) </s> image: tensor,	flips_deaugment reduction: MaybeStrOrCallable = "mean", ) -> Tensor:
# todo: there should be an output for warnings and we should test we get one here </s> def check(s):	test_update_good if response.code >= 300: print data j = json.loads(s) self.assertEqual(j, modified)
pass  # todo </s> pass  # todo	SpotifyPlaylistsProvider def refresh(self): pass  # TODO
#@todo: remove in 0.4.10 </s> pages = int(m.group(1))	handleMultiPages def handleMultiPages(self): try: except: pages = 1
# todo is pexpect thread safe, e.g. could we be blocked on this </s> self.cb[handle]=fn	BluetoothLeDevice except pexpect.TIMEOUT: pass
# todo: if nucleus/symmetryconstraint bug ever fixed: </s> dagfn = mfndagnode (obj)	toMDagPath def toMDagPath(nodeName): obj = toMObject (nodeName) dagPath = MDagPath() dagFn.getPath ( dagPath )
# todo: layer normalization </s> feed_forward_dropout_layer = keras.layers.dropout(	get_transformer@33 hidden_dim=hidden_dim, name='%s-FeedForward' % name, rate=dropout, name='%s-FeedForward-Dropout' % name,
# todo: for now we are not going to track bikes roaming around </s> def update(self, scraper = none):	Nextbike super(Nextbike, self).__init__(tag, meta) self.url = BASE_URL.format(domain = domain) if scraper is None: scraper = utils.PyBikesScraper()
# todo(stephenfin): use a helper </s> server = super()._create_server(networks='none')	_create_server def _create_server(self): self.addCleanup(self._delete_server, server) source_host = server['OS-EXT-SRV-ATTR:host']
# todo: is there a nicer way to do this? if i add a new grep plugin i won't </s> file_path = os.path.join('plugins','tests','grep','data',file_name)	profile_me for _ in xrange(1): for counter in xrange(1,5): body = file( file_path ).read() hdrs = Headers({'Content-Type': 'text/html'}.items())
# todo: not all messages have running status </s> data = data[:-1]	_read_sysex length = self.file.read_byte() data = self.file.read_list(length) message = mido.new('sysex', data=data) dbg('    {}'.format(message))
# todo implement this </s> db_name = "/data/data/com.android.providers.settings/databases/settings.db"	change_settings :param table_name: table name to work on, usually it is system or secure :param name: settings name to set self.get_adb().shell("sqlite3 %s \"update '%s' set value='%s' where name='%s'\"" % (db_name, table_name, value, name))
# # fixme: # todo: remove me </s> with open(os.environ['ail_bin']+'/torcrawler/blacklist_{}.txt'.format(service_type), 'r') as f:	load_blacklist def load_blacklist(service_type): redis_crawler.delete('blacklist_{}'.format(service_type)) lines = f.read().splitlines()
# todo: more advanced logic in determining valid symbols </s> for sym in other_schema_sym_list:	transform_eq_to_generic_schema xnor transformed_eq = raw_eq if sym in transformed_eq: transformed_eq = transformed_eq.replace(sym, tt_schema_sym)
# todo: remove in v1.0.0 </s> @abstractmethod	reset_test_dataloader
# todo: make mac table updates less expensive. </s> chassis_id = str(port.native_vlan.faucet_mac)	send_lldp_beacons if (port.dyn_last_lldp_beacon_time is None or port.dyn_last_lldp_beacon_time < cutoff_beacon_time): org_tlvs = [] for org_tlv in lldp_beacon['org_tlvs']:
# todo: implement counters </s> return str(self.service.version)	getVersion
# todo: take this out later </s> for dir in dirs])	egg_info_path if dir == 'test' or dir == 'tests': dirs.remove(dir) filenames = [f for f in filenames if f.endswith('.egg-info')] if not filenames:
# todo: config of maps of packages </s> res = str(res)	_check_exit res = util.get_re_from_child(child.before,'^EXIT_CODE:([0-9][0-9]?[0-9]?)$') if res not in exit_values or res == None: self.log(util.red('child.after: \n' + child.after + '\n')) self.log(util.red('Exit value from command+\n' + send + '\nwas:\n' + res))
style="toolbutton", # todo: does this cause problems in some macs? </s> self._show_object_by_id(self.forward_links.pop(), true)	navigate_forward if len(self.forward_links) == 0: return
# todo test errors </s> assert_array_equal(a, z[:])	test_append_1d eq(a.shape, z.shape) eq(a.dtype, z.dtype) b = np.arange(105, 205) e = np.append(a, b)
# todo(b/160294509): use tf.compat.v1 when we stop supporting tf 1.15. </s> args:	supply_missing_inputs Supports only tf.Tensor and tf.SparseTensor. Note: Since this returns placeholders, it should be called from within a graph structured_inputs: a dict from keys to batches of placeholder graph tensors. batch_size: an integer representing the size of the batch returned.
# todo: command+c for mac </s> self.rowconfigure(0, weight=1)	ReplayerCodeView self.hbar['command'] = self.text.xview self.vbar['command'] = self.text.yview
self.assertfalse(greps(err, "unit zzz.service not for --user mode")) #todo </s> sx____(cmd.format(**locals()))	test_6133_run_default_services_from_single_service_saved_container [Install] WantedBy=multi-user.target""") cmd = "docker run --detach --name={testname} {image} sleep {sometime}" sh____(cmd.format(**locals()))
#todo: may be this should go in a decorator for use in every command. </s> note: this is intended for private use in this class by now.	_annex_init If you have an object of this class already, there shouldn't be a need to 'init' again.
# todo remove this crap </s> levels["users"][user.mxid] = new_level	_participant_to_power_levels user_has_right_level = (levels["users"][user.mxid] == new_level if user_level_defined else new_level == 0) return True return False
#@todo: remove in 0.4.10 </s> self.package_links += self.getlinks()	handleMultiPages pages = 1 for p in xrange(2, pages + 1):
# todo: optimize this by using send_data(array[from:to]) </s> self.set_setting(self.vcom_lut, self.partial_lut_vcom1)	EPD4in2 self.set_setting(self.W2B_LUT, self.LUT_WB) self.set_setting(self.B2B_LUT, self.LUT_BB) self.set_setting(self.W2W_LUT, self.PARTIAL_LUT_WW1) self.set_setting(self.B2W_LUT, self.PARTIAL_LUT_BW1)
# todo: add detection for randomly generated macs (random 48-bit number with its eighth bit set to 1 as </s> m = re.match(r'(?p<uuid>[0-9a-fa-f]{8}-?([0-9a-fa-f]{4}-?){3}(?p<mac>[0-9a-fa-f]{12}))', str(node.value))	run@51 def run(unfurl, node): if m: u = m.group('uuid')
# todo: use is_accessible once two layer trie is implemented </s> validate_canonical_address(address, title="storage address")	set_balance validate_uint256(balance, title="Account Balance") if self.is_access_restricted:
return 1  # todo +self.rec(expr.index)? </s> def map_bitwise_not(self, expr):	map_bitwise_not
# todo: support speedy mode for running the script </s> if os.path.exists(f):	rm_configs def rm_configs(): "._config" (generated by us), if present.""" os.remove(f) rm_if_exists(".config")
#todo: namespaces too hardwired, clean-up... </s> }	UsernameToken 'wsse:UsernameToken': { 'wsse:Username': username, } def preprocess(self, client, request, method, args, kwargs, headers, soap_uri):
#@todo: remove in 0.4.10 </s> m = re.search(self.pages_pattern, self.html)	handleMultiPages def handleMultiPages(self): pages = int(m.group(1)) except:
# todo: in 0.6.0 change this to "disabled": false </s> option key 'key.pem'	test_enabled_missing option dev_type 'tap' option dh 'dh.pem' option mode 'server' option proto 'udp'
# todo: still doesn't handle the case where the user wants </s> if match:	parse_unwanted_id def parse_unwanted_id(self, data): for id_unwanted_re in self.unwanted_id_regexps: log.debug('unwanted id regexp %s matched %s' % (id_unwanted_re, match.groups())) return True
# todo: check implementation </s> "relu.negative": relunegativepattern,	get_pattern_class "relu.positive": ReLUPositivePattern,
raise notimplemented  # todo: other backends </s> index,	ursula@307 registry_filepath, value, list_, divide
# todo: save message to history </s> return self._new_messages	get_messages
# todo: maybe inefficient. use matrix operation </s> def _cal_similarity(self, query, obj_emb):	_cal_similarity
# todo: remove str() when dropping support for py37. </s> str(self.connection.settings_dict['name'])]	DatabaseClient executable_name = 'sqlite3' def runshell(self): subprocess.run(args, check=True)
# todo: figure out why yask doesn't like it with dse/dle </s> raise valueerror("incorrect input size %s for model of size" % vp.shape +	Model elif vp.shape == self.shape: initialize_function(self._vp, vp, self.nbpml) " %s without or %s with padding" % (self.shape, self.vp.shape))
# todo: check the data! </s> u'totaltravelclaimed': u'5337', u'mpair': u'0', u'spousenumofjourneys': u'1'})	test_csv u'CentrallyPurchasedStationery': u'1149', u'TotalBasicAllowancesExcTravel': u'146282', u'CentralITProvision': u'1223', u'StaffCoverAndOtherCosts': u'0', self.assertTrue(count > 0)
# todo: remove compatability hooks </s> end += 1	_split_version_components end += 1 elif s[start].isalpha(): elif s[start] in (".","-"): pass
except exception:  # todo what exception? </s> f'version {version} of module {id_name}'	import_module_from_directory ModuleVersion.objects.get(id_name=id_name, source_version_hash=version) ' has already been imported' )
# todo: remove warning check once deprecated </s> "b": range(-5, 0),	TestFrameSindex def setup_method(self): data = { "location": [Point(x, y) for x, y in zip(range(5), range(5))], }
# todo: sinpi </s> try:	_mathfun_n def _mathfun_n(f_real, f_complex): return f_real(*(float(x) for x in args)) except (TypeError, ValueError):
# todo: simplest possible unicast learning. </s> args:	edge_learn_port other_valves (list): All Valves other than this one. pkt_meta (PacketMeta): PacketMeta instance for packet received.
# todo: handle marker? </s> keyid=none, profile=none):	describe_alias Given a function name and alias name describe the properties of the alias. Returns a dictionary of interesting properties.
# todo: here the index is correlated to the duals, try if this can be fixed when temp duals are removed. </s> if not var.fixed:	add_affine_cuts concave_cut_valid = True convex_cut_valid = True if ccSlope[var] == float('nan') or ccSlope[var] == float('inf'): concave_cut_valid = False
pass # todo </s> def is_bool(t):	is_bool
# todo extend to nonbinary nodes </s> conditioned on the fixed state of boundary-condition nodes in the	raw_current_marbl @property current timestep.""" if self._raw_current_marbl is not None:
# todo (b/151636380): remove when cl/299961405 is propagated through kokoro. </s> return tf.keras.metrics.falsenegatives(name='false_negatives')	_tf_metric_by_name elif metric_name == 'true_negatives': return tf.keras.metrics.TrueNegatives(name='true_negatives') elif metric_name == 'specificity_at_sensitivity': return tf.keras.metrics.SpecificityAtSensitivity(
# todo: investigate why results are sometimes 'nan' </s> [pupil_detection_result_2d["ellipse"]["center"]],	draw_pupil_outline@48 confidence = pupil_detection_result_2d["confidence"] * 0.7 draw_polyline(pts, 1, RGBA(1.0, 0, 0, confidence)) size=20, color=RGBA(1.0, 0.0, 0.0, confidence),
# todo(yamahata): eliminate dumb polling </s> self.driver.power_off(instance)	power_off_instance def power_off_instance(self, context, instance_uuid): instance = self.db.instance_get_by_uuid(context, instance_uuid) current_power_state = self._get_power_state(context, instance) self._instance_update(context,
# migration todo: remove ? </s> logger.exception("requeuing message with 1s delay: %s", exception)	EnrichWorker declare=[enriched_events_exchange]) self.produced_events_counter.labels(event.event_type).inc() time.sleep(1) message.requeue()
#todo use backup phone </s> form = computer_verification_form(request, user)	verify_computer@121 path='/accounts/verify/', max_age=30*86400, httponly=True) return response token = user.token if token.method in ('call', 'sms'):
pass  # todo </s> else:	_onAddDevice cluster_size = int(properties.get(b"cluster_size", -1)) if cluster_size > 0: device = LegacyUM3OutputDevice.LegacyUM3OutputDevice(name, address, properties) self._discovered_devices[device.getId()] = device
# todo: this is untested. </s> "int (*)(int, x509_store_ctx *)", wrapper)	_VerifyHelper else: return 0 def raise_if_problem(self): if self._problems:
# todo: use flask logger without it triggering the root </s> response.status,	register_teardown_request "%s %s %s\n[Request object]: %s", request.method, pformat(request.get_json()), )
#ack = self.serialport.read() # todo: use ack </s> print "handshake error. please reset the microcontroller."	performHandshake self.serialPort.write(frame) ack = self.serialPort.readline()
# todo: need to figure out how we prevent multiple selfdestructs from </s> if sender_balance < message.value:	_apply_sharding_message if message.should_transfer_value and message.value: with vm.state_db() as state_db: raise InsufficientFunds( "Insufficient funds: {0} < {1}".format(sender_balance, message.value)
except exception:  # todo: refactor this... </s> def test_init(self):	test_init
# todo make this a private api </s> def _parse_json(cls, resources, exactly_one=true):	_parse_json Parse display name, latitude, and longitude from a JSON response. if not len(resources['features']):  # pragma: no cover
# todo: dateparser is a little greedy, consuming the "on " as well as the date </s> random.seed(4321)	DoBTestCase def test_generate(self): fake = faker.Faker() self.assertIn( DateOfBirthFilth.generate(faker=fake),
+ ansi.ansi_normal  # todo: why does it keep it? </s> parser = text2html.html_parser	test_convert_linebreaks self.assertEqual("foo", parser.convert_linebreaks("foo")) self.assertEqual(
# todo: is there a more elegant fix? </s> in in_out_pairs	IoHandler self._handlers = [ OutputHandler(file_in, file_out) ] def wait(self):
# todo: improve handling of s.a < s.b and s.a > s.b cases. </s> regions_transformer(self.view, f)	ViBigL row, _ = self.view.rowcol(r.b) row -= count + 1 self.view.show(target)
# todo: test & review this part </s> :returns: path	vboxmanage_path Returns the path to VBoxManage.
# todo: type: literal["m", "h", "x", "b"] </s> def getfilename(self) -> str:	getFileName
# todo make this a real request </s> self.fields['tenant_id'].choices = [[tenant.id,tenant.id] for tenant in tenant_list]	UserForm def __init__(self, *args, **kwargs): tenant_list = kwargs.pop('tenant_list', None) id = forms.CharField(label="ID") email = forms.CharField(label="Email")
# todo: fails because of missing svg support </s> <style>	test_image_repeat_block @assert_no_logs def test_image_repeat_block(): @page { size: 8px; margin: 0; background: #fff } table { border-collapse: collapse; margin: 2px }
# :todo: implement test. </s> def test_pass_tags_only(self):	FindTransactionsRequestFilterTestCase self.skipTest('Not implemented yet.') def test_pass_addresses_only(self): self.skipTest('Not implemented yet.') def test_pass_approvees_only(self):
# todo log here </s> return factor['id']	get_factor_id@48 if factor['status'].lower() != 'active': not_active = True if not_active: pass
# todo: arrange </s> def createrepo(self):	createRepo repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "name", "testrepo0", self.token)
# todo convert to fit transform </s> randomized_search=true):	random_forest_classifier if hyperparameter_grid is None: max_features = helpers.calculate_random_forest_mtry_hyperparameter(len(self.X_test.columns),
# todo (elliot): put this in the preferences. </s> if facts["version_key"] == "cfbundleversion":	generate_jss_recipe "template_path": "%GROUP_TEMPLATE%" }] keys["Input"]["GROUP_TEMPLATE"] = ( "CFBundleVersionSmartGroupTemplate.xml")
# todo: 搜索和分页 </s> else:	perm_role_push print ret if ret_failed: return HttpResponse(u"推送系统角色： %s" % ','.join(role_names))
#todo: append the rule instead of overwrite the full content </s> def tryexec(cls):	tryexec return GIT.App.tryexec()
# todo check </s> pass	set_metadata @interfacedoc
# todo: figure out a better way to do things </s> def lift(i, gen):	ParentMethods Family ((F[1], 1), (F[2], 1), (F[3], 1), (1, F['a']), (1, F['b'])) F = self.cartesian_factors() cur = list(ids) cur[i] = gen
# todo: requires explicit broadcast in future </s> self.y_out_axes = y.axes - self.y_reduction_axes	DotOp self.y_reduction_axes = self.x_reduction_axes assert self.x_reduction_axes == self.y_reduction_axes axes = self.x_out_axes | self.y_out_axes super(DotOp, self).__init__(
# todo: check whether loaded network has the same number of classes as specified in ilastik! </s> h5py_group[self.hdf5_group_filename] = self._filename	serialize_hdf5
# todo add list as an option </s> +/-inf.	Multiply bytes (two) as ``uint16``. This is done to make overflows less likely to occur. Parameters ----------
# todo: make the get_closest_value to return region </s> def closest_to_caret(item, caret):	closest_to_caret
pass # todo: implement </s> returns none if the url was not valid or the document could not be loaded.	Errors def cursor(self, url, load=False): If load (defaulting to False) is True, the document is loaded print url pass # TODO: implement
# todo jython </s> pass	sslwrap_simple try: sock.getpeername() else: ssl_sock.do_handshake()
# todo: remove in v2.8 </s> return true	has_permission def has_permission(self, request, view): return request.user.is_authenticated
# todo(lyarwood): test drivervolumeblockdevice.driver_detach in </s> self.requested_networks, none, self.security_groups, none)	test_build_networks_if_not_allocated mock_allocate.assert_called_once_with(self.context, instance,
# todo consider more type conversions? </s> y + self.height // 4)	get_midbottomright def get_midbottomright(self): x, y = self.get_origin()
# todo: python-components: for now, we call each preprocessor's graph_fn directly. </s> e.g. api_methods={("stack_run", "run")}. this will create "stack_run" for the stack, which will call	Stack Defaults to {"apply"}. All sub-Components must implement all API-methods in this set. Alternatively, a tuple can be used  (instead of a string), in which case the first tuple-item one by one the "run" methods of the sub-Components. Connecting always works by first calling the first sub-Component's API-method, then - with the
# todo: test this method </s> value = self.cleaned_data['extra']	MetaDataForm class Meta: model = models.MetaData if strip_tags(value).strip(): raise forms.ValidationError("Extra code may not contain text outside tags (advanced use only).")
#reactor.stop() # for unknown reasons, reactor.stop() isn't working.  [ ] todo </s> def when_tub_ready(self):	when_tub_ready
# see optimization description comments and todo for tags in matching public histories query. </s> .filter_by(user=trans.get_user()) \	_get_shared def _get_shared(self, trans): shared_by_others = trans.sa_session \ .join(model.Visualization.table) \ .filter(model.Visualization.deleted == false()) \
# todo: remove </s> print("this daemon might interfere with auto-cpufreq and it will be disabled!")	gnome_power_detect print("python3 gnome_power.py --disable") elif gnome_power_stats == 0: gnome_power_disable() print("\nIf you wish to enable this daemon to run concurrently with auto-cpufreq run:")
# todo: "dob.role is not none" </s> blink_disk(disk.name, total_time, blink_time, sleep_time)	_blink_drive total_time = int(request.data.get('total_time', 90)) blink_time = int(request.data.get('blink_time', 15)) return Response()
#todo - once we drop support for python 2.4, instead of this </s> nnnnnnnnnnnnnnnnnnnn	back_transcribe >>> my_rna UnknownSeq(20, alphabet = Alphabet(), character = 'N') >>> my_dna = my_rna.back_transcribe() >>> my_dna
# todo(rameshg87): need better way of asserting the routes. </s> self.assertisinstance(driver.deploy, iscsi_deploy.iscsideploy)	test_pxe_ssh_driver driver = pxe.PXEAndSSHDriver() self.assertIsInstance(driver.power, ssh.SSHPower) self.assertIsInstance(driver.management, ssh.SSHManagement) self.assertIsInstance(driver.vendor, iscsi_deploy.VendorPassthru)
# todo: should this do the same cleanup as the on_message code? </s> self.set_extra_headers()	head self.finish()
try:  # todo: fix brodcast issue if different </s> if type(val) is np.array:	is_none return False else:
# todo: automate this (by lookup from nn). </s> action_space=self.action_space	test_large_impala_policy_without_agent action_adapter_spec=dict(type="baseline_action_adapter")) test = ComponentTest( ) nn_input = self.input_space.sample(size=(1, 1))
# todo: log in with right permissions and request the page again </s> response = self.client.get('/test_client/unknown_view/')	test_unknown_page "GET an invalid URL"
raise notimplementederror #todo </s> pass #todo	Span
# todo get tri-letter dimensionality from fit-transform as input shape </s> kernel_initializer=self._params['w_initializer'],	_build_shared_model x_in = Input(shape=(dim_triletter,)) x = Dense(units=self._params['dim_hidden'], bias_initializer=self._params['b_initializer'])(x_in) for _ in range(0, self._params['num_hidden_layers']):
# todo: remove when unicode vlens implemented </s> def iter_cb(name, *args):	iter_cb
raise notimplementederror  # todo... </s> return inject_retrieval_code(net_dict, rec_layer_name=args.rec_layer, layers=layers, dropout=args.dropout)	init_net :param args: :param list[str] layers: rnn.engine.use_dynamic_train_flag = True  # will be set via Runner. maybe enabled if we want dropout rnn.engine.init_network_from_config(config=config, net_dict_post_proc=net_dict_post_proc)
# todo: refactor accordingly when v3 websocket api is released </s> return bittrexorderbook	order_book_class @property
""" todo: documentation </s> ```	StreamingCommand ``` If your streaming command modifies the time order of event records you must @Configuration(overrides_timeorder=True) class CountMatchesCommand(StreamingCommand):
# todo: support for 'file' type </s> jsonschema.validate(value, spec)	validate_object def validate_object(spec, value): :param spec: spec for an 'object' type in dict form
# todo get the default name from preferences </s> return	open_last_cb def open_last_cb(self, action): if not self.confirm_destructive_action(): return
# todo results from p0f </s> def _get_name(endpoint):	_get_name return endpoint.machine.name.strip()
# todo print out the certificates </s> w.prnt("", "buffer '%s' will be closed!" %	room_close_cb @utf8_decode W.buffer_get_string(buffer, "name")) return W.WEECHAT_RC_OK
# todo: cleanly remove clipboard code if it is no longer needed </s> model = folder	NewFolderForm class NewFolderForm(forms.ModelForm): fields = ('name',) widgets = {
# todo: start here </s> base_uri = glob_match.group(1)	_ls scheme = urlparse(path_glob).scheme glob_match = GLOB_RE.match(path_glob) else: base_uri = path_glob
# todo trace model </s> yield {k: entry[c] for k, c in self.column}	JsonPipelineDataFormat def __iter__(self): for entry in self._entries: else: yield entry[self.column]
# todo: deprecated function. see ticket #453. </s> else:	_gettilefromset u = openURL(url, '', username = self.username, password = self.password, timeout=timeout or self.timeout) raise ValueError('cannot find zoomlevel %i for TileMap' % z)
#todo this will change when we attempt #35, since this assumes intersection </s> def order_by(self, *field_names):	order_by pass
# todo - make this work on loop with more than two links </s> pass	make_fill_posts
# todo test </s> self.tpm = tpm	StateUnreachableError self.past_state = past_state
# reasons why we said no. todo: allow configurable error messages </s> "sender": user_id,	DirectoryHandler "type": EventTypes.CanonicalAlias, "state_key": "", "content": {}, },
#todo: remove convert_bare true and deprecate this in with_ </s> if "bad configuration option" in err:	_get_connection try: cmd = subprocess.Popen(['ssh','-o','ControlPersist'], stdout=subprocess.PIPE, stderr=subprocess.PIPE) conn_type = "paramiko" except OSError:
# todo check if we can avoid that </s> ntet   = int(items[3])	read_buffer@69 nnodes = int(items[0]) ntria  = int(items[1]) npyra  = int(items[4]) nprism = int(items[5])
# todo: this should be abstracted into a property/method or something </s> if not hasattr(cls, 'template'):	_needs_templates @classmethod raise ImproperlyConfigured, 'You need to register at least one template for Page before the admin code is included.'
# todo: t102735: use the page content model for <1.21 </s> 'for the request to the api should be added via the '	_warn_kwargs @classmethod def _warn_kwargs(cls): '"parameters" parameter.', DeprecationWarning, 3)
# todo: log exception </s> if os.path.isfile(conf["path"]):	scan@71 local = True elif SSH:
# todo: (find a way to) test if the line underneath indeed correctly replaces the reimplemented values method </s> self.write("imode 0")	set_voltage_mode
# todo: test for the _correct_ revision_id value. </s> assert detail.object_type == "resource", \	test_create_package if detail.object_id == package_created['id']: assert detail.object_type == "Package", str(detail.object_type) str(detail.object_type) elif detail.object_id == package_created['resources'][1]['id']:
# todo: refactor this to be more uniform across sources </s> settings["network"] = self.network	activate_source def activate_source(self, settings={}):
# todo: beautify output </s> return options[c]()	check_sub_command 'delete' : delete } except KeyError: chalk.red('Command does not exist!')
# todo: implement me </s> position, format = self.playbin.query_position(gst.format_time)	get_current_time def get_current_time(self): return to_seconds(position) except Exception, e:
# todo: make this section of code easier to understand. </s> for key, val in data.iteritems():	buildTest expected = testData['expected'] data     = testData['data'] if isinstance(val, dict) and val.get('__tag__') == 'code': val = eval(val['python'])
# todo add a conditional to toggle this </s> pathtotransfer: path on disk, including the transfer directory and a	index_transfer_files Returns the number of files indexed. conn: ElasticSearch connection - see connect_and_create_index trailing / but not including objects/ index, type: index and type in ElasticSearch
# todo yoon </s> global kenlm	load_kenlm def load_kenlm():
# todo: ^intel-parallel-studio can mean intel mpi, a compiler or a lib </s> def lapack_libs(self):	lapack_libs return self.blas_libs
raise exceptions.mpdnotimplemented  # todo </s> raise exceptions.mpdnotimplemented  # todo	subscribe@18 Subscribe to a channel. The channel is created if it does not exist already. The name may consist of alphanumeric ASCII characters plus
# todo: could also blame / </s> if val.tag_() == value_e.maybestrarray:	_EvalLhsAndLookupArith Returns: (Python object, lvalue_t) e_die("Can't use assignment like ++ or += on arrays") span_id = word_.SpanForLhsExpr(node)
# todo ... </s> if usepypy:	get_python_ccopts return ["-I", "/usr/local/Cellar/pypy/1.9/include"] else:
#    #todo </s> if isinstance(v, (int, long)):	_nxm_ip def _unpack_value (self, v): return IPAddr(v, networkOrder=True) if v > 32: v = 32 elif v < 0: v = 0
# todo: (longer term) rather than abort, reject this candidate </s> ireq = install_req_from_line(	make_install_req_from_dist "{}=={}".format( canonicalize_name(dist.project_name),
# todo check if this always works? </s> if not layers_name.startswith("_"):	triangulate bm.from_mesh(me) for elem in (bm.faces, bm.edges, bm.verts, bm.loops): layers = getattr(elem.layers, layers_name) for layer_name, layer in layers.items():
"""todo: not implemented""" </s> >>> ser = pd.series([1,2,3])	n @symbolic_dispatch def n(x): >>> n(ser) 3
# todo: delete </s> t4 = 2.0/a_alpha	d_lnphi_dzs C = 1.0/(Z - B) Zm1 = Z - 1.0 t5 = -A/(two_root_two*B) Eis = [t5*(t4*a_alpha_j_rows[i] - bs[i]*b_inv) for i in cmps]
#todo: use a getter </s> self._set_raw()	generate_docs self._set_desc() self._set_params() self.generated_docs = True
# todo(dcramer): we want to be less aggressive on disabling domains </s> class domainblacklisted(badsource):	DomainBlacklisted
# todo: this is untested. </s> def __init__(self, connection, callback):	_VerifyHelper self._problems = [] @wraps(callback)
## todo: # fixme: remove me </s> try:	analyse@32 p.populate_set_out(path, 'Duplicate') msg = 'infoleak:automatic-detection="sql-injection";{}'.format(path) tld = url_parsed['tld'].decode() except:
# todo: we possibly need to sync so all replicas are upto date </s> steps:int, metrics:metrics)->none:	_post_step def _post_step(self, x:Tensor, y:Tensor, logits:Tensor, loss:Tensor,
# todo: summary hash for new current id </s> return self	set_hyperparams def set_hyperparams(self, hyperparams: HyperparameterSamples) -> BaseStep: MetaStepMixin.set_hyperparams(self, hyperparams)
# todo: how would this change with domain? </s> return x[0][1] == value	value_function
# todo: copy other properties (gcps etc). several other </s> )	convert@100 with ctx.obj['env']: outputfile, files = resolve_inout( inputfile = files[0] with rasterio.open(inputfile) as src:
# todo: change these to reflect values set in the database </s> def ph(self):	ph if not self._ph:  # update if needed self.read()
# todo: test. </s> parameters['include_entities'] = 'false'	UsersLookup if screen_name: parameters['screen_name'] = ','.join(screen_name) resp = self._RequestUrl(url, 'GET', data=parameters) try:
# todo: add corrections back in here, rather than at points of use </s> tt_approx = ut1	ut1_jd >>> t.ut1 2456675.56640625 delta_t_approx = interpolate_delta_t(self.delta_t_table, tt_approx) tt_approx = ut1 + delta_t_approx / DAY_S
# todo: should this function go to the cache class and </s> os.remove(self.converter.add_dummy_ending(path))	replace_dummy@26 file.write(self.api.download(path))
# todo: catch and report error if possible </s> return self._args	ConsoleMessage return self._text @property
# todo[jigish] is this required? </s> return api_error('error decoding json', 400)	put_library_repo try: data = json.loads(request.data) if not data or not isinstance(data, list): return api_error('Invalid data')
# todo: refactor more better(tm) </s> return self.b.get_value()	get_biases
sleep(5) # todo: replace this with zmq signalling </s> do_deploy(app)	deploy_app @argument('app') def deploy_app(app):
# todo: handle fancy-index copies by allocating a buffer and </s> def _num_batches(self):	ForcedEvenIterator def _batch_size(self): return self._base_iterator._batch_size return self._base_iterator._num_batches @property
# todo(dtroyer): remove tenant_id when we clean up the sdk refactor </s> vlan_transparent_grp.add_argument(	_add_additional_network_options help=_("VLAN ID for VLAN networks or Tunnel ID for " "GENEVE/GRE/VXLAN networks")) '--transparent-vlan', action='store_true',
# todo: other "expected" error types to catch? </s> pywikibot.output(u"category.articleslist() method is deprecated.",	articlesList level=pywikibot.DEBUG) return sorted(list(set(self.articles(recurse))))
# todo: handle data types </s> input_dict[node.inputs[1]],	TensorflowBackend dtype=tf.float32) else: time_major=True, dtype=tf.float32)
# todo: figure out how to grab the creds from environment variables in travis and then release </s> def analyze_policy(c):	analyze_policy c.run('python3 policy_sentry/bin/policy_sentry analyze policy-file --policy examples/analyze/explicit-actions.json')
pass # todo </s> def collect_incoming_data(self, data):	collect_incoming_data
# todo extend to nonbinary nodes </s> normalize=false)	raw_current_marbl self._raw_current_marbl = self.get_marbl(DIRECTIONS[FUTURE],
# todo: test that the connection gets closed if ping responses stop. </s> with self.assertraises(httperror) as cm:	test_check_origin_invalid_subdomains port = self.get_http_port() url = 'ws://localhost:%d/echo' % port yield websocket_connect(HTTPRequest(url, headers=headers), io_loop=self.io_loop)
# todo: remove input `form` in sage 9.3 </s> sage: nab._del_derived()	_del_derived sage: M = Manifold(4, 'M') sage: E = M.vector_bundle(2, 'E') self._curvature_forms.clear()
#todo now the content must be in cache! (got to handle transfer error) </s> def on_client_error(self, error_type, error):	on_client_error
# todo: remove this </s> if len(s):	_Maybe raise e_die('maybe() passed arg of invalid type %r', obj.__class__.__name__) return [s] else:
# todo: implement </s> ql.mem.write(params['lpwidecharstr'], wide_str)	hook_MultiByteToWideChar def hook_MultiByteToWideChar(ql: Qiling, address: int, params): wide_str = (params['lpMultiByteStr'] + "\x00").encode('utf-16le') return len(wide_str)
except exception:  # todo - which exceptions? </s> if organism_element.tag == ns + 'name':	_parse_organismHost def _parse_organismHost(element): append_to_annotations("organism_host", organism_element.text)
return 'ok' # todo should be a json or something </s> news_limit = len(viewer.events)	event_stream while True: sleep(0.1) for event in viewer.events[announced:news_limit]:
# todo: remove "get_" from the name </s> try:	edge_index index: Tuple of (node1_type, edge_type, node2_type) Returns: index = self.edge_types.index(edge_type) except ValueError:
# todo 目前仅在 华泰子类 中实现 </s> return self.do(self.config['entrust'])	get_entrust
# todo: how to get the api version without split & strip </s> else:	_prefix return "%s/products/%s" % (cls.site, product_id)
# @todo: pheonix </s> self.populateskilltree()	delaySearch def delaySearch(self, evt): else: self.searchTimer.Stop()
# todo properly use locales </s> }	translate_period 'year': 'ano'
# todo(hartikainen): this should get the logdir some other way than </s> if iteration % self._target_update_interval == 0:	_do_training self._sess.run(self._training_ops, feed_dict)
# :todo: implement test. </s> super(sendtransfercommandtestcase, self).setup()	SendTransferCommandTestCase class SendTransferCommandTestCase(TestCase): self.adapter = MockAdapter() def test_wireup(self):
except exception:  # todo: refactor this... </s> requestsnetworkwrapper()	test_init
x.shape[0:], # todo test reshape, dimshuffle </s> assert len([n for n in e if isinstance(n.op, join)]) == 0	test_local_join_1 val = f([[1]]) assert numpy.all(val == [[1]]) assert f.maker.fgraph.outputs[0].dtype == config.floatX s = join(1, a, a)
# todo: figure out why this causes circular import </s> def get_case_counts_closed_by_user(domain, datespan, case_types=none, owner_ids=none):	get_case_counts_closed_by_user
# todo remove compatibility shims for anki 2.1.46 and lower. </s> deck.post_import_filter()	from_json deck.deck_config_uuid = json_dict["deck_config_uuid"] deck.notes = [Note.from_json(json_note) for json_note in json_dict["notes"]] return deck
# todo(okuta): check type </s> .. seealso:: :func:`numpy.rollaxis`	rollaxis start (int): The place to which the axis is moved. Returns: return core.rollaxis(a, axis, start)
# todo remove arch dependent code </s> if isinstance(target_oprnd, x86immediateoperand) or \	_extract_branch_target@571 def _extract_branch_target(self, asm): address = None isinstance(target_oprnd, ArmImmediateOperand): address = target_oprnd.immediate
# todo(jd) add an exception in oslo.db to match foreign key </s> primary_key=true)	ResourceEntity entity_id = sqlalchemy.Column(GUID, sqlalchemy.ForeignKey('entity.id', name = sqlalchemy.Column(sqlalchemy.String, nullable=False) resources = sqlalchemy.orm.relationship(
# todo: sysex messages do not arrive here. </s> return [_api_to_name[n] for n in rtmidi.get_compiled_api()]	get_api_names
# todo(b/155804245) sanitize the names so that they're valid python names </s> instance_name=instance_name,	tfx_component_class_init base_component.BaseComponent.__init__( self,
# todo: reevaluate how to deal with different types of errors; soft </s> else:	XhrController if 'monitor' in request.params: if request.params['monitor'] == 'true': p.monitor = False if 'vlan' in request.params:
#todo: validate keys </s> robinhood().login(	test_login_badpass pytest.xfail('cannot test without valid user/passwd') bad_pass = 'PASSWORD' username=config.get('LOGIN', 'username'), password=bad_pass
# todo: implement me </s> image = self.image.clone()	_test_gradcheck depth = self.depth.clone() depth = utils.tensor_to_gradcheck_var(depth)  # to var
# todo: inplace of df with parent (reflection) </s> if len(df.columns) == 0:  # empty df	df_len_overload @overload(len)  # TODO: avoid lowering? return lambda df: 0 return lambda df: len(df._data[0])
#todo remove str() -- http://github.com/fifengine/fifengine/issues/701 </s> super(ambientsoundcomponent, self).load(db, worldid)	AmbientSoundComponent self.play_ambient(str(soundfile), loop_interval=play_every, position=self.instance.position.center) self.__init() interval = (0, self.__class__.AMBIENT_SOUND_INTERVAL +
# todo: remove </s> exc_info=ex)	process_handler_error logger.error(error_tb) else: try: ex_str = unicode(ex)
# todo: fix this workaround </s> if 'pep492' in str(path):	pytest_ignore_collect if sys.version_info < (3, 5, 0): return True
# todo: add cntk </s> assert len(ys) == 1	gradients "Partial derivates." backend = K.backend() import theano.gradient known_Ys = {k:v for k, v in zip(Ys, known_Ys)}
# todo: should we also remove the keys of the delegated roles? </s> a boolean indicating if the target metadata for 'rolename'	_targets_of_role A list of targets containing target information, conformant to 'tuf.formats.TARGETFILES_SCHEMA'. should be refreshed. <Exceptions>
# todo: parallel processing! </s> args:	get_train_labels @abstractmethod window: Box project: Project
# todo: store the random state and return it to its previous value </s> for tree_node in self._node_list[:-1]:	push_w_to_instance def push_w_to_instance(self): weight_parameter_name = "PHWEIGHT_"+str(tree_node._name) weight_parameter = self._instance.find_component(weight_parameter_name)
# todo: windows debug_mode? </s> if typ == imp.c_extension:	_get_c_extension_suffix for ext, mod, typ in imp.get_suffixes():
# todo: https://github.com/giampaolo/psutil/issues/1035 </s> self.assertequal(conn.status, psutil.conn_none)	test_unix_tcp with closing(bind_unix_socket(name, type=SOCK_STREAM)) as sock: conn = self.check_socket(sock)
# todo: @sbharadwajj implement and test </s> def _jac_mat_prod(self, module, g_inp, g_out, mat):	Conv1DDerivatives raise NotImplementedError def ea_jac_t_mat_jac_prod(self, module, g_inp, g_out, mat): raise NotImplementedError def _jac_t_mat_prod(self, module, g_inp, g_out, mat):
# todo - send file in chunks if file size > some threshold. </s> 'engine_results': engine_results	_parse_scan_result engine_results = [] scan_result = {'overall_status': overall_status, } return (True, scan_result)
# todo : now the logger.info will error if iter > 350000, so use print haha </s> cfg.respth, 'iteration_' + str(it) + '_model_final.pth'))	main@192 if it % 10000 == 0: if dist.get_rank() == 0: if it % cfg.msg_iter == 0 and not it == 0 and dist.get_rank() == 0: loss_avg = sum(loss_avg) / len(loss_avg)
</c:comp-filter>"""], "todo", items=(1, 2, 9)) </s> assert status == 200	test_add_contact status, _, _ = self.request("PUT", path, contact) assert status == 201 assert "ETag" in headers assert headers["Content-Type"] == "text/vcard; charset=utf-8"
# todo remove in v8 </s> self.logger = get_logger('compile_rest', stderr_handler)	CompileRest for plugin_info in self.get_compiler_extensions(): self.config_dependencies.append(plugin_info.name) if not site.debug: self.logger.level = 4
# todo: clean up </s> host=host,	_make_pubsubs ) return tuple( router=router, my_id=host.get_id(),
# todo: remove </s> return lookup_group_plugin(group_type).form_to_db_schema()	_form_to_db_schema
# todo: make this a hard error, instead of a silent overwrite </s> if pid > 0 and alive:	StopInstance else: acpi = False if force or not acpi: utils.KillProcess(pid)
# todo(shilpasd): need to provide support in python - novaclient </s> self.client_logger.addhandler(ch)	setup_debugging self.client_logger = logging.getLogger(client.__name__) ch = logging.StreamHandler()
# todo make atomic </s> self.__cog__.__db_push__(graph)	GraphObject def __db_pull__(self, graph): self.__cog__.__db_pull__(graph)
# todo: kkrampa, shouldn't we wait to save the checkpoint until after we've processed all the data? </s> def ils_clear_stock_data_task():	ils_clear_stock_data_task StockTransaction.objects.filter(report__domain='ilsgateway-test-1').delete() StockReport.objects.filter(domain='ilsgateway-test-1').delete()
# todo dry </s> self.handles[uuid] = int(re.match("\x1b\[khandle: 0x([a-fa-f0-9]{4})",	get_handle raise BluetoothLeError(self.con.before) else: matching_line).group(1), 16) return self.handles.get(uuid)
# todo: assert </s> self.remote.modify_repo(repo, "mirror_locally", "0", self.token)	createRepo repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "name", "testrepo0", self.token) self.remote.save_repo(repo, self.token)
# todo remove once project goes public </s> ),	admin M("CMS", c="cms", f="post"), M("Database", c="appadmin", f="index")( M("Scheduler", c="admin", f="task"), M("Error Tickets", c="admin", f="errors"),
# todo: detect if any database upgrading is needed and acquire the lock only in one place </s> exception to be thrown during a db upgrade function which will cause the old tables to be removed and recreated from	UpgradeImpossible class UpgradeImpossible(Exception):
# todo: formatting test names when non-string names are involved </s> line = "---- " + line + " "	_FormatHeader line += "-" * (end - len(line)) line = line.rstrip()
# todo: change the order of these arguments. </s> return unicode(s, encoding, defaults.decode_errors)	_to_unicode Raises a TypeError exception if the given string is already unicode. if encoding is None:
#todo handle found multiple worksheets with name </s> raise notimplementederror	worksheet
# todo: move this scopes conversion from and to string into a utils function </s> self.oauth2_data = kwargs	PreAuthorizationMixin kwargs['scopes'] = scopes kwargs['application'] = Application.objects.get(client_id=credentials['client_id'])  # TODO: this should be cached one day return super(PreAuthorizationMixin, self).get(request, *args, **kwargs) except errors.OAuth2Error as e:
# todo: remove anytime in 2016 </s> @property	FormsByApplicationFilter and context['selected'][0] != PARAM_VALUE_STATUS_ACTIVE) ) @memoized def drilldown_map(self):
# todo: add documentation </s> raise valueerror("input size must be a three dimensional tensor. got {}"	convert_points_to_homogeneous raise TypeError("Input type is not a torch.Tensor. Got {}" .format(type(points))) .format(points.shape)) return torch.cat([points, torch.ones_like(points)[..., :1]], dim=-1)
# todo: @sbharadwajj implement and test </s> raise notimplementederror	Conv1DDerivatives def _bias_jac_t_mat_prod(self, module, g_inp, g_out, mat, sum_batch=True): raise NotImplementedError def _weight_jac_t_mat_prod(self, module, g_inp, g_out, mat, sum_batch=True): raise NotImplementedError
# todo: check the data! </s> u'staffingcosts': u'88283',	test_csv u'EmployeeTotal': u'222', u'MPRail': u'1473', u'EmployeeNumOfJourneys': u'21', u'CentrallyPurchasedStationery': u'1149',
# todo: verify lldp message (e.g. org-specific authenticator tlv) </s> ofmsgs.append(	send_lldp_beacons org_tlvs=org_tlvs, system_name=lldp_beacon['system_name'], valve_of.packetout( port.number, lldp_beacon_pkt.data))
# todo: remove in 21.08 </s> except exception:	write_cache_text@86 f.write(each.strip() + '\n')
# todo(vek): need to pass context in for access to auth_token </s> def __init__(self, name, state):	InstanceInfo self.name = name assert state in power_state.valid_states(), "Bad state: %s" % state
# todo: remove optional argument used for debugging </s> ip.logger(model.csv_import_username.username, " system_importer_file_csv_cron_ip_created")	add_many2many_attributes for ip_ip in ip_list: ip, created = Ip.objects.get_or_create(ip_ip = ip_ip) system.ip.add(ip)
# todo(garyk) read base mac from configuration file (conf) </s> s = subnet['subnet']	update_subnet with context.session.begin(): subnet = self._get_subnet(context, id)
pass # todo </s> def collect_incoming_data(self, data):	collect_incoming_data
# todo(mriedem): if we can parse the volume_image_metadata field from </s> wait_time = 0	test_server_attach_detach_floating_ip floating_ip ) while wait_time < 60: cmd_output = json.loads(self.openstack(
# todo: start here </s> key = none	get_s3_key except boto.exception.S3ResponseError as e: if e.status != 404: else: key = bucket.get_key(key_name)
# todo (ismailsunni): create custom exception to catch since it </s> implementation of the metadata base class should implement this.	ImpactFunctionMetadata def get_metadata(): This is a static method. You can use it to get the metadata Nothing else needs to be overridden from the base class unless you want to modify the default behaviour.
# todo(nnorwitz): store the function_parameters. </s> self.system = system	Include def __init__(self, start, end, filename, system): Node.__init__(self, start, end) def __str__(self): fmt = '"%s"'
# todo: createpropertyconditionex with propertyconditionflags_ignorecase </s> else:	parent return UIAElementInfo(parent_elem)
# todo: find path properly </s> return	__displayPausedMessagesOnOSD def __displayPausedMessagesOnOSD(self): if self._client and self._client._player and self._client.getPlayerPaused(): self._checkRoomForSameFiles(OSDOnly=True)
#change status of todo </s> notification.subject = "test"	test_invalid_condition def test_invalid_condition(self): frappe.set_user("Administrator") notification.document_type = "ToDo" notification.send_alert_on = "New"
# todo: change this back to a factory in the instance trait some day </s> else:	do_layout def do_layout(self, *args, **kw): if self.use_draw_order and self.component is not None: super(PlotAxis, self).do_layout(*args, **kw) return
@domain_admin_required # todo: will probably want less restrictive permission </s> def locations_list(request, domain):	locations_list@21 context = { 'domain': domain,
"""@todo add progressbar for multisite. ensure the other one is hidden first.""" </s> finally:	capture_sys_output try: sys.stdout, sys.stderr = caputure_out, capture_err sys.stdout, sys.stderr = current_out, current_err
assert study_id == 0  # todo </s> return trial_id	InMemoryStorage assert study_id == 0  # TODO trial_id = len(self.trials) def set_trial_state(self, study_id, trial_id, state): assert study_id == 0  # TODO
#todo(mdietz): apply the rest of the instance_type attributes going </s> def unrescue_instance(self, context, instance_id):	unrescue_instance @exception.wrap_exception context = context.elevated() instance_ref = self.db.instance_get(context, instance_id)
""" todo: documentation </s> _local = false	StreamingCommand head Default: False @property def overrides_timeorder(self):
# todo: move this into removedirectory maybe. doing an external </s> path = filename	checkCompilesNotWithCPython def checkCompilesNotWithCPython(dirname, filename, search_mode): else: path = os.path.join(dirname, filename)
# todo: fix with stubber / before send event </s> def setup(self):	TestApiGateway super(TestApiGateway, self).setUp() self.region = 'us-west-2'
# todo is the processor the correct place to set this? </s> thisbit = 1	SevenPlaneFileProcessor thisbit = 1 if go_board.board.get(pos) == enemy_color: elif plane == 4 and go_board.go_strings[pos].liberties.size() == 2: thisbit = 1
#todo fifechan / fife 0.3.5+ compat </s> devicecaps.filldevicecaps()	get_screen_resolutions _MIN_X = 800 _MIN_Y = 600 for screenmode in devicecaps.getSupportedScreenModes(): x = screenmode.getWidth()
# through a failure or not. todo </s> if iconsumer.providedby(target):	LiteralFileNode target.registerProducer(LiteralProducer(), True) target.open(len(data)) target.unregisterProducer() target.close()
# todo: cleanup directory/basename.* files. </s> any_filter = lambda t: (	LocalLibraryProvider artist_filter = lambda t: filter( lambda a: q == a.name, t.artists) track_filter(t) or album_filter(t) or artist_filter(t) or uri_filter(t))
# todo: improve exception handling </s> warnings.warn("_is_hydrogen is deprecated.", deprecationwarning)	_is_hydrogen can_be_atomic_number: Optional[bool] = False) -> bool: r"""Returns True if the argument corresponds to hydrogen, and False case_sensitive_aliases = ['p', 'p+', 'H', 'D', 'T'] case_insensitive_aliases = ['proton', 'protium', 'deuterium',
# todo deal with errors! </s> reverse(registration))	registration@43 if req.POST["submit"] == "Delete Contact": contact.delete() elif "bulk" in req.FILES: for line in req.FILES["bulk"]:
#todo: multiple replication tasks </s> data.update(deserialized)	AdminUserResource ) user = User.objects.all()[0] form = UserChangeForm( instance=user,
#todo: in some cases we can have something like: a=os.path </s> func = {}	_parse_class def _parse_class(symbol, with_docstrings): docstring = '' name = symbol.name + '(' name += ', '.join([
# todo: add docstring </s> sysdnum, sysdden = zpk2tf(zzeros, zpoles, gain)	_c2dmatched gain = sgain/zgain
# todo: improve the complexity of this algorithm </s> payment_network = node_state.identifiers_to_paymentnetworks.get(payment_network_identifier)	handle_new_token_network events = list() token_network_state = state_change.token_network if payment_network is not None: tokens_to_networks = payment_network.tokenidentifiers_to_tokennetworks
raise exceptions.mpdnotimplemented  # todo </s> *musicpd.org, client to client section:*	subscribe@18 @protocol.commands.add('subscribe') ``subscribe {NAME}`` Subscribe to a channel. The channel is created if it does not exist
# todo: we do another lookup in cast_param, refactor to reduce number of lookups </s> def get_runner():	get_runner
# todo: add error handling for windowserror, a builtin </s> obj.collection_information['preprocess'] = str(	_StoreCollectionInformation filter_query = getattr(self.config, 'filter', None) obj.collection_information['parsers'] = [ bool(self.config.preprocess)) obj.collection_information['recursive'] = str(
# todo (1.5.1) - have another try at simplifying all this... </s> return  # already added that link	add_link linkid = '%s:%s' % (rel, href) linkset = req.chrome.setdefault('linkset', set()) link = {'href': href, 'title': title, 'type': mimetype, 'class': classname} link.update(attrs)
# todo: add logging to the process </s> table = import_function(source.uri, *args, **kwargs)	import_from_source raise ValueError('Plugin (import) "{}" not found'.format(plugin_name))
# todo: should be able to specify fields </s> os.unlink(source.uri)	import_from_source if source.delete:
# todo add locales </s> operation_logger.start()	domain_main_domain if new_main_domain not in domain_list()["domains"]: raise YunohostError("domain_name_unknown", domain=new_main_domain) try: write_to_file("/etc/yunohost/current_host", new_main_domain)
encrypting_key = bytes.fromhex(encrypting_key)  # todo: move / validate </s> shutil.copy(filepath, card.card_dir)	import_card emitter = StdoutEmitter()
# todo: would like to make this a name that a user can re-name. </s> opb = insn.get_next_byte()	notify_ana @ida_entry def notify_ana(self, insn): if opb not in wasm.opcodes.OPCODE_MAP: return 0
#only checking length. this should check functionality as well (todo) and/or import that information from the sram </s> def gen_constant(self, sig_name, v_val):	gen_constant
#todo now the content must be in cache! (got to handle transfer error) </s> print "error :", error_type, " ->", error	on_client_error
# todo: follow-up actions for downtime </s> self.checksum_address = checksum_address	StakeList if checksum_address: if not is_checksum_address(checksum_address): self.__updated = None @property
# todo ... </s> state.error("preprocessor: '" + arg + "' is not a valid macro name")	cpreprocess_evaluate_ifdef def cpreprocess_evaluate_ifdef(state, arg): arg = arg.strip() return False return arg in state.macros
# todo hack! include image digest and status, needed for the downstream notifications handler </s> httpcode = 200	do_subscription_delete if not rc: raise Exception("DB delete failed") except Exception as err: return_object = str(err)
# todo: would need to either avoid this "decorator" approach for </s> return newfunc	serve_path_via_http newfunc = make_decorator(func)(newfunc)
# todo manage tangent? </s> kf.interpolation = 'bezier'	set_interpolation elif interpolation == "CUBICSPLINE": kf.interpolation = 'BEZIER'
# todo: implement me! </s> self.coef_ = coef_	ElasticNet self.intercept_ = 0.0 self._set_coef(coef_) if coef_ is None: self.sparse_coef_ = None
# todo: take _buffercount into account </s> result = -1	hook_strncmp elif string1 > string2: result = 1 return result
# todo: perhaps next loop could be made more efficient </s> coeffs = self[:prec]	O return self
# todo if this is going into production (hopefully it isn't) then check </s> takes a superoperator to a choi matrix	super_to_choi data = q_oper.data.toarray() sqrt_shape = sqrt(data.shape[0])
# todo pseudo code: </s> def seeked(self, position):	Seeked @dbus.service.signal(dbus_interface=player_interface, signature='x')
# todo(b/160294509): use tf.compat.v1 when we stop supporting tf 1.15. </s> self._transformed_metadata = metadata_io.read_metadata(	transformed_metadata @property def transformed_metadata(self): os.path.join(self._transform_output_dir, self.TRANSFORMED_METADATA_DIR))
# todo specify custom/caching document loader in options to speed </s> constraints=ensuredataset() | ensurenone()),	AggregateMetaData doc="""specify the dataset to perform the install operation on.  If no dataset is given, an attempt is made to identify the dataset guess_native_type=Parameter( args=("--guess-native-type",),
# todo(py3.7): add required=true </s> "count", metavar="count", type=count, nargs="?", default=1,	MemoryONFIApplet "start_page", metavar="PAGE", type=address, help="erase starting at block containing page PAGE") help="erase blocks containing the next COUNT pages") p_operation.add_parser(
# todo: try to find a better way to deal with local execution </s> )	detail result_ids=result_ids, readable_plan=sy.serde._detail(worker, readable_plan), plan.name = sy.serde._detail(worker, name) plan.tags = sy.serde._detail(worker, tags)
# @todo: this part takes most of the time. need a better solution. </s> windll.gdi32.createcompatiblebitmap.restypes = hbitmap	_set_restypes windll.user32.EnumDisplayMonitors.restypes = BOOL windll.user32.GetWindowDC.restypes = HDC windll.gdi32.SelectObject.restypes = HGDIOBJ windll.gdi32.BitBlt.restypes = BOOL
# todo handle pgp_expiration_alert and pgp_expiration_notice already included in client/app/data/txt </s> setattr(node, k, v)	apply_cli_options if accepted: node = store.find(Node).one() store.commit() node = store.find(Node).one()
# todo: try to unify user-input collection (both for registers and this kind of </s> def expecting_user_input(self, value):	expecting_user_input self.settings.vi['expecting_user_input'] = value
# todo: adjust dimension order for tf2 broadcasting </s> def sparsemax(logits, axis=-1, name=none):	sparsemax @tf.function For each batch `i` and class `j` we have $$sparsemax[i, j] = max(logits[i, j] - tau(logits[i, :]), 0)$$
# todo username </s> 'id={hostname}'.format(hostname=hostname),	create_mon 'emit', 'ceph-mon',
# todo(rosmaita): simplify when lower_constraints has webob >= 1.8.1 </s> request.headers["accept"] = "application/xml"	test_content_type_from_accept_xml def test_content_type_from_accept_xml(self): result = request.best_match_content_type() self.assertEqual("application/json", result)
# todo: clean this up </s> return await func(self, *args, **kwargs)	owner_only async def wrapper(self, *args, **kwargs): orig_msg = self._get_variable('message') else: return Response("only the owner can use this command", reply=True, delete_after=20)
pass  # todo </s> self._device.close()	RivalMouse self._device.flush() def _device_close(self): self._device = None def _value_type_choice(self, value):
# todo: add more sell close test </s> context.order = none	test_buy_open context.amount = 1 subscribe(context.f1) def handle_bar(context, bar_dict): order = buy_open(context.f1, 1)
# todo : not sure if this is the best check here. </s> return super(pydyccodeprinter, self)._print_function(e)	_generate_pydy_c_printer if e in array_index_map.keys(): return array_index_map[e] def _print_Symbol(self, e): if e in array_index_map.keys():
# todo(b/175815580) update logic based resolution of the issue. </s> args:	is_api_training_job_running job_id: ID for AIP training job. project_id: Project under which the AIP Training job is running.
# todo: logs might contain sensitive data such as contents of the </s> plugin_ep.plugin_cls.inject_parser_options(parser_or_group, name)	add_plugin_args may or may not be displayed as help topics. for name, plugin_ep in plugins.iteritems():
# todo: add here additional variants for other reprog_controls </s> return device.request((feature_index << 8) + (function & 0xff), *params)	feature_request if device.online and device.features: if feature in device.features:
buildlog = f.read() #todo: this may not return all output </s> self.tests.append((buildid, passed, buildlog))	addTest
# todo: don't write them? is *much* slower on re-load (~3x) </s> if a module specifies __load__ we should only load/expose those modules	test__load__ self.update_module() with self.assertRaises(KeyError):
percentiles_to_calculate = range(0, 101, 1)  # todo: get input from user </s> else:	update_summary_stats for stat in self.summary_stats_list: if stat.startswith('p'): self.summary_stats[column][stat] = naarad.utils.normalize_float_for_display(self.calculated_stats[column][stat])
# todo: change this to be architecture independent </s> if not idc.isenabled(ea):	get_adrs_mem def get_adrs_mem(ea): return None nativeSize = get_native_size()
# todo: remove this patching when the `content` property is supported. </s> return cssutils.parsefile(resource_filename(filename))	parse_css
# todo: rethink factorization? </s> def keydown_(self, event):	keyDown_
# todo, using publicsuffix instead of this rewrite rule </s> return	load_https_rules def load_https_rules(rules_path): if not isdir(rules_path): xml_files = [join(rules_path, f) for f in listdir(rules_path)
# todo dunno maybe use the same mypy config in repository? </s> hello  = 'hello'	Modes CONFIG = 'config'
# todo(brett.cannon) implement </s> def mock_implicit_hooks():	mock_implicit_hooks
self.my_sender('text/cache-manifest', bytes(manifest, 'utf-8')) # todo: cache-control/last-modified headers </s> self.my_sender('application/json', bytes(json.dumps(safe_settings), 'utf-8')) # todo: cache-control/last-modified headers	api_view_settings del safe_settings['HTTP_Port'] except KeyError:
# todo: remove this </s> {'create c:\\temp\\hello.txt'},	file Example: .. code:: python 'c:\\temp\\hello.txt', touch=True,
# todo: check for stable/prerelease/current/beta if we support </s> if self.auth and "https" not in self.server:	http_fetch_release if self.server == "ftp.freebsd.org": self.server = "https://download.freebsd.org" self.server = "https://" + self.server elif "http" not in self.server:
# todo: remove: legacy function </s> ordering = ['sort_order', 'name', ]	Meta class Meta: verbose_name = "Buyers Guide Product Category"
# todo: boto3 call can fail with botocore.exceptions.clienterror, </s> for zone_id in zone_ids:	SpinnakerDns dns_json = get_template(template_file='dns_upsert_template.json', dns_elb=dns_elb, self.log.debug('zone_id: %s', zone_id) response = self.r53client.change_resource_record_sets(
# todo 更新用户model后替换 </s> user.id == usergroup.user_id).filter(user.delete_time == none, user.id == user_id)	Group from .user_group import UserGroup query = db.session.query(UserGroup.group_id).join( result = cls.query.filter_by(soft=True).filter(cls.id.in_(query)) groups = result.all()
# todo: remove in 21.08 </s> file_path = os.path.join(dir_path, f)	clear_cache@415 dir_path = os.path.join(mycroft.util.get_cache_directory("tts"), d) if os.path.isdir(dir_path): if os.path.isfile(file_path): os.unlink(file_path)
""" type setting - todo explain """ </s> open 311 discovery	ServiceDiscoverySerializer changeset = serializers.SerializerMethodField('get_changeset') contact = serializers.SerializerMethodField('get_contact')
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
# todo: query the locations </s> return cleaned_datetime.date()	clean_date def clean_date(messy_date_string): if messy_date_string:
# todo: test me @jmcarp </s> if not auth and not user:	can_edit :param Auth auth: Auth object to check :param User user: User object to check raise ValueError('Must pass either `auth` or `user`') if auth and user:
# todo(b/163904067): mimic defun' behavior and reset the step seed to </s> return result	CumSum result = tf.transpose( result,
common_path=prefix,  # todo: add key? </s> def autoresolve_cells(base, decisions, strategies):	autoresolve_cells
# todo: work out a way to set this based on the timespan of the data. </s> def is_datasource_for(cls, **kwargs):	is_datasource_for Determines if the file corresponds to a LYRA LightCurve `~sunpy.timeseries.TimeSeries`.
# todo fix bug for not identifying unused variables in nested exceptions see issue #4391 </s> _foo = 42	test_unused_with_prepended_underscore _ = 24 __a = 24
# todo: implement this </s> for path in paths:	prepare_addable_books def prepare_addable_books(self, paths): tdir = PersistentTemporaryDirectory('_prepare_mtp') try: f = self.filesystem_cache.resolve_mtp_id_path(path)
"""todo: not implemented""" </s> >>> ser = pd.series([1,2,3])	n @symbolic_dispatch def n(x): >>> n(ser) 3
self.__parameters.update(parameters)  # todo test </s> def get_distance_measures(self):	get_distance_measures
# :todo: implement test. </s> def test_fail_depth_string(self):	ReplayBundleRequestFilterTestCase def test_fail_depth_null(self): ``depth`` is null. ``depth`` is a string. self.skipTest('Not implemented yet.')
else:  # todo: localization </s> if key:	__init_client def __init_client(self): self.client = wolframalpha.Client(key) else:
# todo: should this move to case.rebuild? </s> return case	reset_state case.closed = False case.closed_on = None
elif key_type == unicode:  # todo: change to 'str' on python3 </s> return self.fields.values()	field_types @property
# todo implement. </s> validation_split  = parameters['validation_split']	SparkModel nb_epoch          = parameters['nb_epoch'] batch_size        = parameters['batch_size'] self._train(self.dataset_rdd, nb_epoch, batch_size, verbose, validation_split) def _train(self, rdd, nb_epoch, batch_size, verbose, validation_split):
raise glomerror(msg)  # todo: dedicated exception type for this? </s> >>> target = {'a': {'b': 'c', 'd.e': 'f', 2: 3}}	Path syntax won't work or isn't desirable. Use this to wrap ints, datetimes, and other valid keys, as well as >>> glom(target, Path('a', 2)) 3
# todo: set fileinfo to a valid object. </s> target_filepaths.append(file_path)	_get_list_of_target_paths file_path = os.path.join(directory, _file)
#@todo: remove in 0.4.10 </s> self.html = self.loadpage(p)	handleMultiPages for p in xrange(2, pages + 1):
# todo implement this method </s> def __init__(self, device, package_name, env_policy, event_policy):	droidbot class droidbot(object): The main class of droidbot initiate droidbot with configurations :param device: name of device droidbot is going to interact with
return 0.25  # todo: make configurable and/or account for as many factors as we can </s> def get_agc(self):	get_agc
#todo could use original filename to verify this </s> found = [s for s in songs if s['id'] == sid] or none	_assert_get_song if client is None: client = self.wc assert_is_not_none(found) assert_equal(len(found), 1)
annot.annotation_metadata.annotator.email = "todo"  # todo </s> json.dump(jam, f, indent=2)	convert_JAMS f = open(out_file, "w")
#todo: this can probably just be removed now? </s> return name	mash_attrib
# todo add index kwarg and move the playlist if it is specified </s> pass	PlaylistFolder class PlaylistFolder(collections.namedtuple(
# todo: where else is this a problem? </s> return getattr(_vf, name)(*args, **kwargs)	_gen_VF_wrapper def _gen_VF_wrapper(name): return wrapper
# todo: unfortunately, this feature is not yet implemented for python </s> driver.close()	test_result_cursor while cursor.next(): print(cursor["tool.name"])
#todo: read from file or generate... needs to be correlated with hive </s> return {'pop3' : {'server' : '127.0.0.1', 'port' : 2100, 'login' : 'test', 'password' : 'test'}}	get_credentials@56
# todo debug </s> "start"])	parseRuleRecursively raise ValueError("No valid value for 'time' attribute " + "in hour tag.") if (ruleHourNew.start < 0 or ruleHourNew.start > 23):
# todo: finish this. </s> except:	get_about_info def get_about_info(self): try: self.__log.exception("There was an error while acquiring the " "Google Drive client (get_about).")
# todo: what if service nesting depth is more than max python stack depth? </s> self.encapsulatedjob.prepareforpromiseregistration(jobstore)	prepareForPromiseRegistration def prepareForPromiseRegistration(self, jobStore):
# todo: query stored contract and reconstitute </s> def packed_payload(self):	packed_payload
# todo: verify lldp message (e.g. org-specific authenticator tlv) </s> lldp_beacon = port.lldp_beacon	send_lldp_beacons for port in self.dp.lldp_beacon_ports: if (port.dyn_last_lldp_beacon_time is None or org_tlvs = [ (tlv['oui'], tlv['subtype'], tlv['info']) for tlv in lldp_beacon['org_tlvs']]
# todo: separate tpu case from here </s> def process_dataloader(self, dataloader):	process_dataloader
# todo(b/163904067): mimic defun' behavior and reset the step seed to </s> comparator(tf.expand_dims(my_range, 1), tf.expand_dims(my_range, 0)),	CumSum my_range = tf.range(length) comparator = tf.less if exclusive else tf.less_equal x.dtype) result = tf.tensordot(x, mask, axes=[[axis], [0]])
#opusenc invokation (todo: ffmpeg?) </s> continue	media_convert@154 if fid not in file_list: file_list[fid] = list() if write_enabled: fbase = file_get_path('processed/' + drsfile.fname + '/' + str(file_id), write=True)
# here fixme todo </s> self.fbasketseek = self.fbasketseek[:self.fwritebasket]	_postprocess def _postprocess(self, source, cursor, context): self.fBasketBytes = self.fBasketBytes[:self.fWriteBasket] self._source = source self._context = context
pass # todo </s> return rimutabledirectorynode_	add_directory
# todo - probably delete </s> self.models.append(model)	add_model def add_model(self, model):
# todo data alignment stuff </s> def importer(idx, json, gltf):	AccessorImporter pyaccessor.data[idx[0]] = pyaccessor.sparse.data[cpt_idx] cpt_idx += 1 accessor = Accessor(idx, json, gltf) data = AccessorImporter.read(accessor)
#todo : stop properly the module </s> self.db_cursor.execute('set character_set_connection=%s;' % self.character_set)	DBMysql self.db_cursor = self.db.cursor () self.db_cursor.execute('SET NAMES %s;' % self.character_set) except _mysql_exceptions.OperationalError as exp: print "[MysqlDB] Module raise an exception : %s . Please check the arguments!" % exp
# todo(stevemar): assert returned fields </s> items = self.parse_listing(raw_output)	test_object_create def test_object_create(self): raw_output = self.openstack('object create ' + self.CONTAINER_NAME self.assert_show_fields(items, OBJECT_FIELDS)
# todo this is a workaround since exceptions are currently not correctly stacked </s> return tregex_engine(pattern, self.jsflags)	__tregex_compile
# todo: raise a specific exception </s> @param tree: tree containing the document	iter_issues @return: generator of DoorstopError, DoorstopWarning, DoorstopInfo logging.info("checking document {}...".format(self))
#todo: assert that layer inputs are always >= 0 </s> def __init__(self, *args, **kwargs):	EpsilonProxyRule Dummy class inheriting from EpsilonRule for passing along the chosen parameters from super(EpsilonProxyRule, self).__init__(*args, epsilon=epsilon,
# todo(mfedosin): think of a way to avoid this. </s> return [t_ex for t_ex in t_execs	_get_upstream_task_executions t_specs_names = [t_spec.get_name() for t_spec in self.wf_spec.find_inbound_task_specs(task_spec)] if self._is_upstream_task_execution(task_spec, t_ex)]
# @todo: support for branches </s> msg_record_modified = t("unavailability updated"),	PersonUnavailabilityModel label_list_button = T("List Periods of Unavailability"), label_delete_button = T("Delete Unavailability"), msg_record_deleted = T("Unavailability deleted"), msg_list_empty = T("No Unavailability currently registered"))
# todo: add this back in once we've merged back the refactored users code </s> parent.set_password('password')	testLinkOrphanCommCareUser def testLinkOrphanCommCareUser(self): parent.save() couch_user_1 = parent.get_profile().get_couch_user()
# todo: parameters to add to theme.ini: </s> except:	call_func return func(*args, **kwds)
# todo: change internals to use sparse throughout and delete this: </s> self.deps = deps	_set_dependencies def _set_dependencies(self, deps): self.c_tree = get_clique_tree(nodes, deps)
# todo: re-enable for hardware </s> self.one_stack_port_down(self.port_map['port_3'])	test_tunnel_path_rerouted def test_tunnel_path_rerouted(self): src_host, other_host, dst_host = self.net.hosts[:3] self.verify_tunnel_established(src_host, dst_host, other_host, packets=10)
# todo: waiting for a fix: https://developer.blender.org/t53509 </s> col = row.column(align=true)	LuxCoreMaterialHeader if ob: row = layout.row() col.operator("object.material_slot_add", icon="ZOOMIN", text="") col.operator("object.material_slot_remove", icon="ZOOMOUT", text="")
# todo: try testing this </s> r = self.http_client.get(onedrive_api.api_uri + entry_id + '/' + type)	get_link elif type == 'rw': type = 'shared_edit_link' else: type = 'embed' return self.parse_response(r, ProtocolError)['source'] except requests.exceptions.ConnectionError as e:
# todo put this in a .extra w/a subselect </s> def trac_url(self):	trac_url
if path.endswith('/index'):  # todo: remove in v8 </s> "tag_pages_are_indexes": self.site.config['tag_pages_are_indexes'],	provide_context_and_uptodate def provide_context_and_uptodate(self, tag, lang, node=None): kw = { "taglist_minimum_post_count": self.site.config['TAGLIST_MINIMUM_POSTS'], "tzinfo": self.site.tzinfo,
# todo more_itertools? </s> if path is none or kexists(takeout, path):	get_takeouts for takeout in get_files(config.takeout_path, glob='*.zip'):
# todo: remove when unicode vlens implemented </s> def iter_cb(name, *args):	iter_cb
# todo security </s> 'solr_schema': solr_schema,	admin_core_schema hue_core = Core.objects.get(name=core) hue_cores = Core.objects.all() 'hue_core': hue_core, 'hue_cores': hue_cores,
# note/todo: fixed (realpath) path should go. inner logic has to adapt to </s> jsonfilestatusesdb,):	test_AnnexDBs def test_AnnexDBs(): yield _test_AnnexDB, cls
# todo 对接 </s> super(_groupevaluator, self).__init__(eval_metric, topk, workers)	_GroupEvaluator class _GroupEvaluator(_UnionEvaluator): self.group_view = group_view self.groups = self.get_groups()
# todo: handle shortcuts </s> if diagram is self.get_current_diagram():	_on_close_diagram else: log.warn(f"No tab found for diagram {diagram}") self._clear_ui_settings(widget) self._notebook.remove_page(page_num)
arch = 'x86_64-linux-gnu' # todo figure this out </s> self.ubuntu = ubuntuplugin(name, qmlpackageoptions())	QmlPlugin class QmlPackageOptions: package = ["qmlscene", "qtdeclarative5-qtmir-plugin", "mir-graphics-drivers-desktop", "qtubuntu-desktop"] def pull(self): return self.ubuntu.pull()
# todo: use pybossa uploader! only for debugging: </s> mode= zipfile.zip_deflated	make_onefile_memzip memzip = StringIO() try: except: mode= zipfile.ZIP_STORED
# todo: can remove this skip once cupy/cupy/#2330 is merged </s> vector[-pad_width[1]:] = 10	TestPadCustomFunction def test_pad_via_func(self, xp, dtype): def _padwithtens(vector, pad_width, iaxis, kwargs): a = xp.arange(6, dtype=dtype).reshape(2, 3) a = xp.pad(a, 2, _padwithtens)
# todo: could be more precise by only looking the inames' attributes </s> has_barrier_within)	make_codegen_cache_manager else: has_barrier_within.append(False)
# todo: implement in all datasets. </s> embed_outputs = self.sess.run(fetches=self.embedded_inputs, feed_dict=input_feed)	_embed_inputs sentences, self.batch_size, return_lengths=True ) return embed_outputs, sequence_lengths
print(err) # todo show that message in an empty window </s> manual."""	on_help_transform def on_help_transform(self, *args):
# todo checks for being not outside of this repository </s> since they might still need to flush their changes into index	precommit self._batched.close()
pass # todo </s> def collect_incoming_data(self, data):	collect_incoming_data
# todo: this is untested. </s> except error:	_VerifyHelper if self._problems: try: pass raise self._problems.pop(0)
singleton=false,  # todo: re-enable </s> opts,	roster def roster(opts, whitelist=None): Returns the roster modules tag='roster', whitelist=whitelist,
# todo(b/161529310): we flatten and convert the trainable specs to </s> optimizer_state, updated_weights = reconstruction_optimizer.next(	reconstruction_reduce_fn batch_loss = client_loss( y_true=output.labels, y_pred=output.predictions) optimizer_state, _flat_tuple(local_model_weights.trainable), _flat_tuple(gradients))
# todo: remove the paired-end specific plots when all report datasets are single end </s> self.len_dist_plot_data['discarged'][self.s_name] = dict()	set_len_dist self.len_dist_plot_data['singleton'][self.s_name] = dict() self.len_dist_plot_data['collapsed'][self.s_name] = dict() self.len_dist_plot_data['all'][self.s_name] = dict() if self.__read_type == 'single':
#todo: write doc </s> bitmasks[elementname] = bitmask	gatherCollisionBitmasks bitmask = {'name': elementname, 'link': linkname, 'bitmask': element['bitmask']}
# todo: this will incorporated in the future, if needed. </s> vehicles,	I210SubNetwork >>> ) def __init__(self, net_params, initial_config=InitialConfig(),
# todo: some kind of value escape </s> (len(self.keys))	CounterMetric def inc(self, *values): if len(values) != len(self.keys): ) if values not in self.counts:
# todo: this is crazy fragile, indicates we want to refactor </s> cxn.run.side_effect = fedora_exists	returns_fedora_if_fedora_release_exists return Result(connection=cxn, exited=1)
raise notimplementederror # the below does most probably not work anymore todo </s> return "`{0}` is ahead of `{1}` by {2} commits:\n{3}".format(b,a,len(a_to_b),"\n".join(a_to_b))	detail elif not a_to_b: return "`{0}` is ahead of `{1}` by {2} commits:\n{3}".format(a,b,len(b_to_a),"\n".join(b_to_a)) else: return "The branches `{0}` and `{1}` have diverged.\n`{0}` is ahead of `{1}` by {2} commits:\n{3}\n`{1}` is ahead of `{0}` by {4} commits:\n{5}".format(a,b,len(b_to_a),"\n".join(b_to_a),len(a_to_b),"\n".join(a_to_b))
# todo: move to middleware </s> return self.web3.manager.request_blocking(	getUncleByBlock if_hash='eth_getUncleByBlockHashAndIndex', if_number='eth_getUncleByBlockNumberAndIndex', method, [block_identifier, uncle_index],
# todo write me </s> else:	requestLayer key = hasattr(config, '__hash__') and (config, getcwd()) if key in _previous_configs: config = parseConfigfile(config) if key:
# todo: use the kinetic scroller if implemented </s> edge |= _left	edge return _OUTSIDE edge = 0 elif point.x() >= rect.right() - 4: edge |= _RIGHT
# todo: better strategy here? </s> strip_kwargs = {k: strip_symbolic(v) for k,v in kwargs.items()}	singledispatch2 @wraps(dispatch_func) def wrapper(*args, **kwargs): if not args: return dispatch_func(NoArgs(), **strip_kwargs)
# todo: invalidate cache for former latestappinfo </s> else:	get_xform_by_xmlns return None
# todo move to glyf or gvar table proper </s> assert "gvar" not in font	_add_gvar from pprint import pprint pprint(locations) gvar = font["gvar"] = table__g_v_a_r() gvar.version = 1
assert oldsize == self.size, '%s: %d, %d' % (self.type, oldsize, self.size) # todo: remove </s> def read_uint(stream):	read_uint
#todo: remove convert_bare true and deprecate this in with_ </s> correct connection object from the list of connection plugins	_get_connection def _get_connection(self, variables): if not self._play_context.remote_addr: self._play_context.remote_addr = self._host.ipv4_address
# todo: write out full expression for general rhs of the form </s> def applytranspose(self, pc, x, y):	applyTranspose
# update new ratings kodi 17 - todo get ratingid for updates from embydb </s> return true	compare_all return False if not self.compare_boxsets():
# todo: handle this </s> def on_client_error(self, error_type, error):	on_client_error
# todo: shouldn't have to concretize here.  fix dag issues. </s> subparser.add_argument(	setup_parser subparser.add_argument( '-f', '--force', action='store_true', 'spec', nargs=argparse.REMAINDER, help="spec of package extension to activate.")
# todo: commented until i find a way to make it store only episodes really updated. </s> list_missing_subtitles()	update_all_movies logging.info('BAZARR All existing movie subtitles indexed from disk.')
# todo: update the tolerance after the ci moves to torch 1.10 </s> return batch	_load_datasamples def map_to_array(batch): speech, _ = sf.read(batch["file"]) ds = load_dataset("patrickvonplaten/librispeech_asr_dummy", "clean", split="validation") ds = ds.filter(lambda x: x["id"] in ids).sort("id").map(map_to_array)
# todo: support botorch v0.4.0. see: https://github.com/optuna/optuna/issues/2381 </s> requirements.append("cython")	get_install_requires if sys.version_info[:2] > (3, 8):
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
# todo add assertions </s> p.save()	test_creation def test_creation(self): self.assertEqual(Poll.objects.count(), 2) # Cause setup created one already
# todo: why does resourcefilecache return none in some cases? </s> def new_variables(self, value):	new_variables
# todo username </s> cluster=cluster,	create_mon ) if not os.path.exists(done_path): hostname=hostname, )
# todo: make the buffer longer and arrange so partial updates rather than the entire buffer can be sent to clients. </s> return signaltype(kind='mono', sample_rate=self.__demod_rate)	get_output_type
#todo: orgdatetime is not implemented yet </s> return unittest.testloader().loadtestsfromtestcase(	suite TestHeadingRecognizeDatesInHeading)
1  # todo: fill in identifier </s> @pytest.mark.parametrize('privatekey_seed', ['settlement:{}'])	test_settlement @pytest.mark.parametrize('number_of_nodes', [2]) def test_settlement(raiden_network, settle_timeout, reveal_timeout):
# todo could do smarter matching of results to trials if we have extras </s> return np.mean(all_scores)	score_benchmark all_scores = [] for env_id, scores in episode_scores.items():
# todo: handle cpu time differences, where "e" comes before "b" </s> def begin_slice(pid, tid, cat, name, ts, tts):	trace_event_generate_flame_graph@28 open_partial_slices[pid] = {} if tid not in open_partial_slices[pid]: check_thread(pid, tid) open_partial_slices[pid][tid].append({'pid': pid, 'tid': tid, 'cat': cat, 'name': name, 'ts': ts, 'tts': tts, 'children': []})
# todo: allow setting a placeholder dom element, or any widget parent </s> @js	Label this.node = document.createElement('div') flexx.get('body').appendChild(this.node); def _text_changed__js(self, name, old, txt): this.node.innerHTML = txt
# todo need to ditch this requirement </s> return url	absolute_image_url return "https://%s%s" % ( profile.handle.split("@")[1], url,
# todo this should be fixed in pandas 0.18.2 </s> assert raises(typeerror, lambda: d.a.map(d.b))	test_map assert eq(d.b.map(lk), full.b.map(lk)) assert eq(d.b.map(lk, meta=d.b), full.b.map(lk))
# todo: update tests instead? </s> if isinstance(expr.node, var):	find_partial_type_ref_fast_path Otherwise, return None. if not isinstance(expr, RefExpr): result = self.analyze_var_ref(expr.node, expr) if isinstance(result, PartialType) and result.type is not None:
# todo: raise error </s> adyen_id = notification.get("pspreference")	webhook_not_implemented def webhook_not_implemented( notification: Dict[str, Any], gateway_config: GatewayConfig success = notification.get("success", True) event = notification.get("eventCode")
# todo this is a temporary hack python-136 is the right solution for this </s> for slave in slaves:	MasterSlaveConnection raise TypeError("master must be a Connection instance") if not isinstance(slaves, list) or len(slaves) == 0: if not isinstance(slave, Connection): raise TypeError("slave %r is not an instance of Connection" %
# todo: kwargs </s> if len(df.columns) == 0:  # empty df	df_len_overload @overload(len)  # TODO: avoid lowering? return lambda df: 0 return lambda df: len(df._data[0])
#todo fifechan / fife 0.3.5+ compat </s> widget.hide = self._windows.close	SettingsDialog widget = fife_setting._optionsDialog if not hasattr(widget, '__patched__'): widget.mapEvents({ 'cancelButton': widget.hide
#ng_required="true",    # todo: validation </s> 'source_css_id': self.source_css_id,	PrimaryLocationWidget pass return get_template(self.template).render({ 'name': name, 'value': value,
# todo remove this sleep to reproduce http://tracker.ceph.com/issues/8107 </s> self.assertnotequal(pool, none)	_assert_visible r.raise_for_status() pool = self._filter_pool(r.json(), pool_name) return pool else:
# todo ... </s> state.error("preprocessor: '" + arg + "' is not a valid macro name")	cpreprocess_evaluate_ifdef def cpreprocess_evaluate_ifdef(state, arg): arg = arg.strip() return False return arg in state.macros
# todo: only devnet for now </s> return	deploy@84 click.confirm(f"Are you absolutely sure you want to destroy the contract registry at {registry_filepath}?", abort=True) os.remove(registry_filepath) else: raise click.BadArgumentUsage(message=f"Unknown action '{action}'")
## \todo: remove nodegraph fallback when all client code has been updated </s> node.enabledplug().setvalue( value )	__setEnabled @classmethod def __setEnabled( cls, node, value ) :
#todo: am i supposed to be adding the namespace like this? </s> form = form.get_form(conf["form_id"])	Command ], "additional_properties": [] source = form.source.encode("utf8", "replace") # Is this ok? parser = etree.XMLParser(remove_blank_text=True)
# todo make the colors randomly generated from rgb values </s> feature_importances = [tree.feature_importances_ for tree in best_rf.estimators_]	plot_random_forest_feature_importance print(type(trained_rf_classifier)) raise HealthcareAIError('Feature plotting only works with a scikit learn RandomForestClassifier.') standard_deviations = np.std(feature_importances, axis=0) indices = np.argsort(importances)[::-1]
# todo check error message here </s> person = document['data']	test_to_many response = self.app.post('/api/person', data=dumps(data)) assert response.status_code == 201 articles = person['relationships']['articles']['data'] assert ['1', '2'] == sorted(article['id'] for article in articles)
for obj in bpy.data.objects: #todo: this is not the best list to iterate over (there might be multiple scenes) </s> c = sum((mathutils.vector(b) for b in boundingbox), mathutils.vector())	calcBoundingBoxCenter return c / 8
for c in spinner():  # todo change to yield from, when dropping python 2.7 </s> if pos < 0:	sliding_window def sliding_window(): pos = initial pos += original elif pos >= original:
# todo: fix up internal access (again)! </s> self._y,	CheckBox self._frame.canvas.print_at( "[{}] ".format(check_char if self._value else " "), colour, attr, bg) (colour, attr, bg) = self._pick_colours("field", self._has_focus)
# todo: remove "get_" from the name </s> index = none	edge_index index = self.edge_types.index(edge_type) except ValueError: return index
# todo: do we need to skip config.add_slack variable here? </s> for idx, v_model, v_setpoint in zip(norm_constraint_blk.l1_slack_idx, model_vars,	generate_norm1_norm_constraint norm_constraint_blk.L1_slack_var = Var( norm_constraint_blk.L1_slack_idx, domain=Reals, bounds=(0, None)) setpoint_vars): norm_constraint_blk.abs_reformulation.add(
# todo implement </s> return prunable_modules	prunable_modules clf = get_classifier_module(model) if clf in prunable_modules:
# todo assert responses, swipe down </s> "mosaicid": {	test_nem_signtx_xem_as_mosaic }, "mosaics": [ "namespaceId": "nem", "name": "xem",
# todo: заменить на простой json # ld load_resources </s> def get_values_by_property(prop):	get_values_by_property
# todo: make an ascii-art bar </s> return ""	render_time def render_time(self, ctx, data): s = float(data) if s >= 1.0:
# todo extend to nonbinary nodes </s> max_ent_shape = [2 if node in self.nodes else 1	Purview Get the maximum entropy distribution over this purview (this is different from the network's uniform distribution because nodes outside for node in self.network.nodes] return np.divide(np.ones(max_ent_shape),
# todo: this test should fail for now but should succeed once </s> def test_history_correct_shape(self, net_fit):	test_history_correct_shape
# todo: use pybossa uploader! only for debugging: </s> import zlib	make_onefile_memzip def make_onefile_memzip(memfile, filename): memzip = StringIO() mode= zipfile.ZIP_DEFLATED except:
# todo should this even include public_ip if it's always none? </s> source_ip=none,	test_add_event_sessions_have_unique_ids destination_port=None, log_queue=log_queue) source_port=None, destination_ip=None,
## todo: raise an exception? </s> def read_samples(self, num_samples=rtlsdr.default_read_size):	read_samples
# xxx todo: rounding </s> instr, extra_ir = mnemo_func[instr.name.lower()](ir, instr, *args)	get_mnemo_expr raise NotImplementedError('unknown mnemo %s' % instr)
# todo: reflection padding </s> with the same number of filters on both layer	Rk with tf.variable_scope(name): weights1 = _weights("weights1",
# todo: see https://github.com/mozilla/openwpm/issues/867 for when </s> service_args=["--marionette-port", str(marionette_port)],	deploy_firefox@122 firefox_binary=fb, options=fo, ) if browser_params.extension_enabled:
# todo: arrange </s> repo = self.remote.new_repo(self.token)	createRepo @pytest.fixture self.remote.modify_repo(repo, "name", "testrepo0", self.token) self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token)
# todo: backend tensorflow </s> n_iter = 0	Model ) self._test() while n_iter < optimizers.LBFGS_options["maxiter"]: self.train_state.set_data_train(
##? value()  --- todo fix support for tuple assignment </s> class value:	Value
# todo i think this is a hack. it should be an </s> if not isinstance(c, unicode) and self._pos < c.start_pos:	usages return [] if isinstance(user_stmt, pr.ExprStmt): definitions = [v.names[-1] for v in user_stmt.get_defined_names() if unicode(v.names[-1]) ==
# :todo: implement test. </s> filter_type = getbundlescommand(mockadapter()).get_request_filter	GetBundlesRequestFilterTestCase skip_value_check = True def test_pass_happy_path(self):
#todo: the static files should handle collection of their static folder on their own </s> required_subfolders.append('locale')	verify_path Check if this path looks like a Cactus website required_subfolders = ['pages', 'static', 'templates', 'plugins'] for p in required_subfolders: if not os.path.isdir(os.path.join(self.path, p)):
# todo: implement fixture after moto is ready </s> assert true	test_emr@5
# todo: after https://github.com/multiagentlearning/playground/pull/40 </s> args = parser.parse_args()	main@89 parser.add_argument('--game_state_file', default=None, config = args.config record_pngs_dir = args.record_pngs_dir
# todo: save state right here </s> [returnfromkernel(kernel_name=new_kernel_name)])	map_schedule_onto_host_or_device new_schedule.extend( [CallKernel(kernel_name=new_kernel_name)] + current_chunk = [] else:
# todo: fix </s> def _generate_latest(url, django_model):	generate_xforms if not status: print "Sorry. Your credentials were not accepted." start_id = -1 received_count = django_model.objects.count()
else: self.assertequal(end, 1) # todo: simple exec should not wait_testpid!! </s> self.assertfalse(greps(top, "testsleep 99")) # <<<<<<<<<< difference to 5033	test_6133_run_default_services_from_single_service_saved_container top_container2 = "docker exec {testname}x ps -eo pid,ppid,user,args" top = output(top_container2.format(**locals())) self.assertTrue(greps(top, "testsleep 111")) cmd = "docker stop {testname}x" # <<<
#   todo:   2012-11-07 14:05:42 by brian mcfee <brm2132@columbia.edu> </s> half_length = (w - 1) / 2	hann_window if w % 2 == 0: w += 1 half_f      = f/2 half_window = 0.5 * (1 + numpy.cos(numpy.pi / half_length * numpy.arange(half_length)))
assert study_id == 0  # todo </s> assert study_id == 0  # todo	InMemoryStorage assert study_id == 0  # TODO return copy.deepcopy(self.trials[trial_id]) return copy.deepcopy(self.trials)
#todo - files </s> except typeerror:	get_unicode_from_response return unicode(r.content, encoding, errors='replace')
# todo: lamp texture test.. </s> tex['file'] = armutils.extract_filename(image.filepath)	parse_color assets.add(unpack_filepath) else: tex['file'] = armutils.safe_filename(tex['file']) elif node.type == 'TEX_SKY':
# todo: could use -xlinker here, if it's supported </s> return simple_version_match(start=r'intel.*?fortran.*?(?:%s).*?version' % (type,))	intel_version_match
#todo: check cost line </s> consumer.__init__(self)	StorageBuilding Building.__init__(self, x, y, owner, instance) self.inventory = self.settlement.inventory self.local_carriages.append(game.main.session.entities.units[2](self)) def select(self):
# todo: implement this </s> continue	prepare_addable_books f = self.filesystem_cache.resolve_mtp_id_path(path) except Exception as e: base = os.path.join(tdir, '%s'%f.object_id) os.mkdir(base)
# todo(py3.7): add required=true </s> def add_run_arguments(cls, parser, access):	add_run_arguments super().add_run_arguments(parser, access) def i2c_address(arg):
# time.sleep(40)  # todo: should remove after polling get. </s> data_2 = tensor(child=np.array([[567, 98], [78, 25]], dtype=np.int32))	test_tensor_abstraction_pointer@32 clients = get_clients(3) op = getattr(operator, op_str) data_3 = Tensor(child=np.array([[125, 10], [124, 28]], dtype=np.int32)) tensor_pointer_1 = data_1.send(clients[0])
# todo: align series </s> return a geoseries of differences	GeoSeries return GeoSeries([s.difference(other) for s in self], index=self.index) Operates on either a GeoSeries or a Shapely geometry if isinstance(other, GeoSeries):
# now we can kill it. todo: on a slow machine, the node might kill </s> fileutil.make_dirs(basedir)	test_baddir def test_baddir(self): argv = ["--quiet", "start", "--basedir", basedir] out,err = StringIO(), StringIO()
# todo: remove in conan 2.0 </s> from conan.tools.cmake import cmake	client_weird_lib_name conanfile = textwrap.dedent(""" import os, platform from conan.tools.layout import cmake_layout class Pkg(ConanFile):
# todo: write a unit test </s> if the alignment is concatenated on molecular data, there may be more	NodeTaxon class NodeTaxon(TreeElement): From PhyloDB: than one sequence, and these may not necessarily be from the same taxon (e.g., they might be from subspecies). Rank can be used to order these
# todo watch out because urllib.unquote will blow up on unicode text </s> msg = self.next_message()	Http if self.message_waiting:
# @todo: deployment_setting </s> opts = [opt]	org_sector_represent vals = [str(set.get(o)["abrv"]) for o in opts] multiple = True vals = str(set.get(opt)["abrv"]) multiple = False
# todo: progress +kwargs </s> return self.repo.active_branch.name	git_get_active_branch
# todo delete? we should search for valid parser </s> def __init__(self, evaluator, parser, user_context, position, call_signatures_method):	Completion self._evaluator = evaluator self._parser = parser
return # todo raise error </s> logger.debug(u'%s.seeked signaled', player_iface)	Seeked @dbus.service.signal(dbus_interface=PLAYER_IFACE, signature='x') pass
# todo - actually figure out types </s> values = sheet.row_values(i)	sample_data sheet = book.sheet_by_index(0) samples = [] types = sheet.row_types(i) values = [normalize_date(v, book.datemode) if t == xlrd.biffh.XL_CELL_DATE else v for v, t in zip(values, types)]
# @todo: deprecate </s> else:	irs_dispatch output["title"] = T("Send Dispatch Update") current.response.view = "msg/compose.html" raise HTTP(501, BADMETHOD)
# todo: move this into generic agentrootcomponent. </s> def get_queue_size(root):	get_queue_size @rlgraph_api(component=self.root_component)
# todo: update the tolerance after the ci moves to torch 1.10 </s> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")	_load_datasamples speech_samples = ds.sort("id").filter( lambda x: x["id"] in [f"1272-141231-000{i}" for i in range(num_samples)]
#todo: register authorities via annotations? </s> def list_allowed(self):	list_allowed
self.current_width = self.current_width * 2 #todo </s> def _phase_shift(i, r):	layer_phase_shift def _squeeze(x): single_batch = (int(x.get_shape()[0]) == 1)
# todo integrate param when g is a clustered graph </s> else:	plot_graph@10 raise NotImplementedError('TODO') if G.coords.shape[1] == 2: raise NotImplementedError('TODO') else:
# todo: can just leave this in superclass </s> if output is none:	_get_version def _get_version(self): version_check = "%s %s" % (VERSION_CHECK % self.name, QUERY_FORMAT) return None return output
# todo: https://github.com/pycqa/pylint/issues/3139 </s> all_sequences = []	testCrfDecode num_words = inputs.shape[0] num_tags = inputs.shape[1] for tag_indices in itertools.product( range(num_tags), repeat=sequence_lengths):
# todo allow client to pass in constructor arguments/options </s> helper method that loads modules from the given package paths.	_load_modules @type paths: dict @param patsh: a mapping of package directory paths to package names
# todo nix this when fastparquet resolves </s> pass	FastparquetIssue361 The file has zero columns and Fastparquet has a bug. https://github.com/dask/fastparquet/issues/361 -- TODO upgrade
# todo: why does a dataset have this? </s> rval *= x > self.min_x	pdf_func rval /= N.sqrt(2.0 * N.pi * (self.std ** 2.)) rval /= (self.max_x - self.min_x) return rval
# xxx todo register a failure handler that reverses the local state </s> shared_nses = db_session.query(sharedfolder)\	top_level_namespaces for account in user.imapaccounts: account_ns = account.namespace .filter(SharedFolder.user_id == user_id) for shared_ns in shared_nses:
#todo (dwalleck): this is a horrible stopgap. </s> params['headers'] = {'user-agent': 'test-client', 'x-auth-user': user,	basic_auth def basic_auth(self, user, api_key, auth_url): Provides authentication for the target API 'X-Auth-Key': api_key} self.http_obj = httplib2.Http()
# todo- re-implement this to make an iterator instead of returning a </s> returns true if there is an arc from u to v, false otherwise.	has_arc INPUT: The following forms are accepted by NetworkX:
# todo find better documentation </s> }	Sid "SubAuthorityCount": 0x1.to_bytes(length=1, byteorder="little"), "IdentifierAuthority": 0x5.to_bytes(length=6, byteorder="little"), values = b"".join(self.struct.values()) self.addr = ql.heap.mem_alloc(len(values))
pass    # todo </s> for j in self.all_jobs:	test_job_name_attribute assert hasattr(j, "name") assert_equal(self.job0.name, "test_job0")
# todo: don't write them? is *much* slower on re-load (~3x) </s> if a module specifies __load__ we should only load/expose those modules	test__load__ self.update_module() with self.assertRaises(KeyError):
# todo ... </s> assert value.content == 42	test_parse_var_decl_body assert isinstance(v.body, CStatement) value = v.body._leftexpr
pass # todo </s> self._parser.errorhandler = jyerrorhandlerwrapper(err_handler)	setErrorHandler def setErrorHandler(self, err_handler):
#todo: hardcoded to sigma as the model </s> self._me = self.mesh.getedgeinnerproduct()	Me @property def Me(self): return self._Me
final_arr = []  # todo: uncomment it </s> final_arr = []  # todo: uncomment it	Path if self.__folder_exist(ftp, item): final_arr[j] = str(final_arr[j]) + '/' + item ftp.cwd(current) else:
# todo todo </s> debug('format_uri(): ' + uri)	format_uri uri = resource['uri'] if self.config.proxy_host != "": return uri
# todo: retry instead? </s> logger.info(payload)	handle_events_resources @celery_app.task(name=RunnerCeleryTasks.EVENTS_HANDLE_RESOURCES) def handle_events_resources(payload, persist):
# todo: avoid explicit reference to cupy </s> meta = meta.astype(dtype)	meta_from_array meta = np.array(meta) if dtype and meta.dtype != dtype: except ValueError as e: if (
# todo: assert </s> self.remote.modify_repo(repo, "mirror_locally", "0", self.token)	createRepo self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token)
# todo: this can be optimized to one commit/write. </s> a storage class of cryptographic keys.	KeyStore kfrag_splitter = BytestringSplitter(Signature, (KFrag, KFRAG_LENGTH)) def __init__(self, sqlalchemy_engine=None):
# todo: this doesn't return results as often as it should, meaning we end up marking things are more suspicious than they actually are. </s> make a request to the wigle.net api	_api_request @param string api_stub The last part of the path to the API endpoint after /api/v2/ @param dictionary qs_params A dictionary containing key,value pairs to append to the querystring
# todo: add option for attentive reader </s> if isinstance(supports, str):	model "multiple" (multiple answers possible) Returns: supports = (supports,) if isinstance(questions, str):
# todo: ugly n^2 </s> if len(object_definition_tokens) > 1 and object_definition_tokens[-2] == u'with':	get_comment_form {% get_comment_form for object as comment_form %} tokens = token.split_contents() form_options = object_definition_tokens[-1] object = parse_object_definition(tagname, object_definition_tokens[:-2])
# todo: must be implemented </s> pass	should_fetch_kindlegen
# todo(aivanou): t83447589 come up with the proper fix </s> self.assertequal("foo", res.return_values[1])	test_run_function_with_return_value res = self.run_agent(Conf(entrypoint=_echo, args=("foo",), local_world_size=2)) self.assertFalse(res.is_failed())
# todo: skipping time.first() necessary? </s> 2, 3, 4, 5, 6,	test_jacobian 0, 2, 1, 3, 2, 3, 4, 5, 6, ]
#todo: fix the null space </s> self.ordertest()	test_orderXJ def test_orderXJ(self): self.name = "2D - InhomogeneousDirichlet_Inverse J"
# todo: do the computation without the 'sr' enforcement </s> - ``name`` -- (default: ``none``) name given to the riemann tensor;	riemann - \nabla_v \nabla_u w - \nabla_{[u, v]} w \right\rangle for any 1-form  `\omega`  and any vector fields `u`, `v` and `w`. if none, it is set to "Riem(g)", where "g" is the metric's name - ``latex_name`` -- (default: ``None``) LaTeX symbol to denote the
# todo: progress +kwargs </s> return self.repo.active_branch.name	git_get_active_branch
# todo, dot over only 1 dimension </s> compute the spherical harmonic function at point in spherical coordinates.	sph_harmonic Parameters ----------
# todo: dynamically add/remove adapters </s> if not super()._setupcallback(result, error=error, **kwargs):	_setupCallback def _setupCallback(self, result, error=False, **kwargs): :param result: server response return self._addAdapters(self._settings.get("adapters", 0))
# todo implement test for windows. </s> if plat.host_platform == plat.windows:	getWordCountCommand @staticmethod return None else:
# todo: clean up </s> f"lenght of pubsub_routers={pubsub_routers} should be equaled to "	_make_pubsubs def _make_pubsubs(hosts, pubsub_routers): if len(pubsub_routers) != len(hosts): f"hosts={hosts}" )
# todo: kwargs </s> return lambda df: 0	df_len_overload if len(df.columns) == 0:  # empty df
# todo: remove in 1.2 </s> argmax_dim: int = 1	to_categorical def to_categorical( ) -> torch.Tensor: Converts a tensor of probabilities to a dense label tensor
# todo: voltage dependency </s> ctrl_bool.append(val if not np.isnan(val) else false)	_controllable_to_bool def _controllable_to_bool(ctrl): ctrl_bool = [] return np.array(ctrl_bool, dtype=bool)
# todo: better naming </s> with tf.device(device_option):	TensorflowBackend ops = cls._onnx_node_to_tensorflow_op(node, input_dict) output_vals = [] output_vals = [sess.run(op) for op in ops] return namedtupledict('Outputs', node.outputs)(*output_vals)
# todo(phawkins): remove when minimum jaxlib version is 0.1.48 or newer. </s> def fft_impl(x, fft_type, fft_lengths):	fft_impl
# todo  batch size is changed </s> ftype = self.dataset.field2type[k]	__next__ self.pr += self.batch_size cur_data = cur_data.to_dict(orient='list') if ftype == 'token': cur_data[k] = torch.LongTensor(cur_data[k])
# todo: port status changes should cause us to withdraw a route. </s> (str(ip_dst), str(ip_gw)) for ip_dst, ip_gw in vlan.routes_by_ipv(ipv).items()])	_vlan_prefixes_by_ipv vlan_prefixes = [ (str(faucet_vip), str(faucet_vip.ip)) for faucet_vip in vlan.faucet_vips_by_ipv(ipv)] return vlan_prefixes
# todo generator </s> if ap.get('type', none) != 'dataset' or ap['path'] == ds.path]	Drop ds = Dataset(ds_path) handle_dirty_dataset(ds, mode=if_dirty) if not content: continue
# todo: this regex could change based on project req format </s> tree._document_cache[document.prefix] = document  # pylint: disable=w0212	new_document logging.info("imported: {}".format(document))
# todo(lipu): fix this to show front page pictures </s> return len(channels), channels	_createChannels for hit in hits: channel = Channel(*hit)
# todo(parallel): to support nn.dataparallel, this must be changed, as it not a tensor </s> x = x + self.rgb_mean  # back to 0...255	BicubicDownsamplingEnc self.rgb_mean = torch.tensor( [0.4488, 0.4371, 0.4040], dtype=torch.float32).reshape(3, 1, 1).mul(255.).to(pe.DEVICE) x = x.clamp(0, 255.).round().type(torch.uint8) x = resize_bicubic_batch(x, 0.5).to(pe.DEVICE)
# todo: add this test </s> request.setmethod( 'get' )	test_codeDisclosure response = httpResponse(200, body , headers, url, url) request = fuzzableRequest() self.plugin.grep(request, response) self.assertTrue( len(kb.kb.getData('codeDisclosure', 'codeDisclosure')) == 1 )
# todo: remove this log once we find out what's causing oom </s> notification.success = false	attach_webhook project.save() return True notification.reason = AttachWebhookNotification.NO_PERMISSIONS else:
# :todo: implement test. </s> {	GetBundlesRequestFilterTestCase Request is empty. self.assertFilterErrors( 'transaction': [f.FilterMapper.CODE_MISSING_KEY], },
# todo: parse the field contents </s> ae(r'xxx \y a', {'text':'xxx', 'yomi':'a'})	test_xe ae = lambda x, y: self.assertEqual(parse_xe(x, None), y) ae(r'"some name"', {'text':'some name'})
# todo: write tests </s> "when there is an otherwise-unspecified validity error that prevents parsing."	MeiValidityError pass
# todo: rename this to event_data.parser_chain or equivalent. </s> def timezone(self):	timezone @property
# todo: don't create dsdp directory. add these files </s> clonemodel=true, tee=false, keepfiles=false, solver_options=none,	kaug logger='pyomo.contrib.sensitivity_toolbox', version='TBD') streamSoln=False): m = sensitivity_calculation('k_aug', instance, paramSubList, perturbList,
# todo have no idea if is cdecl or stdcall </s> print(traceback.format_exc())	hook_memcpy self.ql.mem.write(params['dest'], data) except Exception as e: print(e) return params['dest']
# todo also handle method and request </s> class dockerproxyclientfactory(proxy.proxyclientfactory):	DockerProxyClientFactory
raise notimplementederror  # todo(mattjj) </s> def defreducer(prim, collective_prim):	defreducer
# todo: actually parse this </s> return super(_valuemixin, self).changes(other, target)	_ValueMixin def changes(self, other, target): if self.value != other.value: def _data(self): ret = super(_ValueMixin, self)._data()
# todo: refactor common tests for all models, e.g. shape checking </s> trans_h = transh(triples_factory=self.factory)	test_trans_h def test_trans_h(self):
# todo: write </s> return list(res.values())[0]	first_key@44 def first_key(command, res): if len(res.keys()) != 1:
# todo confirm we want floor division here </s> coeffrange : tuple of floats, optional	mnl_estimate numalts : int The number of alternatives. Limits of (min, max) to which coefficients are clipped. weights : ndarray, optional
# todo(john-wood-w) allow until plugin validation is added. </s> excep.invalidobject,	test_should_raise_with_mixed_case_wrong_payload_content_type def test_should_raise_with_mixed_case_wrong_payload_content_type(self): self.secret_req['payload_content_type'] = 'TeXT/PlaneS' self.validator.validate, self.secret_req,
# todo: avoid dummy and generate func here when inlining is possible </s> def df_len_overload(df):	df_len_overload if len(df.columns) == 0:  # empty df return lambda df: 0
# todo: arrange </s> self.remote.modify_repo(repo, "mirror_locally", "0", self.token)	createRepo repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "name", "testrepo0", self.token) self.remote.save_repo(repo, self.token)
# todo: plumb gravitylayout.__init__'s arguments into the config file </s> return self.commands.call(command, wm)	QuickTile @dbus.service.method(dbus_interface='com.ssokolow.QuickTile', in_signature='s', out_signature='b')
#     todo </s> cache_path = self.cache.get_path_or_dummy()	chmod os.chmod(cache_path, mode) return
# todo(py3.7): add required=true </s> def test_build(self):	test_build @synthesis_test
# todo(brett.cannon) implement </s> def mock_implicit_hooks():	mock_implicit_hooks
pass # todo </s> if tool_id in all_tools:	_get_tool def _get_tool(self, tool_id): return all_tools[tool_id] else:
# todo(buggay): implement num_vms_per_host functionality </s> return response['ipaddress']	GetIPAddress '--name', self.name ] + self.resource_group.args)
# todo: why so inaccurate? </s> test_op([(5,2,11,8)], lambda x: torch.nn.functional.avg_pool2d(x, (2,2)), tensor.avg_pool2d)	test_avgpool2x2
return none # todo </s> 'playlist: 0',	_status 'random: 0', 'single: 0', 'playlistlength: 0', 'xfade: 0',
# todo: currently the output code directly accesses the format string </s> if self.data_type != event_object.data_type:	GetSources raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format( event_object.data_type))
# todo: remove when fixed in upstream gtk+ </s> line = start.get_line()	do_activate chunk_index = self.linediffer.locate_chunk(self.from_pane, line)[0] if chunk_index is None:
g.configure_new(config) # todo: test for emitted warning </s> g = neat.defaultgenome(gid)	TestCreateNew config.initial_connection = 'partial' config.connection_fraction = 0.5 self.assertEqual(gid, g.key) print("\nThis should output a warning:", file=sys.stderr)
# todo support multiple backends </s> :param name: name of the new playlist	StoredPlaylistsController self.backends[0].stored_playlists.playlists = playlists def create(self, name): :type name: string :rtype: :class:`mopidy.models.Playlist`
# todo: revert this. </s> representation. cylindrical representation replaces (x, y) with (rho, psi)	Heliocentric contains the z-axis and the solar rotation axis, pointing towards the Sun's north pole. where rho is the impact parameter and psi is the position angle in degrees. Parameters
#@todo: integrate this function with the one below - the pipeline() method only works with this function </s> default: int64; specify keys (or indices) for which other type is needed in types dictionary; e.g. types={'scores':tf.float32}	create_placeholders def create_placeholders(corpus, types={}): All dimensions are None Assumes corpus is dict.
# todo refactor this function </s> ret_val['is_highest'] = (	get_version_compare_data base_version_comparable = parse_version_failsafe( base_version.verbose_name) base_version_comparable >= highest_version_comparable) else:
# todo: find a better image here </s> self.instance.session.ingame_gui.hide_menu()	destruct_building def destruct_building(self):
''' todo: change conditional to return on non-http responses </s> parser.add_argument('-d', '--daemon', help='location of ipfs daemon (default 127.0.0.1:5001)', default=ip+':'+port, dest='daemon_address')	checkArgs Check to ensure valid arguments were passed to the indexer and provide guidance on the available options if not parser.add_argument('-o', '--outfile', help='Path of newly created CDXJ. Shows progress by default unless suppressed with -q') parser.add_argument('-p', '--progress', help='Show progress of processing WARC file.', action='store_true')
# todo: should this raise ioerror? </s> data += bytearray.fromhex(line.split('#')[0])	parse_hexdump for line in hexdump.splitlines():
# todo: checking that hour/minute/second are not </s> year += self._century	parserinfo return self.TZOFFSET.get(name) def convertyear(self, year, century_specified=False): if abs(year - self._year) >= 50: if year < self._year:
# todo: give a vanilla example </s> \\right )	feca@26 \\sum_n min \\left ( \\frac{\\sum_n y}{\\sum_{n,t} y}, Attributes ----------
# :todo: implement test. </s> verifies that the command is wired up correctly.	SendTransferCommandTestCase super(SendTransferCommandTestCase, self).setUp() self.adapter = MockAdapter() self.assertIsInstance( Iota(self.adapter).sendTransfer,
# env.reset()  # todo: remove once traffic bug is fixed </s> cam['capture_height'] += round(np.random.random() * 0.01 * cam['capture_height'])	randomize_cameras elif i == 1: cam['relative_position'][i] += (np.random.random() * 100 - 50) cam['capture_width'] += round(np.random.random() * 0.01 * cam['capture_width'])
#            print "monitor[0] in if self.todo.isddc is",  monitor[0] </s> return -1	numCompare if first > second: return 1 return 0
# todo: require rewrite </s> return x[0][1] == value	value_function
# todo: unfortunately, this feature is not yet implemented for python </s> search_term = "hammer"	test_result_cursor def test_result_cursor(self): driver = GraphDatabase.driver("bolt://localhost") cursor = session.run("MATCH (tool:Tool) WHERE tool.name CONTAINS {term} " "RETURN tool.name", {"term": search_term})
# todo: provide more informative errors </s> federated_only = true # const for now	make_alice_control label = bytes.fromhex(request.args['label']) m, n = int(request.args['m']), int(request.args['n']) bob = Bob.from_public_keys({DecryptingPower: bob_pubkey, SigningPower: None},
# todo: this is lazy, we should only reconfigure the drone(s) who are actually </s> private_key, publickey = self._get_zmq_keys(name)	_handle_command_genkeys self.config_commands.send(Messages.OK + ' ' + json.dumps({'public_key': publickey, 'private_key': private_key}))
# todo(harlowja): move this code into </s> return before_comment	_read_hostname (before_comment, _comment) = chop_comment(line, "#") before_comment = before_comment.strip() return default
rec_dict._proxy._handle.close() #todo - better solution </s> iterator = seqio.parse(handle, "fasta")	test_duplicates_to_dict def test_duplicates_to_dict(self): self.assertRaises(ValueError, SeqIO.to_dict, iterator) handle.close()
# todo(cutwater): replace `.decode('utf-8')` call with subprocess </s> daft punk songs.	get_version_name Returns the version name. Minor releases for 3.0.0 will be named after
# todo: logger always requires extras['cls'] : can we fix this? </s> return result	_set_dimension_names f"Unsupported topology_dimension: {self.topology_dimension} ." )
# todo(jeremydw): better headers. </s> threads = []	GoogleCloudStorageDeployment bucket.configure_versioning(False) bucket.configure_website(main_page_suffix='index.html', error_key='404.html') for path, contents in paths_to_contents.iteritems(): thread = threading.Thread(target=self._upload, args=(bucket, path, contents))
# todo: send alert </s> for j in eval(prefix_list):	chk_prefix_in_whitelist@129 white_prefix_list = Whitelist.objects.all() flag = False for i in white_prefix_list: if i.phonenumber_prefix == j:
# todo: use validation set if not-none </s> be crucial to keeping memory use under control.	forget_task for small data sets but deleting such intermediate results can
end_date=none,  # todo: expire me? </s> def draft_before_register_page(auth, node, draft, *args, **kwargs):	draft_before_register_page @autoload_draft @must_have_permission(ADMIN) ret = serialize_node(node, auth, primary=True) ret['draft'] = serialize_draft_registration(draft, auth)
# todo: check that the performance measure is within some range </s> def test_figure_eight(self):	TestBaselines def test_bottleneck2(self): Tests flow/benchmark/baselines/bottleneck2.py Tests flow/benchmark/baselines/figureeight{0,1,2}.py figure_eight_baseline(num_runs=1, sumo_binary="sumo")
loop=asyncio.new_event_loop(),  # todo: this doesn't work without this </s> self.success = none	TestRTMClient @pytest.mark.skipif(condition=is_not_specified(), reason="this is just for reference") @async_test self.text = "This message was sent to verify issue #631" self.rtm_client = RTMClient(token=self.bot_token, run_async=True)
# todo: with only non-mandatory model attributes, it is not possible to get an invalid form </s> data_dict = {	test_system_exporter_spreadsheet_xls_config_post_redirect@80 def test_system_exporter_spreadsheet_xls_config_post_redirect(self): 'spread_xls_system_id': 'on', }
# todo should this reset the pts and such? </s> return update	UpdateState self._updates_available.clear() if isinstance(update, Exception): def get_polling(self): return self._polling
# todo(tdurakov): remove dict to object conversion once rpc api version </s> compute_host_bdms = []	_get_host_volume_bdms instances = objects.InstanceList.get_by_host(context, self.host, use_slave=use_slave)
# todo: maybe_decay_array </s> given a word, returns pattern.ere if has an extglobpart, or pattern.fnmatch	EvalWordToPattern otherwise. NOTE: Have ot handle nested extglob like: [[ foo == ${empty:-@(foo|bar) ]]
# todo - remove by nov 1 2017 if soft assert is never sent </s> return render_to_string(self.template, context)	StaticField 'field_label': self.field_label, 'field_value': self.field_value,
gray_img = color.rgb2gray(state)  # todo: check image conversion doesn't cause problems </s> gray_img = color.rgb2gray(state)  # todo: check image conversion doesn't cause problems	_state_to_tensor@8 downsized_img = transform.resize(gray_img, (84, 84), mode='constant')  # TODO: Check resizing doesn't cause problems return torch.from_numpy(downsized_img).float()  # Return 2D image tensor
# todo: clean up this event print out. we probably want something </s> print out the documentation!	print_docs arg = self.opts.get('fun', None) docs = super(Runner, self).get_docs(arg)
# todo: support grouping and stacking at the same time </s> kw['yscale'] = yscale	Bar kw['group'] = group kw['agg'] = agg kw['xgrid'] = xgrid kw['ygrid'] = ygrid
#todo(qos): support the fields parameter </s> context, network['id'])	_extend_network_policy_data context = kwargs['context'] network = kwargs['network'] network['qos_policy_id'] = policy.id if policy else None
# todo(b/195364460): work around slow xla/cpu implementation of float16 matmul </s> else:	_div_transpose_rule return [ad_util.Zero(x.aval), None]
pass  # todo: implement </s> def predict_final(self, x):	predict_final
# todo: send message with login link. </s> if (request.path.startswith('/api/') and	api_call request.accept_mimetypes.accept_json and not request.accept_mimetypes.accept_html):
# no todo item selected </s> return true	selectable
# todo: this should take a vector </s> return self._me	Me Edge inner product matrix if getattr(self, '_Me', None) is None:
# todo: check rackspace file existence </s> pass	download_name def download_name(self, app, ty):
# todo: make sure we have a test case for the above point. </s> break	_get_simple result = _get_value(item, name) if result is _NOT_FOUND: return result
# todo: add multiple $root support </s> details.append("%s downgrade" % self.downgrades)	PackageCounters if self.upgrades > 1: details[-1] += "s" if self.downgrades > 1: details[-1] += "s"
# todo(b/138845899): consider use span instead of id. </s> def resolve_exec_properties(	Driver latest_model = max(previous_models, key=lambda artifact: artifact.id) return latest_model.uri self, exec_properties: Dict[Text, Any],
# todo: make sure package names can't be changed to look like package ids? </s> if isinstance(entity, str):	_make_unicode return unicode(entity) elif isinstance(entity, list):
# todo - needs tests </s> "plan_choices": settings.plan_choices,  # possibly nuke	payments_settings@8 def payments_settings(request): return { "PAYMENT_PLANS": settings.PAYMENTS_PLANS  # possibly nuke
# todo: log non-bool return values? </s> returns :class:`true` if call is successful, otherwise :class:`false`.	MixerController def set_volume(self, volume): The volume is defined as an integer in range [0..100]. validation.check_integer(volume, min=0, max=100) if self._mixer is None:
# todo(stevemar): assert returned fields </s> self.assert_show_fields(items, object_fields)	test_object_create raw_output = self.openstack('object create ' + self.CONTAINER_NAME + ' ' + self.OBJECT_NAME)
#todo: actually check for change </s> if s.startswith("0x"):	str_to_bool return True try: s = s[2:] r = 16
# todo (cyyber): move to state cache, instead of writing directly </s> if not found:	set_addresses_state if state_object.state_code == state_code: found = True logger.warning('State Code not found %s', state_code) return None
# todo: this is untested. </s> self._problems.append(e)	_VerifyHelper try: result = callback(connection, cert, error_number, error_depth, ok) return 0 else:
# todo check the trigger_id content </s> logout(request)	logout_view def logout_view(request): return redirect('base')
raise exception #todo </s> def add_table(self, name, columns, key):	add_table
# todo: catch unquoting errors, range of chars, charset </s> self.setmessage('header-location', rs.redirect_without_location)	status303 def status303(self):        # See Other
# todo add tests </s> (x1 + (i + 1) * step_size[0], y1 + (i + 1) * step_size[1])	interpolate_point_pair vec = np.float32([x2 - x1, y2 - y1]) step_size = vec / (1 + nb_steps) for i in sm.xrange(nb_steps)]
#  todo: test </s> "command burst",	handler@18 def handler(fit, ship, context): fit.modules.filteredItemIncrease(lambda mod: mod.item.group.name in ), "maxGroupOnline",
# todo: return errors in a universal way </s> parser = parser()	compile_code return (str) - The asm output lexer = Lexer(token_kinds.symbol_kinds, token_kinds.keyword_kinds) ast_root = parser.parse(token_list) code_store = CodeStore()
# todo: then we can pull the descriptor out of the tile_spec </s> return dateutil.parser.parse(time)	_parse_time if isinstance(time, compat.string_types):
"""todo: doesn't remove unused nodes/renumber elements""" </s> elif etype == 'ctetra' and len(elements):	write_tecplot is_hexas = True nnodes_per_element = 8 is_tets = True nnodes_per_element = 4
# todo (#567): bucket the node as suspicious </s> certificate_filepath=certificate_filepath	check_availability requesting_ursula_metadata = this_node.network_middleware.client.node_information( host=initiator_address, ) except NodeSeemsToBeDown:
# todo: find a value for % width </s> if isinstance(child, boxes.inlinereplacedbox):	inline_preferred_width widest_line = 0 current_line = 0 current_line += replaced_preferred_width(child) else:
# todo: implement this </s> help_parser.print_help()	_print_help_for_runner help_parser = ArgumentParser(usage=SUPPRESS, add_help=False) _add_runner_args(help_parser, opt_names,
# todo fix. </s> if not cmp_result:	test_cmova ctx_init = self.__init_context() x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.__save_failing_context(ctx_init) self.assertTrue(cmp_result, self.__print_contexts(ctx_init, x86_ctx_out, reil_ctx_out))
# todo: delete _check_on_hpc_hooks in v1.8 </s> rank_zero_deprecation(	_check_dl_idx_in_on_train_batch_hooks def _check_dl_idx_in_on_train_batch_hooks(trainer: "pl.Trainer", model: "pl.LightningModule") -> None: for hook in ("on_train_batch_start", "on_train_batch_end"): f"Base `LightningModule.{hook}` hook signature has changed in v1.5." " The `dataloader_idx` argument will be removed in v1.7."
# todo: support dynamic conversion </s> return output	roll_fixed_apply_seq ind += 1 for i in range(max(N - offset, 0), N):
# todo(b/172668718) enable tests after b/172668718 is resolved. </s> "requirements.txt"),	auto_one_device_strategy entry_point=os.path.join(self.test_data_path, "mnist_example_using_fit.ipynb"), job_labels={ "job": "auto_one_device_strategy",
# todo: consider passing search_dirs in the constructor. </s> encoding = defaults.string_encoding	_to_unicode def _to_unicode(s, encoding=None): Raises a TypeError exception if the given string is already unicode. return unicode(s, encoding, defaults.DECODE_ERRORS)
#todo: maybe i should still update specials? </s> def run(self, force=false):	ShowUpdater class ShowUpdater(): def __init__(self): updateTime = datetime.time(hour=3) logger.log(u"Checking update interval", logger.DEBUG)
# todo: translate </s> def title(self, lang):	title
# todo: remove </s> return lookup_group_plugin(group_type).form_to_db_schema()	_form_to_db_schema
# todo: handle escape (0x1b) </s> calculated_crc = crc16.crc16xmodem(''.join([chr(item) for item in message[:-2]]))	valid_crc supplied_crc = message[-2] * 256 + message[-1]
).consume()  # todo see issue 170 </s> elbs.extend(page['loadbalancerdescriptions'])	get_loadbalancer_data for page in paginator.paginate():
exported.js_support = current.js_support  # todo: check articles have js_support </s> sys.stdout.write("%s[%s%s] %i/%i\r" % (prefix, "#" * x, "." * (size - x), _i, count))	_show def _show(_i): sys.stdout.flush()
# @todo: is this always true? </s> g.attrs["refine_by"] = pf.refine_by	write_to_gdf@123 if data_comment is not None: g.attrs["data_comment"] = data_comment g.attrs["dimensionality"] = pf.dimensionality g.attrs["domain_dimensions"] = pf.domain_dimensions
# todo this is also doing a mongo query for each owner </s> tag.objects.filter(system=true).values('name', 'pk')})	build_toku_django_lookup_table_cache Tag.objects.filter(system=False).values('name', 'pk')}) lookups.update( lookups.update({format_lookup_key(x['_id'], ContentType.objects.get_for_model(CitationStyle).pk): x['pk'] for x in CitationStyle.objects.all().values('_id', 'pk')})
# todo: when timers are introduced, just make this wait </s> def get_pool_state(self):	get_pool_state
# todo it might still leave unused database entries referring to the community id </s> def database_version(self):	database_version return self._database_version
# todo make this configurable </s> self._leave(nick, date, message, "kick", extra_tags=[])	kick
# todo: grab values from blender lamps </s> 'type': 'directional',	KhrLights 'directional': { 'color': (light.color * light.energy)[:], } elif light.type == 'POINT':
# todo: why do we have an __init__? we should be able to set up the class inside the </s> else:	Marks return "{0}:{1}".format(fname, rowcol_encoded)
#todo: add support for root position v7 chords with missing fifth </s> pitcha = realizerscale.converttopitch(pitcha)	resolveTritone ResolutionException: Pitches do not form a tritone. if not (resolveTo == 'major' or resolveTo == 'minor'): pitchB = realizerScale.convertToPitch(pitchB) dimFifth = interval.stringToInterval('d5')
# todo: tighten this filter to match on the string of the error </s> using an object which was serialized using a syft-based wrapper."""	test_uuid_wrapper_deserialization uid = uuid.UUID(int=333779996850170035686993356951732753684) _uid = UID(value=uid)
# todo: should be able to pass in an optional function for </s> tu_uri = 'tu%s' % self.tu_counter	Writer ] if clade.name: statements += [ (Uri(tu_uri), qUri('rdf:type'), qUri('cdao:TU')),
# todo iterate with schematizer as to exact interface </s> the clientlib will encapsulate this in envelope."""	_publish_to_kafka print "Publishing to kafka {0} {1}".format(topic, message)
# todo docstring </s> for unit in units:	discover_node_configuration d = self._gear_client.list() def applications_from_units(units): applications.append(Application(name=unit.name)) return applications
# todo assert cls.__tablename__ == '' </s> s = model.galaxysession()	galaxy_session @pytest.fixture yield from dbcleanup_wrapper(session, s)
# todo: remove in django 1.10 </s> def test_db(self):	PrefixTests prefix[0] = 'x' Category.objects.cache().count() with self.assertNumQueries(1): list(Category.objects.cache())
# todo: abort cleanly here, as the resolution has been </s> ireq = install_req_from_line(	make_install_req_from_dist "{}=={}".format( canonicalize_name(dist.project_name),
# todo: add cn to domains? </s> :param str success_msg: message to show on successful recovery	_recovery_routine_with_msg self.installer.recovery_routine() reporter = zope.component.getUtility(interfaces.IReporter)
# todo(john-wood-w) allow until plugin validation is added. </s> self.assertraises(	test_should_raise_with_mixed_case_wrong_payload_content_type def test_should_raise_with_mixed_case_wrong_payload_content_type(self): excep.InvalidObject, self.validator.validate,
# todo(bichen): move this color dict to configuration file </s> cap.release()	video_demo@111 print (time_str) if cv2.waitKey(1) & 0xFF == ord('q'): cv2.destroyAllWindows()
# todo: use different flag than .reentrant </s> save our state so we can reentrantly start.	_suspend class _attrholder: pass
# todo: raise an exception </s> extension.info.enabled = false	__uninit_extension def __uninit_extension(self, extension): del self._extension_instances[extension.id]
# todo: this code need to be revised </s> break	handle_missing_data_table_entry tup_path, guid, repository_tool = repository_tools_tup if repository_tool.params_with_missing_data_table_entry: if missing_data_table_entry: sample_tool_data_table_conf = hg_util.get_config_from_disk('tool_data_table_conf.xml.sample',
# todo: these two probably shouldn't reach back to main.. </s> if position >= 0:	scroll_to_current def scroll_to_current(self): path = (position,) self.scroll_to_cell(path)
# todo: cmake imported target shouldn't be namespaced (waiting https://github.com/conan-io/conan/issues/7615 to be implemented) </s> def _source_subfolder(self):	_source_subfolder @property
# todo(dustin): we used to take non when the vcs was unknown. now we'll only </s> except nameerror:	get_root def get_root(): try: return os.path.dirname(os.path.abspath(sys.argv[0]))
# todo: update the tolerance after the ci moves to torch 1.10 </s> ids = [f"1272-141231-000{i}" for i in range(num_samples)]	_load_datasamples def _load_datasamples(self, num_samples): def map_to_array(batch): speech, _ = sf.read(batch["file"])
# xxx todo </s> return winappdbg.hexinput.integer(token)	input_integer
commonname = "goagent xx-net - goagent" #todo: here should be goagent - xx-net </s> if i[0] == "cn":	cn def cn(self): c = None c = i[1] return c
#todo use backup phone </s> send(to=phone,	render_next_step@293 body=ugettext('Your authorization token is %s' % generated_token)) elif method == 'sms': body=ugettext('Your authorization token is %s' % generated_token)) return response
# todo(yamahata): ephemeraln where n > 0 </s> 'args': {'token': output['token'],	get_vnc_console instance_id) rpc.call(context, '%s' % FLAGS.vncproxy_topic, 'host': output['host'], 'port': output['port']}})
# todo: make it possible to plot a plated variable using _subplots function. </s> plotter of a node as a 1-dimensional function	FunctionPlotter class FunctionPlotter(Plotter): See also --------
# todo(emfree): remove after status overhaul. </s> account.gender = response.get('gender')	OutlookAuthHandler account.o_id_token = response.get('user_id') account.o_id = response.get('id') account.link = response.get('link') account.locale = response.get('locale')
# todo(ib-steffen): allow custom ca bundles </s> where j.project_id = %s	Tests AND tr.job_id IN ( SELECT j.id AND j.build_id = ( SELECT b.id
# todo raise exception </s> for journal in filter(lambda x: x.endswith(".journal"), os.listdir(os.path.join(journals_path, category))):	journals_list result = {} for category in os.listdir(JOURNALS_PATH): file_name = journal journal = journal[:-len(".journal")]
# todo. create readme file in <output_dir> </s> target_purity = self.purity	SpeakerChangeDetection class SpeakerChangeDetection(SpeechActivityDetection): def validate_epoch(self, epoch, protocol_name, subset='development', model = self.load_model(epoch).to(self.device) model.eval()
# todo -- make sure more stringent and parse each kext in-memory so we only allow whitelist from .text </s> x86_64_flag = obj.object("int", offset = x86_64_flag_addr, vm = addr_space)	is_64bit_capable @returns True if 64-bit capable. x86_64_flag_addr = addr_space.profile.get_symbol("_x86_64_flag") ret = x86_64_flag == 1 else:
# todo: remove this once sphinx is gone. </s> site = site.objects.get_current()	plugin @cache_page(60 * 60 * 168)  # 1 week. return jingo.render(request, 'search/plugin.html', {'site': site, 'locale': request.locale},
# todo: use zfs plugin </s> aclmode = stdout.decode().strip()	SystemDatasetService@68 'zfs', 'get', '-H', '-o', 'value', 'aclmode', dataset, ], stdout=subprocess.PIPE, stderr=subprocess.PIPE) if aclmode and aclmode.lower() == 'restricted': await run('zfs', 'set', 'aclmode=passthrough', basename, check=False)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	GetTrytesResponseFilter def test_pass_transactions(self): self.skipTest('Not implemented yet.')
recording_name = none  # todo </s> except keyerror:	is_pupil_mobile_recording return info_csv["Capture Software"] == "Pupil Mobile" and "Data Format Version" not in info_csv
# todo: support default period argument </s> for i in numba.parfor.internal_prange(len(a)):	_column_fillna_impl s = B[i] if np.isnan(s):
# todo: wobble </s> vector = _eclipj2000.dot(self.position.au)	ecliptic_position return Distance(vector)
# todo: optimize db call </s> summary = workflowsummary( trans, history )	summarize Formerly call get_job_dict in workflow web controller.
# todo test that these are the right attributes </s> new_reaction['metabolites'] = mets	save_json_model for reaction in model.reactions: new_reaction = {} for key in ['name', 'reversibility', 'subsystem', 'upper_bound', 'lower_bound', 'objective_coefficient', 'notes']:
# todo implement. </s> self._train(self.dataset_rdd, nb_epoch, batch_size, verbose, validation_split)	SparkModel batch_size        = parameters['batch_size'] verbose           = parameters['verbose'] def _train(self, rdd, nb_epoch, batch_size, verbose, validation_split): yaml = self.master_model.to_yaml()
# todo: this is untested. </s> if self._problems:	_VerifyHelper self.callback = _ffi.callback( "int (*)(int, X509_STORE_CTX *)", wrapper) try: _raise_current_error()
# todo(vek): need to pass context in for access to auth_token </s> def __init__(self, name, state):	InstanceInfo self.name = name assert state in power_state.valid_states(), "Bad state: %s" % state
# todo check output </s> f.flush()	test_obj f.write(test_content)
# test for uwsgi -- todo save this somewhere so we only have to do it once. </s> from glob import glob	pack_scripts from subprocess import call cmd = "java -jar scripts/yuicompressor.jar --type js static/scripts/%(fname)s -o static/scripts/packed/%(fname)s"
# todo add read unlock </s> data_files_size = 0	init_data_files def init_data_files(self): state_file = None data_dir = settings.data_folder_path
# todo fetch a real object </s> return false	looks_like_prod_code def looks_like_prod_code(code): try: except: return True
# todo compare file contents? </s> return updater(	_new_updater self.client_dir.name, "https://example.com/metadata/",
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
# todo: create a log message this error? </s> model = context["model"]	licence_list license_register = model.Package.get_license_register() licenses = license_register.values()
# todo: make it more stringent? </s> else:	req_CHECKURL self._last_url = url
# todo(danms) once libvirt has support for lxc hotplug, </s> self._create_domain_and_network(xml, instance, network_info)	resume_state_on_host_boot def resume_state_on_host_boot(self, context, instance, network_info): virt_dom = self._conn.lookupByName(instance['name'])
# todo: what decorator should we put here? in scipy.fftpack there is no planning, </s> contiguous_check=false, scipy_name='scp')	test_ifft_overwrite @testing.for_all_dtypes() def test_ifft_overwrite(self, xp, scp, dtype): x = testing.shaped_random(self.shape, xp, dtype)
# todo: refactor common tests for all models, e.g. shape checking </s> trans_h = transh(triples_factory=self.factory)	test_trans_h def test_trans_h(self):
# todo: more arguments possible: objectdb etc. </s> def git_dummy_command(self):	git_dummy_command
# todo: non-numeric columns should be ignored automatically </s> def test_iloc4(self):	test_iloc4 def test_impl(df, n): return df.iloc[[1,4,9]].B.values
# todo: upsert is too much. insert is fine as all keys are deleted. </s> self.session.query(feature).delete(synchronize_session="fetch")	clear_all def clear_all(self) -> None: self.session.query(FeatureKey).delete(synchronize_session="fetch")
# todo: make sure limits are deterministic then update this </s> def test_double_add_ignored(self):	test_double_add_ignored pass
# todo: check arp reply is valid </s> self.table = fakeoftable(self.num_tables)	setup_valve self.config_file = os.path.join(self.tmpdir, 'valve_unit.yaml') self.faucet_event_sock = os.path.join(self.tmpdir, 'event.sock') self.logger = valve_util.get_logger('faucet', self.logfile, logging.DEBUG, 0) self.registry = CollectorRegistry()
# todo: more? </s> yield entry	gen_entries@253 'name') entry['y:title'] = entry.get('title')
return s #todo return partial result instead of giving up </s> reset_rl_history=false)	down_one_line search=self.config.curtsies_right_arrow_completion)),
# wait until the chunks have added, todo change this to a qtbot.waitsignal </s> np.testing.assert_allclose(	test_tiled_single_scale@205 target_center = np.array([128, 128, 128, 255], dtype='uint8') screen_offset = 3  # Offset is needed as our screenshots have black borders screenshot[screen_offset, screen_offset], target_center )
f = self.uri('#%s' % f[2:]) # todo: can we make formula identifiers urirefs? </s> elif self.mode and self.mode[-1] == 'list':	numericliteral lit = Literal(tok, datatype=XSD.integer) if self.paths: self.lists[-1].append(lit) else: self.triples[-1].append(lit)
# todo: write </s> return dict.fromkeys(key_strings, callback)	string_keys_to_dict@9
## todo: # fixme: remove me </s> date = datetime.datetime.now().strftime("%y%m")	analyse@38 except: tld = url_parsed['tld'] server_statistics.hincrby('SQLInjection_by_tld:'+date, tld, 1)
# todo: remove in v1.5 </s> rank_zero_warn('accelerator method `connect_precision_plugin` was deprecated in v1.3.'	connect_precision_plugin def connect_precision_plugin(self, plugin: PrecisionPlugin) -> None: .. deprecated::v1.3 ' It will be removed in v1.5.') self.setup_precision_plugin(plugin)
# todo: get a buffer to test against </s> 0,	test_msracap ['0'], 0, 0, 0,
# todo: also save as json </s> packs workflow and commandline tools to generate re-runnable workflow object in ro	packed_workflow rel_path = posixpath.join(_posix_path(WORKFLOW), "packed.cwl") with self.write_bag_file(rel_path, encoding=None) as write_pack:
# todo: add docstring </s> in `model`. if not given and total attempts exceed `max_depth`, attributeerror is raised	get_keras_attr `attr` itself is not in `model` before returning `default` or raising AttributeError default: Object, default=object() Returns -------
# todo: make mac table updates less expensive. </s> org_tlvs.append(	send_lldp_beacons chassis_id = str(port.native_vlan.faucet_mac) org_tlvs = [] (org_tlv['oui'], org_tlv['subtype'], org_tlv['info'])) if lldp_beacon['system_name'] is None:
# todo: check the data! </s> u'familytotal': u'0', u'officerunningcosts': u'19848', u'mpotherrail': u'233',	test_csv for i in pipe: count += 1 u'CostofStayingAwayFromMainHome': u'22541', u'StationeryAssocdPostageCosts': u'3471', u'CommsAllowance': u'9767', u'Mileage': u'3358', u'MPMisc': u'20',
# todo: move this to pymatgen </s> p2$_{1}$/c and p-1 is converted to p$\\overline{1}$.	latexify_spacegroup def latexify_spacegroup(spacegroup_symbol): Args: spacegroup_symbol (str): A spacegroup symbol
# todo there is a cleaner way to do this in python 2.6, once </s> y = theano.shared(numpy.random.rand(3,6).astype(config.floatx), 'y')	test_constant try: theano.config.compute_test_value = 'raise' z = T.dot(x,y) assert hasattr(z.tag, 'test_value')
# todo: update after date time test function code clean up. </s> event_object = event_container.events[1]	testParse17 event_container = self._GetEventContainer(event_generator) self.assertEquals(len(event_container.events), 2) self.assertEquals(event_object.timestamp, 1362910309281250) self.assertEquals(
# todo: create unsupportedproviderexception. (?) </s> self.version = version	Spark class Spark: def install( self,
# todo use deepcopy() here </s> for p2 in exterior[1:]:	_is_polygon_line vec_down = np.float32([0, 1]) p1 = exterior[0] vec = np.float32(p2) - np.float32(p1) angle = angle_between_vectors(vec_down, vec)
# todo: funcbody </s> self.assertisnotnone(node)	testFieldExp def testFieldExp(self): p = get_parser('foo') self.assertEqual(1, p._pos) self.assertTrue(isinstance(node, parser.FieldExp))
# end todo </s> num_classes = sqrt_ggn_out.size(2)	bias_diag_ggn def bias_diag_ggn(module, grad_output, sqrt_ggn_out): out_pixels = module.output_shape[2] * module.output_shape[3] sqrt_ggn = sqrt_ggn_out.view(batch, module.out_channels, out_pixels,
# todo: switch to: </s> return self.current_tl_track	get_current_tl_track
# todo we could reload the message </s> returns `sender` but will make an api if necessary.	Forward@78 but instead was forwarded from e.g. a channel. return self._sender if not self.sender and self.original_fwd.from_id: try:
# todo: add test and check this more throughroughly. </s> ret = [layer(inputs[0])]	easy_apply raise ValueError("Layer expects only a single input!")
# todo replace with pec byte check </s> returns a list of `(property, value, unit)` tuples.	get_status fw_human, fw_cam = self._get_fw_versions() status = [
# todo: log exception </s> if result.endswith(' '):	scan@71 virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE) results = [] result = result[:-1] result = result.split(' ')
# todo: push all this code down to vigotoline? </s> w.run_command('close')	ExExit w.run_command('save') w.run_command('close')
# todo enforce uniqueness on arrange panel? </s> return helpers.json_response(response)	contents def contents(request): path = request.GET.get('path', '/home')
# todo: replace with np.isnat </s> if hpat.hiframes_api.isna(b, i):	_column_fillna_impl def _column_fillna_impl(A, B, fill):  # pragma: no cover for i in numba.parfor.internal_prange(len(A)): s = fill A[i] = s
# todo: compare to plain for loop through the labels </s> 'ids' is a list of feature ids	selectFeatures Returns a new Dataset object with a view of the original samples array (no copying is performed).
# todo(b/142684737): the multi-processing api might change. </s> module_file=module_file,	_create_pipeline@152 schema=infer_schema.outputs['schema'], module_file=module_file) transformed_examples=transform.outputs['transformed_examples'], schema=infer_schema.outputs['schema'],
# todo: check that the birth date is not in the future </s> year += 100	_get_birth_date elif year < 1954:
# todo support for absolute episode scrobbling </s> if type(guid) is str:	movie_match guid = Guid.parse(guid) pass
# todo(nnorwitz): this doesn't handle namespaces properly. </s> def _getlinenumber(self, start):	_GetLineNumber
federated_only=self.federated_only)  # todo: 466 </s> self.remember_node(node)	read_nodes_from_storage def read_nodes_from_storage(self) -> set: stored_nodes = self.node_storage.all(federated_only=self.federated_only)  # TODO: 466
# todo: error detection </s> global mongodb	__connect def __connect():
# todo: find another way </s> return i	inline_k_ctrl_k def inline_k_ctrl_k(self, xbegin, i, w):
# todo: handle multiple skip stacks </s> class flexline(list):	FlexLine
# todo: currently this test breaks the bleu implementation (13.03.2016) </s> references = [ref1, ref2, ref3]	test_modified_precision ref3 = str('It is the practical guide for the army always to heed ' 'the directions of the party').split() assert (float(modified_precision(references, hyp1, n=1)) == 1.0) assert(float(modified_precision(references, hyp1, n=2)) == 1.0)
# todo: update the review with a message </s> self.assertequal(self._countphabbadworkingbranches(), bad)	_phabUpdateWithExpectations if total is not None: self.assertEqual(self._countPhabWorkingBranches(), total)
assert isinstance(obj_type, instance)  # todo more flexible </s> elif d.kind == nodes.gdef and d.init:	visit_var_def reg = self.add_local(var) if d.init: init = self.accept(d.init) self.add(SetGR(var.fullname(), init))
# todo: redundancy between all gaze mappers -> might be moved to parent class </s> self.g_pool.quickbar.remove(self.button)	deinit_gui self.g_pool.calibration_menu.remove(self.info) self.menu = None self.button = None
# todo: this will need to be more complicated to support sparse features </s> else:	fetch_action return fetch(b)
# todo: feed.abort() should be done by using exception? not a flag that has to be checked everywhere </s> def _purge(self):	_purge
# todo: handle unsigned </s> def __repr__(self):	TZoneUTC return 'UTC' def dst(self, dt): return '{}()'.format(self.__class__.__name__)
# time.sleep(40)  # todo: should remove after polling get. </s> data_2 = tensor(child=np.array([[567, 98], [78, 25]], dtype=np.int32))	test_tensor_abstraction_pointer@32 clients = get_clients(3) op = getattr(operator, op_str) data_3 = Tensor(child=np.array([[125, 10], [124, 28]], dtype=np.int32)) tensor_pointer_1 = data_1.send(clients[0])
from .mpc import quadcost, lindx # todo: this is messy. </s> return x.unsqueeze(0).expand(*([n_batch] + list(x.size()))), true	expandParam if X.ndimension() in (0, nDim): return X, False else: raise RuntimeError("Unexpected number of dimensions.")
# todo: move to 80 bits </s> def jno(ir, instr, dst):	jno
# todo parameter order? (for clobj_list) </s> return _lib.clobj__int_ptr(self.ptr)	int_ptr @property
# todo(b/134377706): remove the wrapper once the bug is fixed. </s> prev_state = self.evaluate(core.initial_state(self.batch_size))	testComputationAgainstNumPy inputs = self.evaluate( tf.random.uniform([self.batch_size, self.input_size])) core_fn = tf.function(core) if use_tf_function else core outputs, next_state = core_fn(tf.convert_to_tensor(inputs), prev_state)
# todo: support all tzinfo subclasses by calling utcoffset() </s> sign = '+'	tzname if self.offset < 0: sign = '-'
# todo(ytknzw): add more specific assertion with the test case. </s> raise valueerror	fail_objective
col_width = 30  # todo: use screen size </s> return " | ".join(	row def row(*columns: Tuple[str, str, int]) -> str:
# user perm is created on todo but for doctype assignment rule only </s> is_created = add_user_permissions(param)	test_for_apply_to_all_on_update_from_apply_all param = get_params(user, 'User', user.name) is_created = add_user_permissions(param) self.assertEquals(is_created, 0)
# todo: implement positional encoding as described in the paper. </s> rate=self.dropout,	SelfAttentionEncoder inputs = self.position_encoding_reducer.reduce(inputs, position_embedding) inputs = tf.layers.dropout( training=mode == tf.estimator.ModeKeys.TRAIN) outputs = []
# todo: fix/disambiguate. </s> overlap = degreeofoverlap	minimumOverlap elif degreeOfOverlap < overlap:
@pytest.mark.filterwarnings('ignore:missing metadata')  # todo: fix bug for hgs maps </s> scale=u.quantity(aia171_test_map.scale),	hpc_header obstime=aia171_test_map.date, observer=new_observer, projection_code='TAN'
# todo: test require restart </s> "--kasp-db", replica_backup_filename,	TestMigrateDNSSECMaster args = [ "ipa-dns-install", "--forwarder", self.master.config.dns_forwarder, "-U",
# todo add locales </s> if new_main_domain not in domain_list()["domains"]:	domain_main_domain from yunohost.tools import _set_hostname if not new_main_domain: raise YunohostError("domain_name_unknown", domain=new_main_domain) operation_logger.related_to.append(("domain", new_main_domain))
# todo: it has not been decided by the css working group how </s> if text:	initial_text elif len(self.view.sel()) == 1: self.setup_color_class() color = None if self.custom_color_class == self.color_mod_class:
# todo add locales </s> regen_conf()	domain_main_domain if os.path.exists("/etc/yunohost/installed"):
# todo should print message but carry on as if 0 </s> exec_opts.showoptions(argv[i:])	Shopt arg, i = SHOPT_SPEC.Parse(argv) if arg.p:  # print values else: exec_opts.ShowShoptOptions(argv[i:])
# todo: logging </s> contracts = self.__registrar.lookup_contract(contract_name=contract_name)	get_contract_address addresses = [c['addr'] for c in contracts] return addresses
# todo: move to base class </s> max_y = max(arr4)	getNodesRect min_y = min(arr3)
# todo: assert </s> self.remote.modify_repo(repo, "mirror_locally", "0", self.token)	createRepo repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "name", "testrepo0", self.token) self.remote.save_repo(repo, self.token)
# todo: simplest possible unicast learning. </s> ofmsgs.extend(self._add_controller_learn_flow())	_add_default_flows ofmsgs.extend(self._add_vlan_flood_flow())
# todo: can we use a real variablemanager? </s> return none	_get_host
# todo implement for all channels </s> return self._execute_cmd("set", "appliance.control.togglex", payload)	turn_on_channel def turn_on_channel(self, channel):
self.setup()  # todo: perhaps, remove this to pass path in context </s> return all_current_ids_hash	_hash_data_container if all_current_ids_hash is not None: m.update(str.encode(all_current_ids_hash))
# todo: remove in 1.2 </s> assert threshold >= sc.radius	check_threshold while current_leaf: subclusters = current_leaf.subclusters_ current_leaf = current_leaf.next_leaf_
r = redirect("../../..") # todo: no longer correct </s> builder_control = self.builder_control.getbuild(num)	getChild if build_status: build_control = None return StatusResourceBuild(build_status, build_control, self.builder_control)
#todo: implement xml support </s> return 'it did not work\n'	Tenants return "whatever, we don't have XML yet" else:
### todo: need to improve </s> gold_count = len(gold_chunks)	eval_instance total_labels = len(best_path) correct_labels = np.sum(np.equal(best_path, gold)) guess_chunks = utils.iobes_to_spans(best_path, self.r_l_map) guess_count = len(guess_chunks)
# :todo: implement test. </s> iota(self.adapter).sendtransfer,	SendTransferCommandTestCase self.assertIsInstance(
# todo: check </s> pos_heads = pos_exmpls[:, 0:1]	TransR nn.init.xavier_uniform(self.relation_embeddings.weight.data) nn.init.xavier_uniform(self.projection_matrices.weight.data) pos_relations = pos_exmpls[:, 1:2] pos_tails = pos_exmpls[:, 2:3]
# todo: save as yaml file </s> def from_file(self, namespace):	from_file
# todo: confirm if gpu is used in hpo (probably not) </s> return y_pred_proba	_predict_proba y_pred_proba = np.multiply(y_pred_proba, 1/np.sum(y_pred_proba, axis=1)[:, np.newaxis]) if y_pred_proba.shape[1] == 2:
# todo: enable custom config </s> logger.info('on_setup')	on_setup
# todo: some other way? this seems like a slippery slope of convenience functions </s> proc.start()	async proc = multiprocessing.Process( target=self._proc_function, proc.join()  # MUST join, otherwise we leave zombies all over return {'tag': tag, 'jid': jid}
report_config = {}  # todo port to fooddata.from_request </s> nutrientintakesbyfooddata(config=self.report_config),	data_providers return [
# todo: document params </s> def render_share_problem(self, ctx, data):	render_share_problem
# todo: should be named get_fields() ? </s> for name in translations_model.get_translated_fields():	add_meta raise RuntimeError("Adding translations afterwards to an already inherited model is not supported yet.") self._extensions.append(meta) self._fields_to_model[name] = translations_model
return 0 # todo </s> def get_resource_import(self, resource_id):	get_resource_import
# todo:  implement this using in-memory locking </s> if not libdokan.dokanisnameinexpression(pattern,nm,true):	FindFilesWithPattern fpath = pathjoin(path,nm) if self._is_pending_delete(fpath): continue data = self._info2finddataw(fpath,finfo,None)
# todo: exp_block_pairs </s> self.asserttrue(isinstance(node, parser.fieldexp))	testFieldExp node = p._field() self.assertIsNotNone(node) self.assertEqual('foo', node.exp.value.name._data)
# todo: fix this </s> props = [_tag("d", "getcontenttype"),	propfind root = ET.fromstring(xml_request.encode("utf8")) props = [prop.tag for prop in root.find(_tag("D", "prop"))] _tag("D", "resourcetype"), _tag("D", "displayname"),
# todo: does this import need to be delayed because </s> self.solver_results = results	ExtensiveFormAlgorithm if self.get_option("output_solver_results"): print("Results for ef:") if hasattr(results.solver,"user_time") and \ (not isinstance(results.solver.user_time,
""" todo. """ </s> def media_next_track(self):	media_next_track
#todo: add csrf here? or make ids uuid so they can't be guessed? </s> )	_generate_relay_From def _generate_relay_From(original_from_address): relay_display_name, relay_from_address = parseaddr( return relay_from_address, '%s via Firefox Private Relay' % ( original_from_address
# todo: weather data source changed, temporarily disable, will enable it later when new data source is available. </s> return	_on_action_received def _on_action_received(self, evt: Event): action: Action = None for action in evt.payload: from_station_idx: int = action.from_station_idx
pass # todo </s> def collect_incoming_data(self, data):	collect_incoming_data
#todo investigate 3.5/3.6 slowness </s> terminal = true	TestTRPOAgent else: state = (0, 1) agent.add_observation(state=state, action=action, reward=reward, terminal=terminal) rewards[n % 100] = reward
# :todo: implement test. </s> self.skiptest('not implemented yet.')	ReplayBundleRequestFilterTestCase self.skipTest('Not implemented yet.') def test_fail_unexpected_parameters(self): def test_fail_transaction_null(self): ``transaction`` is null.
# todo verify </s> class error(exception):	Error
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> "tenant" : "common",	Test_AcquireTokenWithUsernamePassword class Test_AcquireTokenWithUsernamePassword(unittest.TestCase): def test_acquire_token_with_user_pass(self): "authorityHostUrl" : "https://login.windows.net", "clientId" : "04b07795-8ddb-461a-bbee-02f9e1bf7b46", # xplat's which is supposed to be in every tenant
# todo(bcipolli): bulk saving of logs </s> other_weight = (1.-common_weight)/(n_types-1)	generate_user_type common_weight = user_types[0]["weight"]
# todo: this is a hack to make a rule know </s> feature_str += "|"	_create_feature_key feature_str = "" for state in states: for feature in state.keys(): feature_str += feature + " "
# todo: let this exception propagate </s> with open(file_path, 'wb') as file_handle:	_save_object except StopIteration: return False while len(data_read) > 0: file_handle.write(data_read)
# todo: fix this to properly record the last stream id we've seen. </s> headers = self.decoder.decode(frame.data)	_receive_headers_frame "Max outbound streams is %d, %d open" % (max_open_streams, self.open_outbound_streams) events = self.state_machine.process_input( ConnectionInputs.RECV_HEADERS
pass # todo </s> operation = {	build_operation pixbuf = None else: # Cas des importations uniquement 'tool_id': self.id, 'operation_type': self.operation_type,
#     todo </s> os.chmod(cache_path, mode)	chmod cache_path = self.cache.get_path_or_dummy()
# todo: we probably don't want to say this for re-formulation ones. </s> return_handler = self.getblockreturnhandler()	mayBreak continue_handler = self.getBlockContinueHandler() if continue_handler is not None and continue_handler.mayBreak(): if return_handler is not None and return_handler.mayBreak(): return True
# todo: capture stdout for both the test assert and docs embedding </s> newton.options['iprint'] = -1	test_feature_iprint_neg1@33 prob.setup(check=False) prob['y1'] = 10000 ln_scipy.options['iprint'] = -1 prob.run_model()
# :todo: implement test. </s> sandboxadapter('https://localhost', 'token', none)	test_error_poll_interval_null The implications of allowing this are cool to think about... but not implemented yet.
# todo(kan-bayashi): need to be fixed in pytorch v4 </s> logging.debug('grad norm={}'.format(grad_norm))	update_core optimizer.zero_grad() loss.backward() if math.isnan(grad_norm): logging.warning('grad norm is nan. Do not update model.')
# todo: compare col/row widths before/after - not implemented yet </s> ((2,1), test_date_1),	test_cell ('A1', 'éöà'), ((1,1), 'éöà'), ('A3', test_date_2), ((3,1), test_date_2)]
# todo(b/186451541): reduce the number of calls to model_fn. </s> for _ in range(3):	_run_test state = process.initialize() prev_loss = np.inf state, metric_outputs = process.next(state, datasets) self.assertEqual(
# todo: show deprecation message in the future. </s> def error_routes(self):	error_routes return self.yaml.get('error_routes')
# todo 当我不使用下面的语句时，project和host貌似在线程里面会没有值，也许我要把lazy值设置成select或者其他 </s> try:	rollback_thread port=deploy.host.ssh_port, user=deploy.host.ssh_user, logger.debug("before rollback:") before_rollback = deploy.project.before_rollback.replace("\r", "").replace("\n", " && ")
# todo: improve the unicode checking </s> else:	_batch results_list = json.loads(content) results_dict = self._results_list_to_dict(results_list) raise TransactionException(response.status)
# todo(vek): need to pass context in for access to auth_token </s> assert state in power_state.valid_states(), "bad state: %s" % state	InstanceInfo self.name = name
# todo _cphttptools.applyfilterlist('afterrequestheader') </s> self.socket.setsockopt(socket.sol_socket, socket.so_reuseaddr, 1)	server_bind self.socket.bind(self.server_address)
# todo: maybe[int] and maybe[simple_sum] are invalid </s> self.emit('', depth)	_EmitDict for k in sorted(d): self.Emit('%d: %r,' % (k, d[k]), depth + 1)
# todo(efried): change this to an inner join when we are sure all </s> user = sa.alias(_user_tbl, name="u")	_get_allocations_by_consumer_uuid rp = sa.alias(_RP_TBL, name="rp") consumer = sa.alias(_CONSUMER_TBL, name="c") cols = [ allocs.c.resource_provider_id,
pass # todo </s> class artist(testmodel):	Artist
# todo: check what other filetpyes supported </s> while success:	detect_and_cover fps, (width, height)) count = 0 print("frame: ", count) success, image = vcapture.read()
# todo: [config] modify for new importer </s> * logger	run_check_config_cron_user performed checks: * CSV import user: configured result: * error: stop import
#todo - use a context manager here once we drop python 2.6 </s> self.assertalmostequal(mean[0], 3.5461538461538464)	test_pca [ 5.1, 5.2 ], ]) self.assertAlmostEqual(mean[1], 3.5307692307692311) self.assertAlmostEqual(coordinates[0, 0],  2.0323189722653883)
# todo: remove anytime in 2016 </s> @classmethod	FormsByApplicationFilter return xmlns app_id = identify[1] if len(identify) > 1 else MISSING_APP_ID def get_labels(cls): return [
# todo test </s> def __hash__(self):	__hash__
# todo: test this </s> "when there is an otherwise-unspecified validity error that prevents parsing."	MeiValidityError pass
# todo not an api for now </s> packages = ['package1', 'package2']	test_generated_cmd_hold_packages monkeypatch.setattr(shutil, 'which', lambda *_: 'path') NodeControlUtil.hold_packages(packages)
# todo: reflection padding </s> shape=[3, 3, relu1.get_shape()[3], k])	Rk strides=[1, 1, 1, 1], padding='SAME') relu1 = tf.nn.relu(conv1+biases1) biases2 = tf.get_variable("biases2", [k], initializer=tf.constant_initializer(0.0))
# todo (shea): extract method(s) to get_source_processor() </s> }]	generate_jss_recipe "name": "%GROUP_NAME%", "smart": True, } if facts["version_key"] == "CFBundleVersion":
# todo: also run container and make sure that the env var is set inside the container </s> return ''.join(random.choice(letters) for _ in range(length))	random_word def random_word(length):
# todo stdin </s> if r.get('status', none) == 'notneeded':	__call__@133 cmd_shorty, json.dumps(run_info, indent=1)) continue yield r
# todo: put this into timeframegroup. #316 </s> " " + str(other))	check_for_overlap intersect = self.intersect(other) if not intersect.empty:
# todo: renderer.media_type isn't the right thing to do here... </s> accept_list = ['text/html', '*/*']	_determine_renderer elif (self.REWRITE_IE_ACCEPT_HEADER and request.META.has_key('HTTP_USER_AGENT') and elif request.META.has_key('HTTP_ACCEPT'): accept_list = request.META["HTTP_ACCEPT"].split(',')
# todo generator </s> recursion_limit=recursion_limit,	Drop dataset=refds_path, path=path, action='drop', unavailable_path_status='notneeded',
# todo : pytest.mark.parametrise once nose is gone. </s> pass	SB
# todo: gdal doesn't support signed 8-bit values, so we coerce to uint8, </s> :param resampling: str resampling strategy	warp_affine :param        src: image as ndarray :param        dst: image as ndarray **kwargs -- any other args to pass to implementation :returns: dst
# todo: this should show up in events </s> return annotations	_build_common_annotations if self.name: annotations['hub.jupyter.org/servername'] = self.name
# todo: verify </s> manager = factory.consumer_bind_manager()	test_remove_repo_cleanup def test_remove_repo_cleanup(self): binds = manager.find_by_repo(self.REPO_ID) self.assertEquals(len(binds), 1)
# todo: implement </s> :type line: ...dataformat.converter_object.converterobjectgroup	terrain_defense_upgrade :param converter_group: Tech/Civ that gets the patch. :type converter_group: ...dataformat.converter_object.ConverterObjectGroup :param value: Value used for patching the member. :type value: MemberOperator
# todo: the following skipped suite and fixtures should be enabled </s> @pytest.fixture(autouse=true)	VultrProviderTests domain = 'capsulecd.com' def _filter_headers(self): def skip_suite(self, request): if request.node.get_marker('ext_suite_1'):
# todo sync protocol </s> def on_window(i3, e):	on_window
self.data = fp.read(1_048_576)  # todo: recheck in 0.6.x </s> else:	handle_free self.error(self._("Free download link not found"))
else: self.assertequal(end, 1) # todo: simple exec should not wait_testpid!! </s> logg.info("\n>>>\n%s", top)	test_6133_run_default_services_from_single_service_saved_container logg.info("\n>\n%s", out3) top_container = "docker exec {testname} ps -eo pid,ppid,user,args" self.assertFalse(greps(top, "testsleep 99")) self.assertFalse(greps(top, "testsleep 111"))
##todo(ziad):we need to figure out how to auth to keystone </s> return httpuseproxy(location=self.auth_location)(self.env,	_reject_request self.start_response)
# todo(necula): fix gather bug on tpu </s> np.array([0, 0xffffffff], dtype=np.uint32),	test_prngsplit for rng_key in [jax.random.PRNGKey(42), np.array([0, 0], dtype=np.uint32), np.array([0xFFFFFFFF, 0xFFFFFFFF], dtype=np.uint32) ]:
# todo: refactor the mockrequesthandler </s> self.assertequal(j, n_docs)	TestAsyncIOAwait async for doc in await collection.aggregate(pipeline, cursor={}): self.assertEqual(j, doc['_id']) await collection.remove() @asyncio_test(timeout=120)
# todo unordered float </s> n.zeroextend(instr.mode),	jno meip = mRIP[instr.mode] n = m2_expr.ExprId(ir.get_next_label(instr), dst.size) dst.zeroExtend(instr.mode)) e.append(m2_expr.ExprAff(meip, dst_o))
# todo - temporary until flash algo is rebuilt with 4k page program size </s> data = bytes[i * write_size : (i + 1) * write_size]	programPage write_size = 1024 self.page_size = write_size # temporarily override page size Flash.programPage(self, flashPtr + i * write_size, data) self.page_size = flash_algo['page_size'] # restore page size
# todo handle errors </s> vb_xpcom_to_attribute_dict(machine, "imachine", **kwargs)	vb_list_machines return [
# todo: make sure this openssl command works everywhere, maybe we should use a text_base64_decode? </s> sudo("yum -y update")	package_upgrade_yum
# todo: use mapper </s> return self.cube.mappings.get(dim, dim)	_property def _property(self, dim):
# todo: specific exception handling </s> def get_event_types(self):	get_event_types
# todo: improve error handling </s> server_addr = event["headers"].get("host", none)	get_server_and_client Parse the server and client keys for the scope definition, if possible. client_addr = event["requestContext"].get("identity", {}).get("sourceIp", None) if server_addr is not None: if ":" not in server_addr:
# todo: automate this (by lookup from nn). </s> summary_spec=dict(summary_regexp="time-step", directory="/home/rlgraph/"),	TestIMPALAAgentFunctionality mode="distributed", distributed_spec=dict(cluster_spec=None) dynamic_batching=False, num_actors=4
# todo: replace with assertion test </s> def test__nirwhoislookup(self):	TestNIR net = Net('133.1.2.5') obj = NIRWhois(net)
raise notimplementederror #todo </s> pass #todo	Span
# todo: empty env? </s> sendrc=false, timeout=120, usepty=false, environ={})	TestP4 Expect(['p4', '-p', 'p4dserv:1666', '-u', 'jimmy', '-P', Obfuscated('hushnow', 'XXXXXXXX'), '-c', 'buildbot_test_10', 'sync', '-f'], + 0, Expect(['p4', '-p', 'p4dserv:1666', '-u', 'jimmy', '-P',
# todo verbosity was mistakenly removed from task_parameters on release 0.11.0, need to bring it back </s> if not hasattr(self._parent, 'name'):	parent :return: None self._parent = val raise ValueError("The parent of an agent must have a name") self.full_name_id = self.ap.full_name_id = "{}/{}".format(self._parent.name, self.name)
# todo(b/124466113): remove tf.compat.v2 once tf 2.0 is the default. </s> self.runtestforexporter(exporter.finalexporter)	testFinalExporter
# todo: avoid using default index? </s> (3, 3)	levshape ('c', 'z')], ) internal = self._internal result = internal._sdf.agg(*(F.countDistinct(c) for c in internal.index_scols)).collect()[0]
# todo: once we allow filtering, unit.store.units has to be a qs </s> if matchnames:	get_step_query units_queryset = state_queryset if 'matchnames' in request.GET: match_queryset = units_queryset.none() if 'hassuggestion' in matchnames:
## \todo: remove nodegraph fallback when all client code has been updated </s> node.enabledplug().setvalue( value )	__setEnabled @classmethod def __setEnabled( cls, node, value ) :
# todo: replace with "yield from" when dropping python 2. </s> for __ in value:	StreamMapper yield value else: yield __ except TypeError:
# todo: could refactor this, probably </s> else:	prepare_ud_dataset elif short_name == "en_ewt": strip_mwt_from_conll(input_conllu, input_conllu_copy) shutil.copyfile(input_txt, input_txt_copy) shutil.copyfile(input_conllu, input_conllu_copy)
# todo(pachristopher): remove this once tfdv 0.14 is released. </s> def getinputsourcetoexampleptransform(self) -> beam.ptransform:	GetInputSourceToExamplePTransform
# todo: implement auto-dtype method in general parameters </s> the ionisation edges of the elements present in the current	add_elements energy range will be added automatically. Parameters
# todo should be outside the 'with' block </s> colorspacesequence(name="name", sequence="k0123", qualities="####")	TestSequence ColorspaceSequence(name="name", sequence="T0123", qualities="#####") @raises(FormatError)
# :todo: implement test. </s> iota(self.adapter).preparetransfers,	PrepareTransfersCommandTestCase def test_wireup(self): Verify that the command is wired up correctly. PrepareTransfersCommand, )
# todo: if none, then get from the filename </s> name = filename	load_from_disk if os.path.splitext(name)[1] == ".gz": name = os.path.splitext(name)[0]
# todo: proper partitioning of unittests </s> debug('test', 'running %s on args=%s and kwargs=%s' %	sweepargs kwargs_[argname] = argvalue try: (method.__name__, `args_`, `kwargs_`)) method(*args_, **kwargs_)
await self._stream.reset()  # todo: specify error code </s> return self	__aiter__
# todo(kan-bayashi): need to make more smart way </s> return hpad.new_zeros(hpad.size(0), self.dunits)	zero_state
# todo: fix </s> self.assertnotcontains(response,"exception", status_code=200)	ViewsTestCase self.assertNotContains(response,"Exception", status_code=200) response = self.client.get('/reports/%s/flat/' % case.id) response = self.client.get('/reports/%s/csv/' % case.id) self.assertNotContains(response,"Error", status_code=200)
# todo: document! </s> q_oper.superrep = 'choi'	super_to_choi q_oper = _super_tofrom_choi(q_oper)
if optimizer in ['adam', 'adadelta', 'rmsprop', 'sgd', 'sgdmomentum']: #todo: this could also be done for other optimizers </s> if return_alignment:	gen_sample hyp_scores.append(new_hyp_scores[idx]) hyp_states.append(copy.copy(new_hyp_states[idx])) hyp_alignment.append(new_hyp_alignment[idx]) hyp_scores = numpy.array(hyp_scores)
# todo assert increasing timestamp? </s> get_dataframe().plot()	dataframe def dataframe(): %matplotlib gtk import pandas as pd # type: ignore return pd.DataFrame(p._asdict() for p in measurements()).set_index('dt')
if self._ndim == 3: # todo: use hasz </s> temp = c_double()	xy m = self.__len__() x = array('d') for i in xrange(m): lgeos.GEOSCoordSeq_getX(self._cseq, i, byref(temp))
recording_name = none  # todo </s> return info_csv["capture software"] == "pupil invisible" and "data format version" not in info_csv	is_pupil_invisible_recording def is_pupil_invisible_recording(rec_dir: str) -> bool: info_csv = utils.read_info_csv_file(rec_dir) except KeyError: return False
# todo: refactor accordingly when v3 websocket api is released </s> def order_book_class(self) -> bittrexorderbook:	order_book_class return BittrexOrderBook
# todo change to native framework call, when plex allows token in header </s> sectiontypeurl).get('librarysectiontitle')	viewstate result['sectionType'] = XML.ElementFromURL( sectionTypeUrl).get('viewGroup') result['serverId'] = XML.ElementFromURL( misc.GetLoopBack() + '/identity').get('machineIdentifier')
# todo: make test method </s> finally:	test_gps droid.startLocating() try: droid.stopLocating()
# todo: must be implemented </s> pass	should_fetch_kindlegen
rm.fetch(refspec=refspec, progress=progress, **kwargs)  # todo: progress +kwargs </s> def git_get_active_branch(self):	git_get_active_branch
# todo: implement this </s> raise notimplementederror	_run_task
# todo: with git <= 2.3 keep old mechanism: </s> return self.repo.active_branch.name	git_get_active_branch
# todo: given trial, a repo, and an observer </s> self.events: list[trial] = []	SomeObserver class SomeObserver(_Observer[Tuple[HyperparamsRepository, Trial]]): def on_next(self, value: Tuple[HyperparamsRepository, Trial]): self.events.append(value)
#@todo: re-enable test when exception handling is in place </s> try:	NSDaemonThread self.started=threading.Event() def run(self): self.nsdaemon.requestLoop() finally:
# todo: change this to use assertsetequal: </s> 'popularity': 10,	test__build_doc 'id': 'doc_1', 'title': 'Example doc ☃ 1', } doc_xml = force_unicode(ElementTree.tostring(self.solr._build_doc(doc), encoding='utf-8'))
# todo(ayoung): support the ability for a project acting as a domain </s> trustor_user_ref = self.identity_api.get_user(	_assert_default_domain if (trustee_user_ref['domain_id'] != CONF.identity.default_domain_id): trust_ref['trustor_user_id']) if (trustor_user_ref['domain_id'] !=
# :todo: implement test. </s> skip_value_check = true	GetTransactionsToApproveRequestFilterTestCase class GetTransactionsToApproveRequestFilterTestCase(BaseFilterTestCase): def test_pass_happy_path(self): self.skipTest('Not implemented yet.')
# todo for windows: </s> size_gb_done_before = size_gb_done	main@72 cnt_403_retry += 1 else: if size_GB_done >= SIZE_GB_MAX or cnt_403_retry >= CNT_403_RETRY: kill_cmd = "screen -r -S %s -X quit" % screen_name
# todo(gibi): remove this when live migration is fully supported and </s> server = self._wait_for_state_change(server, 'active')	test_flavor_image_traits_based_scheduling 'HW_CPU_X86_SGX']) server = self._create_server_with_traits( self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
# todo(albert): windows machines don't have a readline module. </s> log. if none, output is not logged.	register_log append to the log. PARAMETERS: self._log = log
# todo: may test file contents </s> self.assert_expected_table(table)	PluginCsvTestCase def test_import_from_csv_fobj(self): with open(self.filename) as fobj: def test_export_to_csv_filename(self): temp = tempfile.NamedTemporaryFile(delete=False)
# todo: refactor codebase to avoid needing this kind of special case check by being more </s> if not hasattr(self, 'uses'):	dependencies return [] deps = []
# todo(chen3feng): reuse cclibrary </s> files.append(self._target_file_path(f))	_thrift_gen_cpp_files def _thrift_gen_cpp_files(self, src): files = [] return files
# todo: the eigenvector associated with the smallest eigenvalue </s> forward = pick_channels_forward(forward, include=ch_names)	dics_epochs@138 picks = pick_types(epochs.info, meg=True, eeg=False, eog=False, stim=False, exclude='bads') if label is not None: vertno, src_sel = label_src_vertno_sel(label, forward['src'])
# todo: integrate this into emrjobrunner </s> cat_from_list(runner, runner.ls_all_logs_s3())	cat_all try: cat_from_list(runner, runner.ls_all_logs_ssh())
# todo: use ledgerprocessor </s> "name": _("name"),	SupplyPointCase [ { }, {
# todo(mlavalle) this notification should be updated to publish when </s> context, _pager=pager, **filters)	get_address_groups page_reverse=False): pager = base_obj.Pager(sorts, limit, page_reverse, marker) return [ self._make_address_group_dict(addr_group, fields)
# todo: warn if not used with -t roles </s> doc['entry_points'][ep] = entry_spec	_build_doc for ep in argspec.keys(): if entry_point is None or ep == entry_point: if len(doc['entry_points'].keys()) == 0: doc = None
# todo fix. </s> cmp_result = self.__compare_contexts(ctx_init, x86_ctx_out, reil_ctx_out)	test_cmova asm = ["cmova eax, ebx"] ctx_init = self.__init_context() if not cmp_result: self.__save_failing_context(ctx_init)
# todo: assert </s> self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token)	createRepo def createRepo(self): repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "mirror_locally", "0", self.token) self.remote.save_repo(repo, self.token)
# todo debug </s> self.timetriggeredfor = 0.0	RuleElement self.timeWhenTriggered = 0.0
pass  # todo </s> assert_equal(sht.range('a1:d4').row_height, 40)	TestSheet sht.range('A1:D4').value = 'test_string' sht.range('A1:D4').row_height = 40 assert_equal(sht.range('A1:D4').column_width, 40) sht.autofit()
# todo add verbose output </s> def domain(self):	LocalizationModel@8 self._domain = domain self._localedir = localedir return self._domain @domain.setter
# todo untested </s> finally:	get_serial_number _api.OPENSSL_free(hex_serial)
# todo: timeline is global, get rid of it </s> return self.titles[lang]	title
#todo: proper implementation </s> else: return true	FilterHandler return getattr(self, self.filter['match_policy'])(target)
# todo debug </s> ruleelement.element = rulenew	parseRuleRecursively ruleNew.type = "or" ruleElement = RuleElement() currentRule.elements.append(ruleElement) parseRuleRecursively(item, ruleNew)
# todo: don't let this crash the robot </s> for i in range(4):	four_legs_inverse_kinematics numpy array (3,4) Matrix of corresponding joint angles. body_offset = config.LEG_ORIGINS[:, i] alpha[:, i] = leg_explicit_inverse_kinematics(
# todo(stubexecutor): (optional) use stubbed_component_map to insert custom stub </s> returns:	get_stub_launcher_class StubComponentLauncher class holding stub executors. return StubComponentLauncher
# todo: get rid of this </s> return "input_start"	compute_smart_home_destination_index def compute_smart_home_destination_index(self): else: return super().compute_smart_home_destination_index()
# todo: refactor this into a django form </s> def _key_to_val(formdata, key):	export_all_form_metadata "timeStart", "timeEnd", "deviceID", "username", "userID", "xmlns", "version") if key == "type":  return xmlns_to_name(domain, formdata.xmlns, app_id=None) else:              return getattr(formdata, key)
# todo: make this more portable with shutil etc. </s> self.run("git", "init", workingdir=self.path)	setUp@45 self.runCommands("mkdir " + self.path)
# todo: test for last revision minus 50 on second page. </s> offset = url_for(controller='revision', action='purge', id=none)	test_purge res = self.app.get(offset) assert 'No revision id specified' in res
# todo(b/1613650790: move this logic to ppoklpenaltyagent. </s> if self._kl_cutoff_factor <= 0.0:	kl_cutoff_loss def kl_cutoff_loss(self, kl_divergence: types.Tensor, return tf.constant(0.0, dtype=tf.float32, name='zero_kl_cutoff_loss') kl_cutoff = self._kl_cutoff_factor * self._adaptive_kl_target
# todo: implement test </s> self.assertfalse(os.system.called)	TestGroups self.verify_succeeded(report, removed=groups) self.assertFalse(report.reboot['scheduled']) self.assertFalse(YumBase.processTransaction.called) def test_uninstall_with_reboot(self):
# todo: write tests </s> pass	MeiValidityError class MeiValidityError(exceptions21.Music21Exception):
# todo xxx graalvm change </s> self.assertequal(calls, [("supermessage", server_context)])	test_sni_callback stats = server_params_test(client_context, server_context, chatty=True, self.check_common_name(stats, 'fakehostname') calls = []
pass # todo doing nothing is also a possibility </s> self.editor_box.set_visible(self.color_widget.props.show_editor)	_update_nav_box def _update_nav_box(self, *args):
# todo: this might need a better test </s> assert utils.check_equal_torch(	test_depth_warper@67 batch_size)) inv_depth_ref = torch.ones(batch_size, 1, height, width) patch_ref[..., :int(height - offset), :int(width - offset)], patch_i[..., int(offset):, int(offset):])
#todo raise error or others </s> process = popen(['gconftool-2', '--all-dirs', dir], stdout=pipe)	KeyDirView self.append_column(column) def update_model(self, dir): stdout, sterror = process.communicate() if sterror:
# todo: arrange </s> self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token)	createRepo def createRepo(self): repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "mirror_locally", "0", self.token) self.remote.save_repo(repo, self.token)
# todo consider adding the individual tiles to the resource? </s> x, y = self.get_origin()	get_midbottomright return (x + self.width // 2 + self.width // 4 + self.width // 8, y + self.height // 4)
# todo(iceboy): rate limit base on ip. </s> @base.sanitize	ProblemDetailHandler class ProblemDetailHandler(base.Handler): @base.require_perm(builtin.PERM_VIEW_PROBLEM) async def get(self, *, pid: document.convert_doc_id): uid = self.user['_id'] if self.has_priv(builtin.PRIV_USER_PROFILE) else None
# todo map relationship backreferences using the django names </s> except keyerror:	get_class return self.Base.classes[DjangoModel._meta.db_table]
# todo this is not tested yet. </s> t = np.double(t)+1e-35	cdf def cdf(t, a, b):
else: # todo support ssl </s> body += "you've got a new tip"	_create_email body += "The comment has been produced by %s\n" % data['source'] body += "and, by the way, the content is:\n%s\n" % data['content'] return string.join(("From: GLBackend postino <%s>" % source, "To: Estimeed Receiver <%s>" % dest,
# todo: implement this for mesh tallies </s> return domain_filter.num_bins	num_subdomains def num_subdomains(self): tally = self.tallies.values()[0]
# todo: exp_block_pairs </s> self.assertequal('foo', node.exp.value.name._data)	testFieldExp self.assertIsNotNone(node) self.assertEqual(1, p._pos)
#there's nothing todo </s> self.__clean_from_node(child_id)	__clean_from_node n = self.node_n_children(nid) while n > 0: n = n-1 self.__remove_node(nid)
# todo: change </s> self.slices = none	InMemoryDataset def __init__(self, root, transform=None): super(InMemoryDataset, self).__init__(root, transform) def __len__(self): return self.slices[self.dataset.keys()[0]].size(0) - 1
# todo: lamp texture test.. </s> return [index_table[i], index_table[i + 1], index_table[i + 2]]	write_triangle def write_triangle(self, triangle_index, index_table):
# todo cache? </s> favs = [s.kind for s in saves if s.text == 'favorited']	test_disappearing@239 def test_disappearing(): [deal_with_it] = [f for f in favs if f.title == '"Deal with it!"'] assert deal_with_it.backup_dt == datetime(2019, 4, 1, 23, 10, 25, tzinfo=pytz.utc)
# todo implement. </s> super(sparkmodel, self).__init__(keras_model, data,	SparkModel class SparkModel(DistributedModel): def __init__(self, sc, rdd, keras_model, data, optimizer, optimizer, master_port) self.spark_context = sc
""" a completed todo must start with an x followed by a date. """ </s> today_str = today.isoformat()	test_set_complete3 todo = TodoBase.TodoBase("Foo") todo.set_completed() self.assertEquals(todo.fields['completionDate'], today) self.assertTrue(re.match('^x ' + today_str + ' Foo', todo.src))
# todo: log.warning("tried to release connection that did not exist any longer : {0}".format(connection)) </s> yield connection	by_node_context def by_node_context(pool, node): Get a connection from the pool and automatically release it back pool.release(connection)
# todo, make connection _cache attr public </s> args = ()	getGhost if isinstance(klass, tuple): klass = self._get_class(*klass) else: args = ()
# @todo: support for branches </s> tablename = "pr_unavailability"	PersonUnavailabilityModel ) def model(self): self.define_table(tablename, self.pr_person_id(ondelete = "CASCADE"),
# todo verify the stream </s> def h264_options(request):	h264_options ))
# todo: consider using eafp here instead. </s> return hasattr(obj, '__call__')	_is_callable
# todo xxx graalvm change </s> server_context.set_servername_callback(none)	test_sni_callback self.assertEqual(calls, [(None, server_context)]) self.check_common_name(stats, SIGNED_CERTFILE_HOSTNAME) stats = server_params_test(client_context, server_context, chatty=True,
##? value()  --- todo fix support for tuple assignment </s> pass	Value
# todo: delet this when all preprintproviders have a mapping </s> else:	banner_path return '/static/img/preprint_providers/{}'.format(self.logo_name)
@unittest.skip('not written')  # todo: finish! </s> def test_simplenamespace(self):	test_SimpleNamespace @unittest.skip('not written')  # TODO: finish! raise NotImplementedError
# todo use abi </s> r = extractref(self, snapshot)	extract_refs self.logger.info("Extract references from snapshots") for i, snapshot in enumerate(self.trace): if not r.run(): self.learnexceptiontext += r.replayexception
# todo: only consider the right ones for sources and targets </s> if propunit.isblank():	convertstore if pounit is "discard": continue targetheader.addnote("\n".join(waitingcomments).rstrip(), "developer", position="prepend")
# todo: is this used? </s> raise backenderror("number of items in mongo document field reference"	coalesce_physical ref.get("extract") )
# todo: currently stubs internal method, should provide stub </s> _, status = yield from utils.simple_get(url, self.loop)	test_failed_pairing def test_failed_pairing(self): yield from self.pairing.start(self.zeroconf) self.assertEqual(status, 500)
# todo: make treecount configurable via an inputslot </s> assert rf.labelcount() == nlabels, "error: oppredictrandomforest, labelcount differs from true labelcount! %r vs. %r" % (rf.labelcount(), nlabels)	OpPredictRandomForest key = roi.toSlice() nlabels=self.inputs["LabelsCount"].value newKey = key[:-1] newKey += (slice(0,self.inputs["Image"].shape[-1],None),)
# todo: must be implemented </s> def should_fetch_kindlegen():	should_fetch_kindlegen
# todo: supporting fat binaries will be annoying. </s> raises |gyperror| if the command return with a non-zero return code."""	GetStdoutQuiet def GetStdoutQuiet(cmdlist): job = subprocess.Popen(cmdlist, stdout=subprocess.PIPE, stderr=subprocess.PIPE) out = job.communicate()[0]
# todo test </s> return list(self)	parser_keys
# todo: clean up this event print out. we probably want something </s> arg = self.opts.get('fun', none)	print_docs def print_docs(self): docs = super(Runner, self).get_docs(arg) for fun in sorted(docs):
# todo order this!!!!!! </s> dtype = [("index", numpy.int32), ("x", numpy.float64, (3,))]	_write_nodes def _write_nodes(fh, points, write_binary): fh.write("Begin Nodes\n".encode("utf-8")) tmp = numpy.empty(len(points), dtype=dtype) tmp["index"] = 1 + numpy.arange(len(points))
self.button_align_test = wx.button(self, label="align test")    # todo maybe align left? </s> self.button_align_test = wx.button(self, label="align test")    # todo maybe align left?	WallpaperPanel self.sizer_setting_span_mode = wx.StaticBoxSizer(wx.VERTICAL, self, "Span mode") self.sizer_setting_slideshow = wx.StaticBoxSizer(wx.VERTICAL, self, "Slideshow") self.button_help = wx.Button(self, label="Help")                # TODO maybe align left?
# todo: where to store config? </s> response = {}	process_transfer if request.user.id: transfer = models.Transfer.objects.get(uuid=transfer_uuid)
# todo: make handlers for modelfactoryevent from within the gui obj </s> recent_files = self.properties.get('recent-files', [])	load_recent filename = recent_files[index] self._load(filename)
# todo optimize: special case where there is only one dynamic </s> def is_complex_type(self):	is_complex_type
# todo integrate param when g is a clustered graph </s> if g.directed:	plot_graph@10 def plot_graph(G): if True: raise NotImplementedError('TODO') if G.coords.shape[1] == 2:
#todo: add csrf here? or make ids uuid so they can't be guessed? </s> settings.relay_from_address	_generate_relay_From def _generate_relay_From(original_from_address): ) return relay_from_address, '%s via Firefox Private Relay' % (
#todo set the correct varname here </s> f = open(self.filename, "r")	BaseFormat def load(self, filename=""): if(filename != ""): self.jsonTree = json.load(f) f.close()
# todo: get rid of this copout and find a way to deal </s> and maximum.	rescale_mid def rescale_mid(x, to=(0, 1), from_=None, mid=0): Parameters ----------
# todo: this should be made more flexibly to handle differeing params for xform submission </s> xform_saved.send(sender="couchforms", xform=xform)	post_xform_to_couch xform = XFormInstance.get(doc_id) for key, val in attachments.items(): return xform except Exception, e:
# todo(pfnet): implement crop function </s> s = (reduced_y_pred == y_true).mean()	accuracy_score reduced_y_pred = np.argmax(y_pred, axis=1)
# todo make this configurable </s> self.display_name = display_name  # type: str	MatrixUser class MatrixUser: def __init__(self, name, display_name): self.power_level = 0              # type: int self.nick_color = ""              # type: str
# todo(yanase): check values </s> def test_calculate_without_prior():	TestParzenEstimator assert len(mus) == 3 assert len(sigma) == 3 consider_prior = False consider_magic_clip = True
# todo: update graph references </s> return handle(self.get_handles_data()[name][1])	get_handle
# todo: figure out a way to not trigger the "action_auth_started" when </s> track_event(category_signup_flow, action_auth_started, "google")	KumaOAuth2LoginView http_referer = request.META.get("HTTP_REFERER") if http_referer: return super().dispatch(request)
self.prob.cleanup()  # closes recorders todo_recorder: need to implement a cleanup </s> case.original_path = os.getcwd()	_setup_test_case_converge_diverge case.recorder.options['record_responses'] = True case.recorder.options['record_objectives'] = True os.chdir(case.dir) prob.run()
# todo: test logging messages. </s> raise message	MockTransport def deliver(self, message): self.messages.append(message) def shutdown(self): self.states.append('stopped')
# todo fix the fold to allow any number of dispositions </s> :return: str: returns a multi-line string description of the results	chart_descriptions :param suffix: str: The assumption is that the charts have similar column names. The census chart adds " Census" to the column names. messages = [] cols = ["hospitalized", "icu", "ventilated"]
# todo: make the get_closest_value to return region </s> def closest_to_caret(item, caret):	closest_to_caret
# todo: will work only if table.fields is ordereddict </s> def locale_context(name, category=locale.lc_all):	locale_context old_name = locale.getlocale(category) locale.setlocale(category, name)
1  # todo: fill in identifier </s> cumulative=stat.tsub,	print_stats recursion=recursion, name=stat.name, total=stat.ttot, average=stat.tavg,
# todo: make daemon? </s> return list(self._received)	received @property
# todo: refactor into one function that just takes nodes </s> url = self.helpers.analyzerequest(request_response).geturl()	doPassiveScan request = self.helpers.analyzeRequest(raw_request) response = self.helpers.analyzeResponse(raw_response) vuln_parameters = self.issues.check_parameters(self.helpers, parameters) is_not_empty = len(vuln_parameters) > 0
# todo ? </s> return 'value'	test_property_subclass_wrapped_classlevel def method1(self): return "method1" prop = myprop(lambda self:None) Foo.foo = prop
# todo: check whether loaded network has the same number of classes as specified in ilastik! </s> h5py_group[self.hdf5_group_filename] = self._filename	serialize_hdf5
assert isinstance(obj_type, instance)  # todo more flexible </s> if d.kind == nodes.ldef:	visit_var_def def visit_var_def(self, d: VarDef) -> int: assert len(d.items) == 1 reg = self.add_local(var) if d.init:
# todo(stevemar): remove the line below when we support multitenancy </s> return	DeleteProject parsed_args.project, )
# todo find out what's going on </s> def maeztu_sainz():	maeztu_sainz@19
# todo(b/141131288): enable complex-valued sorts on tpu. </s> ]	testSliceGrad [(7, 5, 3), (4, 0, 1), (7, 1, 3), None], [(5, 3), (1, 1), (2, 1), (1, 1)], for dtype in float_dtypes for rng_factory in [jtu.rand_default]))
# todo(blk-u): this doesn't look like it works as expected. </s> exceptions.invalidcredentials, auth.keystonev3credentials, **attrs)	testProjectTenantCollision def testProjectTenantCollision(self): attrs = {'tenant_id': 'tenant', 'project_id': 'project'} attrs = {'tenant_name': 'tenant', 'project_name': 'project'} self.assertRaises(
# todo: content-type </s> must be overriden by subclass.	HandleRequest raise NotImplementedError()
# todo(dougwig) - remove this disable when fixing bug #1816874 </s> rtr_2_fip, fip_2_rtr = self.rtr_fip_subnet.get_pair()	floating_ip_removed_dist self.rtr_fip_subnet = self.fip_ns.local_subnets.lookup( self.router_id) fip_ns_name = self.fip_ns.get_name() self._remove_floating_ip_rule(floating_ip)
# # todo: should be able to just access the api from qml. </s> if not self.__additional_components_view:	_createAdditionalComponentsView return path = os.path.join(plugin_path, "resources/qml/UM3InfoComponents.qml") Logger.log("w", "Could not create ui components for UM3.") return
# todo: reimplement this. </s> for i in range(count):	_vi_k_select if mode != modes.SELECT: utils.blink() self.view.window().run_command('soft_undo') return
# todo: add ssl verification </s> )	create_job_object spec=spec,
# todo test for final </s> def canonical_dir(self, ignore_args = false):	canonical_dir
pass  # todo </s> render('dot', 'pdf', '')	test_render_missingfile with pytest.raises(subprocess.CalledProcessError) as e:
# todo: maybe[int] and maybe[simple_sum] are invalid </s> self.emit('', depth)	_EmitDict for k in sorted(d): self.Emit('%d: %r,' % (k, d[k]), depth + 1)
# todo: check for existing record first. </s> class slugs(base, sessionpropertymixin):	Slugs
# todo(stephenfin): remove the hardcoded limit, possibly overridding </s> {'uuid': base.libvirtneutronfixture.network_1['id']},	test_rebuild_server_with_network_affinity extra_spec = {'hw:numa_nodes': '1'} flavor_id = self._create_flavor(extra_spec=extra_spec) ] server = self._test_create_server_with_networks(flavor_id, networks)
# todo: abstract this away into a function. </s> else:	eval_full_command self.view.run_command('maybe_mark_undo_groups_for_gluing') self.view.run_command('vi_run', vi_cmd_data) if vi_cmd_data['_exit_mode'] == MODE_INSERT: utils.blink()
# todo: this is untested. </s> result = callback(connection, cert, error_number, error_depth, ok)	_VerifyHelper error_number = _lib.X509_STORE_CTX_get_error(store_ctx) error_depth = _lib.X509_STORE_CTX_get_error_depth(store_ctx) except Exception as e: self._problems.append(e)
# todo: cache properly </s> if self.multiplier_field:	EnergyPointRule def apply(self, doc): if frappe.safe_eval(self.condition, None, {'doc': doc.as_dict()}): multiplier = doc.get(self.multiplier_field, 1) points = self.points * multiplier
# todo: cannot be loaded with plugins; improve this solution </s> name = slugify(name, separator="_")	_slugify_name MAX_LENGTH = 128 VALID_NAME = r"^[a-zA-Z_]\w{0,%d}$" % (MAX_LENGTH - 1) return name[:MAX_LENGTH]
#todo : what if multiple projects are selected ? </s> self.refresh_list()	on_delete_confirm pr = self.projects[self.tid_todelete[0]][1] pr.delete_task(self.tid_todelete[1])
# todo extend to nonbinary nodes </s> return int(bin(i)[2:].zfill(number_of_nodes)[::-1], 2)	cyphi_index2matlab_index 1 >>> cyphi_index2matlab_index(6, number_of_nodes)
# todo: enhance and use textlib.multitemplatematchbuilder </s> changed = article.change_category(oldcat, newcat,	move_contents found, moved = 0, 0 for article in oldCat.members(): summary=summary) if changed:
# todo: speed up by initalising devicemanager() once - param maybe? </s> "logo": "lighting_logo",	_get_zone_capability_prefix by zone. return { "scroll": "lighting_scroll", "backlight": "lighting_backlight",
# todo replace with collections.sequence subclass </s> spotify.error.maybe_raise(self.error)	did_you_mean def did_you_mean(self): suggestion exists. did_you_mean = utils.to_unicode( lib.sp_search_did_you_mean(self._sp_search))
# todo how to handle this when parent class has no embedders? </s> p = self.get_p_embedder().embed(p + self.dataset.num_relations)	score_po def score_po(self, p, o): o = self.get_o_embedder().embed(o) return self._scorer.score_emb(o, p, all_subjects, combine="sp*")
# todo(dcramer): it'd be nice to support more than this, but its </s> def list_project_issues_scenario(runner):	list_project_issues_scenario project = runner.default_project runner.request(
# todo: actually apply </s> else:	Preferences self.stdscr.hline(self.rows-3,self.div_off+1,"_",self.cols) if self.active_zone != ZONE.ACTIONS: if self.cur_action == 0: self.add_string(self.rows-2,"[{!black,white,bold!}Cancel{!white,black!}] [Apply] [OK]",col=c)
# todo: write tests </s> def features(self):	features @readOnlyCachedProperty
# todo(slaweq): remove that is_admin check and always perform rules checks </s> attribute, target[attribute_name]):	_build_match_rule attr_rule = policy.RuleCheck( 'rule', '%s:%s' % (action, attribute_name)) attr_rule = policy.AndCheck( [attr_rule, _build_subattr_match_rule(
# todo: maybe introduce a "closest_wall_or_door_given_dir" function to decide between right and left </s> if subgoal == 'updatestackifnecessary':	take_action_iterate self.stack.append(('GoNextTo', adj_pos)) return False The only reason this should be used (for now) would be when we need to unblock the way by moving an object that we should have gone
# todo: need to cleanup the named argument mess before it is possible. </s> constant_nodes = []	CPythonExpressionFunctionCall elif star_list_arg.isExpressionConstantRef(): if star_list_arg.isKnownToBeIterable( count = None ): for constant in star_list_arg.getConstant(): constant_nodes.append(
# enforce no minor ticks labels. todo: document? </s> self.spines['left'].set_visible(true)	_alty_overrides if self._alty_child is not None: self._shared_x_axes.join(self, self._alty_child) self.yaxis.tick_left() self.yaxis.set_label_position('left')
# todo: checking for sys.stdout.isatty() prevents shell redirections and pipes (useful for list commands). can we remove this check? </s> else:	_truncate def _truncate(self, text, max_length, omission="..."): if len(text) <= max_length: return "%s%s" % (text[0:max_length - len(omission) - 5], omission)
# todo tags: ipdb </s> self._readers -= 1	reader_release def reader_release(self): with self._rlock: if self._readers == 0: self._no_readers.set()
#todo change to native framework call, when plex allows token in header </s> result[id] = {}	playlistsV3 result = {} for playlist in playlists: result[id]['title'] = playlist.get('title') result[id]['summary'] = playlist.get('summary')
# todo: need to add counter </s> return [self.convert_to_user_id(user) for user in user_ids]	_get_user_ids if isinstance(user_ids, str): user_ids = self.convert_to_user_id(user_ids)
none, fifo_mode=false,  # todo python3: fifo mode fails in py3, needs fix </s> result_store=none	NorefFeatureExtractorTest self.fextractor = MomentNorefFeatureExtractor( [asset], ) self.fextractor.run()
# todo: detect if this untracked pending transaction is a commitment transaction at all. </s> economics=self.economics)	StakeList stake_info=stake_info, staking_agent=self.staking_agent, if onchain_stake.first_locked_period: if onchain_stake.first_locked_period < initial_period:
# todo: finish me </s> if not xlsx_name:	match_location def match_location(domain, xlsx_name, location_type=None): return None, None try:
# todo: find these references and ensure they are closed </s> info[role] = parts[-1]	blame elif firstpart.endswith('-time'): info["%s_date" % role] = int(parts[-1]) else: if firstpart.startswith('filename'):
# todo return empty list if not loaded </s> def did_you_mean(self):	did_you_mean suggestion exists. Will always return :class:`None` if the search isn't loaded.
# todo(rbharath): this should be modified to contain a cluster split so </s> 'time': deepchem.splits.timesplitterpdbbind(dataset.ids)	load_pdbbind_grid splitters = { 'index': deepchem.splits.IndexSplitter(), } splitter = splitters[split]
# todo discont: use offsets instead (note need for int conversion) </s> s += '%s</label>' % escape(dt)	__generate_input_and_label else: key_offset = -1 else: s += '%s<span class="accesskey">%s</span>%s</label>' % (escape(dt[:key_offset]), escape(dt[key_offset:key_offset+1]), escape(dt[key_offset+1:]))
# todo: hints </s> '/' + cfg['build']['build_id'] +	record_config if cfg['build']['delivery'] in ('docker'): self.send_file(cfg['build']['build_db_dir'] + '.cfg', shutit_util.print_config(cfg))
# todo: consider paging for large result sets </s> for item in _input:	pipe_yql@37 url = "http://query.yahooapis.com/v1/public/yql" conf = DotDict(conf) item = DotDict(item) yql = util.get_value(query, item, **kwargs)
# todo: this probably needs app_id too </s> return (casees()	get_case_export_base_query .domain(domain) .case_type(case_type)
# todo: https://github.com/microsoft/ptvsd/issues/137 </s> self.assert_vsc_received(received, [	test_few ) self.send_request() self.expected_response( threads=[
# todo: fix this, this is one of the few cases where using the config </s> self.config.get('filter_by_tags_include_empty_key'))	_make_test_suite_loader suite, self.config.get('filter_by_tags'), except loader.LoaderUnhandledReferenceError as details: raise exceptions.OptionValidationError(details)
# todo: should not apply if the exception is from _run_command </s> return len(key.value)	zcard @command((Key(ZSet),))
# todo: only return distribute if setuptools < 0.7 </s> module = sys.modules.get(packagename)	_handle_ns loader = importer.find_module(packageName) if loader is None: if module is None: module = sys.modules[packageName] = types.ModuleType(packageName)
# todo: https://github.com/horovod/horovod/issues/2438 </s> address_info = ray.init(num_cpus=4)	ray_start_4_cpus @pytest.fixture yield address_info ray.shutdown()
# todo generator </s> if ap.get('type', none) != 'dataset' or ap['path'] == ds.path]	Drop ds = Dataset(ds_path) handle_dirty_dataset(ds, mode=if_dirty) if not content: continue
# todo: wrap backend call in error handling. </s> returns a :class:`mopidy.models.tltrack` or :class:`none`.	get_current_tl_track def get_current_tl_track(self):
# todo: review </s> r = r.unsqueeze(1)	batch_minmax_norm mins = torch.min(x, dim=2)[0] maxs = torch.max(x, dim=2)[0] x = R * (x - mins.unsqueeze(1)) + out_min return x
# todo: deal with the event in which create_circuit fails after the first_hop has been selected </s> yield message	check_pong if not request: yield DropMessage(message, "invalid ping identifier")
# todo(asalkeld) support versions </s> sh = serviceshandler(s, resource=self.resource, hooks=hooks)	cfn_hup if self._is_local_metadata: self._config = self._metadata["config"] sh.monitor_services()
# todo: there's a race with the initial "output" event. </s> if content is not none:	set_module def set_module(self, name, content=None): self.write_module(name, content) self.workspace.install()
# todo: convert non uris to file uris. </s> print repr(typefind.peek(0, 11) == b'[playlist]\n')	detect_pls_header return typefind.peek(0, 11) == b'[playlist]\n'
# todo semantic validation </s> results = self.client.list_server_logs(fqdn)	list_server_logs if not results: return Response(status=status.HTTP_503_SERVICE_UNAVAILABLE)
#todo: search text as well as title, figure out best way to sort results </s> sqlquery = note_tag_search.format(query)	search_notes_by_tag return run_query(wf, log, sqlQuery)
raise notimplementederror  # todo </s> self.command, self.next_command = self.next_command, none	communicate def previous_command_finished(_): if self.command is not None: if self.command is not None: cmd = self.command
# todo: this doesn't clean up the index </s> from sentry.models import group	delete_group @task(name='sentry.tasks.deletion.delete_group', queue='cleanup') try: g = Group.objects.get(id=object_id)
# todo: this is slow, we should have a better accessor </s> assert " " not in t, "internal error: space in storage form"	__generate_input_and_label def __generate_input_and_label(t, dt, keymap, indent, disabled, prefix): if not disabled: dstr = ""
# todo move the file obj into the decrypt to do streaming </s> self.private_key = private_key	AsymmetricCryptoContext def __init__(self, public_key: Optional[_RSAPublicKey] = None, private_key: Optional[_RSAPrivateKey] = None, loop: Optional[asyncio.AbstractEventLoop] = None): self._loop = loop if not loop:
# todo: include args in description </s> objects.  returns a list of searchmatchset objects, one for each	check_consistency def check_consistency(ann_objs, restrict_types=[], ignore_types=[], nested_types=[]): checked criterion that generated matches for the search. match_sets = []
# todo: it can be also: </s> def pred_eq(a):	make_pred_eq @expose(name, [values.W_Object], simple=True)
# todo featureparams nameids </s> return self	mergeLangSyses self.ReqFeatureIndex = 0xFFFF self.FeatureIndex = mergeFeatureLists([l.FeatureIndex for l in lst if l.FeatureIndex])
# todo: implement '-view ' </s> import shutil	cleanup_out_directory shutil.rmtree(dir_name)
#todo: redo this with html parser instead of regex </s> if tag == 'title':	handle_starttag self.state = 'title' if tag == 'magnet' and self.state == 'matched':
# todo: check </s> addr.info = info	lookup_address if sect: addr.section = sect return addr
### todo: method to register this system as brand new </s> time_elapsed = time.time() - install_start	CobblerInterface install_successful = not system_info.get('netboot_enabled') if install_successful: if not install_successful: raise error.HostInstallTimeoutError('Machine %s install '
pass  # todo: finish implementation </s> warnings.warn('use browser.driver.title or browser.get(query.title) instead', deprecationwarning)	title return self.driver.title
# todo handle when something happens during writing of data. </s> self._connect_platform)	starting self.vip.pubsub.subscribe('pubsub', '/platform',
# todo: and results </s> self._name = name	Value class Value(object): def __call__(self, args): return args[self._name]
# @todo: add more comprehensive show validation </s> return self._genericmessage('error', 'unable to find the specified show')	subtitleShow return self._genericMessage('Error', 'Invalid show ID') show_obj = Show.find(app.showList, int(show)) app.showQueueScheduler.action.download_subtitles(show_obj) time.sleep(cpu_presets[app.CPU_PRESET])
replace = re.sub('\$(\d+)', r'\\\1', replace)   #map $1 to \1 etc.   #todo: also need to escape any existing \1 etc. </s> yields (_output):	pipe_strregex@29 kwargs -- other inputs, e.g. to feed terminals for rule values conf: source item after replacing values matching regexes rules = []
# todo: decide which one to use </s> returs instance of `model`. """	model_from_path def model_from_path(path):
# todo(inf) de-wx!, needed for wx3, check if needed in phoenix </s> self.updateuibuttons()	OnComboBox def OnComboBox(self,event): color_key = self.GetColorKey() help = colorInfo[color_key][1]
# todo complete this method </s> cput0 = (time.clock(), time.time())	make_ip def make_ip(self): if not self._made_shared: kconserv = self.kconserv t1, t2, eris = self.t1, self.t2, self.eris
# todo disconnect dealer/router </s> print("message received world: {}".format(msg))	msg_sub
options['taskid'] = none # todo </s> def uninstall(self, id, units, options={}):	ConsumerContentManager options['taskid'] = None # TODO agent = PulpAgent(consumer) Install content on a consumer. @param id: A consumer id.
# todo: test this </s> if name not in style:	get_value def get_value(style, name): return 'initial' values = style[name]
if self.is_direct_mode() or not allow_quick:  # todo: thin mode </s> self._batched.close()	precommit def precommit(self):
# todo: move update_spec to worker. agent should not hold these execution details. </s> summary_spec (optional[dict]): spec-dict to specify summary settings.	DQNAgent optimizer_spec (Optional[dict,Optimizer]): The spec-dict to create the Optimizer for this Agent. observe_spec (Optional[dict]): Spec-dict to specify `Agent.observe()` settings. saver_spec (Optional[dict]): Spec-dict to specify saver settings. auto_build (Optional[bool]): If True (default), immediately builds the graph using the agent's
# todo(pts): unify /encoding dicts globally, not only for </s> objs[obj_num].appendto(output, obj_num)	ParseType1CFonts type1_size = 0 for obj_num in sorted(objs): output.append('(Type1CParser: all OK\\n) print flush\n%%EOF\n') output_str = ''.join(output)
# todo: handle string array reflection </s> for i in numba.parfor.internal_prange(len(a)):	_column_fillna_impl s = B[i] if hpat.hiframes_api.isna(B, i):
# todo: it seems that yahoo! converts relative links to </s> from -- string from where to start the input	pipe_fetchpage@41 _INPUT -- not used since this does not have inputs. conf: to -- string to limit the input token -- if present, split the input on this token to generate items
# todo: pandas formula is better or welford? </s> out_vars.append(aggregate_node.out_key_var)	aggregate_array_analysis all_shapes = [] out_vars = list(aggregate_node.df_out_vars.values()) for col_var in out_vars: typ = typemap[col_var.name]
# todo: g+ has a multiply-valued 'urls' field. ignoring for now because </s> return existing	get_or_save def get_or_save(self): existing = self.key.get() obj = json.loads(self.response_json)
## \todo adjust for #1438. </s> with c2 :	testDefaultFormat c2[GafferImage.Format.defaultFormatContextName] = GafferImage.FormatData( f2 ) with c1 : self.assertEqual( r["out"]["format"].getValue(), f2 )
# todo: use valid_episodes.mask for mean </s> grad_kl_v = torch.dot(flat_grad_kl, vector)	_product grads = torch.autograd.grad(kl, self.policy.parameters(), create_graph=True) grad2s = torch.autograd.grad(grad_kl_v, self.policy.parameters()) flat_grad2_kl = torch.cat([grad.contiguous().view(-1)
raise deprecatedtest # this test is now broken. todo: fix it. </s> remote_prop = property(item, relation='many-to-one', remote_ap='test_ap')	make_test_second_ap def make_test_second_ap():
# todov06 updatetablerow instead </s> self._successregex = successregex	MessageEntryData self._url = url self._name = name self._deleted = deleted return
# todo(solitude): remove this. </s> all_apps = request.amo_user.addons.filter(type=amo.addon_webapp)	activity_log def activity_log(request, userid):
invoice.objects.filter(subscription=sub).latest('date_due').date_due # todo - check query </s> return dict(credit_form=creditform(subscription.id, false),	EditSubscriptionView name = 'edit_subscription' def get_context_data(self): credit_list=None, form=SubscriptionForm(subscription),
# todo add negative sampling? </s> compute cosine similarity between two words.	similarity Example:: >>> trained_model.similarity('woman', 'man')
# todo: this is untested. </s> return name	_get_name if name._name == _ffi.NULL: 1/0
limit = 20  # todo: change to setting </s> md = markdown(escape=true, hard_wrap=true)	test_markdown_emoji def test_markdown_emoji(self): markdown emojify comment_md = md.render(comment) self.assertEqual(comment_md, '<p><img class="comment-emoji" src="%(static)sspirit/emojis/airplane.png">, '
# todo: add more field checks here </s> cache.clear()	SoundOfTheDayTestCase class SoundOfTheDayTestCase(TestCase): fixtures = ['sounds', 'email_preference_type'] def test_no_random_sound(self): random_sound_id = get_sound_of_the_day_id()
# todo(sdague): remove in juno </s> try:	_dispatch_events def _dispatch_events(self): Blocks until native thread indicates some events _c = self._event_notify_recv.read(1) assert _c
# todo, pass complete checkpoint as state dictionary </s> self.__save_end_of_training_weights(model, trainer)	tpu_train_in_process self.__setup_tpu_training(model, trainer) self.trainer.train_loop.setup_training(model) self.transfer_distrib_spawn_state_on_fit_end(model, mp_queue, results)
# todo(akshayka): remove the in_graph_mode check once caching devices are </s> if output_is_sequence else x[0])	output_pack def output_pack(x):
# todo make this smarter about more complex configurations (backup original values, etc) </s> if ok:	Parser else: obj_doc['dps'][switch_found]['interfaces'][self.mirror_ports[switch_found]]['mirror'] = [ if action == 'mirror': obj_doc['dps'][switch_found]['timeout'] = self.reinvestigation_frequency
# todo xxx graalvm change </s> self.check_common_name(stats, signed_certfile_hostname)	test_sni_callback sni_name='notfunny')
#todo load this from somewhere </s> "memory %s" % mem_usage if mem_usage else none	print_process "elapsed %s" % hms(start_elapsed), "exp. remaining %s" % remaining, ] print >> log.v5, ", ".join(filter(None, info))
# todo(b/114938612): eventually remove this override. </s> os.path.join(testdata_path,	TaxiUtilsTest 'train/transformed_exmaples.gz'), coder=beam.coders.BytesCoder())) 'transform_output/transformed_metadata/schema.pbtxt'), schema_pb2.Schema())
# todo implement some tolerance </s> win32api.closehandle(handle)	test_get_num_handles self.assertEqual(after, before+1)
# todo(sharadmv): don't drop the custom derivative rule </s> raise typeerror(f"expected a callable value, got {fun}")	_check_callable def _check_callable(fun): if inspect.isgeneratorfunction(fun): raise TypeError(f"Expected a function, got a generator function: {fun}")
# todo(mattjj): revise this approach </s> next_t = t + dt	_odeint def body_fun(state): i, y, f, t, dt, last_t, interp_coeff = state error_ratios = error_ratio(next_y_error, rtol, atol, y, next_y) new_interp_coeff = interp_fit_dopri(y, next_y, k, dt)
# todo: remove in 1.2 </s> list of the leaf nodes.	_get_leaves Returns ------- leaf_ptr = self.dummy_leaf_.next_leaf_ leaves = []
# todo: if a logographic word list is added to electrum2, might need to revisit this (no spaces) </s> def _config_mnemonic(self, mnemonic_guess, lang, expected_len):	WalletBIP39 self._derivation_salt = "mnemonic" + self._unicode_to_bytes(passphrase) self._config_mnemonic(mnemonic_guess, lang, expected_len) if not mnemonic_guess: init_gui()
# todo: remove when we stop using buildbot. </s> existing_notes = self.get_job_note_list(job_id)	update_after_autoclassification if not settings.AUTOCLASSIFY_JOBS: return if existing_notes: return
# :todo: implement test. </s> self.skiptest('not implemented yet.')	FindTransactionsRequestFilterTestCase def test_pass_addresses_only(self): self.skipTest('Not implemented yet.') def test_pass_approvees_only(self): self.skipTest('Not implemented yet.')
# todo(metzman): change the strategy to record which plugin was used, and </s> return engine_common.get_overridable_timeout(10 * 60,	get_new_testcase_mutations_timeout def get_new_testcase_mutations_timeout():
# todo: more error handling </s> self.async_seconds = int(ds.get('async', 0))  # not async by default	__init__@32 if self.name is None: self.name = self.action self.async_poll_interval = int(ds.get('poll', 10))  # default poll = 10 seconds self.notify = ds.get('notify', [])
# todo: insert or make chord </s> class scorereductionexception(exception):	ScoreReductionException
#todo - oauth </s> tried:	get_unicode_from_response def get_unicode_from_response(r): 1. charset from content-type 2. every encodings from ``<meta ... charset=XXX>``
# todo: replace this with proper storage reporting </s> def getrequest(self, cfg):	IAReqInstanceAlloc return 2 else: The checks for the completeness of the opcode must have already been done.
# todo: register a custom error handling function to replace </s> elif filter_type == cls.shell:	check_filter def check_filter(cls, filter_string, filter_type): if filter_type == cls.REGEX: compiled = cls._compile_shell_pattern(filter_string) return compiled is not None
# todo: this needs refactoring </s> if species.startswith(genus.capitalize()):	in_genus_list def in_genus_list(species, genus_list): return True return False
# todo: hydrate directly instead of via http hydration </s> data = {"self": "%srelationship/%s" % (self.graph_uri, u_rel.id),	Py2neoPackStreamValueSystem rel = Relationship.hydrate(data) else: "start": "%snode/%s" % (self.graph_uri, remote(next_node)._id), "end": "%snode/%s" % (self.graph_uri, remote(last_node)._id),
# todo username </s> '--name=mon.',	create_mon 'ceph-authtool', keyring, '--add-key={monitor_secret}'.format( monitor_secret=monitor_secret,
# todo: migrate to new tilegrid format via library. </s> frames[frame_addr][word_addr] &= 0xffffffff ^ (1 << bit_index)	frame_clear
# todo: handle fixed and % widths </s> widest_line = max(widest_line, current_line)	inline_preferred_width if len(lines) > 2: widest_line = max(widest_line, max(lines[1:-1])) return widest_line
# todo: give a vanilla example </s> fpr^{(n)} = \\frac{fp}{\\left ( fp + tn \\right )}	tpr_fpr@255 def tpr_fpr(predicted_states, ground_truth_states): .. math:: Attributes ----------
# todo: this case is not fused! </s> assert len([n for n in e if isinstance(n.op, join)]) == 0	test_local_join_1 val = f([1]) assert numpy.all(val == [1]) assert f.maker.env.outputs[0].dtype == config.floatX a = TT.matrix('a')
# todo postremora replace the above with this line when remora goes away </s> u = get_object_or_404(userprofile, email=email)	ajax @json_view def ajax(request): return dict(id=u.id, name=u.name)
# todo(efried): due to bug #1782386, swap is not being reported. </s> rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)	test_flavor_image_traits_based_scheduling def test_flavor_image_traits_based_scheduling(self): AND a required trait on the image ends up on the single compute node self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX', 'HW_CPU_X86_SGX'])
# tracks a suggested todo, which will reduce the 3 rpc calls here to only </s> pass	RaidenError
# todo should we pass? </s> number_of_records)	retrieve_most_recent_cached_data return self.sqlite_cursor.execute("SELECT * FROM ? WHERE REQUEST_TIME = (SELECT MAX(REQUEST_TIME) FROM ?) AND" " Location = ? ORDER BY REQUEST_TIME DECS LIMIT ?",
# todo check error message </s> article2.author = person	test_to_many article1 = self.Article(id=1) article2 = self.Article(id=2) self.session.add_all([person, article1, article2]) self.session.commit()
# todo: div by zero error if all data exists at a single point. </s> self.segments.append(tracklog.trkseg())	startElement self.points.append(TrackLog.Trkpt(attrs['lat'], attrs['lon'])) self.segments[-1].points.append(self.points[-1])
# todo private access </s> def __init__(self, executed_param_names):	DynamicExecutedParamName class DynamicExecutedParamName(ParamNameWrapper): super(DynamicExecutedParamName, self).__init__(executed_param_names[0]) self._executed_param_names = executed_param_names
# todo(hartikainen): once tfp.bijectors.chain supports conditioning, </s> event_dims=none,	ConditionalRealNVPFlow num_coupling_layers=2, hidden_layer_sizes=(64,), validate_args=False, name="conditional_real_nvp_flow"):
# todo(b/160795287): deprecate estimator based executor. </s> exporter = tf.estimator.finalexporter('iris', serving_receiver_fn)	trainer_fn train_spec = tf.estimator.TrainSpec( train_input_fn, max_steps=trainer_fn_args.train_steps) eval_spec = tf.estimator.EvalSpec( eval_input_fn,
# todo: use shlex.quote as soon as a newer python version is available. </s> rvm_cmd = os.path.expanduser('~/.rvm/bin/rvm-auto-ruby')	RubocopCommand def run(self, edit): self.check_file(self.view.file_name()) quoted_file_path = pipes.quote(file_path) rubocop_cmd = rvm_cmd + ' -S rubocop ' + quoted_file_path
# todo: support other deployment situations. </s> g.call_after_request = []	after_this_request def after_this_request(func): g.call_after_request.append(func) return func
assert self.restart_seq is none #todo: better handling of this situation </s> else:	on_terminate_program return p.on_terminate(status)
# todo: this test requires manifold access, see: t88318502 </s> c2_model(inputs)[0]["instances"]	TestCaffe2Export c2_model.save_protobuf(d) c2_model.save_graph(os.path.join(d, "test.svg"), inputs=copy.deepcopy(inputs)) ts_model = tracer.export_torchscript() ts_model.save(os.path.join(d, "model.ts"))
# todo remove backwards compatability fix in a future version </s> new_file = os.path.join(sublime.packages_path(), 'user', '.neovintageousrc')	_migrate_rcfile def _migrate_rcfile(): if os.path.exists(old_file): if os.path.exists(new_file):
# todo: use cli.output.write </s> def prase_input_complete(cli):	prase_input_complete
# todo: check if output is spent (still neccessary?) </s> tx = self.compose_request(path_type='explorer', variables=variables)	getrawtransaction variables = {'id': tx_id, 'hex': None}
#todo same issue with batch_size </s> graph.y=tf.one_hot(graph.y, self.config['y_dims'], 1.0, 0.0)	create_graph with tf.device(device): if 'y' in graph: if graph_type == 'full': tf_graph.create(graph)
# todo: requires special treatment? </s> def _process_game_entities(cls, full_data_set):	_process_game_entities for unit_line in full_data_set.unit_lines.values(): cls._unit_line_to_game_entity(unit_line)
# todo: make test method </s> finally:	test_gps droid.startLocating() try: droid.stopLocating()
# todo: only write this when the checksums file changed </s> filename, expected_checksum = get_name_and_checksum(checksums, '_genomic.gbff.gz')	has_gbk_file_changed full_filename = os.path.join(directory, filename) if not os.path.isfile(full_filename):
# todo: check that the birth date is not in the future </s> if len(number) == 9:	_get_birth_date year = 1900 + int(number[0:2]) month = int(number[2:4]) % 50 % 20 if year >= 1980: year -= 100
#todo: http://www.6502.buss.hk/6502-instruction-set/jmp says that there is a indirect </s> self.assertequals(1 , len(ast))	JmpTest self.assertEquals('T_ADDRESS', tokens[1]['type']) self.assertEquals('$1234', tokens[1]['value']) self.assertEquals('S_ABSOLUTE', ast[0]['type']) code = semantic(ast)
# todo: do not store message if the ajax client stats that it will not redirect </s> return _('an unexpected error has occured')	get_error_message def get_error_message(self, exception):
# todo: check output </s> else:	test_hierarchy_iprint output = run_model(prob) if self.comm.rank == 0: self.assertTrue(output.count('\nNL: Newton Converged') == 0)
# :todo: implement test. </s> verifies that the command is wired up correctly.	SendTransferCommandTestCase super(SendTransferCommandTestCase, self).setUp() self.adapter = MockAdapter() self.assertIsInstance( Iota(self.adapter).sendTransfer,
# todo: is wavelet centred properly? </s> sj = s0 * 2 ** (dj * np.arange(0, j + 1))	scales J = int((1 / dj) * np.log2(self.N * self.dt / s0))
# todo(b/186439691): remove when placer is fixed. </s> the leftover placeholders that can result from binding arguments to the	_get_wrapped_function_from_comp A TensorFlow ConcreteFunction. def function_to_wrap(): imported graphdef via `input_map`. The correct signature will be added to this function later, via the `prune` call below.
# todo: remove this when domain decomposition is merged </s> element = et.subelement(self._settings_file, "output")	create_output_subelement def create_output_subelement(self): for key in self._output: subelement = ET.SubElement(element, key)
# todo: return a list of chapters to download </s> return true  # true will download kindlegen in the user directory to use it	should_fetch_kindlegen
#todo: implement xml support </s> con.close()	Tenants "INSERT INTO tenants VALUES ('%s', '%s', %d)" % (tenant_id, tenant_desc, tenant_enabled)) elif content == 'application/xml': return "whatever, we don't have XML yet"
# todo:: fix typing in later version of numpy </s> if isinstance(cats, dataframe):	dummy_matrix dummy variable values cond : ndarray codes = np.column_stack([np.asarray(cats[c].cat.codes) for c in cats]) else:
# todo handling when g is a list of graphs </s> w.setwindowtitle(g.gtype)	pg_plot_graph@144 if G.coords.shape[1] == 2: adj = np.concatenate((np.expand_dims(ki, axis=1), np.expand_dims(kj, axis=1)), axis=1) v = w.addViewBox() v.setAspectLocked()
# todo: make sure this openssl command works everywhere, maybe we should use a text_base64_decode? </s> def package_upgrade_yum():	package_upgrade_yum
# todo get this from config </s> server_addr = '1.2.3.4'	get_my_ip non_open_port = 50000 s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
# @todo: test </s> d.display.refresh(self, stuff)	FittingView 3, so we must create an external-internal mapping self.Freeze() sFit = service.Fit.getInstance() fit = sFit.getFit(self.activeFitID)
# todo/perf: users_id iterates over all items of all collections </s> except oserror as e:	load_library_item try: with bpy.data.libraries.load(library_path, link=True) as (data_from, data_to): raise ExternalFileFailed from e try:
# todo(yanase): change `task` in storages to `direction`. </s> def system_attrs(self):	system_attrs return self.storage.get_study_system_attrs(self.study_id)
# todo(jay-lau-513) translate the contents to a json stdin </s> def service_delete(uuid):	service_delete LOG.debug("service_delete %s" % uuid) try:
# todo: try mlp rather than bilinear </s> def get_variables(self):	get_variables
# todo: another way to provide a (set of) potential defined object(s) </s> raise(asn1objerr('{0}: invalid value, {1!r}'.format(self.fullname(), val)))	_safechk_val elif isinstance(val[0], str_types): if re.match('_unk_[0-9]{1,}', val[0]): else: self._get_val_obj(val[0])._safechk_val(val[1])
# todo: handle situations where share is password protected </s> return self._session['servername']	getServerName
# todo have one global properties object so this is no longer necessary </s> }	LuxCoreNodeVolOutput "absorption": [100, 100, 100],
# todo: weather data source changed, temporarily disable, will enable it later when new data source is available. </s> station_bikes = station.bikes	_on_action_received if from_station_idx < 0 or to_station_idx < 0: continue executed_number = min(station_bikes, action.number) if executed_number > 0:
# todo: detect if this untracked pending transaction is a commitment transaction at all. </s> checksum_address: str = none,	StakeList @validate_checksum_address def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs)
# todo is_compiled_with_cuda() has not been moved </s> nvcc = subprocess.check_output([which,	_find_cuda_home if cuda_home is None: try: 'nvcc']).decode().rstrip('\r\n') cuda_home = os.path.dirname(os.path.dirname(nvcc))
# todo: test re-authentication when the token expires </s> test_data.test_uri,	test_can_list_reviews def test_can_list_reviews(self): test_data = phldef_conduit test_data.ALICE.user, test_data.ALICE.certificate)
# todo(b/171088214): remove it after the control dependency in </s> (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)	apply_gradients experimental_aggregate_gradients=True): grads, tvars = list(zip(*grads_and_vars)) return super(AdamWeightDecay, self).apply_gradients( zip(grads, tvars),
# todo username </s> disk = os.path.join('/dev', disk)	colon_separated raise argparse.ArgumentTypeError('must be in form HOST:DISK')
# todo: make truly async </s> def __init__(self):	__init__@10
# todo: fails because of missing svg support </s> td { height: 2px }	test_image_repeat_block table { border-collapse: collapse; margin: 2px } th, td { border: none; padding: 0 } img { display: block } </style>
# todo: determine between create and/or start? </s> def slow(self, container_names, state):	slow
# todo(jeremydw): thread pool. </s> logging.info('connecting to gcs...')	GoogleCloudStorageDeployment fp.close() def dump(self, pod): connection = boto.connect_gs(self.access_key, self.secret, is_secure=False) bucket = connection.get_bucket(self.bucket)
# todo log/warn </s> object. see the :ref:`module docs <loader-factory>` for more details."""	set_preferred_default_implementation def set_preferred_default_implementation( global _DEFAULT_FACTORY #pylint:disable=global-statement _DEFAULT_FACTORY = implementation_factory
# todo: handle exceptions </s> raise error if file's extension is not in the accept_format_list	check_file_format if accept_format_list: name, extension = os.path.splitext(file_name)
# todo document - how to deal with web media vs. normal? </s> return self.result.title	title @property def title(self):
# todo: is this the right str? </s> (10, 'spam'),	test_few def test_few(self): with self.launched(default_threads=False): (11, 'pydevd.eggs'), (12, ''),
# todo!! add more assertions for the smaller subsystems </s> assert len(mip.partitioned_constellation) == 1	standard_example_is_correct assert len(mip.unpartitioned_constellation) == 4
'type': 'string', #todo: resolve </s> def fetch_result_metadata(self):	fetch_result_metadata @query_error_handler
# todo(jaypipes): port nova's fault infrastructure </s> self.assertequals(v, images[0][k])	test_get_images images = res_dict['images'] self.assertEquals(len(images), 1)
# todo: fix 'extract_primitives' so that the value of 'material' is none and not empty string </s> modifiers: optional[bpy.types.objectmodifiers],	gather_primitives def gather_primitives( blender_mesh: bpy.types.Mesh, export_settings ) -> List[gltf2_io.MeshPrimitive]:
# @todo: return only packages for the current architecture </s> self.uri = f'/rpc/?{self.params}'	get_task def get_task(self, loop):
# todo: will need some code for the tabs active terms to work </s> gtk.main_quit()	deregister_window err('%s is not in registered window list' % window) if len(self.windows) == 0:
# todo legacy method to be removed/refactored </s> from corehq.apps.orgs.models import organization	get_organizations return filter(None, [Organization.get_by_name(org) for org in self.organizations])
federated_only=self.federated_only,  # todo: 466 </s> self.config_root = config_root	NodeConfiguration config_root = constants.UNINITIALIZED_CONFIGURATION else: self.config_file_location = config_file_location self.is_me = is_me
# todo error on missing levels </s> self.sizes = sizes	_LinePlotter self.palette = palette self.dashes = dashes def plot(self, ax, kws): orig_color = kws.pop("color", None)
#        #todo: refresh all k-buckets further away than this node's closest neighbour </s> setattr(item, self.attr, self.key(item))	_cacheValues def _cacheValues(self):
# todo: remove .nocache() when iterator() is dropped </s> def cache(self, *args, **kwargs):	cache
#todo use logging </s> model.set(iter, column_display, '<b>%s</b>\n%s' % (appname, desc))	do_app_unchanged
# todo: validate and mask this before it's selected </s> uninstallation of older slots when appropriate.	_greedy_slots differ from the slot of the highest visible match. When blocker_lookahead is True, slot atoms that would trigger a blocker highest_pkg, in_graph = self._select_package(root_config.root, atom) if highest_pkg is None:
# todo generator </s> in subdirectories within a dataset as always done automatically. an	Drop a common (super)dataset is given explicitly, the given paths are interpreted relative to this dataset. optional recursion limit is applied relative to each given input path. By default, the availability of at least one remote copy is verified,
# todo: for training data (but not validation), do data augmentation here. </s> height = int(bbox[3])	_get_planetlab_details x = int(bbox[0]) y = int(bbox[1]) bboxes.append({ "left": x,
# todo: remove in 0.7.0 </s> self._parent = value	parent if value is not None:
# todo - override the 'create' method for this view, </s> allows greater-than / less-than filtering for stock quantity	StockFilter min_stock = NumberFilter(name='quantity', lookup_expr='gte') max_stock = NumberFilter(name='quantity', lookup_expr='lte')
# todo should this reset the pts and such? </s> with self._updates_lock:	UpdateState def set_polling(self, polling): self._polling = polling self._updates.clear() polling = property(fget=get_polling, fset=set_polling)
# todo: is this recalculation really necessary? (see below as well) </s> return v - j.dot(j.transpose()).dot(v)	projNullSpace def projNullSpace(self, J, v): else: return v
# todo(vek): need to pass context in for access to auth_token </s> self.state = state	InstanceInfo def __init__(self, name, state): self.name = name
# todo: move to base class </s> arr4 = [i[3] for i in rectangles]	getNodesRect arr1 = [i[0] for i in rectangles] arr2 = [i[2] for i in rectangles] if any([len(arr1) == 0, len(arr2) == 0, len(arr3) == 0, len(arr4) == 0]): return None
self.parent().hide() # todo: make configurable </s> except attributeerror:	translateUI def translateUI(self): try: pass # not in Qt 4.6
# todo placeholder; implement </s> "name",	RoleInfo class RoleInfo: "role_world_size", "local_world_size",
# steps = 0 # todo </s> self.matrices += matrix	Duplis self.matrices = matrix self.count = 1 self.count += 1
# # todo: mrcc energy is way off expected </s> values, vectors = cc.kernel_ip_sd(self.ccsd, self.ccsd.t1, self.ccsd.t2, nroots=self.nroots)	test_ip_sd def test_ip_sd(self):
# todo do something with temp </s> for s in self.states_list:	resample_states def resample_states(self,temp=None):
if not file_name:  # todo: file completion </s> self.window.active_group(),	set_current_group_height_to_n self.window.layout(),
# todo (#567): bucket the node as suspicious </s> return response({'error': 'invalid ursula'}, status=httpstatus.bad_request)	check_availability requesting_ursula = Ursula.from_metadata_bytes(request.data) requesting_ursula.mature() else: initiator_address, initiator_port = tuple(requesting_ursula.rest_interface)
# todo: error checking </s> 'closed': false,	add_card@118 'attrs': { 'name': name, 'idBoard': board_id, "idList": list_id,
# todo: should we enable auto-retry, </s> def __init__(self, queue, include_raw_message):	ConsumerConfig class ConsumerConfig(object): self.queue = queue self.include_raw_message = include_raw_message
# todo: handle if there is a selection </s> self.settextcursor(cursor)	backspace if cursor.position() > self.start_of_current_line: cursor = self.textCursor()
else: self.assertequal(end, 1) # todo: simple exec should not wait_testpid!! </s> package = package_tool(image)	test_6133_run_default_services_from_single_service_saved_container python_coverage = coverage_package(image) if _python.endswith("python3") and "centos" in image: refresh = refresh_tool(image) cov_option = "--system"
# todo: make this status more clear </s> _, item = heapq.heappop(self.timers)	handle_nothing ready_time, _ = self.timers[0] wait_time = ready_time - now self.output_queue.put(item) else:
# todo: add run on all columns functionality </s> with open(file_path, "r") as fp:	txt_file_to_indicator_list file_data = fp.read() indicator_list = []
# todo: handle other hosts </s> nem_id_element.text = str(nem_id)	add_emane_interface nem_element_id = "%s/%s" % (host_id, nem_name) nem_element = etree.SubElement(platform_element, "nem", id=nem_element_id, name=nem_name) add_mapping(transport_element, "nem", nem_element_id)
logger.warn('zzz')  # todo: buffer .. </s> if not event in locks:	handle_lock logger.warn('%r failed.', (event, data)) pipe.send((event, False,)) logger.error('%r failed.', (event, data)) else:
# todo ??? other type of cases </s> def test_pep440_version_default():	test_pep440_version_default
# todo: error handling like numba callwrappers.py </s> ):	check_element_type with c.builder.if_then( cgutils.is_null(c.builder, typobj), c.builder.store(cgutils.true_bit, errorptr) loop.do_break()
# todo(john-wood-w) allow until plugin validation is added. </s> excep.invalidobject,	test_should_raise_with_mixed_case_wrong_payload_content_type def test_should_raise_with_mixed_case_wrong_payload_content_type(self): self.secret_req['payload_content_type'] = 'TeXT/PlaneS' self.validator.validate, self.secret_req,
# todo, can we avoid the copy? </s> memory += self._xbc.dat.nbytes	getInfo from mpi4py import MPI memory = self._x.dat.nbytes + self._y.dat.nbytes if hasattr(self, "_ybc"): memory += self._ybc.dat.nbytes
# todo(vek): need to pass context in for access to auth_token </s> def __init__(self, name, state):	InstanceInfo self.name = name assert state in power_state.valid_states(), "Bad state: %s" % state
# todo: implement </s> self.uciok.set()	_uciok
# todo(jk0): this will eventually need to take ssl into consideration </s> return os.path.join(common.remove_version_from_href(self.base_url),	generate_bookmark self.project_id, "images", str(image_id))
# todo: use sqlalchemy to build this query! </s> card_id = long(req.data['id'])	answer_card @noReturnValue def answer_card(self, col, req): ease = int(req.data['ease']) card = col.getCard(card_id)
# todo: setup/teardown must be methods of a class so we can reuse them </s> def xpath(element, xpath, namespaces):	xpath
# todo(dcramer): re-enable test when find_green_parent_sha is turned </s> result=result.passed,	test_without_newer_green project=project, source=older_source, ) current_build = self.create_build(  # NOQA
# todo: reenable in the features branch </s> this method can only be called when this node is active	addfinalizer in a setup chain, for example during self.setup(). self.session._setupstate.addfinalizer(fin, self)
# todo add docstring </s> table : table	lamda_query coll_partner_index : string Returns Examples --------
# todo update this code once keras > 2.0.4 is released </s> z += 1	SpeakerEmbedding Y[i] = y if y != prev_y: Z[i] = z i += 1
# todo results from p0f </s> def _get_name(endpoint):	_get_name return endpoint.machine.name.strip()
# todo(iceboy): rate limit base on ip. </s> uid = self.user['_id'] if self.has_priv(builtin.priv_user_profile) else none	ProblemDetailHandler @base.route_argument @base.sanitize pdoc = await problem.get(self.domain_id, pid, uid) if pdoc.get('hidden', False):
# todo: change logging </s> self.joined = true	join_get_exc self.join() return self.exc
# todo this requires fleshing out some more.. </s> class foo(object):	Foo
# todo append masters as named-instances as well; needs .designspace change. </s> del axis_mins, axis_maxs, axis_defaults	_add_gvar else: v = (v - defaultval) / (maxval - defaultval) axis_mins = {tag:min(loc[tag] for loc in locations) for tag in axis_tags} axis_maxs = {tag:max(loc[tag] for loc in locations) for tag in axis_tags}
# todo(laigd): remove this check when 313682500 is in the release. </s> padded_x = py_utils.padortrimto(x, y.shape, pad_val=0)	test2DStaticShape with self.session(use_gpu=False, graph=tf.Graph()): x = tf.random.normal(shape=(3, 3), seed=123456) self.assertEqual(padded_x.shape.as_list(), [4, 6]) real_x = self.evaluate(padded_x)
# @todo - link changes with colud </s> :param value: value of given property	worksheet :param property: A property of a worksheet. If there're multiple worksheets with the same title, first one will Example. Getting worksheet named 'Annual bonuses' >>> sht = client.open('Sample one')
#time = "todo" </s> f.close()	create_JAMS fill_annotation(lab_file, annot) f = open(out_file, "w")
# todo: fix self.cursor_x >= w </s> self.comm = self.comm[:i]	inline_k_ctrl_k def inline_k_ctrl_k(self, xbegin, i, w):
# todo may want to pass iv and dv instead of expr (especially since iv/dv may not be variables) </s> effect_size = design['effect size'] if ('effect size' in design) else [.2, .5, .8] # default range unless user defines	execute_test@347 sample_size = 0 for df in data_props.dataframes: alpha = design['alpha'] if ('alpha' in design) else .05 stat_test = find_test(dataset, expr, data_props, design, sample_size=sample_size, effect_size=effect_size, alpha=alpha)
# todo: use is_accessible once two layer trie is implemented </s> if self.is_access_restricted:	set_balance def set_balance(self, address, balance): validate_canonical_address(address, title="Storage Address") if keccak(address) + BALANCE_TRIE_PREFIX not in self.write_list: raise UnannouncedStateAccess(
# todo generator </s> @staticmethod	Drop check=check_argument, if_dirty=if_dirty_opt, @datasetmethod(name=_action) @eval_results
# tracks a suggested todo, which will reduce the 3 rpc calls here to only </s> pass	RaidenError
# todo: fix with stubber / before send event </s> self.client = self.session.create_client(	TestApiGateway def setUp(self): super(TestApiGateway, self).setUp() 'apigateway', self.region) self.stubber = Stubber(self.client)
#todo: this should never happen, dep on equiv </s> attributes=none, id=none):	create_span if attributes is None: attributes = {}
# todo - add hubbub package when available. </s> def _source_subfolder(self):	_source_subfolder return "source_subfolder"
# todo use display_message and add_messages_to_json </s> j_dic['offset'] = 0	enrich_json_with_base j_dic['entities'] = [] j_dic['events'] = []
# todo refactor and remove </s> def sqrt_hessian_sampled(self, module, g_inp, g_out):	sqrt_hessian_sampled
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
# todo(ytknzw): add more specific assertion with the test case. </s> raise valueerror	fail_objective
#@todo: remove in 0.4.10 </s> m = re.search(self.pages_pattern, self.html)	handleMultiPages def handleMultiPages(self): pages = int(m.group(1)) except:
# todo: timeline is global, get rid of it </s> 'actions': [],	Indexes if not lists: yield { } for lang in kw["translations"]:
# todo: sort the functionality by name and by vuln class </s> for url in resource_urls:	create_tabs description_text = str(self.data["functionality"][parent]["vulns"][vuln_name]["description"]) resource_urls = self.data["functionality"][parent]["vulns"][vuln_name]["resources"] resource_text = resource_text + str(url) + "\n" description_textarea = JTextArea()
# todo(#6071): our executeprocessrequest expects a specific string type for arguments, </s> return ['runtime_classpath']	product_types @classmethod
# todo tmp return, to unify with monitor auto-fetch later </s> action_proj = []	act for a, agent in enumerate(self.agents): state = state_space.get(a=a)
1  # todo: fill in identifier </s> cumulative_depth = line.recursion	print_stats avg=line.average, ) if cumulative_depth >= line.recursion: cumulative_depth = float('inf')
# todo: numba or cython to improve performance of this kernel </s> >>> g = nk.graph.hypercube(length=20, n_dim=1, pbc=true)	Heisenberg Examples: Constructs a ``Heisenberg`` operator for a 1D system. >>> hi = nk.hilbert.Spin(s=0.5, total_sz=0, graph=g) >>> op = nk.operator.Heisenberg(hilbert=hi)
# todo(necula): produces mismatched outputs on gpu. </s> devices=("cpu", "gpu"),	atan2 return [ missing_tf_kernel( modes=("eager", "graph"))
# todo add shape check </s> return self._sqrt_hessian_sampled(module, g_inp, g_out)	sqrt_hessian_sampled
# todo(crcrpar): annotate this correctly. </s> warnings.warn(	experimental_version @functools.wraps(func) def new_func(*args: Any, **kwargs: Any) -> Any: "{} is experimental (from version: {}). " "The interface can change in the future".format(func.__name__, version))
# todo: look at args for remotedata </s> k += len(_keys)	delete_intermediates if delete_keys: _keys = delete_keys[:] yield center.delete_data(keys=_keys) yield gen.sleep(1 - (time() - start))
# todo: remove once typeshed supports literal types </s> raise ipcexception("the namedpipe at {} was not found.".format(self.name))	IPCClient try: _winapi.WaitNamedPipe(self.name, timeout) except WindowsError as e: if e.winerror == _winapi.ERROR_SEM_TIMEOUT:
# todo switch to transform </s> network only when `close()` is called.	BufferedDatasetWriter This allows incremental updates to datasets using formats that don't otherwise support updates, such as JPEG.
# todo test </s> self.message = message	StateUnreachableError self.current_state = current_state self.past_state = past_state
logg.error("too long") #todo </s> sx____("docker stop -t 6 " + testname)	rm_docker def rm_docker(self, testname): sx____("docker rm -f " + testname)
# todo: unittest before committing </s> ----------	hstack No dataset attributes from any source dataset will be transferred into the stacked dataset. datasets : tuple Sequence of datasets to be stacked.
# todo: remove this compatibility layer with rally 1.1.1 </s> metainfoscope.cluster: {},	clear_meta_info self._meta_info = {
raise exceptions.mpdnotimplemented  # todo </s> def subscribe(context, channel):	subscribe@18 *musicpd.org, client to client section:* ``subscribe {NAME}``
# todo: does updating sigma here (as opposed to after regret) miss out </s> for player_i in range(n_players)	new_game pot = Pot() players = [ ] state = ShortDeckPokerState(players=players)
# todo: kwargs </s> def df_len_overload(df):	df_len_overload if len(df.columns) == 0:  # empty df return lambda df: 0
# todo check default function methods and return them </s> new_scopes = []	follow_path_old else: print 'error follow_path:', p_class, repr(c.parent) for s in tuple(scopes): new_scopes += follow_path(s, tuple(path))
pass # todo </s> assert form.a() == u'''<input id="a" name="a" type="text" value="0" />'''	test_IntegerField b = IntegerField() form = F(DummyPostData(a=['v'], b=['-15'])) assert form.b.data == -15 assert form.b() == u'''<input id="b" name="b" type="text" value="-15" />'''
# todo not implemented yet </s> if row_len < 3:	set_current_group_height_to_n Set current group height to N (default: highest possible) layout = self.window.layout() return current_group_num = self.window.active_group()
# todo: report </s> return true	handle_timeout_sustain
# todo: deprecate `pages` in favor of `page_limit` since it is less confusing </s> :param timeout:     session response timeout in seconds, integer	write_posts_to_csv :param group:       Facebook group id :param filename:    File name, defaults to <<account_posts.csv>> :param sleep:       Sleep time in s before every call, integer :param credentials: Credentials for login - username and password, tuple
# :todo: implement test. </s> self.adapter = mockadapter()	SendTransferCommandTestCase class SendTransferCommandTestCase(TestCase): def setUp(self): def test_wireup(self): Verifies that the command is wired up correctly.
# todo: wrap backend call in error handling. </s> return self._current_tl_track	get_current_tl_track def get_current_tl_track(self):
# todo: replace with stream-changed </s> def get_current_tl_track(self):	get_current_tl_track
# todo: unescape values </s> return xml_from_string(xml_to_string(element)).xpath(xpath,	xpath namespaces=namespaces)
'''todo: add docs''' </s> return [x for x in self.columns if x['type'] == 'discretecolumn']	discrete_columns @property
# todo: implement </s> text = self.mainwindow().currentview().textcursor().selection().toplaintext()	copy_dehyphenated def copy_dehyphenated(self):
# todo: currently can't initialize hadooprunner without setting these </s> self._old_environ = os.environ.copy()	blank_out_environment os.environ.clear()
# todo: if the main thread receives a signal and we have no timeout, we </s> return openssl.crypto.load_certificate(openssl.crypto.filetype_pem,	GetCertificate self.ssl_cert_pem)
# todo - molecule type - see issue 363 / pull request #1005 </s> self.assertequal(str(e), "couldn't parse feature location: '-2..492'")	test_001_negative_location_warning try: SeqIO.read(path.join("GenBank", "negative_location.gb"), "genbank") else: self.assertTrue(False, "Expected specified BiopythonParserWarning here.")
# todo(ahundt) softmax is pre-applied, so need different train, inference, evaluate. </s> x = atrous_identity_block(3, [512, 512, 2048], stage=5, block='c', weight_decay=weight_decay, atrous_rate=(2, 2), batch_momentum=batch_momentum)(x)	AtrousFCN_Resnet50_16s x = identity_block(3, [256, 256, 1024], stage=4, block='f', weight_decay=weight_decay, batch_momentum=batch_momentum)(x) x = atrous_conv_block(3, [512, 512, 2048], stage=5, block='a', weight_decay=weight_decay, atrous_rate=(2, 2), batch_momentum=batch_momentum)(x) x = Convolution2D(classes, 1, 1, init='he_normal', activation='linear', border_mode='same', subsample=(1, 1), W_regularizer=l2(weight_decay))(x) x = BilinearUpSampling2D(target_size=tuple(image_size))(x)
# todo: add this back in once we've merged back the refactored users code </s> self.assertequals(len(couch_user_2.commcare_accounts),0)	testLinkOrphanCommCareUser self.assertEquals(couch_user_1.commcare_accounts[0].registering_phone_id,commcare_imei) self.assertEquals(couch_user_1.commcare_accounts[0].user_data['random_user_info'],commcare_user_data) commcare_users_count = CouchUser.view("users/commcare_users_by_domain_username", key=[self.domain, self.commcare_username]).total_rows self.assertEquals(commcare_users_count, 1)
# todo: move this to a common test module (tests/common.py?) </s> 'http://localhost:' + str(self.port+1) + '/' + self.random_string(),	test_download_url_to_tempfileobj_and_urls self.target_data_length) self.assertRaises(requests.exceptions.ConnectionError, self.target_data_length) url_with_unsupported_uri = self.url.replace('http', 'file')
# todo pydocs </s> super(bigquerycursor, self).__init__(service=service, project_id=project_id)	BigQueryCursor https://github.com/dropbox/PyHive/blob/master/pyhive/presto.py https://github.com/dropbox/PyHive/blob/master/pyhive/common.py self.buffersize = None self.page_token = None
# todo: perhaps something more self-documenting for variables names? </s> recovered_trade = zp.datasource_unframe(msg)	FinanceTestCase trades = factory.create_trade_history( a, b, c, ts, dt ) for trade in trades: feed_msg = zp.FEED_FRAME(recovered_trade) recovered_feed = zp.FEED_UNFRAME(feed_msg)
# todo: add tcp-like buffering </s> return ('unix', self.address)	getDestination
# todo: if py3k, override unpickler.find_class(). </s> return self.counter_tick_core(myroot, incrementing=1, mycpv=mycpv)	counter_tick
def __init__(self, p_args, p_todolist, #pragma: no branch </s> return """\	help def help(self):
# todo ... </s> return arg in state.macros	cpreprocess_evaluate_ifdef if not is_valid_defname(arg): state.error("preprocessor: '" + arg + "' is not a valid macro name")
# todo: update graph references </s> return handle(self.get_handles_data()[name][1])	get_handle
# todo we need to work on the unit from the exposure. </s> :type display_null: bool	hazard_class_style :type layer: QgsVectorLayer :param display_null: If we should display the null hazard zone. Default to :param classification: The hazard classification to use. :type classification: dict safe.definitions.hazard_classifications
# todo: sync with link.smart_link() to choose a linker </s> alltools.extend(linktools)	exists@54 for langvar, linktools in linkers.items(): if langvar in env: # use CC over CXX when user specified CC but not CXX return SCons.Tool.FindTool(alltools, env) # find CXX or CC
# todo: support speedy mode for running the script </s> rm_if_exists(".config")	rm_configs os.remove(f)
# todo: check </s> self.relation_embeddings = nn.embedding(num_relations, self.entity_embedding_dim)	TransR self.relation_embedding_dim = self.entity_embedding_dim margin_loss = config[MARGIN_LOSS] self.projection_matrices = nn.Embedding(num_relations, self.entity_embedding_dim * self.relation_embedding_dim) self.margin_loss = margin_loss
# todo(huanxuan): remove this if condition once the fixed </s> ('project', identity_fakes.project_name),	test_quota_show_with_class ] verifylist = [ ] parsed_args = self.check_parser(self.cmd, arglist, verifylist)
# todo: edge dps could use a different forwarding algorithm </s> returns:	edge_learn_port Args: other_valves (list): All Valves other than this one. port to learn host on, or None. if pkt_meta.port.stack is None:
# todo add method if no aws creds </s> except clienterror as e:	S3Service bucket_exists = True try: if e.response['Error']['Code'] == '404': bucket_exists = False
# todo: check output </s> g2.linear_solver.precon.options['maxiter'] = 2	test_hierarchy_iprint g2.nonlinear_solver = NewtonSolver() g2.linear_solver = ScipyIterativeSolver() prob.set_solver_print(level=2) prob.setup(vector_class=PETScVector, check=False)
# todo(nakago): check why tolerance is high </s> return nfp(out_dim=out_dim)	model @pytest.fixture
# todo: conflict detection/resolution </s> self._settings.update(kwargs)	settings def settings(self, **kwargs):
# todo: cannot be loaded with plugins; improve this solution </s> if name is not none:	__read_convert_schema def __read_convert_schema(self, dataframe): schema = Schema() dtype = dataframe.index.get_level_values(index).dtype type = self.__read_convert_type(dtype)
# todo: deprecated - remove in version 0.10 </s> def _create_ensemble(policies):	_create_ensemble if policies is None: return SimplePolicyEnsemble([])
# todo: handle marker? </s> return {'error': salt.utils.boto.get_error(e)}	describe_alias else: return {'alias': None}
#todo : manage more than just quit </s> return self.schedulers.keys()	what_i_managed
# todo: come up with a proper detect() routine...and enable it. </s> slist.append(s)	emit_rmic_classes s = src.rfile() s.attributes.java_classdir = classdir stub_suffixes = ['_Stub'] if env.get('JAVAVERSION') == '1.4':
# todo: we need to insert a linebreak here, but there is no </s> if len(l): d.label = l	set_labels def set_labels(self, lst): assert len(lst) <= len(self.bufferdata)
# todo: support preferences file in backup under linux (is in different directory). </s> extract the whole archive to the given target path.	_extractArchive @staticmethod :param archive: The archive as ZipFile. :param target_path: The target path.
# todo: not sure if this is pg only or standard </s> context.assert_(	test_alter_column_type op.alter_column("t", "c", type_=String(50))
# todo crossplataform?: </s> manager_url = "http://{0}/tasks".format(app.config['flamenco_manager'])	get_task def get_task():
# todo setup mahout, must be checked out from repo atm: </s> http://archive.cloudera.com/docs/ec2.html	_setup_hadoop@198 http://archive.cloudera.com/cdh/3/
# todo: "annotator" is a very confusing term for a web service </s> if not allowed_to_read(doc_path):	assert_allowed_to_read raise AccessDeniedError # Permission denied by access control
# todo(amaranth-0.4): remove </s> yosys_version = yosys.version()	_convert_rtlil_text def _convert_rtlil_text(rtlil_text, *, strip_internal_attrs=False, write_verilog_opts=()): script = [] script.append("read_ilang <<rtlil\n{}\nrtlil".format(rtlil_text))
# todo: temporary thing (there is no uia based wrappers tree yet) </s> controltype = _known_control_types[self._elementinfo.controltype]	FriendlyClassName if self._elementInfo.controlType not in _known_control_types.keys(): self.friendlyclassname = str(self._elementInfo.controlType) if (ControlType not in pywinauto_control_types.keys()) or (pywinauto_control_types[ControlType] is None): self.friendlyclassname = ControlType
# todo: avoid flush all caches </s> return pwndbg.memory.page(start, end-start, 4, 0, name)	find_boundaries end   = pwndbg.memory.find_upper_boundary(addr) if start < min:
# todo(toshihikoyanase): remove the constraints when tensorflow==2.7.0 is released. </s> "alembic",	get_install_requires def get_install_requires() -> List[str]: "cliff", "cmaes>=0.8.2",
# todo: implement persistence for configuredname </s> def _on_remote_key(self, value):	_on_remote_key
# todo: should not assume presence of any kind of parameter </s> except valueerror:	_get_chunksizes raise DatacubeException('Dataset contains invalid chunking values, cannot write to storage.') try: raise DatacubeException('Dataset contains invalid chunking values, cannot write to storage.') if not chunksizes:
# todo: include a validation step for x </s> node_1, node_2 = node_pair	_pairwise_intersections @staticmethod def _pairwise_intersections(min_intersection, node_pair): data['node_indices'] = tuple((node_1[0:3], node_2[0:3])) data['intersection'] = np.intersect1d(node_1[3], node_2[3])
# todo(nmakhotkin): simplify calculating task output. </s> action_input_collection[index].update(iter_context)	calc_for_each_input if index >= len(action_input_collection): action_input_collection.append(iter_context) return action_input_collection
return  # todo return placeholder "[loading]" album? </s> name=name,	to_playlist tracks = filter(None, tracks) return models.Playlist( tracks=tracks)
# todo: try to figure out the right lexer for these files </s> if chunkindex < 0 or chunkindex >= len(file['chunks']):	build_diff_fragment key = 'diff-fragment-%s' % file['filediff'].id if chunkindex: raise UserVisibleError("Invalid chunk index %s specified." % \ chunkindex)
# todo: rebalance if output distributions are 1d instead of 1d_var </s> col_name = self.str_const_table[col_var]	_process_df_build_map for item in items_list: col_var = item[0].name df_cols[col_name] = item[1] return df_cols
# todo: this comparison should happens only if users </s> class enum(object):	Enum
# todo: run filters here. </s> self.libusb_device = usb.core.find(idvendor=idvendor, idproduct=idproduct)	USBProxyDevice name = "Base class for proxied USB devices" SET_ADDRESS_REQUEST = 5 if self.libusb_device is None: raise DeviceNotFoundError("Could not find device to proxy!")
# todo integrate into keypoints </s> coords_proj = np.array(coords).astype(np.float32)	project_coords@31 from_height, from_width = from_shape[0:2] to_height, to_width = to_shape[0:2] coords_proj[:, 0] = (coords_proj[:, 0] / from_width) * to_width coords_proj[:, 1] = (coords_proj[:, 1] / from_height) * to_height
self.assertequals(status, 200) # todo: should be 202 </s> status, body = self.get(path)	test_get_bind (self.CONSUMER_ID, self.REPO_ID, self.assertEquals(status, 200) self.assertTrue(body is not None)
# todo: parse human-friendly logmaxsize ... e.g. 10mb </s> file=sys.stderr	read_config "WARNING: ';' character was found in URI: {}. Please note that ';' has been replaced '?' in Ramona 1.0. This can lead to Ramona apparent freeze during start.".format( config.get(sec, valname) ) stashdir = config.get('ramona:notify', 'stashdir')
# todo make atomic </s> self.__cog = cog(node(self.__primarylabel__))	GraphObject @property def __cog__(self): node = self.__cog.subject_node if not hasattr(node, "__primarylabel__"):
# todo: deprecate </s> redirect_uri=request.redirect_uri,	save_authorization_code item = AuthorizationCode( code=code, scope=request.scope, user_id=request.user.id,
# todo(b/186451541): reduce the number of calls to model_fn. </s> )).batch(2)	test_orchestration_execute collections.OrderedDict( x=[[1.0, 2.0], [3.0, 4.0], [1.0, 0.0], [-1.0, -1.0]], federated_ds = [ds1, ds2] server_state = iterative_process.initialize()
# todo transition this once pushy is out </s> ],	create@11 'mon.{hostname}'.format(hostname=hostname)
# todo remove in next major version 5.0.0 see serializers.reservedfieldnamesmixin </s> "received data is not a valid json:api resource identifier object"	JSONParser method = request and request.method if not isinstance(data, dict): ) if method in ("PUT", "POST", "PATCH"):
#todo - complete implementation of these apis </s> except exception.networknotfound as e:	_items return dict(ports=result)
# todo: enable admin tests </s> cmd = cmd + ' --schedule={}'.format(schedule)	celery_beat if schedule:
# todo: use get_cstr_and_len instead of getitem </s> def __init__(self, dmm, fe_type):	StringArrayPayloadModel @register_model(StringArrayPayloadType) members = [ ('size', types.int64),
# todo: refactor this to avoid additional lookup in cast_params </s> schedule an action execution.	_invoke_action @staticmethod :type action_exec_spec: :class:`ActionExecutionSpecDB` :param params: Parameters to execute the action with.
# todo: will need some code for the tabs active terms to work </s> for terminal in self.terminator.terminals:	group_all def group_all(self, widget): group = _('All') terminal.set_group(None, group) self.terminator.focus_changed(self.terminator.last_focused_term)
# todo(albert): implement stub. </s> def is_locked(self):	is_locked return self._status.get('lock', True)
# todo: assert </s> repo = self.remote.new_repo(self.token)	createRepo @pytest.fixture self.remote.modify_repo(repo, "name", "testrepo0", self.token) self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token)
# todo: use different flag than .reentrant </s> state.sorting = colorsorter.sorting	_suspend state = _attrholder() if len(ColorSorter._gl_name_stack) > 1: state._sorted = ColorSorter._sorted state._immediate = ColorSorter._immediate
# fixme todo replace with proper url section joining taking unicode inputs </s> return unicode(val, 'utf-8').upper()	coerce_http_method assert isinstance(val, basestring) and len(val) > 0 except AssertionError:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	ReplayBundleRequestFilterTestCase self.skipTest('Not implemented yet.') def test_fail_depth_null(self): def test_fail_depth_string(self): ``depth`` is a string.
# todo add shape check </s> return self._sqrt_hessian_sampled(module, g_inp, g_out)	sqrt_hessian_sampled
#todo: not integrated with cbar yet... </s> assert offt[0] in ['g', 'b', 'o', 'e'], msg	_init_offt_bit bit = None offt = field8 assert offt[1] in ['G', 'B', 'O', 'E'], msg assert offt[2] in ['G', 'B', 'O', 'E'], msg
# todo: handle non numpy alloc types </s> last_block.body.pop()	remove_none_return_from_block assert (isinstance(last_block.body[-1], ir.Assign) and isinstance(last_block.body[-1].value, ir.Const)
# todo: make previous blocking instead of sleep </s> time.sleep(0.1)	shareConstant@45 import scoop while all(key in elements[scoop.worker] for key in kwargs.keys()) is not True:
# todo: cat, slice, array, clocksignal, resetsignal, memory </s> else:	Simulator def __init__(self, fragment_or_module, generators, clocks={"sys": 100}): if isinstance(fragment_or_module, _Fragment): self.fragment = fragment_or_module.get_fragment() if not isinstance(generators, dict):
# todo: find out how to get global usernames </s> print("  favorited: " + re.sub('<[^<]+?>', '', faved['content']))	fav def fav(mastodon, rest): mastodon.status_favourite(rest)
#todo the tolerance needed to pass is very high for float32(0.16). is this acceptable? expected? </s> assert (temp < 1e-5).all()	exec_multilayer_conv_nnet assert (N.abs(hidval-hidval1)<1e-5).all() temp = N.abs(outval.reshape(bsize,-1) - hidval) else: hid = img #we don't need it, but it make the flow easier flow
# todo: is there a better way to check this? </s> def __delattr__(self, name):	__delattr__
# todo -- we might need to expanduser taking .user into account </s> ssh(	init_remote_repo return False if isinstance(dataset.repo, AnnexRepo): "git -C {} annex init {}".format( sh_quote(path),
# todo. percentile </s> d = 1. - f.cosine_similarity(	pdist distances = [] for i in range(1, n_sequences): fX[i, :].expand(i, -1), fX[:i, :], dim=1, eps=1e-8)
# todo: add .data and .grad to syft tensors </s> for p in model.parameters():	module_is_missing_grad if p.grad is None: return True
# :todo: implement test. </s> the implications of allowing this are cool to think about...	test_error_poll_interval_null def test_error_poll_interval_null(self): but not implemented yet. with self.assertRaises(TypeError):
# todo: only handle it if the trigger refers to the current sensor </s> config=sensor_config)	_get_sensor_instance (self._sensor_class_name)) sensor_config = self._get_sensor_config() return sensor_instance
name='unknown',  # todo currently not storing requester </s> self.check_product_stock(self.sp, product, initial + amt, amt)	test_balance_first_doc_order transfers = [(p._id, float(50 - 10*i)) for i, p in enumerate(self.products)] self.submit_xml_form(balance_first(balance_amounts, transfers))
# todo: let's see if we can find sane versioning for `latest` from upstream </s> ['metadata.namespace', '=', release_data['namespace']], ['metadata.name', '=', resource_name]	parse_k8s_resource_tag return resource = self.middleware.call_sync( ] )
pass # todo </s> def collect_incoming_data(self, data):	collect_incoming_data
# todo: index arg? </s> self.reverse_copies[inst.value.name] = inst.target.name	_get_reverse_copies def _get_reverse_copies(self, body): for inst in body: return
# todo subject.cn from cert? </s> reason="need a mac with codesign to run")	TestMac class TestMac: def codesign_display(self, path):
#todo only apply this filter if set by the user </s> if isinstance(vdict[k], tuple) and len(vdict[k]) == 1:	make_single >>> make_single(d) {'xx': 1} vdict[k] = vdict[k][0] return vdict
# todo: remove in v.0.6 </s> csep = class_separation(cov.transform(self.iris_points), self.iris_labels)	TestCovariance def test_iris(self): cov = Covariance() self.assertAlmostEqual(csep, 0.72981476)
# todo: create xxx_failure test </s> self.asserttrue(check('test_space', failure_tests),	test_result_space_failure r = run_set(p, 'result') failure_tests = r['failure'] lookup('test_space', tests))
# todo(t2r_contributors): switch to using gin config for all saver params. </s> def create_exp_decaying_learning_rate(initial_learning_rate = 0.0001,	create_exp_decaying_learning_rate decay_steps = 10000, decay_rate = 0.9,
# todo(cmaloney): switch to a sane http server </s> groups,	run_server@1435 bind_http_https, ssl_certs): processor = MarathonEventProcessor(marathon, bind_http_https, ssl_certs)
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> context.acquire_token_with_username_password(resource, sampleparameters['username'], sampleparameters['password'], sampleparameters['clientid'], callback)	Test_AcquireTokenWithUsernamePassword def callback(err, tokenResponse): print(tokenResponse)
# todo: test that valueerror is raised </s> return tuple(_map[index] for index in indices)	indices2labels def indices2labels(self, indices):
evaluation.message('setstreamposition', 'todo2', name)   #todo </s> </dl>	OpenAppend <dl> <dt>'OpenAppend["file"]' mode = 'a' stream_type = 'OutputStream'
# todo(b/129148632): the current apache-beam 2.11.0 do not work with py3 </s> "dataset class must inherit from `dataset_builder.datasetbuilder`.")	test_baseclass def test_baseclass(self): self.assertIsInstance(
# todo: hand derive these results. </s> self.order(self.sid_to_buy_and_hold, 100)	BuyAndHoldAlgorithm if not self.holding:
# todo: refactor </s> notes	get_num_examples def get_num_examples(self): ----- Infinite datasets have float('inf') examples.
# todo: ensure it's up to date </s> "bug? (%r.modflag_asserts() failed; %r %r %r): " % \	modflag_asserts hopetrue = ( (not self._modified) == (self._model_change_counter == self._change_counter_when_reset_changed) ) if not hopetrue: (self, self._modified, self._model_change_counter, self._change_counter_when_reset_changed) )
# todo: have the transform on this label be less hacky </s> }	build_css axis_path = { "fill" : "none", self.css[".xaxis line"] = axis_path return self.css
# todo(jamalex): burn it all down! </s> child_availability.append(child.get("available", false))	recurse_nodes child_ids = [child.get("id") for child in node.get("children", [])] for child in node.get("children", []): if child_ids: node["children"] = child_ids
# todo: pace ourselves (send through the uploader...) </s> 'id': buf_id	get_buf def get_buf(self, buf_id, view=None): self.send({ }) buf = self.bufs[buf_id]
# todo filter chamber in case person went between chambers same day? </s> for c in qs.in_bulk(chunk).values():	fetch_in_chunks while all_items: chunk = all_items[0:chunk_size] yield c return len(all_items), make_iter(list(all_items))
# todo: handle `stream.close` and `stream.reset` </s> return self.listen_maddr.encapsulate(multiaddr(f"/p2p/{self.peer_id}"))	listen_maddr_with_peer_id @property
# todo: perhaps something more self-documenting for variables names? </s> def test_orders(self):	FinanceTestCase self.assertTrue(risk.trading_calendar.is_trading_day(known_trading_day)) self.assertFalse(risk.trading_calendar.is_trading_day(known_holiday)) allocator = AddressAllocator(8) sockets = allocator.lease(8)
# todo: support minp arg end_range etc. </s> return arr	rolling_fixed
# todo: should this raise an exception if the script didn't finish? </s> self.warp(map_group, map_id, x, y)	start_trainer_battle_lamely coordinates, pressing the direction button for a full walk step (which ideally should be blocked, this is mainly to establish direction), and self.nstep(100)
# todo: get file name from user. </s> self._cursor_changed_id = self.cursor.connect('cursor-changed', self._on_cursor_change)	load_store self._treeview.set_model(store) self._set_menu_items_sensitive(True) else: if self._cursor_changed_id and self.cursor:
# todo: must be implemented </s> pass	should_fetch_kindlegen
# todo: replace and deprecate? </s> a deferred that fires once the data is deleted (or none if	Model return self.manager.store(self) def delete(self): using a synchronous manager). return self.manager.delete(self)
# todo: check lens </s> s = b[i]	_column_fillna_impl def _column_fillna_impl(A, B, fill):  # pragma: no cover if np.isnan(s): s = fill
# todo: remove after implementing django-constance </s> else:	system@187 if systems_created_counter > 0: if systems_created_counter  == 1: messages.success(request, str(systems_created_counter) + ' systems were created.') if systems_updated_counter > 0:
# todo i think this is a hack. it should be an </s> if not completion_parts.path and not completion_parts.has_dot:	Completion completion_names += self._simple_complete(completion_parts) return completion_names scope = self._parser.user_scope() if not scope.is_scope():  # Might be a flow (if/while/etc).
# todo(obondarev): use neutron_lib constant </s> raise qos_exc.qosrulenotfound(policy_id=policy_id, rule_id=rule_id)	get_policy_rule policy_object.QosPolicy.get_policy_obj(context, policy_id) rule = rule_cls.get_object(context, id=rule_id) return rule
# todo find better documentation </s> "subauthoritycount": 0x1.to_bytes(length=1, byteorder="little"),	Sid def __init__(self, ql): self.struct = { "IdentifierAuthority": 0x5.to_bytes(length=6, byteorder="little"), "SubAuthority": 0x12345678.to_bytes(length=ql.pointersize, byteorder="little")
# todo: remove this - cura-4482 </s> def addglobal(self):	addGlobal return self._add_global
# todo check </s> def set_metadata(self, metadata):	set_metadata pass
raise notimplementederror #todo </s> pass #todo	Span
# todo: provide a kernel which will describe how coordinates are extruded. </s> assert integrate_assemble_p0(family, degree) < 1.0e-14	test_firedrake_extrusion_assemble @pytest.mark.parametrize(('family', 'degree'), [('DG', 0)])
# todo: python-2442 use _asdict() instead </s> else repr(self.document_class))	_arguments_repr def _arguments_repr(self): document_class_repr = ( uuid_rep_repr = UUID_REPRESENTATION_NAMES.get(self.uuid_representation, self.uuid_representation)
# todo: maybe we should open this one to the users, as it lets them </s> elif detection.shape[1] == 3:	TrackedObject self.hit_counter += 2 if detection.shape[1] == 2: matched_points_idx = detection[:, 2] > self.detection_threshold matched_parts_idx = np.array([[v, v] for v in matched_points_idx]).flatten()
# todo(mordred) when this changes to rest, force interface=admin </s> _tasks.machinenodeportget(port_id=mac))	get_nic_by_mac def get_nic_by_mac(self, mac): try: except ironic_exceptions.ClientException: return None
# todo: remove compatability hooks </s> while end < len(s) and s[end].isalpha():	_split_version_components while end < len(s) and s[end].isdigit(): end += 1 end += 1 elif s[start] in (".","-"):
# todo: test this! </s> if enable:	printk def printk(enable): fpk.write("4") else:
# todo(dolph): remove this check after default-domain migration </s> expire_delta = datetime.timedelta(seconds=conf.token.expiration)	default_expire_time def default_expire_time(): Expiration time varies based on configuration (see ``[token] expiration``). return timeutils.utcnow() + expire_delta
# todo: assert metrics. </s> void func2() {	test_get_touched_functions "file.cpp", int i = 1; int i = 2; }""",
# todo: make truly async </s> projects = []	GCPFacade def __init__(self): self._resourcemanager_client = discovery.build('cloudresourcemanager', 'v1', cache_discovery=False, cache=MemoryCache()) request = self._resourcemanager_client.projects().list() while request is not None:
# todo add checks for source space </s> if np.all(stc1.times != stc2.times):	_check_stc@10 def _check_stc(stc1, stc2): if stc1.data.shape != stc2.data.shape: raise ValueError('Times of two stcs must match.')
# todo(porg) not sure if created should ever throw... maybe warning/log? </s> return self.query_all(lambda x: x.xpath_all(query))	Query org = Org.from_file(str(of)) yield from query(org)
# todo: move install_time away from app_setting </s> with open('/etc/ssowat/conf.json.persistent', 'w+') as f:	app_makedefault if 'redirected_urls' not in ssowat_conf: ssowat_conf['redirected_urls'] = {} json.dump(ssowat_conf, f, sort_keys=True, indent=4) os.system('chmod 644 /etc/ssowat/conf.json.persistent')
#todo - check for or silence the expected warning? </s> def test_str_find(self):	test_str_find
# todo: 'ignore_status' could/should be removed when globalres.log is </s> log.info("timing command: %s", cmd_str)	measure_cmd_resources microsecs = int(float('0.' + frac) * pow(10, 6)) return timedelta(0, hours*3600 + mins*60 + secs, microsecs) with open(self.cmd_log, 'a') as fobj: ret, timedata = time_cmd(cmd, stdout=fobj)
# todo(vek): need to pass context in for access to auth_token </s> self.state = state	InstanceInfo def __init__(self, name, state): self.name = name
# todo: it would be nice to be async about this. set 1 second timeout. </s> r = requests.post(service, data=json.dumps(payload), timeout=1)	version_check@36 'version': bayeslite.version.__version__ } if FAIL_VERSION_CHECK or r.status_code == 200 and r.json.result != "current": warnings.warn('Bayeslite is not up to date. Version %s is available.\nSee %s' % (r.json.version, r.json.url))
# todo setup mahout, must be checked out from repo atm: </s> raise importerror("error installing seal: missing pydoop.  please install pydoop first")	install_seal import pydoop except ImportError: export_str = "export JAVA_HOME='%s'" % java_home env.safe_sudo("%s && pip-python install seal" % export_str)
# todo change when v4 web3.py will released </s> assert 100 * (index + 1) == escrow.call().getlockedtokens(owner)	test_pre_deposit assert 100 * (index + 1) == escrow.call().getTokens(owner)
# todo send the key to the master for approval </s> if 'qemu_nbd.clear' in __salt__:	umount_image Unmount an image mountpoint CLI Example:: if 'img.mnt_{0}'.format(mnt) in __context__: __salt__['qemu_nbd.clear'](__context__['img.mnt_{0}'.fomat(mnt)])
# todo: remove this when fixed in: https://github.com/seleniumhq/selenium/issues/767 </s> loader = percy.resourceloader(	percy @pytest.fixture(scope='session') def percy(request, browser): root_dir=settings.STATIC_ROOT, base_url=urllib.quote(settings.STATIC_URL),
# todo: support multi-index columns </s> return pd.dataframe(	pdf1 def pdf1(self):
# todo private access.. </s> def wrapper(cls, metaclasses):	get_metaclass_filters for metaclass in metaclasses: if metaclass.py__name__() == 'ModelBase' \
return 'ok' # todo should be a json or something </s> announced = 0	event_stream while True: sleep(0.1)
# todo: is write=true a reasonable way to do it? </s> self.repo.index.commit(msg)	GitRepo to be implemented. See TODO in AnnexRepo regarding a decorator for this purpose. if not msg:
# todo investigate different results between mac and linux/win platforms </s> func.stack_adjustment = func.stack_adjustment	test_function_stack def test_function_stack(self): funcinfo = [] func.reg_stack_adjustments = func.reg_stack_adjustments func.create_user_stack_var(0, binja.Type.int(4), "testuservar")
# todo debug </s> currentrule.elements.append(ruleelement)	parseRuleRecursively ruleElement = RuleElement() ruleElement.type = "rule" parseRuleRecursively(notItem, ruleNew) elif not sensorItem is None:
# todo: invalidate cache for former latestappinfo </s> if not forms:	get_xform_by_xmlns def get_xform_by_xmlns(self, xmlns, log_missing=True): return None else:
# todo -- can we do this without a subscription? </s> speech_text = render_template("play_library_text")	play_library first_song_id = queue.reset(tracks) first_song_id = queue.shuffle_mode(True) return audio(speech_text).play(stream_url)
## todo delete me? </s> pass	updateTextFile except KeyError:
# todo replace with collections.sequence subclass </s> did_you_mean = utils.to_unicode(	did_you_mean suggestion exists. Will always return :class:`None` if the search isn't loaded. lib.sp_search_did_you_mean(self._sp_search)) return did_you_mean if did_you_mean else None
# todo: reproduce and submit traceback to issue 41 </s> :return: httpresponse object	MyIpView def MyIpView(request): return the IP address (can be v4 or v6) of the client requesting this view. return HttpResponse(request.META['REMOTE_ADDR'], content_type="text/plain")
# todo: refactor accordingly when v3 websocket api is released </s> def order_book_class(self) -> bittrexorderbook:	order_book_class @property
# todo: remove requestor and integrate with wallet from gateway </s> return self.cc_name	get_cc_name
# todo is there a way to actually test that the creds work? </s> if "username: bokeh" not in out:	verify_anaconda_credentials out = system.run(f"anaconda -t {token} whoami")
# todo: write me </s> db.connection.commit()	unlock (coord.row, coord.column, coord.zoom, format))
"""todo: not implemented""" </s> return len(x)	n 3 if isinstance(x, pd.DataFrame):
# todo: scalar, min is a workaround </s> dfbn = num_groups - 1	f_oneway ssbn += _square_of_sums(a - offset) / float(len(a)) ssbn -= _square_of_sums(alldata) / float(bign) dfwn = bign - num_groups msb = ssbn / float(dfbn)
# todo: replace this with a report api </s> "raised when waiting on links times out"	LinkExhaustedError
# todo use deepcopy() here </s> for p2 in exterior[1:]:	_is_polygon_line vec_down = np.float32([0, 1]) p1 = exterior[0] vec = np.float32(p2) - np.float32(p1) angle = ia.angle_between_vectors(vec_down, vec)
# todo: parse text </s> def parse_linebreak(self, m, state):	parse_linebreak
# todo: remove when we stop supporting python < 3.5 </s> points, or 2d array of indices of quadruplets if the metric learner	_QuadrupletsClassifierMixin quadruplets : array-like, shape=(n_quadruplets, 4, n_features) or \ (n_quadruplets, 4) uses a preprocessor. Returns
# todo: test this </s> else:	get_value return values.value
# todo: remove this (ssh_user is a legacy arg) </s> ))	_user_or_ssh_user logger.warning(( 'The `ssh_user` argument is deprecated in `ssh.*` operations, ' return user or ssh_user
# todo: process bind. </s> broker = plugin.getbroker()	producer @classmethod def producer(cls): url = str(broker.url) cls.__producer = Producer(url=url)
# :todo: implement test. </s> },	GetBundlesRequestFilterTestCase }, { ) def test_fail_transaction_not_trytes(self):
# todo: write this method if possible </s> for address in addresslist:	BlockTrail return self.request(url_path, variables, method) def getbalance(self, addresslist): res = self.compose_request('address', address) balance += int(res['balance'])
# xxx todo </s> def input_integer(self, token):	input_integer
# todo: add logger here </s> else:	save_raw_output@65 project_uuid=project_uuid) session.add(new_scan) each_host = parsed_dict['nmaprun']['host'] address = each_host['address']['@addr']
# todo: remove "get_" from the name </s> numerical edge type index	edge_index Args: index: Tuple of (node1_type, edge_type, node2_type) try: index = self.edge_types.index(edge_type)
#todo: assuming constant mu </s> @property	Me
# todo: need to update this so that it flushes bulk queue </s> return self.es_connection.index(index=index, doc_type=doc_type, doc=body, bulk=bulk)	ElasticsearchClient if doc_id: return self.es_connection.index(index=index, doc_type=doc_type, doc=body, id=doc_id, bulk=bulk) except pyes.exceptions.NoServerAvailable: raise ElasticsearchBadServer()
# todo: remove deprected flags in 1.2 </s> return leaves	_get_leaves while leaf_ptr is not None: leaves.append(leaf_ptr)
# todo: add `coerce_float`, `params`, and 'parse_dates' parameters </s> - if 'coerce', then invalid parsing will be set as nat	to_datetime or DataFrame/dict-like errors : {'ignore', 'raise', 'coerce'}, default 'raise' - If 'ignore', then invalid parsing will return the input format : string, default None
nullcontext = contextlib.exitstack()  # todo: use contextlib.nullcontext after python 3.7 </s> yield (name, input_path, output_path)	iterate_path input_path = fmtutils.path_from_format(args.directory, args.format, name=name, ext='in') output_path = fmtutils.path_from_format(args.directory, args.format, name=name, ext='out')
# todo: rename this test </s> self.int_func(['1', '2', 'a'])	test_int_list with self.assertRaises(RuntimeTypeError): self.int_func('5') with self.assertRaises(RuntimeTypeError): self.int_func(['a', 1, 'b', 5.0])
# todo: is there a benefit from differing bid and ask? </s> distance_to[quote_currency] = distance_to[base_currency] + graph[base_currency][quote_currency]	relax distance_to[quote_currency] = distance_to[base_currency] + graph[base_currency][quote_currency] predecessor[quote_currency] = base_currency predecessor[quote_currency] = base_currency
raise  # todo: what if our seed node fails verification? </s> verify=false,	EvilMiddleWare Try to get Ursula to propagate a malicious (or otherwise shitty) interface ID. mock_client = self._get_mock_client_by_ursula(ursula) data=bytes(shitty_interface_id)) return response
# todo - do these in a single transaction? </s> pass	explicit
# todo: explain these prior terms </s> def fit(self, x, b, t, w=none):	Weibull super(Weibull, self).fit(X, B, T, W, k=1)
pass # todo </s> return t == 'bool'	is_bool
# todo error on too many levels </s> markers = dict(zip(style_levels, markers))	_LinePlotter elif isinstance(markers, dict): pass self.attributes = product(hue_levels, style_levels, size_levels) self.palette = palette
# todo: "wildcards" other than <any> </s> __collect_type_list(n, types)	__type_hierarchy_to_list root_nodes = hierarchy types = [] return types
# todo implement </s> "lpwidecharstr": pointer,	hook_MultiByteToWideChar "dwFlags": UINT, "lpMultiByteStr": STRING, "cchWideChar": INT })
#todo todo todo todo todo todo todo todo todo </s> componenttype = generalname()	GeneralNames class GeneralNames(univ.SequenceOf):
entry['meta']['type'] = 'padding'  # todo handle padding, summarize and transfer </s> postings = realization.get_postings(self.real_accounts)	notes return self._journal_for_postings(postings, Note)
# todo: use madmom.utils.open </s> config.layer_sizes = layer_sizes	cross_validation config.task = task config.bidirectional = bidirectional config.layer_types = [layer_type] * len(layer_sizes) config.momentum = momentum
# todo: this check may hide a bug a should be removed. </s> return valid[default]	query_yes_no sys.stdout.write(question + prompt) choice = raw_input().lower() elif choice in valid: return valid[choice]
# todo: improve the unicode checking </s> else:	_batch return results_dict
# todo: fix </s> log.debug('url: %s %s' % (pkl['url'], url))	wsdl_parse warnings.warn('version or url mismatch! discarding cached wsdl', RuntimeWarning) if debug: force_download = True else:
# todo: series support is not implemented yet. </s> def agg(self, func_or_funcs, *args, **kwargs):	agg
# todo maybe a better function would do here </s> stemmed_word = stem(word)	_build_index for position, word in enumerate(words):
# todo: remove temporary hack giving special status to "*" </s> checked criterion that generated matches for the search.	check_consistency def check_consistency(ann_objs, restrict_types=[], ignore_types=[], nested_types=[]): Searches for inconsistent annotations in given Annotations match_sets = [] m = eq_text_neq_type_spans(ann_objs, restrict_types=restrict_types, ignore_types=ignore_types, nested_types=nested_types)
# todo: bob crashes if he hasn't learned about this ursula #999 </s> m: int = none,	generate_policy_parameters n: int = None, duration: int = None,
# todo: automate detection of max string length to set up numpy array accordingly </s> def set_sheet(self,s):	set_sheet
# todo: span_id= </s> self.exec_opts.showshoptoptions(opt_names)	Shopt if arg.o:  # use set -o names self.exec_opts.ShowOptions(opt_names) return 0 if arg.q:  # query values
# todo pass this to json schema validation </s> def delete(self, list_id, movie_id, session=none):	MovieListMovieAPI 'message': 'could not find movie with id %d in list %d' % (movie_id, list_id)}, 404 return jsonify(movie.to_dict()) try: movie = ml.get_movie_by_id(list_id=list_id, movie_id=movie_id, session=session)
# todo: name, exp_init, exp_end, exp_step, block </s> self.asserttrue(isinstance(node, parser.fieldexp))	testFieldExp node = p._field() self.assertIsNotNone(node) self.assertEqual('foo', node.exp.value.name._data)
# todo generator </s> handle_dirty_dataset(ds, mode=if_dirty)	Drop assert(not completed) for ds_path in content_by_ds: content = [ap['path'] for ap in content_by_ds[ds_path] if ap.get('type', None) != 'dataset' or ap['path'] == ds.path]
# todo: with only non-mandatory model attributes, it is not possible to get an invalid form </s> )	system_exporter_spreadsheet_xls_config_view@83 { 'form': form, else: model = SystemExporterSpreadsheetXlsConfigModel.objects.get(system_exporter_spreadsheet_xls_config_name = 'SystemExporterSpreadsheetXlsConfig')
# todo: assert </s> def createrepo(self):	createRepo repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "name", "testrepo0", self.token)
# todo: verify some output </s> return agent['hostname']	find_agent_hostname if 'slaves' in state: for agent in state['slaves']:
# todo: uncomment when adding support for literal hex bytes </s> x = bytearray([1,2,3])	test_setattr def test_setattr(self): try: x.attr = 42
# todo: act </s> self.fail("good field (%s=%s) raised exception: %s" % (fname, fg, str(e)))	test_create_distro_positive result = self.remote.modify_distro(distro, fname, fg, self.token) self.assertTrue(result) result_save_success = self.remote.save_distro(distro, self.token) self.assertTrue(result_save_success)
# todo log? </s> def title(self) -> optional[str]:	title @property
# todo: look this up in one query </s> result = connection.execute(query, {"playlist_mbid": playlist_mbid})	delete_playlist_by_mbid with ts.engine.connect() as connection:
# todo: remove when botfactory can force everything to be unthreaded </s> assert isup.get_site_url(site) == expected	test_get_site_url @pytest.mark.parametrize('site, expected', VALID_SITE_URLS)
# todo: explicitly exploit symmetry and set onesided=true </s> ) -> torch.tensor:	HolE def forward_inverse_cwa( self, h = self.entity_embeddings.weight r = self.relation_embeddings(batch[:, 0])
# todo: and netcdf writer will be more generic </s> index.storage.add_many(storage_units)	store_datasets storage_type = index.storage.types.get(storage_type_id) _LOG.info('Using %s to store %s datasets', storage_type, datasets)
# todo: the delay was a dirty hack. </s> min( red, 63 )	LedCtrlChar red   *= 21 green *= 21 max( red,  0 ) min( green, 63 )
# todo this dosn't work yet </s> except typeerror as e:	testInvalidTraceback def testInvalidTraceback(self): try: self.assertIn("__traceback__ must be a traceback", str(e)) else:
# todo: add assertions </s> rmses = rmses - rmses[:, :, [0], :]	test_regularization@448 rmses = np.zeros(probes.shape) for i, probe in enumerate(probes.flat): plt.figure(figsize=(8, 12)) X, Y = np.meshgrid(filters, regs)
# todo: just access the original event position, rather </s> self._update_transform()	resize_event
# todo: remove this skip after fixing </s> assert_image_equal("screenshot", 'visuals/circle2.png')	test_circle_draw@20 color=(1, 0, 0, 1), border_color=(0, 1, 1, 1)) gloo.clear() ellipse = visuals.Ellipse(pos=(75, 35, 0), radius=20,
sil_ph = ["sil", "end"]  # todo fix hardcoded values </s> mel_basis = librosa.filters.mel(sampling_rate, fft_size, num_mels, fmin, fmax)	logmelfilterbank spc = np.abs(x_stft).T  # (#frames, #bins) fmin = 0 if fmin is None else fmin return np.log10(np.maximum(eps, np.dot(spc, mel_basis.T))), x_stft
# todo(ahundt) for multi-label try per class sigmoid top as follows: </s> x = atrous_identity_block(3, [512, 512, 2048], stage=5, block='c', weight_decay=weight_decay, atrous_rate=(2, 2), batch_momentum=batch_momentum)(x)	AtrousFCN_Resnet50_16s x = identity_block(3, [256, 256, 1024], stage=4, block='f', weight_decay=weight_decay, batch_momentum=batch_momentum)(x) x = atrous_conv_block(3, [512, 512, 2048], stage=5, block='a', weight_decay=weight_decay, atrous_rate=(2, 2), batch_momentum=batch_momentum)(x) x = Convolution2D(classes, 1, 1, init='he_normal', activation='linear', border_mode='same', subsample=(1, 1), W_regularizer=l2(weight_decay))(x) x = BilinearUpSampling2D(target_size=tuple(image_size))(x)
# todo: ensure it's up to date </s> if 1: ###@@@ maybe should be: if debug_flags.atom_debug:	modflag_asserts def modflag_asserts(self): #bruce 060123; revised 060125 hopetrue = ( (not self._modified) == (self._model_change_counter == self._change_counter_when_reset_changed) ) if not hopetrue:
# todo extend to nonbinary nodes </s> self._raw_current_marbl = self.get_marbl(directions[future],	raw_current_marbl if self._raw_current_marbl is not None: return self._raw_current_marbl normalize=False) return self._raw_current_marbl
# todo: we're in need of more meta schema tests </s> unknown_property="skip"	test_unknown_property_skip validate( 1,
# todo uncomment the actual test below after we have implemented the l1 attack </s> _, adv_label = self.model(x_adv).max(1)	test_attack_strength eps_iter=.05, ord=np.inf, clip_min=.5, clip_max=.7, nb_iter=5, sanity_checks=False) adv_acc = adv_label.eq(ori_label).sum().to(torch.float)\ / self.normalized_x.size(0)
'type': 'string', #todo: resolve </s> def fetch_result_metadata(self):	fetch_result_metadata @query_error_handler
# todo: implement @plist </s> pass	MeiValidityError class MeiValidityError(exceptions21.Music21Exception):
#todo(jogo) make admin=false work </s> def cmd(self, cmd, action, flags='', params='', fail_ok=false):	ClientTestBase self.identity.uri)) flags = creds + ' ' + flags cmd = ' '.join([CONF.cli.cli_dir + cmd, flags, action, params])
pass # todo: explain </s> if not self.response.parsed_hdrs.has_key('location'):	status303 self.setMessage('header-location', rs.REDIRECT_WITHOUT_LOCATION)
# todo what should the swissnum _actually_ be? </s> self.assertequal(version, expected_version)	test_version The client can return the version. version = yield self.client.get_version()
# todo(brian): s/_container/container once other changes propogate </s> :class:`~openstack.object_store.v1.container.container`	get_account_metadata :rtype:
# todo: use regexps </s> return none	_get_event_handler if '_events' in dir(cls): if event in cls._events:
# todo(lucasagomes): backward compatibility with :hexraw, </s> :param mac: a mac address string in the format xx:xx:xx:xx:xx:xx.	_get_pxe_mac_path :param delimiter: The MAC address delimiter. Defaults to dash ('-'). :returns: the path to the config file.
# todo -- can we do this without a subscription? </s> return audio(speech_text).play(stream_url)	play_library first_song_id = queue.shuffle_mode(True) stream_url = api.get_stream_url(first_song_id)
# :todo: implement test. </s> ``transaction`` is not a trytescompatible value.	ReplayBundleRequestFilterTestCase ``transaction`` is null. self.skipTest('Not implemented yet.') self.skipTest('Not implemented yet.') def test_fail_transaction_not_trytes(self):
# todo: put an explanation here </s> p2, v2 = segment._at(t)	_at def _at(self, t): p, v = self.first._at(t) p += p2 v += v2
# todo: implement this </s> self.writestarted.emit(self)	requestWrite def requestWrite(self, nodes: List[SceneNode], file_name: Optional[str] = None, limit_mime_types: bool = False, self._addPrintJobToQueue()
# todo : docker download </s> sys.stderr.write("image not created\n")	docker_destroy_docker target = Path(args.target[0]) unpacked_info = read_dict(target / '.reprounzip') sys.exit(1)
# todo: put the formula in terms of conventions and give a vanilla example </s> parameters	re@49 def re(predicted_power, df_appliances_ground_truth): ---------- predicted_power: Pandas DataFrame of type {appliance :
# todo(brian): s/_container/container once other changes propogate </s> :class:`~openstack.object_store.v1.container.container`	get_account_metadata def get_account_metadata(self): return _container.Container().head(self.session)
# todo private access! </s> if type_var_dict:	define_generics a different type var name. for type_var in self.list_type_vars(): return ValueSet([GenericClass( self,
# todo: depreciated </s> raise modelerror("%s has no name" % object_type)	_get_name except KeyError:
# todo: workaround the fact that skiptest is not defined by unittest2.testcase </s> self.service.restart(timeout=120)	test_autologin def test_autologin(self): reader = service.jobs.oneshot("search index=internal | head 1") self.assertIsNotNone(reader)
# todo test that this is called on next/prev/end-of-track </s> for listener_ref in actorregistry.get_by_class(backendlistener):	_trigger_started_playing_event def _trigger_started_playing_event(self): if self.current_track is None: listener_ref.proxy().started_playing(track=self.current_track)
# todo need to cache this </s> module = importlib('nkms.crypto.block.' + algorithm['symmetric']['cipher'])	symmetric_from_algorithm@21 return module.Cipher
# todo exceptions don't seem to be using parent constructors at all. </s> def __str__(self):	SalesforceAuthenticationFailed def __init__(self, code, message): self.code = code return u'{code}: {message}'.format(code=self.code, message=self.message)
# todo: avoid dummy and generate func here when inlining is possible </s> return lambda df: len(df._data[0])	df_len_overload def df_len_overload(df): if len(df.columns) == 0:  # empty df
# todo also test these! </s> print(w)	test_all_estimators e = E() clone(e)
raise exceptions.mpdnotimplemented  # todo </s> underscore, dash, dot and colon.	subscribe@18 ``subscribe {NAME}`` Subscribe to a channel. The channel is created if it does not exist raise exceptions.MpdNotImplemented  # TODO
# todo: if py3k, override unpickler.find_class(). </s> return self.pl_anchors	get_anchors
# @todo: check permissions & avoid db updates in gets </s> return s3db.req_send_commit()	send_commit def send_commit():
# :todo: implement test. </s> self.assertfilterpasses(filter_)	GetInclusionStatesRequestFilterTestCase bytearray(self.trytes2), ], self.assertDictEqual( filter_.cleaned_data,
# todo: @sbharadwajj implement and test </s> raise notimplementederror	Conv1DDerivatives def _bias_jac_mat_prod(self, module, g_inp, g_out, mat): raise NotImplementedError def _weight_jac_mat_prod(self, module, g_inp, g_out, mat): raise NotImplementedError
#todo assert old_r.pubmed_id == new_r.pubmed_id </s> short = record.seq.tostring()[:19] \	checksum_summary if len(record.seq) < 25 : short = record.seq.tostring() + "..." + record.seq.tostring()[-3:] return "%s [%s] len %i" \
# todo: en passant. </s> def shift_2_right(b):	shift_2_right
# todo: fix with stubber / before send event </s> _send.return_value = mock.mock(	TestApiGateway 'accepts': 'application/yaml' } status_code=200, headers={}, content=b'{}') self.client.get_export(**params)
# todo implement. </s> validation_split  = parameters['validation_split']	SparkModel nb_epoch          = parameters['nb_epoch'] batch_size        = parameters['batch_size'] self._train(self.dataset_rdd, nb_epoch, batch_size, verbose, validation_split) def _train(self, rdd, nb_epoch, batch_size, verbose, validation_split):
# todo (#2743, see also #2556): make a portable constant or remove completely </s> joins two retrievalresult objects.	with_result If both objects contain cfrags from the same Ursula, the one from `result` will be kept.
elif room_hosts:  # todo: shouldn't this be remote_room_host? </s> )	get_pagination_rows with_feedback=True
# todo: implement me </s> depth = utils.tensor_to_gradcheck_var(depth)  # to var	_test_gradcheck def _test_gradcheck(self): image = self.image.clone() image = utils.tensor_to_gradcheck_var(image)  # to var assert gradcheck(tgm.losses.depth_smoothness_loss,
#todo this is wrong - need to add to biases only on no reuse </s> self.reuse_scope_count = 0	reuse self._reuse = True
# todo: calculate and write fingerprint </s> def verify_admin(self, admin_pin):	verify_admin
# todo: consider alternatives to using pillow here. </s> limit_classes, classes	imagenet random.seed(seed) classes = random.sample(classes, limit_classes) )) if only_filename:
# todo: exception for coalesces that represents all sub_specs tried </s> class d(b):	D
# todo: store this data </s> value = value * 91 + ord(ch) - 33	_parse_base91 def _parse_base91(text): value = 0 return value
# todo: investigate why this fails </s> firstmodel = first_model_def.model_class()	test_simple_foreign_key_between_mutable_models first_model_def = self.model_def second_model_def = ModelDefinition.objects.create(app_label='app', SecondModel = second_model_def.model_class() ForeignKeyDefinition.objects.create(model_def=first_model_def,
# todo: allow configurable headers. </s> boto_connection = boto.connect_s3(	bucket @webapp2.cached_property self.config.access_key, self.config.access_secret, calling_format=connection.OrdinaryCallingFormat())
# todo(yanase): check values </s> assert len(mus) == 2	TestParzenEstimator assert len(weights) == 2
# todo skip this in the future </s> return (lines[3][0], lines[3][1], lines[3][2])	process_image proc = subprocess.Popen(["pnmscale", "-xsize", "1", "-ysize", "1"], stdin=subprocess.PIPE, stdout=subprocess.PIPE) out = proc.communicate(tmp[0])
# todo(hirofumi0810) fix this for after supporting transformer </s> xs_pad, ilens, ys_pad = super().__call__(batch, device)	CustomConverter_v2 Returns: tuple(torch.Tensor, torch.Tensor, torch.Tensor) if self.asr_task: ys_pad_asr = pad_list([torch.from_numpy(np.array(y[1])).long()
# todo: add logging to indicate the failure </s> def add_router(self, router: ipeerrouting) -> none:	add_router
# todo: add location info for invalid backslash </s> otherwise.	EvalWordToPattern def EvalWordToPattern(self, word): NOTE: Have ot handle nested extglob like: [[ foo == ${empty:-@(foo|bar) ]] pass
# todo action required that updates the endpoint </s> ip_endpoints = self.sdnc.endpoints_by_ip(device)	_get_endpoints if hash_endpoint: eps.append(hash_endpoint) if len(ip_endpoints) > 0: return ip_endpoints
#if (have_kratos is true): # todo: implement natively </s> if write_binary:	_write_nodes def _write_nodes(fh, points, write_binary): dtype = [("index", numpy.int32), ("x", numpy.float64, (3,))] tmp = numpy.empty(len(points), dtype=dtype)
# todo track_set_seen() </s> 'playlistfolder', ['id', 'name', 'type'])):	PlaylistFolder pass
# todo: support multi-index here </s> set the index to become the 'month' column:	set_index 1      4  2014    40 2      7  2013    84 >>> df.set_index('month')  # doctest: +NORMALIZE_WHITESPACE year  sale
# todo: larger gains expected with scipy.signal.signaltools.fftconvolve(). </s> def should_continue(f, *args):	should_continue
# todo(aditya): temporarily we are filtering out todos </s> .exists())	worker_has_reviewer_status .filter(worker=worker, role=WorkerCertification.Role.REVIEWER, return has_reviwer_status
# todo: make locking work for mssql </s> yield (	check_conn_type_null session.rollback() pass 'The conn_type column in the connection '
# todo: move this into the operations code for its caller </s> if val not in ("", model_pam3, model_pam5):	readmmp_info_chunk_setitem pass print "fyi: info chunk save_as_pam with unrecognized value %r" % (val,) # deferred_summary_message? val = ""
# todo(lbragstad): sleeping after the response status has been checked </s> cls.username = cls.creds.username	IdentityUsersTest def resource_setup(cls): super(IdentityUsersTest, cls).resource_setup() cls.password = cls.creds.password cls.tenant_name = cls.creds.tenant_name
# todo: this is a case we should deal with, but there are probably </s> best_len = int(round(ws_len * 1.0 / tab_space_size)) * tab_space_size	L003_eval ws_len = segment.raw.count(' ') if ws_len % TAB_SPACE_SIZE != 0: return LintResult( anchor=segment,
# todo: fix this </s> self.stop()	_onLoginStateChanged if is_logged_in: self.start()
# todo implement this </s> def __init__(self, event_dict):	ScriptEvent "target_view": VIEW_ID, "event_type": EVENT_TYPE_VAL self.event_dict = event_dict @staticmethod
# todo - this should be moved to the `finalize` method of the base resource, as it's not cross-service </s> self.parse_elb_policies_callback,	_parse_elb_policies [],
# todo: tf and jax sort [inf, nan] differently. </s> ]:	test_prngsplit np.array([0xFFFFFFFF, 0], dtype=np.uint32), np.array([0, 0xFFFFFFFF], dtype=np.uint32), self.ConvertAndCompare(f_jax, rng_key)
## fixme: # todo: remove me </s> paste_parent = self.r_serv_onion.hget('onion_metadata:{}'.format(self.domain), 'paste_parent')	get_last_crawled_pastes def get_last_crawled_pastes(self):
# todo: add and store preprocessing errors. </s> if system_product:	LinuxDistributionPlugin system_product = text_file_object.readline() system_product = system_product.strip() knowledge_base.SetValue('operating_system_product', system_product)
# todo: change docstring or remove unsqueeze(-1) </s> return torch.zeros((batchsize, n_atoms, n_atoms - 1, 3))	cell_offset @pytest.fixture
# todo col.type.python_type contains the type that </s> def sa_identity_key(self):	sa_identity_key @property
# xxx todo </s> error checking for most win32 api calls.	RaiseIfZero The function is assumed to return an integer, which is C{0} on error. In that case the C{WindowsError} exception is raised.
# todo(rosmaita): bug #1745003 </s> def _pre_upgrade_ocata_expand01(self, engine):	_pre_upgrade_ocata_expand01
# todo: remove in v.0.6 </s> csep = class_separation(cov.transform(self.iris_points), self.iris_labels)	TestCovariance def test_iris(self): cov = Covariance() self.assertAlmostEqual(csep, 0.72981476)
except exception:  # todo - which exceptions? </s> for organism_element in element:	_parse_organismHost if organism_element.tag == NS + 'name': append_to_annotations("organism_host", organism_element.text)
# todo: consider filtering by location type </s> if messy_date_string:	clean_date cleaned_datetime = parse(messy_date_string) return cleaned_datetime.date()
#todo: make more general (if possible) </s> ndarray_type_dict = db_dict['ndarray_types'].get(record_dict['ndarray_type_tag'])	get_ndarray_config ndarray_types = database.submit_query(SQL) for record_dict in ndarray_types.record_generator(): if ndarray_type_dict is None: ndarray_type_dict = {'ndarray_type_tag': record_dict['ndarray_type_tag'],
# todo: let the globe return the semimajor axis always. </s> def boundary_distance(xy):	boundary_distance
pass # todo: explain </s> if not self.response.parsed_hdrs.has_key('location'):	status303 def status303(self):        # See Other
# todo: fix as soon as we have preprocessvector (different parallel dict preprocessor for different spaces in a dict) </s> return self.call(self.preprocessor[key].preprocess, states)	ActorComponent ret = (preprocessed_states, actions) + ((last_internal_states,) if last_internal_states else ()) return ret
# temporary for testing. todo: remove </s> self.events[bought_asset].buys.append(	add_buy_to_events fee_cost = fee_price_in_profit_currency * trade_fee gross_cost = bought_amount * buy_rate BuyEvent( amount=bought_amount,
# todo:  we might need additional logic comparing the state of git-annex </s> )	Publish git_opts=git_opts, annex_opts=annex_opts, @staticmethod @datasetmethod(name='publish')
# todo: test that valueerror is raised </s> _map = dict(zip(self.node_indices, self.node_labels))	indices2labels return tuple(_map[index] for index in indices)
# todo: remove </s> return lookup_group_plugin(group_type).form_to_db_schema()	_form_to_db_schema
# todo(yanase): remove number from system_attrs after adding trialmodel.number. </s> with pytest.raises(valueerror):	test_get_study_id_from_name_and_get_study_name_from_id study = optuna.create_study(storage=storage, study_name=study_name) assert storage.get_study_name_from_id(study.study_id) == study_name storage.get_study_id_from_name('dummy-name') with pytest.raises(ValueError):
#ng_maxlength="30",     # todo: validation </s> args:	PrimaryLocationWidget Options for this field are dynamically set in JS depending on what options are selected for 'assigned_locations'. This works in conjunction with LocationSelectWidget. css_id: css_id of primary_location field source_css_id: css_id of assigned_locations field
#todo: add way to check if alt is pressed </s> if self.debug:	addDebugString def addDebugString(self, string):
# todo signals instead of direct dialog creation? </s> else:	NoteModel self.anki_dict = utils.merge_dicts(note_model_dict, self.anki_dict) if new_model: collection.models.update(self.anki_dict) collection.models.flush()
#todo: handle ipv6 </s> :param device_id: device identifier	get_device_instance def get_device_instance(self, device_id, instance_dict): :param instance_dict: dictionary containing the instances :returns: device instance
# todo(jaypipes): port nova's fault infrastructure </s> res_dict = json.loads(res.body)	test_get_images } req = webob.Request.blank('/images') self.assertEquals(res.status_int, 200) images = res_dict['images']
# todo: detect if any database upgrading is needed and acquire the lock only in one place </s> exception to be thrown during a db upgrade function which will cause the old tables to be removed and recreated from	UpgradeImpossible the new model.
pass  # todo </s> pass  # todo	TestSheet def test_pictures(self): pass  # TODO
# todo: fill the blank space at the bottom of the page </s> def block_replaced_width(box, containing_block, device_size):	block_replaced_width replaced_box_width.without_min_max(box, device_size) block_level_width.without_min_max(box, containing_block)
# todo: send `goodbye` req then disconnect </s> return self.listen_maddr.encapsulate(multiaddr(f"/p2p/{self.peer_id}"))	listen_maddr_with_peer_id @property
# todo add support </s> :return: str -- the current indentation (e.g. "  ").	ind hierarchy.
# todo consolidate </s> v = parser.get_token() or none	migrate_rc break k = k.lower() if v is not None: try:
@unittest.skip('not written')  # todo: finish! </s> def test_simplenamespace(self):	test_SimpleNamespace @py3_only
# todo: let the globe return the semimajor axis always. </s> def boundary_distance(xy):	boundary_distance
#todo: decide if this method really adds anything of value ... </s> raise exception("string {s} does not match character class {cc}".format(s=a_string, cc=char_class))	assert_matches_character_class def assert_matches_character_class(self, char_class, a_string):
# todo: figure out why this fails </s> np.testing.assert_equal(actual_brle, expected_brle)	test_brle_strip actual_brle, actual_padding = rl.brle_strip(brle_data)
# todo(guillermooo): add regexes </s> if viewinspector(view).is_pubspec:	DartBuildProjectCommand @action One of: 'primary', 'secondary' self.window.run_command('dart_build_pubspec', { 'action': action,
# todo: look this up in one query </s> delete from playlist.playlist	delete_playlist_by_mbid Returns: True if the playlist was deleted, False if no such playlist with the given mbid exists WHERE playlist.mbid = :playlist_mbid with ts.engine.connect() as connection:
# todo raise exception if source_group is none? </s> ):	EBSBackend raise InvalidVolumeIdError(volume_id) def attach_volume( volume = self.get_volume(volume_id) instance = self.get_instance(instance_id)
# todo: add and store preprocessing errors. </s> if not knowledge_base.getvalue('operating_system_product'):	LinuxDistributionPlugin text_file_object = dfvfs_text_file.TextFile(file_object, encoding='utf-8') system_product = text_file_object.readline() if system_product: knowledge_base.SetValue('operating_system_product', system_product)
# todo: inform cluster </s> try:	scheduler_server sock = socket.socket(addrinfo.family, socket.SOCK_STREAM) sock = AsyncSocket(sock, keyfile=self.cluster_keyfile, certfile=self.cluster_certfile) sock.bind((addrinfo.ip, self.scheduler_port)) except:
# todo: non standard, uses convs </s> clf_module.is_classifier = true	mark_classifier clf_module = get_classifier_module(model)
# todo: support minp arg end_range etc. </s> def rolling_fixed(arr, win):  # pragma: no cover	rolling_fixed
# todo: switch to: </s> return self.current_tl_track	get_current_tl_track
# self.assertisnotnone(result_set.service_processing_time_in_millis)  # todo flaky test </s> from many_rows a	test_cancel def test_cancel(self, cursor): query_id, future = cursor.execute( CROSS JOIN many_rows b )
# todo: cleanup directory/basename.* files. </s> elif field == 'track':	LocalLibraryProvider artist_filter(t) or uri_filter(t)) if field == 'uri': result_tracks = filter(track_filter, result_tracks) elif field == 'album':
# todo: restore... needed for complete compatibility with tomxobjects... </s> return mxstafflayout	staffLayoutToXmlStaffLayout seta(staffLayout, mxStaffLayout, 'staff-distance', 'distance') setb(staffLayout, mxStaffLayout, 'number', 'staffNumber')
# todo: check if "class" in current line, add class name </s> count = 0	countspaces while count < len(txt) and txt[count] == ' ': count += 1
# todo: remove this skip after fixing </s> assert_image_equal("screenshot", 'visuals/reactive_ellipse5.png')	test_reactive_draw@108 gloo.clear() ellipse.start_angle = 140. gloo.clear() ellipse.span_angle = 100.
# todo(b/183565702): support integer convolutions on cpu/gpu. </s> operands = {'x': [np.ones(5), np.arange(5)]}	test_reduce_correctly_works_with_pytrees init_values = {'x': [0., 0]} result = lax.reduce(operands, init_values,
# todo: remove this and figure out why queue thread does not properly exit </s> del labels	_error_rate_in_batches del poses
# todo: this will incorporated in the future, if needed. </s> requires from net_params:	I210SubNetwork * **on_ramp** : whether to include vehicle on the on-ramp * **ghost_edge** : whether to include the downstream slow-down edge in the
# todo check if varnos are exactly the crosscat variables. </s> return 'normal', {}	_default_numerical
# todo: check </s> proj_neg_heads = self._project_entities(neg_h_embs, proj_matrix_embs)	TransR self.entity_embedding_dim) proj_pos_heads = self._project_entities(pos_h_embs, proj_matrix_embs) proj_neg_tails = self._project_entities(neg_t_embs, proj_matrix_embs) pos_score = self.compute_score(h_embs=proj_neg_heads, r_embs=pos_r_embs, t_embs=pos_t_embs)
# todo: this stuff should be generated by a template of some sort </s> head += '<meta http-equiv="refresh" content="%d">\n' % reload_time	head head = '' reload_time = self.get_reload_time(request) return head
# todo: make it pass </s> self.assertequals((1, 4), finder.last_open_parens())	test_get_last_open_parens2 def test_get_last_open_parens2(self):
# todo: add for morph targets data. </s> rotation = convert_swizzle_rotation(rotation)	decompose_transition translation, rotation, scale = matrix.decompose() if context == 'NODE': scale =  convert_swizzle_scale(scale) rotation = mathutils.Quaternion((rotation[1], rotation[2], rotation[3], rotation[0]))
# todo(twd2): do more visibility check eg. contest </s> self.response.text = drrdoc['content']	DiscussionTailReplyRawHandler ddoc = await discussion.get(self.domain_id, did) drdoc, drrdoc = await discussion.get_tail_reply(self.domain_id, drid, drrid)
# end todo </s> module, grad_output, sqrt_ggn_out)	diag_ggn def diag_ggn(module, grad_output): sqrt_ggn_out = CTX._backpropagated_sqrt_ggn
# todo: figure out way to paramaterize this test </s> assert mountpoint(osd_path).exists	TestOSDs cluster=node["cluster_name"], osd_id=osd_id,
# todo message </s> | 0.711009	0.687453	RegisterImport | Example File Format | Created by Angus | 0.246540	0.433973 | 0.926871	0.887255
#todo: increase timeout based on number of plugins </s> if not addr:	get_base_url def get_base_url(request): proto = 'https' if request.is_secure() else 'http' addr = request.get_host() else:
# time.sleep(40)  # todo: should remove after polling get. </s> tensor_pointer_2 = data_2.send(clients[1])	test_tensor_abstraction_subsets@73 data_2 = Tensor(child=np.array([[567, 98], [78, 25]], dtype=np.int32)) data_3 = Tensor(child=np.array([[125, 10], [124, 28]], dtype=np.int32)) tensor_pointer_3 = data_3.send(clients[2]) mpc_1_2 = op(tensor_pointer_1, tensor_pointer_2)
# todo(rbharath): how does distance need to be modified here to </s> for ind, (z_cell_min, z_cell_max) in enumerate(z_bins):	put_atoms_in_cells break if y_ind is None: if z_coord >= z_cell_min and z_coord <= z_cell_max: z_ind = ind
# todo refactor to .session </s> set_session_value('ex_substitute_last_pattern', pattern, persist=true)	set_ex_substitute_last_pattern
# todo refactor set position cursor after operation into reusable api. </s> for i in range(count):	_vi_k_select if mode != modes.SELECT: utils.blink() self.view.window().run_command('soft_undo') return
# todo: insert more test values here </s> self.assertraises(zerodivisionerror, impedance_to_norm, 0, 0)	test_impedance_to_norm self.assertEqual(impedance_to_norm(0), 0) self.assertEqual(impedance_to_norm(50), 1)
self.setup()  # todo: perhaps, remove this to pass path in context </s> return all_current_ids_hash	_hash_data_container if all_current_ids_hash is not None: m.update(str.encode(all_current_ids_hash))
# todo[lauren]: add proper authorizers to draftregistrationapproval </s> json.dumps(all_drafts),	get_drafts :return: True if general administrator False if not all_drafts = get_all_drafts() content_type='application/json'
# todo: use validation set if not-none </s> for small data sets but deleting such intermediate results can	forget_task def forget_task(self, task_name): Signal that it is OK to delete any features / statistics etc related be crucial to keeping memory use under control. pass
# todo(twd2): check permission for visibility. (e.g. test). </s> async def get(self):	RecordMainView@52 @app.route('/records', 'record_main') rdocs = await record.get_multi().sort([('_id', -1)]).to_list(50) await asyncio.gather(user.attach_udocs(rdocs, 'uid'),
csv_reader = caches[caches_celery_query_result_key].get(_result_key(notebook)) # todo check if expired </s> result = download_to_file.asyncresult(notebook['uuid'])	_patch_status def _patch_status(notebook):
# todo dry </s> self.handles[uuid] = int(re.match("\x1b\[khandle: 0x([a-fa-f0-9]{4})",	get_handle raise BluetoothLeError(self.con.before) else: matching_line).group(1), 16) return self.handles.get(uuid)
# todo(python3): modify pool to context manager (with statement) </s> if self.verbose:	create_db_index result = conn.execute(stmt) conn.close() exec_time = toc-tic print('Time to create index: %.2f secs' % exec_time)
# todo: run this with mock file </s> result = file.read()	TestTemplateOptions selection = SinglePage(notebook, page) with tests.LoggingFilter('zim.formats.latex', 'Could not find latex equation'): self.assertIn('\section{Head1}', result) # this implies that document_type "article" was indeed used
1  # todo: fill in identifier </s> highest_time = 0	print_stats line = list(' ' * (recursion + 1)) line[7::7] = len(line[7::7]) * '.' full_stack = [] for stat in ordered_stats:
# todo: test for external ring source values is missing as it needs </s> self.chart_object.yscale("datetime")	test_yscale def test_yscale(self):
# todo ?? </s> def id(self) -> str:	id return str(self.row['_id'])
# todo defensive? </s> class user_config: # type: ignore[no-redef]	user_config
# todo: deal with error </s> self = cls(fontdata, fontnumber)	fromPath if fontData is None: with open(fontPath, "rb") as f: return self
# todo: handle % widths </s> current_line += replaced_preferred_width(child)	inline_preferred_width current_line = 0 for child in box.children: else: assert isinstance(child, boxes.TextBox)
# todo: finish this. </s> try:	_GDriveFS raise FuseOSError(EIO) def mkdir(self, filepath, mode): (parent_clause, path, filename, extension, mime_type, is_hidden, \ just_info) = _split_path(filepath)
# todo(b/178123173) enable tests after b/193022465 is resolved. </s> "requirements.txt"),	auto_one_device_strategy entry_point=os.path.join(self.test_data_path, "mnist_example_using_fit.ipynb"), job_labels={ "job": "auto_one_device_strategy",
# todo: sync the doc. </s> return first_series(super().idxmin(skipna))	idxmin
#  todo: test </s> src.getmodifieditemattr("shipbonusics5"),	handler@15 def handler(fit, src, context): fit.drones.filteredItemBoost(lambda drone: drone.item.requiresSkill("Ice Harvesting Drone Operation"), skill="Industrial Command Ships"
# todo: move space inference here. </s> return x	deep_tuple elif isinstance(x, dict): return type(x)(dict(map(lambda i: (i[0], deep_tuple(i[1])), x.items())))
# todo: out to file </s> if res.status_code not in self.ignored_error_codes:	_fetch url = "{}://{}.{}".format(self.proto, uri, self.target) try: self._print_response(res.status_code, url, res.headers) except RequestHandlerException as e:
# todo: deprecate this?  properties are generally preferred over "set*()" </s> return self.unfix()	free
# todo: the following skipped suite and fixtures should be enabled </s> @pytest.fixture(autouse=true)	VultrProviderTests domain = 'capsulecd.com' def _filter_headers(self): def skip_suite(self, request): if request.node.get_marker('ext_suite_1'):
# todo(elliot): include the rest of the necessary keys </s> cmd = "autopkg search -p %s" % app_name	create_existing_recipe_list exitcode, out, err = get_exitcode_stdout_stderr(cmd) if exitcode == 0:
# @todo: widget? </s> title_upload = t("import answers"),	DataCollectionModel title_display = T("Answer Details"), title_list = T("Answers"), label_list_button = T("List Answers"), label_delete_button = T("Delete Answer"),
# todo: renable when t34648262 is fixed </s> self, t1: torch.tensor, s1: torch.tensor, fn	_manual_broadcast_matrix_scalar ) -> torch.Tensor: return fn(t1, (t1 * self.zero_tensor) + s1).float()
# todo: this should support multi-db </s> self.assertraises(assertionerror, self.assertcolumnexists,	assertColumnDoesntExists table_name, field_name)
# todo(dcramer): this should respect rate limits/etc and use the normal </s> def is_enabled(self):	is_enabled
# todo ideally this happens a layer higher, but this is a bad </s> dataset_source = dataproviders.dataset.datasetdataprovider( dataset )	chunk_dataprovider @dataproviders.decorators.dataprovider_factory( 'chunk', dataproviders.chunk.ChunkDataProvider.settings ) return dataproviders.chunk.ChunkDataProvider( dataset_source, **settings )
# todo align api and test after - now tip_id is ignored </s> r5	TipsCollection This interface return the summary list of the Tips available for the authenticated Receiver GET /tips/<receiver_token_auth/tip
#todo: dont unfold all, but allow enum_all() to work </s> menu_proc(self.h_menu, menu_show, command='')	tree_on_menu def tree_on_menu(self, id_dlg, id_ctl, data='', info=''):
# todo: try/catch </s> a = class_for_name(actor["module"], actor["class"])	setup_actors def setup_actors(self): act = a(actor["id"], actor["params"]) self.actors.append(act)
# todo: change this to be architecture independent </s> return dbgdword(ea)	get_adrs_mem if nativeSize is 16: return DbgWord(ea) if nativeSize is 64: return DbgQword(ea)
# todo: commented out so we don't prompt for installing vc or vcp until they </s> 'agentid': 'sqlhistorian-sqlite',	do_platform_historian def do_platform_historian(): datafile = os.path.join(get_home(), 'data', 'platform.historian.sqlite') 'connection': { 'type': 'sqlite',
# todo: refactor common tests for all models, e.g. shape checking </s> trans_h = transh(triples_factory=self.factory)	test_trans_h def test_trans_h(self):
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
raise skiptest() # todo fixme </s> assert not os.path.exists("artifacts")	test_setup_with_dexy_conf_file dexy.commands.run() assert os.path.exists("custom")
# todo: warn on failure to delete? </s> l.append(s)	__generate_input_and_label s += '%s<span class="accesskey">%s</span>%s</label>' % (escape(dt[:key_offset]), escape(dt[key_offset:key_offset+1]), escape(dt[key_offset+1:]))
# todo -- validate other options </s> targets = config.get('general', 'targets')	validate_config def validate_config(config): if not config.has_section('general'): if not targets: die("No targets= item in [general] found.")
# todo: use utils.normalize when rebased onto develop </s> def micro_indices(self):	micro_indices return tuple(sorted(self.hidden_indices + self.output_indices))
# todo: seems to be doing < rather than <= ??? </s> if _box_in_box(el, el2): return -1	_comp_bbox if _box_in_box(el2, el): return 1
# use the mean of the previous noise values (todo: be smarter here). </s> example:	FixedNoiseGP `.posterior` on the model will be on the original scale). input_transform: An input transfrom that is applied in the model's >>> train_X = torch.rand(20, 2) >>> train_Y = torch.sin(train_X).sum(dim=1, keepdim=True)
# todo: clean up </s> host=host,	_make_pubsubs ) return tuple( router=router, my_id=host.get_id(),
# todo check behavior when not loaded </s> will always return :class:`none` if the album isn't loaded.	is_available @property if not self.is_loaded: return None
#@todo: remove in 0.4.10 </s> pages = 1	handleMultiPages m = re.search(self.PAGES_PATTERN, self.html) pages = int(m.group(1)) for p in xrange(2, pages + 1): self.html = self.loadPage(p)
# todo: ignore models which were killed early by scheduler (eg. in hyperband). how to id these? </s> logger.debug('feature_importances_unused: %s' % len(feature_importances_unused))	debug_features_to_use feature_importances_used = feature_importances[feature_importances['splits'] >= (total_splits/feature_count)] logger.debug(feature_importances_unused) logger.debug('feature_importances_used: %s' % len(feature_importances_used)) features_to_use = list(feature_importances_used['feature'].values)
# todo remove! </s> class alreadyevaluated(frozenset):	AlreadyEvaluated
raise skiptest  # todo: figure out why this randomly started failing. </s> results = wc.query('marque-page')	test_clean_hyphens def test_clean_hyphens(self): eq_(1, len(results))
# todo: distributed search messages need to implemented </s> self.chatrooms.roomsctrl.userjoinedroom(msg)	UserJoinedRoom def UserJoinedRoom(self, msg): else: self.logMessage("%s %s" %(msg.__class__, vars(msg)), 4)
# todo: optimize queries </s> if len(parts) < 2:	is_valid_origin if ':' in value: return False return False return True
# todo: remove in galaxy 20.xx, for running jobs at gx upgrade </s> job = self.get_job()	default_compute_environment def default_compute_environment(self, job=None): return SharedComputeEnvironment(self, job)
# todo: speedup by allocating the denominator directly instead of constructing it by sum </s> big = tf.cast(big,dtype=dtype)	get_minibatch_features big = np.zeros((batch_size, batch_size)) big += np.eye(batch_size) abs_dif = tf.reduce_sum(tf.abs(tf.expand_dims(activation,3) - tf.expand_dims(tf.transpose(activation, [1, 2, 0]), 0)), 2) mask = 1. - big
reorder_attributes(root_element)  # todo: remove when support is python 3.8+ </s> raise valueerror(msg)	get_score_index except ValueError: msg = 'Unable to get the score index for Tally since "{}" ' \ return score_index
# todo: maps of packages </s> self.log(util.red('child.after: \n' + child.after + '\n'))	_check_exit if res not in exit_values or res == None: if res == None: self.log(util.red('Exit value from command+\n' + send + '\nwas:\n' + res)) msg = '\nWARNING: command:\n' + send + '\nreturned unaccepted exit code: ' + res + '\nIf this is expected, pass in check_exit=False or an exit_values array into the send_and_expect function call.\nIf you want to error on these errors, set the config:\n[build]\naction_on_ret_code:error'
# todo: uncomment this test case when #1217 is fixed. </s> assert link.url.startswith("file://")	test_finder_priority_file_over_page assert all(version.location.scheme == 'https' for version in all_versions[1:]), all_versions
# todo - fix meta.submission to point to real submission </s> get_rows for documentation of the parameters.'''	_get_cursor sql = " SELECT s.*, su.submit_time FROM %s s " % self.table_name + \ " JOIN xformmanager_metadata m ON m.raw_data=s.id " + \
# todo: test </s> raise valueerror("dag_id should be provided")	task_state execution_date = request.args.get('execution_date') subdir = request.args.get('subdir') if task_id is None: raise ValueError("task_id should be provided")
# todo(b/160795287): deprecate estimator based executor. </s> fn_args.serving_model_dir, fn_args.model_run_dir)	GenericExecutor raise RuntimeError('run_fn failed to generate model.') absl.logging.info(
# todo(bnemec): this should be documented as an ivar, but can't be due </s> this works the same as :meth:`.post_execute`, but for the revert phase.	post_revert
# todo: https://github.com/fonttools/fonttools/issues/842 </s> anon = self.parse("anon test { # a\nfoo\n } test; # qux").statements[0]	test_anon self.assertIsInstance(anon, ast.AnonymousBlock) self.assertEqual(anon.tag, "TEST")
# todo(b/160795287): deprecate estimator based executor. </s> io_utils.copy_dir(source, dest)	copy_model else: raise ValueError('Invalid input tag: {}.'.format(tag)) absl.logging.info('%s model copied to: %s.', tag.capitalize(), dest)
# todo: update this when we have replaced elements that do not </s> assert resume_at is none	add_word_spacing new_box, resume_at, _ = split_text_box( document, box, 1e10, 0) x_advance += extra_word_spacing * nb_spaces box.width = new_box.width
# todo todo todo </s> value_min=0.5, value_max=1.0,	__on_single_scan_ocr self.set_progression(src, 0.5, _("Reading ...")) self.__scan_start = time.time() total_time=self.__config.scan_time['ocr'])
# todo permissions scoping </s> }	EditProgramView }, 'pagination_limit_options': range(self.DEFAULT_LIMIT, 51, self.DEFAULT_LIMIT), @property def program_id(self):
# todo subject.cn from cert? </s> assert proc.returncode == 0, "return code not 0"	TestMac stdout=subprocess.PIPE, stderr=subprocess.STDOUT) return self.codesign_display_parse(out) def codesign_display_parse(self, out):
# todo check executions for dict contents </s> :param position: the position as a line/column tuple, default is infinity.	_get_defined_names_for_position names are returned. :type     scope: :class:`parsing_representation.IsScope` names = scope.get_defined_names() if (not position or isinstance(scope, (iterable.Array, er.Instance))
if self.is_direct_mode() or batch or not allow_quick:  # todo: thin mode </s> self._run_annex_command('rmurl', annex_options=[file_] + [url])	rm_url ---------- file_: str
# todo: why does pipeline not look like a regular result? </s> pipeline: pipeline = self.driver.pipeline(flush_every=0)	test_fails_on_bad_syntax with self.assertRaises(CypherError): pipeline.push("X")
# todo: what's the right status code here?  202?  different if we already knew about the node(s)? </s> response = response("no treasure map with id {}".format(treasure_map_id),	provide_treasure_map log.info("{} providing TreasureMap {}".format(this_node.nickname, treasure_map_id)) except KeyError: status=404, headers=headers) return response
# todo: wells if display config has more than one column </s> bar = ('username', 'userid')	mycmp def mycmp(x, y): if x in foo and y in foo: return -1 if foo.index(x) == 0 else 1
# todo: eliminate asap, for backwards compatibility only </s> params["headers"] = headers	build_headers headers = params.get("headers", {}) self.add_content_type(headers) return headers
# todo: we might want to handle still_alive, e.g. to allow for </s> return self._get_log_setting('env')	log_env @property
# todo: share code with the loader attribute here. </s> template_rel_path = none	CustomizedTemplate Pass False for no extension (i.e. extensionless template files). template = None template_rel_directory = None template_name = None
# todo: cache the list of components that were deactivated </s> else:	LinearComplementarity_BilevelTransformation exp += B2_[uid] * ub_dual if type(exp) in six.integer_types or type(exp) is float: block.c1.add( exp == 0 ) return block
# todo: how to compare images? </s> ax.text(0.5, 0.5, "hello world!", ha='center', va='center', size=24)	test_figure fig, ax = tfplot.subplots(figsize=(4, 4))
raise  # todo </s> with open(registrar_filepath, 'w+') as registrar_file:	_write_registrar_file Writes the registrar data dict as JSON to the registrar file. If no file exists, it will create it and write the data. If a file does exist registrar_file.seek(0) registrar_file.write(json.dumps(registrar_data))
# todo(pkilambi): process the output as needed </s> return false	service_delete LOG.error("Couldn't delete service  %s due to error %s" % (uuid, e))
# llamo al método remoto: </s> wsdl = "http://pruebas.ecodex.com.mx:2044/servicioseguridad.svc?wsdl"	TestCFDI class TestCFDI(unittest.TestCase): client = SoapClient(wsdl=WSDL, ns="ns0", soap_ns="soapenv") retval = client.ObtenerToken(RFC="AAA010101AAA", TransaccionID=1234)
# todo ... </s> x = dot(input_data.placeholder, w)	LinearLayer b = None with tf.name_scope("linear"): if self.with_bias: x = tf.add(x, b, name="add_bias")
#todo: check if/where this is used; if not used externally - remove </s> self._update_ui()	remove_surface self.surfaces.remove(surface)
pytest.config.argon_skip_now("argon transformer error")  # todo triage </s> init_inner=constantinit(w_recur_val),	test_change_recurrent_axis_length activation=Tanh()) rec3 = recurrent_layer_cls(nout=hidden_size, activation=Tanh()) N = ng.make_axis(length=batch_size, name='N')
# todo: use mode, state </s> def addfillpath(self, *args, **kwargs):	addFillPath
pass # todo </s> def isreadonly(self):	isreadonly return self['Desktop Entry']['X-Zim-ReadOnly']
# todo: targets are always updated if destination directory is new, right? </s> logger.exception(error_message)	__check_configuration_on_add except: error_message = \ raise InvalidConfiguration(error_message) return repository_mirror_hostnames
# todo: remove compatability hook </s> new_exe.__dict__.update(exe.__dict__)	get_executables for ln in fIn: fOut.write(ln) new_exe.script = script exe = new_exe
if self._ndim == 3: # todo: use hasz </s> def coordinates(self):	coordinates return self.array
# todo(vek): need to pass context in for access to auth_token </s> self.state = state	InstanceInfo def __init__(self, name, state): self.name = name
#todo fixme: we need to check that we aren't adding a duplicate </s> c += 2	main@52 claim = pywikibot.Claim(repo, claims[c]) claim.setTarget(pywikibot.ItemPage(repo, claims[c+1])) generator = gen.getCombinedGenerator() for page in generator:
# todo(b/147499373): if none-arguments were uniformly represented as empty </s> identity_report)	up_to_merge_computation zero = zero_comp() return intrinsics.federated_aggregate(value_to_aggregate, zero,
# todo: to be removed in v2.8.0 </s> @cached_property	DefinesSubMenuTemplatesMixin if template_name: return get_template(template_name) def sub_menu_template(self): return self.get_sub_menu_template()
'location_type': loc.location_type.name,  # todo: remove when types aren't optional </s> return map_reduce(lambda (k, v): [(p, k) for p in v],	parent_child def parent_child(domain): Returns a dict mapping from a location type to its possible data=dict(location_hierarchy_config(domain)).iteritems())
# todo: implement this </s> ans.append((path, e, traceback.format_exc()))	prepare_addable_books try: f = self.filesystem_cache.resolve_mtp_id_path(path) continue base = os.path.join(tdir, '%s'%f.object_id)
# todo : uncomment these out to set default to densejacobian, once we have resolved further </s> inputs = self._var_abs_names['input']	_set_partials_meta Set subjacobian info into our jacobian. with self.jacobian_context() as J: for wrt_name, wrt_vars in (('output', outputs), ('input', inputs)): for abs_key in product(outputs, wrt_vars):
# todo(tr3buchet) - remove comment in multi-nic </s> def get_public(self):	get_public
# todo: deprecated - remove in version 0.10 </s> if self.enable_feature_string_compression:	_create_feature_key def _create_feature_key(self, states): compressed = zlib.compress(bytes(feature_str, "utf-8")) return base64.b64encode(compressed).decode("utf-8")
# todo: reduce_mean? </s> features = tf.nn.relu(self.l1(features))	CategoricalQFunc **kwargs) def call(self, inputs): features = tf.nn.relu(self.l2(features)) if self._enable_dueling_dqn:
pass  # todo </s> self._device = open(self._device_path, "wb")	RivalMouse def _find_device(self): pass  # TODO def _device_write(self, *bytes_): Arguments:
# todo: finish </s> self.push_stack_string(all_str)	loadStack envs_ptr = [] apple_ptr = [] ptr = self.stack_esp for item in self.argvs[::-1]:
# todo: remove this - cura-4482 </s> return self.getqualitydefinitionid(self._global_container_stack.getbottom())	activeQualityDefinitionId if self._global_container_stack:
# todo(user): keepalive is not enabled on the netperf control socket. </s> _install(vm)	YumInstall
# todo: this is not handling decoding errors all that well. </s> def getimportname(self):	getImportName
kwargs['application'] = application.objects.get(client_id=credentials['client_id'])  # todo: this should be cached one day </s> log.debug('fatal client error, should redirecting to error page.')	PreAuthorizationMixin self.oauth2_data['user_id'] = request.user.id return super(PreAuthorizationMixin, self).dispatch(request, *args, **kwargs) return HttpResponseBadRequest()
# todo: add these lines back </s> a_t (the add vector) is length m with no such restrictions.	write@30 The writing procedure as described in 3.2. w_t is a length N weighting over the rows as above. We first multiply the memory matrix pointwise by [1-w_t(i)e_t] Then we do M_t(i) <- w_t(i)a_t.
# todo: move this to sublime_lib; make it accept a point or a region. </s> return view.full_line(middle)	find_line elif get_line_nr(view, middle) > target: hi = getBOL(view, middle) - 1 return -1
x_vec = np.matrix(xs).t  # todo python3: fix np.matrix </s> if enable_mapping:	AggrScorePerfMetric groundtruths_ = list(map( lambda x: aggre_method(x) if hasattr(x, '__len__') else x, predictions_ = cls.sigmoid_adjust(predictions, groundtruths_) else:
# todo: add docs </s> def update_fn(g, state):	dual_averaging g_avg = 0.  # average of dual sequence t = 0 x_t, x_avg, g_avg, t, prox_center = state t = t + 1
"""todo doc me""" </s> ['foo', 'bar']	fields >>> from petl import fields >>> table = [['foo', 'bar'], ['a', 1], ['b', 2]] it = iter(table) return it.next()
# todo:eliminate this </s> return os.path.join(index_dir, ds_name + '.db')	__get_dataset_path
# todo find out what is best used here! </s> l1_ratio = uniformfloathyperparameter("l1_ratio", 0, 1, default=0.15)	SGD default="l2") alpha = UniformFloatHyperparameter("alpha", 10**-7, 10**-1, fit_intercept = UnParametrizedHyperparameter("fit_intercept", "True") n_iter = UniformIntegerHyperparameter("n_iter", 5, 1000, default=20)
# todo: load state into here </s> current_chunk +	map_schedule_onto_host_or_device new_kernel_name = kernel_name_gen() new_schedule.extend( [ReturnFromKernel(kernel_name=new_kernel_name)]) new_schedule.extend(
# todo instead of 3*t, use log_sf </s> def __init__(self,t,state_dim,*args,**kwargs):	HSMMStatesEigen self.sample_forwards_codestr = hsmm_sample_forwards_codestr % {'M':state_dim,'T':T} super(HSMMStatesEigen,self).__init__(T,state_dim,*args,**kwargs)
# todo: enhance this method to set a flag and alert an admin to review content since </s> fd, uncompressed = tempfile.mkstemp( prefix='repo_%d_upload_gunzip_' % repository.id, dir=os.path.dirname( uploaded_file_name ), text=false )	__handle_gzip gzipped_file = gzip.GzipFile( uploaded_file_name, 'rb' ) while 1:
# todo: implement link-local handling in networkmanager backend and move this test into commontests() </s> with open(self.config, 'w') as f:	test_link_local_disabled def test_link_local_disabled(self): f.write('''network: renderer: %(r)s
# todo: create xxx_failure test </s> failure_tests = r['failure']	test_result_space_failure def test_result_space_failure(self): p = op.join(app.config['ROOT'], 'tests/fixtures/ttf/Font-Light!.ttf') tests = exclude_from_resultlist(r, 'failure') self.assertTrue(check('test_space', failure_tests),
# todo: we should consider the probabilities of `task1 failure -> task2 failure` and </s> return [task.replace("test-linux64-", "test-linux1804-64-") for task in tasks]	rename_tasks
# todo: does this import need to be delayed because </s> start_time = time.time()	ExtensiveFormAlgorithm self.destroy_ef() if self.get_option("verbose"): generate_weighted_cvar = False cvar_weight = None
for joint in annos:  # todo : speed up with affine transform </s> parameters	threading_data def threading_data(data=None, fn=None, thread_count=None, **kwargs): ----------- data : numpy.array or others
# (todo) chagne the dgl link </s> split_dict = replace_numpy_with_torchtensor(torch.load(osp.join(self.root, 'split_dict.pt')))	get_idx_split def get_idx_split(self):
# todo check </s> return "gstreamerdecold"	id @staticmethod @interfacedoc
# todo: find better way to avoid entering "__hh_previous_frame" to avoid traceback added by `tracers.locationtracer` </s> def flatten(l):	flatten
#todo: this variable plan_id is never used </s> return render_to_response(template, variables,	retail_rate_view 'form': form, 'user': request.user, context_instance=RequestContext(request))
# todo: get these values from the same place as setup.py </s> return appdirs.user_data_dir("ice","scott rice")	app_data_directory@29
#todo: define tests which check db contents </s> suite = unittest.testsuite(suite_list)	test_suite ] suite_list = map(unittest.defaultTestLoader.loadTestsFromTestCase, return suite
# todo: remove in 1.3 </s> warning_message = "consider decreasing the number of bins."	test_redundant_bins def test_redundant_bins(strategy, expected_bin_edges): X = [[0], [0], [0], [0], [3], [3]] with pytest.warns(UserWarning, match=warning_message): kbd.fit(X)
return  # todo return placeholder "[error]" track? </s> tracks = [to_track(sp_track) for sp_track in sp_playlist.tracks]	to_playlist name = '/'.join(folders + [name]) if username is not None and sp_playlist.owner.canonical_name != username: tracks = filter(None, tracks) return models.Playlist(
# todo: self.assertfalse(prop.is_valid(np.bool8(false))) </s> self.assertfalse(prop.is_valid(1.0))	test_Instance self.assertFalse(prop.is_valid(0)) self.assertFalse(prop.is_valid(1)) self.assertFalse(prop.is_valid(1.0+1.0j)) self.assertFalse(prop.is_valid(""))
# todo is this serious enough to raise a canerror exception? </s> :param str channel_name:	bindSocket Binds the given socket to the given interface. :param int socketID: The interface name to find and bind. :return:
# @todo: deprecate </s> t = current.t	irs_dispatch - this will be formatted as an OpenGeoSMS if r.representation == "html" and \ msg = current.msg record = r.record
# todo new message here </s> factory)	startWorker reactor.connectTCP( factory.defaults['host'],
# todo: remove return </s> ([0, 3, 10], [0, 3], [0, 10]),	test_brle_strip def test_brle_strip(self): for brle_data, expected_brle, expected_padding in ( ([5, 3], [0, 3], [5, 0]), ([5, 3, 0, 0], [0, 3], (5, 0)),
# todo: if you try to fix this first read issue #958 and 1018 </s> button.connect("notify::tooltip-text", set_tooltip_text)	add_button button.connect("notify::sensitive", set_sensitive) def set_tooltip_text(button, property):
# todo(nzw0301) support intloguniform </s> "to `false` in the constructor of `cmaessampler`, "	_log_independent_sampling "by using `{}` instead of `CmaEsSampler` " "(optimization performance may be degraded). " "if this independent sampling is intended behavior.".format( param_name, trial.number, self._independent_sampler.__class__.__name__
# todo: add and store preprocessing errors. </s> raises:	LinuxDistributionPlugin knowledge_base (KnowledgeBase): to fill with preprocessing information. file_object (dfvfs.FileIO): file-like object that contains the artifact errors.PreProcessFail: if the preprocessing fails. text_file_object = dfvfs_text_file.TextFile(file_object, encoding='utf-8')
#todo check max_amount on conversion?? </s> if token.allowance_of(self.our_address, spender_address) < wad(2 ** 128 - 1):	setup_allowance logging.info(f"Approving {spender_name} ({spender_address}) to access our {token.name()} balance directly") if not token.approve(spender_address):
# todo(jflesch): check last_mod ! </s> is a suggestion.	find_suggestions suggestions Return: return []
# todo: to be implemented </s> for module_name, module_info in sorted(all_modules.items()):	get_all_modules all_modules[module_name] = (mod, None) user_modules = self.get_user_modules() yield (module_name, module_info)
# todo test that citext.sql gets loaded with 9.0.x </s> with config_manager.context() as config:	TestSetupDB def test_execute_postgres_with_acceptable_errors(self): config_manager = self._setup_config_manager() with mock.patch(self.psycopg2_module_path) as psycopg2: pge = ProgrammingError()
# todo: use bezier curve instead of polygon. perhaps aggdraw module. </s> if not pattern:	draw_pattern_fill fill = layer.tagged_blocks.get_data('PATTERN_FILL_SETTING') pattern_id = fill[b'Ptrn'][b'Idnt'].value.rstrip('\x00') logger.error('Pattern not found: %s' % (pattern_id)) return None
#todo_jay: put seed in job </s> self.set_api(token=token)	set_api_token
# todo: remove after py2.5 deprecation </s> self.assertequal(30, hsp.query_from)	test_tab_2226_tblastn_001 self.assertEqual(43, hsp.ali_len) self.assertEqual(28, hsp.mismatch_num) self.assertEqual(72, hsp.query_to) self.assertEqual(1743, hsp.hit_from)
# todo: replace sdolenc with tducret after merge </s> assert isinstance(products, amazonscraper.products)	test_amazonscraper_get_products_with_url products = amazonscraper.search( search_url=url, assert len(products) == _MAX_PRODUCT_NB product = products[0]
# todo if the other logging that is happening is less frontpage </s> else:	LogFilter if names_filter == 'names': def filter(self, record): def filter(self, record): return self.target_names.match(record.name)
self.group_self(self._grouped_on)  # todo: think about removing </s> def sample_frac(frac):	sample_frac
if isinstance(err, asyncio.cancellederror):  # todo: remove when updated to 3.8 </s> def path(self):	Ledger@705 def private_key_to_wif(private_key): return b'\x1c' + private_key + b'\x01' return os.path.join(self.config['data_path'], self.get_id()) def add_account(self, account: Account):
# todo: signal the user </s> @rpc_callback	FileHandler callback=self._fetch_callback, plus=[content_type, filename], def _fetch_callback(self, caller, data, plus, error=None): page, that just collects the response.
# pycryptodome does not expose the mode attribute </s> self.assertequal("'(\x17\x94l\xd7ao\x03\xd4fi\x05}mp\x1ax5c7\xf0_\xa9\xb0\xac\xba{r\x1f\x12\x8f",	test_client_ecdh_parameters_generation_matches_fixed_data self.assertEqual("\x04%s%s" % (tlsc.int_to_str(client_keys.pub.x), tlsc.int_to_str(client_keys.pub.y)), client_pubkey) tls_ctx.premaster_secret)
# todo: what actually raises valueerror in the following code? </s> self.finished = true	_GzipStreamFile if len(self.unused_buffer) > size: return self.read_from_buffer() buf, self.unused_buffer = self.unused_buffer, b'' return buf
# todo: test that valueerror is raised </s> n = d1.squeeze().ndim	hamming_emd by state, one dimension per node) using the Hamming distance between states as the transportation cost function. d1, d2 = flatten(d1), flatten(d2) return emd(d1, d2, _hamming_matrix(N))
# todo docs </s> query_reply = job_collection \	run_with_configuration job_data = { 'configuration': configuration .insert(projectId=project, body=job_data) \ .execute()
# todo: command+c for mac </s> self.vbar.grid(row=0, column=2, sticky=tk.nsew)	ReplayerCodeView def __init__(self, master): ttk.Frame.__init__(self, master) self.hbar = ttk.Scrollbar(self, orient=tk.HORIZONTAL) self.hbar.grid(row=1, column=0, sticky=tk.NSEW, columnspan=2)
# todo support intloguniformdistribution </s> "if this independent sampling is intended behavior.".format(	_log_independent_sampling "to `False` in the constructor of `CmaEsSampler`, "
# todo complete this method </s> logger.timer_debug1(self, 'eom-ccsd ip intermediates', *cput0)	make_ip self.Wooov = imd.Wooov(self._cc, t1, t2, eris, kconserv) self.Wovoo = imd.Wovoo(self._cc, t1, t2, eris, kconserv) return self
# todo: move into toolevaluator test(s) </s> return none	get_filename def get_filename(self, *args, **kwds): if kwds.get("base_dir", "") == "job_work":
# todo(jamalex): could do md5 checks here instead, to be ultra-safe </s> overall_progress_update(f.file_size)	Command srcpath = paths.get_content_storage_file_path(filename) dest = paths.get_content_storage_file_path(filename, datafolder=data_dir) continue with transfer.FileCopy(srcpath, dest) as copy:
# todo(rbharath): can this be removed? </s> elif os.path.splitext(name)[1] == ".csv":	load_from_disk return old_joblib.load(filename) except ValueError: df = pd.read_csv(filename, header=0) df = df.replace(np.nan, str(""), regex=True)
# todo(cmaloney): good exception catching, etc </s> start_response('200 ok', [('content-type', 'text/html')])	wsgi_app@10 def wsgi_app(env, start_response): return "Got it"
except (testtransactionfailed, validationerror, valueerror):  # todo: 1950 </s> return receipt	disable_winding_down def disable_winding_down(self) -> dict:
# todo! get destination path from user save dialog. </s> def export_sprite_sheet(self, layer):	CoaExport sprite.position = [width, pos, height] sprite.resource_path = path offset = layer.offsets width = layer.width / 2 + offset[0]
#todo: add support for network load balancer </s> pricing_records, cost = phelper.calculate_price(consts.service_ec2, computedb, qtyquery, pdim.instancecount, pricing_records, cost)	calculate@108 hrsQuery = query & (priceQuery['Unit'] == 'Hrs' ) qtyQuery = query & (priceQuery['Unit'] == 'Quantity' ) if pdim.offeringType in (consts.SCRIPT_EC2_PURCHASE_OPTION_NO_UPFRONT, consts.SCRIPT_EC2_PURCHASE_OPTION_PARTIAL_UPFRONT): reservedInstanceHours = pdim.instanceCount * consts.HOURS_IN_MONTH * 12 * pdim.years
el_movieposter.set('onselect', "atv.loadurl('"+el_path+"')")  # todo: 'select' - show metadata </s> indent(xml.getroot())	XML_prettyprint XML.write(sys.stdout)
#todo: check for continous or discrete, only continuous supported right now </s> for e in d:	gram@252 dico = 'C' if e.real >= 0: raise ValueError, "Oops, the system is unstable!"
#todo(#212): use a map construct instead of unrolling. </s> def convert_element_type_dtype_rule(operand, new_dtype, old_dtype):	convert_element_type_dtype_rule
#todo: namespaces too hardwired, clean-up... </s> header.marshall(k, self.token, ns=false, add_children_ns=false)	UsernameToken k = 'wsse:Security'
'"ed25519_secret_seed", "pre_auth_tx", "sha256_hash"'.format(version_byte_name))  # todo </s> def encode_ed25519_secret_seed(data: bytes) -> str:	encode_ed25519_secret_seed @staticmethod
# todo: replace </s> mean_rank = np.mean(ranks)	compute_mean_rank ranks_object_based, _ = _compute_metrics(all_entities=all_entities, kg_embedding_model=kg_embedding_model, triples=triples, corrupt_suject=False, device=device) stop = timeit.default_timer() log.info("Evaluation took %s seconds \n" % (str(round(stop - start))))
# todo: uncomment when adding support for literal hex bytes </s> except attributeerror as err:	test_setattr x.attr = 42
# todo: this is not using any cache... </s> content = h2t.handle(content).strip()	clean_html content = unicode(soup.body or soup) if convert_to_markdown: return content.encode('utf-8')
# todo: limit/check colorcode </s> green *= 21	LedCtrlChar char = max( char, 0) * 8 if blue is None: blue   =  0 min( red, 63 )
# todo move to a proper logging framework </s> for item in list:	flatten for list in L:
# todo optimize: special case where there is only one dynamic </s> return true	is_complex_type
# todo: remove anytime in 2016 </s> def drilldown_map(self):	FormsByApplicationFilter return context @property final_map = [] map_active = []
# todo return, catch exception in main() </s> print static ascii and utf-16 strings from provided file.	print_static_strings :param path: input file :param min_length: minimum string length
# todo: implement me </s> when a version is created from a fileupload, the files are removed. in	run_wat def run_wat(results, upload_pk): Run the wat scanner on a FileUpload and store the results. addition, we usually delete old FileUpload entries after 180 days. - `results` are the validation results passed in the validation chain. This
mock = create_mock_json('tests/resources/list_race_details.json')  # todo </s> assert self.scores.url == '%s%s' % (self.scores.client.api_uri, 'scores/json-rpc/v1')	test_url
# todo 目前仅在 华泰子类 中实现 </s> def get_entrust(self):	get_entrust
codecs.decode('5050827d08000000d00745b2cf451b00', 'hex'),  # tcp random cmd_ack_ok todo: generate proper sequenced response </s> @patch('zk.base.zk_helper')	test_udp_connect def test_udp_connect(self, helper, socket): helper.return_value.test_ping.return_value = True # ping simulated
# todo document </s> this is the distance from the subsystem's constellation to the null	conceptual_information concept.""" return constellation_distance(subsystem.constellation(), ())
#todo the tooltip should actually hide on its own. ticket #1096 </s> self.instance.session.ingame_gui.hide_menu()	destruct_building Tear(self.instance).execute(self.instance.session)
# todo: check the values from the related manager </s> self.assertin(response.status_code, (302, 403))	WorkoutManagerEditTestCase response = self.client.post(self.edit_url, self.data_update) entry_after = self.object_class.objects.get(pk=self.pk) self.assertTemplateUsed('login.html') self.assertEqual(entry_before, entry_after)
#todo(bcwaldon): use the schema to actually validate something </s> super(testschemas, self).setup()	TestSchemas class TestSchemas(functional.FunctionalTest): self.cleanup() self.start_servers(**self.__dict__.copy())
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
# todo figure out something useful to do with the newbranch param </s> git = githandler(self, self.ui)	findoutgoing def findoutgoing(self, remote, base=None, heads=None, force=False): base, heads = git.get_refs(remote.path) out, h = super(hgrepo, self).findoutgoing(remote, base, heads, force)
# todo: move to pytest.mark.parametrize once nose gone </s> def test_getoutput(self):	SubProcessTestCase self.assertNotEqual( status, 0, "The process wasn't interrupted. Status: %s" % (status,) out = getoutput('%s "%s"' % (python, self.fname)) try:
# todo - get rid of these </s> self.draw_mode = self.mode_year	TimeLine fact["delta"].days * 24 for fact in date_facts])) start_date = facts[0]["start_time"].date() self.start_date = start_date.replace(month=1, day=1) self.end_date = end_date.replace(month=12, day=31)
#todo implement transformation for corner nodes </s> scenter = (sxx + syy)/2.	thetadeg_to_principal thetadeg : np.ndarray Array with angles for which the given stresses are transformed to the thetarad = np.arctan2(Sxy, Scenter - Syy) return np.rad2deg(thetarad)/2.
# :todo: implement test. </s> self.skiptest('not implemented yet.')	ReplayBundleRequestFilterTestCase self.skipTest('Not implemented yet.') def test_fail_depth_float(self): def test_fail_depth_too_small(self): ``depth`` is < 1.
# todo: check output </s> self.asserttrue(output.count('\nnl: newton converged') == 0)	test_hierarchy_iprint if self.comm.rank == 0: self.assertTrue(output.count('\nNL: Newton Converged') == 1)
# todo(b/138845899): consider use span instead of id. </s> last_blessed_model.custom_properties['current_model_id'].int_value)	Driver previous_blessed_models, key=lambda artifact: artifact.id) return ( else: return None, None
# todo: check whether it's already installed?. see yum notes  yum list installed "$@" >/dev/null 2>&1 </s> shutit.log('returning false.',level=logging.debug)	get_exit_value shutit.log('Returning true.',level=logging.DEBUG) return True return False
# todo: @sbharadwajj implement and test </s> def _weight_jac_t_mat_prod(self, module, g_inp, g_out, mat, sum_batch=true):	Conv1DDerivatives raise NotImplementedError
# todo: must be implemented </s> pass	should_fetch_kindlegen
# todo debug </s> def __init__(self):	RuleElement self.type = None self.triggered = False
# todo: ignoring repeat letters </s> rtf = romantext.rtfile()	testExternalA from music21 import romanText from music21.romanText import testFiles rth = rtf.readstr(tf) # return handler, processes tokens s = romanTextToStreamScore(rth)
# todo(dcramer): remove in 7.6.x </s> for plugin in plugins.for_project(instance.project):	on_alert_creation def on_alert_creation(instance, **kwargs): safe_execute(plugin.on_alert, alert=instance)
# todo: this is all just debugging stuff and can be removed </s> axes_cov = [ellipsis] + list(range(abs(axis),	right_rotate_covariance if axis >= 0: axis -= ndim 0, -1))
# todo: improve this. </s> start=start,	find_next_lone_bracket nested = 0 while True: end=next_closing_bracket.b, flags=sublime.IGNORECASE)
# todo in python 2.7 or later, this should be </s> instance of a sqlalchemy model.	DeserializationException pass
pass                # todo(nnorwitz): impl </s> return 1 + self.source.count('\n', 0, start)	_GetLineNumber
# todo(dspasovski): fix this. </s> return category.objects.create(name='slap tickling', slug='booping',	get_new_cat def get_new_cat(self):
# todo users aren't going to match </s> key = unicode(modm_object._id).lower()	get_pk def get_pk(self, modm_object): from website.models import Tag as MODMTag if key in self.modm_to_django: return self.modm_to_django[key]
transparent = false  # todo </s> border_min_x, border_max_x, border_min_y, border_max_y = self._border	_calc_offset def _calc_offset(self, region_size, view_camera_offset): offset_x = width_raw * border_min_x + 1 offset_y = height_raw * border_min_y + 1
# todo: check the data! </s> u'staffingcosts': u'88283', u'employeenumofjourneys': u'21',	test_csv u'description': u'Total allowances claimed, inc travel: 151619<br>Total basic allowances claimed, ex travel: 146282<br>Total Travel claimed: 5337<br>MP Mileage: 3358<br>MP Rail Travel: 1473<br>MP Air Travel: 0<br>Cost of staying away from main home: 22541<br>London Supplement: 0<br>Office Running Costs: 19848<br>Staffing Costs: 88283', u'TotalAllowancesClaimedIncTravel': u'151619', u'SpouseTotal': u'31', u'CentrallyPurchasedStationery': u'1149', u'TotalBasicAllowancesExcTravel': u'146282', u'CentralITProvision': u'1223', u'StaffCoverAndOtherCosts': u'0',
# todo: test me </s> kwargs=kwargs,	calculate_key view_method=view_method, request=request,
pass # todo: explain </s> if not self.response.parsed_hdrs.has_key('location'):	status303 def status303(self):        # See Other
# todo: the following should be handled within inputspecs ? </s> self.inputinfo.showinfo()	InputTabWidgets if self.DEBUG: print("input_widgets.updateAll:\n",self.sender().objectName()) self.inputSpecs.color_design_button("ok") self.inputCoeffs.show_coeffs() self.inputPZ.showZPK()
# todo(jblespiau): we can simply use buf.xla_shape() when version 0.1.58 is </s> return xla_client._xla.buffer_to_dlpack_managed_tensor(buf)	to_dlpack if not take_ownership: raise ValueError(
# todo check </s> def set_metadata(self, metadata):	set_metadata @interfacedoc
pass # todo </s> def collect_incoming_data(self, data):	collect_incoming_data
_imdb_queue['quality'] = 'dvd' # todo: get defaul from config somehow? </s> return '<imdbqueue(%s qual %s)>' % (self.imdb_id, self.quality)	ImdbQueue self.immortal = immortal self.added = datetime.datetime.now()
# todo: manage other values </s> height = size(node.get("height"))	node_format def node_format(node): viewbox = node.get("viewBox") if viewbox:
# todo: order of attributes is not assured; allow for any order. </s> self.assertequal(raw.count('<bracket'), 4)	testBracketA from music21.musicxml import testPrimitive s = converter.parse(testPrimitive.directions31a)
# todo differentiate integer and float? using custom renderers and </s> returns	to_dataframe Parameters ---------- ------- dataframe : Pandas DataFrame
self.assertequal(out.strip(), "inactive") # todo real "failed" </s> self.asserttrue(greps(out, r"^description="))	test_2027_show_unit_for_oneshot_service self.assertEqual(end, 0) self.assertTrue(greps(out, r"^Id=")) self.assertTrue(greps(out, r"^MainPID=")) self.assertTrue(greps(out, r"^LoadState="))
return none # xxx todo return accordingly generated command for initiating this proxy </s> raise configfileexception("core.key is invalid.")	__loadCore def __loadCore(self, core): self.key = str(core["key"]) if not core["server"].has_key("ip"): raise ConfigFileException("core.server.ip must be specified.")
# todo: handle 2d and 3d data </s> just leaves all frames unchanged.	_no_change Processing function for IsolatedFrameExporter.
# todo(ls): revert this loop to "yield from" </s> yield _format_final_exc_line(stype, self._str)	format_exception_only if smod not in ("__main__", "builtins"): stype = smod + '.' + stype return filename = self.filename or "<string>"
pass # todo </s> sizehints.height = height	make_window sizehints.x = x sizehints.y = y sizehints.flags = xlib.USSize | xlib.USPosition xlib.XSetNormalHints(dpy, win, sizehints)
# todo remove me </s> emission['r'], emission['g'], emission['b'], 1.0))	exportSDFMaterial if 'emissionColor' in materialdata: emission = materialdata['emissionColor'] tagger.ascend() return "".join(tagger.get_output())
#todo: check if/where this is used; if not used externally - remove </s> self.save_surface_definitions_to_file()	remove_surface self.gui.remove_surface(surface) self.surfaces.remove(surface)
# todo(vish): move this into the driver layer </s> instance_id,	terminate_instance raise exception.Error('trying to destroy already destroyed' ' instance: %s' % instance_id) power_state.NOSTATE, 'shutting_down')
# todo: constants file for "broadcast" </s> .format(**load)	ClearFuncs 'Authentication attempt from {id} failed, the public ' 'key in pending did not match. This may be an ' ) with salt.utils.fopen(pubfn_denied, 'w+') as fp_:
# todo change to native framework call, when plex allows token in header </s> arguments[arguments.index('section') + 1])	viewstate if 'section' in arguments: try: except Exception, e: Log.Exception(
# todo remove after unified backends </s> print("eval episode rewards:")	test_with_final_eval break
# test_qnetwork_weight_quantization: todo </s> x)	test_qrnn@152 recurrent_quantizer=quantized_bits(8, 0, 1, alpha=1.0), bias_quantizer=quantized_bits(8, 0, 1, alpha=1.0), x = QDense( 4,
# todo: make grouper in query </s> return geopolygon.from_boundingbox(boundingbox(left, bottom, right, top), crs)	get_bounds right = max([d.extent.to_crs(crs).boundingbox.right for d in datasets]) top = max([d.extent.to_crs(crs).boundingbox.top for d in datasets])
# todo: replace with "yield from" when dropping python 2. </s> return http.post(*args, **kwargs)	ajax def ajax(*args, **kwargs):
# todo: it would be nice to be async about this. set 1 second timeout. </s> if `pathname` is unspecified or ``none``, a temporary in-memory	bayesdb_open def bayesdb_open(pathname=None, do_version_check=None): BayesDB instance is created. if do_version_check is None:
# todo(dspasovski): fix this. </s> return category.objects.create(name='slap tickling', slug='booping',	get_new_cat def get_new_cat(self):
# todo: make this a hard error, instead of a silent overwrite </s> name = instance.name	StopInstance if name is not None and not force: raise errors.HypervisorError("Cannot shutdown cleanly by name only") acpi = instance.hvparams[constants.HV_ACPI] else:
# todo: don't rely on the touch command </s> print >> sys.stderr, "will wait ",	on_exception def on_exception(e, delay): print >> sys.stderr, delay
raise notimplementederror  # todo </s> def __init__(self, perturbation_function, steps, recompute_analysis=false):	Perturbation raise NotImplementedError  # TODO def sort(self):
# todo: look this up in one query </s> where playlist.mbid = :playlist_mbid	delete_playlist_by_mbid True if the playlist was deleted, False if no such playlist with the given mbid exists query = sqlalchemy.text(""" with ts.engine.connect() as connection: result = connection.execute(query, {"playlist_mbid": playlist_mbid})
# todo: change template so it iterates through form and not formfields </s> except objectdoesnotexist:	dashboard template_data['plan'] = plan try: weight = False template_data['weight'] = weight
# todo complete this method </s> return self	make_ip self.Wovoo = imd.Wovoo(self._cc, t1, t2, eris, kconserv) self.made_ip_imds = True
# todo(sbauza): the current placement noauthmiddleware returns a 401 </s> if volume_id == cinderfixture.swap_err_new_vol:	fake_initialize_connection return () return {}
# todo: chunk transmissions into more managable lenths </s> flat=true)	send_transmissions dbm = Message.objects.get(pk=message_id) transmissions = Transmission.objects.filter(id__in=transmission_ids) router = BlockingRouter() router.start()
# todo: verify exception type once those exists </s> def test_dotifle(self):	test_dotifle assert False
# todo assert cls.__tablename__ == '' </s> s = model.galaxysession()	galaxy_session @pytest.fixture yield from dbcleanup_wrapper(session, s)
# todo executor, max_workers </s> raise valueerror('{0} datatype not supported'.format(col_type))	to_sql_type_mappings elif col_type == 'bytes': return 'BINARY' return 'STRING'
# todo - use new error message api! ts </s> in qgis.	connectLayerListener ..seealso:: disconnectLayerListener Args:
# todo: fix comment for this and simplifier </s> this function takes the attributes of a pointertensor and saves them in a dictionary	_simplify_pointer_tensor Args: PointerTensor: a PointerTensor
# todo: do we need to do this? </s> yield	iter_array while len(self._current) > 0: self._push() self._pop() self._parser.advance(ItemEnd())
# todo: when hytra is supported on windows, we shouldn't skip the test and throw an assert instead </s> def setupclass(cls):	setupClass logger.info('starting setup...') cls.original_cwd = os.getcwd()
# todo xxx graalvm change </s> self.assertequal(calls, [("supermessage", server_context)])	test_sni_callback stats = server_params_test(client_context, server_context, chatty=True, self.check_common_name(stats, 'fakehostname') calls = []
# todo(ytknzw): add more specific assertion with the test case. </s> raise valueerror	fail_objective
# todo handle valueerror </s> tokens = status_line.split('->')	__trimgit def __trimgit(status_line): return tokens[1].strip() tokens = status_line.split(':')
#todo - use a context manager here once we drop python 2.6 </s> self.assertalmostequal(pc[1, 2],  0.13819106187213354)	test_pca self.assertAlmostEqual(pc[0, 3],  0.26145408875373249) self.assertAlmostEqual(pc[1, 0],  0.05073770520434398) self.assertAlmostEqual(pc[1, 3],  0.19782544121828985) self.assertAlmostEqual(pc[2, 0], -0.63000893660095947)
# todo don't break, exhaust the iterator, otherwise </s> return tlobject.pretty_format(self.to_dict(), indent=0)	stringify
# todo: configurable rsync options? </s> copy_it = false	_list_files_to_copy continue elif filename.endswith(".spec"): for extension in self.cvs_copy_extensions: if filename.endswith(extension):
# todo: needs further implementation </s> def rendered_report_title(self):	rendered_report_title @cached_property
# todo: make sort function that sorts events by mechanism so that </s> return self.expand_repertoire(directions[past], purview, repertoire,	expand_cause_repertoire over the entire subsystem's state space."""
# todo: down to empty </s> assert str(pdeque([])) == 'pdeque([])'	test_str def test_str():
# todo: avoid dummy and generate func here when inlining is possible </s> def df_len_overload(df):	df_len_overload if len(df.columns) == 0:  # empty df return lambda df: 0
# todo: assert </s> repo = self.remote.new_repo(self.token)	createRepo @pytest.fixture self.remote.modify_repo(repo, "name", "testrepo0", self.token) self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token)
# todo(b/160786085): move this logic into overriding vars logic itself, </s> return load_checkpoint_from	GetSpecificCheckpoint if tf.io.gfile.isdir(load_checkpoint_from): return tf.train.latest_checkpoint(load_checkpoint_from) raise ValueError('Invalid load_checkpoint_from: %s' % load_checkpoint_from)
#todo : make a real log mangment </s> self.linkify_h_by_tp(timeperiods)	linkify self.linkify_h_by_h() self.linkify_h_by_cmd(commands)
#todo: check login_required? </s> data = data_provider.get_data( **kwargs )	_raw_data if msg: return msg return data
# todo: revisit this when we have a users app </s> def profile_url(user):	profile_url@7 return '/tiki-user_information.php?locale=en-US&userId=%s' % user.id
# todo(shardy): may be able to remove when the bug above is fixed </s> except typeerror:	authenticate@76 service_type=service_type, service_name=service_name, nova = client.Client(con.service_user, con.service_password, con.tenant, con.auth_url,
# todo(rakhmerov): here we can define more informative messages </s> policies_spec = self.task_spec.get_policies()	_after_task_complete for p in policies.build_policies(policies_spec, self.wf_spec): p.after_task_complete(self.task_ex, self.task_spec)
# todo: manage errors </s> singleton to return only on instance of project.	instance @staticmethod :returns: instance of Project if not hasattr(Project, "_instance"):
# todo: handle multiple skip stacks </s> replaced_box_width.without_min_max(box, containing_block)	block_replaced_width @handle_min_max_width block_level_width.without_min_max(box, containing_block)
# todo: next major version, remove cam id (unique_id is already in path) </s> return camera_ids	count_cameras_opencv if temp_camera.isOpened(): temp_camera.release()
assert oldsize == self.size, '%s: %d, %d' % (self.type, oldsize, self.size) # todo: remove </s> return struct.unpack('>i', stream.read(4))[0]	read_uint
# todo: replace with stream-changed </s> def get_current_tl_track(self):	get_current_tl_track
# todo: merge scopes and claims </s> def check_authorization_endpoint(cls, request):	check_authorization_endpoint if is_openid_request(request, cls.RESPONSE_TYPES): wrap_openid_request(request)
raise notimplementederror #todo, implement! </s> else:	SetDefinition legacyset = LegacySetDefinition.parsexml(root) self.graph = rdflib.Graph() self.graph = rdflib.Graph() self.graph.parse(location=url, format=format)
common_path=prefix,  # todo: add key? </s> def autoresolve_cells(base, decisions, strategies):	autoresolve_cells
# todo(developer): uncomment and set to a path to your audio file. </s> from google.cloud import speech_v1p1beta1 as speech	transcribe_file_with_metadata@71 client = speech.SpeechClient() with io.open(speech_file, 'rb') as audio_file:
# todo: identify the specific structure we're finding and document this a bit better </s> def __init__(self, pdb_names):	PdbSigantureScanner class PdbSigantureScanner(interfaces.layers.ScannerInterface): overlap = 0x4000 super().__init__() self._pdb_names = pdb_names
writetimeout=10000, # todo 1.3.8 rename to write_timeout for pyserial >= 3.x </s> busy_default = 4.0	_getTemperatureTimerInterval target_default = 2.0 if self.isBusy():
# todo: determine proper template to use. </s> return exitcode, out, err	get_exitcode_stdout_stderr proc = Popen(args, stdout=PIPE, stderr=PIPE) out, err = proc.communicate()
# todo: fix these repos finally! </s> assert("already exists" in cm.out)	test_AnnexRepo_instance_from_clone with swallow_logs() as cm: assert_raises(GitCommandError, AnnexRepo, dst, src)
# todo: check that the performance measure is within some range </s> reported on the website, or other).	TestBaselines class TestBaselines(unittest.TestCase): Tests that the baselines in the benchmarks folder are running and def test_bottleneck0(self): Tests flow/benchmark/baselines/bottleneck0.py
# todo: ensure that if multiple flags are provided, the *last* one overrides </s> exec_opts.setoption(opt_name, b)	Shopt raise NotImplementedError  # Display options for opt_name in argv[i:]: else: exec_opts.SetShoptOption(opt_name, b)
# todo there's probably a better way besides np.where, something from </s> new_cell_sets[name] = []	remove_lower_dimensional_cells new_cell_data[name] += [data[idx]] for name, data in self.cell_sets.items(): new_cell_sets[name] += [data[idx]] else:
# todo(mattjj): not passing forece_broadcast recursively... intentional? </s> return abstracttuple(map(shaped_aval, xs))	shaped_jaxtuple
# todo(justinsb): mock doesn't yet do this... </s> servers = self.api.get_servers()	test_get_servers for server in servers: LOG.debug("server: %s" % server)
# todo: fix this! </s> lang_name = self.settings.get("language_name").lower()	add_features_packages if self.settings.get('feature_office'): logging.debug(_('Add libreoffice language package')) if lang_name == "english": lang_packs = ['en-GB', 'en-US', 'en-ZA']
# todo: should probably replace with input handler to remain consistent </s> for k, v in gotomap.items():	_flatten_fdir -1 - shape[1] ) fdir[fdir == k] = v fdir.flat[flat_idx] += flat_idx
# todo: make sure pub is always correct </s> kwargs['locales'] = self.event.settings.get('locales')	I18nFormSet def __init__(self, *args, **kwargs): self.event = kwargs.pop('event', None) super().__init__(*args, **kwargs)
# todo(b/130724878): these conversions should not be needed. </s> class obj(object):	Obj Attributes: model: A ModelWeights structure, containing Tensors or Variables.
# todo: test me @jmcarp </s> )	conference_results Q('tags', 'eq', meeting) & Q('is_public', 'eq', True) & data = [ _render_conference_node(each, idx)
recording_software_name = none  # todo </s> info_csv = utils.read_info_csv_file(rec_dir)	is_pupil_invisible_recording try: return info_csv["Capture Software"] == "Pupil Invisible" and "Data Format Version" not in info_csv
# todo: verify </s> binds = manager.find_by_repo(self.repo_id)	test_remove_repo_cleanup def test_remove_repo_cleanup(self): self.test_bind() self.assertEquals(len(binds), 1) manager = factory.repo_manager()
# todo(guillermooo): we cannot access the ouput panel used by exec. </s> return target_path	get_target_path target_path = project.path_to_web if project.is_path_under(project.path_to_web, view.file_name()):
# todo: remove when 36lts is discontinued </s> with open(recorded_cmdline, 'r') as cmdline_file:	retrieve_cmdline return None
# todo: use color instead of [ ] </s> if 'abbrev-' in opts.ast_format:	PrintAst ast_f = fmt.HtmlOutput(f) else: tree = node.AbbreviatedTree() else:
# todo: rip that out </s> for key, value in d.copy().items():	flatten def flatten(d): if len(value) == 1: d[key] = value[0]
#todo force redraw rather than queue? (like before) </s> in the gtk world, this event is triggered when a widget's configuration	on_configure def on_configure(self, widget, event): is modified, for example when its size changes. So, when this event is triggered, we tell the local :class:`~pympress.surfacecache.SurfaceCache`
# todo: clean up </s> for i, p in enumerate(processes):	render_chunks processes.append(p) p.start() log.debug("Waiting for proc %d", i) p.join()
# todo: make sure the image is present or pull it </s> letters = string.ascii_lowercase	random_word def random_word(length):
# todo: more advanced widget for this </s> logger.debug('q: %s', self.question)	QuestionDialog def run(self): Returns True if the user clicked 'Yes', False otherwise. gtk.MessageDialog.run(self) self.destroy()
# todo remove sorted? completions should be sorted? </s> print('test @%s: %s' % (line_nr-1, line))	run_completion_test@29 completions = functions.complete(source, line_nr, 999, completion_test_dir) print(traceback.format_exc()) return 1
# todo: replace with stream-changed </s> def get_current_tl_track(self):	get_current_tl_track
# todo: this completion may not be good, since it resets to 0 later. </s> args = parser.parse_args()	main@60 help='A list that defines the weights of style') parser.add_argument('--bars', default=8, type=int, dest='bars', style = args.style samples = 5
time_behind = time.time() - cblock.ntime   # todo: block times are not very reliable. </s> pass	VersionError
# todo: verify/modify these lists </s> to_return.is_repeatable = is_repeatable	CompatibilityTestCase to_return.allowable_values = allowable_values to_return.short_name = short_name to_return.min_occurs = min_occurs to_return.tag = tag
# todo: this actually looks like a bug, but this code </s> return self.builder.getbuild(self.number - 1)	getPreviousBuild def getPreviousBuild(self): if self.number == 0:
# todo: handle ta seeds </s> best = np.argmax(acq_vals)	SMBO configuration, acq_val = self.local_search.maximize(start_point) found_configs.append(configuration) return found_configs[best].get_array()[np.newaxis, :]
device.tags[q] = self.data.tags[ids] #todo </s> self.learning_rate = learning_rate	TrainProcess def __init__(self, network, devices, data, batches, learning_rate, gparams, updater, start_batch = 0): super(TrainProcess, self).__init__('train', network, devices, data, batches, start_batch) self.gparams = gparams def initialize(self):
# todo deprecated, remove in 1.4.0 </s> if self._gcode:	_do_abort self._gcode.abort()
# todo add options to maodify the sorted by key and the header options </s> print('thank you for using poseidon')	do_quit def do_quit(self, arg): self.close() return True
# todo: get this from ourselves. </s> new_statement = self.onstatementssequence( statement )	_onStatementsFrame assert statements, statements_sequence for count, statement in enumerate( statements ): else: new_statement = self.onStatement( statement )
# todo: arrange </s> def createrepo(self):	createRepo repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "name", "testrepo0", self.token)
logg.error("too long") # todo </s> cmd = "{systemctl} reload-or-restart zz-unknown.service -vv"	test_3805_reload_or_restart_some_unknown testdir = self.testdir() root = self.root(testdir) out, err, end = output3(cmd.format(**locals())) logg.info(" %s =>%s \n%s\n%s", cmd, end, err, out)
# todo(ib-steffen): allow custom ca bundles </s> values (%s, %s, %s) returning id	Projects abort('Repo already connected') project = g.db.execute_one_dict(''' project_id = project['id'] g.db.execute('''
# todo implement </s> lpbuffer = params["lpbuffer"]	hook_WriteFile def hook_WriteFile(ql, address, params): ret = 1 nNumberOfBytesToWrite = params["nNumberOfBytesToWrite"] lpNumberOfBytesWritten = params["lpNumberOfBytesWritten"]
#todo: geli detach -l </s> proc = self.__pipeopen("geli attach %s -k %s %s" % (	geli_attach_single _passphrase = "-p" else: _passphrase, key,
# todo: remove update flicker. for win32console we could set the cursor </s> self._output_module = getattr(options, u'output_module', none)	_ParseOutputOptions BadConfigOption: if the options are invalid.
# todo: move this function somewhere else </s> return _(instance.pootle_path[len(translation_project.pootle_path):])	DirectoryFormField class DirectoryFormField(forms.ModelChoiceField):
# todo: handle escape (0x1b) </s> calculated_crc = crc16.crc16xmodem(''.join([chr(item) for item in message[:-2]]))	valid_crc def valid_crc(self, message): return supplied_crc == calculated_crc
# todo: the logic here for ion concentration setting is in two </s> return self.energy / self.composition.num_atoms	energy_per_atom def energy_per_atom(self): energy per atom of the pourbaix entry
# todo(tr3buchet) - remove comment in multi-nic </s> def get_public(self):	get_public
# todo(nakago): check why tolerance is high </s> def model():	model return GraphAttentionNetworks(out_dim=out_dim)
# todo: cleanup </s> def createrepo(self):	createRepo repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "name", "testrepo0", self.token)
except httperror as e:  # @todo ask for server instead </s> chosenserverlist = bestserverlist[random.randrange(0, len(bestserverlist) - 1)]	chooseBestServer chosenServer = chosenServerList[0]  # the first value, server return chosenServer
# todo: raise proper error </s> def __init__(self, mem):	SplitContext echo $x  # uses default shell IFS IFS=':' myfunc  # new splitter self.mem = mem self.splitters = {}  # type: Dict[str, IfsSplitter]  # aka IFS value -> splitter instance
# todo: replace with is_finite() instead of checking cardinality? </s> def monoid_generators(self):	ParentMethods class ParentMethods: Return the generators of ``self``. EXAMPLES::
# @todo: return only packages for the current architecture </s> self.uri = f'/rpc/?{self.params}'	get_task return https_client_task(loop, self.host, self.uri)
# todo: write validate method </s> self.kibanasavedobjectmeta = dict()	BaseKibanaDashboardObject self.panelsJSON = str()  # To je wazne self.optionsJSON = str()  # to tyz self.version = 1 if title:
# todo: determine correct computation for panning. http://en.wikipedia.org/wiki/pan_law seems relevant but was short on actual formulas. may depend on headphones vs speakers? this may be correct already for headphones -- it sounds nearly-flat to me. </s> self._rebuild_demodulator(mode=mode)	set_mode self.demodulator.set_mode(mode) self.mode = mode
# todo: ensure template has resources </s> with tests.loggingfilter('zim.formats.latex', 'could not find latex equation'):	TestTemplateOptions exporter = build_page_exporter(file, 'latex', 'Article', page) notebook = tests.new_notebook(fakedir='/foo') exporter.export(selection) result = file.read()
# todo: make options for "text included" vs. "text matches" </s> match_sets.append(m)	check_consistency match_sets = [] m = eq_text_neq_type_spans(ann_objs, restrict_types=restrict_types, ignore_types=ignore_types) return match_sets
# todo: fetch spoolup option </s> for val in values:	attrDirectMap @staticmethod target[val] = source.getModifiedItemAttr(val)
# todo(mriedem): call select_destinations() with a </s> self.migration.source_node = self.instance.node	LiveMigrationTask self.instance.availability_zone = ( availability_zones.get_host_availability_zone( self.migration.dest_node = dest_node self.migration.dest_compute = self.destination
# todo: check return value of attachthreadinput properly </s> please do not use in production code yet - not tested fully	PopupWindow popup = win32functions.GetWindow(self, win32defines.GW_HWNDNEXT) return popup
pass # todo </s> self.add_deltas(event.delta_x, event.delta_y, 10)	on_scroll_on_area
# todo: process </s> def getsecret(cls, consumer):	getsecret secret = None certificate = consumer.get('certificate')
# todo(ihrachys): replace with port.create() once we get an object </s> _test_class = policy.qospolicy	QosPolicyBaseTestCase
# despite copystat mtime is not copied. todo </s> return os.fstat(fd.fileno()).st_ino	inode def inode(fname):
log_importance_weight = none  # todo: check the reason/behavior for this </s> global _current_trace	_begin_trace global _current_trace_previous_variable global _current_trace_replaced_variable_proposal_distributions
# todo(jeremydw): read manifest and takedown old content here. </s> self.out_dir = out_dir	FileSystemDeployment class FileSystemDeployment(base.BaseDeployment): def deploy(self, pod): pod.dump(out_dir=self.out_dir)
# todo: write this method if possible </s> res = self.proxy.getrawtransaction(txid)	getrawtransaction def getrawtransaction(self, txid):
raise notimplementederror #todo </s> class span(object):	Span
# todo: make an ascii-art bar </s> if s >= 0.001:	render_time return "%.2fs" % s if s >= 0.01: return "%.1fms" % (1000*s) return "%dus" % (1000000*s)
pass  # todo </s> if not self._device:	RivalMouse def _device_write(self, *bytes_): Arguments: return; self._device.write(bytearray(bytes_))
# todo: add strict mode and expose warnings. </s> if op.do_all:	_ConstStringReplacer self.pat = pat self.replace_str = replace_str return s.replace(self.pat, self.replace_str) elif op.do_prefix:
# todo add locales </s> raise yunohosterror("domain_name_unknown", domain=new_main_domain)	domain_main_domain if not new_main_domain: return {"current_main_domain": _get_maindomain()} operation_logger.related_to.append(("domain", new_main_domain)) operation_logger.start()
raise exceptions.mpdnotimplemented  # todo </s> already. the name may consist of alphanumeric ascii characters plus	subscribe@18 *musicpd.org, client to client section:* ``subscribe {NAME}`` underscore, dash, dot and colon. raise exceptions.MpdNotImplemented  # TODO
# todo: test me @jmcarp </s> return {	conference_results _render_conference_node(each, idx) for idx, each in enumerate(nodes) 'data': json.dumps(data), 'label': meeting,
# @todo: extend entity_types within the template </s> represent an image as a clickable thumbnail	doc_image_represent @param filename: name of the image file if not filename:
# bezel correction todo don't show if span mode is multi image or simple span. </s> show_message_dialog(msg, "error")	onAlignTest if (inches == "") or (len(inches) < NUM_DISPLAYS): msg = "You must enter a diagonal inch value for every \ inches = [float(i) for i in inches] bezels = self.tc_bez.GetLineText(0).split(";")
# todo: sinpi </s> return f	_mathfun_n except (TypeError, ValueError): return f_complex(*(complex(x) for x in args))
# todo: dynamically define all endpoints </s> tornado.ioloop.ioloop.instance().add_callback(self._shutdown)	on_stop if self.zeroconf_service: self.zeroconf_service.unpublish()
#todo discont: actually use offsets instead of (start, end)! </s> dstr = ' disabled="disabled"'	__generate_input_and_label if not disabled: dstr = "" s  = indent+'    <input id="%s%s" type="radio" name="%stype" value="%s" %s/>' % (prefix, t, prefix, t, dstr) s += '<label for="%s%s">' % (prefix, t)
# todo: do we change this to something like "threshold" </s> except keyerror as e:	make_alice_control bob = Bob.from_public_keys({DecryptingPower: bob_pubkey, SigningPower: None}, return Response(str(e), status=500) new_policy = drone_alice.create_policy(bob, label, m, n,
# todo(piyush): current api-site doesn't contain this api description. </s> 'progress': progress	update_snapshot_status def update_snapshot_status(self, snapshot_id, status, progress): post_body = { } post_body = json.dumps({'os-update_snapshot_status': post_body})
return  # todo: return a 200, with whatever policy metadata. </s> super().__init__(*args, **kwargs)	NuCypherSeedOnlyDHTServer def __init__(self, *args, **kwargs):
# todo: action value doesn't exist for beta </s> reward_estimation = dict(horizon=10, discount=0.99, estimate_horizon=false)	test_no_horizon_estimate self.unittest(reward_estimation=reward_estimation)
#todo: python2 specific, remove </s> '-c', '--color', action='store_true', dest='color',	get_args help='Run in debug mode (display extra info).', ) help='Output in color (only works with debug on POSIX).', )
#todo: overly broad exception needs fixing </s> logger.error(e)	delete_all except pyelasticsearch.exceptions.ElasticHttpNotFoundError as e:
# todo(b/161332815): make jax actor work with batched or unbatched inputs. </s> key=next(self._rng),	RecurrentActor def select_action(self, observation: types.NestedArray) -> types.NestedArray: action, new_state = self._recurrent_policy( observation=observation, core_state=self._state)
# todo fix. </s> if not cmp_result:	test_cmova ctx_init = self.__init_context() x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) self.__save_failing_context(ctx_init) self.assertTrue(cmp_result, self.__print_contexts(ctx_init, x86_ctx_out, reil_ctx_out))
# todo: arrange </s> self.remote.modify_repo(repo, "mirror_locally", "0", self.token)	createRepo self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token)
#todo this is not valid in multiprocessing! </s> class testarbiterprocess(testarbiterthread):	TestArbiterProcess impl = 'process'
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
# todo find out if this is good because of sparcity... </s> upper=10 ** 5,	get_hyperparameter_search_space UniformFloatHyperparameter(name="threshold_lambda", log=True, default=10 ** 4)) fit_intercept = cs.add_hyperparameter(UnParametrizedHyperparameter(
# todo: change the frontend to pass seconds instead. </s> return {'sub': 'email', 'email': email, 'exp': id_token_expiration_timestamp}	userinfo_mock
# todo: remove after pylint 1.4+ </s> return '<br/>'.join(sorted(tldextract.tldextract.tld_extractor.tlds))	TLDSet class TLDSet(object): def GET(self): # pylint: disable=invalid-name,no-self-use
# todo(ytknzw): add more specific assertion with the test case. </s> raise valueerror	fail_objective
# fixme: todo </s> posting.price is not none)	is_special pos = posting.position return (pos.lot.currency == CCY_FIFO and
#todo: assuming constant mu </s> @property	Me
# todo test cases </s> new_sensor_alert_states.append(sensor_alert_state)	_filter_sensor_alerts continue
# todo(pkilambi): process the output as needed </s> return false	service_delete try: out, err = utils.trycmd('kubectl', 'delete', 'service', uuid) except Exception as e: LOG.error("Couldn't delete service  %s due to error %s"
# todo(developer): uncomment these lines and replace with your values. </s> client = tasks.cloudtasksclient()	purge_queue@197 queue_path = client.queue_path(project, location, queue) response = client.purge_queue(queue_path)
#todo: actually check for change </s> if s.startswith("0x"):	str_to_bool return True try: s = s[2:] r = 16
# todo: password is a required field for a galaxy user record. however, it should not be required </s> def add_output_dataset_collection(self, name, dataset_collection_instance):	add_output_dataset_collection
# todo: remove snap when updating this image </s> def test_global_map():	test_global_map @ImageTesting(['global_map'], plt.axes(projection=ccrs.Robinson()) plt.plot(-0.08, 51.53, 'o', transform=ccrs.PlateCarree())
# todo: decorate with abstractmethod after torchhook is extended </s> def __exit__(self):	BaseHook raise NotImplementedError
# todo: extend it for estimator from other frameworks - mllib, h20, vw </s> ids_by_class = list(map(lambda label: np.where(y==label), labels))	topk_tfidf_features_by_class def topk_tfidf_features_by_class(X, y, feature_names, class_index, min_tfidf=0.1, top_n=25): feature_df = _topk_tfidf_features_overall(X[ids_by_class[class_index]], feature_names, min_tfidf, top_n) feature_df.label = ids_by_class[class_index]
# todo : </s> self.ymin = spec_min	setspecrange self.ymax = spec_max self.verticalScale.setScaleDiv(self.verticalScaleEngine.transformation(),
# todo: must be implemented </s> pass	should_fetch_kindlegen
# todo setup mahout, must be checked out from repo atm: </s> http://archive.cloudera.com/docs/ec2.html	_setup_hadoop@198 def _setup_hadoop(config):
# todo: add proper checks (e.g. check if input stuff is pandas full of objects) </s> return check_array(x, dtype=none, ensure_2d=false)	check_ts_array@19 def check_ts_array(X):
# todo: instead of raising, we should do something </s> self._m = map_in_the_clear[0]	orient else:
# todo: change the frontend to pass seconds instead. </s> def userinfo_mock(selfless, request):	userinfo_mock
# todo placeholder; implement </s> ]	RoleInfo "role_world_size", "local_world_size", def __init__(self, name, role_world_size, local_world_size, worker_infos): self.name = name
# todo remove input dropout, just here for testing </s> r"""implementation of the conve kge model."""	ConvE def __init__(self, config: Config, dataset: Dataset, configuration_key=None): super().__init__(config,
# todo: ... </s> returns a list of child entities that don't have cchq_case_id set	get_children_only_theirs dhis2_api = Dhis2Api(settings.DHIS2_HOST, settings.DHIS2_USERNAME, settings.DHIS2_PASSWORD) return dhis2_api.gen_instances_with_unset('Child', 'cchq_case_id')  # TODO: Or would that be 'CCHQ Case ID'?
raise  # todo </s> periods = int(math.ceil(hours / int(constants.hours_per_period)))	BlockchainPolicy for ursula in ursulas: delta = expiration - maya.now() blockchain_arrangement = BlockchainArrangement(author=self.alice, miner=ursula, value=deposit, lock_periods=periods,
#todo: add metadata support when it is merged from develop </s> jid = "%s" % jid	_escape_jid def _escape_jid(jid): jid = re.sub(r"'*", "", jid) return jid
timeout = 0.1  # todo: receive as a parameter </s> if answer.lower().strip() not in ('y', 'yes'):	Command 'dataset table.' ) exit() table = Table.objects.for_dataset(dataset_slug).named(tablename)
# todo: move this file counting into the `productgraph`. </s> self.log.debug('subproc_map result still not ready...')	subproc_map while not res.ready(): res.wait(60)  # Repeatedly wait for up to a minute. return res.get() except KeyboardInterrupt:
# todo: make sure reply_email is unique </s> parse srs0=8lgw=y6=outlook.com=abcd@mailsl.meo.ovh and return abcd@outlook.com	parse_srs_email local_part = srs[: srs.find("@")]  # srs0=8lgw=y6=outlook.com=abcd local_email_part = local_part[local_part.rfind("=") + 1 :]  # abcd
# todo: logging, or warning </s> nsec = 0.5 * (2.0 ** float(recurse_depth))	exponential_backoff content = ''.join(lines) return cPickle.loads(content) print "Waiting " + str(nsec) + " seconds and trying again" time.sleep(nsec)
# todo: convert [n, c, v] to  new convention [v, n, c] </s> sqrt_mc_h /= sqrt(n)	sqrt_hessian_sampled sqrt_mc_h = (probs_unsqueezed - classes) / sqrt(M) if module.reduction == "mean": return sqrt_mc_h
# todo xxx graalvm change </s> sni_name=none)	test_sni_callback calls = [] stats = server_params_test(client_context, server_context, self.assertEqual(calls, [(None, server_context)]) self.check_common_name(stats, SIGNED_CERTFILE_HOSTNAME)
# todo still need to remove the file </s> move_win(uploaded_file.name, newpath)	move_to_temp_dir_and_restore_filename newdirpath = tempfile.mkdtemp() newpath = os.path.join(newdirpath, origfilename) else: shutil.move(uploaded_file.name, newpath)
# todo(rbg): remove duplicated code </s> self._prefetch_process.start()	RoIDataLayer self._prefetch_process = BlobFetcher(self._blob_queue, self._roidb, def cleanup(): print 'Terminating BlobFetcher'
# todo: remove when pre-csrf token templatetags are no longer supported </s> html = template.render(c)	test_csrf_token_GET_form c = Context({'form': TestForm(), 'form_helper': form_helper, 'csrf_token': _get_new_csrf_key()})
# todo: remove in 21.08 </s> log.debug("cache file 'cache_text.txt' already exists")	generate_cache_text@57 text_file.close() LOG.debug("Completed generating cache") except Exception: LOG.exception("Could not open text file to write cache")
assert study_id == 0  # todo(akiba) </s> assert study_id == 0	set_study_param_distribution def set_study_param_distribution(self, study_id, param_name, distribution):
# todo: this check was too simple, and broke a few things: </s> word = next(w for w in m.groups() if w is not none)     # pragma: part covered	dollar_replace if word == "$": return "$"
# todo it would be good to support different kind of shells </s> 'ssh_allowed': admin_unix.pw_shell.strip() != "/bin/false",	_get_user_for_ssh 'username': 'admin', 'fullname': '', 'shell': admin_unix.pw_shell, 'home_path': admin_unix.pw_dir,
raise notimplementederror # todo </s> for s in self.states_list:	resample_states s.resample()
# todo generator </s> recursive=false,	Drop def __call__( path=None, recursion_limit=None, check=True,
# todo find out if this is good because of sparcity... </s> def __init__(self, alpha, fit_intercept=false, normalize=false,	RidgeRegression copy_X=False, max_iter=None, tol=0.001, solver='auto', random_state=None):
# todo: fixed by using realpath, but there should be a cleaner </s> yield _test_annexdb, cls	test_AnnexDBs def test_AnnexDBs(): for cls in (PhysicalFileStatusesDB,
# '-rs',  # @todo: manually remove dependencies of conflicting packages, </s> if not all_upgradeable_package_names:	cli_upgrade_packages ] + [ u.Name for u in aur_updates print('\n{} {}'.format( color_line('::', 10),
# todo : error on unmatched alias </s> }	ChatopsController supported_filters = { 'name': 'name', query_options = { 'sort': ['pack', 'name']
self.mechanism_bin = serialize(value, to_bytes=true)  # todo: techdebt fix </s> def obj(self, value: any) -> none:	obj@30 self.mechanism_bin = serialize(value, to_bytes=True)  # TODO: techdebt fix
# todo(okuta): check type </s> cupy.ndarray: the standard deviation of the input array along the axis.	std keepdims (bool): If ``True``, the axis is remained as an axis of size one. .. seealso:: :func:`numpy.std` return a.std(axis=axis, dtype=dtype, out=out, ddof=ddof,
# todo enable ssh on the nodes by changing the image </s> config and watch docker ps output.	DeploymentTests return d def test_deploy(self): temp = FilePath(self.mktemp()) temp.makedirs()
# todo: change this to use a getter </s> self.invalid_yaml_msg_single_quotes = '%s (found single quotes not double quotes)' % self.invalid_yaml_msg	YamlValidatorTool self.invalid_yaml_msg = '%s => YAML INVALID' % self.filename
#todo: add_land arg </s> mpstate.master().set_mode_rtl()	cmd_rtl
raise notimplementederror # todo </s> return hash(tuple(map(phash,d)))	phash if isinstance(d,np.ndarray): return d.__hash__()
prob_dist = results[0]#[-1] # todo: used for old model architecture </s> print('composition', composition)	main@73 history.append([note_hot, beat_input, completion_input, style]) composition.append(note) mf = midi_encode_melody(composition) midi.write_midifile('out/melody_{}.mid'.format(sample_count), mf)
# todo: delete me </s> raise nosuchfileexception(jobstorefileid)	readFile def readFile(self, jobStoreFileID, localFilePath): headers = self.encryptedHeaders with open(localFilePath, 'w') as writeable: self._getKey(jobStoreFileID, headers).get_contents_to_file(writeable, headers=headers)
raise notimplementederror # todo </s> raise notimplementederror # todo	Site raise self.MultipleObjectsReturned def save(self, item): def remove(self, item): Remove/delete the item from the backend storage
# todo? don't consider the empty set here </s> else:	big_acmip log.debug("Found unpartitioned account.") if not unpartitioned_account: ac_mip = _null_ac_bigmip(context, direction) ac_mip.alpha = float('inf')
# todo: add a check for similarly valid value back in time. maybe if it the </s> try:	PlsRecallParser file_object: A file-like object. Raises: is_pls = self.VerifyFile(file_object) except (IOError, construct.FieldError) as exception:
# todo(jakevdp): remove when minimum jaxlib is has extension version 4 </s> def define_integer(self, name, default, *args, **kwargs):	DEFINE_integer
# todo: add a better throttling mechanism </s> group (union[str, int, none]): facebook group id e.g. 676845025728409	write_posts_to_csv ): Args: filename (str): Filename, defaults to <account or group>_posts.csv encoding (str): Encoding for the output file, defaults to locale.getpreferredencoding()
# todo move to common? </s> def __init__(self, files: sequence[path]) -> none:	Query self.files = files @mcachew(
# todo(dcramer): this would make more sense as part of the xunit handler </s> try:	sync_job@93 ).filter( JobPlan.job_id == job.id, if not job_plan: raise UnrecoverableException('Got sync_job task without job plan: %s' % (job.id,))
# todo_recorders - need to pass in parent info instead of none </s> system = self._system	NonLinearRunOnce absolute error. float from openmdao.recorders.base_recorder import push_recording_iteration_stack, \ print_recording_iteration_stack, pop_recording_iteration_stack, \
# todo: fill these in </s> if n == 0:	_FindLastSimpleCommand return None assert hasattr(node, 'children'), node return None return _FindLastSimpleCommand(node.children[-1])
except (asyncio.cancellederror, asyncio.timeouterror) as err:  # todo: is this needed? </s> log.debug("connection made to %s:%i", self.peer_address, self.peer_port)	connection_made def connection_made(self, transport: asyncio.Transport): self.transport = transport
# todo: refactor me, please! </s> path = self._args['folder_altered']	ImageProcessing if (not os.path.isdir(folder_path)): os.makedirs(folder_path, exist_ok=True) self.__image_steps = [self.__input_path] + [ os.path.join(path, "{}.png".format(p().__class__.__name__))
# todo: this needs serious refactoring </s> return {"max_cred_num": self._batch_size}	_on_pool_create_ext_params
# todo for pytorch 2.0.4, need to set dim=1 for log_softmax or use softmax then take log </s> output_c = f.elu(self.dense_c(output))	_get_rnn_output output, hn = self.rnn(input, hx=hx) output = self.dropout_rnn(output) return (output_h, output_c), hn, mask, length
# todo: i bet this interferes with views whose column names can </s> def _from_serialized_columns(cls, t_id, columns):	_from_serialized_columns d_columns = dict((attr, cls._deserialize_column(attr, val)) for (attr, val)
# todo: use tor proxy session </s> print(jdata)	apiPlaceOrder except Exception as e: print("error='FAILED JSON PARSING'")
# todo document after plugin data is refactored </s> log.warn("\"plugin.json\" at \"{0}\" does not exist.")	load_plugin if not os.path.isdir(plugin_path): raise IOError raise IOError log.info("Loading plugin {0}".format(plugin_json_path))
# todo: migrate to sql </s> duration = sync_log_dict.get('duration')	Command headers ) cases = len(sync_log_dict.get('cases_on_phone', [])) dependent_cases = len(sync_log_dict.get('dependent_cases_on_phone', []))
# todo: remove when transition to python3 complete </s> return frd(-self.fresp, self.omega)	__neg__
self.__top.start()  # todo overriding internals </s> self.data += chunk	dataReceived
# todo: check if this logic is sufficient </s> rshape, cshape,	eigen_tensor cstart = 0 if (rshape, cshape) != expr.shape: rstart, cstart)) else:
# todo: investigate django how this can be avoided </s> and its querysets from polymorphicqueryset - throw assertionerror if not"""	validate_model_manager @classmethod if not issubclass(type(manager), PolymorphicManager): e = 'PolymorphicModel: "' + model_name + '.' + manager_name + '" manager is of type "' + type(manager).__name__
# todo: determine language based on preprocessing information. </s> type=str, default=none, help=(	AddStorageOptions argument_group (argparse._ArgumentGroup): argparse argument group. argument_group.add_argument( 'The path of the storage file. If not specified, one will be made '
# todo: if not standalone, call ipc directly rather than </s> from zim.search import searchselection, query	cmd_search query = query.strip() if not query: raise AssertionError, 'Empty query'
# todo: allow units to be added/removed </s> return false	close_position def close_position(self, currency_pair): else: ps = self.positions[currency_pair]
# todo(tsileo): handle tombstone </s> def clean_html(html):	clean_html
# todo put this in a .extra w/a subselect </s> contract = models.foreignkey(	ContractHour ) hours = models.DecimalField( ProjectContract, related_name='contract_hours') date_requested = models.DateField()
# todo: implement this. </s> self.assertequal(keypairs.encryptingkeypair, type(keypair))	test_ecies_keypair_generation def test_ecies_keypair_generation(self): self.assertEqual(bytes, type(keypair.privkey)) self.assertEqual(bytes, type(keypair.pubkey))
# [[shape], kernel, stride] #todo: add padding </s> assert(np.allclose(f_result, workspace.fetchblob("y"),	test_avgpool@95 f_ng = importer.get_op_handle("Y") ex = ExecutorFactory() atol=1e-4, rtol=1e-3, equal_nan=False))
# todo: validate </s> elif mask & self.knights:	piece_type_at mask = BB_SQUARES[square] if mask & self.pawns: return KNIGHT elif mask & self.bishops:
" # todo: i18n", </s> @mock.patch.object(log, "exception")	ToolTest ) self.assertTrue(os.path.isfile(self.rej_file)) def test_process_hunks_no_hunks(self, mock_log): tool = BowlerTool(Query().compile(), write=True, interactive=False)
# todo: should be a method on shape. </s> def add_tri(self, x, y, z):	add_tri
# todo: check syntax </s> if not self.response.parsed_hdrs.has_key('location'):	status303 def status303(self):        # See Other
# todo: return errors in a universal way </s> return code_store.full_code()	compile_code ast_root = parser.parse(token_list) code_store = CodeStore()
# todo(john sirois): clean this up when build parse refactoring is tackled. </s> self.resources = resources	ScalaLibrary exclusives=exclusives) if (sources is None) and (resources is None): self._java_sources = [] self._raw_java_sources = util.resolve(java_sources)
# todo: verify behavior </s> self.assert_vsc_received(received, [	test_few ) self.send_request() self.expected_response( threads=[
if python_version < 340 or true: # todo: temporarily reverted: </s> display_file = not isfullcompat(),	onEnterNode ),
# todo put an index.html in front of this bucket </s> if key_name.endswith('/index.html')}	publish_docs previous_version_keys) changed_keys |= {key_name[:-len('index.html')] changed_keys |= {''} changed_paths = {prefix + key_name
pass  # todo: raise error to force subclasses to implement it </s> def delete(self):	GLObject def __del__(self): if self._need_delete: self._delete()
# todo(rkukura): filter on extended provider attributes. </s> session = context.session	create_network (network_type, physical_network, vlan_id) = self._process_provider_create(context, with session.begin(subtransactions=True): if not network_type:
for char in ('\\', '\n', '\t'):  # todo: more escapes? </s> return none	Hostname def deserialize(self, value): validators.validate_required(value, not self.optional) try: socket.getaddrinfo(value, None)
# todo: out to file </s> url = "{}://{}/{}".format(self.proto, self.target, uri)	_fetch :param uri: URI to fuzz :param sub_domain: If True, build destination URL with {URL}.{HOST} else {HOST}/{URL} else: url = "{}://{}.{}".format(self.proto, uri, self.target)
# todo: paginate </s> return render(request, 'spirit/admin/topic/topic_closed.html', context)	topic_closed def topic_closed(request): topics = Topic.objects.filter(is_closed=True)
# todo: reconsider logging level when we have consistent practice. </s> self.node_dimension = node	_set_dimension_names ] else: self.edge_dimension = edge self.face_dimension = face
# todo: add test and check this more throughroughly. </s> ret = layer(inputs)	easy_apply def easy_apply(layer, inputs): Apply a layer to input[s]. except (TypeError, AttributeError): if len(inputs) != 1:
# todo: must be implemented </s> def should_fetch_kindlegen():	should_fetch_kindlegen
# todo: cleanup and deprecate worker_address in config files, leaving only checksum_address </s> process.start()	get_provider_process if start_now:
# directory exists, but no todo.txt file - create an empty one </s> if os.path.isdir(file_path):	get_real_path@57 def get_real_path(filename, description): exit_with_error("ERROR: Specified {0} file is a directory.".format(description)) if not os.path.exists(file_path):
# todo: set cchq_case_id in dhis2 </s> })	get_children_only_theirs 'ou': top['id'], 'ouMode': 'DESCENDANTS', child_entities = dhis2_entities_to_dicts(json) if child_entities:
# todo: error handling like numba callwrappers.py </s> def generic(self, args, kws):	QuantileType @infer_global(quantile) @infer_global(quantile_parallel) assert not kws assert len(args) in [2, 3]
if field.unique or field.primary_key:  # todo: multi-fields. </s> if klass in seen:	dependencies seen = set() while stack: continue seen.add(klass)
# todo -1 here added because that is done in rot90, but will have to be fixed </s> rs2_copy = copy.deepcopy(rs2)	_same_rs def _same_rs(rs1, rs2): rnd1 = rs1_copy.randint(0, 10**6) rnd2 = rs2_copy.randint(0, 10**6)
'xception'      : [testmodels.coremlemit, testmodels.cntkemit, testmodels.mxnetemit, testmodels.pytorchemit, testmodels.tensorflowemit], #  todo: caffe(crash) testmodels.kerasemit(too slow) </s> test_table = get_test_table()	test_caffe tester = TestModels(test_table) tester._test_function('caffe', tester.CaffeParse)
# todo: refactor, move to utils </s> field_dict[field.name] = field	get_field_dict field_dict = SortedDict() names = [] return field_dict
# todo - send file in chunks if file size > some threshold. </s> ...	_parse_scan_result threat_found: '<Threat Name>'|'', scan_result: '<Value from MD_SCAN_RES_CODES> ] }
# todo: should we ignore and use 0.0.0.0, or try using what the user asked and fail? </s> override this method if you wish to handle the decoded data	_handle_decoded_payload differently. data[u'to'] = int(data.get(u'to', self.opts[u'timeout'])) - 1
# todo: add prompt to specify user and pass manually. </s> if len(tables) >= 5:	anubis_parse reports = [] soup = BeautifulSoup(page) table = tables[4] cols = table.findAll('td')
return # todo raise error </s> pass	Seeked @dbus.service.signal(dbus_interface=PLAYER_IFACE, signature='x') def Seeked(self, position):
# self.assertisnotnone(result_set.query_planning_time_in_millis)  # todo flaky test </s> self.assertisnone(result_set.description)	test_cancel cursor.cancel(query_id) result_set = future.result() self.assertIsNone(result_set.fetchone()) self.assertEqual(result_set.fetchmany(), [])
# todo: test without file </s> n = 1001	test_nunique df.A[2] = 0 return df.A.nunique() np.testing.assert_almost_equal(hpat_func(n), test_impl(n))
# todo: move this into onnx main library </s> feed_dict_raw.items()))	TensorflowBackend assert len(node.inputs) == len(inputs) feed_dict_raw = dict(zip(node.inputs, inputs)) ops = cls._onnx_node_to_tensorflow_op(node, input_dict) output_vals = []
# todo: handle multiple skip stacks </s> pass	FlexLine
# todo this is not right, but does it need to be? </s> format_value(x, self.table) for x in self.values]	SetRule self.values = values def as_sql(self): set_check = '%s IN (%s)' % ( self.column.format(),
# todo change this and other yml names to match the tutorial </s> def setup(self):	MoveTests Similar to http://doc-dev.clusterhq.com/gettingstarted/tutorial/ moving-applications.html#moving-an-application pass def test_moving(self):
# todo do a proper mro resolution. currently we are just listing </s> the typevars in the resulting classes have sometimes different names	define_generics def define_generics(self, type_var_dict): from jedi.inference.gradual.base import GenericClass and we need to check for that, e.g. a signature can be: def iter(iterable: Iterable[_T]) -> Iterator[_T]: ...
# todo handle 4 types of transition exceptions </s> return transactionstatus.cancelled	_next_from_successful if action == TransactionAction.complete: Logger.get('payment').info("Ignored complete action on successful status") else: Logger.get('payment').error("Invalid manual action {} on successful status".format(action))
# todo uncomment the actual test below after we have implemented the l1 attack </s> self.assertclose(objective, grad.abs().sum())	test_optimize_linear_linf self.assertEqual(grad.size(), eta.size())
# todo(solitude): remove this. </s> return jingo.render(request, 'account/activity.html', {'log': _get_items(none, all_app)})	activity_log @login_required def activity_log(request, userid):
# todo: not close enough </s> np.testing.assert_allclose(corr_fun, x)	test_xcorr_vs_old_implementation -0.34214866] corr_fun = correlate(self.a, self.b, shift=15) self.assertAlmostEqual(corr, 0.96516076) self.assertEqual(shift, -5)
# todo: better error reporting </s> if address is none:	ConnectNode @param name: the node name @type address: str address = name self.nc[name] = http.HttpClientRequest(address, self.port, http.HTTP_PUT,
# todo support startblock, endblock </s> if self._filtered:	_processTxEvents def _processTxEvents(self, block): self.handle_filtered_tx(block, tx_id, er) else:
# todo(jin feng) always output the unhandled exception details into a log file. </s> self.do_exit(result=1, msg=msg)	exit_and_fail
# todo: translate </s> return self.titles[lang]	title
# todo: check proactive neighbor resolution </s> self.sock.connect(self.faucet_event_sock)	setup_valve self.valve = valve_factory(dp)(dp, 'test_valve', self.notifier) self.valve.update_metrics(self.metrics)
# todo: call into expression language. </s> (( a=1+2 ))	_ReadArithExpr def _ReadArithExpr(self): ${a[ 1+2 ]} ${a : 1+2 : 1+2}
# todo: consider using eafp here instead. </s> return hasattr(obj, '__call__')	_is_callable
# todo return empty list if not loaded </s> return did_you_mean if did_you_mean else none	did_you_mean spotify.Error.maybe_raise(self.error) did_you_mean = utils.to_unicode(
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> authorityurl = sampleparameters['authorityhosturl'] + '/' + sampleparameters['tenant']	Test_AcquireTokenWithUsernamePassword "username" : "crwilcox@microsoft.com", "password" : "SUPER SECRET PASSWORD" resource = '00000002-0000-0000-c000-000000000000' # or 'https://management.core.windows.net/' cache = None # TODO: Make this a cache driver
# todo(termie): this stuff should probably be moved to middleware </s> return {}	noop
# todo have one global properties object so this is no longer necessary </s> props.set(utils.create_props(helper_prefix, helper_defs))	LuxCoreNodeTexOutput "type": "constfloat3", "value": color,
# todo: wrap exception </s> all_args = [x for x in possible if x]	get_all_arg_names possible = spec.args + [spec.varargs, spec.varkw] + spec.kwonlyargs
# todo: _busy_wait should timeout after n seconds </s> from inky import inkymockphat	test_init_mock_phat_black InkyMockPHAT('black')
if isinstance(err, asyncio.cancellederror):  # todo: remove when updated to 3.8 </s> to_restore.append((descriptor, sd_blob, content_fee))	StreamManager@374 ) if not descriptor: await asyncio.gather(*[ recover_stream(
# todo(slaweq): change this to neutron floating ips and turn neutron </s> test that waiting for a server returns the server instance when	test_wait_for_server @mock.patch.object(shade.OpenStackCloud, "get_active_server") @mock.patch.object(shade.OpenStackCloud, "get_server") its status changes to "ACTIVE". building_server = {'id': 'fake_server_id', 'status': 'BUILDING'}
# todo: make the get_closest_value to return region </s> if is_prefixed_property(view.substr(get_previous_line(view,line_prev_region))):	get_nearest_indent if not first_indent: if not is_prefixed_property(line_prev): first_is_ok = False else:
# todo: py3 typeerror: a bytes-like object is required, not 'str' </s> if self.passwd_file:	get_user return self._get_user_from_file(wanted_user) return self._get_user_from_host(wanted_user)
#@todo: recheck in 0.4.10 </s> if not link:	Http url    = re.sub(r'^(jd|py)', "http", pyfile.url) netloc = urlparse.urlparse(url).netloc return for _i in xrange(2):
pass  # todo: implement this </s> :param count: number of lines to delete.	insert_lines def insert_lines(self, count=1): displayed **at** and below the cursor move down. Lines moved count = count or 1 top, bottom = self.margins
# todo(dcramer): we should default offset to previous entry in logsource </s> additional_lines = int(line_num_info.split(',')[1])	_generate_sample_coverage_data line_num_info = line_num_info.replace('@@', '') if ',' in line_num_info: max_line_for_current_file = line_number + additional_lines else:
# todo: if !blocking... </s> if self.__is_exclusive():	SharedLock before calling this function. self.__lock.acquire() self.__exc = None if self.__nwait_shr > 0:
# todo: rate limiting </s> ctx.in_body_doc[k] = val	decompose_incoming_envelope for k, v in form.lists(): val = ctx.in_body_doc.get(k, []) for k, v in files.items(): val = ctx.in_body_doc.get(k, [])
# todo(rakhmerov): it's not stable, need to avoid race condition. </s> db_api.tasks_get(wb_name, execution['id'])	TaskRetryTest retry_count, _, __ = task_spec.get_retry_parameters() execution = self.engine.start_workflow_execution(WB_NAME,
# todo: theme me </s> for ae in d]	create_geoms for ae in d] geom_ae = [ae for d in [l.geom.REQUIRED_AES, matched = set(all_ae) & set(geom_ae) & set(self.key.columns) matched = matched - set(l.geom.manual_aes)
# todo: clean up </s> f"lenght of pubsub_routers={pubsub_routers} should be equaled to "	_make_pubsubs def _make_pubsubs(hosts, pubsub_routers): if len(pubsub_routers) != len(hosts): f"hosts={hosts}" )
step = 0.1  # todo </s> def is_sun_above_the_horizon(t):	sunrise def sunrise(ephemeris, topos): sun = ephemeris['sun'] return topos_at(t).observe(sun).apparent().altaz()[0].degrees > -0.8333 return is_sun_above_the_horizon
# todo remove </s> module = input.get_parent_until()	_evaluate_list_comprehension nested_lc = input.expression_list()[0] if isinstance(nested_lc, pr.ListComprehension): loop = pr.ForFlow(module, [input], lc.stmt.start_pos, lc.middle, True) loop.parent = parent or lc.get_parent_until(pr.IsScope)
# todo: add also video files? </s> root.accept('path')	validator def validator(self): from flexget import validator bundle = root.accept('list') bundle.accept('path')
#assert false, "todo: implement" </s> for k in range(j, 6):	filenames yield "K%c%cvK%c%c" % (PCHR[i], PCHR[j], PCHR[k], PCHR[l]) for i in range(1, 6): for l in range(1, 6): yield "K%c%c%cvK%c" % (PCHR[i], PCHR[j], PCHR[k], PCHR[l])
# todo: prettify output </s> print("this daemon might interfere with auto-cpufreq and should be disabled!\n")	gnome_power_detect_snap def gnome_power_detect_snap(): print("Due to Snap limitations, it needs to be disabled manually by running, i.e:") print("cd ~/auto-cpufreq/auto_cpufreq")
# todo: implement this method </s> callback(self, signum)	cb_wrapper
# todo repition of normalization may be wasteful on large phase diagrams </s> an exception class for phase diagram generation.	PhaseDiagramError pass
# todo: should we restore the user-registered handler? </s> pass	_DoNothing
# todo: hack </s> if not materials:	ThreeMFWorkspaceReader for material_container_file in material_container_files: container_id = self._stripFileToId(material_container_file) material_container = xml_material_profile(container_id) material_container.deserialize(archive.open(material_container_file).read().decode("utf-8"))
# todo: improve logic to handle simple types like list of strings? </s> if errors:	load_project_config except ValueError as e: msg.fail(invalid_err, e, exits=1) msg.fail(invalid_err, "\n".join(errors), exits=1) validate_project_commands(config)
# todo: support multi-index here </s> parameters	set_index Set the DataFrame index (row labels) using one or more existing columns or arrays (of the correct length). The index can replace the ---------- keys : label or array-like or list of labels/arrays
# todo: obviously incrementing the rows individually is bad. how </s> return token.doc.tensor[token.i]	get_token_vector_via_tensor
# todo: not implemented yet </s> match_sets = []	check_consistency Searches for inconsistent annotations in given Annotations objects.  Returns a list of SearchMatchSet objects, one for each m = eq_text_neq_type_spans(ann_objs, restrict_types=restrict_types, ignore_types=ignore_types) if len(m) != 0:
# todo extend to nonbinary nodes </s> return self.index < other.index	__lt__
# todo. optionally sort on birthdate </s> if cond:	display_nav_links ('contact', _('Contact'), self.report.use_contact), ] url = url_fname + self.ext if self.up:
# todo: remove when we stop supporting python < 3.5 </s> preprocessor=self.preprocessor_,	_QuadrupletsClassifierMixin else: check_is_fitted(self) estimator=self, tuple_size=self._tuple_size) return (self.score_pairs(quadruplets[:, 2:]) -
# todo: this is a debug level log </s> if i > 0:	split_args_in_optional_and_positional positions = [] for i, arg in enumerate(args): previous = args[i - 1] if (not arg.startswith("-")) and (
#todo: mock the socket! </s> self.asserttrue('connected' in session)	Telnet_Tests self.assertTrue(delta.seconds < 2) self.assertTrue(session['attacker_ip'] == '192.168.1.200') self.assertTrue('login_tries' in session) self.assertEqual(session['protocol'], 'telnet')
# todo replace with clone </s> raise valueerror(	_generate_subspaces elif self.subspace_size == self._FEATURES_SQRT_INV: k = n_features - round(math.sqrt(n_features)) + 1 f"Invalid subspace_size: {self.subspace_size}.\n" f"Valid options are: int [2, M], float (0., 1.],"
# todo: check aligned nans, (s1.notna() != s2.notna()).any() </s> for i in numba.parfor.internal_prange(len(a)):	_column_fillna_impl s = B[i] if np.isnan(s):
# todo: capture makedirs invocation here </s> self.teardowncommand()	TestGit def setUp(self): self.setUpCommand() def test_simple(self): self.patch_getCommand('git', 'path/to/git')
#todo: dataset/hda by id (from history) or check_ownership for anon user </s> {	_summary_hda_dict def _summary_hda_dict( self, trans, history_id, hda ): 'id'    : < the encoded dataset id >, 'name'  : < currently only returns 'file' >,
# todo(cp16net): need to set the return code correctly </s> path = "/{tenant_id}/instances"	API self._instance_router(mapper) def _instance_router(self, mapper): mapper.resource("instance", path, controller=instance_resource)
pass # todo: pass link problem upstream? </s> out.append(u"<td>%s%%</td>" % red.gzip_savings)	format_droid out.append(self.format_yes_no(red.ims_support)) out.append(self.format_yes_no(red.inm_support)) else: out.append(self.format_yes_no(red.gzip_support))
# todo(stephenfin): the mock of 'migrate_disk_and_power_off' should </s> flavor_id = self._create_flavor(extra_spec=extra_spec)	test_create_server_with_physnet_and_tunneled_net This should pass because we've requested a single-node instance and the requested networks share at least one NUMA node. networks = [ {'uuid': base.LibvirtNeutronFixture.network_1['id']},
# todo: update this to the correct kaggle.gcp path once we no longer inject modules </s> else:	monkeypatch_bq if explicit_project_id is None and specified_credentials is None and not has_bigquery: print("Using Kaggle's public dataset BigQuery integration.") if specified_credentials is None: kwargs['credentials'] = KaggleKernelCredentials()
# todo extract that to a method in otc(...) ? </s> if token.allowance_of(self.our_address, spender_address) < wad(2 ** 128 - 1):	setup_allowance print(f"Approving {spender_name} ({spender_address}) to access our {token.name()} balance directly...") if not token.approve(spender_address):
# rbarlow_todo: convert this callrequest into a celery task call </s> agent_manager = managers_factory.consumer_agent_manager()	cancel_agent_request :type call_report: pulp.server.dispatch.call.CallReport task_id = call_report.call_request_id agent_manager.cancel_request(consumer_id, task_id)
# todo: this should be handled in run_script </s> elif sig == signal.sigusr2 or sig == signal.sigill:	signal_handler if sig == signal.SIGUSR1 or sig == signal.SIGTERM or sig == signal.SIGINT: stderr('Got quit signal, shutting down.') stderr('Got restart signal.') p.restart('Restarting')
# todo: normalise by s**-.5? </s> s0 = self.s0	scales that still adequately samples scale. Smaller dj gives finer scale resolution. J = int((1 / dj) * np.log2(self.N * self.dt / s0)) sj = s0 * 2 ** (dj * np.arange(0, J + 1))
#todo get this working </s> if 'wui' in test:	_test_can def _test_can(self, action, user, modes, test=['wui', 'rest']): ok_wui = self._do_test_wui(action, user, mode) assert ok_wui, '(%i) Should be able to %r %r as user %r (WUI interface)' % (i, action, mode, user.name)
# todo: figure out what to do with this list </s> return self.code % self.fd	raw
# todo: this test is covered by the previous class, do we need a dedicated one? </s> 5.2)	TestMunsellColourToMunsellSpecification (10.0, 2.0, 4.0, 1)) self.assertEqual( self.assertEqual( colour.computation.colourspaces.munsell.munsell_colour_to_munsell_specification("0.0YR 2.0/0.0"),
# todo: this might be too slow because of the addition </s> pipe.llen(resolution_key)	Timeseries elif config.get('compress', False): pipe.hvals(resolution_key) res = pipe.execute() for idx,data in enumerate(res):
# todo: check against plural_rules[lang]['nplurals'] </s> if parameters:	twtranslate raise TranslationError("No English translation has been defined for TranslateWiki key %r" % twtitle) if code_needed: return trans % parameters else:
# todo include efficientnet </s> assert isinstance(layer, nn.linear)	reduce_linear_layer new_layer = nn.Linear(layer.in_features, n_classes) if keep_weights:
from vyper.old_codegen.expr import expr  # todo rethink this circular import </s> _label_counter += 1	_generate_label global _label_counter
# todo: figure out how to best show this kind of warning to the </s> isinstance(obj, bytes) or	is_simple def is_simple(obj): isinstance(obj, bytearray) or isinstance(obj, bytes) or
# @todo: possible optimisation: create a re.compile list </s> return d is false	is_enable except AttributeError: return True
# todo: there’s a vertical 0.5px shift on the second page </s> def to_pix(pixels_str):	to_pix
# todo: sound feedback to signal that this is an invalid action </s> if tps == 0: # pause	display_speed def display_speed(self): text = u'' text = u'0x' elif tps == GAME_SPEED.TICKS_PER_SECOND: # normal speed, 1x
except exception as error:  # todo: be specific </s> with self._mutex:	add_job self._jobs.append(job)
# todo: ... </s> def get_dump_path(file_name):	get_dump_path
# todo: finish this. </s> def write(self, filepath, data, offset, fh):	_GDriveFS logging.exception("Could not remove _OpenedFile for handle with ID" "(%d) (release)." % (fh)) self.__marker('write', { 'path': filepath, '#data': len(data), 'offset': offset, 'fh': fh })
assert self.stop_seq is none #todo: better handling of this situation </s> for p in self.roaster:	on_terminate_program if pid != p.pid: continue return p.on_terminate(status)
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> "authorityhosturl" : "https://login.windows.net",	Test_AcquireTokenWithUsernamePassword def test_acquire_token_with_user_pass(self): sampleParameters = { "clientId" : "04b07795-8ddb-461a-bbee-02f9e1bf7b46", # xplat's which is supposed to be in every tenant "username" : "crwilcox@microsoft.com",
return -1  # todo: followup after decision around returning none </s> name = "\\device\\{}".format(self.deviceobject.get_device_name())	_FILE_OBJECT def file_name_with_device(self) -> str: name = "" try: name += self.FileName.String
# todo: more variables on the same line? </s> not_just_variable_declarations = false	postprocess imported_files = [] def parse(f): in_variable_declarations = False in_comment = False
# todo(luotao): use clone() method to flush the program.desc in force, </s> for input_arg in current_op.input_arg_names:	_adjust_input def _adjust_input(self): for i in range(len(self.block.ops)): if input_arg in self.input_map: current_op.rename_input(input_arg,
# todo: move to base class </s> scene.setscenerect(qtcore.qrectf(0, 0, 10, 10))	createScene def createScene(self): scene = QGraphicsScene(self) return scene
# todo: the following skipped suite and fixtures should be enabled </s> if request.node.get_marker('ext_suite_1'):	VultrProviderTests return ['API-Key'] @pytest.fixture(autouse=True) pytest.skip('Skipping extended suite')
# todo do something with temp </s> s.resample(temp=temp)	resample_states def resample_states(self,temp=None):
f.writable = false # @todo: currently this hides the widget from update forms instead of just rendering read-only! </s> query = (aitable.activity_id == activity_id) & \	req_need_status_update for activity in activities: activity_id = activity.id (aitable.deleted == False) & \ (aitable.item_pack_id == iptable.id)
# todo policyuniverse can't expand resource wildcards so further thought is needed here </s> on create set r.firstseen = timestamp()	load_policies WITH pnode MATCH (aa:AWSAccount{id: {AWS_ACCOUNT_ID}}) SET r.lastupdated = {aws_update_tag} for policy in policies:
# todo tests for this </s> parsed_version = parse_version(version)	get_installable_version installed (CLI and node).
# todo: header fields might vary across file types, thus prior sensing would be needed </s> temp_input_1 = inputs[i]	diff_values diffs = [] for i in range(len(inputs)): temp_input_2 = inputs[i+1] if np.any(temp_input_1[key] != temp_input_2[key]):
# todo: check the data! </s> pipe = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def)	test_csv def test_csv(self): pipe_file = 'pipe_UuvYtuMe3hGDsmRgPm7D0g.json' count = 0 for i in pipe:
# todo verbosity was mistakenly removed from task_parameters on release 0.11.0, need to bring it back </s> raise valueerror("the parent of an agent must have a name")	parent self._parent = val if self._parent is not None: self.full_name_id = self.ap.full_name_id = "{}/{}".format(self._parent.name, self.name)
# todo check if lus can be more than one token </s> processed[sentence_id]['sentence'] = sentence	read_full_results sentence_id = row['id'] sentence = h.unescape(row['sentence'].decode('utf-8')) processed[sentence_id]['frame'] = row['frame'] processed[sentence_id]['lu'] = row['lu']
# todo: error detection </s> mongodb = connection('localhost', 27017)['cobbler']	__connect def __connect():
# todo: i should make sure to escape single quotes here </s> self.send(key, 'null', decode_fn=lambda x: x, safe=safe)	send_null
# todo: check that the performance measure is within some range </s> def test_grid1(self):	TestBaselines def test_grid0(self): Tests flow/benchmark/baselines/grid0.py Tests flow/benchmark/baselines/grid1.py grid1_baseline(num_runs=1, sumo_binary="sumo")
# todo assert cls.__tablename__ == '' </s> s = model.galaxysession()	galaxy_session def galaxy_session(model, session, user):
except:  # todo: do not use bare except </s> return bytesio()	create_payload_buffer
# todo(brett.cannon) implement </s> return []	mock_implicit_hooks
# todo(b/157460932): migrate to glaziererror </s> terminator.log_and_exit('failed to build the task list',	AutoBuild b = builder.ConfigBuilder(self._build_info) b.Start(out_file=task_list, in_path=root_path) self._build_info, 4302, e) try:
# todo: test permissions, non-writable fields </s> 'url': node.display_absolute_url,	test_csl_single_author 'given': node.creator.given_name, 'family': node.creator.family_name, 'issued': datetime_to_csl(node.logs.latest().date), 'title': node.title,
# todo check .item() migration </s> def attention(attention, attention_feat, query, v, w):	attention sum_ = attention_feat + torch.mm(query, w) score = F.softmax(torch.mm(F.tanh(sum_), v.unsqueeze(1)).t(), dim=-1)
# todo: implement "n-v.a" form and fix this test </s> self.assertequal(len(result), 0)	ModuleSubjectTest def test_nsv(self): subj = ModuleSubject(MODULE_NSVA) def test_nsap(self): subj = ModuleSubject(MODULE_NSAP)
# todo: we really need a "builtin" name ref node to avoid creating a </s> node.getsourcereference(),	FixupNewStaticMethodVisitor new_node.parent = node self.signalChange( "Added missing staticmethod decoration to __new__ method"
# todo: write tests </s> "when there is an otherwise-unspecified validity error that prevents parsing."	MeiValidityError pass
# todo test </s> def __str__(self):	StateUnreachableError self.message = message
# todo: the status should be infeasible here, i think </s> def available(self):	available
# todo: support more than just lists </s> self.visit(node.body)	LocalExtractor def visit_For(self, node): self.visit_in_assign(node.target) self.visit(node.orelse) def visit_withitem(self, node):
# todo: handle case where entity has never been assigned components </s> def __init__(self):	ComponentE self.items = {"itema": None, "itemb": 1000} self.points = [a + 2 for a in list(range(44))]
# todo: look this up in one query </s> playlist_mbid: the mbid of the playlist to delete	delete_playlist_by_mbid def delete_playlist_by_mbid(playlist_mbid: str): Returns: True if the playlist was deleted, False if no such playlist with the given mbid exists
# todo: add input option for sample_size </s> **input_options,	schema samples=samples, import_fields=import_fields, ) export_fields = _get_export_fields(table.field_names, fields_exclude)
# todo(nakago): check why tolerance is high </s> return nfp(out_dim=out_dim)	model @pytest.fixture
# todo: activate this code if we have limits at webmail level </s> if not self.limiter.hit(self.rate,"client-ip",clientip):	Limiter if not self.limiter.test(self.rate,"client-ip",clientip): raise RateLimitExceeded() raise RateLimitExceeded() def reset(self,clientip):
pass  # todo </s> sys.stdout.seek(0)	test_failure_message_success failureMessage('fail')
# :todo: implement test. </s> def test_pass_tags_only(self):	FindTransactionsRequestFilterTestCase self.skipTest('Not implemented yet.') def test_pass_addresses_only(self): self.skipTest('Not implemented yet.') def test_pass_approvees_only(self):
# todo uncomment the actual test below after we have implemented the l1 attack </s> model_fn=self.model, x=self.normalized_x, eps=1.,	test_attack_strength def test_attack_strength(self): eps_iter=.05, ord=np.inf, clip_min=.5, clip_max=.7, nb_iter=5, sanity_checks=False)
# todo: inputs and outputs could be pretty long. these may be worth </s> command = join_cmdline(command)	normalize_command else: if command and command[0] == "--": else: command = ensure_unicode(command)
# todo: warn </s> def png_handler(file_like):	png_handler @register_format('image/png')
# todo(jd) move into prepare_service gettextutils and eventlet? </s> self.keystone = ksclient.client(	AgentManager def create_polling_task(self): return PollingTask(self) username=cfg.CONF.service_credentials.os_username, password=cfg.CONF.service_credentials.os_password,
# todo: can we not reestablish the connection earlier? </s> container = container_factory(fooservice, rabbit_config)	test_proxy_remote_error container.start() with ServiceRpcProxy("foobar", rabbit_config) as proxy:
ioloop.current().close()  # never reached. todo: clean shutdown of ioloop </s> logging.getlogger("tornado").setlevel(logging.critical)	run_center@17 from distributed import Center from tornado.ioloop import IOLoop center = Center('127.0.0.1', port) center.listen(port)
# hack to support saving/loading pytorch models. todo: improve </s> if hasattr(layer._mem._mem, 'get'):	Model layer.ops = NumpyOps() layer.Ops = NumpyOps layer._mem._mem = layer._mem._mem.get() layer._mem.ops = layer.ops
#todo: mock the socket! </s> delta = datetime.utcnow() - session['last_activity']	Telnet_Tests self.assertTrue(len(str(session['id'])) > 20) delta = datetime.utcnow() - session['timestamp'] self.assertTrue(delta.seconds < 2) self.assertTrue(session['attacker_ip'] == '192.168.1.200')
# todo: handle boolean overrides </s> @click.option('--federated-only', '-f', help="connect only to federated nodes", is_flag=true)	alice@50 @click.option('--teacher-uri', help="An Ursula URI to start learning from (seednode)", type=click.STRING) @click.option('--min-stake', help="The minimum stake the teacher must have to be a teacher", type=click.INT, default=0) @click.option('--network', help="Network Domain Name", type=click.STRING) @click.option('--config-root', help="Custom configuration directory", type=click.Path())
# todo: hand derive. current value is just a canary to detect changes. </s> def initialize(self):	BuyAndHoldAlgorithm class BuyAndHoldAlgorithm(TradingAlgorithm): self.holding = False def handle_data(self, data):
# todo: kickoff syncing process with this peer </s> def listen_maddr_with_peer_id(self) -> multiaddr:	listen_maddr_with_peer_id @property
# todo: consider a case where len(url_info_list) > 1. </s> )	_create_buffer_tokens BufferToken(raw_buffers[i], i)
# store the reason into entry, todo: plugin in the future? </s> for entry in self.entries:	verbose_details_entries if self.manager.options.details:
# todo(dcramer): we want to be less aggressive on disabling domains </s> class cannotfetch(badsource):	CannotFetch
# todo(py27): python versions < 3.3 do not support this syntax. </s> log.debug('inserting none')	BackgroundGenerator def run(self): for item in self.generator: self.queue.append(None) def _insert_item(self, item):
# todo: discuss api </s> assert study_id == 0	set_study_param_distribution def set_study_param_distribution(self, study_id, param_name, distribution):
# todo debug </s> def __init__(self):	RuleElement self.type = None self.triggered = False
# todo(ultrotter): import/export still to be converted to os api 10 </s> return true	AddNode utils.WriteFile(name, data=content, mode=0600) utils.AddAuthorizedKey(auth_keys, sshpub)
# todo extend to nonbinary nodes </s> data_type = a.dtype if r is 0 else np.dtype([('', a.dtype)]*r)	combs if r is 0: return np.asarray([]) b = np.fromiter(combinations(a, r), data_type) return b.view(a.dtype).reshape(-1, r)
# todo: update the entity with program_data? </s> return none	FormRepeaterDhis2EventPayloadGenerator event = dhis2_api.form_to_event(risk_id, form, RISK_ASSESSMENT_EVENT_FIELDS, case['external_id']) return json.dumps(event, default=json_serializer) if event else None
raise skiptest  #todo: figure out why this randomly started failing. </s> response = self.client.get(reverse('search'), {'q': 'audio', 'w': 3})	test_content eq_('text/html; charset=utf-8', response['Content-Type']) eq_(200, response.status_code)
# todo add arch arm/aarch/mips/mips64/sparc/sparc64 </s> raw_data = f.read()	disassemble_file with open(fpath, 'rb') as f:
#todo - return self? </s> this allows dealing with a position like 123^456. this	BetweenPosition o position - The default integer position o left - The start (left) position of the boundary indicates that the start of the sequence is somewhere between 123 and 456. It is up to the parser to set the position argument
# todo all this for now, until someone fixes the codegen. </s> yield issue_str_line("extra space", filename, line, num, match.start(1))	find_issues_with_lines yield issue_str_line("Missing space", filename, line, num, match.start(1) + match.start(2)) match = EXTRA_SPACES_RE.search(line);
# todo: refactor to remove all references to "set_detection_mapping_mode" from codebase </s> g_pool.gui.update_button(button, action, mods)	on_window_mouse_button
# todo: align series </s> else:	GeoSeries if isinstance(other, GeoSeries): return Series([s[0].contains(s[1]) for s in zip(self, other)], return Series([s.contains(other) for s in self], index=self.index)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	GetInclusionStatesRequestFilterTestCase def test_fail_tips_wrong_type(self): self.skipTest('Not implemented yet.') def test_fail_tips_contents_invalid(self): self.skipTest('Not implemented yet.')
# todo generator </s> continue	Drop content = [ap['path'] for ap in content_by_ds[ds_path] if ap.get('type', None) != 'dataset' or ap['path'] == ds.path] for r in _drop_files(ds, content, check=check, **res_kwargs): yield r
# todo: give a vanilla example </s> \\frac{\\sum_n \\hat{y}}{\\sum_{n,t} \\hat{y}}	feca@26 fraction = \\sum_n min \\left ( \\right ) Attributes
# todo: instead of discarding pending jobs, maintain them </s> sock.listen(32)	scheduler_server addrinfo.ip, self.scheduler_port) raise StopIteration while 1: conn, addr = yield sock.accept()
# todo: then observer.events[0] == trial </s> self.events: list[trial] = []	SomeObserver class SomeObserver(_Observer[Tuple[HyperparamsRepository, Trial]]): def on_next(self, value: Tuple[HyperparamsRepository, Trial]): self.events.append(value)
f.write(self.pastie_content.encode('utf8'))  # todo error checking </s> return random.choice(yamlconfig['user-agent']['list'])	getRandomUserAgent if yamlconfig['user-agent']['random'] and yamlconfig['user-agent']['list']:
# todo: allow partial prase complete </s> def prase_input_complete(text):	prase_input_complete
# todo add locales </s> check the current main domain, or change it	domain_main_domain @is_unit_operation() Keyword argument: new_main_domain -- The new domain to be set as the main domain
# todo: allow multiple callbacks for each hotkey without overwriting the </s> state.remove_last_step = remove	set_index state.suppressed_events.append(event) return False return False
# todo: use pabot options here </s> def difference(self, from_items):	DynamicTestItem variables = options.get('variable', [])[:] variables.append("DYNAMICTEST:"+self.name) return [] def contains(self, other):
# todo(b/123883319) convert to tf.function. </s> optimizer=none)	testInitializeRestoreAgent self._time_step_spec, self._action_spec, observations = [tf.constant([[1, 2], [3, 4]], dtype=tf.float32)] time_steps = ts.restart(observations, batch_size=2)
# todo: this is a hack to make a rule know </s> return any(	_prev_action_listen_in_state @staticmethod PREV_PREFIX + ACTION_LISTEN_NAME in state_name and prob > 0 for state_name, prob in state.items()
return deserialize(self.entity_bin, from_bytes=true)  # todo: techdebt fix </s> __tablename__ = "entity"	Entity@25 name = Column(String(255), primary_key=True) entity_bin = Column(LargeBinary(3072))
# todo: not implemented yet </s> power_of_two = 2.0**(msb_power-ii)	csd2dec msb_power = len(csd_str)-1 # dec_val = 0.0 if csd_str[ii] == '+' : dec_val += power_of_two
# todo: exp_block_pairs </s> self.assertequal(1, p._pos)	testFieldExp p = get_parser('foo') node = p._field() self.assertTrue(isinstance(node, parser.FieldExp)) self.assertEqual('foo', node.exp.value.name._data)
# @todo: call onaccept if this starts doing anything other than just setting 'master' </s> s3.stylesheets.append("s3/dc_results.css")	html "title": title, } scripts_append = s3.scripts.append if s3.debug:
# todo: create an asynchronous celery task for this </s> event.ical_url = none	clear_export_urls event.xcal_url = None event.pentabarf_url = None
if ursula_accepts:  # todo: read the negotiation results from rest </s> delta = expiration - maya.now()	BlockchainPolicy candidate_ursulas = random.sample(selected_ursulas, sample_quantity) accepted, rejected = set(), set() hours = (delta.total_seconds()/60) / 60 periods = int(math.ceil(hours/int(constants.HOURS_PER_PERIOD)))
# forward all other methods. todo(l.zou): could use a proxy to automate these </s> return self._optimizer.apply_gradients(*args, **kwargs)	apply_gradients
# todo: fix string formatting to match python/pandas </s> for i in numba.parfor.internal_prange(len(a)):	_column_fillna_impl s = B[i] if np.isnan(s):
# todo: we don't support assigning permissions on key value pairs yet </s> elif resource_type == resourcetype.sensor:	get_resolver_for_resource_type :rtype: :class:`PermissionsResolver` if resource_type == ResourceType.PACK: return SensorPermissionsResolver elif resource_type == ResourceType.ACTION:
#todo: make this actually read all </s> return false	tryConnect return False if len(rlist) == 0: try: w, addr = l.accept()
# todo explicit inputs to nodes (right now each node is implicitly </s> self.nodes = nodes	Purview class Purview: def __init__(self, nodes): def max_entropy_distribution(self): Get the maximum entropy distribution over this purview (this is
# todo: remove anytime in 2016 </s> if map_active:	FormsByApplicationFilter map_active.append(app) if (bool(map_deleted) + bool(map_active)) > 1: final_map.append( self._map_structure(PARAM_VALUE_STATUS_ACTIVE, _('Active CommCare Applications'), map_active)
# todo: what other exceptions might be returned? </s> return sorted(list(set(self.articles(recurse))))	articlesList def articlesList(self, recurse=False):
self.binary = serialize(value, to_bytes=true)  # todo: techdebt fix </s> def object(self, value: any) -> none:	object@33 @object.setter
# todo(unno): too large and too small axis is deprecated in numpy 1.13 </s> .. seealso:: :func:`numpy.broadcast_to`	broadcast_to shape (tuple of int): The shape of the desired array. Returns: return core.broadcast_to(array, shape)
# todo this should be a return and printed elsewhere </s> feature_importances = [tree.feature_importances_ for tree in best_rf.estimators_]	plot_random_forest_feature_importance print(type(trained_rf_classifier)) raise HealthcareAIError('Feature plotting only works with a scikit learn RandomForestClassifier.') standard_deviations = np.std(feature_importances, axis=0) indices = np.argsort(importances)[::-1]
# todo: does not find everything it should when contentproxy content </s> map = self._translation_map()	_to_inventory inventory = dict((region, [ (pk, map[ct]) for pk, ct in items
# todo(rosmaita): bug #1745003 </s> def _pre_upgrade_ocata_expand01(self, engine):	_pre_upgrade_ocata_expand01
# todo fetch a real object </s> try:	looks_like_prod_code int(code) return False
# todo: skipped due to gh-4436 </s> assert_raises(valueerror, ds.create_sibling_ria, 'ria+file:///some/where',	test_invalid_calls assert_raises(TypeError, ds.create_sibling_ria)
# todo: may be check whether it fits to tracking branch </s> return self.repo.active_branch.name	git_get_active_branch
# drl_todo: this is the opposite of what i would expect </s> schema_fields = {	build_schema 'id': ID(stored=True, unique=True), 'django_ct_s': ID(stored=True),
## todo : log error </s> try:	stringify_listvars @param mylist: A list/tuple of values, or a list/tuple of value list/tuples. @return: a tuple of string values or a tuple of string value tuples if type(mylist[0]) in (tuple,list): for row in mylist:
# todo: exc_info. </s> self.started = false	rewind self.delegate.rewind()
# todo: check error location </s> 'test': graphqlfield(test_type),	get_fields def get_fields(self): 'nest': GraphQLField(DataType(), resolver=lambda *_: Data())
# todo: handle marker? </s> returns a dictionary of interesting properties.	describe_alias def describe_alias(functionname, name, region=None, key=None, keyid=None, profile=None): CLI Example: .. code-block:: bash
# todo(kenta oono) </s> return ret[0], ret[-1]	precision def precision(y, t, label_num=None):
#   todo: doing this for the last len(x) terms should be enough </s> set used during initialization	update_filter ---------- b : array-like Returns -------
# todo remove? </s> module = set([d.get_parent_until() for d in definitions])	usages if not isinstance(user_stmt, pr.Import): definitions = usages.usages_add_import_modules(self._evaluator, module.add(self._parser.module()) names = usages.usages(self._evaluator, definitions, module)
# todo: compute the average cost of these feature relative to hash feature </s> return	optimize_statement optimize_statement(statement.child) return
if self._ndim == 3: # todo: use hasz </s> d = c_double()	getX def getX(self): lgeos.GEOSCoordSeq_getX(cs, 0, byref(d)) return d.value
# todo switch to transform </s> return "<{} buffereddatasetwriter name='{}' mode='{}'>".format(	BufferedDatasetWriter This allows incremental updates to datasets using formats that don't otherwise support updates, such as JPEG. self.closed and 'closed' or 'open', self.name, self.mode)
# todo: direct use of json['error'] is deprecated; use display_message('...', 'error') instead. </s> j_dic['triggers'] = []	enrich_json_with_base j_dic['offset'] = 0 j_dic['entities'] = [] j_dic['modifications'] = [] j_dic['equivs'] = []
# todo: make this cache preparation configurable </s> text = re.sub(r'>\s{2,}<', space_between_tags, text)	_carefully_strip_whitespace def _carefully_strip_whitespace(text): return text
# todo: httpok is only handled by the httpexceptions </s> self.setheader('www-authenticate', 'basic realm="%s"' % realm, 1)	_unauthorized self.setStatus(401) realm = self.realm
# todo: instead of arrays, vbos should be used here, as a large part of </s> return self.__title	title
#todo: this is just for backwards compatibility. it should be removed in v0.98 with p2.6 </s> return 0	ExtraIndicoFilter class ExtraIndicoFilter(logging.Filter): def filter(self, record): return 1
# todo change to test models separated from ralph </s> result_perm_key = 'change_category_code_field'	test_get_perm_key perm_key = get_perm_key('change', 'category', 'code')
# todo(b/182316162): unify publisher handing so that post-execution artifact </s> finally:	TaskManager if task is None: continue if self._process_all_queued_tasks_before_exit: while True:
'scoped': false,  # todo </s> def all(self, user=none, authority=none):	ProfileGroupService self.session = session self.route_url = route_url open_groups = self.open_group_finder(authority) or [] private_groups = []
# todo: serialize properly </s> def put(self, id):	RepoResource serialized = http_error_obj(404) return self.not_found(serialized) parameters = self.params() delta = parameters.get('delta', None)
# todo: for now we dependend on the timestamp to be </s> def parsebundle(self, filehandles):	ParseBundle Args: filehandles: A list of open file like objects.
# todo: read the email sender and recipient from configs. </s> email = mail.mail(	EmailUtil email_content: String of the email content (or body). Returns: mail.Email(self.email_sender), email_subject,
# todo: implement textures properly </s> for material in defs.defaultmaterials:	createPhobosMaterials def createPhobosMaterials(): mat = defs.defaultmaterials[material] if not material in materials:
""" todo: documentation </s> streamable search commands. see the `streaming` configuration	GeneratingCommand def local(self): head setting. Default: False
# todo: change to appropriate 'clone' method </s> self._split_criterion = split_criterion	split_criterion format(split_criterion, 'vr')) self._split_criterion = 'vr'
# todo: message depending on narrowing/float-conversion </s> yield y	gen for j in range(*i.indices(len(self))): self.filehandle.gettr(x, j, 1, 1)
# todo remove these in enaml version 0.8.0 </s> raise notimplementederror	set_icon
# todo: if py3k, override unpickler.find_class(). </s> self.update(d)	MtimeDB if k not in mtimedbkeys: writemsg(_("Deleting invalid mtimedb key: %s\n") % str(k)) self._clean_data = copy.deepcopy(d) def commit(self):
# todo: update once lakshmi's pr is merged </s> parent_python_path = os.environ.get('pythonpath', '')	get_sandbox_python_path running inside virtual environment. :type inherit_parent_virtualenv: ``str`` parent_python_path = parent_python_path.split(':') parent_python_path = [path for path in parent_python_path if path]
# todo: sinpi </s> return f_real(*(float(x) for x in args))	_mathfun_n def _mathfun_n(f_real, f_complex): def f(*args, **kwargs): except (TypeError, ValueError): return f_complex(*(complex(x) for x in args))
# todo: add at least reflection tests before adding notimplemented version </s> - uses negative indexes, like ``playlistinfo "-1"``, to request	playlistinfo argument is given, displays information only for the song ``SONGPOS`` or the range of songs ``START:END``. the entire playlist if parameter is None or parameter == '-1':
@unittest.skip('not written')  # todo: finish! </s> @py3_only	test_SimpleNamespace def test_SimpleNamespace(self): raise NotImplementedError
# todo: progress +kwargs </s> return self.repo.active_branch.name	git_get_active_branch
# todo: different codec to be used </s> raise(asn1objerr('{0}: invalid value, {1!r}'.format(self.fullname(), val)))	_safechk_val def _safechk_val(self, val): if not isinstance(val, tuple) or len(val) != 8 \
# todo: the stuff </s> def plugin(self):	SimpleTaskPersistence self.task = task self.taskname = task.name return self.task.current_plugin
# todo: implement </s> push_notification('item:unlock', item=str(filter.get('_id')), user=str(user))	unlock updates = {LOCK_USER: None, LOCK_SESSION: None, 'lock_time': None, 'force_unlock': True} item_model.update(filter, updates) item = item_model.find_one(filter) return item
pass # todo </s> def on_scroll_on_area(self, area, event):	on_scroll_on_area
#todo: test size=var, with shape that change from call to call </s> gsample2 = g()	test_deterministic u2 = R2.uniform(size=sample_size) g = theano.function([], u2) assert numpy.allclose(fsample1, gsample1) assert numpy.allclose(fsample2, gsample2)
# todo - this isn't actually the correct way to set the vary header, </s> raise errorresponse(status.http_405_method_not_allowed,	http_method_not_allowed def http_method_not_allowed(self, request, *args, **kwargs):
# todo: use unshare() here </s> exit_stack.callback(lambda:	mk_dm proc = subprocess.Popen(cmd, stdin=subprocess.PIPE) proc.communicate(table.encode('ascii')) quiet_call( 'dmsetup remove --'.split() + [devname]))
# todo(unno): numpy.matrix is used for scipy.sparse though </s> m = self.make(xp, sp, self.dtype)	test_A @testing.numpy_cupy_allclose(sp_name='sp') return m.A
# todo: check if this logic is sufficient </s> try:	eigen_tensor :arg index: a tuple of integers used to determine row and column information. This is provided by the SplitKernel row, col = index except ValueError:
#todo: implement check_equals </s> blocks = typeblocks.from_blocks(	set_index Return a new frame produced by setting the given column as the index, optionally removing that column from the new Frame. column_iloc = self._columns.loc_to_iloc(column) self._blocks._drop_blocks(column_key=column_iloc)) columns = self._columns._drop_iloc(column_iloc)
# todo(rbharath, enf): figure out why pi_stack is slow and cation_pi </s> contents,	load_pdbbind_labels else: contents.append(line.split()) columns=("PDB code", "resolution", "release year", "-logKd/Ki", "Kd/Ki", "ignore-this-field", "reference", "ligand name"))
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
# todo action required that updates the endpoint </s> eps.append(hash_endpoint)	_get_endpoints return eps hash_endpoint = self.sdnc.endpoint_by_hash(device) return eps ip_endpoints = self.sdnc.endpoints_by_ip(device)
# todo username </s> args=[	create_mon ], ) 'ceph-mon', '--cluster', cluster,
# todo: remove this function </s> get data for a single host in this inventory.	get_host_data return self.host_data.get(hostname, {})
#todo: can't initialize these values in the __init__? </s> push_socket.connect(addr)	connect_push_socket def connect_push_socket(self, addr): self.sockets.append(push_socket) self.out_socket = push_socket
# todo: this check is to maintain backwards compatibility with the old way of creating </s> return _lookup_plugin(package_type).form_to_db_schema()	_form_to_db_schema
# todo: this doesn't seem necessary; test passes without it </s> f.write('')	test_watch_ignore watcher.watch(tmpdir + '/*', ignore=lambda o: o.endswith('.ignore')) assert watcher.examine() == (None, None) assert watcher.examine() == (None, None)
# todo(pvp): this and other input_ids i tried for generation give pretty bad results. not sure why. model might just not be made for auto-regressive inference </s> model = tfxlmforquestionansweringsimple(config)	create_and_check_xlm_qa is_impossible_labels, input_mask, inputs = {"input_ids": input_ids, "lengths": input_lengths} start_logits, end_logits = model(inputs)
# todo: better error reporting </s> @type name: str	ConnectNode @param name: the node name @type address: str
# todo: replace with "yield from" when dropping python 2. </s> "live": true	ard_mediathek "playpath": info["_stream"], "pageUrl": self.url, } stream = RTMPStream(self.session, params)
# todo: use flask logger without it triggering the root </s> "%s %s %s\n[request object]: %s",	register_teardown_request @app.after_request def teardown(response): request.method, request.path,
# todo: refactor </s> module_lines = f.readlines()	_copy_module if 'ansible_python_interpreter' in host_variables: interpreter = host_variables['ansible_python_interpreter'] if '#!' and 'python' in module_lines[0]: module_lines[0] = "#!%s" % interpreter
# todo: refactor </s> self.current = v.__version__	initialize_options sys.stdout.write('Previous version:\033[33m {}\033[0m.\n'.format(self.current))
# todo: remove "get_" from the name </s> args:	edge_index def edge_index(self, edge_type): index: Tuple of (node1_type, edge_type, node2_type) Returns:
# domain=domain,  # todo: why isn't domain being saved on commcarecaseindexsql objects? </s> [form.form_id, form.problem, form.state]	update_form_problem_and_state with connection.cursor() as cursor: cursor.execute(
raise exceptions.mpdnotimplemented  # todo </s> underscore, dash, dot and colon.	subscribe@18 already. The name may consist of alphanumeric ASCII characters plus
# todo include results from results.albums(), etc. too </s> def __hash__(self):	__hash__
# todo: this is incomplete. </s> block_level_width(box, containing_block)	block_replaced_width def block_replaced_width(box, containing_block, device_size):
# todo implement. </s> def train(self, data):	Trainer self.master_model = keras_model.to_json() self.features_column = features_col raise NotImplementedError
# todo come up with a better error reporting mechanism so that we don't need this as a special case. </s> self._target_dependencies_by_address[address].update(target_adaptor.dependencies)	_index_target address = target_adaptor.address target = self._instantiate_target(target_adaptor) for dependency in target_adaptor.dependencies: self._target_dependees_by_address[dependency].add(address)
# :todo: implement test. </s> preparing a bundle with an auto-generated change address.	PrepareTransfersCommandTestCase def test_pass_change_address_auto_generated(self):
# todo only ubuntu 12.04 is supported right now </s> '-i',	lsb_release args = [ 'lsb_release', '-c', ]
# todo(amotoki): due to neutron bug 1378525, neutron disables </s> 'readonly'})	UpdateForm elif kwargs.get('initial', {}).get('mode') == 'distributed': mode_choices = [('distributed', _('Distributed'))] self.fields['mode'].choices = mode_choices else:
# todo:check the note in docstring. change that behavior to return the joint map </s> returns cliques used for belief propagation.	get_cliques def get_cliques(self):
# todo: remove when support for django 1.4 is dropped </s> def _postgisadapter_prepare(self, conn):	_PostGISAdapter_prepare
#todo handle partial datetime values </s> conf:	pipe_filter Keyword arguments: _INPUT -- source generator MODE -- filter mode, either "permit" or "block" COMBINE -- filter boolean combination, either "and" or "or"
#todo - once we drop support for python 2.3, this helper function can be </s> from bio import alignio	_iterate_via_AlignIO for align in AlignIO.parse(handle, format) : for record in align :
# todo: support domain-specific settings </s> return urllib.quote(text.encode('utf-8'))	clean_outgoing_sms_text try: return urllib.quote(text)
### todo: code this! </s> for attr in attrs:	_handle_input_tag_inside_form def _handle_input_tag_inside_form(self, tag, attrs): if attr[0].lower() == 'type' and attr[1].lower() == 'file': f.hasFileInput = True
# todo: speed up </s> return [vector[0], vector[1]]	WriteVector2D
# todo: clean up </s> )	_make_pubsubs raise ValueError( f"lenght of pubsub_routers={pubsub_routers} should be equaled to " return tuple( Pubsub(
# todo(yanase): remove number from system_attrs after adding trialmodel.number. </s> with storagesupplier(storage_mode) as storage:	test_get_study_id_from_name_and_get_study_name_from_id @pytest.mark.parametrize('storage_mode', STORAGE_MODES) function_name = test_get_study_id_from_name_and_get_study_name_from_id.__name__ study_name = function_name + '/' + storage_mode
# todo(nateh): fix for arch's multi-file net config </s> return hostname	_select_hostname def _select_hostname(self, hostname, fqdn): if not hostname:
# todo check error message </s> yield "/etc/yunohost/apps/%s/settings.yml" % app	app_expected_files def app_expected_files(domain, app): yield "/etc/nginx/conf.d/%s.d/%s.conf" % (domain, app) yield "/etc/yunohost/apps/%s/manifest.json" % app yield "/etc/yunohost/apps/%s/scripts/install" % app
# todo: handle multiple credentials (username/password pairs). </s> "type": "+"},	authorization_runtime_search def authorization_runtime_search(self, actor_id, actorstore_signature, callback): extra_requirement = [{"op": "actor_reqs_match", {"op": "current_node", "kwargs": {},
# todo: check what kind of exception raising if no location </s> except:	Profile try: q = _stats_source[1].attrs['data-count'] self.following_count = None try:
# todo: shouldn't this be none? </s> if isinstance(item, layer):	__isub__ self.remove_layer(item) elif isinstance(item, Control):
# todo: this scrolling is lame and centers text :/ </s> self.put(json.dumps({'name': 'msg', 'data': msg}))	send_msg def send_msg(self, msg):
# todo consolidate </s> v = int(v)	migrate_rc v = parser.get_token() or None if v is not None: except ValueError: if v.lower() in bools:
return none # todo </s> 'single: 0',	_status 'volume: 0', 'repeat: 0', 'consume: 0', 'playlist: 0',
# todo: improve this. </s> self.view.show(target)	ViBigL row -= count + 1 target = self.view.text_point(row - 1, 0)
# todo: verify the choice between failover and migration </s> def testinstanceremove(instance):	TestInstanceRemove AssertCommand(["gnt-instance", "remove", "-f", instance["name"]]) qa_config.ReleaseInstance(instance)
# todo: respond with "user has been added to the list?" </s> self.loop.create_task(self.send_message(entry.meta['channel'], 'now playing in %s: **%s**' % (	on_play entry.meta['author'].mention, entry.title, player.voice_client.channel.name ))) player.voice_client.channel.name, entry.title
# todo: remove this compatibility layer with rally 1.1.1 </s> self._meta_info = {	clear_meta_info def clear_meta_info(self): MetaInfoScope.cluster: {}, MetaInfoScope.node: {}
#todo: check if all selected objects are on visible layers (option bpy.ops.object.select_all()?) </s> def exportscenetosmurf(path):	exportSceneToSMURF
# todo: this here always returns empty response. if/when we want to </s> f'but expected tags [{expected_tags_str}]'	compare_account_data expected_set = set(expected_tags) if expected_tags else None msg = ( ) assert got_set == expected_set, msg
#todo: this is a horrible thing to do, we consume lots of memory </s> v['pass'] )	_reconfigureUrllib for v in kb.kb.getData( 'basicAuthBrute' , 'auth' ): self._w3af_core.uriOpener.settings.setBasicAuth( v.getURL(),
raise exception('lol') #todo fixme </s> self._listeners.append(l)	Settings def save(self): self._config_obj.write() def notify(self, name, old_value, new_value): for l in self._listeners:
# todo: add possible tags </s> "--no-metadata explicitly to hide this warning."	run_errands if self.parsed.output_file == "-" and self.parsed.no_metadata is False: logger.warn( ) self.parsed.no_metadata = True
# todo: consolidate these trivial group by dispatched funcs </s> strip_args = map(strip_symbolic, args)	singledispatch2 pipe_no_args(dispatch_func) @wraps(dispatch_func) strip_kwargs = {k: strip_symbolic(v) for k,v in kwargs.items()} if not args:
# todo: this is a very flaky assumption. find a better one. </s> if not utils.istext(structure.username):	PlsRecallParser current_timestamp = timelib.Timestamp.GetNow() if timestamp > current_timestamp + self._SIX_YEARS_IN_MICRO_SECONDS: return False
# todo: fails because of missing svg support </s> th { height: 4px }	test_image_repeat_block @page { size: 8px; margin: 0; background: #fff } table { border-collapse: collapse; margin: 2px } td { height: 2px } img { display: block }
# todo: handle overwrite case </s> result = discover(o)	discover_ssh def discover_ssh(data, **kwargs): with sample(data) as fn: return result
# todo(vish): move this into the driver layer </s> def terminate_instance(self, context, instance_id):	terminate_instance @defer.inlineCallbacks logging.debug("Got told to terminate instance %s", instance_id) instance_ref = db.instance_get(context, instance_id)
# todo: remove once multiple meanings in vocabulary are </s> self.stopwatch().unpause()	show_manage_card_types_dialog self.flush_sync_server() self.component_manager.current("manage_card_types_dialog")\
# todo: this class is incorrect: buildbot.slave.bot.slavebuilder </s> if self.stdiologname is not none and self.stdiologname in self.logs:	addHeader @util.deferredLocked('loglock') @defer.inlineCallbacks log_ = yield self._unwrap(self.logs[self.stdioLogName]) log_.addHeader(data)
# todo test for final </s> def canonical_dir(self, ignore_args = false):	canonical_dir
# todo why not just expect *only* the attribute value response, </s> self._backend.send_command(	bond if permanent: self._backend.set_bondable(True) CommandBuilder.sm_encrypt_start( self._handle, constants.bonding['create_bonding']))
# todo: need to add counter </s> return [user_ids]	_get_user_ids def _get_user_ids(self, user_ids): if isinstance(user_ids, str): return [self.convert_to_user_id(user) for user in user_ids]
# todo - use new error message api! ts </s> none	connectLayerListener Returns: None if qgisVersion() >= 10800:  # 1.8 or newer QgsMapLayerRegistry.instance().layersWillBeRemoved.connect(
# todo(solitude): remove this. </s> all_apps = request.amo_user.addons.filter(type=amo.addon_webapp)	activity_log def activity_log(request, userid):
raise skiptest("buggy")  # todo(mattjj): fix </s> x = onp.reshape(onp.arange(8., dtype=onp.float32), (2, 2, 2))	testTransposeAndAddRank3 def testTransposeAndAddRank3(self): def fun(x): expected = fun(x) ans = _parallelize(fun)(x)
# todo(guillermooo): generalize class so it can run any polymer command. </s> return target_path	get_target_path target_path = project.path_to_web if project.is_path_under(project.path_to_web, view.file_name()):
# :todo: implement test. </s> def test_fail_unexpected_parameters(self):	FindTransactionsRequestFilterTestCase self.skipTest('Not implemented yet.') def test_fail_all_parameters_empty(self): self.skipTest('Not implemented yet.') def test_fail_bundles_wrong_type(self):
# todo: this remembers every file user ever saw in nautilus. </s> object fails to (re)connect.	cb_syncthing_disconnected def cb_syncthing_disconnected(self, *a): Check if connection was already finished before and clears up stuff in that case.
# todo: we should store a storage version number in later releases. </s> note: this method only works for immutable graph index	asbits Parameters ----------
# todo: check that path combined with uri does not go above </s> def z_domain(self, chall):	DVSNIResponse return z.hexdigest()
# see optimization description comments and todo for tags in matching public histories query. </s> 'slug'     : p.page.slug,	_get_shared return [{'username' : p.page.user.username,
#todo: overly broad exception needs fixing </s> except pyelasticsearch.exceptions.elastichttpnotfounderror as e:	delete_all def delete_all(): try: logger.error(e) logger.error("The index 'website' was not deleted from elasticsearch")
# todo: what are these doing here? </s> ignore_types = type_info.get(case.type, {}).get("ignore_relationship_types", [])	get_children get_children(i.referenced_case, i.referenced_type, seen) for i in case.indices if i.referenced_id not in seen if referenced_type and referenced_type in ignore_types: return None
# todo: check if this is a windows symbol requirement, otherwise ignore it </s> :type context: ~volatility.framework.interfaces.context.contextinterface	KernelPDBScanner there is a fixed mapping between the physical and virtual addresses of the kernel.  On more recent versions a search is conducted for a structure that will identify the kernel's virtual offset. :param potential_kernels: Dictionary containing `GUID`, `age`, `pdb_name` and `mz_offset` keys :type potential_kernels: dict
singleton=false,  # todo: re-enable </s> returns the roster modules	roster return NewLazyLoader(_module_dirs(opts, 'roster', 'roster'), opts,
# todo: does it get closed properly after process gets killed? </s> raise connectionfailedexception(msg)	_resolve_executable msg = "Executable '%s' not found. Please check your configuration!" % executable if not executable.startswith("/"):
# todo: this should be abstracted into a property/method or something </s> if not hasattr(cls, 'template'):	_needs_templates def _needs_templates(cls):
# @todo: catch warnings </s> (('organizationalunitname', 'ssl'),),	test_get_common_name (('stateOrProvinceName', 'Delaware'),), (('localityName', 'Wilmington'),), (('commonName', 'somemachine.python.org'),))} self.assertEqual(self.httplib_object._get_common_name(cert)[0],
# todo: fix </s> from jesse.services import auth	logout_jesse_trade return authenticator.unauthorized_response()
# todo use a proper category instead </s> local(" ".join(args))	html "--toc"] + [i["file"] for i in CONFIG["MARKDOWN_FILES"]]
# todo(b/157460932): migrate to glaziererror </s> os.remove(location)	AutoBuild if not FLAGS.preserve_tasks and os.path.exists(location): logging.debug('Purging old task list.') except OSError as e: terminator.log_and_exit('Unable to remove task list', self._build_info,
pass # todo </s> def collect_incoming_data(self, data):	collect_incoming_data
#todo: should raise an exception or warning </s> def get_textbounds(self):	get_textbounds
# todo does not work after multiprocessing branch merge </s> *musicpd.org, reflection section:*	_reflection_commands @handle_pattern(r'^commands$') ``commands`` Shows which commands the current user has access to.
# todo(b/182316162): unify publisher handing so that post-execution artifact </s> def _process_exec_node_task(self, scheduler: ts.taskscheduler,	TaskManager else: scheduler.cancel() task: task_lib.ExecNodeTask) -> None: try:
# todo: optimise this to move some of the work to the workers. </s> to_key = int(config.from_key)	get_pagination_rows @defer.inlineCallbacks if config.to_key: from_key = int(config.to_key)
# todo implement this </s> exception in telnet connection	DroidBotAppConnException pass
# todo is_compiled_with_cuda() has not been moved </s> cuda_home = '/usr/local/cuda'	_find_cuda_home else: cuda_home = cuda_homes[0] if not os.path.exists(cuda_home): cuda_home = None
# todo: look at this </s> n.set_nameserver(self.servername)	NMBTests print resp def test_gethostbyname(self): resp = n.gethostbyname(self.serverName, nmb.TYPE_SERVER) print resp.entries
# (todo) chagne the dgl link </s> return split_dict	get_idx_split def get_idx_split(self):
# todo: create a hard bounce receipt rule in ses </s> payload_size = len(payload)	_get_attachment fn = part.get_filename() ct = part.get_content_type() if fn: extension = os.path.splitext(fn)[1]
# todo: finish this </s> print 'colores:', col1, col2, s	hex2dec def hex2dec(s): return int('0x' + s, 16)
#! todo: this is currently limited to siso systems </s> def __neg__(self):	__neg__
# resume normal sphinx.ext.autodoc operation </s> return super(functiondocumenter, self).format_name()	SaltFunctionDocumenter if len(self.objpath) > 1:
# todo: rewrite this simpler, we're using less than written </s> self['nscount'] = 0	NAME_REFRESH_REQUEST self['FLAGS'] = OPCODE_REFRESH | 0x1 self['QDCOUNT'] = 1 self['ARCOUNT'] = 1
# todo add help_text and label when they are available </s> model = twofieldmodel	DynamicSerializer class Meta:
# todo get fileid from event </s> def downloadfilestart(fileid):	downloadFileStart
# todo: decide what we consider to be a trending pack, for now we just take the last 9 that were updated </s> trending_sound_ids = list(download.objects.order_by('-created').values_list('sound_id', flat=true)[0:9])	Command popular_searches = ['wind', 'music', 'footsteps', 'woosh', 'explosion', 'scream', 'click', 'whoosh', 'piano', 'swoosh', 'rain', 'fire'] cache.set("trending_sound_ids", trending_sound_ids,  cache_time)
"""todo: to be implemented""" </s> 'password',	getBindParams def getBindParams(self): bind_keys = [ 'system_type', 'interface_version',
# todo: consider revising to use new disk.target_name property. </s> compression = 'no'	_validate_compression def _validate_compression(request): compression = request.data.get('compression', 'no') if (compression not in settings.COMPRESSION_TYPES): e_msg = ('Unsupported compression algorithm(%s). Use one of '
''' todo: change conditional to return on non-http responses </s> results = parser.parse_args()	checkArgs parser.add_argument('warcPath', help="Path to a WARC[.gz] file")
# todo(yanase): remove number from system_attrs after adding trialmodel.number. </s> study_name = function_name + '/' + storage_mode	test_get_study_id_from_name_and_get_study_name_from_id def test_get_study_id_from_name_and_get_study_name_from_id(storage_mode): with StorageSupplier(storage_mode) as storage: storage = optuna.storages.get_storage(storage) study = optuna.create_study(storage=storage, study_name=study_name)
#todo: the static files should not run everything on __init__ </s> for p in required_subfolders:	verify_path required_subfolders = ['pages', 'static', 'templates', 'plugins'] if self.locale is not None: if not os.path.isdir(os.path.join(self.path, p)): logger.info('This does not look like a (complete) cactus project (missing "%s" subfolder)', p)
# todo project_id = 'your google cloud project id' </s> from google.cloud.asset_v1beta1.proto import asset_service_pb2	export_assets@26 def export_assets(project_id, dump_file_path): client = asset_v1beta1.AssetServiceClient() parent = client.project_path(project_id)
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
pass  # todo </s> def delete(self, uri):	SpotifyPlaylistsProvider self._backend = backend def create(self, name): pass  # TODO def lookup(self, uri):
# todo: only decrypt metadata if header is present </s> if local_buckets[name]:	LocalBotoConn raise bex.S3ResponseError(404, 'Bucket does not exist') def delete_bucket(self, name): raise RuntimeError('Attempted to delete nonempty bucket') del local_buckets[name]
# todo: solution is not really elegant. should find </s> updates = super(weightdecay, self).init_layer_update(layer)	WeightDecay :network:`WeightElimination` decay_rate = NonNegativeNumberProperty(default=0.1) modified_updates = [] step = layer.step or self.variables.step
extruder_stack.userchanges.setproperty(key, "value", new_value)  # todo: nested property access, should be improved </s> def activequalitydefinitionid(self) -> str:	activeQualityDefinitionId if self._global_container_stack: return self.getQualityDefinitionId(self._global_container_stack.getBottom())
# todo hard-disable tests for now, since rapidcheck not in spack </s> conflicts("%clang@:3.5")	Krims description="Compile examples") conflicts("%intel", msg="krims only builds with gcc and clang") depends_on("cmake@3:", type="build")
# todo implement callback </s> def is_file_full(self, current_file_size):	is_file_full
# todo: this only works on local files !!! </s> filename='%s_%s_json.zip' % (name, ty)	download_name def download_name(self, app, ty): super(JsonExporter, self).download_name(app, ty) return filename
# todo: duplicate checking </s> class vpplicensecursor(vppcursor):	VPPLicenseCursor
# todo check if we can avoid that </s> cells["quad"] = out - 1	read_buffer@94 cells["triangle"] = out - 1 if nquad > 0 : if ntria > 0 : out = numpy.fromfile( f, count=ntria , dtype=int, sep=" ")
# @todo: the values must be in cgs already right? </s> f["grid_particle_count"] = pf.h.grid_particle_count	write_to_gdf@121 f["grid_left_index"] = pf.h.grid_left_edge f["grid_level"] = pf.h.grid_levels g = f.create_group("data") for grid in pf.h.grids:
output_zero_point = none # todo non-zero zero point </s> if not torch.allclose(self.zero_point, other.zero_point):	check_zero_points_same def check_zero_points_same(self, other): if self.training is not None and self.training: raise RuntimeError("Zero points are different")
# todo: before merge should discuss with syft core team on max time. </s> if ctr % 100 == 0:	SMPCExecutorService if store_object_self is None: ctr -= 1 print("Waiting for Object to arrive...🚀") else:
# todo: turn this into a runtime error </s> hasattr(x, "__module__")	is_plugin_module return (
# todo: we currently lack a reference family that passes this test! </s> print ("test fail with a bad font ({})...".format(fontfile))	test_id_101 status, message = list(test(font_meta, font_fnames))[-1] assert status == PASS status, message = list(test(font_meta, font_fnames))[-1] assert status == FAIL
file_object = open(file_path, mode='r') #todo add  encoding='utf8' for version python3 </s> :param x_list:e.g. [1,10,3,5,...]	pad_truncate_list :return:result_list:a new list,length is maxlen result_list=[0 for i in range(maxlen)] #[0,0,..,0]
# todo: read this parameter from the command line by implementing make_cmdline_parser and parse_known_cmdline_args! </s> def getselectedexportsourcename(self):	TrackingBaseDataExportApplet self._gui.set_exporting_operator(self.export_op) self._gui.set_default_export_filename(self._default_export_filename) if self._gui is not None: return self._gui.selectedExportSource
#@todo: move this and other methods out of this file, into a general </s> return lcolor	_light_color lcolor[0] = 0.5 * (1.0 + color[0]) lcolor[1] = 0.5 * (1.0 + color[1])
# todo ensure that if you try to filter on an invalid field, it returns a useful error. </s> return true	convert_value def convert_value(self, key, value): value = value.strip() elif value in self.FALSY: return False
# todo: return errors in a universal way </s> code_store = codestore()	compile_code token_list = lexer.tokenize(source) parser = Parser() ast_root.make_code(code_store) return code_store.full_code()
if lang is none:  # todo: remove in v8 </s> return path	_add_extension def _add_extension(self, path, extension):
# todo: force this somehow so that this isn't just a warning but </s> return kb.kb.getdata( 'urls', 'fuzzable_requests' )	get_fuzzable_requests_from_kb
# todo: not all values have exact matches in flexget, need to update flexget qualities </s> log.info("    tmdb id: %s" % entry["tmdb_id"])	CouchPotato log.info("    Title: %s" % entry["title"]) log.info("    URL: %s" % entry["url"]) log.info("    Quality: %s" % entry["quality"]) continue
# todo: only handle events that are new. </s> self.rtm_thread.start()	bootstrap def bootstrap(self): self.client = SlackClient(settings.SLACK_API_TOKEN)
# todo: skipped due to gh-4436 </s> ds.repo.call_git(['annex', 'move', 'one.txt', '--from', 'bare-git'])	_test_bare_git_version_2 eq_(len(ds.repo.whereis('one.txt')), 3) ds.drop('.') eq_(len(ds.repo.whereis('one.txt')), 2) ds.repo.call_git(['annex', 'move', 'one.txt', '--to', 'bare-git'])
# todo: if table_name is "2019" the final name will be "field_2019" - must </s> verify_ssl,	schema def schema( input_encoding, output_format, fields,
raise notimplementederror # todo </s> for s in self.states_list:	resample_states def resample_states(self):
# todo: delete </s> test_filetype_to_instance(".blf", can.blfwriter)	testLoggerTypeResolution with can.Logger(filename) as writer: self.assertIsInstance(writer, klass) test_filetype_to_instance(".csv", can.CSVWriter) test_filetype_to_instance(".db" , can.SqliteWriter)
# todo: does this import need to be delayed because </s> findrootnode().\	ExtensiveFormAlgorithm scenario.update_solution_from_instance() self._manager.scenario_tree.snapshotSolutionFromScenarios() computeExpectedNodeCost() if self.gap is not undefined:
pass  # todo... </s> return mod	_make_mod base_name=self.name, code_version=self.gen_base.code_version, code=self._make_code())
# todo this seems not to be very convenient... </s> if self.categ in defs.def_settings['controllers'][con]['categories']]	controllerlist def controllerlist(self, context): return items
# :todo: implement test. </s> but not implemented yet.	test_error_poll_interval_null def test_error_poll_interval_null(self): ``poll_interval`` is ``None``. with self.assertRaises(TypeError): SandboxAdapter('https://localhost', 'token', None)
pass # todo </s> if entry['billable']:	check_truncs self.make_entries(user=self.user2) entries = timepiece.Entry.objects.date_trunc(trunc) self.assertEqual(entry['hours'], billable) else:
# todo(harlowja): should we be a little more cautious about </s> self._logbook = self._catalog.create_or_fetch(self)	logbook @property def logbook(self): return self._logbook
# todo: assert </s> repo = self.remote.new_repo(self.token)	createRepo @pytest.fixture self.remote.modify_repo(repo, "name", "testrepo0", self.token) self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token)
#todo need gutter of scrollbar - how do we get that? </s> if self.keymask | x != self.keymask:	on_key_press_event def on_key_press_event(self, object, event): self.keymask |= x for l in self.linkmap[:self.num_panes-1]:
# todo: this should be made more flexibly to handle differeing params for xform submission </s> attachments[key] = item	get_instance_and_attachment@16 else: for key, item in request.FILES.items(): else: instance = request.raw_post_data
# todo: this currently looks only in current table; </s> bbox_vert_aligned_right(bbox_from_span(c[i]), bbox_from_span(c[0]))	is_vert_aligned_right return (all([c[i].is_visual() and
# todo(wentingli): create manifest from dependency jars later if needed </s> return true	_is_fat_jar_excluded if name.startswith(exclusion):
# todo(b/142684737): the multi-processing api might change. </s> metadata_path),	_create_pipeline@142 metadata_connection_config=metadata.sqlite_metadata_connection_config(
# todo: for now, circumnavigate the detached head issue. </s> list(source.repo.git_get_branch_commits("master")))	test_publish_simple@42 ok_clean_git(src_path, annex=False) ok_clean_git(dst_path, annex=False) eq_(list(target.git_get_branch_commits("git-annex")), list(source.repo.git_get_branch_commits("git-annex")))
# todo: --bytype </s> return x + x.join(hl * i for i in col_widths) + x	tr_hline
node.test = gast.call(gast.attribute( # todo any over dim 0 </s> self.generic_visit(node)	FuseAttributes class FuseAttributes(gast.NodeTransformer): if not isinstance(node.value, gast.Name): return node
# todo: slow </s> return [vector[0], vector[1]]	WriteVector2D
# todo: raise a specific exception for invalid separator characters </s> def parent(self, value):	parent @auto_save
# todo: smart profiling for parameter back-time (if it set up to 'auto') </s> if val is not none:	monitoring_data val = self.get_value(item)
# todo obtain this from entitlements </s> return index.blob	get_codesig_blob def get_codesig_blob(codesig_cons, magic): for index in codesig_cons.data.BlobIndex: raise KeyError(magic)
# todo -- can we do this without a subscription? </s> def play_library():	play_library if api.is_indexing(): return statement(render_template("indexing"))
# todo: fails for rsa256_key </s> from letsencrypt.crypto_util import make_ss_cert	test_it make_ss_cert(RSA512_KEY, ['example.com', 'www.example.com'])
# python3-todo: use yield from </s> for u in upset:	add_elements def add_elements(e): upset = frozenset(self.depth_first_search(e)) for m in self.neighbor_in_iterator(u): if (m not in upset and
# todo: is this a public attribute? </s> self.localworkerfactory = none	LocalWorker class LocalWorker(Worker, WorkerAPICompatMixin): def checkConfig(self, name, workdir=None, usePty=False, **kwargs): self._registerOldWorkerAttr("LocalWorkerFactory", pattern="BuildWorker") try:
# todo add list as an option </s> segments+1, intensity_image=image[..., c])	_replace_segments nb_channels = image.shape[2] for c in sm.xrange(nb_channels): for ridx, region in enumerate(regions): if replace_samples[ridx % len(replace_samples)] > 0.5:
# todo (a8): add user to models </s> return httpnotimplemented()	obj_delete
f.write(self.pastie_content.encode('utf8'))  # todo error checking </s> return random.choice(yamlconfig['user-agent']['list'])	getRandomUserAgent def getRandomUserAgent(): return None
# todo: cache this result so multiple failing calls don't keep hitting the db </s> def product_name(product_id):	product_name return Product.get(product_id).name
# todo: prefer to enumerate specific </s> rule=aws.s3.bucketserversideencryptionconfigurationruleargs(	sse_configuration def sse_configuration() -> aws.s3.BucketServerSideEncryptionConfigurationArgs: apply_server_side_encryption_by_default=aws.s3.BucketServerSideEncryptionConfigurationRuleApplyServerSideEncryptionByDefaultArgs( sse_algorithm="aws:kms",
raise notimplementederror #todo </s> class span(object):	Span
# todo: switch _ignore_connection_reset for _ignore_transmission_error, or provide retry mechanism </s> return 'finished'	SessionInfo return False @property
# todo: strip leading whitespace for ''' and r''' </s> elif id_ in (id.char_octal3, id.char_octal4):	EvalCStringToken return consts.LookupCharC(c) elif id_ == Id.Char_Stop:  # \c returns a special sentinel if id_ == Id.Char_Octal3:  # $'\377' (disallowed at parse time in Oil) s = value[1:]
# todo: support steps and times (motion blur) </s> def __init__(self, exported_obj, matrix):	Duplis self.exported_obj = exported_obj self.matrices = matrix
# todo(bowen): check </s> self.logp_ratio = 5	MoleculeEnv self.possible_bond_types = np.array(possible_bonds, dtype=object)  # dim self.max_atom = 38 + len(possible_atoms) # ZINC self.qed_ratio = 1 self.sa_ratio = -0.1
# todo: purge expired tokens </s> log.audit('token is not found in header.')	_validate_token@78 def _validate_token(self, env): raise exceptions.TokenNotProvidedError('Token is not provided.') token = Token.get(env['HTTP_X_AUTH_TOKEN'])
# @todo: move this to a popup behind an action button, to make it clearer that this isn't a maintained link </s> if not result:	custom_prep def custom_prep(r): if callable(standard_prep): return False method = r.method
# todo more through test </s> tests wether sparsedataset can be loaded and	test_iterator@9 initialize iterator x = np.ones((2, 3))
# todo: implement this! </s> om.out.debug('executing: ' + command )	_exec def _exec( self, command ): response = apply( self._execMethod, ( command ,)) om.out.debug('"' + command + '" returned: ' + response )
# todo: add at least reflection tests before adding notimplemented version </s> argument is given, displays information only for the song	playlistinfo *musicpd.org, current playlist section:* ``playlistinfo [[SONGPOS] | [START:END]]`` ``SONGPOS`` or the range of songs ``START:END``. *ncmpc and mpc:*
# todo(b/161952382): replace with keras premade models and </s> features.categorical_feature_max_values)	_build_keras_model num_buckets=num_buckets, default_value=0) for key, num_buckets in zip( ] indicator_column = [
# todo(phawkins): remove this after a jaxlib release. </s> def norm(x):	testQr for index in onp.ndindex(*shape[:-2]): nq[index], nr[index] = onp.linalg.qr(a[index], mode=mode) n = onp.linalg.norm(x, axis=(-2, -1)) return n / (max_rank * onp.finfo(dtype).eps)
# todo: implement </s> adaptation_schedule.append(adapt_window(0, num_steps - 1))	build_adaptation_schedule def build_adaptation_schedule(num_steps): adaptation_schedule = [] return adaptation_schedule
# todo use deepcopy() here </s> return len(angles) <= 1	_is_polygon_line vec = np.float32(p2) - np.float32(p1) angle = angle_between_vectors(vec_down, vec)
# todo: explicitly commit files by name </s> return exitcode == 0	is_available def is_available():
# todo add options to modify the columns </s> self.close()	do_quit def do_quit(self, arg): 'Stop recording and exit:  QUIT' return True
# todo: make sure loop index is not used for calculations in </s> array_dists[arg0] = new_dist	_analyze_call if ndim0==1 and ndim1==1: new_dist = Distribution(min(array_dists[arg0].value, array_dists[arg1] = new_dist return
pass ## fixme: todo </s> self.assertequal([a('a'), a('b')], list(x))	TestMiscUtils class A(str): pass ntuple = Something(1, 2, SomethingElse(A('a'), None, 2), [A('b'), 'c'], 5) def test_index_key(self): objects = [object() for _ in range(10)]
# todo merged db? </s> setup_logzero(logger)	main@15 logger = get_logger()
# todo: --csr could have a priority, when --domains is </s> for name, plugin_ep in plugins.iteritems():	add_plugin_args def add_plugin_args(self, plugins): Let each of the plugins add its own command line arguments, which parser_or_group = self.add_group(name, description=plugin_ep.description) plugin_ep.plugin_cls.inject_parser_options(parser_or_group, name)
# todo(chunla) move this to engine specific module </s> did_tps_drop_to_zero = any({x == 0 for x in tps_array})	_WaitForWorkloadToFail vm, _FAILOVER_TEST_TPS_FREQUENCY_SECONDS, benchmark_spec, sysbench_thread_count) did_all_succeed = retcode == 0 and not did_tps_drop_to_zero if did_all_succeed:
self.key_vb   = f.tanh(self.hid_2_key(hidden_vb)).view(-1, self.num_heads, self.mem_wid)    # todo: relu to bias the memory to store positive values ??? check again </s> raise notimplementederror("not implemented in base calss")	_update_usage
# todo / fixme : here we are ignoring error messages ... </s> if self.is_vulnerable_to_meltdown():	SecurityDiagnoser dependencies = [] def run(self): yield dict(meta={"test": "meltdown"}, status="ERROR",
# todo isoformat? </s> title = pr['title']	_get_summary pr = pl['pull_request'] action = pl['action'] return f"{action} PR {title}", link elif tp == "IssuesEvent":
# todo: need to test this logic </s> sub_d = as_tuple(sub_d)	domain_args sub_d = self.sub_domain if isinstance(sub_d, str): sub_d = [as_tuple(i) for i in sub_d] ndim = self.function_space().mesh()._plex.getDimension()
logging.info(f'running command\n`{cmd}`') #todo: consider with ilya </s> parser.add_argument('--do-not-load-snapshots', action='store_true')	parse_args def parse_args(): parser = argparse.ArgumentParser() parser.add_argument('--templates-pattern', default='**/template.yaml') return parser.parse_args()
# todo: arrange </s> def createrepo(self):	createRepo repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "name", "testrepo0", self.token)
# todo: typing for pb </s> for proto in maddr.protocols()	parse_conn_protocol def parse_conn_protocol(maddr: Multiaddr) -> int: proto_codes = set( ) proto_cand = proto_codes.intersection(_supported_conn_protocols)
# todo speed! </s> return self.title	AdminUserDraft ts = models.DateTimeField(editable=False, auto_now_add=True) def __unicode__(self): return "Autosaved %s (%s)" % (self.ct, self.ts) class Meta:
# todo: see get_scale_factor() to choose 72 px on hidpi </s> def _scroll_handler_id(self, value):	_scroll_handler_id if self._scroll_handler_id_value is not None: self.disconnect(self._scroll_handler_id_value)
# todo: refactor accordingly when v3 websocket api is released </s> def order_book_class(self) -> bittrexorderbook:	order_book_class @property
# todo: use stream_with_context instead of list </s> audit = config.get('audit')	__after__ __after__ is called after every action :param response: the previously created response - for modification c.audit['administrator'] = getUserFromRequest(request).get("login") audit.log(c.audit)
# todo: if needed allow other handling (like adding values) </s> def num_bands(self):	num_bands return self._num_bands
# todo. tune this "10" hyperparameter </s> distances.append(d)	pdist fX[i+1:, :], p=2, eps=1e-06).view(-1)
# todo(mgius): tests for views using this api call </s> user_id, email, password, tenant_id)	user_create def user_create(request, user_id, email, password, tenant_id):
"""todo: doesn't remove unused nodes/renumber elements""" </s> raise valueerror(msg)	write_tecplot msg = 'Cant hstack...\n' msg += 'xyz.shape=%s\n' % str(self.xyz.shape) fmt = ' %15.9E' * (3 + nresults) else:
# todo device strategy in pytorch? </s> def get_device_assignments(self, device_names=none):	get_device_assignments
# todo: rename dest to cron, since this does more than just quiet </s> help='print flexget version and exit.')	OptionParser OptParser.__init__(self, **kwargs) self.version = flexget.__version__ self.add_option('--debug', action='callback', callback=self._debug_callback, dest='debug', help=SUPPRESS_HELP)
# todo: replace use of request.forms with json </s> def error500(error):	error500 return "Internal server error"
# todo: improve logic to handle simple types like list of strings? </s> return config	load_project_config dir_path = path / subdir if not dir_path.exists():
# todo: test for last revision on first page. </s> offset = url_for(controller='revision', action='purge', id=none)	test_purge res = self.app.get(offset) assert 'No revision id specified' in res
# todo: remove them when the old workflow system will be </s> this_user = get_configured_user_email()	is_daemon_user daemon_user = get_daemon_user()
# todo: remove when we stop supporting python < 3.5 </s> quadruplets = check_input(quadruplets, type_of_inputs='tuples',	_QuadrupletsClassifierMixin check_is_fitted(self, 'preprocessor_') else: preprocessor=self.preprocessor_, estimator=self, tuple_size=self._tuple_size)
# todo: warn/error: check if this var has units: assigning </s> for vardata in self.values():	setlb Set the lower bound for this variable.
# todo: [phil] i think we could avoid this and use a bytes buffer in memory instead, zipfile supports it </s> vm.globals[tmp_name] = form_string	_process_file log.debug("2. Added VBA form variable %r = %r to globals." % (global_var_name.lower(), form_string)) tmp_name = global_var_name_orig.lower() + ".*" if (log.getEffectiveLevel() == logging.DEBUG): log.debug("2. Added VBA form variable %r = %r to globals." % (tmp_name, form_string))
# todo add arch arm/aarch/mips/mips64/sparc/sparc64 </s> if self.mode in (architecture.x86_16_att, architecture.x86_32_att, architecture.x86_64_att):	disassemble def disassemble(self, code, addr): arch, mode, endian = self.get_arch_mode("capstone") cs.syntax = capstone.CS_OPT_SYNTAX_ATT for i in cs.disasm(bytes(code), addr):
for line in json_generator:     # todo: save file here as zip on cdn </s> del local_functions[k]	_module_functions if not inspect.isfunction(v) or k.startswith('_'):
"rulesactivated": false,  # todo for testing to be compatible with protocol 0.6 </s> internal function to check sanity of the registration sensors list.	_checkMsgRegSensorsList def _checkMsgRegSensorsList(self, sensors: Dict[str, Any], :param sensors: :param messageType:
# todo lauren: make shorter by pulling out lamda and then filtering </s> raise valueerror('cannot pass both `auth` and `user`')	can_edit if not auth and not user: raise ValueError('Must pass either `auth` or `user`') user = user or auth.user if auth:
# todo: we really shouldn't be using socket.request.db at all, but </s> event, otherwise a dict containing information about the event.	handle_user_event Inspects the embedded user event and decides whether or not the passed socket should receive notification of the event. if socket.request.authenticated_userid != message['userid']: return None
# todo: update this code to return statistics on deleted objects once we </s> :param purge_incomplete: true to also delete executions which are not in a done state.	purge_executions@73 :type timestamp: ``datetime.datetime :param action_ref: Only delete executions for the provided actions. :type purge_incomplete: ``bool`` if not timestamp:
raise deprecatedtest # this test is now broken. todo: fix it. </s> remote_prop = property(item, relation='many-to-one', remote_ap='test_ap')	make_test_second_ap def make_test_second_ap():
# todo: st api does not allow us to say "do not focus this new view" </s> global opening_files	fullfill opening_files[file_path] = (None, resolve)  # type: ignore
# todo: confirmation in "r" mode </s> if context.input_is_ready():	process_events def process_events(context): break api.process_events()
# todo: non-numeric columns should be ignored automatically </s> return df.iloc[[1,4,9]].b.values	test_iloc4 @unittest.skip("TODO: support A[[1,2,3]] in Numba") def test_iloc4(self): hpat_func = hpat.jit(test_impl) n = 11
# todo: remove </s> print("due to snap limitations, it needs to be disabled manually by running, i.e:")	gnome_power_detect if os.getenv('PKG_MARKER') == "SNAP" and gnome_power_stats == 0: print("\nDetected running GNOME Power Profiles daemon service:") print("cd ~/auto-cpufreq/auto_cpufreq") print("python3 gnome_power.py --disable")
# todo proper unit testing </s> assert (expected_tte == times_to_event).all(), '  time_to_event failed'	test_censoring_funs@73 times_to_event = padded_events_to_tte(continuous_events, discrete_time=False) not_censored = padded_events_to_not_censored(continuous_events, assert (expected_is_censored != not_censored).all(), 'not_censored failed'
# todo: make idempotent </s> ) -> none:	aws ctx: click.Context,
# todo support domain delegation, which will allow us to set a sub-account to execute as. we can then </s> return service	get_conn@47 http = httplib2.Http() http_authorized = credentials.authorize(http)
# todo: migrate to glaziererror </s> - exit glazier with code 1	_LogFatal * The user-facing help message containing where to look for logs and where to go for further assistance. Args: msg: The error message to accompany the failure.
# todo(mordred) add this back wnen ksa releases </s> def test_get_host_no_detail(self):	TestInventory host_found = True self._test_host_content(host) host = self.inventory.get_host(self.server_id, expand=False) self.assertIsNotNone(host)
raise pathaccesserror()  # todo: path </s> strings with dots that shouldn't be expanded.	Path class Path(object): syntax won't work or isn't desirable. >>> target = {'a': {'b': 'c', 'd.e': 'f', 2: 3}} >>> glom(target, Path('a', 2))
# todo: make sure this works </s> 'access_token': session.data.get('auth_user_access_token') or '',	get_globals 'webpack_asset': paths.webpack_asset, 'waterbutler_url': settings.WATERBUTLER_URL,
##todo(ziad):we need to figure out how to auth to keystone </s> "keystone uri='%s'" % self.auth_location)])(env,	_reject_request [("WWW-Authenticate",
# todo: move to validator or helper </s> return (	can_edit is_api_node = auth.api_node == self else: (user and self.has_permission(user, 'write')) or is_api_node
# todo(developer): uncomment and set to a path to your audio file. </s> content = audio_file.read()	transcribe_file_with_word_level_confidence@251 from google.cloud import speech_v1p1beta1 as speech client = speech.SpeechClient() audio = speech.types.RecognitionAudio(content=content) config = speech.types.RecognitionConfig(
# todo: send alert </s> try:	get_country_id prefix_obj = Prefix.objects.filter(prefix__in=eval(prefix_list)) country_id = prefix_obj[0].country_id.id
# todo check for types that are not classes and add it to </s> return false	_is_name_break_scope Returns True except for nested imports and instance variables. if stmt.isinstance(pr.ExprStmt): elif isinstance(stmt, pr.Import) and stmt.is_nested(): return False
observable_pars = np.abs(np.random.randn(observable_class.n_params)) #todo: some operations fails when parameters are negative (e.g. thermal state) but par_domain is not fine grained enough to capture this </s> obj = plugin.resolve()	test_resolve_all_plugins @data_provider(all_plugins) self.assertIsNotNone(obj, msg="Plugin "+plugin.name+" advertised entry point "+str(plugin)+" but it could not be resolved.")
# todo: sumo has a single es_url and that's the zlb and does </s> for doc in seq_a]	diff_it_for_realz seq_a = [ HashableWrapper( seq_b = [ HashableWrapper(
# np.datetime64[ns] which invalid, todo: fix pa </s> s = fill	_column_fillna_impl if hpat.hiframes_api.isna(B, i):
#todo(wwolf) get correct value for these </s> link_node.setattribute('rel', link['rel'])	VersionsXMLSerializer version_node.setAttribute('updated', version['updated']) for link in version['links']: link_node.setAttribute('href', link['href']) version_node.appendChild(link_node)
raise notimplementederror # todo </s> for s in self.states_list:	resample_states s.resample()
raise exception('lol') #todo fixme </s> l.setting_changed(name, old_value, new_value)	Settings self._listeners.append(l) def notify(self, name, old_value, new_value):
#todo _rule_for_parents needs to made into a generator </s> ancestors_list = self._get_ancestors_observation(observed_list)	active_trail_nodes >>> student.active_trail_nodes('diff') ['diff', 'intel'] Direction of flow of information up ->  from parent to child
# todo: fix this somehow? better use a helper func which goes over the structure. </s> ptr = ctypes.cast(ptr, ctypes.c_void_p)	_ctype_ptr_get_value def _ctype_ptr_get_value(ptr):
# todo find out what is best used here! </s> def get_hyperparameter_search_space(dataset_properties=none):	DecisionTree 'output': PREDICTIONS, 'preferred_dtype': np.float32} criterion = CategoricalHyperparameter( "criterion", ["gini", "entropy"], default="gini")
# todo(laigd): remove this check when 312743821 is in the release. </s> [-0.571142, -0.432439, 0.413158, 0., 0., 0.],	test2DStaticShape real_x = self.evaluate(padded_x) expected_x = [ [0.255314, -0.985647, 1.461641, 0., 0., 0.], [0., 0., 0., 0., 0., 0.],
time.sleep(40)  # todo: should remove after polling get. </s> exp_res_2 = op(data_2, data_3)	test_tensor_abstraction_subsets@76 time.sleep(40)  # TODO: should remove after polling get. exp_res_1 = op(data_1, data_2) assert (mpc_2_3.reconstruct() == exp_res_2.child).all() exp_res_3 = op(exp_res_1, exp_res_2)
# todo: we want to create a state group for this set of events, to </s> for key, events in conflicted_state.items():	_resolve_state_events auth_events ) if key[0] == EventTypes.Member: logger.debug("Resolving conflicted member lists %r", events)
# todo: pandas returns dataframe, maybe return namedtuple instread of </s> assert isinstance(last_block.body[-1], ir.return)	remove_none_return_from_block last_block.body.pop() assert (isinstance(last_block.body[-1], ir.Assign)
# todo: uncomment once outstanding issues with this feature are addressed </s> user = _get_current_user()	get_globals def get_globals(): return { 'user_name': user.username if user else '',
# todo: write this </s> x = np.zeros((bs,3,224,224), dtype=np.float32)	test_efficientnet def test_efficientnet(self): Y = np.zeros((BS), dtype=np.int32) train_one_step(model,X,Y)
# todo: logging </s> verifies an ses bounce message.	is_verified if self._verified is None: try:
# todo: log exception </s> except subprocess.calledprocesserror as e:	scan@71 if local: try: output = e.output else:
# todo find commit hash </s> pkgs.update(repo_pkgs)	index_command pkgs: Dict[str, Any] = {} for (repo, data) in repos.items(): json.dump(pkgs, sys.stdout, indent=4)
# todo(twilson) we can remove this when we require ovs>=2.12.0 </s> 'chassis_private'}):	MetadataAgentOvnSbIdl None, connection_string, helper) if chassis: self.tables[table].condition = [['name', '==', chassis]] if events:
# todo put this in a .extra w/a subselect </s> return settings.trac_url % self.trac_environment	trac_url
#todo also check type!! </s> def order_by(self, *field_names):	order_by @not_implemented
# todo: parse flags, error checking, etc. </s> self.var_stack.append(self.top)	Push self.top = {}
# todo: either fix this or remove this code </s> self.change_id = none	SearchEntry Initializes the entry EntryWithClearButton.__init__(self) self.entry.connect('changed', self.on_entry_changed) def on_entry_changed(self, *e):
# todo: add tests for `in_rebase` true </s> @p.expand(testlongbranchstatustestcases)	TestLongBranchStatus class TestLongBranchStatus(DeferrableTestCase): def tearDown(self): def test_format_branch_status_for_status_dashboard(self, status_lines, expected): git = GitCommand()
# todo(stephenfin): the mock of 'migrate_disk_and_power_off' should </s> flavor_id = self._create_flavor(extra_spec=extra_spec)	test_create_server_with_physnet_and_tunneled_net This should pass because we've requested a single-node instance and the requested networks share at least one NUMA node. networks = [ {'uuid': base.LibvirtNeutronFixture.network_1['id']},
# todo: fix once issue #306 addressed in dfvfs </s> 'unable to open file system with error: {0!s}'.format(exception))	_ExtractPathSpecsFromFileSystem dfvfs_errors.AccessError, dfvfs_errors.BackEndError, dfvfs_errors.PathSpecError) as exception: return try:
assert study_id == 0  # todo </s> assert study_id == 0  # todo	InMemoryStorage assert study_id == 0  # TODO self.trials[trial_id].intermediate_values[step] = intermediate_value self.trials[trial_id].system_attrs[attr_name] = attr_value def get_trial(self, study_id, trial_id):
# todo: figure out way to paramaterize node['osd_ids'] for this test </s> def test_osd_listens_on_6800(self, node, socket):	test_osd_listens_on_6800
system_info = none #todo </s> try:	is_pupil_mobile_recording def is_pupil_mobile_recording(rec_dir: str) -> bool: return info_csv["Capture Software"] == "Pupil Mobile" and "Data Format Version" not in info_csv except KeyError:
# todo link subscribers_changed in docstring to callback docs </s> 'playlistfolder', ['id', 'name', 'type'])):	PlaylistFolder pass
# todo: use proper file name </s> program = mime_type[6:] + "topnm"	process_image with open(filename, "rb") as raw: proc = subprocess.Popen([program, filename], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
# todo: replicate complete behaviour of urllib.urlopener.retrieve </s> target_filepath = parsed_url.path.lstrip( '/' )	TUFancyURLOpener return destination_directory, filename headers = {} if filename is None: destination_directory, filename = mkdtemp( target_filepath )
# todo if not found, getmessages to find the sender and chat </s> self.is_group = (	NewMessage self._chat = None self._sender = None isinstance(message.to_id, (types.PeerChat, types.PeerChannel)) and not message.post
"history_id": self.history_id,  # todo: shouldn't be needed :( </s> content = self.dataset_populator.get_history_dataset_content(self.history_id, dataset=datasets[0])	test_upload_api_option_space_to_tab self.dataset_populator.wait_for_tool_run(self.history_id, run_response) datasets = run_response.json()["outputs"] assert content == ONE_TO_SIX_WITH_TABS content = self.dataset_populator.get_history_dataset_content(self.history_id, dataset=datasets[1])
# todo: unit test! </s> try:	edge_index index: Tuple of (node1_type, edge_type, node2_type) Returns: index = self.edge_types.index(edge_type) except ValueError:
# todo: remove pragma when we drop 2.7 </s> elif token == "w":	_parse_token parts["am_pm"] = "am" elif value in (self.locale.meridians["pm"], self.locale.meridians["PM"]): parts["weekdate"] = value
#todo fixme: we should provide an option to create the page </s> pywikibot.output('%s doesn\'t have a wikidata item :(' % page)	addClaims@29 item = pywikibot.ItemPage.fromPage(page) pywikibot.output('Processing %s' % page) return False for claim in claims:
# todo implement with unfold </s> _, _, k_x, k_y = module.weight.shape	ConvTranspose2DDerivatives N, C_out, H_out, W_out = module.output.shape mat_reshape = mat.reshape(V, N, G, C_out // G, H_out, W_out) u = unfold_by_conv_transpose(module.input0, module).reshape( N, C_in // G, G, K_X, K_Y, H_out, W_out
# todo: op should be initialized with op = be.constant(np_val, name=node.name) </s> op = one_inputs_ops[op_type](name_to_op[inputs[0]])	create_neon_graph@88 if op_type in two_inputs_ops: op = two_inputs_ops[op_type](name_to_op[inputs[0]], name_to_op[inputs[1]], name=node.name) elif op_type == 'Identity': op = name_to_op[inputs[0]]
return # todo raise error </s> pass	Seeked @dbus.service.signal(dbus_interface=PLAYER_IFACE, signature='x') def Seeked(self, position):
print("error, how does this happen?") #todo </s> return self.rec(expr.child)	map_bitwise_not
# todo implement this method </s> this class describes a ui event of app, such as touch, click, etc	UIEvent class UIEvent(AppEvent):
# todo: verify logic for create -- we shouldn't 'annexify' non-annexified </s> class _earlyexit(exception):	_EarlyExit
# todo: collapse identical parameter values in a single one </s> if name in hv_defs and hv_defs[name] == self.op.hvparams[name]:	_RevertToDefaults def _RevertToDefaults(self, cluster): hv_defs = cluster.SimpleFillHV(self.op.hypervisor, self.op.os_type, {}) del self.op.hvparams[name] be_defs = cluster.SimpleFillBE({})
'units': '1',  # todo: where does this come from??? </s> end_time=to_datetime(sources.time.values[-1]).strftime('%y%m%d%h%m%s%f')))	get_filename def get_filename(path_template, index, sources): return Path(str(path_template).format(tile_index=index,
# todo increase precision </s> return [(degree-k, k) for k in range(degree+1)]	create_monomial_exponents2
# todo: check md5sum if available </s> self.i = e + 1	decode_int def decode_int(self): e = self.bytes.index('e', self.i) return n
# todo: does this import need to be delayed because </s> if self.get_option("verbose") or self.get_option("output_times"):	ExtensiveFormAlgorithm filename, self.get_option("symbolic_solver_labels")) print("Time to write output file=%.2f seconds" % (time.time() - start_time))
# @todo: pheonix </s> self.populateskilltree()	delaySearch def delaySearch(self, evt): else: self.searchTimer.Stop()
# todo: unit tests </s> for child in reversed(node.nodes):	get_dashboard_nodes def get_dashboard_nodes(node, auth): if child is not None and not child.is_deleted and child.resolve().can_view(auth=auth) and node.can_view(auth): rv.append(child)
# todo: fixme-  assumes only one topic (next two lines) </s> self.consumer = _consumer	StreamTask self.application_id = _application_id self.partitions = _partitions self.producer = _producer self.recordCollector = RecordCollector(self.producer)
# todo: refactor common tests for all models, e.g. shape checking </s> trans_h = transh(triples_factory=self.factory)	test_trans_h def test_trans_h(self):
# todo: manage errors </s> singleton to return only on instance of project.	instance @staticmethod :returns: instance of Project if not hasattr(Project, "_instance"):
# todo: when tool prints input bam filename, use that instead </s> 'suffix': '%',	dedup_general_stats_table 'description': 'Percentage of reads categorised as a technical duplicate', 'min': 0, 'scale': 'OrRd', 'format': '{:,.0f}',
# todo(nnorwitz): enable test. </s> tokens = gettokens('bar<foo, blah<x>, bling>')	testTemplateWithMultipleTemplateArgsMid result = ast._ConvertBaseTokensToAST(list(tokens)) self.assertEqual(1, len(result))
# todo: save as yaml file </s> with open(filename) as calib_file:	from_file def from_file(self, namespace): self.from_yaml(calib_file.readall())
except oserror:  # todo: use filenotfounderror once drop python 2 </s> if ss == 'e':	ss_to_index C=2 if ss == 'H': return 1 if ss == 'C':
# todo: fix later </s> return batch_size	select_batch_size
except exception:  # todo - which exceptions? </s> append_to_annotations("organism_host", organism_element.text)	_parse_organismHost def _parse_organismHost(element): for organism_element in element:
# todo: also dispatch server-sent event </s> 'runnertestcase',	get_runner def get_runner():
with prepare_file(["#todo this is todo"], none) as (lines, filename): </s> output,	test_text_logs self.assertRegex(
# @todo: "smart" & ssh keys for non-localhost </s> result = setup_monitor_server_disable(r.id)	setup_monitor_server_disable_interactive Disable Monitoring for a Server - Remove all related Tasks current.session.confirmation = result redirect(URL(f="server"))
# todo: this scrolling is lame and centers text :/ </s> self.put(json.dumps({'name': 'msg', 'data': msg}))	send_msg def send_msg(self, msg):
else:  # todo(@rasooli) t44144867: remove this logic after a while. </s> def get_decoder_lang_code(self, lang_id):	get_decoder_lang_code
raise notimplementederror # todo </s> raise notimplementederror # todo	Site raise self.MultipleObjectsReturned def save(self, item): def remove(self, item): Remove/delete the item from the backend storage
# todo: position independend compare </s> self.bkey = none	_setupKey_ def _setupKey_(self): if self.bkey is None: self.bkey = binascii.unhexlify(
# todo: update npt.assert_almost_equal calls to use distancematrix </s> for e in self.table1:	test_faith_pd def test_faith_pd(self): expected.append(faith_pd(e, tree=self.tree1, otu_ids=self.oids1)) expected = pd.Series(expected)
# todo: remove this skip after fixing </s> raise skiptest	test_circle_draw@20 @requires_application() def test_circle_draw(): with TestingCanvas() as c: ellipse = visuals.Ellipse(pos=(75, 35, 0), radius=20,
# todo: make it irrelevent whether we test a python or a tf component (api and handling should be 100% identical) </s> input_ = np.asarray([[1.11], [2.22], [3.33], [4.44]])	test_python_sequence_preprocessor recursive_assert_almost_equal( out, np.asarray([[[1.0, 1.0, 1.1]], [[2.0, 2.0, 2.2]], [[3.0, 3.0, 3.3]], [[4.0, 4.0, 4.4]]]) out = sequencer._graph_fn_apply(input_) self.assertEqual(sequencer.index, 2)
# todo: write me </s> grant_access("test", "test", "metadata")	setup_class cls.client = make_logged_in_client(username='test', is_superuser=False) cls.user = User.objects.get(username='test') grant_access("test", "test", "navigator") cls.api = NavigatorApi()
### todo: etc </s> else:	_update_directory except exc.InvalidGitRepositoryError: if os.path.isdir(path): print(ERROR, long_name, "isn't a repository!") else:
# todo: rf to use --batch where possible instead of splitting </s> if "fatal:" in cml.out:	_fake_exception_wrapper list(self._run_annex_command_json( 'status', args=options_, expect_stderr=False)) raise CommandError(cmd="git annex status", msg=cml.out, stderr=cml.out)
# todo: check against plural_rules[lang]['nplurals'] </s> be used for plural support.	twtranslate @param code The language code @param twtitle The TranslateWiki string title, in <package>-<key> format The translations are retrieved from i18n.<package>, based on the callers import table.
# todo - exponential backoff </s> return future.get(metadataresponse)	request_metadata def request_metadata(self, topics=[]):
# todo: can this be replaced by action_channel()? </s> host = self['system_info']['results'].get('hostname', ''),	get_connection_info user = self['system_info']['results'].get('whoami', ''),
n)  # todo: access alice's private key inside this method. </s> return msgpack.dumps(self.ids)	packed_payload
# todo delete me </s> tagger.attrib('diffuse', '{0} {1} {2} {3}'.format(	exportSDFMaterial tagger.attrib('ambient', '{0} {1} {2} {3}'.format( ambient['r'], ambient['g'], ambient['g'], alpha)) diffuse['r'], diffuse['g'], diffuse['b'], alpha)) specular = materialdata['specularColor']
# todo(leofang): test newer rocm versions </s> a = cupy.asfortranarray(a)	test_fftn_orders for order in ['C', 'F']: a = testing.shaped_random(self.shape, cupy, dtype) out = cupy.fft.fftn(a, s=self.s, axes=self.axes) fft_func = _default_fft_func(a, s=self.s, axes=self.axes)
#todo rewrite this part of pdfkit.py </s> self.asserttrue(r.source.isstring())	test_html_source_line def test_html_source_line(self):
# todo: fix with stubber / before send event </s> with mock.patch('botocore.endpoint.endpoint._send') as _send:	TestApiGateway 'exportType': 'swagger', 'accepts': 'application/yaml' _send.return_value = mock.Mock( status_code=200, headers={}, content=b'{}')
pass # todo </s> d.addcallback(lambda children: children.has_key(name))	has_child def has_child(self, name): exists a child of the given name, False if not.""" return d
# todo fix. </s> self.__save_failing_context(ctx_init)	test_cmova if not cmp_result:
# todo: handle non-numerical (e.g. string, datetime) columns </s> def f(df_arr):  # pragma: no cover	_run_call_len return [ir.Assign(ir.Const(0, lhs.loc), lhs, lhs.loc)] nodes = [] return len(df_arr) return self._replace_func(f, [arr], pre_nodes=nodes)
# @todo: refactor: split to smaller routines </s> pacmantaskworker(['-qu', ])	find_repo_updates def find_repo_updates(): ).execute() packages_updates_lines = result.stdouts
# todo: assert </s> def createrepo(self):	createRepo repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "name", "testrepo0", self.token)
# todo : use starttls when mailnag has been migrated to python 3 </s> self._conn = conn	Pop3Backend conn.getwelcome() conn.user(self.user) except: try:
# todo use case-xml case creation workflow </s> multiaction_enabled=true,	make_psi_config def make_psi_config(domain): c = CommtrackConfig( multiaction_keyword='s', actions = [
# todo(eric_k): unicorn@1.0.2rc1 doesn't like writing to </s> perms,	map_memory_callback hex(address) if type(address) is int else "0x??", hr_size(size), "-", f"{name}:{hex(offset) if name else ''}",
# todo: properly resolve samaccounname in gc </s> if str(e).find('broken pipe') >= 0:	rpc_resolve_sids try: resp = lsat.hLsarOpenPolicy2(dce, lsat.POLICY_LOOKUP_NAMES | MAXIMUM_ALLOWED) return else:
# todo add rising vs top </s> pass	Error
# todo: how to check it? meybe we can omit this test </s> f_ng = importer.get_op_handle("y")	test_constant workspace.RunNetOnce(net) importer = C2Importer() f_result = ngt.make_transformer().computation(f_ng)() assert(np.ma.allequal(f_result, workspace.FetchBlob("Y")) and f_result[0] == val)
# todo: add logger here </s> browser = webdriver.phantomjs(	make_screenshot@45 '--ssl-protocol=any', '--proxy=' + proxy, '--proxy-type=socks5'] else: service_args=service_args, executable_path="phantomjs") browser.set_window_size(1024, 768)
# todo: fixme </s> self.geometry_properties['caero_control_surfaces'].opacity = 0.5	_set_caero_representation def _set_caero_representation(self, has_control_surface): if 'caero' not in self.geometry_actors: return
# todo(junxian): transform to decoder state size </s> "name": "stochastic_connector"	StochasticConnector ``` return { } def _build(self, distribution, batch_size):  # pylint: disable=W0221
# todo(b/80125832): enable nccl in tests </s> contents = f.read()	testTraceFileStepStatsProto step_stats = step_stats_pb2.StepStats()
# todo logging </s> template = templates_env.get_template(template_name)	render_file with open(os.path.join(output_dir, output_name), "w") as f: f.write(template.render(**context))
''' # todo filter in the database? </s> gather_data_sql = gather_data_sql_pat % (qt, qcn)	data_suff_stats SELECT %s FROM %s qt = sqlite3_quote_name(table) cursor = bdb.sql_execute(gather_data_sql) count = 0
# todo fix. </s> ctx_init = self.__init_context()	test_cmova def test_cmova(self): x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) cmp_result = self.__compare_contexts(ctx_init, x86_ctx_out, reil_ctx_out)
# todo: slugification should be abstracted out somewhere reusable </s> self.permalink = re.sub(	Post self.permalink = \ re.sub(":title", re.sub("[ ?]", "-", self.title).lower(), ":filename", re.sub( "[ ?]", "-", self.filename).lower(), self.permalink)
# todo: log errors to log file </s> else:	populate_cmds_args_opts if key == "kubectl": for opt in key_map[key]['options'].keys(): for opt in key_map[key]['options'].keys(): self.all_opts.append(opt)
# todo: should be able to just access the api from qml. </s> return	_createAdditionalComponentsView self.__additional_components_view = self._application.createQmlComponent(path, {"manager": self}) if not self.__additional_components_view: self._application.addAdditionalComponent("monitorButtons", self.__additional_components_view.findChild(QObject, "networkPrinterConnectButton"))
time.sleep(1)  # todo: avoid race conditions in other way </s> self.assertequal(self._call_csr('csr-nosans.pem'), [])	test_csr_no_sans
# todo confirm we want floor division here </s> weights : ndarray, optional	mnl_estimate GPU : bool, optional coeffrange : tuple of floats, optional lcgrad : bool, optional beta : 1D array, optional
#todo take image as a factor </s> if isinstance(node, tag):	text_len return cur_node.text_len text_len = 0 text_len += self.text_len(node) elif type(node) is NavigableString:
# todo: checks for being not outside of this repository </s> self._run_annex_command('rmurl', annex_options=[file_] + [url])	rm_url ---------- file_: str
# todo: no-op component? </s> if self.type == "single":	define_api_methods
"""the event which triggered the message."""#todo elaborate </s> def __init__(self, jenni):	JenniWrapper self.bot = jenni def __getattr__(self, attr):
# todo: could use trim_silence() here or a better vad. </s> def trim_silence(audio, threshold):	trim_silence energy = librosa.feature.rms(audio) frames = np.nonzero(np.array(energy > threshold))
# todo: allow prerelease for now </s> extension)	_get_tarball_name def _get_tarball_name(self, name, extension): tarball = '{0}.{1}'.format( return tarball
# todo: add recovery test </s> processupdatedrepo(self.conduit, "phab", "origin")	_phabUpdateWithExpectations def _phabUpdateWithExpectations(self, total=None, bad=None): with phlsys_fs.chDirContext("phab"): if total is not None: self.assertEqual(self._countPhabWorkingBranches(), total)
# todo: remove in 21.08 </s> self.result.clear()	reset@46
# todo(developer): uncomment these lines and replace with your values. </s> client = tasks.cloudtasksclient()	pause_queue@212 queue_path = client.queue_path(project, location, queue) response = client.pause_queue(queue_path)
# todo: truffle change end </s> def hexdigest(self):	hexdigest
# todo: make sure reply_email is unique </s> local_part = srs[: srs.find("@")]  # srs0=8lgw=y6=outlook.com=abcd	parse_srs_email def parse_srs_email(srs) -> str: local_email_part = local_part[local_part.rfind("=") + 1 :]  # abcd rest = local_part[: local_part.rfind("=")]  # srs0=8lgw=y6=outlook.com
# todo! make this whole operation one undo </s> print('position: {pos}'.format(pos=self.pos))	Sprite print('Name: {name}'.format(name=self.name))
# todo: passing config is wrong, but changing this revealed more complicated issues </s> def _template_file_default(self):	_template_file_default @default('template_file')
# todo: replace with "yield from" when dropping python 2. </s> return http.post(*args, **kwargs)	ajax def ajax(*args, **kwargs):
# todo: test </s> typ_val = _h5_typ_table[arr_typ.dtype]	get_type_enum_overload @overload(get_type_enum) return lambda a: np.int32(typ_val)
# populated by coredb._solve(). todo: find a better solution for that. </s> f.is_include_file = false	_parse_component if f.isIncludeFile == "true": f.is_include_file = True f.logical_name = f.logicalName f.copyto = ""
# todo(b/147242148): introduce principled artifact structure (directory </s> input_dict: key -> channel mapping for inputs generated in logical	resolve_input_artifacts called in normal cases (except head of the pipeline) since it handles artifact info passing from upstream components. pipeline. exec_properties: Dict of other execution properties, e.g., configs.
# todo: remove this log statement when invoking this method on each iteration of the goal state loop (currently it is invoked only on a new goal state) </s> elif protocol_version in version_info.get_supported():	check_wire_protocol_version preferred = version_info.get_preferred() if PROTOCOL_VERSION == preferred: logger.info("Wire protocol version:{0}", PROTOCOL_VERSION) logger.info("Server preferred version:{0}", preferred)
# compare filesizes todo print analysis of this :) </s> else:	read_log log_file = os.path.join(DATA_ROOT, '%(login)s/%(project_id)s.process.log' % locals()) if os.path.exists(log_file): return ''
#todo - use the following more helpful error, but update unit tests </s> this is useful if you want to add two alignments which use the same	MultipleSeqAlignment This sorts the rows alphabetically using the SeqRecord object id. Currently no advanced sort options are available, although this may record identifiers, but in a different order. For example, >>> from Bio.Alphabet import generic_dna
# todo(shardy): remove when we no longer support essex </s> else:	authenticate@136 'username': con.username, 'api_key': con.password, headers = {'Content-Type': 'application/json'} o = urlparse.urlparse(con.aws_auth_uri)
# todo 找出数据重复的原因 </s> repost_cont = get_statusinfo.get_status_info(repost_url, session, user_id, user_name, headers)	_get_current_reposts@220 repost_html = repost_json['data']['html'] repost_urls = status_parse.get_reposturls(repost_html) if repost_cont is not None: spread_other_and_caches.append(repost_cont)
# todo: remove this </s> ):	file present=True, assume_present=False, user=None, group=None, mode=None, touch=False, Add/remove/update files. + name: name/path of the remote file
# todo(ja): reclaiming space should be done lazy and low priority </s> image_service,	copy_image_to_volume def copy_image_to_volume(self, context, volume, image_service, image_id): image_id, self.local_path(volume),
# todo find a better way of checking for no pregenerated thresholds </s> )	strm_decl self.code_gen_dict["$STREAMDECLARATIONS$"] = [] self.code_gen_dict["$STREAMDECLARATIONS$"].append( self.code_gen_dict["$STREAMDECLARATIONS$"].append( 'hls::stream<ap_uint<{}>> out ("out");'.format(self.PE)
# todo: the test currently demonstrates broken behavior </s> mode)	Tests the lock is taken. stepcontrollers, master, builder_ids = \ yield self.assert_two_builds_created_one_after_another( stepcontrollers, master, builder_ids)
# @todo: replace with consent tracking </s> @param all: check whether the user has all of the roles	s3_has_roles None - for any entity 0 - site-wide if self.override or not roles: return True
# todo: remove need for --no-strict-optional </s> if not self.allow(full_name):	add_mypy_cmd def add_mypy_cmd(self, name: str, mypy_args: List[str], cwd: Optional[str] = None) -> None: return args = [sys.executable, '-m', 'mypy'] + mypy_args
# name. it should be handled by making a simple class to hold todo </s> return	Todo heading = d.find_current_heading() if not heading: todo_states = d.get_todo_states(strip_access_key=False) if not todo_states:
# todo: kodi 17 compat removal cleanup </s> label="user",	LoginManual self.close() def _add_editcontrol(self, x, y, height, width, password=False): font="font13", textColor="FF00A4DC",
# todo: only remove excess partitions if new data has fewer </s> return self._get_items(new_keys)	_get_first_partitions def _get_first_partitions(self, keys: list):
# no todo item selected </s> return true	selectable
# todo: complex numbers return complex </s> resolver = arrayattribute.resolve_argsort.__wrapped__	resolve_argsort @bound_function("array.argsort") sig = resolver(self, ary, args, kws) sig.return_type = if_arr_to_series_type(sig.return_type)
# todo: add other types to this table (e.g., functional.all_types) </s> expected = ('\ncreate table mytable (\n\tcol1 string, \n\tcol2 tinyint, '	test_sqlalchemy_impala4_compilation@63 'transactional_properties': 'insert_only' }) '\n\tcol3 INT, \n\tcol4 DOUBLE, \n\tcol5 DATE, \n\tcol6 VARCHAR(10)\n)' '\nPARTITIONED BY (part_col STRING)\nSTORED AS PARQUET\n'
# todo: fill some sane numbers here </s> def getsearchid(self):	getSearchId
# todo: explicitly exploit symmetry and set onesided=true </s> p_fft = a_fft * b_fft	HolE a_fft = torch.rfft(h, signal_ndim=1, onesided=False) b_fft = torch.rfft(t, signal_ndim=1, onesided=False) composite = torch.irfft(p_fft, signal_ndim=1, onesided=False, signal_sizes=h.shape[1:]) scores = torch.sum(r * composite, dim=-1, keepdim=True)
# todo: optimize this implementation </s> input_rows, input_cols, num_input_maps,	conv2d_forward_batch dtype=self.dtype) brainstorm.handlers._cpuop.im2col( kernel_shape[0], kernel_shape[1], padding, padding, padding, padding, stride[0],
# todo: this will currently still fail, since it has children </s> correct_parents = [steps['step-3'], steps['step-5'], steps['step-6']]	test_pipeline_sentinel case = unittest.TestCase()
# todo complete this method </s> kconserv = self.kconserv	make_ip if not self._made_shared: self._make_shared() t1, t2, eris = self.t1, self.t2, self.eris self.Woooo = imd.Woooo(self._cc, t1, t2, eris, kconserv)
# todo: proper test </s> 'tf': transferfunction([1, 1], [1, 2, 1], dt)}	dsystem_dt 'ssmimo': StateSpace(A, B, C, D, dt),
# todo: replace this by bulk update if we can </s> return safedeletequeryset(self.model, using=self._db)	all_with_deleted
# todo: add checks for broken paddings/encrypted values and malformed enc_data </s> self.assertraises(exception, hsm.decrypt, "data", "iv")	test_07_encrypted_key_file self.assertFalse(hsm.is_ready) self.assertRaises(Exception, hsm.setup_module, {}) self.assertRaises(Exception, hsm.setup_module, {"password": "wrong PW"})
# todo: make truly async </s> def __init__(self):	__init__@10
# todo: work out a way to set this based on the timespan of the data. </s> determines if the file corresponds to a rhessi x-ray summary	is_datasource_for @classmethod `~sunpy.timeseries.TimeSeries`. if 'source' in kwargs.keys():
# todo: index </s> return len(df_arr)	_run_call_len def f(df_arr):  # pragma: no cover
except(wx._core.pyassertionerror): #todo: error win64 </s> splash.show()	InVesalius splash = SplashScreen() self.control = splash.control return True
# todo: test 2: quando para a data no estado tem a planilha de total e outras </s> new[deaths] = row[deaths]	row_with_sorted_columns confirmed = f"confirmados_{day}_{month}" deaths = f"mortes_{day}_{month}" return new
# xxx todo: rounding </s> raise notimplementederror('unknown mnemo %s' % instr)	get_mnemo_expr def get_mnemo_expr(ir, instr, *args): instr, extra_ir = mnemo_func[instr.name.lower()](ir, instr, *args) return instr, extra_ir
# todo: parlist, dots, block </s> p = get_parser('foo')	testFieldExp node = p._field() self.assertIsNotNone(node)
# - todo default from user if citizen </s> def customise_br_home():	customise_br_home
# todo: remove / replace? </s> scriptstr = [opcodenames[80 + int(data['number_of_sigs_n'])] if x == 'op_n' else x for x in scriptstr]	script_to_string redeemscript_str = script_to_string(data['redeemscript']) scriptstr = [redeemscript_str if x == 'redeemscript' else x for x in scriptstr] return ' '.join(scriptstr)
# todo: remove </s> def _form_to_db_schema(group_type=none):	_form_to_db_schema
# todo(b/131719250): add option to output a sample of anomalous examples for </s> coder=beam.coders.protocoder(	validate_examples_in_tfrecord@43 | 'WriteStatsOutput' >> beam.io.WriteToTFRecord( output_path, statistics_pb2.DatasetFeatureStatisticsList))) return stats_gen_lib.load_statistics(output_path)
# todo: remove method in 0.24 </s> presort=presort,	DecisionTreeRegressor random_state=random_state, min_impurity_decrease=min_impurity_decrease, ccp_alpha=ccp_alpha) def fit(self, X, y, sample_weight=None, check_input=True,
# todo: give a vanilla example </s> \\frac{\\sum_n \\hat{y}}{\\sum_{n,t} \\hat{y}}	feca@26 fraction = \\sum_n min \\left ( \\right ) Attributes
# todo: scale and translation could be merged into a single network </s> assert mask.get_shape() == shape[1:]	get_mask mask = checkerboard(shape[1:], parity=self.parity, dtype=dtype)
# todo error on too many levels </s> pass  # todo	_LinePlotter if palette is None: cmap_name = plt.rcParams["image.cmap"] if data["size"].isnull().all(): size_levels = [None]
# todo: this sometimes segfaults. i must fix this! </s> id=id,	_get_all_devices dev = io.Device(name=devinfo.name, input=devinfo.input != 0, interf=devinfo.interf, opened=devinfo.opened != 0)
#todo: use ffi function </s> del self._shapes[shape._hashid_private]	_remove_shape def _remove_shape(self, shape): cp.cpSpaceRemoveShape(self._space, shape._shape)
#todo: support broadcast case: (x,) (x, y) </s> momentum=1 means that new information is never incorporated. the	spatial_batchnorm_forward - eps: Constant for numeric stability - momentum: Constant for running mean / variance. momentum=0 means that default of momentum=0.9 should work well in most situations. - running_mean: Array of shape (D,) giving running mean of features
#todo : parse unit  25, 25m, 25 ft, etc. </s> else:	OSM_QUERY print(str(e)) self.report({'ERROR'}, "Overpass query failed") print('Overpass query success') self.build(context, result, geoscn.crs)
#todo: update this to try fsfindfolder and fallback to this </s> shell_folder_name = {	_get_win_folder_from_registry registry for this guarantees us the correct answer for all CSIDL_* names. "CSIDL_APPDATA": "AppData", "CSIDL_COMMON_APPDATA": "Common AppData",
# todo(yamahata): creating volume simultaneously </s> :param volume_id: volume id	remove_volume :param context: security context
# todo: put this in network definitions: </s> return hdwalletkey(self.parent_id, session=session)	parent
# todo do something with reccomendation </s> self.logger.debug('****************')	print_endpoint_state logger.debug('None') for l, s in ENDPOINT_STATES:
# todo: split lines here? </s> for fn in (afn, bfn):	main_diff@39 if not os.path.exists(fn): print("Missing file {}".format(fn))
# todo: log discarded bytes? </s> calculated_crc = crc16.crc16xmodem(''.join([chr(item) for item in message[:-2]]))	valid_crc def valid_crc(self, message): return supplied_crc == calculated_crc
nullcontext = contextlib.exitstack()  # todo: use contextlib.nullcontext after python 3.7 </s> if not input_path.exists() and not output_path.exists():	iterate_path name = '{}-{}'.format(args.name, str(i).zfill(args.width)) input_path = fmtutils.path_from_format(args.directory, args.format, name=name, ext='in') yield (name, input_path, output_path)
# todo: improve performance </s> bmat = einsum('boc->cbo', (bmat, )).contiguous()	__reshape_for_conv_in def __reshape_for_conv_in(self, bmat, module): batch, in_channels, in_x, in_y = module.input0.size() bmat = bmat.view(num_classes * batch, in_channels, in_x, in_y) return bmat
# todo: change the frontend to pass seconds instead. </s> return {'sub': 'email', 'email': email, 'exp': id_token_expiration_timestamp}	userinfo_mock
# todo: add error parameter </s> ('c', 'z')],	levshape >>> midx  # doctest: +SKIP MultiIndex([('a', 'x'), ) >>> midx.levshape
pass  # todo </s> 'address_uncompressed': self.address_uncompressed,	Input return { 'prev_hash': binascii.hexlify(self.prev_hash).decode('utf-8'), 'public_key': self.public_key, 'public_key_hash': self.public_key_hash,
# todo: make input channels configurable, not hard-coded to three channels for rgb </s> parser = subparser.add_parser(	add_parser "export", help="exports model in ONNX format", formatter_class=argparse.ArgumentDefaultsHelpFormatter )
# todo assert cls.__tablename__ == '' </s> def galaxy_session(model, session, user):	galaxy_session s = model.GalaxySession() yield from dbcleanup_wrapper(session, s)
# todo: it's better for us to have checked this a while ago so that this situation is impossible.  #443 </s> return true	interface_is_valid interface_is_valid = self._interface_signature.verify(message, self.public_keys(SigningPower)) self.verified_interface = interface_is_valid else: raise self.InvalidNode
# todo: support steps and times (motion blur) </s> self.count = 1	Duplis def __init__(self, exported_obj, matrix): self.exported_obj = exported_obj def add(self, matrix): self.matrices += matrix
# todo compare file contents? </s> "https://example.com/metadata/",	_new_updater def _new_updater(self): return Updater( "https://example.com/targets/", self.sim
# todo: get current encoding </s> else:	result_renderer_cmdline if cmdlineargs.report is None or len(cmdlineargs.report) > 1: ichr = jchr = '\n' jchr = ', ' ichr = ' '
# todo make this a private api </s> def geocode(self, query, exactly_one=true, timeout=default_sentinel):	OpenMapQuest self.api_key = api_key self.api = "%s://open.mapquestapi.com/nominatim/v1/search" \ Geocode a location query. :param str query: The address or query you wish to geocode.
# todo: this is wrong. globe.semiminor_axis does </s> 0.59010, 0.62291, 0.66488, 0.71809, 0.78540],	TestAzimuthalEquidistant [0.34907, 0.35079, 0.35601, 0.36497, 0.37803, 0.39579, 0.41910, 0.44916, 0.48772, 0.53724], [0.69813, 0.70119, 0.71046, 0.72626, 0.74912, 0.77984, 0.81953, 0.86967, 0.93221, 1.00969],
'target': 20000, # todo target max size </s> of examples. hashing is used to ensure that the same examples are loaded each	_get_instance_generator split: (string), split name ('train', 'val', or 'test') preproc_dir: (string) path to preprocessing dir epoch. Returns:
# :todo: implement test. </s> def test_fail_depth_string(self):	GetTransactionsToApproveRequestFilterTestCase self.skipTest('Not implemented yet.') def test_fail_depth_float(self): self.skipTest('Not implemented yet.') def test_fail_depth_too_small(self):
# todo: use widgets.dialog </s> self._editor.open(self.tree.get_selected_datafile_controller())	OnTreeSelection def OnTreeSelection(self, message):
# todo: test! </s> return httpresponse(json.dumps(data), content_type='application/json', status=status)	json_response def json_response(data=None, status=200):
# todo: convert 'pre' to 'code' ? </s> return file.ischild(self.folder) and not file.exists()	_check_valid def _check_valid(self, filename):
# todo: remove all elements of the list and remove the allowlist </s> tfa.callbacks,	test_api_typed modules_list = [ tfa, tfa.image, tfa.losses,
# todo: if there are resources, combine them into a zip file </s> exporter = exporter_map[format]()	NbconvertPostHandler SUPPORTED_METHODS = ('POST',) @web.authenticated model = self.get_json_body() nbnode = to_notebook_json(model['content'])
# todo: fire error </s> (optional) convenience operator to unregister y from x	__isub__ Equivalent to: y.unregister() @return: x
# todo: non-numeric columns should be ignored automatically </s> np.testing.assert_array_equal(hpat_func(df, n), test_impl(df, n))	test_iloc4 hpat_func = hpat.jit(test_impl) n = 11
# todo - log details about the test </s> self.module.fail_json(msg=msg)	SolidFireConnection else: failed = False else: self.module.exit_json()
# todo: add more complicated testcases </s> score = aom(self.scores, 3, method='dynamic', replace=true,	TestAOM random_state=42) assert_equal(score.shape, (4,)) random_state=42) assert_equal(score.shape, (4,))
# todo(ohta): convert `study` and `trial` to single objective versions before passing. </s> def sample_relative(	RandomSampler ): return self._sampler.infer_relative_search_space(study, trial) self, study: "mo.study.MoStudy",
#todo: combine all slp frames to one single png, much more efficient loading and editing possible.. </s> else:	get_pcolor_for_player return 16 * player + self.base_color
# todo: add type and value checkings </s> self._pinhole_ref = none  # to be filled later	DepthWarper self.height = height self._pinholes = pinholes def compute_homographies(self, pinhole, scale): pinhole_ref = scale_pinhole(pinhole, scale)
# todo: abstract this away into a function. </s> if vi_cmd_data['_mark_groups_for_gluing']:	eval_full_command return vi_cmd_data = self.parse_action(vi_cmd_data) self.view.run_command('maybe_mark_undo_groups_for_gluing') self.view.run_command('vi_run', vi_cmd_data)
# todo: accept external hrefs </s> elif item[0] == cairo.path_line_to:	point_following_path for item in path: if item[0] == cairo.PATH_MOVE_TO: new_point = item[1] length = distance(
# todo: fetchable files, boot files, etc. </s> self.asserttrue(self.remote.remove_system("testsystem1",self.token))	test_97_remove_system def test_97_remove_system(self):
# todo: check windows does not modify \n to \r\n here </s> with self.assertraises(ioerror):	test_truncate_fails def test_truncate_fails(self): with self.ro.write_bag_file("file.txt") as f: f.truncate(0)
#todo(wwolf) get correct value for these </s> version_node.appendchild(link_node)	VersionsXMLSerializer link_node = self.xml_doc.createElement('atom:link') link_node.setAttribute('rel', link['rel']) return version_node def default(self, data):
# todo: we can't wait for this, since the loop is running with .loop_forever() </s> }	Mqtt "no_auto_fg": True, "gas": None, mqtt.username_pw_set(_util.json_minimal(username)) headers = {
# :todo: implement test. </s> iota(self.adapter).sendtransfer,	SendTransferCommandTestCase def test_wireup(self): Verifies that the command is wired up correctly. SendTransferCommand,
# :todo: implement test. </s> self.skiptest('not implemented yet.')	GetTrytesResponseFilter filter_type = GetTrytesCommand(MockAdapter()).get_response_filter skip_value_check = True def test_pass_no_transactions(self): self.skipTest('Not implemented yet.')
#time = "todo" </s> f = open(out_file, "w")	create_JAMS fill_global_metadata(jam, lab_file) annot = jam.sections.create_annotation() json.dump(jam, f, indent=2) f.close()
# todo sync protocol </s> events.append(e)	on_window
# todo: refactoring </s> }	push@125 'key': { 'key': 'value', locale = kwargs.get('locale', 'en') file = kwargs.get('file', 'test.json')
# todo use all awcs, not just ones with data </s> def lang(self):	lang @property
# @todo: multisystem... </s> cpu['guest_nice'] = cputimes.guest_nice	__get_percpu if hasattr(cputimes, 'guest'): cpu['guest'] = cputimes.guest self.percpu_percent.append(cpu) self.timer_percpu = Timer(self.cached_time)
# todo: fixme! this throws an index out of range exception </s> inf.setname(name)	set_info def set_info(name, desc): inf = info.info() inf.setDesc(desc) kb.kb.append(self, 'http_vs_https_dist', inf)
# todo: add auto_detect_types=true parameter </s> if name != 'field'])	detect_field_types columns = zip(*sample_rows) assert len(columns) == len(field_names) none_type = set([type(None)]) detected_types = OrderedDict([(field_name, None)
# todo: implement this method </s> def setup_kickstart(self, data):	setup_kickstart
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
# todo fix. </s> self.__save_failing_context(ctx_init)	test_cmova x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) cmp_result = self.__compare_contexts(ctx_init, x86_ctx_out, reil_ctx_out) self.assertTrue(cmp_result, self.__print_contexts(ctx_init, x86_ctx_out, reil_ctx_out))
# todo debug </s> raise valueerror("no valid value for 'time' attribute "	parseRuleRecursively ruleWeekdayNew.time = str(weekdayItem.attrib["time"]) if (ruleWeekdayNew.time != "local" and + "in weekday tag.") ruleWeekdayNew.weekday = int(weekdayItem.attrib[
# todo(b/158462888): use aggregete losses that works with replicas. </s> step=self.train_step_counter)	kl_cutoff_loss tf.compat.v2.summary.scalar( name='kl_cutoff_loss', return tf.identity(kl_cutoff_loss, name='kl_cutoff_loss')
# todo private access </s> return valueset.from_sets(p.infer() for p in self._executed_param_names)	DynamicExecutedParamName if allowed:
raise exceptions.mpdnotimplemented  # todo </s> def mount(context, path, uri):	mount@19 *musicpd.org, mounts and neighbors section:* ``mount {PATH} {URI}``
# todo : component handling </s> dagfn.getpath ( dagpath )	toMDagPath dagPath = MDagPath()
pass ## fixme: todo </s> self.assertequal([datetime.date(2013, 5, x) for x in range(6, 11)],	TestMiscUtils date1 = datetime.date(2013, 5, 6) date2 = datetime.date(2013, 5, 11) list(misc_utils.iter_dates(date1, date2))) self.assertEqual([], list(misc_utils.iter_dates(date2, date1)))
# xxx todo </s> self.kifastsystemcall )	__syscall event.debug.break_at( pid, module.resolve("KiIntSystemCall"), self.KiIntSystemCall )
# todo find the correct hbin </s> def parent(self):	parent
# :todo: implement test. </s> sendtransfercommand,	SendTransferCommandTestCase Verifies that the command is wired up correctly. self.assertIsInstance(
# todo change format of formatted_preds in qa (list of dicts) </s> dataset, tensor_names, baskets = processor.dataset_from_dicts(dicts, indices, rest_api_schema, return_baskets=true)	_create_datasets_chunkwise indices = [d[0] for d in chunk]
#todo check our balance </s> logging.info(f"approving {spender_name} ({spender_address}) to access our {token.name()} balance directly")	setup_allowance def setup_allowance(self, token: ERC20Token, spender_address: Address, spender_name: str): if not token.approve(spender_address): raise RuntimeError("Token approval failed!")
# todo(nnorwitz): enable test. </s> class('bling')]	testTemplateWithMultipleTemplateArgsMid self.assertEqual(1, len(result)) types = [Class('Foo'), self.assertEqual(Class('Bar', templated_types=types), result[0])
# todo: integrate this into emrjobrunner </s> except logfetcherror:	cat_all def cat_all(runner): try: cat_from_list(runner, runner.ls_all_logs_s3())
pass  # todo </s> assert self.train_started	train_finish_epoch def train_finish_epoch(self): assert len(self.forward_data_queue) == 0, "Not all forwardings were used?" assert self.is_forwarding_finished, "Forwarding not finished?"
# todo: fix arrows </s> if adjacency_mat.shape[0] != adjacency_mat.shape[1]:	set_data arrows = None node_coords = None raise ValueError("Adjacency matrix should be square.") num_nodes = adjacency_mat.shape[0]
# todo: migrate new article ids to oldrecipearticleredirect </s> model = 'article'	Migration class Migration(BasePublishableDataMigration): table = '%s_%s' % (app_label, model) publishable_uncommon_cols = {
# todo extend to nonbinary nodes </s> conditioned on the fixed state of boundary-condition nodes in the	raw_current_marbl @property current timestep.""" if self._raw_current_marbl is not None:
# todo: only works for ratios </s> self.assertequal(4, len(	test_to_json_delta self.assertEqual(1, len(json_object['variants'][0]['metrics'])) self.assertEqual(1, len(json_object['variants'][0]['metrics'][0]['subgroup_metrics'])) json_object['variants'][0]['metrics'][0]['subgroup_metrics'][0]['subgroups'][0]['statistics'])) self.assertEqual(1, len(
# todo: log exception </s> local = true	scan@71 def scan(filelist, conf=DEFAULTCONF): elif SSH: local = False
# todo: fix with stubber / before send event </s> def setup(self):	TestApiGateway super(TestApiGateway, self).setUp() self.region = 'us-west-2'
# todo refactor when recurse_differ supports list_differ </s> init function	mod_init return True
# todo: add random string. </s> except badzipfile:	get_infos try: with ZipFile(zip_path, "r") as archive: raise CuckooPackageError("Invalid Zip file")
# todo curves </s> last_args = none	hvcurveto @staticmethod def hvcurveto(args): if len(args) % 2 == 1: lastStraight = len(args) % 8 == 5
# todo(henry-nash): add implementation here. </s> try:	_assert_not_schema_downgrade def _assert_not_schema_downgrade(version=None): current_ver = int(six.text_type(get_db_version())) if int(version) < current_ver:
pass # todo </s> return sum(costfunc(d) for d in mydata.values())	_get_engine_costs @dv.remote(block=True)
pass #todo: show multi select menu </s> super(selectiontool, self).end()	SelectionTool game.main.onEscape = game.main.showPause def end(self): def mouseDragged(self, evt): if evt.getButton() == fife.MouseEvent.LEFT and hasattr(self, 'select_begin'):
# todo rename "search_string" argument "pattern" </s> s.a -= 1	_vi_gq if s.b > s.a and view.substr(s.b - 1) == '\n': s.b -= 1 return s if mode in (VISUAL, VISUAL_LINE):
# todo: remove in v2.7 </s> filter |= q(device__virtual_chassis=self.virtual_chassis, mgmt_only=false)	vc_interfaces if self.virtual_chassis and self.virtual_chassis.master == self:
# todo: use transe embeddings for initialization.. </s> conv_out = self.conv(conv_inp).view(-1, self.embedding_dim * self.num_filters)	ConvKB r = self.relation_embeddings(batch[:, 1]) t = self.entity_embeddings(batch[:, 2]) hidden = self.relu(conv_out) hidden = self.hidden_dropout(hidden)
#todo this should get extracted somewhere </s> if not token.approve(spender_address):	setup_allowance def setup_allowance(self, token: ERC20Token, spender_address: Address, spender_name: str): if token.allowance_of(self.our_address, spender_address) < Wad(2 ** 128 - 1): raise RuntimeError("Token approval failed!")
# todo: real error handling </s> storage.mount_filesystems()	turn_on_filesystems storage.turn_on_swap()
# todo: i should make sure to escape single quotes here </s> def send_null(self, key, safe=false):	send_null
# todo link parameters </s> min_num_of_features = 5,	_extract_eomi lrgraph = self.lrgraph, stems = self._stems, verbose = self.verbose, logpath = None
# todo: this type conversion seems to be bottle neck </s> return q_values	QFunc q_values = v_values + (advantages - tf.reduce_mean(advantages, axis=1, keepdims=True)) else:
# todo print out profile changes </s> current_time - server.device_check_timestamp > 600):	matrix_timer_cb if not server.next_batch: return W.WEECHAT_RC_OK W.prnt(server.server_buffer, "{prefix}matrix: Querying user devices.".format(
# todo: will implement along with test_fuzzy_find </s> self.assertequals(none, actual_date_str)	test_parser_date actual_date_str = receipt4.parse_date() print(actual_date_str) receipt5 = Receipt(self.config, ["01.55.2016\n"]) actual_date_str = receipt5.parse_date()
# todo: the following skipped suite and fixtures should be enabled </s> @pytest.fixture(autouse=true)	VultrProviderTests domain = 'capsulecd.com' def _filter_headers(self): def skip_suite(self, request): if request.node.get_marker('ext_suite_1'):
# todo ... </s> return false	cpreprocess_evaluate_ifdef state.error("preprocessor: '" + arg + "' is not a valid macro name")
# todo: this is untested. </s> pass	_VerifyHelper try: _raise_current_error() raise self._problems.pop(0)
# todo move away </s> wildcards = [ self.read.sequence[self.rstart + i] for i in range(self.length)	AdapterMatch ATNGNA matches ATCGTA, then the string 'CT' is returned. If there are indels, this is not reliable as the full alignment if self.adapter.sequence[self.astart + i] == wildcard_char and self.rstart + i < len(self.read.sequence) ] return ''.join(wildcards)
# todo: refactor exceptioncache to be usable by multiple indexers. </s> logger.log('unable to update scene exceptions for {0}. error: {1}'.format	_get_custom_exceptions ['identifier']][indexer_id][scene_season]] custom_exceptions[indexer][indexer_id] = alias_list (indexer, error), logger.ERROR) continue
# todo: sort </s> class httpdbactive(httpdb, dbactive):	HttpDBActive
# todo: do this automatically when using the `+` operator on dataoprecords. </s> self.preprocessor = preprocessorstack.from_spec(preprocessor_spec)	ActorComponent exploration_spec (Union[dict,Exploration]): A specification dict for an Exploration object or an Exploration object directly. self.policy = Policy.from_spec(policy_spec) self.num_nn_inputs = self.policy.neural_network.num_inputs
# todo: add broadcasting to get_rotation_matrix2d for center </s> 'angle={0}, center={1})' \	Rotate return self.__class__.__name__ + '(' \
self._data[arraytype.mxnet] = mxnet.ndarray.array(nparray, ctx=mxnet.gpu(0)) # todo on which device ? </s> return array._ns.subtract(other, self)	__isub__
# todo improve tests - read the output more thoroughly </s> for key in ret.keys():	test_imachine_object_default self.assertIsNotNone(expected_attributes, "%s is unknown")
#todo: check the data! </s> self.assertequal(count, 4)	test_feed for i in p: count += 1
# todo: move to base class </s> n_rect = qtcore.qrectf(n.scenepos(), qtcore.qpointf(n.scenepos().x() + float(n.w), n.scenepos().y() + float(n.h)))	getNodesRect if activeGraphOnly: if n._rawNode.graph() != self.graphManager.activeGraph(): rectangles.append([n_rect.x(), n_rect.y(), n_rect.bottomRight().x(), n_rect.bottomRight().y()]) arr1 = [i[0] for i in rectangles]
# todo: error logging </s> urldata = urlparse.urlparse(url)	connect_endpoint endpoint = urldata.path conn_class = self.conn.get_endpoint(endpoint)
# todo: expose from marshal </s> except keyerror:	_bytes_from_bytecode raise ImportError("bytecode is stale for {}".format(fullname)) try: pass else:
raise notimplementederror # the below does most probably not work anymore todo </s> return "`{0}` is ahead of `{1}` by {2} commits:\n{3}".format(a,b,len(b_to_a),"\n".join(b_to_a))	detail if not a_to_b and not b_to_a: return "It does not differ from `{0}`.".format(b) elif not b_to_a: return "`{0}` is ahead of `{1}` by {2} commits:\n{3}".format(b,a,len(a_to_b),"\n".join(a_to_b))
# todo(joshblum): workflow_step slugs may not be unique across </s> task.assignments.count() * (iteration_duration + pickup_delay))	_new_assignment_start_datetime def _new_assignment_start_datetime(task): return task.start_datetime + previous_assignment_duration + PICKUP_DELAY
# todo: make this pretty </s> name = utils.get_directory_name(jobs[0])	detail def detail(request, uuid): is_waiting = jobs.filter(currentstep='Awaiting decision').count() > 0 return render(request, 'transfer/detail.html', locals())
# todo: not all messages have running status </s> dbg('    {}'.format(message))	_read_sysex if data[-1] == 0xf7: data = data[:-1] return message
#todo : parse unit  25, 25m, 25 ft, etc. </s> api = overpy.overpass()	OSM_QUERY self.report({'ERROR'}, "Too large extent") return {'FINISHED'} query = queryBuilder(bbox, tags=list(self.filterTags), types=list(self.featureType), format='xml') print(query)
# todo: replace with "yield from" when dropping python 2. </s> if isinstance(stream_, list):	ard_mediathek for stream in media["_mediaStreamArray"]: server = stream.get("_server", "").strip() if not stream_: continue
# todo: assign a suitable letter </s> else:	create_span attributes=None, id=None): if attributes is None: attributes =  json_loads(attributes) for attr in attributes:
# todo -- this block is repeated in lots of places, refactor </s> else:	_run_task self.callbacks.on_ok(host) if not host in self.invocations: self.invocations[host] = self.invocations[host] + 1 if results.get('changed', False):
# :todo: implement test. </s> def test_fail_transactions_null(self):	GetInclusionStatesRequestFilterTestCase 'foo': [f.FilterMapper.CODE_EXTRA_KEY], }, self.skipTest('Not implemented yet.') def test_fail_transactions_wrong_type(self):
# todo: make targetadaptor return a 'sources' field with an empty snapshot instead of </s> name = 'lint-v2'	Lint
# todo(datapipe-1509|abrar): currently we have </s> return none	process_bind_param if value is None:
# todo: implement </s> get the shipping address from the request. this abstracts the fact that users	get_shipping_address_from_request@19 def get_shipping_address_from_request(request):
# todo: may be in the future, add a check here to ensure that we are not over-writing any existing file. </s> def execute_adb_shell_command(adb_prefix, adb_command, piped_into_cmd=none):	execute_adb_shell_command
#todo: assuming constant mu </s> def me(self):	Me if getattr(self, '_Me', None) is None: self._Me = self.mesh.getEdgeInnerProduct()
transform = self.affine  # todo </s> self.closed and 'closed' or 'open', self.name, self.mode)	BufferedDatasetWriter otherwise support updates, such as JPEG. def __repr__(self):
# todo: rewrite variables with env. variables ( current implementation not final ) </s> def catch_shutdown():	catch_shutdown self.stop()
max_rates, intercepts = none, none  # todo: determine from gain & bias </s> if (ens.n_eval_points is not none	gen_eval_points n_points = default_n_eval_points(ens.n_neurons, ens.dimensions) eval_points = eval_points.sample(n_points, ens.dimensions, rng) and eval_points.shape[0] != ens.n_eval_points): warnings.warn("Number of eval_points doesn't match "
# todo: return errors in a universal way </s> return (str) - the asm output	compile_code def compile_code(source): lexer = Lexer(token_kinds.symbol_kinds, token_kinds.keyword_kinds) token_list = lexer.tokenize(source)
# todo: need to update this so that it flushes bulk queue </s> "mappings":{}	ElasticsearchClient else: mapping = ''' }''' self.es_connection.indices.create(index=index_name, update_all_types='true', body=mapping)
# todo: update the review with a message </s> self.assertequal(self._countphabworkingbranches(), total)	_phabUpdateWithExpectations runCommands("git fetch origin -p") processUpdatedRepo(self.conduit, "phab", "origin") if bad is not None: self.assertEqual(self._countPhabBadWorkingBranches(), bad)
# todo support going from customer encryption to other forms </s> def notice(message):	notice
bufferview = gltf.data.buffer_views[accessor.buffer_view] # todo initialize with 0 when not present! </s> offset       = sparse.indices.byte_offset	get_data_from_sparse def get_data_from_sparse(gltf, sparse, type_, type_val=None): if type_ == "indices": component_nb = gltf.component_nb_dict['SCALAR'] elif type_ == "values":
#todo: this isn't actually most_recently_used (as defined in histories) </s> 'name'  : < currently only returns 'file' >,	_summary_hda_dict Returns a dictionary based on the HDA in .. _summary form:: { 'type'  : < name of the dataset >, 'url'   : < api url to retrieve this datasets full data >,
# todo(py3.7): add required=true </s> def test_build(self):	test_build @synthesis_test
# todo results from ml </s> def _get_name(endpoint):	_get_name @staticmethod
# todo improve tests - read the output more thoroughly </s> interface = "imachine"	test_imachine_object_default imachine = XpcomConversionTests._mock_xpcom_object(interface) ret = vb_xpcom_to_attribute_dict(imachine, interface_name=interface)
'units': '1',  # todo: where does this come from??? </s> start_time=to_datetime(sources.time.values[0]).strftime('%y%m%d%h%m%s%f'),	get_filename def get_filename(path_template, index, sources): end_time=to_datetime(sources.time.values[-1]).strftime('%Y%m%d%H%M%S%f')))
#todo: check the data! </s> count += 1	test_feed p = pipe2py.compile.parse_and_build_pipe(pipe_def, verbose=True) count = 0 self.assertTrue("the" in i.get('description')) self.assertEqual(count, 4)
# todo: copy icons into directory </s> if _is_kernel_dir(pjoin(dir, f))}	_list_kernels_in if not os.path.isdir(dir): os.makedirs(dir, mode=0o644)
pass # todo </s> pass # todo	test_data_relation_without_version def test_data_relation_without_version(self):
# todo(dcramer): deal with case when the user cannot create orgs </s> if organization is none:	HomeView class HomeView(BaseView): def get(self, request): url = reverse('sentry-create-organization') else:
# todo(b/142684737): the multi-processing api might change. </s> pusher = pusher(	_create_pipeline@169 model=trainer.outputs['model'], baseline_model=model_resolver.outputs['model'], model=trainer.outputs['model'], model_blessing=model_analyzer.outputs['blessing'],
# todo: why is this late imported </s> @param dirname: the directory whose contents should be deleted.	delete_dir_contents def delete_dir_contents(dirname): for node in os.listdir(dirname): node_fullpath = os.path.join(dirname, node)
pass # todo </s> def collect_incoming_data(self, data):	collect_incoming_data
# todo: test logging messages. </s> self.messages = messages	MockTransport def __init__(self, states, messages): self.ephemeral = False def startup(self): self.states.append('running')
a = 0 #todo wrong </s> "relu": nn.relu(),	activations "gelu": nn.GELU(), "lrelu": nn.LeakyReLU(0.2), "relu6": nn.ReLU6(), "selu": nn.SELU(),
# todo: remove when #980 has been merged </s> if len(video_description) == 0:	CinemassacreIE webpage, u'title') video_description = self._html_search_regex(r'<div class="entry-content">(?P<description>.+?)</div>', video_description = None playerdata = self._download_webpage(playerdata_url, video_id)
# todo: handle agg_columns. </s> def idxmin(self, skipna=true):	idxmin
# todo: build url with python </s> messages.error(request, "csv import file does not exist. check config or provide file!")	config_check_cron@138 stop_system_importer_file_csv_cronbased = True else: stop_system_importer_file_csv_cronbased = True else:
# assume ethernet, todo: infiniband, wifi, vlan </s> retval[key] = value	bond_options_ksdata_to_dbus key, _sep, value = option.partition("=")
return 0  # todo: aria2 doesn't provide this information </s> def realid(self):	realid return self._gid
# todo check transformation to the reference element </s> self.equation.evaluate(dw_mode="vector", asm_obj=b, diff_var=none, u=u2[:, :])	RK3Solver u3[:, 0] = self.boundary_cond["left"] u3[:, -1] = self.boundary_cond["right"] u3[0, 1:-1] = u[0, 1:-1, it - 1] / 3 \ + 2*u2[0, 1:-1] / 3 \
# todo: need to cleanup the named argument mess before it is possible. </s> operands = [ constant.getconstant() for constant in operands ]	_optimizeConstantOperandsOperation def simulate(): return node.getSimulator()( *operands )
# @todo: save the results for the onaccept </s> try:	InvItemVirtualFields "pack_value", ] v = self.inv_inv_item.quantity * self.inv_inv_item.pack_value return v
#                assert false # todo </s> self.outputdatapath.setvalue( outputpath )	OpBatchIo self._internalPath = '/volume/data' self.OutputDataPath.setValue( outputPath + self._internalPath ) elif formatId == ExportFormat.Tiff: self.OutputDataPath.setValue( outputPath )
# todo: fix with stubber / before send event </s> sent_request = _send.call_args[0][0]	TestApiGateway _send.return_value = mock.Mock( status_code=200, headers={}, content=b'{}') self.assertEqual(sent_request.method, 'GET') self.assertEqual(
# todo: move 'vote-%d.%d.%s' to settings or something </s> kwargs['allow_anonymous'] = true	AnonymousRatingField class AnonymousRatingField(RatingField): super(AnonymousRatingField, self).__init__(*args, **kwargs)
# todo: check that the birth date is not in the future </s> day = int(number[4:6])	_get_birth_date def _get_birth_date(number): year = 1900 + int(number[0:2]) if len(number) == 9: if year >= 1980:
return -1  # todo: followup after decision around returning none </s> def file_name_with_device(self) -> str:	_FILE_OBJECT name = "" if self._context.memory[self.vol.layer_name].is_valid(self.DeviceObject):
# todo: check error location </s> return {	get_fields 'test': GraphQLField(test_type), 'nest': GraphQLField(DataType(), resolver=lambda *_: Data())
# todo: test for the _correct_ revision_id value. </s> if detail.object_id == package_created['id']:	test_create_package for detail in new_details: assert detail.activity_id == activity.id, str(detail.activity_id) assert detail.object_type == "Package", str(detail.object_type) elif detail.object_id == package_created['resources'][0]['id']:
pass # todo </s> def collect_incoming_data(self, data):	collect_incoming_data
# todo(b/145514490): this is a bit heavy handed, there maybe caches where </s> def type_signature(self):	type_signature return self._type_spec
# @todo: test </s> self.fvsnapshot = none	FittingView self.Bind(wx.EVT_RIGHT_DOWN, self.scheduleMenu) self.SetDropTarget(FittingViewDrop(self.swapItems)) self.itemCount = 0 self.itemRect = 0
# todo(soren): we need this until we can stop polling in the rpc code </s> 'dns_name': '10.0.0%s' % num,	test_instance_update_state 'instance_id': 'i-%s' % num, 'image_id': 'ami-%s' % num, 'ami_launch_index': str(num), 'instance_type': 'fake',
# todo: can't do this until we find a way to link with the </s> __visit_name__ = 'integer'	INTEGER
# todo: should also assert that the task is in the expected state once that's hooked up </s> assert len(logger.handlers) == 1	test_get_event_logger_add_unique_handlers Kubernetes Taskevent logger, to prevent duplicate log output. logger = mock_kubernetes_task.get_event_logger()
# todo(neuberg): this will need to be adapted against an already trained weight </s> "iters": validation_iters,	parse_logs "iters": training_iters, "loss": training_loss "loss": validation_loss
# todo: pytest.warns is not supported until pytest >= 2.8.0, whose </s> def bar(self):	bar @rpc
# todo: determine if this puts the case properties in the expected order. </s> existing_subcases = {c.name:c for c in form.actions.subcases}	Command question_dict = {q["value"]:FormQuestion.wrap(q) for q in form.get_questions(["en"])} question_ids = {"/data/" + q for q in conf["questions"]}.intersection(question_dict.keys()) for question in questions: for option in question.options:
# todo a more reliable way of getting the windows location </s> return "\n".join(installed)	installed_conda if stderr: return "Could not get package list"
# todo: add option for attentive reader </s> questions = set(questions)	model if isinstance(answers, str): answers = (answers,) candidates = set(candidates) answers = set(answers)
# todo: do the computation without the 'sr' enforcement </s> input:	_del_derived def _del_derived(self, del_restrictions=True): r""" - ``del_restrictions`` -- (default: ``True``) determines whether the restrictions of ``self`` to subdomains are deleted.
# todo(jheek): consider re-introducing the tracer check </s> return iter(self._frames)	CallStack finally: self._frames.pop(-1) def __len__(self): return len(self._frames)
# todo implement through browser </s> driver = browser.get_browser()	delete_cookie Parameters: name: value cookie = driver.get_cookie(name) if not cookie:
assert value == '' or value.isdigit(), 'bad call'  # todo remove assertion </s> return get_session_view_value(view, 'motion_count', '')	get_motion_count
# todo: add the rest of the api actions here and call them directly from the api controller </s> def __init__(	VisualizationsService class VisualizationsService(ServiceBase): Provides the logic of the actions invoked by API controllers and uses type definitions self, security: IdEncodingHelper,
# todo: test on linux, assuming same as macos right now </s> elif isinstance(value, dict) and isinstance(add_to[key], dict):	updateDict add_to[key] = copy.deepcopy(value)
# (@todo: add a js i18n formatter for the tooltips) </s> if feed is none:	S3ExportPOI references=["location_id"]) if update_feed: ftable.insert(location_id = lx, tablename = tablename,
@jtu.skip_on_devices("tpu")  # todo(phawkins): re-enable </s> self.asserttrue(onp.all(samples < hi))	testRngRandint compiled_samples = crand(key) for samples in [uncompiled_samples, compiled_samples]: self._CheckKolmogorovSmirnovCDF(samples, scipy.stats.randint(lo, hi).cdf)
#! todo: this needs to be made more intelligent </s> =====	bode def bode(sys, omega=None, dB=False, Hz=False): (magh, phaseh) = bode(sys, omega=None, dB=False, Hz=False) Plots a Bode plot for the system over a (optional) frequency range.
annot.annotation_metadata.validation_and_reliability = "todo" #todo </s> for possible_annot in xrange(3):	create_JAMS return jam = jams.Jams() if os.path.isfile(os.path.join(path, "textfile" + str(possible_annot+1) + ".txt")):
# todo: common crud method </s> nthash = hashlib.new('md4', cleartext.encode('utf-16le')).digest()	nt_password return binascii.hexlify(nthash).decode().upper()
# todo(b/110096942): more efficient gather </s> assert not env	parallel_callable pvals = [PartialVal((aval, core.unit)) for aval in avals] with core.new_master(JaxprTrace, True) as master: compiled, _ = compile_replicated(jaxpr, axis_name, axis_size, consts, *avals) del master, consts, jaxpr, env
# todo(jflesch): i18n/l10n </s> full_suggestions.sort()	_update_results full_suggestions = [] for suggestion in suggestions: for suggestion in full_suggestions: self.liststoreSuggestion.append([suggestion])
# @todo: threadpool().map() </s> if name not in deps:	get_aur_pkg_deps_and_version_matchers deps: Dict[str, VersionMatcher] = {} for dep in (aur_pkg.depends or []) + (aur_pkg.makedepends or []) + (aur_pkg.checkdepends or []): deps[name] = version_matcher else:
# todoc: i have no clue what this is doing </s> value += (byte_value << (8*byte_shift))	toInt byte_shift = 5-i byte = self._value[i] return value
# todo curves </s> it = _everyn(args, 4)	hvcurveto if len(args) % 2 == 1: lastStraight = len(args) % 8 == 5 try: while True:
# todo @chris ... </s> ... (like in lmdataset.phoneseqgenerator)	hmm_fsa_for_word_seq :param list[str] word_seq: sequences of words
# todo: posts_per_year is global, kill it </s> "messages": self.site.messages,	Archive name = "render_archive" def gen_tasks(self): "translations": self.site.config['TRANSLATIONS'], "output_folder": self.site.config['OUTPUT_FOLDER'],
# todo: update user icon on button to user avatar </s> self.setpagetext(index, newname)	renameRoutinePage
# todo ??? other type of cases </s> pep440_version()	test_pep440_version_default
# todo: log exception </s> return none	scan@71 try: output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"]) output = output.decode("utf-8", errors='replace') virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE)
# todo: should export bytesio as stringio in libcloud.utils.py3 </s> (compression_type))	decompress_data@33 return gzip.GzipFile(fileobj=cls(data)).read() else:
# todo delay </s> except exception as e:	sonosite_upload@77 response_data = {} try: response_data['result'] = 'failed' response_data['message'] = 'Could not load config file: %s' % (e.message)
"size": 50  # todo: support pagination. </s> self.user = none	ProjectIndex self.user = get_object_or_404(User, username=self.kwargs.get('username')) queryset = queryset.filter(user=self.user) if self.kwargs.get('tag'): self.tag = get_object_or_404(Tag, slug=self.kwargs.get('tag'))
# todo remove backwards compatability patch in version 2.0 </s> settings.set('inverse_caret_state', false)	_cleanup_views for view in window.views(): settings = view.settings() settings.erase('vintage')
# todo: use ctx.send_help() once pr #519 is merged. </s> except commanderror as tag_error:	ErrorHandler@127 if not await tags_get_command.can_run(ctx): log.debug(log_msg) log.debug(log_msg) await self.on_command_error(ctx, tag_error)
# todo: if an attachment is filtered, the score is not complete </s> for i in details:	check_urls def check_urls(urls, keywords): if swt(i["url"], keywords): return True
# todo simplify </s> trained_model (baseestimator): a scikit-learn trained algorithm	metrics Args:
# todo: add 3ph sgens </s> ppc["bus"][disco, vm] = np.nan	_set_buses_out_of_service def _set_buses_out_of_service(ppc): ppc["bus"][disco, VA] = np.nan ppc["bus"][disco, PD] = 0
# todo ensure that if you try to filter on an invalid field, it returns a useful error. </s> elif value in self.falsy:	convert_value value = value.strip() if value in self.TRUTHY: return False elif value == 'me' and not self.request.user.is_anonymous():
# todo: move the inside snippets to the corresponding snippets dict </s> elif 'type-value' in args:	expand_value if ret == '' and 'default-value' in args: return '[{0}]'.format(args['default-value']) return str(args['type-value']) return args.get('keyword-value', '')
# todo: this is temporary until the function argument list is refactored to work with kwargs only. </s> max_intensity (int): maximum intensity of the heatmap. defaults to 1.	heatmap gradient (list of (int, int, int, float)): Color gradient of the heatmap, as a list of `RGBA`_ colors. The color order defines the gradient moving towards the center of a point. dissipating (bool): True to dissipate the heatmap on zooming, False to disable dissipation. precision (int): Number of digits after the decimal to round to for lat/lng values. Defaults to 6.
## slightly different api - todo test if this works </s> with javascript:	setSteering self[...].setSteering( amount, wheel[...] )
# todo(dcramer): ideally we could just send the signal to the subprocess </s> ))	save_chunk text=text, offset=self.cur_offset, db.session.commit() self.cur_offset += text_len
# todo: for now, circumnavigate the detached head issue. </s> eq_(list(target.git_get_branch_commits("git-annex")),	test_publish_simple@42 ok_clean_git(dst_path, annex=False) eq_(list(target.git_get_branch_commits("master")), list(source.repo.git_get_branch_commits("git-annex")))
# todo(b/184055743): once tensorflow is released with </s> inputs['x_center'] = inputs['x'] - tft.mean(inputs['x'])	preprocessing_fn def preprocessing_fn(inputs):
raise notimplementederror # todo </s> return open(filename, 'rb')	file_opener def file_opener(filename): return _opener
# todo: adjust edges </s> xx, yy = np.meshgrid(xx, yy)	_get_meshgrid def _get_meshgrid(self, xmin, xmax, ymin, ymax, step=100): xx = np.linspace(xmin, xmax, step) return xx, yy
if not config.testnet:  # todo </s> data += struct.pack(curr_format, binascii.unhexlify(contract_id), gasprice, startgas, value, payload)	compose@17 curr_format = FORMAT + '{}s'.format(len(payload))
pass  # todo </s> pass  # todo	RivalMouse def _value_type_choice(self, value): pass  # TODO def _value_type_none(self): pass  # TODO
# todo add something like this in the future, its cleaner than the </s> for star_module in self.star_imports():	iter_star_filters def iter_star_filters(self, search_global=False):
# todo support multiple backends </s> currently stored playlists.	StoredPlaylistsController self.core = core @property Read/write. List of :class:`mopidy.models.Playlist`. futures = [backend.stored_playlists.playlists
### todo put memozation here </s> fq2_cmd = fq2_cmd.format(fq2=fq2)	rapmap_pseudoalign cmd += "-r {fq1_cmd} " else: cmd += "-1 {fq2_cmd} -2 {fq2_cmd} " with file_transaction(out_file) as tx_out_file:
#set limit to 25 --> currently hardcoded --> todo: add a setting for this ? </s> cursor.execute(setartsql,(setid,"set",update_type,artwork[update_type]))	addBoxsetToKodiLibrary setartsql = "INSERT INTO art(media_id, media_type, type, url) VALUES(?,?,?,?)"
# todo: the defs should be util or elsewhere... </s> return numpy.arccos(	angle numpy.dot(norm1, norm2) / (numpy.linalg.norm(norm1)*numpy.linalg.norm(norm2)))
# todo: context manager </s> )	ships ship['station']['name'], str(ship['value']['total']) return out return [
# todo: disconnect </s> def listen_maddr_with_peer_id(self) -> multiaddr:	listen_maddr_with_peer_id return self.listen_maddr.encapsulate(Multiaddr(f"/p2p/{self.peer_id}"))
walk_related=false,  # todo i'm not sure what this should be </s> def occurrence(self):	occurrence return get_occurrence_case_structure( self.occurrence_id,
# todo: i am not at all sure why we need to </s> ( "disjuncts", "active" ),	_pprint ("Active", self.active), ], lambda k, v: [ [x.name for x in v.disjuncts], v.active, ]
# todo: only do this if needed (depending on the storage backend the whole file will be downloaded) </s> src_file = src_storage.open(src_file_name)	_move_file self.is_public = not self.is_public self.file.delete_thumbnails() src_file.open() self.file = dst_storage.save(dst_file_name,
# todo: remove this logging statement when all other todos </s> self._name = minfo['name'] if 'name' in minfo else none	BattleshipTransaction minfo = {} super(BattleshipTransaction, self).__init__(minfo) self._action = minfo['Action'] if 'Action' in minfo else None def __str__(self):
#@todo: remove in 0.4.10 </s> def downloadfinished(self, pyfile):	downloadFinished
# todo consolidate this and pr plotter into 1 function </s> print('\nfeature importances saved in: {}'.format(source_path))	plot_random_forest_feature_importance if save: plt.savefig('FeatureImportances.png') plt.close(figure) else:
# todo: need to add counter </s> user_ids = self.convert_to_user_id(user_ids)	_get_user_ids def _get_user_ids(self, user_ids): return [user_ids] return [self.convert_to_user_id(user) for user in user_ids]
# todo: add highlighting line </s> self.functions_list.show()	update_functions for module_info_base in self.app.dwarf.database.modules_info: module_info = self.app.dwarf.database.modules_info[module_info_base] for function in module_info.functions: functions_list[function.name] = function.address
return  # todo return placeholder "[unavailable]" track? </s> name=name,	to_playlist uri=sp_playlist.link.uri,
# todo use libssl if available </s> return next(dc for dc in self.dc_options	_get_dc 'Cannot determine the required data center IP address. ' 'Stabilise a successful initial connection first.') if dc.id == dc_id and bool(dc.cdn) == cdn) except StopIteration:
# for ~otheruser/src.  todo: should this be cached? </s> try:	_EvalTildeSub@182 val = self.mem.GetVar('HOME') assert val.tag == value_e.Str, val e = pwd.getpwnam(prefix) except KeyError:
# todo model? </s> data_home = res.pick_and_save(	one_step cutter = VideoCutter() res = cutter.cut(video_path, compress_rate=compress_rate) stable, frame_count,
# logger ..." todo: this should be done before plugins discovery </s> may or may not be displayed as help topics.	add_plugin_args def add_plugin_args(self, plugins): for name, plugin_ep in plugins.iteritems(): parser_or_group = self.add_group(name, description=plugin_ep.description)
# todo ... </s> return ["-i", "/usr/local/cellar/pypy/1.9/include"]	get_python_ccopts def get_python_ccopts(): else: return [
# todo actions may contain wildcards, e.g. sts:* -- this can be solved by using policyuniverse to </s> pnode.attachmentcount = {attachment_count},	load_policies ON CREATE SET pnode.policyid = {POLICY_ID}, pnode.firstseen = timestamp(), pnode.createdate = {CREATE_DATE} SET pnode.name = {POLICY_NAME}, pnode.path = {PATH}, pnode.defaultversionid = {DEFAULT_VERSION_ID}, pnode.lastupdated = {aws_update_tag} WITH pnode
# todo: this is a jump. </s> return vi_cmd_data	vi_big_m vi_cmd_data['is_jump'] = True vi_cmd_data['motion']['command'] = 'vi_big_m'
# todo: set file creation mask to 640 </s> nut_configs = {nut_config: collections.ordereddict(),	pre_process_nut_config :param config: sanitized config dict from form entry :return: dict of OrderedDicts indexed by file ie multiple entries of NUT_UPS_CONFIG: collections.OrderedDict(), NUT_UPSD_CONFIG: collections.OrderedDict(),
# todo: add logging </s> fixcache[params] = result	fixlist result = fixcache[params] except KeyError: return result
#todo(#212): use a map construct instead of unrolling. </s> def convert_element_type_dtype_rule(operand, new_dtype, old_dtype):	convert_element_type_dtype_rule
# todo: why is this failing? </s> expected_json_output = file.read()	read_file_v1 def read_file_v1(self, state): filepath = "tests/expected_output/v1_{state}.json".format(state=state) return expected_json_output
raise skiptest("broken test")  # todo(mattjj): fix </s> jnp.full((n,), 0x243f6a88, jnp.uint32),	testThreefry2x32Large result = random.threefry_2x32( (np.uint32(0x13198a2e), np.uint32(0x03707344)), jnp.full((n,), 0x85a308d3, jnp.uint32) ]))
# todo(andym): delete once personas migration is live. </s> response = self.client_post(rev=['a592'], qs='?result_type=json')	test_ppal_return_url_not_relative assert json.loads(response.content)['url'].startswith('http')
return skiptest("test doesn't pass yet")  # todo(frostig) </s> expected = x + x.t	testTransposeAndAddRank3 def fun(x): return x + x.T pfun, axis_name = papply(fun, 2) ans = serial_pmap(pfun, axis_name)(x)
# todo add switch to make tarball/zip </s> if info is none:	BIDS2Scidata 'Comment[Data Record Accession]': repo_accession, 'Comment[Data Record URI]': repo_url}, yield dict( status='error',
# todo: i think this should use '$ fileregions' </s> if page_number < 1:	get_page_buffer logger.warning('unexpected page number requested: %d', page_number) offset = self.page_size * page_number
# todo: test bytearray </s> :param prefix: specify versionbyte prefix in hexstring or bytes. normally doesn't need to be specified, method uses default prefix from network settings	address_uncompressed def address_uncompressed(self, prefix=None): :type prefix: str, bytes :return str: Base58 encoded address
# todo: same as above, what if there's no splitfrac </s> sc(w, stk, depth, strngcomps)	sc stk.append(v) for w in adj[v]: back[v] = min(back[w], back[v]) elif w in stk:
# todo: remove when #980 has been merged </s> 'thumbnail': thumbnail,	ViddlerIE '_type': 'video', 'id': video_id, 'uploader': uploader, 'duration': duration,
# todo: refactor linodeexception, args[0] should be error_id </s> data=item['target'], extra=extra, zone=zone,	_to_record 'port': item['PORT'], 'weight': item['WEIGHT']} type = self._string_to_record_type(item['TYPE']) driver=self) return record
blockchain.connect()  # todo: leave this here? </s> staking_address,	stake@91 registry_filepath, provider_uri, worker_address, withdraw_address,
# todo support for keccack </s> evm['opcodes'] = data['opcodes_runtime']	format_to_output_dict if 'bytecode_runtime' in data: evm['object'] = data['bytecode_runtime'] if 'source_map' in data: evm['sourceMap'] = data['source_map']['pc_pos_map_compressed']
# todo: parallelize this loop </s> blockfilepath = os.path.join( blockfilepath, "{}_{:08d}".format( axis, start ) )	getDatasetDirectory def getDatasetDirectory( self, blockstart ): blockFilePath = os.path.split(self.descriptionFilePath)[0] return blockFilePath
# todo: handle "other" </s> might be 'image of a california sea hare').	Uri class Uri(PhyloElement): In general, this is expected to be an URL (for example, to link to an image def __init__(self, attributes, value=None): PhyloElement.__init__(self, attributes, value=value)
# todo: remove this logging statement when all other todos </s> logger.error("in check_valid, create is not fully implemented")	BattleshipTransaction raise BattleshipException('game already exists')
# todo _cphttptools.applyfilterlist('afterrequestbody') </s> self.socket.bind(self.server_address)	server_bind def server_bind(self):
#ack = self.serialport.read() # todo: use ack </s> print "handshake error. please reset the microcontroller."	performHandshake self.serialPort.write(frame) ack = self.serialPort.readline()
# todo: connect to agent code </s> vbox.addlayout(buttonhbox)	AIGameWindow vbox = QVBoxLayout() vbox.addLayout(imgHBox) mainWidget.setLayout(vbox) self.show()
# todo: may test file contents </s> def test_export_to_csv_fobj(self):	PluginCsvTestCase rows.export_to_csv(utils.table, temp.name) table = rows.import_from_csv(temp.name) temp = tempfile.NamedTemporaryFile(delete=False) self.files_to_delete.append(temp.name)
percentiles_to_calculate = range(0, 100, 1)  # todo: get input from user </s> else:	update_summary_stats for stat in self.summary_stats_list: if stat.startswith('p'): self.summary_stats[column][stat] = naarad.utils.normalize_float_for_display(self.calculated_stats[column][stat])
# todo: earlier warning in conf check ? </s> args=(self.machine_serial_number,)))	get_machine_url else: return "{}{}".format(tls_hostname.rstrip('/'),
# todo: move this wrapping logic into a common templatetag. </s> 'user_count': user.objects.count(),	dashboard Displays the administration dashboard, containing news updates and useful administration tasks. 'reviewgroup_count': Group.objects.count(), 'defaultreviewer_count': DefaultReviewer.objects.count(),
# todo: if clang will be extended with an extra analyzer option in </s> return false	contains_intrinsic_headers for f in os.listdir(include_dir): if f.endswith("intrin.h"):
# todo -- can we do this without a subscription? </s> return audio(speech_text).play(stream_url)	play_library first_song_id = queue.shuffle_mode(True) stream_url = api.get_stream_url(first_song_id)
# todo make sure this works </s> pass	createGroup
# todo: add some unit tests for this. </s> def _is_callable(obj):	_is_callable
# todo: add support for composite primary keys. </s> else:	_order_by column = getattr(self.model, attribute) if reverse: yield column.asc()
# todo: what about alpha (rgba)? </s> pass_name = output_name	_import_aov elif output_name.startswith("RADIANCE_GROUP"): pass_name = lightgroup_name blender_pass = render_layer.passes[pass_name] convert_func(width, height, buffer, blender_pass.as_pointer(), aov.normalize)
# todo: skipped due to gh-4436 </s> ds = dataset(path).create()	test_invalid_calls @with_tempfile assert_raises(TypeError, ds.create_sibling_ria) assert_raises(ValueError, ds.create_sibling_ria, 'ria+file:///some/where',
# todo(anjalisridhar): you can pass test name and sample type when creating </s> datagen.fit(x_train)	benchmarkCifar10Cnn height_shift_range=0.1,  # randomly shift images vertically (fraction of total height) horizontal_flip=True,  # randomly flip images model.fit_generator(datagen.flow(x_train, y_train, batch_size=self._batch_size),
# todo: decouple code generator. perhaps allow  pydy and/or pyodesys </s> if states is none: # or other checks?	states @states.setter states = sp.Matrix([]) self.n_states = len(states)
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
setattr(model, "require_backward_grad_sync", false)  # todo: needed? </s> if self.model is not none and self.model.trainer.state.fn != trainerfn.fitting:	_wrap_optimizers return optimizers return self._reinit_optimizers_with_oss(optimizers)
# todo fix. </s> self.__save_failing_context(ctx_init)	test_cmova x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) cmp_result = self.__compare_contexts(ctx_init, x86_ctx_out, reil_ctx_out) self.assertTrue(cmp_result, self.__print_contexts(ctx_init, x86_ctx_out, reil_ctx_out))
# todo remove? </s> def parent(self):	parent @property
# todo: if empty, add 'pass' </s> out = out[n:]	AligningWalker return elif n: break else:
# todo: switch _ignore_connection_reset for _ignore_transmission_error, or provide retry mechanism </s> @property	SessionInfo class SessionInfo(object): def __init__(self, db_filename): def procmon_results(self): return {1: 'procmon results not saved in current database format'}
match_mode = datawalker.repeat  # todo should be determined by modes of input walkers </s> if nesting == self._output_nesting:	what_is_next nesting = levelsOflist(self._stack[-1]) else: return DataWalker.VALUE else:  # todo add the case when next element has too less nested levels
# todo: do we require graphs with no nodes/edges to have the same schema?  currently </s> unbatch a batched graph	unbatch Unbatched list of graphs. Examples >>> import dgl >>> import torch as th
# todo: support aa </s> def untruncate(self, to: int) -> "activations":	Activations assert len(lh) == len(po) == len(ah) == len(aa) all_args = zip(lh, po, ah, aa) xp = self.xp raise NotImplementedError
# todo : documentation pending </s> return none	get_confirm_token for key, value in response.cookies.items(): if key.startswith('download_warning'):
# todo: implement </s> text = text.replace('>', '&gt;')	serialize_text def serialize_text(self, text, quot=False): text = text.replace('&', '&amp;') if quot: text = text.replace('"', '&quot;')
# todo ... </s> state.error("preprocessor: '" + arg + "' is not a valid macro name")	cpreprocess_evaluate_ifdef def cpreprocess_evaluate_ifdef(state, arg): arg = arg.strip() return False return arg in state.macros
#todo classes broken </s> elif(type == 'feature'):	GANWebServer sample_file = "sample.png" if(type == 'batch'): self.sample_iterate_z(sample_file, z_iterate, target_value, seed) elif(type == 'linear'):
#todo: add way to check if alt is pressed </s> if self.debug:	addDebugString def addDebugString(self, string):
# todo: shouldn't it take place only on paste? </s> if self.started_undo_blocks == 0:	undo_block_stop self.started_undo_blocks -= 1
# todo: test the size when the field is new </s> self.rt1.unset_field(radiotap.radiotap_flags)	test_07_unset_fields self.assertEqual(self.rt1.get_size(),len(self.frame_orig_1)) self.assertEqual(self.rt1.get_header_size(),24) self.assertEqual(self.rt1.get_size(),len(self.frame_orig_1)-1) self.assertEqual(self.rt1.get_header_size(),24-1)
raise notimplementederror #todo, implement! </s> legacyset = legacysetdefinition.parsexml(root)	SetDefinition root = tree.getroot() if root.tag != '{' + NSFOLIA + '}set': self.graph = rdflib.Graph() legacyset.rdf(self.graph)
# todo this is not tested yet. </s> t = np.double(t)+1e-35	cdf def cdf(t, a, b):
#todo: call _update_node_parents and _update_node_rule_for_parents </s> pass	_update_node_rule_for_parents
# todo(b/130724878): these conversions should not be needed. </s> attributes:	Obj @attr.s(cmp=False, frozen=False) model: A ModelWeights structure, containing Tensors or Variables. optimizer_state: A list of Tensors or Variables, in the order returned by
pass # todo </s> }	build_operation 'pixb_y': self.get_selection().get_future_coords()[1], 'local_dx': self.local_dx, return operation
# todo generator </s> gitrepo.is_valid_repo(ap['path']) and \	Drop yield ap continue not ap['path'] == refds_path: ap['process_content'] = True
# todo(skeen): think of a better solution </s> def __nonzero__(self):	__nonzero__
assert oldsize == self.size, '%s: %d, %d' % (self.type, oldsize, self.size) # todo: remove </s> def read_uint(stream):	read_uint
# todo: disconnect </s> return self.listen_maddr.encapsulate(multiaddr(f"/p2p/{self.peer_id}"))	listen_maddr_with_peer_id @property
# todo: if empty, add 'pass' </s> finally:	uncompyle_test uncompyle_find(2.7, co, 33)
# @todo: handle lama correctly </s> activation_fn=activation_fn,	Actor hiddens=[state_size] + hiddens, layer_fn=layer_fn, norm_fn=norm_fn, bias=bias,
ndpi = (96, 96) # todo: read real dpi </s> obj.identifier = newid	addobj def addobj(self, obj): self.objects.append(obj)
# :todo: implement test. </s> this does the same thing as generating a regular address from the	MultisigAddressBuilderTestCase ) def test_success_single_digest(self): corresponding key. self.skipTest('Not implemented yet.')
# todo: this should not be hard coded </s> raise lookuperror('%s not in config' % item)	getset_item return obj[item] except (TypeError, LookupError, ValueError):
# todo: write tests </s> "when there is an otherwise-unspecified validity error that prevents parsing."	MeiValidityError class MeiValidityError(exceptions21.Music21Exception):
return deserialize(self.binary, from_bytes=true)  # todo: techdebt fix </s> self.binary = serialize(value, to_bytes=true)  # todo: techdebt fix	BinObject@20 def object(self, value):
# todo: cache this value. same value should be used for future tests in this policy or other policies when handling this request </s> else:	authorize decisions.append(self.combined_policy_decision(requirement_request)) if all(x == decisions[0] for x in decisions): return self.create_response("indeterminate") return self.create_response(self.combined_policy_decision(request))
# todo: test 1: quando para a data no estado só tem a planilha de total, </s> row_dates = set()	row_with_sorted_columns for key in row.keys(): if not key.startswith("confirmados"):
# todo: is this right? </s> @rtype:  integer	num_mutations def num_mutations (self): @return: Number of mutated forms this primitive can take. return self.bit_field.num_mutations()
# todo allow multiple barriers to be executed </s> op = self._get_op(op)	reduce self._check_contiguous(out_array) stream = self._get_stream(stream) self._comm.reduce( in_array.data.ptr, out_array.data.ptr,
# todo: retry our one ping. we should not have to retry. </s> return {}	get_matching_flow_on_dpid if re.search(exp_flow, flow): return json.loads(flow)
# todo you should put some extra protection on this, so a user can only </s> msg = '{} says hello from {}'.format(username, ip)	non_fresh_protected username = jwt_identity  # Access identity through jwt_identity proxy
# todo (a8): add user to models </s> def obj_delete(self, request=none, **kwargs):	obj_delete
# todo(b/207464757): tf compilation is disabled </s> x = jax2tf.convert(jnp.sin)(1.)	test_argument_eager_tensor def test_argument_eager_tensor(self):
# todo(b/141131288): enable complex-valued sorts on tpu. </s> for rhs_dilation in [none, ()]	testConvTranspose0D for strides in [()] for padding in ["VALID", "SAME"] for rng_factory in [jtu.rand_small])) def testConvTranspose0D(self, lhs_shape, rhs_shape, dtype, strides,
# todo: check utf-8 </s> return fasl_integer_inner(b)	read_fasl_integer_stream def read_fasl_integer_stream(stream):
# todo: rename style -> parser </s> return 0	TextCursorShim class TextCursorShim(object): def setPosition(self, pos): pass
#todo: make more straightforward (somehow) </s> encoded_id = trans.security.encode_id( hda.id )	_summary_hda_dict 'url'   : < api url to retrieve this datasets full data >, } return { 'id'    : encoded_id,
# todo: remove linear scale to exponential scale </s> else:	get_progress days_till_due = p_todo.days_till_due() length = get_length() or 14 return 0
# todo: support this instead of failing. </s> self.do(d_a+u_a)	test_on_press_nonblocking def test_on_press_nonblocking(self):
raise notimplementederror # todo </s> def id(self):	id @property
# todo: drop non-callable keys in dramatiq v2. </s> return false	decr while True: value, cid = client.gets(key) value -= amount if value < minimum:
# todo: make not hardcoded </s> self.write(('notice', dest), text)	notice def notice(self, dest, text):
# todo: log exception </s> scan += '"' + item + '";'	scan@71 cmdline = conf["cmdline"] scan = '/SCAN=' cmdline.insert(0, conf["path"]) cmdline.append(scan)
# todo move to common? </s> return get_files(user_config.paths)	inputs
# todo add ruby personality. </s> source file is in source format, while target and template files use	convert_store target format. for template_unit in self.template_store.units:
#todo same issue with batch_size </s> tf_graph.create_generator(graph)	create_graph if graph_type == 'full': tf_graph.create(graph) else: raise Exception("Invalid graph type")
# todo: this is just for compatibility with </s> except (valueerror, assertionerror):	get_positive_int val = int(val) assert val >= 0 raise exc.HTTPBadRequest('Invalid %s: %s (should be positive int)' % (param, val))
# todo: don't resize image but change create_tiles. </s> return features, labels	extract_features_and_labels geotiff_path, shapefile_paths, tile_size) features += tiled_features
# todo: needs input cleansing and validation </s> payment_details = request.args['payment']	make_alice_control m, n = int(request.args['m']), int(request.args['n']) expiration_time = maya.MayaDT.from_iso8601( federated_only = True # const for now bob = Bob.from_public_keys({DecryptingPower: bob_pubkey,
# todo: i don't think we need these here because likes of to_parent </s> def any_dict_to_unicode(self, cls, value, **_):	any_dict_to_unicode
# todo: no unit tests cover any of this. </s> self.index = 0	pre_run def pre_run(self):
# todo: breaklines should be displayed correctly </s> print("published: " + rest)	toot @command def toot(mastodon, rest):
# todo: maybe we should do this in a thread </s> from freenasui.common.system import backup_database	Model ), ) def save(self, *args, **kwargs): try: backup_database()
# todo return somethign useful ;) </s> raise notimplementederror	score_po
# todo: use more broadly numpy </s> def getnbeegchannels(self):	getNbEEGChannels
# todo: maybe check if we are inside a classic snap and don't do </s> argv += ['--root-dir', self.root_dir]	NetplanGenerate return argv = [utils.get_generator_path()] if self.mapping: argv += ['--mapping', self.mapping]
return  # todo: update </s> setattr(field, key, value)	create_personal_data_fields for key, value in data.iteritems():
# todo: should wait for the end of the ongoing test case, and stop gracefully netmon and procmon </s> if not target.procmon.start_target():	restart_target self._fuzz_data_logger.log_info("restarting target process") if stop_first: return False time.sleep(3)
# todo: fix this, this is one of the few cases where using the config </s> return suite	_make_test_suite_loader except KeyboardInterrupt: raise exceptions.JobError('Command interrupted by user...') for i in range(len(suite)): suite[i] = [test.DryRunTest, suite[i][1]]
# todo remove </s> def get_full_url(self):	BaseModel except Exception as ex: logger.error("notify sipder", ex) site = Site.objects.get_current().domain url = "https://{site}{path}".format(site=site, path=self.get_absolute_url())
# todo: replace suite with testcases </s> try:	load_json_file def load_json_file(json_file): json_content = json.load(data_file) except exceptions.JSONDecodeError:
# todo: implement this </s> def get_max_memory(self):	get_max_memory
# todo: investigate why this happens </s> args.color       =  fill_list(args.color, [none], length=num_graphs)	fill_args args.width = [2] args.xcol = fill_list(args.xcol, [args.xcol[-1]], num_graphs) args.style       =  fill_list(args.style, length=num_graphs) args.marker      =  fill_list(args.marker, length=num_graphs)
# todo: remove this when domain decomposition is merged </s> subelement = et.subelement(element, key)	create_output_subelement if not self._output is None: element = ET.SubElement(self._settings_file, "output") subelement.text = str(self._output[key]).lower() self.create_output_path_subelement()
# todo: wobble </s> x, y, z = self.position.au	to_skycoord from astropy.units import au
# todo: this check is to maintain backwards compatibility with the old way of creating </s> def _form_to_db_schema(self, package_type=none):	_form_to_db_schema
# todo(blk-u): shouldn't need to clear the registry here, but some </s> auth.controllers.auth_methods.clear()	clear_auth_plugin_registry auth.controllers.AUTH_PLUGINS_LOADED = False
# :todo: implement test. </s> self.skiptest('not implemented yet.')	PrepareTransfersCommandTestCase ) def test_pass_inputs_not_needed(self): def test_pass_inputs_explicit(self): Preparing a bundle with specified inputs.
# todo: process </s> secret = hash.hexdigest()	getsecret if certificate: hash = hashlib.sha256() return secret
# todo: handle a possible deadlock more gracefully. </s> class sleeperthread(thread):	test_decorator def test_decorator(self): time1 = 0 def __init__(self, sleeptime): Thread.__init__(self)
"""todo: not implemented""" </s> >>> df = pd.dataframe({'x': ser})	n >>> ser = pd.Series([1,2,3]) >>> n(ser) >>> n(df) 3
# todo add locales </s> logger.success(m18n.n("main_domain_changed"))	domain_main_domain app_ssowatconf() if os.path.exists("/etc/yunohost/installed"):
# todo: this never checks in case one of the operations fails </s> return	make_dirs os.mkdir( trainpath+classname ); os.mkdir( testpath+classname );
# todo: check rootfs fs against parameter injection </s> fs = filesystem(rootfs)	autostart if script: fs.put(script, "/etc/init.d/S90_user_autostart", right=555)
# todo: probabilistic with each route </s> def max_speed(self):	max_speed
# todo: must be the same if we merged/pushed before, if not -- skip </s> none, check_installed=true, purpose='publishing')	Publish path = dataset.path if isinstance(dataset, Dataset) else dataset if not dataset and not path: if since and not dataset: raise InsufficientArgumentsError(
#todo verify input </s> for tag in all_tags:	get_all_tags all_tags = r_serv_tags.smembers('list_tags') list_tags = [] list_tags.append( tag ) id += 1
# todo: give a vanilla example </s> re[appliance] = matrix	confusion_matrices@176 np.max(ground_truth_states[appliance])+1]) for time in predicted_states[appliance]: return re
# todo operators </s> current_dir = os.path.dirname(os.path.realpath(__file__))	get_bin_directory return os.path.join(current_dir, "bin")
common_path=prefix,  # todo: add key? </s> def autoresolve_cells(base, decisions, strategies):	autoresolve_cells
# todo: check rootfs fs against parameter injection </s> print("put_content is not implented for {}".format(self.rootfs))	put_content
# todo: add logging </s> none if the block is a valid python codeblock.	codeblock_stripping def codeblock_stripping(self, msg: str): Strip msg in order to find Python code. if msg.count("\n") >= 3: if re.search("```(python|py)\n((?:.*\n*)+)```", msg, re.IGNORECASE):
# todo this makes self variables non-breakable. wanted? </s> :type     scope: :class:`parsing_representation.isscope`	_get_defined_names_for_position - If `position` is given, delete all names defined after `position`. - For special objects like instances, `position` is ignored and all :param    scope: Scope in which names are searched. :param position: The position as a line/column tuple, default is infinity.
# todo: for now, circumnavigate the detached head issue. </s> target = gitrepo(dst_path, create=true)	test_publish_simple@42 source = install(path=src_path, source=origin, recursive=True) for subds in source.get_dataset_handles(recursive=True): target.git_checkout("TMP", "-b") source.repo.git_remote_add("target", dst_path)
# todo make this cancellable with is_cancellable_behavior </s> via the executable module :class:`anki_vector.reserve_control`:	ReserveBehaviorControl the Robot and keep Vector still between SDK control instances.  Care must be taken when blocking background behaviors, as this may make Vector appear non-responsive. .. code-block:: bash python3 -m anki_vector.reserve_control
# todo: make treecount configurable via an inputslot </s> self.outputs["pmaps"].setdirty(slice(none,none,none))	OpPredictRandomForest elif slot == self.inputs["LabelsCount"]: if self.configured():
# todo: error handling and such </s> metadata = json.loads(content['value'])	from_entry [content] = entry['content']
# todo: conflict detection/resolution </s> self._settings.update(kwargs)	settings return self
# todo - add tinfo when available </s> def _source_subfolder(self):	_source_subfolder return "source_subfolder"
# todo(boris-42): make it work through assertisinstance </s> pass	EngineFake3 class EngineFake3(EngineFake2):
# todo: gae: support parents via gaekeyfield </s> return value	to_datetime A datetime object with date set to value.year - value.month - value.day and time set to 0:00 if value is a datetime.date elif isinstance(value, datetime.datetime): return value
# todo: is there a check we can do to ensure that we've matched the </s> diff = self.schedule.market_close - self.schedule.market_open	_minutes_per_session @lazyval diff = diff.astype('timedelta64[m]') return diff + 1
# todo: checking that hour/minute/second are not </s> :param yearfirst:	parserinfo (e.g. 01/05/09) as the day (``True``) or month (``False``). If ``yearfirst`` is set to ``True``, this distinguishes between YDM Whether to interpret the first value in an ambiguous 3-integer date (e.g. 01/05/09) as the year. If ``True``, the first number is taken
# todo html </s> return "{}: {}".format(from_name, message.text)	HtmlFormatter Return HTML for a message, showing reply message, forward headers, view count, post author, and media (if applicable). def _format(self, context_id, file, *args, **kwargs): entity = self.get_entity(context_id)
# todo(mvandijk): enable cluster security once trove features are in </s> mock_secure.assert_called_once_with(none)	test_prepare_member def test_prepare_member(self, mock_secure, mock_config): self._prepare_method("test-id-3", "member", None) self.manager.app.status.set_status.assert_called_with( ds_instance.ServiceStatuses.BUILD_PENDING)
pass  # todo </s> def __init__(self, prev_hash, output_index, script_sig, sequence=b'\xff\xff\xff\xff', id=0, public_key=''):	Input if not isinstance(output_index, bytes): output_index = struct.pack('L', output_index) self.id = id self.prev_hash = prev_hash
# todo: async </s> self._timeout = self.io_loop.add_timeout(time.time() + 5,	TornadioXHRPollingSocketHandler if not self.session.set_handler(self): print 'Failed to set handler' self._polling_timeout) self.session.flush()
# todo: devise a way so we don't need to "always trust". </s> >>> gpg = gpg(gpghome="keys")	gen_key def gen_key(self, input): Generate a key; you might use gen_key_input() to create the control >>> input = gpg.gen_key_input() >>> result = gpg.gen_key(input)
# todo: expand to full set of info </s> print(task_status_table, 'table was created and had a task status update added')	DatabaseHandler task_status_table = create_task_status_table(info['task_id'], info['run_id'], self.meta) self.meta.create_all(self.eng) else: task_status_table = self.meta.tables[info['run_id'] + str(info['task_id'])]
# todo log. </s> return	visit def visit(b, dirname, filenames): for filename in filenames: pathname = os.path.join(dirname, filename)
# todo: provide a kernel which will describe how coordinates are extruded. </s> def test_firedrake_extrusion_assemble(family, degree):	test_firedrake_extrusion_assemble assert integrate_assemble_p0(family, degree) < 1.0e-14
date_format = "aman"  ## todo: fix this </s> jsonify(structure(status="ok", msg=messages(	session_kill a 200 HTTP response with set-cookie to "expired" to unset the cookie on the browser "browser_session_killed")))) res.set_cookie("key", "", expires=0)
# todo: it would be good to be able to notify an external </s> vlan_inst = [valve_of.apply_actions(mirror_act)] + vlan_inst	port_add_vlan_tagged if mirror_act:
## todo: impala attempt to speed up final pass after lstm. </s> internal_states (optional[any]): the initial internal states going into an rnn-based neural network.	get_action_layer_output def get_action_layer_output(self, nn_input, internal_states=None): Args: Returns: any: The raw output of the action layer of the ActionAdapter (including possibly the last internal states
# todo - remove the line below and use repo_url as your foundation </s> if 'svnrepository' in sourceforge_package_data:	_get_sourceforge_repo_url return sourceforge_package_data['SVNRepository'].get('location', '') elif 'CVSRepository' in sourceforge_package_data:
# todo: this is untested. </s> try:	_VerifyHelper "int (*)(int, X509_STORE_CTX *)", wrapper) def raise_if_problem(self): _raise_current_error() except Error:
# todo: test filter functionality more </s> self.assertapproximates(delta_1, delta_2, 200, msg)	__run msg = 'varying delta: %i %i %i' % (delta_1, delta_2, delta_3)
'license': none, # todo </s> 'active': app_server.sub_active,	subscription_state_get def subscription_state_get(): app_server.subscription_update()
# todo lib </s> return self._items_renamed	items_renamed @property
pass # todo: explain </s> if not self.response.parsed_hdrs.has_key('location'):	status303 self.setMessage('header-location', rs.REDIRECT_WITHOUT_LOCATION)
# todo: remove when #980 has been merged </s> u"uploader_id": u"wb",	AppleTrailersIE u"thumbnail": u"http://trailers.apple.com/trailers/wb/manofsteel/images/thumbnail_6899.jpg", u"title": u"Teaser", }, }
# todo: remove parametrized workaround once collection structure contains parametrization. </s> "activate.bat",	_in_venv "activate.csh", "activate.fish", "Activate.ps1", )
# todo: remove this line when issue #2062 is fixed </s> @sy.func2plan	test_fetch_plan_built_remotely def test_fetch_plan_built_remotely(hook): hook.local_worker.is_client_worker = False def plan_mult_3(data): return data * 3
# todo: figure out why not working </s> mspfavg, mspfstd, mspfmed = visual.getmsperframe(self.win,nframes=60, showvisual=true)	test_refresh_rate def test_refresh_rate(self): if self.win.winType=='pygame': utils.skip_under_xvfb()             # skip late so we smoke test the code assert (1000/150.0 < msPFavg < 1000/40.0), \
# todo check </s> @interfacedoc	id def id(): return "gstreamerdecold"
# todo: remove when 3.8 with https://github.com/getpelican/pelican/pull/2256 </s> self.body.append('</div>\n')	depart_container
#todo: make sure that the body is actually json. </s> request body:	Tenants @route ('/tenants', method='POST') def create_tenant(): {"tenant": {
# todo: use error page with message and redirect </s> logout_response = logout(request)	social_logout def social_logout(request): from openid_consumer.views import signout as oid_signout if getattr(settings, 'LOGOUT_REDIRECT_URL', None): return HttpResponseRedirect(settings.LOGOUT_REDIRECT_URL)
# todo unordered float </s> meip = mrip[instr.mode]	jno def jno(ir, instr, dst): n = m2_expr.ExprId(ir.get_next_label(instr), dst.size) dst_o = m2_expr.ExprCond(of,
"meta.deleted": false,  # todo(tsileo): retrieve deleted and expose tombstone </s> return bleach.clean(html, tags=allowed_tags)	clean_html
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
# todo xxx postremora: uncomment when remora goes away </s> eq_(r.status_code, 400)	TestEmailChange eq_(r.status_code, 404) url = reverse('users.emailchange', args=[9945, self.token, self.hash]) url = reverse('users.emailchange', args=[self.user.id, self.token, self.hash[:-3]])
# todo (aron): add i18n by varying the language of the topic tree here </s> raise notimplemented("operation not implemented yet for playlists.")	obj_delete
# todo: fix circular import </s> node=node, fork=fork, user=user, save=false	after_fork the addon. :return: A tuple of the form (cloned_settings, message) ) if self.user_settings and self.user_settings.owner == user:
# todo test errors </s> c = [1, 2, 3]	test_append_1d eq(e.dtype, z.dtype) eq((10,), z.chunks) f = np.append(e, c) z.append(c)
# todo: find a better random value </s> try:	AccessPoint if default is DEFAULT_PARAMETER: raise ItemDoesNotExist results.next() except StopIteration:
# todo deprecate </s> :param hyperparameter_grid: an optional custom hyperparameter grid	random_forest_classifier A light wrapper for Sklearn's RandomForestClassifier that performs randomized search over a default (and overideable) hyperparameter grid. :return: a trained model algorithm = RandomForestClassifier(n_estimators=trees)
# todo: fixed by using realpath, but there should be a cleaner </s> jsonfilestatusesdb,):	test_AnnexDBs def test_AnnexDBs(): yield _test_AnnexDB, cls
# todo: deal with any overwrite issues </s> raise notimplementederror("tupleprovider b2a_rng not implemented.")	B2A_rng
# todo: this logic below was borrowed from `dataframe.pandas_df` to set the index </s> def agg(self, func_or_funcs, *args, **kwargs):	agg
# todo: check if contact is cached (same contact as </s> response,	NIRWhois nets = [] for match in re.finditer( re.MULTILINE ):
# todo: do we change this to something like "threshold" </s> bob = bob.from_public_keys({decryptingpower: bob_pubkey,	make_alice_control m, n = int(request.args['m']), int(request.args['n']) payment_details = request.args['payment'] SigningPower: None}, federated_only=True)
# todo: common crud method </s> nthash = hashlib.new('md4', cleartext.encode('utf-16le')).digest()	nt_password return binascii.hexlify(nthash).decode().upper()
# todo: parse output and check if succeeded </s> else:	StopInstance utils.KillProcess(pid)
# todo: replace with remoteunixcommand </s> backup_info.set_attribute('end_offset', 0)	stop_backup (decoded_segment[ 2] + 1) << 24)) else: raise Exception('Cannot terminate exclusive backup. You might '
# todo do this abstractly </s> return np.array(cat_x), np.array(num_x)	preprocess_type def preprocess_type(x, type, feats):
self.req.setoption("timeout", 60)  #@todo: remove in 0.4.10 </s> if self.pyload.debug and kwargs.get('trace'):	log_debug self._log("debug", self.__type__, self.__name__, args)
# todo: if mainchare constructor is *always* going to be threaded, we can remove the second condition </s> class __readonlies(object):	__ReadOnlies
# todo: deprecate </s> @abstractmethod	reset_test_dataloader
# todo(jd) move into prepare_service gettextutils and eventlet? </s> task.add(	AgentManager def setup_notifier_task(self): task = PollingTask(self) pollster, self.pipeline_manager.pipelines)
# todo: handler_node.name and handler_node.type </s> return dis.opmap[name]	_opcode
# todo: renable when tagging is removed from the analysis report. </s> else:	_MockNsrlsvrSocket if self._data == u'QUERY {0:s}\n'.format(NsrlSvrTest.EVENT_1_HASH): self._data = None self._data = None return u'OK 0'
# todo: move the inside snippets to the corresponding snippets dict </s> if args['property-name'] in color_property:	expand_value if 'color' in args and not args['color']: return '#'
#todo: let's not use the solitude transaction id if we can help it. </s> qs = contribution.objects.filter(uuid=contrib_uuid,	pay_status JWT postback. After that the UI is free to call app/purchase/record to generate a receipt. addon__addonpurchase__user=au, type=amo.CONTRIB_PURCHASE)
# todo: give a vanilla example </s> print total_energy_predicted	feca@26 appliance_energy_ground_truth = np.sum(df_appliances_ground_truth[appliance].values) total_energy_ground_truth = np.sum(df_appliances_ground_truth.values) print appliance_energy_ground_truth print total_energy_ground_truth
# todo: this is wrong. globe.semiminor_axis does </s> lats, lons = np.mgrid[0:100:10, 0:100:10]	TestAzimuthalEquidistant [-3.14159265, 3.14159265], decimal=6) assert_almost_equal(np.array(aeqd.y_limits), result = aeqd.transform_points(geodetic, lons.ravel(), lats.ravel()) expected_x = np.array([
# todo: update consumer </s> @type repo_id: str	__distributor def __distributor(self, repo_id, distributor_id): Find the distributor by id. @param distributor_id: A distributor id. @type distributor_id: str
# todo: explicitly exploit symmetry and set onesided=true </s> entity_embeddings=entity_embeddings,	HolE triples_factory=triples_factory, criterion=criterion, preferred_device=preferred_device, random_seed=random_seed,
# todo: use the dbobject instance instead of it's qualified_name </s> for schema, owner in dbcontext.get_all_schemas_and_owners().items():	determine_all_nonschema_privileges def determine_all_nonschema_privileges(role, objkind, dbcontext): all_writes = set() if role == schema and role == owner: continue
# todo implement .!{cmd} (ex shell out) test for windows and osx </s> actual = output_panel.substr(self.r(0, output_panel.size()))	TestExShellOutNoInput expected = '\\"Testing!\\"\n' else: self.assertEqual(expected, actual)
# todo(sirp): should this be a dict, or a list of dicts? </s> def detail(self, req):	detail
svg = apply_template(svg, {"maxpoints":maxpoints, "trdn": trdn, "msg":"", "valuemid":"0.5", "timemid":"10s", "datapoints":"","init_max_y": "false", "max_y": 0, "seconds_scale":0, "y_shift": 0, "zero": 0, "l_y":""}) # todo templating engine </s> for sub in tokensubscriptions[ptoken][:]:	feeder lClean.append(ptoken) continue sub.put("%s\t%s\t%s" % (hashstr, updateHash, data)) for ptoken in lClean:
# todo(pfnet): implement crop function </s> s = (reduced_y_pred == y_true).mean()	accuracy_score reduced_y_pred = np.argmax(y_pred, axis=1)
# todo(b/80125832): enable nccl in tests </s> contents = f.read()	testTraceFileStepStatsProto step_stats = step_stats_pb2.StepStats()
# todo - add some tests to this response </s> 'part',	TestReportTests Tests for the StockItem TestReport templates fixtures = [ 'location', 'stock',
# todo(erikbern): should compute jacobian of this one </s> def predict(self, ts, confidence_interval=false):	Model def fit(self, C, N, B): pass pass @abc.abstractmethod
# todo add description field to the model </s> if is_admin:	GroupManager :rtype:   sqlalchemy query is_admin = trans.user_is_admin() if deleted is None: pass
# todo multi-level import non-breakable </s> names_new.append(n)	_get_defined_names_for_position if n.start_pos[0] is not None and n.start_pos < position:
# todo(ytknzw): add more specific assertion with the test case. </s> raise valueerror	fail_objective
# todo there are more rules for adjusting rx, ry </s> def m(self, x, y):	m
# todo check output </s> self._test_obj(f.name, test_content)	test_obj test_content = 'test content' f.write(test_content)
# todo: support media types </s> normal_matcher = cssselect2.matcher()	parse_stylesheets important_matcher = cssselect2.Matcher() for stylesheet in find_stylesheets(tree):
# todo xxx graalvm change </s> calls = []	test_sni_callback sni_name='supermessage') self.assertEqual(calls, [("supermessage", server_context)]) stats = server_params_test(client_context, server_context, chatty=True,
# todo: create unsupportedproviderexception. (?) </s> set -e	Spark --write-out "%{{http_code}}" {m}:8080 )" spark/sbin/start-slaves.sh s=shlex.quote('\n'.join(cluster_info.slave_hosts)),
# todo: implement this. </s> return message	handle_outbound
# todo(b/163904067): mimic defun' behavior and reset the step seed to </s> x.dtype)	CumSum comparator = tf.less if exclusive else tf.less_equal mask = tf.cast( result = tf.tensordot(x, mask, axes=[[axis], [0]]) if axis != -1 and axis != rank - 1:
# todo(mottodora): find better way to ignore non connected </s> self.n_heads, self.out_channels))	GATUpdate h_j = functions.reshape(h, (mb, atom, 1, self.n_edge_types, self.n_heads, self.out_channels)) e = functions.concat([h_i, h_j], axis=5) e = functions.transpose(e, (0, 3, 4, 1, 2, 5))
# todo: fix maximum size limit handling to create new stream. </s> temp_file = os.path.join(temp_directory, u'storage.plaso')	testWriteStorageMetadata def testWriteStorageMetadata(self): storage_file = zip_file.ZIPStorageFile() storage_file.Open(path=temp_file, read_only=False)
# todo: remove "get_" from the name </s> return edge type index from the type tuple	edge_index Args: index: Tuple of (node1_type, edge_type, node2_type)
raise notimplementederror  # todo ... </s> import debug	_at_exit_handler print("All threads:")
# todo: how do i make the __iter__ thread safe? </s> obj = cpickle.loads(r[0])	__reversed__@165 for r in cursor:
system_info = none #todo </s> return false	is_pupil_invisible_recording try: return info_csv["Capture Software"] == "Pupil Invisible" and "Data Format Version" not in info_csv
# todo: expose more of the connection create parameters (instead of </s> in microseconds since midnight, january 1st, 0 ad nominal gregorian.	_parse_time def _parse_time(self, time): Taken from: https://github.com/joekickass/python-btsnoop In order to avoid leap-day ambiguity in calculations, note that an equivalent epoch may be used of midnight, January 1st 2000 AD, which is represented in
# todo: 3.5 does not maintain order but it should be deprecated soon </s> r = r[0]	test_failure_and_error_in_setup step('teardown step') runfix.run_test(code) assert len(r['errors']) == 2 assert r['result'] == ResultsEnum.FAILURE
# todo: raise warning if computed output is already in cache. </s> if o in inputs:	Model not_required_steps.add(step) not_required_steps |= self.graph.ancestors(step) inputs.remove(o) required_steps -= not_required_steps
# todo: remove warning check once deprecated </s> self.df.set_geometry(	TestFrameSindex assert self.df._sindex is not None def test_sindex_rebuild_on_set_geometry(self): [Point(x, y) for x, y in zip(range(5, 10), range(5, 10))], inplace=True )
# todo: process form submission </s> delete_collection(sid)	col_delete_single @login_required def col_delete_single(sid): flash("%s's collection deleted" % (source.journalist_designation,), "notification") return redirect(url_for('index'))
## todo: # fixme: remove me </s> paste_parent = self.r_serv_onion.hget('onion_metadata:{}'.format(self.domain), 'paste_parent')	get_last_crawled_pastes return self.get_all_pastes_domain(paste_parent)
#todo - uncomment if read/write and zero init sections can be moved into a separate flash algo section </s> flash one page	programPage bytes = self.overrideSecurityBits(flashPtr, bytes) self.target.writeBlockMemoryUnaligned8(self.begin_data, bytes)
# todo debug </s> self.timetriggeredfor = 0.0	RuleElement self.type = None self.triggered = False self.element = None
# todo: figure out why </s> if self._graph_widget is not none:	redraw_graph def redraw_graph(self):
'username': 'fakeuser@dimagi.com',  #todo </s> return case_id	get_case_id if case_id == '(_No_ID_)': return None
# todo ... </s> arg = arg.strip()	cpreprocess_evaluate_ifdef if not is_valid_defname(arg): state.error("preprocessor: '" + arg + "' is not a valid macro name")
# todo - need to parameterize this into generate_match_filters, </s> def add_entry(entry):	add_entry
pass # todo </s> def collect_incoming_data(self, data):	collect_incoming_data
# todo(rlrossit): these look like dicts, but they're actually versioned </s> break	_add_floating_ip fixed_address = body['addFloatingIp']['fixed_address'] for fixed in fixed_ips: else: msg = _('Specified fixed address not assigned to instance')
# todo: uncomment report abuse gets ported to mkt. </s> eq_(res.status_code, 200)	test_record_multiple_installs res = self.client.post(self.url)
# todo: refactor this to be more uniform across sources </s> self.g_pool.plugins.add(ndsi_source, args=settings)	activate_source def activate_source(self, settings={}):
# todo xxx graalvm change </s> chatty=true,	test_sni_callback calls = [] server_context.set_servername_callback(None) sni_name='notfunny') self.check_common_name(stats, SIGNED_CERTFILE_HOSTNAME)
# todo(nnorwitz): enable test. </s> self.assertequal(class('bar', templated_types=types), result[0])	testTemplateWithMultipleTemplateArgsMid types = [Class('Foo'), Class('Blah', templated_types=[Class('x')]),
raise exceptions.mpdnotimplemented  # todo </s> underscore, dash, dot and colon.	subscribe@18 ``subscribe {NAME}`` Subscribe to a channel. The channel is created if it does not exist raise exceptions.MpdNotImplemented  # TODO
# todo: let the globe return the semimajor axis always. </s> def boundary_distance(xy):	boundary_distance
# todo see issue 1935 </s> pass	EditorSession def autosave(self): pass def save(self, name): self.world_editor.save_map(PATHS.USER_MAPS_DIR, name)
# todo - perhaps time_created could go here too? </s> return self.time_received - self.time_created	time_flight
# todo: let the globe return the semimajor axis always. </s> def boundary_distance(xy):	boundary_distance
# todo(sbauza): the current placement noauthmiddleware returns a 401 </s> return ()	fake_initialize_connection if volume_id == CinderFixture.SWAP_ERR_NEW_VOL:
# todo: pass fail_silently or whatever. </s> raise notimplementederror	_build_mails
# todo: assert metrics. </s> int i = 1;	test_get_touched_functions assert touched_functions == {("func1", 1, 3)} metrics = code_analysis_server.metrics( } void func3() {
# todo find a better way of checking for no pregenerated thresholds </s> self.code_gen_dict["$streamdeclarations$"].append(	strm_decl def strm_decl(self, node): 'hls::stream<ap_uint<{}>> in0 ("in0");'.format(self.SIMD) )
# todo: show in-app notification? </s> return self.stack_perspectives.get_child_by_name(name)	_get_child
# todo: should we enable auto-retry, </s> self.prefetch_count = 1	ConsumerConfig def __init__(self, queue, include_raw_message): self.queue = queue
# todo(mordred) remove this, it's a waste of a call. it's here for </s> raise openstackcloudexception(	delete_keypair except OpenStackCloudException: raise "Unable to delete keypair %s: %s" % (name, e)) return True
ndpi = (96, 96) # todo: read real dpi </s> self.objects.append(obj)	addobj def addobj(self, obj): newid = len(self.objects)+1
# todo: out to file </s> else:	_fetch :param sub_domain: If True, build destination URL with {URL}.{HOST} else {HOST}/{URL} if not sub_domain: url = "{}://{}.{}".format(self.proto, uri, self.target) try:
# now we can kill it. todo: on a slow machine, the node might kill </s> self.failunless("does not look like a directory at all"	test_baddir self.failUnlessEqual(rc, 1)
# todo: progress +kwargs </s> def git_get_active_branch(self):	git_get_active_branch
"""todo: very inneficient code tag""" </s> return {'content':c}	show_content request = context['request'] c = Content.get_content(page, l, content_type, True) return {'content':''}
# todo: should this raise ioerror? </s> for line in hexdump.splitlines():	parse_hexdump def parse_hexdump(hexdump): data += bytearray.fromhex(line.split('#')[0]) return data
# todo: add for morph targets data. </s> rotation = mathutils.quaternion((rotation[1], rotation[2], rotation[3], rotation[0]))	decompose_transition scale =  convert_swizzle_scale(scale)
#todo: check the data! e.g. pubdate etc. </s> for i in p:	test_feed pipe_def = self._get_pipe_def("testpipe1.json") p = pipe2py.compile.parse_and_build_pipe(self.context, pipe_def) count += 1 self.assertTrue("the" in i.get('description'))
# todo pydocs </s> def description(self):	BigQueryCursor self.job_id = None self.buffer = [] raise NotImplementedError def close(self):
# todo: test! </s> return httpresponse(json.dumps(data), content_type='application/json', status=status)	json_response def json_response(data=None, status=200):
# todo: replace with specific error when exceptions are refactored </s> def left(self, value):	left self.xl.left_position.set(value)
annot.annotation_metadata.annotation_rules = "todo" #todo </s> logging.warning("path not found %s", path)	create_JAMS def create_JAMS(in_dir, metadata, out_file): path = os.path.join(in_dir, "data", metadata[0]) return jam = jams.Jams()
# todo: this regex could change based on project req format </s> parent=parent)	new_document raise exc from None document = Document.new(tree, logging.warning(exc) _DOCUMENTS.append(document)
# todo equip `unique()` with a tolerance </s> assert line.strip() == 'endloop'	_read_facet line = f.readline().decode('utf-8')
# todo(sloria): test me </s> auth = ...	DropboxNodeLogger file_obj = DropboxFile(path='foo/bar.txt') file_obj.save() nodelogger = DropboxNodeLogger(node, auth, file_obj) nodelogger.log(NodeLog.FILE_REMOVED, save=True)
#todo - use a context manager here once we drop python 2.6 </s> self.assertalmostequal(coordinates[2, 1],  1.9994412069101073)	test_pca self.assertAlmostEqual(coordinates[1, 2],  1.4818772348457117) self.assertAlmostEqual(coordinates[1, 3],  0.0) self.assertAlmostEqual(coordinates[2, 2], -1.000720598980291) self.assertAlmostEqual(coordinates[2, 3],  0.0)
# @todo need a py2/3 way to compare xml easily. </s> self.assertequal(params, {"unitdp": 4}, "test 4dps can be enabled explicitly")	test_unit4dps self.assertEqual(params, {}, "test 4dps not enabled by default") manager = Manager('contacts', credentials, unit_price_4dps=True) manager = Manager('contacts', credentials, unit_price_4dps=False) uri, params, method, body, headers, singleobject = manager._filter()
# todo make comments clearer, see _viterbi_decode </s> score = self.start_transitions[tags[0]]	_compute_score assert mask[0].all() seq_length, batch_size = tags.shape score += emissions[0, torch.arange(batch_size), tags[0]] for i in range(1, seq_length):
# todo: use invalidation time </s> logger.error("integrity error: %s", str(error))	dump_forward self.conn.rollback()
# todo-me move sorting and add more sorting options </s> if playlist.title.startswith('most popular movies'):	delete_playlist print("...Deleted {playlist.title} for '{user}'." .format(playlist=playlist, user=user)) playlist.delete() print("...Deleted {playlist.title} for '{user}'."
# todo: the problem was fixed in vobject 0.9.5 </s> filter_ = filter_[1:]	_comp_match if filter_[0].tag == _tag("C", "time-range"): if not _time_range_match(item.item, filter_[0], tag): return all( _prop_match(item, child) if child.tag == _tag("C", "prop-filter")
# todo: "wildcards" other than <any> </s> root_nodes = hierarchy	__type_hierarchy_to_list types = [] for n in root_nodes:
# todo: consider using eafp here instead. </s> return hasattr(obj, '__call__')	_is_callable
# todo: may want to validate its grouper </s> groupings = __data.grouper.groupings	fast_mutate @singledispatch2(DataFrameGroupBy) def fast_mutate(__data, **kwargs): for name, expr in kwargs.items(): res = grouped_eval(__data, expr)
# todo: re-enable for hardware </s> self.verify_all_stack_up()	test_tunnel_path_rerouted self.one_stack_port_down(self.port_map['port_3']) src_host, other_host, dst_host = self.net.hosts[:3]
# todo: is this behavior desired? </s> self.assertfalse(self.sp1.delete.called)	test_delete_with_unknown_uri_scheme_does_nothing self.core.playlists.delete('unknown:a')
# todo: test filter functionality more </s> msg = 'varying delta: %i %i %i' % (delta_1, delta_2, delta_3)	__run delta_2 = self.__run1(block, in_size * 2, ratio) delta_3 = self.__run1(block, in_size * 10, ratio) self.assertApproximates(delta_1, delta_2, 200, msg) self.assertApproximates(delta_2, delta_3, 200, msg)
# todo notify orgadmin </s> icons = {"revise": "fa fa-exclamation-triangle",	workflow_tag_represent Color-coded and icon-supported representation of facility approval workflow tags "REVIEW": "fa fa-hourglass", "APPROVED": "fa fa-check",
# todo(laigd): remove this check when 312743821 is in the release. </s> [0.255314, -0.985647, 1.461641, 0., 0., 0.],	test2DStaticShape expected_x = [ [0.38615, 2.975221, -0.852826, 0., 0., 0.], [0., 0., 0., 0., 0., 0.], ]
return  # todo: handle images </s> self.blocks.append(b)	process_item stylizer = Stylizer(item.data, item.href, self.oeb, self.opts, self.opts.output_profile) for body in XPath('//h:body')(item.data): self.process_block(body, b, stylizer, ignore_tail=True)
# todo: implement me </s> image = utils.tensor_to_gradcheck_var(image)  # to var	_test_gradcheck image = self.image.clone() depth = self.depth.clone() assert gradcheck(tgm.losses.depth_smoothness_loss, (depth, image,), raise_exception=True)
# todo: explicitly commit files by name </s> exitcode, output, error = run_command(["bzr", "version"])	is_available def is_available():
# todo improve precision </s> [frac(8, 7), frac(5, 7), frac(5, 7), frac(5, 7), frac(5, 7)]	CohenGismalla [[0.0, 0.0], [+u, -v], [-u, +v], [-v, -u], [+v, +u]] ) ) else:
# todo: overwrite databus values with function in this class </s> self.energy_out = 0	UsageSimulator class UsageSimulator(object): def __init__(self, *args): self.voltage = [0, 0, 0] self.current = [0, 0, 0]
#todo todo todo todo todo todo todo todo todo </s> componenttype = generalname()	GeneralNames class GeneralNames(univ.SequenceOf):
return cursor_offset, line #todo not implemented </s> return cursor_offset - to_delete, line[:cursor_offset - to_delete] + line[cursor_offset:]	backspace return cursor_offset, line if not line[:cursor_offset].strip(): #if just whitespace left of cursor return (cursor_offset - 1, line[:cursor_offset - 1] + line[cursor_offset:])
# todo: account for point size </s> "type": "sphere",	sphere_3d_box face_color = face_color.to_js() return [ "coords": [coords.pos() for coords in self.points], "radius": self.radius,
# todo remove hardcoded path </s> nlp.add_pipe(el_pipe, last=true)	add_el def add_el(kb, nlp): text = "In The Hitchhiker's Guide to the Galaxy, written by Douglas Adams, " \ "Douglas reminds us to always bring our towel. " \
raise notimplementederror # todo </s> class notoneobjectreturned(exception): pass	Site class Site(object): class MultipleObjectsReturned(NotOneObjectReturned): pass class ObjectDoesNotExist(NotOneObjectReturned): pass
# todo(mordred) add this back wnen ksa releases </s> self._test_host_content(host)	TestInventory host = self.inventory.get_host(self.server_id) self.assertIsNotNone(host) self._test_expanded_host_content(host) host_found = False
# todo: follow `references` to add reference information here </s> description = f"optimaderester connected to:\n{provider_text}"	describe def describe(self): Provides human-readable information about the resources being searched by the OptimadeRester. return description
# todo: could chain of 'source' with the spid </s> else:  # text output	PrintAst node = command.CommandList(nodes) if opts.ast_format == 'none': f = sys.stdout if opts.ast_format in ('text', 'abbrev-text'):
# todo(b/160795287): deprecate estimator based executor. </s> a dict of the following:	trainer_fn trainer_fn_args: Holds args used to train the model as name/value pairs. schema: Holds the schema of the training examples. - estimator: The estimator that will be used for training and eval. - train_spec: Spec for training.
# todo: add more complicated testcases </s> score = aom(self.scores, 3, method='static', replace=true,	TestAOM manual_score = np.mean(manual_scores, axis=1) assert_array_equal(score, manual_score) random_state=42) assert_equal(score.shape, (4,))
# todo: test! </s> model = topicnotification	NotificationForm class NotificationForm(forms.ModelForm): is_active = forms.BooleanField(widget=forms.HiddenInput(), initial=True, required=False) fields = ['is_active', ]
gle = [-1, -1, -1]  # todo calculate based on mesh </s> f.close()	_is_valid requirements = ["openPMD", "basePath", "meshesPath", "particlesPath"] for i in requirements: return False if "1.0." in f.attrs["openPMD"]:
# todo docstring </s> pass	extract@33 logging.info('Begin extract')
# (which seems a bit odd - todo - check with ncbi?) </s> self.check("psiblast", applications.ncbipsiblastcommandline)	test_psiblast
# todo: possibly emit an onscenechanged event </s> log.warning('could not unload %s', name, exc_info=true)	unloadPlugins module.unload(self) log.message('Unloaded plugin %s', name)
# todo: refactor to compose a list and join with ';', would be more clean. </s> exec_dir=abspath( getcwd() ),	build_command@23 captured_return_code = True commands += "; cd %s; " % abspath( getcwd() ) tmp_dir=job_wrapper.working_directory, dataset_files_path=job.app.model.Dataset.file_path,
# todo: once/if we have gpu and language labels then we might be </s> typer.echo(f"{img:<44}: {stdout}")	echo_extensive_versions img, entrypoint=["printenv", "ORCHEST_VERSION"] )
# todo: fix test </s> '\n'.join(err_msg)	test_print_rec_hypothesis check_print = True else: assert any([x.getMessage().startswith("No matching index") for x in caplog.records]), \ '\n'.join(err_msg)
# todo: need to close computations on this node? </s> def _terminate_scheduler(self, task=none):	_terminate_scheduler
# todo: really need crs specified properly in agdc-metadata.yaml </s> dst_crs=dst_projection,	reproject src_crs=source.projection, src_nodata=source.nodata, dst_nodata=dst_nodata, resampling=resampling,
@unittest.skip('not written')  # todo: finish! </s> raise notimplementederror	test_SimpleNamespace @unittest.skip('not written')  # TODO: finish! @py3_only
# todo: deprecate `extra_info` in favor of `options` </s> print("couldn't get any posts.", file=sys.stderr)	write_posts_to_csv :return:            CSV written in the same location with <<account_name>>_posts.csv list_of_posts = list(get_posts(account=account, group=group, **kwargs)) return keys = list_of_posts[0].keys()
# todo: remove when #980 has been merged </s> prefer_http = 1 if 'rtmp' in format['url'] else 0	_sortkey qidx = ['low', 'med', 'high', 'veryhigh'].index(format['3sat_qualityname'])
# todo: test </s> for i in numba.parfor.internal_prange(len(a)):	_column_fillna_impl s = B[i] if hpat.hiframes.api.isna(B, i):
# todo(aarontp): remove hard-coded sudo in commands: </s> evidence: a turbinia.evidence.rawdisk or subclass object.	PostprocessUnmountDisk@63 def PostprocessUnmountDisk(evidence): umount_cmd = ['sudo', 'umount', evidence.mount_path] log.info('Running: {0:s}'.format(' '.join(umount_cmd)))
# todo(dspasovski): fix this. </s> return category.objects.create(name='slap tickling', slug='booping',	get_new_cat def get_new_cat(self):
# todo: infer kernel arguments </s> while i <= end_idx:	map_schedule_onto_host_or_device schedule_required_splitting = False i = start_idx sched_item = schedule[i] if isinstance(sched_item, RunInstruction):
# todo: remove this skip after fixing </s> c.draw_visual(rpolygon)	test_regular_polygon_draw1@20 translate=(50, 50))
# todo: test </s> return lambda a: np.int32(typ_val)	get_type_enum_overload @overload(get_type_enum) def get_type_enum_overload(arr_typ):
# @todo: make this configurable </s> def vulnerability_group_duplicate(item):	vulnerability_group_duplicate if item.tablename == "vulnerability_group": table = item.table
# todo(piyush): current api-site doesn't contain this api description. </s> post_data = {'subnetpool': kwargs}	create_subnetpools def create_subnetpools(self, **kwargs): return self.create_resource(uri, post_data)
#todo: popen2("dot -tpng | display") and actually make the graph window pop up </s> th()	fast_call def fast_call(self): start_time = time.time() self.n_thunks += len(self.thunks) self.n_calls += 1
# todo(remove this when 11.6.1 is no longer supported) </s> executes the containing object's cm :meth:`~f5.bigip.cm.cm.exec_cmd`	Device_Group sync_cmd = 'config-sync to-group %s' % self.name cm.exec_cmd('run', utilCmdArgs=sync_cmd) method to sync the configuration FROM the device-group. :note:: Both sync_to, and sync_from methods are convenience
# todo: uncomment when adding support for literal hex bytes </s> except attributeerror as err:	test_setattr x.attr = 42
# todo: we can't do this; this shells out for each selection change... </s> raise valueerror	check_repo_root if not os.path.exists(os.path.join(location, self.VC_DIR)):
#todo: check system tables instead of using cql thrifteries </s> con.execute('drop table {};'.format(cf_name))	delete_table cf_name = model.column_family_name() with connection_manager() as con: except CQLEngineException as ex: if 'Cannot drop non existing column family' not in unicode(ex):
# todo, pass also best score </s> rank_zero_only.rank = self.trainer.global_rank	ddp_train self.trainer.local_rank = self.trainer.node_rank self.trainer.global_rank = self.trainer.node_rank model.trainer = self.trainer model.init_ddp_connection(
# todo: handle parser errors. </s> print repr(typefind.peek(0, 11) == b'[playlist]\n')	detect_pls_header def detect_pls_header(typefind):
raise notimplementederror # todo </s> for s in self.states_list:	resample_states s.resample()
node_list = ursula.batch_from_bytes(nodes, federated_only=self.federated_only)  # todo: 466 </s> tls_curve=curve)	NodeConfiguration encrypting=encrypting, tls=tls, if not no_registry and not self.federated_only: self.write_registry(output_filepath=self.registry_filepath,
# todo remove after v0.19 </s> def guess_can_open(self, filename_or_obj):	guess_can_open
# todo: avoid dummy and generate func here when inlining is possible </s> def df_len_overload(df):	df_len_overload if len(df.columns) == 0:  # empty df return lambda df: 0
# todo: totally not in vim </s> buf = vim.buffers[buf_num]	get_buf def get_buf(self, buf_num): except IndexError: return None
# todo: check error location </s> return {	get_fields 'test': GraphQLField(test_type), 'nest': GraphQLField(DataType(), resolver=lambda *_: Data())
# todo: this can be formulated more efficiently </s> ctx._backpropagated_sqrt_ggn = backpropagate_sqrt_ggn(	diag_ggn def diag_ggn(module, grad_output): module, grad_output, sqrt_ggn_out)
# todo: toots with only html do not display (images, links) </s> print("  favorited: " + re.sub('<[^<]+?>', '', faved['content']))	fav def fav(mastodon, rest): mastodon.status_favourite(rest)
self.whitelist.add(user_id) # todo: fix passing ints to file.write() </s> self.loop.create_task(self.send_message(entry.meta['channel'], '%s - your song **%s** is now playing in %s!' % (	on_play player.skip_state.reset()
# compare filesizes todo print analysis of this :) </s> else:	read_log log_file = os.path.join(DATA_ROOT, '%(login)s/%(project_id)s.process.log' % locals()) if os.path.exists(log_file): return ''
# todo: backwards compatibility; remove in favor of class method </s> result_notification=note	_load_recorded_calibrations logger.debug(str(err)) continue ) self.add(calibration)
# todo: for now the memory-server will be booted when jupyter </s> args:	JupyterDockerManager return container.attrs['NetworkSettings']['Networks'][self.network]['IPAddress'] def launch_pipeline(self, uuid: str, pipeline_dir: str) -> IP: uuid: UUID of pipeline that is launched. pipeline_dir: path to pipeline files.
# todo: reinstate </s> last_radio_checked_html = '<input checked="checked" id="selected2_%s"' % oldest_rev.revision_id	test_0_read_history assert first_radio_checked_html in main_res, '%s %s' % (first_radio_checked_html, main_res)
# todo: implement </s> def _uciok(self):	_uciok
#todo: check the data! </s> count += 1	test_feed p = pipe2py.compile.parse_and_build_pipe(pipe_def, verbose=True) count = 0 self.assertTrue("the" in i.get('description')) self.assertEqual(count, 4)
# todo: need to test this logic </s> nullspace._apply(self._jac, transpose=transpose, near=near)	set_nullspace def set_nullspace(self, nullspace, ises=None, transpose=False, near=False): if nullspace is None: if self.Jp is not None: nullspace._apply(self._pjac, transpose=transpose, near=near)
'user_id': 'f72265c0-362a-11e0-9e24-005056aa7fb5',  #todo </s> or putting it in the wrong field like patientsname	get_case_id def get_case_id(patient_xml): This is the case_id if it's extracted, assumed to be in the PatientID exam_root = etree.fromstring(patient_xml) case_id = exam_root.find("PatientID").text
# todo: also check for already-existing path. </s> ))	merge_index if len(already_downloaded): click.echo('{} remote checkpoints turned to local.'.format( click.echo('{} remote checkpoints removed.'.format( len(missing_ids) - len(already_downloaded)
# todo: see issue #944 </s> namespace['type-var'])	arguments args[namespace['code-var-name'](k)] = ctypes.cast(int(v),
# todo: actually tests mismatchs, this only ensures the code-path is run </s> def test_totaltime(self):	test_totaltime assert_almost_equal(self.universe.trajectory.totaltime, 1000.0,
# todo(pts): move reused /encoding dicts to separate objects. </s> f = open(data_tmp_file_name, 'rb')	ParseType1CFonts data_tmp_file_name) assert False, 'Type1CParser failed (no output)' try: data = f.read()
# todo remove this eventually </s> assert self.choice_flag in (action.asis, action.apply)	SingletonImportTask self.item = item self.is_album = False if self.choice_flag is action.ASIS: return (self.item.artist, self.item.title)
# todo figure out something useful to do with the newbranch param </s> return out	findoutgoing git = GitHandler(self, self.ui) base, heads = git.get_refs(remote.path) else: #pragma: no cover return super(hgrepo, self).findoutgoing(remote, base, heads, force)
return # ::todo:: </s> def xsectorsize1(self, bad, number, of, args):	xSectorSize1
gc.collect()  # todo: see first comment above </s> assert_raises(valueerror, c, ['a', 'list'])	test_EnsureDataset def test_EnsureDataset(): c = EnsureDataset() assert_raises(ValueError, c, (1, 2, 3)) assert_raises(ValueError, c, {"what": "ever"})
# todo ditto </s> if form_errors: validation_errors.extend(form_errors)	_get_validation_errors def _get_validation_errors(self, form, form_entry): validation_errors = [] apps_used = [] for app_name in form.apps.all():
# todo fix. </s> cmp_result = self.__compare_contexts(ctx_init, x86_ctx_out, reil_ctx_out)	test_cmova asm = ["cmova eax, ebx"] ctx_init = self.__init_context() if not cmp_result: self.__save_failing_context(ctx_init)
# todo: proper java error? </s> raise notimplementederror()	call_static_float_method @native_method
# todo - actually figure out types </s> break	sample_data if i == 0: continue values = [] for c in row:
# todo: check if we can use orm to do that </s> conclusion = versioned.get_conclusion()	PublishableContentManager title = versioned.title introduction = _('[[i]]\n|Ce contenu a été rédigé par {} qui a quitté le site.\n\n')\ sha = versioned.repo_update(title, introduction, conclusion, commit_message='Author unsubscribed',
# todo: renaming function paramters </s> self.assertequals('new_var = 10\nnew_var = 10 + new_var / 2\n', refactored)	test_renaming_multiple_names_in_the_same_line def test_renaming_multiple_names_in_the_same_line(self):
pass  ## fixme: todo </s> way by choice.)	interpolate_group transaction altogether, or just not include the postings in it. (An alternative behaviour would be to return only the list of valid postings, errors = [] incomplete = []
# todo: add this to simulator_objects </s> sim = self.simulator(self.model)	_make_simulator self.sim = sim
# todo: fix to avoid the implementation depended on the order of dict implicitly </s> tfds_kwargs = dataset_kwargs.pop("tfds_kwargs", {})	setup_dataset def setup_dataset(config, subset, seed): DatasetClass = config.DATASET_CLASS if tfds_kwargs: if issubclass(DatasetClass, ObjectDetectionBase):
# todo: support the <base> html element, but do not use </s> obj.__dict__[self.__name__] = value	cached_property value = self.func(obj)
# todo: something a bit less heavy than eval </s> else:	flatten for term in terms: if isinstance(term, list): ret.append(term) return ret
# todo: move this to sublime_lib; make it accept a point or a region. </s> return view.full_line(middle)	find_line elif get_line_nr(view, middle) > target: hi = getBOL(view, middle) - 1 return -1
# todo assert input entity is not none to avoid weird errors </s> require_input=true):	NewMessage it matches it will NOT be handled) or a whitelist (default). def __init__(self, incoming=None, outgoing=None, if incoming and outgoing: raise ValueError('Can only set either incoming or outgoing')
# todo this should be more modular </s> printexception(e)	GCPBaseConfig if not ignore_list_error:
el_movieposter.set('onselect', "atv.loadurl('"+el_path+"')")  # todo: 'select' - show metadata </s> xml.write(sys.stdout)	XML_prettyprint def XML_prettyprint(XML):
# todo: raise description error if 2 values not provided </s> c = context({'form':form})	as_uni_form template = get_template('uni_form/uni_form.html')
raise skiptest("buggy")  # todo(mattjj): fix </s> ans = _parallelize(fun)(x)	testTransposeAndAddRank3 return x + x.T x = onp.reshape(onp.arange(8., dtype=onp.float32), (2, 2, 2)) self.assertAllClose(ans, expected, check_dtypes=False)
# todo: think of something more sensible to do than sum(). on one </s> pairs = izip(prediction, target)	CrossEntropy ce = lambda x, z: x * tensor.log(z) + (1 - x) * tensor.log(1 - z) if isinstance(prediction, tensor.Variable): return sum([ce(p, t).sum(axis=1).mean() for p, t in pairs])
#@todo: move to utils in 0.4.10 </s> see `set_config`	setConf return self.set_config(*args, **kwargs)
# todo should we pass? </s> return self.sqlite_cursor.execute("select * from ? where request_time = (select max(request_time) from ?) and"	retrieve_most_recent_cached_data " Location = ? ORDER BY REQUEST_TIME DECS LIMIT ?", self.tables[request_type], self.tables[request_type], location,
# todo: date problem here </s> new = {"municipio": row["municipio"]}	row_with_sorted_columns continue label, day, month = key.split("_") for date_str in sorted(row_dates, reverse=True): year, month, day = date_str.split("-")
# todo: we need a clean consistent way to get the type of a cap string </s> return node.create_empty_directory(path[0])	_maybe_create def _maybe_create(f):
pass # todo </s> def collect_incoming_data(self, data):	collect_incoming_data
# todo complete this method </s> return self	make_ip self.Wovoo = imd.Wovoo(self._cc, t1, t2, eris, kconserv) self.made_ip_imds = True
# todo: implement this. </s> keypair = self.ks.gen_ecies_keypair()	test_ecies_keypair_generation self.assertEqual(keypairs.EncryptingKeypair, type(keypair)) self.assertEqual(bytes, type(keypair.privkey))
# todo: if the procpool has been exhausted this will block. </s> if self._channel is not none:	kill if self._consumers: for c in self._consumers: self._channel.close()
# todo: remove in v8 </s> yield task	flatten def flatten(task): else: for t in task:
# todo deal with other types </s> def has_range_key(self):	has_range_key @property
# todo(py3.7): add required=true </s> self._log("data=<%s>", data.hex())	Memory24xInterface if data is None: self._log("unacked") return data async def write(self, addr, data):
# todo test </s> self.past_state = past_state	StateUnreachableError from any state or from a given past state.""" def __init__(self, current_state, past_state, tpm, message): self.tpm = tpm self.message = message
# todo: untested </s> def resize_output(self):	PytorchWrapper raise NotImplementedError def to_cpu(self): raise NotImplementedError def resize_input(self):
# todo: clarify language here. </s> def event(self):	Failure return FatalEvent(self.args[0])
@unittest.skip('not written')  # todo: finish! </s> raise notimplementederror	test_SimpleNamespace @unittest.skip('not written')  # TODO: finish! @py3_only
# todo timeout? </s> return	xcp_get_basic_information if msg.arbitration_id != rcv_arb_id: return if msg.data[0] == 0xff: callback(msg)
# todo: temporary! remove me once profiling is supported for v2 </s> if get_maximum:	_profile_memory_per_line with smart_open(self.output_file, fmode=fmode) as f: self._stream = f memory_profiler.show_results(self._prof, stream=self._stream)
# todo; not sure what's wrong here. possible bug? </s> words = ["i", "like", "cheese", "."]	test_matcher_no_match doc = get_doc(matcher.vocab, words) assert matcher(doc) == []
# todo: return a list of chapters to download </s> def should_fetch_kindlegen():	should_fetch_kindlegen
# todo implement through browser </s> raise exception('cookie "{}" was not found'.format(name))	delete_cookie driver = browser.get_browser() cookie = driver.get_cookie(name) else: driver.delete_cookie(name)
# todo private access </s> super(dynamicexecutedparamname, self).__init__(executed_param_names[0])	DynamicExecutedParamName class DynamicExecutedParamName(ParamNameWrapper): Simulates being a parameter while actually just being multiple params. self._executed_param_names = executed_param_names def infer(self):
# todo: for now we use fake eth1 monitor. </s> if self.boot_info.trinity_config.has_app_config(beaconappconfig):	on_ready self.start()
# todo: handle fancy-index copies by allocating a buffer and </s> batch_size, num_batches))	ForcedEvenIterator "%d batches of equal size with batch_size" " %d, but %d batches were requested" % else: num_batches = int(dataset_size / batch_size)
# todo you should put some extra protection on this, so a user can only </s> username = jwt_identity  # access identity through jwt_identity proxy	non_fresh_protected @jwt_required def non_fresh_protected(): msg = '{} says hello from {}'.format(username, ip) return jsonify({'msg': msg})
# todo: some way of indicating progress. </s> new_remotes = [	merge_index for checkpoint in already_downloaded: checkpoint['status'] = 'LOCAL' c for c in remotes_in_local.values() if not c['id'] in missing_ids  # Checkpoints to remove.
# todo: cleanup directory/basename.* files. </s> self._tracks = dict((t.uri, t) for t in library.get('tracks', []))	JsonLibrary config['local']['data_dir'], b'library.json.gz') def load(self): return len(self._tracks) def add(self, track):
# todo: support steps and times (motion blur) </s> requires_scene_edit = camera | object | material | visibility | world	Change WORLD = 1 << 5
# todo: add option to only show error runs </s> 'styled_body': styled_body,	FileController 'run': run, 'filename': filename, } @expose(generic=True, template='files.html')
# todo fix. </s> ctx_init = self.__init_context()	test_cmova def test_cmova(self): x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) cmp_result = self.__compare_contexts(ctx_init, x86_ctx_out, reil_ctx_out)
feature_dim,  # todo define proper size </s> if data.size > 0:	_add_to_session_data ):
# todo how to get number of cells from mesh? </s> val, iels = term.evaluate()	Equation def assemble(self): A = nm.zeros((len(self.mesh.coors), len(self.mesh.coors)), dtype=nm.float64) A[iels] = A[iels] + val return A
#todo: make service manager configurable </s> in service implementation, reads in ok byte that preceeds each	TCPROSServiceClient else: b.seek(pos) response. The OK byte allows for the passing of error messages instead of a response message
# todo: currently this test breaks the bleu implementation (13.03.2016) </s> ref1 = str('it is a guide to action that ensures that the military '	test_modified_precision assert (round(hyp1_unigram_precision, 4) == 0.2857) self.assertAlmostEqual(hyp1_unigram_precision, 0.28571428, places=4) 'will forever heed Party commands').split() ref2 = str('It is the guiding principle which guarantees the military '
# @todo: enclosures </s> cutoff = current.request.utcnow - day	send_email limit = settings.get_mail_limit() if limit: table = current.s3db.msg_channel_limit check = current.db(table.created_on > cutoff).count()
# todo: discriminate between worksheet & workbook ranged names </s> return self.app.activeworkbook.worksheets(s).activate()	set_sheet
# todo: assert </s> self.remote.modify_repo(repo, "name", "testrepo0", self.token)	createRepo @pytest.fixture def createRepo(self): self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token) self.remote.modify_repo(repo, "mirror_locally", "0", self.token)
pass # todo </s> def collect_incoming_data(self, data):	collect_incoming_data
# todo: may use sys.stdout.encoding if output_file = '-' </s> diff = set(new_field_names) - set(table_field_names)	_get_field_names new_field_names = make_header(field_names.split(','), permit_not=permit_not) else: diff = set(field_name.replace('^', '')
# todo: handle situations where share is password protected </s> return self._session['servername']	getServerName
# todo: find a better way to handle indentation? </s> util.cursordebugwrapper = databasestattracker	process_init self.old_cursor = util.CursorDebugWrapper
# todo: output_path is an integer, who knows why? </s> if rule is not true:	check_config_runtime (os.getuid() != 0, 'Cannot run as root user.'), ) raise Exception(failure_reason) return True
# todo look at media_item_json_status["status"] for individual errors </s> self._enddate = utils.string_to_date(val)	end_date @end_date.setter def end_date(self, val):
# todo: parallelize this </s> return fetched_policies	_get_inline_policies raise e else: try: for policy_name in policy_names:
logfile = open(os.path.join(self.config.logdir, 'exceptions.log'), 'a') #todo: make not hardcoded </s> print >> sys.stderr, 'please fix this and then run willie again.'	create_logdir except Exception, e: print >> sys.stderr, 'There was a problem creating the logs directory.' os._exit(1)
# todo clean up how this is passed around? </s> def get_local_private_key(self) -> bytes:	BaseSession return self.insecure_conn.next_stream_id() def get_local_peer(self) -> ID: return self.local_private_key def get_remote_peer(self) -> ID:
pass  # todo - should this do something </s> network_ref = listmgmtnetworks.get_value(iter, 0)	on_acceptmgmtinterface_clicked pif = self.xc_servers[self.selected_host].all_pif[pif_ref] iter = combomgmtnetworks.get_active_iter() if pif['network'] != network_ref: change = True
# todo check initial residual with original weights </s> )	_optimize_c2 _scheme_from_dict, expand_symmetries, return _optimize( content,
## todo: check if idle is supported </s> while true:	_get_mail_count def _get_mail_count(self): if self.connection is None: if self.security == "ssl":
# todo: check how to be writeable only from same group </s> self.tegra.open(interval=500)	loop datagram = self.sock_ctrl.recv(1024) print("Datagram: {datagram}".format(datagram=datagram)) elif datagram == "stop": self.tegra.close()
except exception:  # todo: refactor this... </s> requestsnetworkwrapper()	test_init
# :todo: implement test. </s> self.skiptest('not implemented yet.')	FindTransactionsRequestFilterTestCase def test_fail_tags_wrong_type(self): self.skipTest('Not implemented yet.') def test_fail_approvees_wrong_type(self): self.skipTest('Not implemented yet.')
# todo: add a parameter to condition the derivative being returned </s> if j.shape[1] > 0:	projNullSpace return v - J.dot(J.transpose()).dot(v) else:
# todo(mcgallaspy): get rid of old integration tests and refactor the mixin methods </s> self.browser = context.browser	ContextWithMixin class ContextWithMixin(CreateAdminMixin):
# @todo do not generate __complete if type has no url attribute </s> assert isinstance( attributes[ "name" ], ( str, unicode ) )	__useAttributes assert isinstance( attributes[ "email" ], ( str, unicode ) ) self.__email = attributes[ "email" ] self.__name = attributes[ "name" ]
# todo: handle cancellation robustly </s> def sqrt(s):	sqrt
# todo: test with_polls! </s> 'p_user': p_user,	_activity page_number=request.GET.get('page', 1) ) context_name: items }
# todo tell it to some human operator </s> class invaliddata(exception):	InvalidData
# todo: add only public info </s> def update_timestamp(mapper, conn, target):	update_timestamp @event.listens_for(Webhook, 'after_update')
pass  # todo: replace this </s> r"""returns the particle symbol."""	particle @property return self._particle_symbol
# todo allow (1, none) and set to identity matrix if k == 1 </s> samples_d = self.d.draw_samples((nb_images,), random_state=ia.new_random_state(seed))	BilateralBlur result = images nb_images = len(images) samples_sigma_color = self.sigma_color.draw_samples((nb_images,), random_state=ia.new_random_state(seed+1)) samples_sigma_space = self.sigma_space.draw_samples((nb_images,), random_state=ia.new_random_state(seed+2))
# todo: put this into timeframegroup. #316 </s> if not intersect.empty:	check_for_overlap def check_for_overlap(self, other): raise ValueError("Periods overlap: " + str(self) + " " + str(other))
# todo implement </s> lpnumberofbyteswritten = params["lpnumberofbyteswritten"]	hook_WriteFile hFile = params["hFile"] lpBuffer = params["lpBuffer"] lpOverlapped = params["lpOverlapped"] if hFile == STD_OUTPUT_HANDLE:
"""todo: fixed code to test passed """ </s> start = code.index('20')	test_extract_method_containing_uncomplete_lines2 def test_extract_method_containing_uncomplete_lines2(self): end = code.index('another') + 5 with self.assertRaises(rope.base.exceptions.RefactoringError):
# todo make test_var `nonlocal` once we drop py2 -- it can just be a </s> validator(tmpdir.strpath, "path")	test_file_is_readable_validator_not_a_file with pytest.raises(ConfigurationError) as e:
# todo: support minp arg end_range etc. </s> return arr	rolling_fixed
# todo: test me. </s> utils.blink()	eval_full_command self.reset() else: self.reset() self.enter_insert_mode()
#todo (this should be done even when add_trust hasn't been </s> consolidate_clients = name_obj.single_client()	add_rrset_non_existent S.add_node(node_str, id=node_id, label=node_label, fontsize='10', **attr) self.node_subgraph_name[node_str] = zone_top_name rrset_serialized = rrset_info.serialize(consolidate_clients=consolidate_clients) if warnings_map:
# todo: think of better way doing this. </s> return self.get_queryset().cache(*args, **kwargs)	cache
#todo: rally_land points </s> mpstate.master().set_mode_rtl()	cmd_rtl
# todo(sfinucan): remove this warning when the named config options </s> value = utils.validate_num_values(	AggregateNumInstancesFilter host_state, 'max_instances_per_host') aggregate_vals, max_instances_per_host, cast_to=int) except ValueError as e:
# todo generator </s> this command takes any number of paths of files and/or directories. if	Drop @build_doc a common (super)dataset is given explicitly, the given paths are interpreted relative to this dataset.
# todo: should this test support cff as well? </s> continue	addGlyph font['hmtx'][glyph] = [0, 0] for table in font['cmap'].tables: if not table.cmap:  # Skip UVS cmaps continue
# todo: for backward compatibility only, remove if not used anymore </s> if vms:	get_vm args = {} args['projectid'] = self.get_project(key='id') for v in vms['virtualmachine']: if vm in [ v['name'], v['displayname'], v['id'] ]:
# todo(vek): need to pass context in for access to auth_token </s> def __init__(self, name, state):	InstanceInfo self.name = name assert state in power_state.valid_states(), "Bad state: %s" % state
# todo: truffle revertme once doctest is supported (once io can read text files) (gr-9151) </s> for a, b in examples:	assertAllClose self.assertIsClose(a, b, *args, **kwargs)
# todo: add kwargs in command </s> response = type(new_self).handle_method_command(new_command)	LogTensor print("Logtensor logging method", cmd) new_self, new_args = syft.frameworks.torch.hook_args.hook_method_args(cmd, self, args) response = syft.frameworks.torch.hook_args.hook_method_response( cmd, response, wrap_type=cls
# todo(qos): figure out why it passes locally but fails in gate </s> raise ml2_exc.mechanismdrivererror(	raise_mechanism_exc def raise_mechanism_exc(*args, **kwargs):
if event.keysym in ("control_l", "control_r", "command"):  # todo: check in mac </s> self.active_object_tags.add(object_tag)	handle_vm_message sequence = "<Control-Button-1>" self.text.tag_bind(object_tag, sequence, self.text.mark_set("output_end", self.text.index("end-1c")) self._insert_prompt()
# todo: timeline is global, get rid of it </s> }	RenderPosts "translations": self.site.config["translations"], "timeline": self.site.timeline, flag = False for lang in kw["translations"]:
# todo(b/142684737): the multi-processing api might change. </s> latest_model_resolver, trainer, model_analyzer, model_validator,	_create_pipeline@149 pipeline_root=pipeline_root, components=[ pusher ],
# todo: drape topography? </s> rxid = []	genLocs_2D nSrc = SrcLoc.shape[0] nRx = RxLoc.shape[0] nLeg = [] for iSrc in range(nSrc-2):
# todo(user): remove version when we update aws-cli </s> try:	YumInstall vm.RemoteCommand('yum list installed awscli') except errors.VirtualMachine.RemoteCommandError:
# todo: verify these values and formula </s> self.headerhash = json_blockheader['headerhash'].encode('latin1')	json_to_blockheader for v in v1: self.vote_hashes.append(v.encode('latin1')) self.hash = json_blockheader['hash'].encode('latin1') self.timestamp = json_blockheader['timestamp']
session.add(job)  # todo review this after remapping job (required to lazy-load attr) </s> rating = 9	TestVisualizationRatingAssociation def test_table(self, cls_): assert cls_.__tablename__ == 'visualization_rating_association' obj = cls_(user, visualization, rating) with dbcleanup(session, obj) as obj_id:
# todo remove after pytorch 1.0 </s> .format(iteration_num, self._batch_count, self._epoch_count))	train_for self.console.info("Breaking on request from callback.") break self.callbacks.call(self.callbacks.BEGIN_OF_TRAINING_ITERATION, iteration_num=iteration_num)
# todo: download bwta </s> try:	run_game@104 docker_image=args.docker_image, docker_opts=opts time_start = time.time() launch_game(players, launch_params, args.show_all, args.read_overwrite)
# todo: do we change this to something like "threshold" </s> return response('policy created!', status=200)	make_alice_control return Response(str(e), status=500) new_policy = drone_alice.create_policy(bob, label, m, n, @alice_control.route("/grant", methods=['POST']) def grant():
# todo: there's still some code duplication with static method gitrepo.get_git_dir() </s> original copyright:	GitProgress Copyright (C) 2008, 2009 Michael Trier and contributors Original license:
#todo - these are not currently implemented as properties, this means </s> so this allows us to deal with a position like ((1^2)..100). to	BetweenPosition o position - The start position of the boundary. o extension - The range to the other position of a boundary. represent that with this class we set position as 1 and the extension as 1.
# todo: this test requires manifold access, see: t88318502 </s> def setup(self):	TestCaffe2Export @unittest.skipIf(os.environ.get("CI"), "Require COCO data and model zoo.") setup_logger() def _test_model(self, config_path, device="cpu"):
# todo log here </s> time.sleep(0.2)	auth_okta@159 poll_url = poll.get('href') if not poll_url: try: response = utils.request.get(
# todo (#567): bucket the node as suspicious </s> requesting_ursula = ursula.from_metadata_bytes(request.data)	check_availability @rest_app.route("/check_availability", methods=['POST']) def check_availability(): requesting_ursula.mature() except ValueError:
# todo(vek): fails until remove_fixed_ip() added </s> req.method = 'post'	test_add_fixed_ip last_add_fixed_ip = (None, None) body = dict(addFixedIp=dict(networkId='test_net')) req.body = json.dumps(body) req.headers['content-type'] = 'application/json'
# todo: re-enable </s> return (self.opts['master_ip'],	master_pub @property def master_pub(self): 4505)
# todo: if input path was http, revert to that and try again. </s> "macupdate: %s" % err)	get_app_description break else: return (description, warning)
# todo: verify how pandas sorts column names </s> def df_len_overload(df):	df_len_overload if len(df.columns) == 0:  # empty df return lambda df: 0
# todo(stephenfin): the mock of 'migrate_disk_and_power_off' should </s> host_pass_mock = mock.mock(wraps=pci_filter_class().host_passes)	_PCIServersTestBase super(_PCIServersTestBase, self).setUp() host_manager = self.scheduler.manager.driver.host_manager self.mock_filter = self.useFixture(fixtures.MockPatch( 'nova.scheduler.filters.pci_passthrough_filter'
# todo implement </s> if hfile == std_output_handle:	hook_WriteFile nNumberOfBytesToWrite = params["nNumberOfBytesToWrite"] lpNumberOfBytesWritten = params["lpNumberOfBytesWritten"] s = ql.uc.mem_read(lpBuffer, nNumberOfBytesToWrite) ql.stdout.write(s)
# todo check the actual transformation matrix. </s> transform = np.dot(rot_x, np.dot(rot_y, rot_z))	TestICP rot_z = [[ cos(theta[2]), -sin(theta[1]),  0            ], [ sin(theta[2]),  cos(theta[1]),  0            ], source = np.random.RandomState(42).randn(100, 3) self.source = pcl.PointCloud(source.astype(np.float32))
#@todo: remove in 0.4.10 </s> self.setstorage("androidphonenotify", time())	AndroidPhoneNotify 'application': "pyLoad", 'event'      : event,
# @todo: releases </s> wget.download(url)	getdump print('Downloading...' + url)
# todo: remove this block once migration is complete </s> also includes require_post decorator	no_conflict_require_POST def no_conflict_require_POST(fn): return require_POST(no_conflict(fn))
pass # todo </s> all_tools = self.image.window.tools	_get_tool if tool_id in all_tools: return all_tools[tool_id]
# todo: reimplement run_config_yaml against user process </s> context,	test_get_schedule ).get_repository(main_repo_name()), ) GET_SCHEDULE, variables={'scheduleName': 'partition_based_multi_mode_decorator'},
# todo it would be nice if we didn't need to copy this or run the </s> pid = args.target_pid	handle_switches run_cmd = None args.su_cmd = ["su", args.user] if args.user else [] elif args.target_name: pid = get_remote_pid(device, args.target_name)
# todo(leofang): support multi-gpu callback (devices is ignored) </s> axes (tuple of ints): axes over which to compute the fft.	fftn shape (None or tuple of ints): Shape of the transformed axes of the output. If ``shape`` is not given, the lengths of the input along overwrite_x (bool): If True, the contents of ``x`` can be destroyed. plan (:class:`cupy.cuda.cufft.PlanNd` or ``None``): a cuFFT plan for
# todo: replace with "yield from" when dropping python 2. </s> url = self._generate_security_url(type_, video_id)	WAT for __ in self._create_streams('webhd', video_id).items(): yield __ res = http.get(url) return HDSStream.parse_manifest(self.session, res.text, cookies=res.cookies)
# todo cleanup/merge with above `simple_shell` once we have `subprocess.run` after dropping python 2 support? </s> print('+ ' + ' '.join(args))	shell args = ['cmd', '/E:ON', '/V:ON', '/C', run_with_env] + args return subprocess.check_call(' '.join(args), env=env, cwd=cwd)
# todo: remove after implementing django-constance </s> row_counter += 1	system@201 if len(systemquery) == 1: if dfirtrack_config.CSV_SKIP_EXISTING_SYSTEM: continue elif not dfirtrack_config.CSV_SKIP_EXISTING_SYSTEM:
assert self.stop_seq is none #todo: better handling of this situation </s> for p in self.roaster:	on_terminate_program if pid != p.pid: continue return p.on_terminate(status)
# todo: multi dp route resolver needs to flood out stack ports </s> self.one_stack_port_down(first_stack_port)	test_tunnel_path_rerouted def test_tunnel_path_rerouted(self): self.verify_stack_up() src_host, other_host, dst_host = self.hosts_name_ordered()[:3] self.verify_tunnel_established(src_host, dst_host, other_host, packets=10)
# todo(nate): temporarily disabled </s> ))	ManifestJsonHandler build_id=self.step.job.build_id, project_id=self.step.project_id, self.step.result = Result.infra_failed db.session.add(self.step)
# todo - if we want to enable single-direction </s> top_side_tile = (curr_x-1,y-1)	Map curr_x = line_start / self.tile_size[0] for i in range(num_tiles_in_line): bottom_side_tile = (curr_x-1,y) curr_x -= 1
# todo: move this to sublime_lib; make it accept a point or a region. </s> while lo <= hi:	find_line return -1 if end == -1: end = view.size() middle = lo + (hi - lo) / 2 if get_line_nr(view, middle) < target:
# todo(remove this when 11.6.1 is no longer supported) </s> method to sync the configuration from the device-group.	Device_Group cm.exec_cmd('run', utilCmdArgs=sync_cmd) def sync_from(self): :note:: Both sync_to, and sync_from methods are convenience methods which usually are not what this SDK offers.
# todo: handle temperr case (e.g. dns timeout) </s> refused_email_url = (	handle_spam email_log.refused_email_id = refused_email.id db.session.commit() URL + f"/dashboard/refused_email?highlight_id=" + str(email_log.id) )
# todo: fixed by using realpath, but there should be a cleaner </s> jsonfilestatusesdb,):	test_AnnexDBs for cls in (PhysicalFileStatusesDB,
response=none,  # todo: fix this, make param </s> if not bootstrap:	IPWhois results = {'nir': None} asn_data = None log.debug('ASN lookup for {0}'.format(self.address_str)) asn_data, asn_response = self.net.lookup_asn(
# todo: support unicode </s> def len_set_str_overload(a):	len_set_str_overload if A == set_string_type: def len_impl(A):
# todo: in #5022 </s> site = site.objects.get_current()	get_organization return {"@type": "Organization", "name": site.name}
# todo: align series </s> return a geoseries of differences	GeoSeries return GeoSeries([s.difference(other) for s in self], index=self.index) Operates on either a GeoSeries or a Shapely geometry if isinstance(other, GeoSeries):
# .. todo :: disconnect done slot/signal </s> qgisvectorlayer.setrendererv2(myrenderer)	setVectorStyle myRenderer.setMode( QgsGraduatedSymbolRendererV2.EqualInterval) qgisVectorLayer.saveDefaultStyle()
# todo use properties here to infer mechanism and purview from </s> return self._expand_repertoire('past', mechanism, purview, repertoire,	expand_cause_repertoire over the entire subsystem's state space."""
return  # todo return placeholder "[loading]" track? </s> return  # todo return placeholder "[loading]" playlist?	to_playlist if not isinstance(sp_playlist, spotify.Playlist): return name = sp_playlist.name if name is None:
# todo: request whole file from server </s> def quick_panel(self, *args, **kwargs):	quick_panel
# todo: should be able to change dash, plus and pipe </s> return {field_name: max(len(value) for value in column)	_max_column_sizes def _max_column_sizes(field_names, table_rows): for field_name, column in zip(field_names, columns)}
#todo generate the labels for the dict automatically from labels </s> --------	get_obssumm_file See Also -------- >>> import sunpy.instr.rhessi as rhessi >>> rhessi.get_obssumm_file(('2011/04/04', '2011/04/05'))
# todo consider adding the individual tiles to the resource? </s> y + self.height // 4)	get_midbottomright def get_midbottomright(self): x, y = self.get_origin()
'lb': [], #todo on apache level config </s> @task	update_code def update_code(): with cd(env.code_root):
# todo: check error location </s> return {	get_fields 'test': GraphQLField(test_type), 'nest': GraphQLField(DataType(), resolver=lambda *_: Data())
#todo: this isn't actually most_recently_used (as defined in histories) </s> returns a dictionary based on the hda in .. _summary form::	_summary_hda_dict { 'id'    : < the encoded dataset id >,
# todo make "master" not hard-coded, fetch it from some metadata </s> if not homepath.exists():	VoluminousOptions return self.opt_help() if self["pool"] is None: homePath.makedirs() self["pool"] = homePath.path
return # todo: determine cause of double-callbacks </s> def is_running(self):	is_running
# todo assert cls.__tablename__ == '' </s> s = model.galaxysession()	galaxy_session @pytest.fixture yield from dbcleanup_wrapper(session, s)
# todo(rbharath): this will cause an issue with duplicates! </s> tiled_cells = tf.split_v(tiled_cells, n)	get_cells_for_atoms cells_for_atoms: tf.Tensor Shape (N, 1) tiled_coords = tf.reshape(tf.tile(coords, (1, n_cells)), (n_cells*N, ndim)) tiled_coords = tf.split_v(tiled_coords, N)
# todo: update the tolerance after the ci moves to torch 1.10 </s> )[:num_samples]["audio"]	_load_datasamples lambda x: x["id"] in [f"1272-141231-000{i}" for i in range(num_samples)]
# todo(bowen): check </s> begin_idx = b.getbeginatomidx()	MoleculeEnv d_e = len(self.possible_bond_types) E = np.zeros((n, n, d_e)) end_idx = b.GetEndAtomIdx() bond_type = b.GetBondType()
raise notimplementederror #todo </s> pass #todo	Span
# todo: handle those values correctly </s> msg="have gotten two splits")	testConfusionMatrix msg="We should have gotten 4th label") matrices = cm.matrices          # separate CM per each given set self.failUnless((matrices[0].matrix + matrices[1].matrix == cm.matrix).all(), msg="Total votes should match the sum across split CMs")
#    todo: </s> self._insideselect = false	wmlParser if tag == 'go' : self._insideForm = False if tag.lower() == 'script': self._insideScript = False
# todo: issue an warning </s> lagg_interface = models.interfaces(	LAGGInterfaceForm lagg_member_list = self.cleaned_data['lagg_interfaces'] with DBSync(): int_interface=lagg_name, int_name=lagg_name,
# todo: if the editor is badly set up, this fails </s> pass	set_labels
# todo fix. </s> x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init)	test_cmova def test_cmova(self): asm = ["cmova eax, ebx"] cmp_result = self.__compare_contexts(ctx_init, x86_ctx_out, reil_ctx_out) if not cmp_result:
# todo implement. </s> self.dataset_rdd = rdd	SparkModel super(SparkModel, self).__init__(keras_model, data, optimizer, master_port) self.num_workers = num_workers def train(self, parameters):
# todo: skips header parsing </s> model.read_abaqus_inp(abaqus_inp_filename)	read_abaqus def read_abaqus(abaqus_inp_filename, log=None, debug=False): return model
return -1 # todo: followup after decision around returning none </s> if self._context.memory[self.vol.layer_name].is_valid(self.deviceobject):	_FILE_OBJECT class _FILE_OBJECT(objects.Struct, ExecutiveObject): def file_name_with_device(self) -> str: name = "\\Device\\{}".format(self.DeviceObject.helper_device_name) try:
# todo(crcrpar): support botorch v0.4.0. </s> "alembic",	get_install_requires def get_install_requires() -> List[str]: "cliff", "cmaes>=0.7.0",
# todo: better check for train_flag. </s> output_with_activation=self.output_before_activation,	_init_loss return self.loss.init( target=self._get_target_value())
# todo: fix this poor naming convention to better support regression tasks. </s> return {"matthews_corr": matthews_corrcoef(gold, outputs)}	matthews_corr
# todo: make the get_closest_value to return region </s> return min(math.fabs(item[1] - caret), math.fabs(item[1] + len(item[0]) - 1 - caret))	closest_to_caret
# todo: check/show the limits of low and high </s> elif args[0].lower() == 'set':	cmd_tuneopt return if args[0].lower() == 'reset': if len(args) < 4: print('Usage: tuneopt set OPTION LOW HIGH')
# todo: change when multiple envs </s> log_prob = action_distribution.log_prob(action)	PPOPolicy action_std = th.ones(mean_actions.size()) * self.log_std.exp() action_distribution = Normal(mean_actions, action_std) if len(log_prob.shape) > 1: log_prob = log_prob.sum(axis=1)
# make user perm on doctype 'todo' in assignment rule (unrelated doctype) </s> is_created = add_user_permissions(param)	test_for_apply_to_all_on_update_from_apply_all param = get_params(user, 'User', user.name) is_created = add_user_permissions(param) self.assertEquals(is_created, 0)
# clone it: (todo: use install here, once it is redone) </s> raise skiptest("todo")	test_get_recurse_subdatasets
# todo: check travis build and remove skip when test passed. </s> self.loop.run_until_complete(connect())	test_create_zmq_connection_ambiguous_args3 bind=1, connect=2)
# todo: skipped due to gh-4436 </s> assert_raises(valueerror, ds.create_sibling_ria, 'ria+file:///some/where',	test_invalid_calls def test_invalid_calls(path): ds = Dataset(path).create() name='some', storage_name='some')
# todo node_tags? </s> wh = numpy.where(inn == out)[0] + j	orient_lines wh = numpy.where(inn == out)[0] + j if len(wh) == 0: point_pair_ids[j:] = numpy.flip(point_pair_ids[j:], axis=1) oriented[j:] ^= True
# todo stub </s> def verify_table_exists(engine, table, schema=none):	verify_table_exists
#todo a tester </s> ohoster = chostergui().checkhoster(shosterurl)	showResult sHosterUrl = str(aEntry) sHosterUrl = sHosterUrl.decode("iso-8859-1", 'ignore') if (oHoster != False): oHoster.setDisplayName(sMovieTitle2)
# todo will i ever return false? </s> and optionally the branch and sha from the request arguments.	_get_refs :param str branch: Branch name. If None, return the default branch from the repo settings.
# todo extend to nonbinary nodes </s> return successive :math:`r`-length combinations of elements in the array	combs def combs(a, r): `a`. :param a: the array from which to get combinations
# todo(luotao): use clone() method to flush the program.desc in force, </s> current_op = self.block.ops[i]	_adjust_input def _adjust_input(self): for input_arg in current_op.input_arg_names: if input_arg in self.input_map:
raise exception('not valid monitor')  # todo: custom exception </s> return super(monitorsuite, self).addtest(monitor)	MonitorSuite elif isinstance(monitor, tuple): return self._add_monitor_from_tuple(monitor_tuple=monitor) raise Exception('Not valid monitor')  # TODO: Custom exception def _add_monitor_from_class(self, monitor_class, name=None):
common_path=prefix,  # todo: add key? </s> return autoresolve_generic(base, decisions, strategies)	autoresolve_cells
#todo _rule_for_states needs to made into a generator </s> observed_list = self._get_observed_list()	active_trail_nodes >>> student.add_observations({'grades': 'A'}) >>> student.active_trail_nodes('diff') ancestors_list = self._get_ancestors_observation(observed_list) Direction of flow of information
# todo: consider removing; so many system accounts/groups exist, it's likely to fail </s> return os.path.expandvars(os.path.expanduser(path))	expand_path
# todo: handle timeout </s> :type data: binary object	TCPClient return self.__socket.recv(1024) def write(self, data): self.__socket.sendall(data) @property
# todo/fixme: check that the shapes are correct! </s> self.u[ind] = utils.add_leading_axes(u[ind], ndim - ndim_u)	Stochastic ndim = len(shape) ndim_u = np.ndim(self.u[ind]) elif ndim < ndim_u: raise Exception("The size of the variable's moments array "
# todo: a smart way of choosing the number of streams, see #612. </s> r[:3] = matvecmodm(a, v[:3], m1)	multMatVect def multMatVect(v, A, m1, B, m2): r[3:] = matVecModM(B, v[3:], m2) return r
# todo parse to dataframe </s> overtime_payload['req'] = json.dumps(self.interest_overtime_widget['request'])	interest_over_time def interest_over_time(self): req_url = "https://www.google.com/trends/api/widgetdata/multiline" overtime_payload['token'] = self.interest_overtime_widget['token'] overtime_payload['tz'] = self.input['tz']
# todo: errors </s> return false	set_label elif isinstance(c, Gtk.Label): c.set_markup(message)
# todo: figure out if both android:name and name tag exist which one to give preference </s> return [tag for tag_list in all_tags for tag in tag_list]	find_tags ) for i in self.xml
# todo: how to create a super instance using continuations? </s> raise schemeexception("structpropertypredicate nyi")	W_StructPropertyPredicate self.property = prop @make_call_method([W_Object])
marked[id(atom)] = atom # since marked means "it's been appended to the todo list" </s> if totalselected > alreadyselected:	ops_connected_Mixin totalSelected = len(self.selatoms.values()) self.w.history.message("%d new doubly connected atom(s) selected (besides the %d initially selected)." % \ self.o.gl_update() return
# todo: make it really async. </s> api = sqlmanagementclient(credentials.credentials, credentials.subscription_id)	TransparentDataEncryptions self.server_name = server_name self.database_name = database_name return api.transparent_data_encryptions.get( self.resource_group_name, self.server_name, self.database_name)
# todo (straya): implement </s> if full_node_peer in peers:	_num_needed_peers c.get_peer_info() for c in self.server.global_connections.get_full_node_connections() self.log.info( f"Will not attempt to connect to other nodes, already connected to {full_node_peer}"
""" todo: documentation </s> @classmethod	GeneratingCommand def streaming(self): return type(self)._streaming def fix_up(cls, command):
# todo(shivaniagrawal): rescaling might be expensive for softmax; move </s> theta: named tuple with the weight matrix for the embedding.	EmbLookup def EmbLookup(self, theta, ids): ids: A rank-N int32 tensor. Returns:
### todo: change to support different metrics. </s> idx2ent = self.model.config.knowledge_graph.read_cache_data('idx2entity')	infer_tails tails_op = self.model.infer_tails(h,r,topk) tails = self.sess.run(tails_op) idx2rel = self.model.config.knowledge_graph.read_cache_data('idx2relation') print("head: %s" % idx2ent[h])
# todo: add logging </s> return result	fixlist except KeyError: result = tuple(fixparam[x] for x in params)
# todo make sure we can still read an unconstrained successor </s> if state is none:	_ip_overwrite_with_chain exploit an ip overwrite using rop :param chain: rop chain to use state = self.crash.state sp = state.se.any_int(state.regs.sp)
# todo also test these! </s> e = e(clf)	test_all_estimators continue with warnings.catch_warnings(record=True) as w: else: e = E()
# todo: write </s> return dict.fromkeys(key_strings, callback)	string_keys_to_dict@9
# todo: implement </s> example demonstrating how an application would normally talk to the queue.	sketch class MyWorkItem(WorkItem): @inlineCallbacks
# todo(ecastill) create a signature and do a look up </s> transposed_outs = []	_get_args_transposed transposed_args.append(self._transpose_element(arg, iax, shape)) args = transposed_args for out, iox, coredims in zip( outs, output_axes, self._output_coredimss):
# todo: shouldn't this be the other way around?! </s> w = wmlparser(response)	test_parser_simple_form <postfield name="tipdat" value="D"/> </go>""" forms = w.get_forms() self.assertEqual(len(forms), 1)
# todo: for backward compatibility only, remove if not used anymore </s> return self._get_by_key(key, self.vm)	get_vm def get_vm(self, key=None): vm = self.module.params.get('vm') if not vm:
# todo: implement smarter approach to merging </s> def processpage(self, page):	HarvestRobot in temp.getReferences(redirectsOnly=True, namespaces=[10], follow_redirects=False)] titles.append(temp.title(withNamespace=False)) Process a single page item = pywikibot.ItemPage.fromPage(page)
# todo: we may want to log this as soon as mobile ucr stops hitting this </s> return []	get_choices@622 def get_choices(data_source, filter, search_term=None, limit=20, page=0): table = get_indicator_table(data_source) sql_column = table.c[filter.field]
# todo: base on ssa instead. </s> to_name,	generateListCreationCode context.removeCleanupTempName(element_name) emit( count, element_name
# todo: log exception </s> except exception as e:	_write_missing_config@230 for module in sorted(storage_classes): try: continue ConfNeedsWrite = True
# todo: hack so every disk is not synced independently during boot </s> disk.pop('enabled', none)	DiskService return self.middleware.call('datastore.query', 'storage.disk', filters, options) @private return disk
# todo: perhaps we might merge (without duplicates) </s> if not cell:	condition_for_cell * ``attributes`` - attributes that are involved in the conditions. This should be used for join construction. return Condition([], None) attributes = set()
# todo: is there a way to do this without eval?  eval allows arbitrary </s> for key in submeta:	check_fields continue if section not in FIELDS: if key not in FIELDS[section]: raise ValueError("in section %r: unknown key %r" %
# todo: https://github.com/turicas/brasil.io/issues/209 </s> elif city == 'importados/indefinidos':	format_spreadsheet_rows_as_dict@16 data = {'confirmados': confirmed, 'mortes': deaths} if city == 'TOTAL NO ESTADO': result['importados_indefinidos'] = data else:
# todo(py3.7): add required=true </s> self._i2c_addr = i2c_address	BMP280I2CInterface class BMP280I2CInterface: def __init__(self, interface, logger, i2c_address): async def read(self, addr, size): await self.lower.write(self._i2c_addr, [addr])
# todo(boris-42): make it work through assertisinstance </s> pass	EngineFake3 class EngineFake3(EngineFake2):
# todo fix redirect loop here </s> return base64.standard_b64encode(raw_key).decode()	_build_api_key def _build_api_key():
pass # todo(denero) implement </s> self.assignment_name = 'assignment'	test_invalid_assignment_name
# todo results from ml </s> def _get_name(endpoint):	_get_name @staticmethod
# todo: remove this once connexion can validate enums with openapi3. </s> actions.append(ret_action_api)	format_all_app_actions_api ret_action_api.update(format_app_action_api(action_api, app_name, action_type))
# todo: if this is the initial load of logging config we might not </s> key, value = remainder.split('=', 1)	parse_override section, remainder = override.split('/', 1)
f.write(self.pastie_content.encode('utf8'))  # todo error checking </s> return random.choice(yamlconfig['user-agent']['list'])	getRandomUserAgent def getRandomUserAgent(): return None
# todo(kgriffs): uncomment when 3.0 development opens </s> provide both a us-ascii version using `title` and a	add_link title (str): Human-readable label for the destination of the link (default ``None``). If the title includes non-ASCII Unicode version using `title_star`. title_star (tuple of str): Localized title describing the
# todo: what actually raises valueerror in the following code? </s> self.unused_buffer += self.decoder.decompress(unused_raw)	_GzipStreamFile unused_raw = self.decoder.unused_data if self.decoder else None self.decoder = zlib.decompressobj(16 + zlib.MAX_WBITS) def read_from_buffer(self, size): part = self.unused_buffer[:size]
# todo: add a .parse() method that includes boths steps? </s> self.settings.vi['expecting_user_input'] = value	expecting_user_input @expecting_user_input.setter
# todo: test coverage </s> elif confirm:	subscribe_user )[0] if instance.subscribed: instance.subscribed = True instance.save()
# todo consider to use ansible's 'to_nice_yaml' from </s> if logconfig_path:	_set_logging def _set_logging(logconfig_path=None): with open(logconfig_path, "rb") as f: logging.config.dictConfig(
# todo: error checking </s> self._cookie = none	logout@48 'GET', headers = headers,
if is_v6 or self.is_direct_mode() or batch or not allow_quick:  # todo: thin mode </s> self.cmd_call_wrapper._log_opts['outputs'] = old_log_state	_fake_exception_wrapper json_list = \ list(self._run_annex_command_json( if "fatal:" in cml.out: raise CommandError(cmd="git annex status",
# todo: refactor this method. </s> @return: x	__isub__ def __isub__(self, y): (Optional) Convenience operator to unregister y from x @rtype Component or Manager if y.manager is not y:
print_settings["bottom_thickness"] = none  # todo; can be different per extruder & per mesh </s> preferences.getinstance().addpreference("info/asked_send_slice_info", false)	SliceInfo super().__init__() Application.getInstance().getOutputDeviceManager().writeStarted.connect(self._onWriteStarted) if not Preferences.getInstance().getValue("info/asked_send_slice_info"): self.send_slice_info_message = Message(catalog.i18nc("@info", "Cura collects anonymised slicing statistics. You can disable this in preferences"), lifetime = 0, dismissable = False)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	FindTransactionsRequestFilterTestCase def test_fail_tags_contents_invalid(self): self.skipTest('Not implemented yet.') def test_fail_approvees_contents_invalid(self): self.skipTest('Not implemented yet.')
# todo: implement </s> text = text.replace('&', '&amp;')	serialize_text text = text.replace('<', '&lt;') text = text.replace('>', '&gt;')
# todo: implement </s> def backward_pytorch(dy_data, sgd=none):	PytorchWrapper x_var = torch.autograd.Variable(torch.Tensor(x_data), requires_grad=True) dy_var = torch.autograd.Variable(torch.Tensor(dy_data)) torch.autograd.backward((y_var,), grad_variables=(dy_var,))
# todo: make the get_closest_value to return region </s> found_indent = none	get_nearest_indent line_region = view.line(view.sel()[0]) line = view.substr(line_region) first_indent = None first_is_ok = True
# todo: check number of outputs is equal to the expected number </s> for input in parent_step.inputs:	collect_steps_from def collect_steps_from(output): parent_step = output.step collect_steps_from(input)
# todo debug </s> ruleelement = ruleelement()	parseRuleRecursively if ruleMinuteNew.start > ruleMinuteNew.end: raise ValueError("'start' attribute not allowed to be " ruleElement.type = "minute" ruleElement.element = ruleMinuteNew
# todo: remove dep on parent </s> pos = self.mapfromparent(event.pos())	pipeDragMoveEvent self.currentDropIndex = int( self._columns * (pos.y() // self.squareSize) +
# todo: i think this should use '$ fileregions' </s> offset = self.page_size * page_number	get_page_buffer def get_page_buffer(self, page_number): if page_number < 1: return self.buf[offset:offset + self.page_size]
# todo, pass complete checkpoint as state dictionary </s> device_ids = [self.trainer.root_gpu]	get_device_ids return device_ids
#todo_ismeal_quesataion: i prefer get_* for getters </s> self.set_api(token=token)	set_api_token
# todo: arrange </s> self.remote.modify_repo(repo, "mirror_locally", "0", self.token)	createRepo repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "name", "testrepo0", self.token) self.remote.save_repo(repo, self.token)
# todo expression </s> f.write(u'\n')	print_Say f.write(u"\"%s\"" % (escape_string(stmt.what), )) if stmt.with_ is not None:
# todo: check for error 'toomanyregistrationsfortargetid' </s> if target_group is none:	deregister_targets def deregister_targets(self, target_group_arn, instances): raise TargetGroupNotFoundError() target_group.deregister(instances)
# todo(b/160795287): deprecate estimator based executor. </s> keras_model=_keras_model_builder(), config=run_config)	trainer_fn save_checkpoints_steps=999, keep_checkpoint_max=1) run_config = run_config.replace(model_dir=trainer_fn_args.serving_model_dir) eval_receiver_fn = lambda: _eval_input_receiver_fn(schema) return {
# todo: 'package' won't work with unpack() </s> raise exception("could not find unpacked source dir")	get_dir dir_path = join(tempdir, lst[0]) if isdir(dir_path):
# todo(ruoyu): deprecate this in favor of pre_execution once migration to </s> previous_blessed_models.append(a)	Driver a.custom_properties['blessed'].int_value == 1 and a.custom_properties['component_unique_name'].string_value == if previous_blessed_models: last_blessed_model = max(
# todo: custom gremlin method </s> raise notimplementederror	put_edge
# todo: unit-test this method. </s> find and return the template with the given name.	Loader text = f.read() return self.unicode(text, encoding) Arguments: name: the name of the template.
# todo: what could go wrong here? </s> connection = connection.objects.get(pk=form["connection_id"])	ajax_POST_send_message return self._send_message(connection, form["text"])
# todo: 重构，写到socket fd里面的都是合法的 </s> return next_fd	_store_fd next_fd = self._file_descriptor_counter self._file_descriptor_counter += 1
#todo: check the data! </s> for i in p:	test_feed pipe_def = self._get_pipe_def("testpipe1.json") p = pipe2py.compile.parse_and_build_pipe(pipe_def, verbose=True) count += 1 self.assertTrue("the" in i.get('description'))
# todo: to be implemented </s> except keyboardinterrupt:	create_ami + " --custom" ) LOGGER.info("\nExiting...") sys.exit(0)
# todo: get rid of warnings </s> assert_equal([], result.messages)	transform_document_is_applied_to_document_before_conversion with open(test_path("single-paragraph.docx"), "rb") as fileobj: result = mammoth.convert_to_html(fileobj=fileobj, transform_document=transform_document)
# todo: remove parametrized workaround once collection structure contains parametrization. </s> bindir = path.join("scripts" if sys.platform.startswith("win") else "bin")	_in_venv def _in_venv(path: py.path.local) -> bool: if not bindir.isdir(): return False
# todo numberpadding? </s> if value.same(symbol('automatic')):	check_ExponentFunction return self.default_ExponentFunction
# todo: order matters? </s> return 1 - d	compute_ssm D = distance.pdist(X, metric=metric) D = distance.squareform(D)
# todo: must be implemented </s> pass	should_fetch_kindlegen
# todo: differentiate wrt some inputs only </s> cur_vars = thin(cur_vars, thin_prob)	gen_function out_vars = map(fresh_var, out_types) eqns.append(Eqn(arg_vars, out_vars, fun)) out_vars = gen_subset(cur_vars) return Fun(in_vars, out_vars, eqns), [v.vartype for v in out_vars]
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
):  # todo? this equates the outputs </s> wires = [wires]	_parse_pipe util.pythonise(module['id'])) wires = json_pipe['wires'] for wire in wires: pipe['graph'][util.pythonise(wire['src']['moduleid'])].append(
# todo: also use backward pass </s> batch_size = tf.shape(story)[0]	get_selective_model@196 order = tf.placeholder(tf.int64, [None, None], "order") placeholders = {"story": story, "story_length": story_length, sentences = [tf.reshape(x, [batch_size, -1]) for x in tf.split(1, 5, story)] lengths = [tf.reshape(x, [batch_size])
# todo: test me @jmcarp </s> 'options': [	node_register_page auth = kwargs['auth'] node_to_use = kwargs['node'] or kwargs['project'] { 'template_name': metaschema['name'],
# todo: implement this, mandatory </s> state change	MySeriesAnnotator Returns ------- ------------ creates fitted model (attributes ending in "_")
# :todo: implement test. </s> def test_pass_compatible_types(self):	ReplayBundleRequestFilterTestCase def test_pass_happy_path(self): Request is valid. Request contains values that can be converted to the expected types.
# todo test not just the 'starts with </s> see :py:meth:`icommandlinescript.main` for parameter documentation.	ChangeStateScript def main(self, reactor, options):
else: # todo: find all defaults location for .wine , or request it directely to the user if not found. </s> def random_name():	random_name
# todo not sure from here, did not found variables inside the emulator </s> return ret	hook_GetTickCount @winapi(cc=STDCALL, params={}) def hook_GetTickCount(ql, address, params):
# todo: add compute method </s> assert (10,) == x0.shape	test_instantiation assert isinstance(node, InputNode) assert 'default/InputNode_0' == node.name assert 'default/InputNode_0/0' == x0.name
# todo: add multi_log_processor </s> @abc.abstractmethod	LogProcessor def emit(self, log_data: LogData): @abc.abstractmethod def force_flush(self, timeout_millis: int = 30000): been exported.
# todo implement this effectively </s> print('run all tests in collection')	run_full_test_suite @task
# todo find out if this is good because of sparcity... </s> name="max_depth", lower=1, upper=10, default=1, log=false))	get_hyperparameter_search_space name="loss", choices=["linear", "square", "exponential"], default="linear")) return cs
# todo: it's broken </s> log.info("iou instance {} has been created".format(self.name()))	_setupCallback self.updated_signal.emit() else: self.created_signal.emit(self.id()) self._module.addNode(self)
# @todo: "smart" & ssh keys for non-localhost </s> s3method for interactive requests	setup_monitor_server_disable_interactive def setup_monitor_server_disable_interactive(r, **attr): Disable Monitoring for a Server result = setup_monitor_server_disable(r.id) current.session.confirmation = result
#@todo: remove in 0.4.10 </s> except:	handleMultiPages try: m = re.search(self.PAGES_PATTERN, self.html) pages = 1 for p in xrange(2, pages + 1):
# todo: remove in v8 </s> if isinstance(task, dict):	flatten yield task else:
# todo: if empty, add 'pass' </s> uncompyle_find(2.7, co, 33)	uncompyle_test uncompyle(2.7, co, sys.stdout, 1) print() finally: del frame
# todo(bcipolli): add a link, with querystring args that auto-checks this video in the topic tree </s> "title": "home",	homepage topics = filter(lambda node: node["kind"] == "Topic" and not node["hide"], topicdata.TOPICS["children"]) my_topics = [dict([(k, t[k]) for k in ('title', 'path')]) for t in topics] "topics": my_topics, "registered": Settings.get("registered"),
# :todo: implement test. </s> self.skiptest('not implemented yet.')	FindTransactionsRequestFilterTestCase def test_fail_addresses_contents_invalid(self): self.skipTest('Not implemented yet.') def test_fail_tags_contents_invalid(self): self.skipTest('Not implemented yet.')
# self.todolist was loaded with old identifier settings </s> self.error)	test_list17 def test_list17(self): command.execute() self.assertFalse(self.todolist.dirty)
pass # todo </s> def _get_engine_costs(costfunc):	_get_engine_costs return sum(costfunc(d) for d in mydata.values())
# todo: make class for that </s> self.visualization.visstate = base.basekibanavisstate(	KibanaVisualizationDoc self.metric_id = 1 self.type = "visualization" title=title, type="line") self.visualization.visState.params = base.BaseKibanaParams()
# todo xxx graalvm change </s> server_context, other_context, client_context = self.sni_contexts()	test_sni_callback @needs_sni def test_sni_callback(self): client_context.check_hostname = False def servername_cb(ssl_sock, server_name, initial_context):
# todo: syn-103: remove "origin" and "destination" keys. </s> self.room_members = [self.u_apple, self.u_onion]	test_started_typing_remote_recv @defer.inlineCallbacks yield self.mock_federation_resource.trigger("PUT", "/_matrix/federation/v1/send/1000000/",
# todo: +kwargs </s> return commit	get_last_commit_hash ['git', 'log', '-n', '1', '--pretty=format:%H'], expect_fail=True) except CommandError as e: if 'does not have any commits' in e.stderr:
# todo remove when we remove dispersy </s> return	_on_channel_search_results :param object_id: Must be None. :param search_results: The result dictionary which has 'keywords', 'results', and 'candidate'. keywords = search_results['keywords'] results = search_results['torrents']
# todo yoon </s> import kenlm	load_kenlm def load_kenlm():
## todo other builtins prototype hacks. see above. </s> return arr	__object_keys__ . this is different from Object.keys because it traverses the prototype chain. arr = []
# todo: can be removed in v0.2.19 </s> if slot in self.equipment:	equip raise EquipmentException('slot for equipment has already busy') if slot not in SLOTS._ALL:
# todo: fix </s> def alloc_i1d(self, d0: int, *, dtype: optional[dtypesint] = "int32") -> array1d:	alloc_i1d
# :todo: implement test. </s> iota(self.adapter).sendtransfer,	SendTransferCommandTestCase self.assertIsInstance(
return "" # todo: followup after decision around returning none </s> pass	_FILE_OBJECT try: name += self.FileName.String return name
# todo: figure out exact thresholds to use here </s> else:	ImageSubsetLayerState def _get_image(self, view=None): if self.layer.data is self.viewer_state.reference_data: pixel_coords = [self.viewer_state.reference_data[pix, view] for pix in self.layer.pixel_component_ids]
#todo: fix this off-by-one when materials become 0-indexed </s> if order is not none:	SpatialLegendreFilter filter_type = 'spatiallegendre' def __init__(self, order=None, uid=None, new=True, index=None): self.order = order @property
raise skiptest  # todo: figure out why this randomly started failing. </s> wc = wikiclient()	test_clean_hyphens results = wc.query('marque-page') eq_(1, len(results))
pass  # todo </s> else:	Env lives = self.env.env.ale.lives() if lives < self.lives: self.lives = lives self.t += 1
# todo: assert </s> self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token)	createRepo def createRepo(self): repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "mirror_locally", "0", self.token) self.remote.save_repo(repo, self.token)
# todo_recorders - need to pass in parent info instead of none </s> super(scipyiterativesolver, self)._set_solver_print(level=level, type_=type_)	_set_solver_print except for failures, and set to -1 to disable all printing including failures. type_ : str if self.precon is not None and type_ != 'NL': self.precon._set_solver_print(level=level, type_=type_)
# todo: mock this test (failed in #493) </s> for playlist_id in expect_playlist_ids	test_get_playlists ] expect_playlists = [ ] playlists = spotify_tools.get_playlists("uqlakumu7wslkoen46s5bulq0")
# todo generator </s> recursion_limit=none,	Drop path=None, dataset=None, check=True, if_dirty='save-before'):
# todo: this is wrong in some cases, please fix it </s> def __init__(self, data, aliases):	AliasedMultiDict class AliasedMultiDict(object): self.data = data self.aliases = aliases
# todo(jmcarp) handle multiple results better </s> return count, reports.order_by(desc(reports_class.coverage_end_date)).paginate(page_num, per_page, true).items	get_reports if args['cycle'] != '*': reports = reports.filter(reports_class.cycle.in_(args['cycle'].split(',')))
# todo: use a better name </s> def render_share_problem(self, ctx, data):	render_share_problem
# todo: also check for match *[0-9]* </s> if not ret:	get_hg_node def get_hg_node(self): return node
# todo: this is a guess. make sure this is correct. </s> return "datepart(%s, %s)" % (lookup_type, table_name)	get_date_extract_sql
# todo: should this raise? </s> @param pageids: an iterable that returns pageids (str or int),	load_pages_from_pageids Pages are iterated in the same order than in the underlying pageids. Pageids are filtered and only one page is returned in case of or a comma- or pipe-separated string of pageids (e.g. '945097,1483753, 956608' or '945097|483753|956608')
raise notimplementederror()  # todo: fix </s> for model_path in args.model_paths]	get_ensemble_pred_probs probs = model.predict(x, verbose=1) return probs probs = np.mean(all_model_probs, axis=0) return probs
# todo: do_cert? </s> def supported_protocol(result):	supported_protocol
# \todo we didn't return `unicode` here because </s> {	setDisplayMode def setDisplayMode( self, displayMode ) : self.DisplayMode.Normal : QtWidgets.QLineEdit.Normal, self.DisplayMode.Password : QtWidgets.QLineEdit.Password,
# todo this help should be modified in case of webaccs. </s> ids = [ids]	infos if not ids: return [] return [cls.info(id_) for id_ in ids]
# todo: update this to support categorical </s> note: as an internal function, returns torch.tensor.	_betas def _betas(self):
#todo - use sql for this, much more efficient! </s> return self.execute_and_fetch_col0(	list_biodatabase_names "SELECT name FROM biodatabase")
# todo: this here to avoid having to manually clean up after </s> prior_ids[index_name] = to_case_id	set_index def set_index(self, from_case_id, index_name, to_case_id): self.indices[from_case_id] = prior_ids
# todo: waffle here </s> return (self._allows_public_posting() or	allows_posting_by def allows_posting_by(self, user):
# todo: show deprecation message in the future. </s> def error_routes(self):	error_routes @property
raise notimplementederror #todo </s> pass #todo	Span
# todo: re-enable after server bug is fixed </s> with self.driver.session() as session:	test_map result = session.run("RETURN {one: 'eins', two: 'zwei', three: 'drei'}") self.assertEqual(result.single().value(), {"one": "eins", "two": "zwei", "three": "drei"})
# todo: try wrapping idapython_execscript in a safe handler instead </s> pass	term
# todo: throw special accessdeniederror </s> :type user: :class:`userdb`	user_is_admin def user_is_admin(user): Return True if the provided user has admin rule, false otherwise. :rtype: ``bool`` return user_has_role(user=user, role=SystemRole.ADMIN)
# todo: remove the duplicate entry for config_name, by </s> 'username':             partial(valid_str, 'nodes'),	ValidateNode config_class =              ConfigNode validators = { 'hostname':             partial(valid_str, 'nodes') }
# todo: add support for windows </s> if clk in self.clocks:	add_period_constraint raise ValueError("A period constraint already exists") self.clocks[clk] = period
#todo : manage more than just quit </s> self.sched.nb_checks_send += len(res)	get_checks def get_checks(self , do_checks=False, do_actions=False): return res
# todo: - torch.abs(h_emb + r_emb - t_emb) </s> :param pos_exmpl:	TransE score = self.calc_score(h_emb=head_emb, r_emb=relation_emb, t_emb=tail_emb) return score :param neg_exmpl: :return:
# todo do we actually need to round here? </s> return emd(d1, d2, _hamming_matrix(n))	hamming_emd Singleton dimensions are sqeezed out. N = d1.squeeze().ndim
# todo: properly parse </s> handler.setitems(items)	LabelEditor for attr in self._editor.getLabelClassAttributes(lc): if attr == 'class': continue self._layout.addWidget(handler) else:
self.mapping = create_mapping() # todo: optimize by focusing only on this new test </s> if type(e) == watchdog.events.filedeletedevent:	on_any_event return if type(e) == watchdog.events.FileCreatedEvent: self._file_deleted(e.src_path) if type(e) == watchdog.events.FileModifiedEvent:
# todo: use shlex.quote as soon as a newer python version is available. </s> rubocop_cmd = rvm_cmd + ' -s rubocop ' + quoted_file_path	RubocopCommand def check_file(self, file_path): rvm_cmd = os.path.expanduser('~/.rvm/bin/rvm-auto-ruby') self.run_shell_command(rubocop_cmd) def run_shell_command(self, command, working_dir='.'):
# todo(leofang): how about ptds? </s> def test_add_scalar(self):	test_add_scalar
# todo remove this asap, as soon as the test has been fixed </s> i, index = index, index + 1	gen_names index = 0 for c0 in "01234567": yield i, '{0}/{1}/plop'.format(c0, c1)
# todo remove? </s> except attributeerror:	usages for d in set(definitions): try: names.append(classes.Definition(self._evaluator, d)) else:
#set limit to 25 --> currently hardcoded --> todo: add a setting for this ? </s> if ( update_type in existing_type_map ):	addBoxsetToKodiLibrary artwork["fanart"] = API().getArtwork(boxset, "Backdrop") art_types = ['poster','fanart','landscape','clearlogo','clearart','banner','discart'] if ( existing_type_map[update_type] != artwork[update_type] ) and artwork[update_type] != '': setupdateartsql = "UPDATE art SET url = ? where media_type = ? and media_id = ? and type = ?"
# :todo: implement test. </s> )	PrepareTransfersCommandTestCase self.assertIsInstance( Iota(self.adapter).prepareTransfers, def test_pass_inputs_not_needed(self): Preparing a bundle that does not transfer any IOTAs.
# todo: remove this skip after fixing </s> color=(1, 0, 0, 1))	test_circle_draw@20 raise SkipTest with TestingCanvas() as c: c.draw_visual(ellipse) assert_image_equal("screenshot", 'visuals/circle1.png')
# todo(nakago): investigate why the test fails. </s> return rsgcn(out_dim=out_dim, dropout_ratio=0.)	model_no_dropout @pytest.fixture
# todo - needs tests </s> "payment_plans": settings.payments_plans  # possibly nuke	payments_settings@8 return { "STRIPE_PUBLIC_KEY": settings.STRIPE_PUBLIC_KEY,
# todo: assert </s> repo = self.remote.new_repo(self.token)	createRepo @pytest.fixture self.remote.modify_repo(repo, "name", "testrepo0", self.token) self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token)
# todo implement. </s> def __init__(self, sc, rdd, keras_model, data, optimizer,	SparkModel num_workers=4, master_port=5000): super(SparkModel, self).__init__(keras_model, data,
# todo: optimize me </s> if not auth and not user:	can_edit :param Auth auth: Auth object to check :param User user: User object to check raise ValueError('Must pass either `auth` or `user`') if auth and user:
# todo(dylan): better error handling here </s> r = requests.get(request_url)	Command language = Settings.get("subtitle_language") request_url = "http://%s/static/data/subtitledata/srts_by_language/%s.json" % (settings.CENTRAL_SERVER_HOST, language) available_srts = set((r.json)["srt_files"]) except:
# todo: actually test this once i figure out how to do this in py.test </s> strain = strain.replace(' ', '_')	get_strain_label return entry['assembly_accession'] def cleanup(strain): strain = strain.replace(';', '_') strain = strain.replace('/', '_')
# todo add </s> reader.history = new_history	read_history_file with open(expanded) as f: for line in f:
# todo: seems to be doing < rather than <= ??? </s> xpath.add_condition("@x1 <= %s" % x1)	xpath_overlaps_bbox_function x0,y0,x1,y1 = map(float, fn.arguments[0].value.split(",")) xpath.add_condition("@x0 >= %s" % x0) xpath.add_condition("@y1 <= %s" % y1) return xpath
# todo: warn on error </s> if (selector.pseudo_element is none and	parse_stylesheets normal_declarations, important_declarations = parse_declarations( rule.content) not selector.never_matches): if normal_declarations:
#ng_required="false",   # todo: validation </s> pass	PrimaryLocationWidget 'text': loc.get_path_display(), } return get_template(self.template).render({ 'css_id': self.css_id,
# todo: sort the functionality by name and by vuln class </s> for url in resource_urls:	create_tabs description_text = str(self.data["functionality"][parent]["vulns"][vuln_name]["description"]) resource_urls = self.data["functionality"][parent]["vulns"][vuln_name]["resources"] resource_text = resource_text + str(url) + "\n" description_textarea = JTextArea()
# todo: reformat or delete </s> camera.lookat[1] = 0.85	sawyer_pusher_camera_upright camera.trackbodyid = 0 camera.distance = .45 camera.lookat[2] = 0.45 camera.elevation = -50
# todo: handle other file operations other than just extend/write </s> size(int): size of the entry	listdir path(str): absolute path of the entry realpath(str): absolute real path of the entry (if SYMLINK) mode(int): file mode/permission uid(int): user id of entry owner
# todo testing </s> if register.read_only:	set_point def set_point(self, point_name, value, priority=None): raise  IOError("Trying to write to a point configured read only: "+point_name) if priority is not None and priority < self.min_priority:
# todo tell it to some human operator </s> class invaliddata(exception):	InvalidData
#todo: where is the set_param method?! </s> automl = autosklearnclassifier(time_left_for_this_task=15,	test_classification_fit_ensemble_returns_self def test_classification_fit_ensemble_returns_self(self): per_run_time_limit=5, ensemble_size=0)
# todo: implement me </s> (depth, image,), raise_exception=true)	_test_gradcheck depth = utils.tensor_to_gradcheck_var(depth)  # to var image = utils.tensor_to_gradcheck_var(image)  # to var
# todo find a better way of checking for no pregenerated thresholds </s> self.code_gen_dict["$streamdeclarations$"].append(	strm_decl def strm_decl(self, node): 'hls::stream<ap_uint<{}>> in0 ("in0");'.format(self.SIMD) )
self.current_height = self.current_height * 2 #todo </s> return {	activations "celu": nn.CELU(), "gelu": nn.GELU(),
# todo: remove in ros 1.3 </s> try:	is_alive return True #not started is equivalent to alive in our logic s = self.ssherr data = s.read(2048) if not len(data):
pass # todo </s> f = dummyfield('hi<>bye')	test_textarea self.assertEqual(TextArea()(f), '<textarea id="" name="f">hi&lt;&gt;bye</textarea>')
# todo: python-components: for now, we call each preprocessor's graph_fn directly. </s> connecting always works by first calling the first sub-component's api-method, then - with the	Stack is used as the Stack's API-method name and the second item is the sub-Components' API-method name. E.g. api_methods={("stack_run", "run")}. This will create "stack_run" for the Stack, which will call result - calling the second sub-Component's API-method, etc.. This is done for all API-methods in the given set.
# todo only return results within uri roots given by ``uris`` </s> for track in tracks:	LocalLibraryProvider logger.info( 'Loading tracks from %s using %s', self._uri_mapping[track.uri] = track def lookup(self, uri):
# todo(gibi): remove this when live migration is fully supported and </s> 'hw_cpu_x86_sgx'])	test_flavor_image_traits_based_scheduling resource provider that also has that trait in Placement. rp_uuid = self._get_provider_uuid_by_host(self.compute2.host) server = self._create_server_with_traits( self.flavor_with_trait['id'], self.image_id_with_trait)
# todo: remove in sopel 8 </s> 'search_rules',	is_triggerable allowed_attrs = ( 'rule', 'event', 'intents',
# todo: we should filter out some of these columns </s> config_id=report.config_id	test_updating_report_that_shares_data_source report = builder_form.create_report() report_two = ReportConfiguration( ) report_two.save()
# todo docstring </s> class unparseableversion(exception):	UnparseableVersion
if func=='tag':  # todo </s> indent(xml.getroot())	XML_prettyprint def XML_prettyprint(XML):
# todo : include project name here! </s> model_config = self.__get_model_config(model_group, model_name)	CompileTask jinja = jinja2.Environment(loader=jinja2.FileSystemLoader(searchpath=src_path)) for f in files: if not model_config.get('enabled'): continue
pass  # todo </s> def close(self):	Env pass  # TODO
#       todo: update for d4 </s> return self._parent  # should be inherited from element!	parent
# todo could keep_trailing_newline fix this better? </s> if var.group('quote') and var.group('backslash') == '\\':	parse_value out = [var.group('value')] else: backslash = True elif out and out[-1].endswith('\\'):
# todo: check if user has access to this topic/poll </s> 'votes': choice_votes	voters@67 ) context = { } return render(request, 'spirit/comment/poll/voters.html', context)
# todo tell it to some human operator </s> pass	InvalidData
# todo: arrange </s> self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token)	createRepo def createRepo(self): repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "mirror_locally", "0", self.token) self.remote.save_repo(repo, self.token)
#todo do it better 21/08/13 12:36:08 </s> file_words[lw][0] += 1	near_duplicate if lw not in file_words.keys(): file_words[lw] = [1,0] seg_list = tmp_cuttor.cut(content2) for w in seg_list:
# todo(gibi): remove when nova only supports compute newer than </s> force = strutils.bool_from_string(force, strict=true)	_get_force_param_for_live_migration def _get_force_param_for_live_migration(self, body, host): if force is True and not host: message = _("Can't force to a non-provided destination")
# must remove folder or copytree fails todo: smarter version </s> buff.writeindented("thisexp.saveaspickle(filename)\n")	writeEndCode if self.params['Save wide csv file'].val: buff.writeIndented("thisExp.saveAsWideText(filename+'.csv')\n") if self.params['Save log file'].val: buff.writeIndented("logging.flush()\n")
# todo: raise an invalidparameters instead and stop using cli_ui in gitlabform.gitlab </s> "groups/%s/ldap_group_links",	delete_ldap_group_link group_id = self.get_group_id_case_insensitive(group) data["id"] = group_id group_id, method="DELETE",
# todo implement. </s> def _train(self, rdd, nb_epoch, batch_size, verbose, validation_split):	SparkModel self._train(self.dataset_rdd, nb_epoch, batch_size, verbose, validation_split)
# todo stub </s> return engine.has_table(table, schema=schema)	verify_table_exists
raise notimplementederror()  # todo </s> not implemented yet!	MouseSettings raise NotImplementedError()  # TODO def remove_settings_profile(self, profile_name): :param str profile_name: The name of the profile to remove. raise NotImplementedError()  # TODO
# todo: more efficient implementation (lua script per shard?) </s> self.crypter = none	_setup_encryption sub_fernets = [self.make_fernet(key) for key in symmetric_encryption_keys] self.crypter = MultiFernet(sub_fernets)
# todo: wobble </s> arctan2(-position[1], -position[0], out=phi)	to_polar theta = Angle(r.shape) phi = phi_class(r.shape) phi += pi return r, theta, phi
# todo: implement this </s> def mkdtemp(self):	Boss def __call__(self, gui): self.gui = gui self.container_count += 1 return tempfile.mkdtemp(prefix='%05d-' % self.container_count, dir=self.tdir)
# todo materials are read-only for now </s> update_lookup_field = 'name'	CalendarSerializer model = freppledb.input.models.Calendar fields = ('name', 'description', 'category', 'subcategory', 'defaultvalue', 'source', 'lastmodified') partial = True
# todo i18n text entries </s> _ignore, failed, spinner, control = sbox.get_children()	_write_async control.set_sensitive(False) failed.set_visible(False)
# todo implement for all channels </s> return self._execute_cmd("set", "appliance.control.togglex", payload)	turn_on_channel def turn_on_channel(self, channel):
# todo: remove </s> return get_object_or_404(self.for_access(user), user=user, pk=pk)	for_update_or_404@41 if user.is_moderator: return get_object_or_404(self._access(user=user), pk=pk)
# todo(b/142684737): the multi-processing api might change. </s> example_gen, statistics_gen, infer_schema, validate_stats, transform,	_create_pipeline@137 pipeline_name=pipeline_name, pipeline_root=pipeline_root, trainer, model_analyzer, model_validator, pusher ],
# todo test </s> return hash((frozenset(self.nodes), self.current_state.tostring(),	__hash__ def __hash__(self):
# todo: must be implemented </s> pass	should_fetch_kindlegen
# todo-blocker(jamalex): re-enable this conditional once tastypie endpoints invalidate cached session value </s> if user.is_teacher:	compute_total_points return None else:
# todo get variable size </s> default=false,	generate_option_parser help="Get the mangled name of the function (i.e. Swift)") parser.add_option("-l", "--load_address", dest="load_address", help="Only print out the simple description with method name, don't print anything else")
#todo: test me </s> def free(self):	free
# todo: wobble </s> to be in the dynamical system of that particular date.  otherwise,	position_of_radec and declination from any viewing position in the Solar System, to very high precision (within a few hundredths of a microarcsecond). they will be assumed to be ICRS (the modern replacement for J2000). theta = _to_array(dec_degrees) / 360.0 * tau
assert self.start_seq is none #todo: better handling of this situation </s> for p in self.roaster:	on_terminate_program if pid != p.pid: continue return p.on_terminate(status)
# todo(unno): sub for boolean array is deprecated in numpy>=1.13 </s> def test_doubly_broadcasted_lt(self):	test_doubly_broadcasted_lt
# todo: check how to sort inputs for multichannel inputs </s> def get_output_datatype(self):	get_output_datatype
# todo: find out if this method is used anywhere, remove if not. </s> assert channel_x not in g or g[channel_x] is x	redo_theano inputs = c_mapping.flatten(nested_theano_args[i + 1], return_tuple=True) assert channel_X.type == X.type g[channel_X] = X
# todo: self.assertfalse(prop.is_valid(np.bool8(true))) </s> self.asserttrue(prop.is_valid(foo()))	test_Instance self.assertFalse(prop.is_valid(())) self.assertFalse(prop.is_valid([])) self.assertFalse(prop.is_valid(Bar())) self.assertFalse(prop.is_valid(Baz()))
# todo: handle case where the creation is rejected for some reason (should </s> if not results:	list_server_logs def list_server_logs(self, request, fqdn): return Response(status=status.HTTP_503_SERVICE_UNAVAILABLE) return Response(sorted(results[fqdn]))
# todo: legacy behavior, should remove after new case processing </s> def has_case_id(case_block):	has_case_id
# todo: revert this pushing all 3 changes at once, its' just a </s> return phlgit_log.make_revisions_from_hashes(self, hashes)	make_revisions_from_hashes from any of 'hashes'. :hashes: a list of commit hash strings
# todo - use smarter weights (e.g. hamming window) </s> prediction : (n_sequences, n_samples, n_classes) numpy array	SequenceLabelingAggregation Mono-batch of sequences of features Returns Mono-batch of sequences of predictions See `FileBasedBatchGenerator` base class for details.
# todo handle custom_objects </s> feature_iterator, label_iterator = tee(data_iterator, 2)	AsynchronousSparkWorker self.master_metrics = master_metrics def train(self, data_iterator): x_train = np.asarray([x for x, y in feature_iterator]) y_train = np.asarray([y for x, y in label_iterator])
# todo take this as input from outside? </s> continue	supportScalar if peak == 0.: continue if lower < 0. and upper > 0.: continue
# update new ratings kodi 17 - todo get ratingid for updates from embydb </s> return false	compare_all if not self.compare_boxsets():
# todo: we do not currently have type-safety for keys suitable for decoding *and* </s> this constructs an rsa key from a json web key, the result can be used as key to	rsa_key_from_jwk :func:`encode` and should be directly passed to it. :param jwk: The JSON Web Key as encoded JSON.
# todo: remove when support for django 1.3 is dropped </s> def _postgisadapter_prepare(self, conn):	_PostGISAdapter_prepare
# todo: replace with "yield from" when dropping python 2. </s> if extra_params:	_get_streams_from_media extra_params = extra_params.get("extraParams", {}) if not params: swf_url = extra_params.get("videoSwfURL") mode = params.get("mode")
raise notimplementederror # todo </s> return self._id	id @property
# todo: let the globe return the semimajor axis always. </s> return boundary.project(sgeom.point(*xy))	boundary_distance
summary=f'added book "{b.title}"', # todo shelf? </s> date_read=_parse_date(read_at),	iter_books shelves=book_shelves, date_added=da,
# todo: must be implemented </s> def should_fetch_kindlegen():	should_fetch_kindlegen
# todo: log exception </s> results[str_entry[0]] = value[:255]	_get_version_info@456 value = str_entry[1].encode('ascii') except Exception as e: elif hasattr(entry, 'Var'):
"""todo doc me""" </s> ['foo', 'bar']	fields >>> from petl import fields >>> table = [['foo', 'bar'], ['a', 1], ['b', 2]] it = iter(table) return it.next()
""" type setting - todo explain """ </s> fields= (	ServiceDiscoverySerializer def get_endpoints(self,obj): return DISCOVERY['endpoints'] 'changeset','contact','key_service','endpoints'
# todo, pass complete checkpoint as state dictionary </s> self.trainer.train_loop.setup_training(model)	tpu_train_in_process trainer = self.trainer trainer.call_setup_hook(model) results = self.train_or_test() self.__save_end_of_training_weights(model, trainer)
pass # todo </s> priority = sys.argv[3]	pri def pri(self): if number and priority: if re.match('^[A-Z]$', priority):
# todo: fix inconsistent handling of water </s> else:	_loop_to_string line = "\n" elif len(line) + len(val) + 2 < self.maxlen: s += line line = '\n  ' + val
# todo : map extra parents to the extras file </s> if self.ui.config('extensions', 'hgext.bookmarks') is not none:	check_bookmarks def check_bookmarks(self):
# todo(mordred) when this changes to rest, force interface=admin </s> except ironic_exceptions.clientexception:	get_nic_by_mac _tasks.MachineNodePortGet(port_id=mac))
# todo (a8): add user to models </s> return httpnotimplemented()	obj_delete
# todo(yuriyz): change to 404 (bug 1200517) </s> {'extra': extra})	test_update_byid pdict = dbutils.get_test_port() extra = {'foo': 'bar'} self.assertEqual(response.content_type, 'application/json') self.assertEqual(response.status_code, 200)
# todo context, etc (allophones) </s> except unicodedecodeerror:	_iter_txt for l in f: try: l = l.decode("latin_1")  # or iso8859_15? l = l.strip()
# todo: restore this code after resolution of the following issue: </s> x, y = get_xy_dim_coords(cube)	snapshot_grid def snapshot_grid(cube): Helper function that returns deep copies of lateral dimension coordinates return x.copy(), y.copy()
# todo: does the gas and from defaulting in `sendtransaction` make sense for this? </s> [block_identifier],	getUncleCount ) return self.web3.manager.request_blocking(
svg = apply_template(svg, {"maxpoints":maxpoints, "trdn": trdn, "msg":"", "valuemid":"0.5", "timemid":"10s", "datapoints":"","init_max_y": "false", "max_y": 0, "seconds_scale":0, "y_shift": 0, "zero": 0, "l_y":"[0,0,0,0,0,0,0,0,0,0]"}) # todo templating engine </s> return ""	feeder try: if hashstr in lhosts and ip != lhosts[hashstr]: except KeyError: return ""
# todo: remove patch and update test once calculation_magic is implemented </s> _update_lpq_eligibility(project_id=17, cutoff=10)	test_not_eligible_not_lpq def test_not_eligible_not_lpq(self, store) -> None:
# todo: delet this when all preprintproviders have a mapping </s> def banner_path(self):	banner_path if self.logo_name: return '/static/img/preprint_providers/{}'.format(self.logo_name)
).consume()  # todo see issue 170 </s> elbs = []	get_loadbalancer_data def get_loadbalancer_data(boto3_session, region): client = boto3_session.client('elb', region_name=region, config=_get_botocore_config()) for page in paginator.paginate(): elbs.extend(page['LoadBalancerDescriptions'])
# todo: sort out blending on non-rgb systems </s> [(self._colour, screen.a_bold, 0), (0, 0, 0)],	SerpentExplosion self._y, cos(direction), self._life_time, self._explode,
#todo: fix this </s> info( controller.name + ' <->' )	configureRoutedControlNetwork prefixLen=16 ): For use with the user datapath only right now.""" cip = ip snum = ipParse( ip )
# todo: make a signature attribute for transactions </s> return len(key.value)	zcard @command((Key(ZSet),))
# todo!! add more assertions for the smaller subsystems </s> np.testing.assert_almost_equal(	standard_example_is_correct sum(C.phi for C in mip.unpartitioned_constellation), 1.5833, sum(c.phi for c in mip.partitioned_constellation), 0.5)
# todo todo todo </s> if not self.__dialog.lists['docs']['include_current_doc']:	ActionRemoveDoc model.remove(selection_iter) for line_idx in range(0, len(self.__dialog.lists['docs']['model'])): line[0] = _("Document %d") % (line_idx + 1) elif line_idx != 0:
# todo: the following skipped suite and fixtures should be enabled </s> provider = provider	VultrProviderTests provider_name = 'vultr' domain = 'capsulecd.com'
# todo: create xxx_failure test </s> tests = exclude_from_resultlist(r, 'failure')	test_result_space_failure p = op.join(app.config['ROOT'], 'tests/fixtures/ttf/Font-Light!.ttf') r = run_set(p, 'result') self.assertTrue(check('test_space', failure_tests), lookup('test_space', tests))
# todo: either fix this or remove this code </s> self.entry.emit('activate')	SearchEntry self.entry_activate) def entry_activate(self, *e):
pass  # todo(zcd) </s> grad_tensor.set(data, place)	__set_tensor__ data = np.ones(out_tensor.shape(), dtype=np.float32) else:
# todo: use pybossa uploader! only for debugging: </s> cached_apps.n_volunteers(id)	warm_cache cached_apps.overall_progress(id) cached_apps.last_activity(id) if n_task_runs >= 1000 or featured: print "Getting stats for %s as it has %s task runs" % (short_name, n_task_runs)
# todo: move navigation part to base class </s> return qtcore.qrect(qtcore.qpoint(min_x, min_y), qtcore.qpoint(max_x, max_y))	getNodesRect max_x = max(arr2) min_y = min(arr3)
#todo gtk3: workaround here for bug https://bugzilla.gnome.org/show_bug.cgi?id=680638 </s> 'pevent': clipevent,	map2class 'source-link': ClipSourceLink, 'citation-link': ClipCitation, 'eventref': ClipEventRef, 'mediaobj': ClipMediaObj,
# todo: use tx hash instead of key to avoid multiple queries for the same tx </s> qr = qr.filter_by(account_id=account_id)	_get_account_defaults qr = self._session.query(DbKey).\ filter_by(wallet_id=self.wallet_id, purpose=self.purpose, depth=depth, network_name=network) acckey = qr.first() if len(qr.all()) > 1:
# todo test </s> self.current_state = current_state	StateUnreachableError class StateUnreachableError(ValueError): from any state or from a given past state.""" self.past_state = past_state self.tpm = tpm
# todo: log. </s> self.map_ = self.done(self.map_)	poke self.n -= 1 if not self.n: self.callback(self.map_) self.evt.set()
# todo: add other exceptions here </s> def get_api_id(cls):	get_api_id return 'INFOBIP'
# :todo: implement test. </s> self.skiptest('not implemented yet.')	GetTrytesResponseFilter def test_pass_transactions(self): self.skipTest('Not implemented yet.')
#todo(wwolf) get correct value for these </s> root.appendchild(self._create_version_node(version))	VersionsXMLSerializer def _versions_to_xml(self, versions): root = self.xml_doc.createElement('versions') return root def _create_version_node(self, version):
# todo. create readme file in <output_dir> </s> hypothesis = peak.apply(predictions[uri], dimension=1)	SpeakerChangeDetection for current_file in getattr(protocol, subset)(): reference = current_file['annotation'] hypothesis = hypothesis.to_annotation() uem = get_annotated(current_file)
extruder_stack.userchanges.setproperty(key, "value", new_value)  # todo: nested property access, should be improved </s> def activequalitydefinitionid(self) -> str:	activeQualityDefinitionId if self._global_container_stack: return self.getQualityDefinitionId(self._global_container_stack.getBottom())
# todo(harlowja): the bug 1214083 is causing problems </s> 'flow': str(details['flow']),	attach_debug_listeners def task_log_change(state, details): LOG.debug(_("%(flow)s has moved %(runner)s into state %(state)s with" 'runner': str(details['runner']), 'result': details.get('result')})
heading=x.heading, # todo include the rest? </s> def _iterate(self, f: path) -> iterable[orgnote]:	Query cache_path=lambda _, f: cache_dir() / 'orgmode' / _sanitize(f), force_file=True, depends_on=lambda _, f: (f, f.stat().st_mtime), o = Org.from_file(f) for x in o.iterate():
# todo: maybe a chardet integration </s> def _set_eol(self, eol):	_set_eol
# todo: replace with a call to dt.timestamp() when we drop python 2.7 </s> if token == "ss":	DateTimeFormatter return str("{:04d}".format(int(dt.microsecond / 100))) if token == "SSS": return str("{:02d}".format(int(dt.microsecond / 10000))) if token == "S":
# todo use **kwargs instead 4 dummy parameters </s> file_path,	download_photo def download_photo(self, add_extension=False, progress_callback=None):
# todo assert the key passes deeper validation </s> with open(pk_path, 'r') as f:	MigrationScript pk_path = os.path.join(tor_dir, 'private_key') hn_path = os.path.join(tor_dir, 'hostname') r = f.read() if not r.startswith('-----BEGIN RSA PRIVATE KEY-----\n'):
# todo: refactor into some kind of utility </s> for idx in range(0, count):	CollectionTestBase }, 'tags': "Tag1 Tag2", self.add_note(data)
fp = open("%s.py" % name, "w")   # todo: confirm file overwrite </s> '151619<br>total basic allowances claimed, ex travel: '	test_csv u'MPMisc': u'20', u'title': u'Mr Mark Lancaster', '146282<br>Total Travel claimed: 5337<br>MP Mileage: ' '3358<br>MP Rail Travel: 1473<br>MP Air Travel: 0<br>'
# todo(vek): need to pass context in for access to auth_token </s> self.state = state	InstanceInfo def __init__(self, name, state): self.name = name
tasks = ss("#todo-list>li") </s> s("#filters a[href='#/active']").click()	test_create_task@6 tasks.insist(texts("1", "2", "3")) s("#todo-count").insist(text(3)) tasks[:2].insist(texts("1", "2")) tasks[2].insist(absent)
# todo unordered float </s> n.zeroextend(instr.mode),	jno meip = mRIP[instr.mode] n = m2_expr.ExprId(ir.get_next_label(instr), dst.size) dst.zeroExtend(instr.mode)) e.append(m2_expr.ExprAff(meip, dst_o))
# todo: uncomment when notificationsubscription is implemented </s> return (	can_edit is_api_node = auth.api_node == self else: (user and self.has_permission(user, 'write')) or is_api_node
# todo: make this a hard error, instead of a silent overwrite </s> if name is none:	StopInstance def StopInstance(self, instance, force=False, retry=False, name=None): if name is not None and not force: name = instance.name acpi = instance.hvparams[constants.HV_ACPI]
# todo: parallelize the following loop with joblib </s> self.preprocessor = preprocessor	CharNGramAnalyzer self.charset = charset self.min_n = min_n def analyze(self, text_document): if hasattr(text_document, 'read'):
# todo: documentation pending </s> elif is_train is not none and self.is_train is not none:	_check_mode def _check_mode(self, is_train): if is_train is None and self.is_train is None: if is_train == self.is_train: logging.warning("Training / inference mode redefined redundantly. Please EITHER use the argument `is_train` OR `Model.train()` / `Model.eval()` to define the mode.")
self.assertequals(status, 200) # todo: 202 when asynchronous </s> self.repo_id,	test_get_bind bind = manager.bind(self.CONSUMER_ID, self.REPO_ID, self.DISTRIBUTOR_ID) path = '/v2/consumers/%s/bindings/%s/%s/' % \ self.DISTRIBUTOR_ID) status, body = self.get(path)
if abs(skew) > 5: # todo: make configurable </s> self.setmessage('header-location', rs.redirect_without_location)	status303 def status303(self):        # See Other
# todo: automate this: batch in -> batch out; time in -> time out; batch+time in -> batch+time out, etc.. </s> override get_logits_parameters_log_probs api-method to not use the state-value, which must be sliced.	get_logits_parameters_log_probs _, logits = self.call(self.get_state_values_and_logits, nn_output) return (logits,) + tuple(self.call(self._graph_fn_get_parameters_log_probs, logits))
# todo: what about other auth types? </s> raise networkerror('urllib3 httperror {0}'.format(error))	_request_wrapper except urllib3.exceptions.TimeoutError: raise TimedOut() if 200 <= resp.status <= 299: return resp.data
# todo(kpy): remove support for legacy urls in mid-january 2012. </s> self.response.clear()	head def head(self): self.request.method = 'GET'
# todo: we have no format to save volumes yet! </s> return	test_findlb def test_findlb(): dirs, paths = core.findlib.generate_candidate_libs(['libzip']) assert paths
# todo(hub-cap): turn this into middleware </s> super(api, self).__init__(mapper)	API class API(wsgi.Router): def __init__(self): self._instance_router(mapper) def _instance_router(self, mapper):
# todo(vek): need to pass context in for access to auth_token </s> def __init__(self, name, state):	InstanceInfo self.name = name assert state in power_state.valid_states(), "Bad state: %s" % state
raise notimplementederror #todo </s> class span(object):	Span
# todo: replace with `keras.backend.pad` </s> (-1, 2)	ObjectSegmentation ], 1 ) )
# todo: fix this </s> else:	propfind if xml_request: root = ET.fromstring(xml_request.encode("utf8")) props = [_tag("D", "getcontenttype"), _tag("D", "resourcetype"),
# todo(cmaloney): switch to a sane http server </s> start_response('200 ok', [('content-type', 'text/html')])	wsgi_app@420 def wsgi_app(env, start_response): return "Got it"
# todo this should be a return and printed elsewhere </s> plt.xlim([-1, x_train_shape])	plot_random_forest_feature_importance x_train_range = range(x_train_shape) plt.bar(x_train_range, importances[indices], color="r", yerr=standard_deviations[indices], align="center") plt.gca().set_ylim(bottom=0) plt.tight_layout()
# todo: needs further implementation </s> def rendered_report_title(self):	rendered_report_title @cached_property
# todo: wait for event instead. </s> elif documentchanges:	LspApplyWorkspaceEditCommand path = uri_to_filename(uri) self.open_and_apply_edits(path, file_changes) for document in documentChanges: uri = document.get('textDocument').get('uri')
# todo: convert non uris to file uris. </s> print repr(typefind.peek(0, 11) == b'[playlist]\n')	detect_pls_header return typefind.peek(0, 11) == b'[playlist]\n'
#todo - two staight lines is only a good approximation for small </s> label_elements = []     # holds diagram elements belonging to feature labels	draw_feature_set Returns a tuple (list of elements describing features, list of labels for elements) for feature in set.get_features(): if self.is_in_bounds(feature.start) or self.is_in_bounds(feature.end):
# todo(kevinbenton): remove after bug/1666493 is resolved </s> msg = (_("ipv6 address %(address)s can not be directly "	_test_fixed_ips_for_port subnet['cidr'] != n_const.PROVISIONAL_IPV6_PD_PREFIX): if (is_auto_addr_subnet and device_owner not in "assigned to a port on subnet %(id)s since the " "subnet is configured for automatic addresses") %
# todo xxx postremora: uncomment when remora goes away </s> self.assertequal(self.user_profile.email, 'jbalogh@mozilla.com')	TestEmailChange r = self.client.get(url, follow=True) eq_(r.status_code, 400) url = reverse('users.emailchange', args=[self.user.id, self.token, self.hash])
# todo log here </s> return false	auth_okta@95 return False poll = links.get('poll') poll_url = poll.get('href') if not poll_url:
#todo finish me </s> return rv	figshare_hgrid_urls connect = Figshare.from_settings(node_settings.user_settings) rv = article_to_hgrid(node, connect.project(node_settings, node_settings.figshare_id))
# todo should we pass? </s> self.tables[request_type], self.tables[request_type], location,	retrieve_most_recent_cached_data def retrieve_most_recent_cached_data(self, request_type, location, number_of_records=1): return self.sqlite_cursor.execute("SELECT * FROM ? WHERE REQUEST_TIME = (SELECT MAX(REQUEST_TIME) FROM ?) AND" number_of_records)
# todo: add parameters to supply to integratedgradients.attribute? </s> def __init__(self, input_size=256, pretrained=false):	TinyMultiModal super().__init__() if pretrained:
# todo: consider returning an empty [] rather than raising </s> eq_(unique2.issuperset(unique1), false)	test_issuperset eq_(twin1.issuperset(twin2), True) eq_(twin2.issuperset(twin1), True) not_an_identity_set = object() assert_raises(TypeError, unique1.issuperset, not_an_identity_set)
#todo fixme: we should provide an option to create the page </s> claim.settarget(pywikibot.itempage(repo, claims[c+1]))	main@45 c = 0 while c != len(claims): real_claims.append(claim) c += 2
# todo: something a bit less heavy than eval </s> if isinstance(term, list):	flatten def flatten(terms): ret = [] ret.extend(term) else:
# todo ... </s> return false	cpreprocess_evaluate_ifdef arg = arg.strip() if not is_valid_defname(arg): return arg in state.macros
# todo: implement </s> p_ginzame = run_cmd(["ginzame", input_file])	test_ginzame p_ginza = run_cmd(["ginza", "-m", "ja_ginza", "-f", "2", input_file]) assert p_ginzame.returncode == 0
# todo: convert to casetransaction object </s> def _empty_actions():	_empty_actions
if not is_checksum_address(checksum_address):  # todo: more? </s> if not bool(filepath) ^ bool(checksum_address):	__read_tls_public_certificate @validate_checksum_address raise ValueError("Either pass filepath or checksum_address; Not both.") if not filepath and checksum_address is not None:
# todo remove .as_posix when requiring python 3.6 </s> logging.warning(	write@108 if not any(c.type == "tetra" for c in mesh.cells): raise WriteError("TegGen only supports tetrahedra") "TetGen only supports tetrahedra, but mesh has {}. Skipping those.".format( ", ".join([c.type for c in mesh.cells if c.type != "tetra"])
# todo(ivanlei): should score the vt results here and only add them if they're interesting </s> vt = virustotalapi(self._api_key)	LookupHashesFilter only_lookup_when=only_lookup_when, is_suspicious_when=is_suspicious_when, api_key='virustotal') reports = vt.get_domain_reports(self._all_iocs) for md5 in reports.keys():
# todo: weather data source changed, temporarily disable, will enable it later when new data source is available. </s> from_station_idx: int = action.from_station_idx	_on_action_received if evt is None or evt.payload is None: return to_station_idx: int = action.to_station_idx if from_station_idx < 0 or to_station_idx < 0:
#todo fix this </s> return none	__getMediaType elif xbmc.getCondVisibility('Container.Content(movies)'): return "movie"
# todo save the error to the plugin </s> self.activate_integration_app(plugins, force_reload=force_reload)	_activate_plugins plugins = self.plugins.items() logger.info(f'Found {len(plugins)} active plugins')
# todo: changing 'detailpage' to 'embedded' allows age-restricted content </s> if tag == 'meta' \	handle_starttag def handle_starttag(self, tag, attributes): and 'name' in attribute_dict \ and attribute_dict['name'] == "description":
# todo this closure is ugly. it also doesn't work with </s> return completion_names	Completion module = self._parser.module() completion_names += imports.completion_names(self._evaluator, if names is None and not isinstance(user_stmt, tree.Import): if not path and not dot:
# xxx todo: is there a better approach to handle the absense of a </s> ``__searchable__`` fields, indicating they need to be indexed. with these	after_flush we update the whoosh index for the model. If no index exists, it will be created here; this could impose a penalty on the initial commit of a model.
# todo: update authors' num_sounds (when not handled via trigger) </s> else:	get_channels_display return u"Mono" elif self.channels == 2: return self.channels
# todo(remove this when 11.6.1 is no longer supported) </s> method to sync the configuration from the device-group.	Device_Group cm.exec_cmd('run', utilCmdArgs=sync_cmd) def sync_from(self): :note:: Both sync_to, and sync_from methods are convenience methods which usually are not what this SDK offers.
# todo: support negative indexes </s> self.assertequal(self.linked_list.index_of_value('not exist'), none)	test_index_of_value self.assertEqual(self.linked_list.index_of_value('upon'), 1) self.assertEqual(self.linked_list.index_of_value('a'), 2)
# todo: this is just an example filter. </s> if all([hub, group, project]):	IBMQProvider config_dict = { 'url': url, config_dict.update({ 'hub': hub,
# todo: this really shouldn't be in this class </s> [p.key for p in self._printers])	_addPrintJob printer = next(p for p in self._printers if job.printer_uuid == p.key or job.assigned_to == p.key) except StopIteration: print_job = job.createOutputModel(printer) self._print_jobs.append(print_job)
# todo 目前仅在 华泰子类 中实现 </s> super(httrader, self).read_config(path)	read_config self.fund_account = self.__get_user_name()
# todo: set n_chunks to 1 when signals.shape[1] is small enough </s> series.shape[0] * percentile must be greater than n_confounds.	high_variance_confounds percentile: float Highest-variance series percentile to keep before computing the detrend: bool If True, detrend timeseries before processing.
# todo reenable </s> try_iter_content(types)	eval_all for key, element_values in self.unpack(): for element in element_values:
#todo wrap the lp api or use library </s> self.name = pkg	AbsPkg class AbsPkg: self.des = des
# todo: improve error message(add error code) </s> def validate_update_list(self, data):	BaseNestedFieldSerializerFactory return self.validate_data_list(data, partial=False) def validate_remove_list(self, data): if isinstance(data, dict): self.validate_pk_list(data.keys())
#todo would be better to log.exception here </s> args = list(args)	accept_singleton @decorator def wrapper(function, *args, **kw): args[position] = [args[position]] args = tuple(args)
# todo #1497 #1358 </s> receipt = self._set_winding_down(value=false)	disable_winding_down def disable_winding_down(self) -> TxReceipt:
# todo: encode data ids and decode ids. </s> outputs.append( output.get_api_value() )	ToolsController } outputs = rval[ "outputs" ] return rval
# todo: clarify variable names </s> manager = _iscmanager(collection_mgr)	get_manager :return: The object to manage the server with. global MANAGER  # pylint: disable=global-statement return MANAGER
# todo(b/155239129): used list_physical_device in `setup` for gpu tests. </s> v = eager_tf_executor.to_representation_for_type(	test_to_representation_for_tf_variable tf.Variable(10, dtype=tf.int32), {}, type_spec=computation_types.TensorType(tf.int32))
# todo data alignment stuff </s> def importer(component_type, type, json, gltf):	importer sparse = Sparse(component_type, type, json, gltf) SparseImporter.read(sparse)
""" only show the top todo. """ </s> "|  3| (c) baz @context1 +project1 key:value\n")	test_list17 command.execute() self.assertFalse(self.todolist.is_dirty()) self.assertEqual(self.errors, "")
#todo: fix this. </s> logger.info('this does not look like a (complete) cactus project (missing "%s" subfolder)', p)	verify_path if not os.path.isdir(os.path.join(self.path, p)):
# todo(dcramer): we need create a public api for 'sort_value' </s> sort_by = default_sort_option	ProjectGroupIndexEndpoint query_kwargs['assigned_to'] = request.user sort_by = request.GET.get('sort') query_kwargs['sort_by'] = sort_by tags = {}
# todo: fix inconsistent handling of water </s> s += line	_loop_to_string line = '\n  ' + val
""" type setting - todo explain """ </s> fields= (	ServiceDiscoverySerializer def get_endpoints(self,obj): return DISCOVERY['endpoints'] 'changeset','contact','key_service','endpoints'
pass  # todo </s> start = state['start']	tap def tap(x, y): if start is None: state['start'] = vector(x, y)
# todo also test these! </s> if e in dont_test:	test_all_estimators estimators = all_estimators() clf = LDA() continue with warnings.catch_warnings(record=True) as w:
# todo: log exception </s> output = subprocess.check_output(cmdline)	scan@71 output = "" if local: except subprocess.CalledProcessError as e: output = e.output
from gi.repository import glib  # todo: to fix </s> self.conversionaddressingtable[original_indice] = table.get(original_indice)	updateConversionAddressingTableWithTable def updateConversionAddressingTableWithTable(self, table):
# todo: check return value of attachthreadinput properly </s> return popup	PopupWindow def PopupWindow(self): Please do not use in production code yet - not tested fully
# todo add verbose output </s> def localedir(self, new_localedir):	LocalizationModel@18 def localedir(self): return self._localedir self._localedir = new_localedir
# todo: make truly async </s> while request is not none:	GCPFacade async def get_projects(self): projects = [] response = request.execute() projects.extend(response.get('projects', []))
# todo: what happens to sysex messages? </s> get all portmidi devices.	_get_all_devices _initialize() devices = []
# todo handle second-order transitions (trigrams) </s> n_classes : int	count_trans@9 Parameters ---------- Number of distinct labels. trans = np.zeros((n_classes, n_classes), dtype=np.int32)
# todo: fix this case with correct thresholding </s> self.dataset, pad_batches=false)	test_generator_evaluator_dc_metric_multitask_single_point def test_generator_evaluator_dc_metric_multitask_single_point(self): evaluator = GeneratorEvaluator(self.model, generator, []) metric = dc.metrics.Metric(dc.metrics.mae_score)
# todo remove set! duplicates should not be normal </s> comp_str = str(sorted(set([str(c) for c in completions])))	completion_test@43 print traceback.format_exc() fails += 1 if comp_str != correct: print 'Solution not correct, received %s, wanted %s' % \
#todo really dereference item? (sample pipe seems to suggest so: surprising) </s> value = reduce(lambda i,k:i.get(k), [item] + key['subkey'].split('.')) #forces an exception if any part is not found	pipe_rssitembuilder@30 for key in conf: try: else: value = util.get_value(conf[key], kwargs)
d.addcallback(self.failunlessisbardottxt) # todo: check headers </s> def _check(children):	failUnlessNodeKeysAre for k in expected_keys: assert isinstance(k, unicode) self.failUnlessEqual(sorted(children.keys()), sorted(expected_keys)) d.addCallback(_check)
# todo: use slotssequenceelement to render this. </s> def render_share_problem(self, ctx, data):	render_share_problem
# todo - is there a less expensive way to get these from the database </s> returns:	AjaxMixin else: return self.request.GET.get(name, None) dict object (empty) return {}
"""todo: is this function deprecated? </s> if key not in obj:	setDefault :type obj: bpy_types.Object :param key: The key to add to the object. obj[key] = value return obj[key]
#todo: we also need to account for presto/groups/comps metadata </s> if os.path.lexists(link_path):	_delete_ks_link log.info("Unlinking %s" % link_path)
os.remove(zip_path) # todo: caching (at least for the public version?) </s> md_file_contenu = get_blob(repo.commit(sha).tree, article_version['text'])	MEP repo = Repo(article.get_path()) manifest = get_blob(repo.commit(sha).tree, 'manifest.json') html_file = open( os.path.join(
#todo resend message + throttling </s> if request.session.test_cookie_worked():	verify_computer@122 user = authenticate(user=user, computer_id=computer_id) if user and user.is_active: request.session.delete_test_cookie() return HttpResponseRedirect(redirect_to)
# todo: replicate complete behaviour of urllib.urlopener.retrieve </s> self.__repository_mirrors = kwargs.get( "repository_mirrors", {} )	TUFancyURLOpener class TUFancyURLOpener( urllib.FancyURLopener ): def __init__( self, *args, **kwargs ): self.__tuf_updater = tuf.client.updater.Updater( "tuf_updater",
# todo add verbose output </s> @property	LocalizationModel@8 @domain.setter def domain(self, new_domain): def localedir(self): return self._localedir
# todo/fixme: combine and refactor all these rotation transformations </s> if axis >= 0:	right_rotate_covariance raise ValueError("Axis must be an integer") if axis < -ndim or axis >= ndim: axis -= ndim axes_R = [Ellipsis, abs(axis)+1, abs(axis)]
# todo: check success summary the same way. </s> examples.failure_reasons.reasonwithsubstorydi().b.run()	test_reason_without_protocol examples.failure_reasons.ReasonWithSubstoryDI().b() assert str(exc_info.value) == expected assert str(exc_info.value) == expected
# todo: all constant stuff should be calculated once, make this a class or something </s> return "n/a".rjust(len(fmt.human_size(0)))	fmt_sz try: return fmt.human_size(intval)
return rc  # todo?: parse </s> def searchauctions(self, *args, **kwargs):	searchAuctions
pass # todo: raise exception here </s> c = date[-5:-4]	strdate_to_time if (c == '+') or (c == '-'): date = date[:-6]
# todo better way to test this? </s> self.pooled_db.mt_test.remove({})	test_multithread for _ in range(20): t = SaveAndFind(self.pooled_db)
# todo (@awaelchli): standardize this across all plugins in lightning and lite. related refactor: #7324 </s> def on_validation_start(self) -> none:	on_validation_start
# todo(phawkins): enable test after jaxlib 0.1.22 is released. </s> x = x / onp.sum(x, axis=-1, keepdims=true)	testDirichletLogPdf shapes = (shapes[0] + (dim,), shapes[1] + (dim,)) def args_maker(): return [x, alpha] self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
# todo: we need to pick the rank from `comm_shm`, not `comm`, </s> options = kwargs['options']	DeviceOpenACCNoopOperator @classmethod @timed_pass(name='specializing.IET') if options['mpi']: mpiize(graph, mode=options['mpi'])
# todo: move upstream </s> def _kl_delta(p, q):	_kl_delta @register_kl(Delta, Distribution)
# todo: hack by genie for temporary markdown support </s> def _clean_up_line(line: text) -> text:	_clean_up_line @staticmethod
# todo: a better way. loot at https://github.com/openmined/pysyft/issues/5249 </s> if func:	generate_func def generate_func(target_method: str) -> Callable: def func(self: TypeAny, *args: TypeAny, **kwargs: TypeAny) -> TypeAny: return func(*args, **kwargs) else:
return none  # todo better error handling here </s> args["approval_prompt"] = "force"	authorize_redirect_url } if email_address: return url_concat(OAUTH_AUTHENTICATE_URL, args)
# todo handle valueerror </s> raise notimplementederror('%s must be implemented in subclass' % caller)	__abstract http://norvig.com/python-iaq.html """ import inspect
# todo(hartikainen): make this consistent such that there's no need </s> self._eval_n_episodes)	RLAlgorithm else: paths = rollouts(evaluation_env, policy, total_returns = [path['rewards'].sum() for path in paths] episode_lengths = [len(p['rewards']) for p in paths]
# todo: add explicit close of file_system. </s> raise runtimeerror(u'missing source.')	SourceIsFile Raises: RuntimeError: if source path specification is not set. return (not self.SourceIsStorageMediaImage() and self._source_file_entry.IsFile())
# todo for windows: </s> dst_label = dst_prefix_string + "{0:03d}".format(id) + ":"	main@123 with open('current_sa.txt', 'w') as fp: fp.write(str(id)+'\n') open_cmd = "screen -d -m -S %s " \ "rclone copy --drive-server-side-across-configs --rc -vv --ignore-existing " \
# update cache todo: should this be in the txn? </s> "select id from application_services where token=?",	_get_as_id_txn def _get_as_id_txn(self, txn, token): (token,) )
# todo this dosn't work yet </s> except typeerror as e:	testInvalidTraceback def testInvalidTraceback(self): try: self.assertIn("__traceback__ must be a traceback", str(e)) else:
# todo: move this hard-coded mixin/manager injections to maybe a model </s> return ordereddict(	schema "string": rows_fields.TextField, "text": rows_fields.TextField, [ (n, db_fields_to_rows_fields.get(t, rows_fields.Field))
# todo, awni, get rid of this on next pytorch update </s> end_tok = labels[0][-1]	end_pad_concat def end_pad_concat(labels): max_len = max(len(l) for l in labels) cat_labels = np.full((batch_size, max_len),
# todo(john sirois): clean this up when build parse refactoring is tackled. </s> return extension is none	has_sources def has_sources(self, extension=None):
# todo: errors </s> return false	set_label elif isinstance(c, Gtk.Label): c.set_markup(message)
# todo(henry-nash): add implementation here. </s> if int(version) < current_ver:	_assert_not_schema_downgrade if version is not None: try: raise migration.exception.DbMigrationError( _("Unable to downgrade schema"))
elif not self.urls and not self.packages:  #@todo: remove in 0.4.10 </s> try:	handleMultiPages m = re.search(self.PAGES_PATTERN, self.html) pages = int(m.group(1))
# todo(jblespiau): we can simply use buf.xla_shape() when version 0.1.58 is </s> args:	to_dlpack def to_dlpack(x: xla.DeviceArray, take_ownership: bool = False): Takes ownership of the contents of `x`; leaves `x` in an invalid/deleted x: a `DeviceArray`, on either CPU or GPU. take_ownership: If ``True``, JAX hands ownership of the buffer to DLPack,
os.rename(checkpoint_handler._saved[-1][1][-1], os.path.join(tb_logger.writer.log_dir, weights_name))  # todo: pr in ignite to have better access to saved file paths (cleaner) </s> logger.info("convert to tensor, pad and trim to trim_length")	get_data_loaders def get_data_loaders(args, tokenizer, trim_length, add_clf_token=None): Add a classification token at the end of each sample if needed. """ tensor_datasets = {} for split_name in ['train', 'valid']:
#todo change to native framework call, when plex allows token in header </s> user = arguments[arguments.index('user') +1]	playlistsV3 if len(args) > 0: arguments = [item.lower() for item in list(args)[0]] else: Log.Info('Invalid params for playlists.list ignored')
# todo: move to data class </s> resource_text = resource_text + str(url) + "\n"	create_tabs resource_urls = self.data["functionality"][parent]["vulns"][vuln_name]["resources"] resource_text = "" description_textarea = JTextArea() description_textarea.setLineWrap(True)
# todo: enable once pyyaml requirement resolved with python 3.8 </s> if value is not none and hasattr(self, key):	_update_from_dict settings (dict): The settings to update. updated = False if isinstance(value, str): value = value.strip()
srcs = [sourcelayer(n_out=n_in, x_out=x_in, name='')] #todo </s> def __init__(self, source, index, n_in, n_out, activation = t.nnet.sigmoid, reverse = false, truncation = -1, dropout = 0, mask = "unity", name = "lstm"):	LstmPeepholeLayer super(LstmPeepholeLayer, self).__init__(source, index, n_in, n_out, activation, reverse, truncation, dropout, mask, name = name) self.peeps_in = self.create_peeps(n_out)
# todo we could potentially check at the unit level and only reject </s> except exception as e:	_import_file@82 store, created = Store.objects.get_or_create(pootle_path=pootle_path) if rev < store.get_max_unit_revision(): raise ValueError("Could not create %r. Missing Project/Language? (%s)" % (file.name, e)) store.update(overwrite=True, store=pofile)
# todo(b/134950354): test embedding column for non-eager mode only for now. </s> preprocessing_layers=[	test_error_raised_if_missing_preprocessing_layer tensor_spec.TensorSpec([5], tf.float32), tensor_spec.TensorSpec([5], tf.float32) tf.keras.layers.Lambda(lambda x: x), ],
# todo: fix client side code to send us time information then adapt the next line </s> return 0	BooleanField def calculate_price(self, registration_data): data = registration_data.field_data.versioned_data return data.get('price', 0) if registration_data.data else 0 def get_friendly_data(self, registration_data):
raise deprecatedtest # this test is now broken. todo: fix it. </s> remote_prop = property(item, relation='many-to-one', remote_ap='test_ap')	make_test_second_ap def make_test_second_ap():
# todo: not for checkbox (should have checkbox class) </s> error_row=u'%s',	as_span def as_span(self): return self._html_output( row_ender='</span>', help_text_html=u'',
# todo: handle fancy-index copies by allocating a buffer and </s> start = lambda i: self.pixels_per_channel * i	design_mat_to_topo_view ' channels and ' + str(self.pixels_per_channel) + ' pixels per channel asked to convert design' stop = lambda i: self.pixels_per_channel * (i + 1) channels = [X[:, start(i):stop(i)].reshape(*channel_shape)
# todo: remove when botfactory can force everything to be unthreaded </s> assert isup.get_site_url(site) == expected	test_get_site_url @pytest.mark.parametrize('site, expected', VALID_SITE_URLS)
# todo: check nd reply is valid </s> self.logger = valve_util.get_logger('faucet', self.logfile, logging.debug, 0)	setup_valve self.faucet_event_sock = os.path.join(self.tmpdir, 'event.sock') self.logfile = os.path.join(self.tmpdir, 'faucet.log') self.registry = CollectorRegistry() self.metrics = faucet_metrics.FaucetMetrics(reg=self.registry) # pylint: disable=unexpected-keyword-arg
#todo factorisation with accessor code ? </s> def importer(component_type, type, json, gltf):	importer sparse = Sparse(component_type, type, json, gltf) SparseImporter.read(sparse)
#todo: check y.lod_level = 0 dtype </s> if isinstance(paddings, int):	unfold else: assert isinstance(dilations, list) and (len(dilations) == 2), \ paddings = [paddings] * 4 elif isinstance(paddings, list):
# todo: find out how to get global usernames </s> faved = mastodon.status(rest)	fav mastodon.status_favourite(rest)
# todo check for other files </s> self.logger.debug(	Parser def event(self, message): data = {} 'got faucet message for l2_learn: {0}'.format(message)) data['ip-address'] = message['L2_LEARN']['l3_src_ip']
# todo remove this assertion and test </s> interior_coords = x.reshape(-1, 2)	flip_delaunay def flip_delaunay(x): coords = X.copy() coords[is_interior_node] = interior_coords
# todo: replace with "yield from" when dropping python 2. </s> if match:	_get_stream_info def _get_stream_info(self): res = http.get(self.url) config = match.group(1) return parse_json(config, "config JSON",
# todo username </s> '--name=mon.',	create_mon 'ceph-authtool', keyring, '--add-key={monitor_secret}'.format( monitor_secret=monitor_secret,
assert study_id == 0  # todo </s> if param_name in t.params and t.result is not none	collect_param_result_pairs return [ (t.params[param_name], t.result)
#todo: it should be a way to use relative path for platform independent. </s> self.addlocation(path, qtcore.qt.unchecked)	loadItemToList for path in enabledTM: self.addLocation(path) self.ui.spinSimilarity.setValue(World.settings.value("Similarity", QtCore.QVariant(75)).toInt()[0]) self.ui.spinMaxCandidate.setValue(World.settings.value("Max_Candidates", QtCore.QVariant(10)).toInt()[0])
# todo: use new schema from caso_full when its ready </s> def boletim(self):	Boletim return get_table_model("covid19", "boletim")
#convert regex to python format: todo use a common routine for this </s> replace = re.sub('\$(\d+)', r'\\\1', replace)   #map $1 to \1 etc.   #todo: also need to escape any existing \1 etc.	pipe_strregex@28 for rule in conf['RULE']: match = util.get_value(rule['match'], kwargs) #todo use subkey? rules.append((match, replace)) for item in _INPUT:
# todo: really dirty. figure out a better way. </s> return all_actions	_apply_actions_limit current_num_actions = sum(x for _, x in actions_mapping.items()) limit = LIMITS_MAPPING[action_type] remaining_num_actions = limit - current_num_actions if remaining_num_actions < 0:
raise exceptions.mpdnotimplemented  # todo </s> underscore, dash, dot and colon.	subscribe@18 ``subscribe {NAME}`` Subscribe to a channel. The channel is created if it does not exist raise exceptions.MpdNotImplemented  # TODO
# todo log here </s> except httplib.httpexception:	get_user_id@24 'Authorization': 'SSWS %s' % OKTA_API_KEY, }, return None if response.status_code != 200:
# todo: verify behavior </s> self.expected_response(	test_few self.send_request() received = self.vsc.received threads=[ {'id': 1, 'name': 'spam'},
# todo: how does this work with backfilling? </s> with open(schema_path(schema)) as schema_file:	read_schema A string containing the database schema.
# todo: 289 </s> signing_power = self._crypto_power.power_ups(signingpower)	generate_self_signed_certificate return signing_power.generate_self_signed_cert(self.stamp.fingerprint().decode())
# todo(mnaser): remove this in patch resolving the issue </s> def _setup_compute_service(self):	_setup_compute_service
# todo: jrk: chunking times points needs to be simplified </s> n_jobs : int	GeneralizationAcrossTime Default: 'cross-validation' scorer : object | None Number of jobs to run in parallel. Defaults to 1. Attributes
# todo replace hardcoded integers into constants/flags from cffi </s> class servernotfoundinkerberosdatabase(gsserror):	ServerNotFoundInKerberosDatabase
#todo: implement more realistic closing semantics </s> return (a,b)	pair b_to_a = MessageChannel() a = cls(a_to_b, b_to_a)
# todo: nix unittest for pytest </s> self.assertequal(key.get_name(), "ecdsa-sha2-nistp384")	test_generate_ecdsa msg.rewind() self.assertTrue(key.verify_ssh_sig(b"jerri blank", msg)) key = ECDSAKey.generate(bits=521) msg = key.sign_ssh_data(b"jerri blank")
# todo: use a contextmanager to ensure we always delete the callback from the list. </s> asyncio.ensure_future(self.populate_not_full_buckets())	bond if not got_pong: logger.debug("bonding failed, didn't receive pong from {}".format(node)) return False yield from self.wait_ping(node)
# todo: clean this up. shouldn't we move the "checks" stuff to the </s> return "all-agents" in tags_of_host(hostname)	is_all_special_agents_host
# todo: enable this check when #1502 is fixed </s> 'name': u'package_%s' % str(i).zfill(2),	setup_class packages = [] for i in range(cls.num_packages_in_large_group): 'groups': u'group_00' })
# todo: fix this, is it needed? </s> channel_req = mcschanneljoinrequestpdu()	RDP@117 "Expected: Channel Join/Client Security Packet.Got Nothing.") return v = channel_req.parse(data) if v < 0:
# todo fitness? </s> u = self.softmax(u)	mixture_from_payoff def mixture_from_payoff(self, payoff, sum_dim, memory): u = np.reshape(u, [len(memory)]) print(u)
# todo: check if a success http code can be returned with an empty body </s> raise endpoints.notfoundexception("card not found.")	timeline_get name="timeline.get") def timeline_get(self, card): return card
# todo: add for morph targets data. </s> translation = convert_swizzle_location(translation)	decompose_transition def decompose_transition(matrix, context, export_settings): translation, rotation, scale = matrix.decompose() rotation = convert_swizzle_rotation(rotation) scale =  convert_swizzle_scale(scale)
# todo(hirofumi): apply character lm here </s> cs (np.ndarray): array of next labels. a tensor of size `[beam_width]`	CTCPrefixScore def __call__(self, hyp, cs, r_prev): Args: r_prev (np.ndarray): previous CTC state Returns:
# todo: move load and cpuload to sysinfo </s> get_avail_gov = s.getoutput("cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_available_governors")	avail_gov get_gov_state = s.getoutput("cpufreqctl --governor") gov_state = get_gov_state.split()[0]
except (testtransactionfailed, validationerror, valueerror):  # todo: 1950 </s> return receipt	disable_winding_down def disable_winding_down(self) -> dict:
# todo: large gains also expected when precalculating psi. </s> return f < f	should_continue
# todo don't use exceptions to control program flow </s> return (pagesize, orderby)	_get_parameters_values current_url = resolve(request.path_info).url_name pagesize = request.GET.get('count', request.session.get('%s_count' % current_url, default_count))
#todo: actually optimize free space on the texture. </s> if write:	file_open flags = 'w' else:
from vyper.old_codegen.expr import expr  # todo rethink this circular import </s> min_return_size = abi_return_t.min_size()	_unpack_returndata return ["pass"], 0, 0 return_t = calculate_type_for_external_return(return_t) max_return_size = abi_return_t.size_bound() ret_ofst = buf
# todo: figure out a solution that doesn't require hoping that </s> self.assert_dispatched_count(1, 'vas2nets.event')	test_send_sms_success yield self.worker._process_message(self.make_outbound("outbound"))
# todo find out what's wrong </s> 54 + p_m * 18*sqrt5 + 6*sqrt39 + p_m * 2*sqrt195	StroudSecrest nu, xi = numpy.sqrt(-50 + p_m*10*sqrt5 + 10*sqrt39 - p_m*2*sqrt195) eta = sqrt(36 + 4*sqrt39) ) A = (1725 - 26*sqrt39) / 2940
# todo: exit codes (not only for this, but for other exceptions) </s> poa=poa,	ursula@237 if dev: ursula_config = UrsulaConfiguration(dev_mode=True, download_registry=False, registry_filepath=registry_filepath,
# todo: use different flag than .reentrant </s> state.glpane = colorsorter.glpane	_suspend state._immediate = ColorSorter._immediate state._gl_name_stack = ColorSorter._gl_name_stack state._initial_transforms = ColorSorter._initial_transforms state.sorted_by_color = ColorSorter.sorted_by_color
# todo: move error code and field outside function </s> "`allowed_client_hosts` configuration."	validate_storefront_url@30 if not validate_host(domain, settings.ALLOWED_CLIENT_HOSTS): error_message = ( ) raise ValidationError(
# todo: deprecated - remove in version 0.10 </s> return simplepolicyensemble(policies)	_create_ensemble if policies is None: return SimplePolicyEnsemble([]) elif isinstance(policies, PolicyEnsemble): return policies
# todo debug </s> self.type = none	RuleElement class RuleElement: self.triggered = False self.timeWhenTriggered = 0.0
# todo: error handling like numba callwrappers.py </s> class quantiletype(abstracttemplate):	QuantileType @infer_global(quantile) def generic(self, args, kws): assert not kws
#todo need gutter of scrollbar - how do we get that? </s> self.keymask |= x	on_key_press_event def on_key_press_event(self, object, event): x = self.keylookup.get(event.keyval, 0) for l in self.linkmap[:self.num_panes-1]: a = l.get_allocation()
# todo: add test here as well </s> def test_wut():	test_wut
raise notimplementederror # the below does most probably not work anymore todo </s> if not a_to_b and not b_to_a:	detail return "It does not differ from `{0}`.".format(b) elif not a_to_b:
# todo: type ignored -- breaks liskov substitution. </s> option set.	_check_all_skipped import doctest all_skipped = all(x.options.get(doctest.SKIP, False) for x in test.examples)
# todo: remove when botfactory can force everything to be unthreaded </s> assert isup.get_site_url(site) == expected	test_get_site_url @pytest.mark.parametrize('site, expected', VALID_SITE_URLS)
# todo: marker </s> if self.debug:	addDebugString self.debugString += string
if source_file:  # todo: should we error here or something if the source_file doesn't exist? </s> else:	SpecFileGenerator else: if suggested_extension.lower() == original_extension.lower(): try: original_format = extension_to_format(original_extension)
# todo remove comment parameter. </s> def cast_char(value, size):	cast_char
self.info          = {}  #@todo: remove in 0.4.10 </s> if elapsed_time < self.getconf("sendtimewait"):	AndroidPhoneNotify if self.core.isClientConnected() and not self.getConfig("ignoreclient"): return return if elapsed_time > 60:
# todo: log modification too? </s> if attributes is none:	create_span def create_span(directory, document, start, end, type, attributes = {} else:
# todo: maybe change this later to push some more info, not just the </s> if self.serverorclient:	communication_win for addr in messages: for message in messages[addr]: self.authenticated = False c = self.lookup_client_pub((addr, 0))
# todo(pradeep): try not to use the function of a member object. may be expose </s> embedded_input = embedding_layer(input_layer)	load_encoder input_length = memory_network_model.get_input_shape_at(0)[0][1] self.max_sentence_length = input_length encoded_input = encoder_layer(embedded_input) self.encoder_model = Model(input=input_layer, output=encoded_input)
# todo: this logic does not prevent duplicate test cases, need to address this in the future. </s> self._sock.send(data)	SocketConnection :param data: Data to send. :return: None elif self.proto == socket.SOCK_DGRAM: if len(data) > self.max_udp:
# todo (aron): add i18n by varying the language of the topic tree here </s> last_entry = self.entries.order_by('-sort_order').all()[0]	add_entry already existing sort order. if 'sort_order' not in kwargs:  # by default, append entry to the playlist kwargs['sort_order'] = last_entry.sort_order + 1 except IndexError:  # no entries yet
# todo: abstract and require implementation? </s> self._getkey(jobstorefileid, headers).get_contents_to_file(writeable, headers=headers)	readFile if not self.exists(jobStoreFileID): raise NoSuchFileException(jobStoreFileID)
self.asserttrue(greps(out, "zzc.service.pid")) # todo ? </s> cmd = "{systemctl} show a.service"	test_2027_show_unit_for_oneshot_service Type=oneshot ExecStart=/bin/echo foo out, end = output2(cmd.format(**locals())) logg.info(" %s =>%s\n%s", cmd, end, out)
# todo(shardy): may be able to remove when the bug above is fixed </s> try:	authenticate@128 In the case of EC2 style authentication this will also set the username in the context so we can use it to key in the database. nova = client.Client(username=con.username, api_key=con.password,
# todo: exception for coalesces that represents all sub_specs tried </s> pass	D
self._cloud_flow_complete_message.addaction("", i18n_catalog.i18nc("@action", "review your connection"), "", "", 1) # todo: icon </s> device.connect()	_onAddDevice global_container_stack = CuraApplication.getInstance().getGlobalContainerStack() if global_container_stack and device.getId() == global_container_stack.getMetaDataEntry("um_network_key"): device.connectionStateChanged.connect(self._onDeviceConnectionStateChanged)
# :todo: implement test. </s> self.skiptest('not implemented yet.')	ReplayBundleRequestFilterTestCase self.skipTest('Not implemented yet.') def test_pass_optional_parameters_excluded(self): def test_fail_empty(self): Request is empty.
# todo: separate the service from the node model </s> def ping(self, request: qrl_pb2.pingreq, context: object) -> qrl_pb2.pongresp:	Ping
# todo simplify </s> trained_model (baseestimator): a scikit-learn trained algorithm	metrics Args:
# todo: keep the parser around, so we don't have to </s> info_ptr = pm.lib.pm_getdeviceinfo(id)	_get_all_devices _initialize() devices = [] if info_ptr: devinfo = info_ptr.contents
# todo hacky console hacky-ness </s> self.insert([str(pc), op['op']], [tag, op['op']])	set_opcodes tag = "NoSource" else:
# todo: handle this in the ui </s> layers.removeobserver(	endSelfLayersObservation def endSelfLayersObservation(self): observer=self, notification="LayerSet.DefaultLayerWillChange") layers.removeObserver(
# todo: serialize the policy </s> character control endpoint for policy granting.	make_alice_control return Response('Policy created!', status=200) @alice_control.route("/grant", methods=['POST']) pass return alice_control
# todo: direct use of json['error'] is deprecated; use display_message('...', 'error') instead. </s> j_dic['equivs'] = []	enrich_json_with_base j_dic['events'] = [] j_dic['triggers'] = [] j_dic['infos'] = []
# :todo: implement test. </s> verifies that the command is wired up correctly.	SendTransferCommandTestCase super(SendTransferCommandTestCase, self).setUp() self.adapter = MockAdapter() self.assertIsInstance( Iota(self.adapter).sendTransfer,
# todo make fetch_result _not_ a pd.dataframe </s> self.assertequal(fetch_result, processresult(	test_table_index_under fetch_result = fetch(url=url, tablenum=0)
# todo it may be interesting to get a random bucket among the acquirable </s> return buckets	create_buckets_ordered_randomly Produce buckets for [0, #LHS) x [0, #RHS) and shuffle them. buckets = create_buckets_ordered_lexicographically(nparts_lhs, nparts_rhs)
# todo: support format.binary once it is supported in </s> if span is none:	active @property def active(self): return None span_context = SpanContextShim(span.get_context())
# todo(leofang): test float16 ('e') once cupy/cupy#5346 is resolved </s> y = cupy.empty_like(x)	test_raw_grid_3D arr2[x, y, z] = arr1[x, y, z] l, m, n = (2, 3, 4) f(((l+1)//2, (m+1)//2, (n+1)//2), (2, 2, 2), (x, y, l, m, n)) assert (x == y).all()
# todo use the json output option to ceph so we can stop scraping </s> the test if this cluster can't handle that".	CephControl Some configuration arguments may be interpreted by a dev implementation as a "simulate this", while a real-cluster def configure(self, server_count, cluster_count=1): Tell me about the kind of system you would like.
# todo: test for the _correct_ revision_id value. </s> 'user':testactivity.normal_user.name}	test_create_package model.activity.ActivityDetail).all()) before = datetime.datetime.now() request_data = self._make_test_package() package_created = package_create(context, request_data)
# todo: only works for ratios </s> relative (boolean): if relative==true, then the values will be returned	subgroup_deltas less observations are given, then NaN is returned. nruns (integer): number of bootstrap runs to perform if assume normal is as distances below and above the mean, respectively, rather than the absolute values. In	this case, the interval is mean-ret_val[0] to
# todo: make this config driven </s> buf = os.read(src_fd, blksize)	copy_image blksize = 64 * 1024 log.debug("copying %s to %s" % (src, dst)) if len(buf) <= 0: log.debug("%d %d blocks written." % (blks, blksize))
# todo: figure out a better way of handling rpc style calls. </s> for msg in msg_list:	IOPubStreamRouter class IOPubStreamRouter(ZMQStreamRouter): def _on_zmq_reply(self, msg_list): client.write_message(msg) def forward_unicode(self, client_id, msg):
# todo: remove this method in v2.5 </s> )	remove_from_device partition=self.want.partition
# todo: this is a temporal fix </s> flag to print es related information.	es The pixel data for the image, where the last axis represents the number of channels. Default: False Raises
# todo: distinguish between urllib and urllib2 contracts </s> raise notimplementederror	go_away
node.test = gast.call(gast.attribute( # todo any over dim 0 </s> node.value.ctx, none)	FuseAttributes if not isinstance(node.value, gast.Name): return node
# todo: identify the specific structure we're finding and document this a bit better </s> if pdb_name in self._pdb_names:	PdbSigantureScanner if (null - sig - self._RSDS_format.size) <= 100: name_offset = sig + 4 + self._RSDS_format.size (g3, g2, g1, g0, g5, g4, g7, g6, g8, g9, ga, gb, gc, gd, ge, gf, a) = \ self._RSDS_format.unpack(data[sig + 4:name_offset])
# :todo: implement test. </s> self.assertequal(tryte[2].value, -1)	test_init_mixed_types self.assertEqual(tryte[0], Trit(1)) self.assertEqual(tryte[1], Trit(1))
# todo log this </s> if aggobj_path:	Search rtype = r.get('type', None) rpath = r['path'] aggobj_path = opj(agg_base_path, aggobj_path) md = jsonload(aggobj_path)
kind = 'article'  # todo: recognise pages </s> posts = json.loads(handle.read().decode('utf-8'))	get_posterous_posts request = urllib_request.Request(url) request.add_header("Authorization", "Basic %s" % base64string.decode()) return posts
# todo: when merging forward to 0.56.x.x, arnold lights use </s> return arnold.atstringtostr( value )	__aiMetadataGetStr if arnold.AiMetaDataGetStr( nodeEntry, paramName, name, value ) :
# todo: warn the user if mode of ensemble </s> votes = np.zeros((n_samples, n_classes))	transform2votes def transform2votes(output, n_classes): for i in range(n_samples): idx = output[i]
# todo: the following reproduces the old behavior of </s> return 1 / (tmp[idx + 1] - tmp[idx - 1]) * \	_ctr_fun raise IndexError("list index out of range")
# todo we could reload the message </s> @property	Forward@122 async def get_input_sender(self): Returns `input_sender` but will make an API call if necessary. def chat(self): The :tl:`Channel` where the original message was sent. This may be
# todo change to native framework call, when plex allows token in header </s> else:	viewstate if param in GET: self.function = param pass elif metode == 'post':
# todo(b/151468119): remove this branch after next release. </s> def _run_model_inference(self, model_path: text,	Executor model_spec) logging.info('BulkInferrer generates prediction log to %s', output_path) example_uris: Mapping[Text, Text], output_path: Text,
# todo: fill this in </s> key.get()	_objects_filter key = self.Object(key)
# todo: log exception </s> if ('ts' in request.args and 'uid' in request.args):	get_notes@753 task = db.get_task(task_id) if not task: ts = request.args.get('ts', '') uid = request.args.get('uid', '')
# todo (t65593688): this should be removed after </s> train_filename=test_file_name,	get_tensorizers schema = {"source_sequence": str, "dict_feat": Gazetteer, "target_sequence": str} data_source = TSVDataSource.from_config( field_names=["source_sequence", "dict_feat", "target_sequence"], ),
# todo cachew for all commits? </s> gr = git.repo(str(repo))	repo_commits for r in gr.references: yield from _repo_commits_aux(gr=gr, rev=r.path)
# todo: fetch that from the api with paraminfo </s> possible values of 'group' may vary depending on wiki settings,	has_group but will usually include bot. if not self.logged_in(sysop):
# todo(vek): need to pass context in for access to auth_token </s> assert state in power_state.valid_states(), "bad state: %s" % state	InstanceInfo self.name = name
aiplayer.load_abstract_buildings(self.session.db) # todo: find a better place for this </s> but on the coast of an island"""	get_random_possible_coastal_ship_position @decorators.make_constants() offset = 2 while True:
#todo(#212): use a map construct instead of unrolling. </s> def convert_element_type_dtype_rule(operand, new_dtype, old_dtype):	convert_element_type_dtype_rule
# todo: handle dbkeys </s> trans.app.security_agent.set_all_dataset_permissions( new_dataset.dataset, hda_permissions )	ToolsController visible=False ) target_history.add_dataset( new_dataset ) data_provider = data_provider_registry.get_data_provider( trans, original_dataset=input_dataset, source='data' ) trans.app.object_store.create( new_dataset.dataset )
# todo move interpretation of data into column config </s> if self._order_by:	order_by @property return [ (col.field, order)
# todo replace with collections.sequence subclass </s> will always return :class:`none` if the search isn't loaded.	did_you_mean @property def did_you_mean(self): spotify.Error.maybe_raise(self.error) did_you_mean = utils.to_unicode(
# todo delete me </s> specular = materialdata['specularcolor']	exportSDFMaterial diffuse = materialdata['diffuseColor'] tagger.attrib('diffuse', '{0} {1} {2} {3}'.format( tagger.attrib('specular', '{0} {1} {2} {3}'.format( specular['r'], specular['g'], specular['b'], 1.0))
# todo find out if this is good because of sparcity... </s> def get_hyperparameter_search_space(dataset_properties=none):	get_hyperparameter_search_space learning_rate = UniformFloatHyperparameter( name="learning_rate", lower=0.0001, upper=1, default=0.1, log=True)
#todo(chris): implement service_catalog </s> super(httpclient, self).__init__(timeout=timeout)	HTTPClient class HTTPClient(httplib2.Http): USER_AGENT = 'python-novaclient' self.user = user self.apikey = apikey
# todo(ylc/zhifengc): add this to a policy module and test it. </s> args:	PreprocessTpuEmbeddingInputBatch Used by CreateTpuEmbeddingEnqueueOps(). Override this method in input generators to preprocess the TPU embedding inputs before using them to input_batch: The input batch to process. Returns:
# todo: check if output is spent </s> return res['hex']	getrawtransaction def getrawtransaction(self, txid):
# todo note this is inefficient since we are running the raw dataframe through the pipeline twice. </s> returns a plot of the roc curve of the holdout set from model training.	roc_curve_plot pass
except exception:  # todo: refactor this... </s> def test_init(self):	test_init
# todo: find better ways... </s> return self.char_categories[cate]['invoke']	unkown_invoked_always def unkown_invoked_always(self, cate): return False
# todo: set the following parameter </s> if self.request:	request_host return self.request.host
# todo: this isn't a true unit test.  it depends on the test cmakelists.txt file having been written correctly. </s> platform.write_test_cmakelist([])	test_write_compiler_test_file assert(os.path.exists("cmake_test_compile/CMakeLists.txt"))
# todo - verify contents </s> self.assertequal(response.status_code, 301)	testReviewDetail0 def testReviewDetail0(self):
duration_s = none  # todo </s> except keyerror:	is_pupil_mobile_recording return info_csv["Capture Software"] == "Pupil Mobile" and "Data Format Version" not in info_csv
# todo: document! </s> q_oper = _super_tofrom_choi(q_oper)	super_to_choi def super_to_choi(q_oper): Takes a superoperator to a Choi matrix q_oper.superrep = 'choi' return q_oper
# todo: more arguments possible: objectdb etc. </s> raise notimplementederror	git_dummy_command
# todo: t196619 </s> if self.site.mw_version < '1.32':	_add_slots * list=alldeletedrevisions More info: return request = self.request
# end todo </s> sqrt_ggn = einsum('bijc->bic', sqrt_ggn)	bias_diag_ggn out_pixels = module.output_shape[2] * module.output_shape[3] sqrt_ggn = sqrt_ggn_out.view(batch, module.out_channels, out_pixels, return einsum('bic->i', (sqrt_ggn**2))
# todo: change to 'internal cat' (issue 1013) </s> def all():	All
#! todo: consider making linestyle configurable </s> usage	n_circles def n_circles(phases, mag_min=-40.0, mag_max=12.0): Gol is an open-loop transfer function, and Gcl is a corresponding ===== contours = n_circles(phases, mag_min, mag_max)
# todo: convert to utils.retry </s> 'a11']}.	NiceSort def NiceSort(name_list): Given a list of names C{['a1', 'a10', 'a11', 'a2']} this function The sort algorithm breaks each name in groups of either only-digits or no-digits. Only the first eight such groups are considered, and
# todo: can cause an endless loop for single track repeat. </s> return self.current_tl_track	get_current_tl_track
# todo: avoid dummy and generate func here when inlining is possible </s> return lambda df: 0	df_len_overload if len(df.columns) == 0:  # empty df
# todo: may test with codecs.open passing an encoding </s> self.assert_expected_table(table)	PluginCsvTestCase self.files_to_delete.append(temp.name) rows.export_to_csv(utils.table, temp.file)
# todo(hub-cap): turn this into middleware </s> mapper = routes.mapper()	API class API(wsgi.Router): super(API, self).__init__(mapper) self._instance_router(mapper)
#todo - is there a nice way to return an interator and </s> local=true):	pairwise_alignment_check def pairwise_alignment_check(self, query_seq, targets = list(targets) alignments = list(alignments)
#todo: use universe / ag that </s> with pbc active.	time_bbox_pbc self.ag.bbox(pbc=True)
# todo: verify </s> else:	get_omit_set for nid, comp in zip(omit.ids, omit.components): for compi in comp: raise NotImplementedError(omit) return omit_set_map
# todo: for some reason this test used non-stadard slashing parameters (#354) </s> user_escrow_proxy, _ = testerchain.interface.deploy_contract(	user_escrow_proxy escrow, _ = escrow policy_manager, _ = policy_manager 'UserEscrowProxy', token.address, escrow.address, policy_manager.address) linker, _ = testerchain.interface.deploy_contract(
# todo(higumachan): remove this "if" section after tensorflow supports python 3.7. </s> study = optuna.create_study(pruner=deterministicpruner(false))	test_keras_pruning_callback@22 study = optuna.create_study(pruner=DeterministicPruner(True)) study.optimize(objective, n_trials=1) study.optimize(objective, n_trials=1) assert study.trials[0].state == optuna.structs.TrialState.COMPLETE
# todo: add attachements to test notebook </s> exporter = build_page_exporter(file, 'latex', 'article', page)	TestTemplateOptions dir =  Dir(self.create_tmp_dir()) file = dir.file('test.tex') notebook = tests.new_notebook(fakedir='/foo') selection = SinglePage(notebook, page)
# xxx todo </s> def input_integer(self, token):	input_integer
# todo: use actual github clone string used by github </s> flash(_("repository %s succesfuly removed (but files remain on the server)" % project_id))	delclone db.session.commit()
# todo: dump to file </s> if type(expr) != str:	regexp expr = str(expr) regex = re.compile(expr)
)  # todo(unilight): is changing to ilens_ds_st right? </s> returns:	base_plot_keys and `validation/main/loss` values. also `loss.png` will be created as a figure visulizing `main/loss` list: List of strings which are base keys to plot during training. plot_keys = ["loss", "l1_loss", "l2_loss", "bce_loss"]
#ack = self.serial_port.read() # todo: use ack </s> except serial.serialexception:	Disconnect try: if self.serial_port.isOpen(): sys.stderr.write("Error closing the port {0}".format(self.serial_name))
# todo: does this need to be smarter? </s> if component.name() in ignoredcomponents:	addPropertyToAllComponents @param property: the property to add @type property: L{Property} continue component.addProperty(property)
# todo: how to check it? meybe we can omit this test </s> f_ng = importer.get_op_handle("y")	test_constant workspace.RunNetOnce(net) importer = C2Importer() f_result = ngt.make_transformer().computation(f_ng)() assert(np.ma.allequal(f_result, workspace.FetchBlob("Y")) and f_result[0] == val)
# @todo: resolve makedeps in case if it was specified by provides, </s> def last_installed_hash(self):	last_installed_hash if self.is_installed: with open(self.last_installed_file_path) as last_installed_file:
# todo: figure out way to paramaterize node['osds'] for this test </s> def test_osds_listen_on_public_network(self, node, socket):	TestOSDs @pytest.mark.no_docker def test_ceph_osd_package_is_installed(self, node, Package): for x in range(0, node["num_devices"] * 2): port = "680{}".format(x)
# @todo: also add 'clear' button to clear all elements & start from a blank slate </s> "incident_type_id",	custom_prep current.s3db.gis_location.addr_street.label = T("Street Address or Location Details") from s3 import S3SQLCustomForm (T("Who am I speaking with?"), "reported_by"), (T("How can we contact you?"), "contact"),
# todo(py3.7): add required=true </s> def test_build(self):	test_build self.assertBuilds()
# todo: confirm necessity of this session clearing and lay out mechanics. </s> 'http://localhost:' + str(self.port) + '/' + self.random_string(),	test_download_url_to_tempfileobj_and_urls self.random_string(), self.target_data_length) self.assertRaises(requests.exceptions.HTTPError, self.target_data_length) self.assertRaises(requests.exceptions.ConnectionError,
# todo: is this required for a visual operation? </s> if vi_cmd_data['count'] == 1:	vi_big_c def vi_big_c(vi_cmd_data): vi_cmd_data['mode'] = _MODE_INTERNAL_NORMAL vi_cmd_data['motion']['command'] = 'move_to' vi_cmd_data['motion']['args'] = {'to': 'eol', 'extend': True}
return none  # todo better error handling here </s> return url_concat(oauth_authenticate_url, args)	authorize_redirect_url if email_address: args['login_hint'] = email_address
# todo: why the reverse order? </s> def from_polar(direction=0.0, magnitude=1.0):	from_polar
# todo remove this in a future version </s> to reject an event """	RejectEvent
# todo: remove in v8 </s> for t in task:	flatten if isinstance(task, dict): yield task for ft in flatten(t): yield ft
# todo: it would be nice to be async about this. set 1 second timeout. </s> '  version %s is available.\nsee %s'	version_check@47 warnings.warn('Bayeslite is not up to date.'
# todo: find a way to make the reduction only once, so we don't need to clone. </s> _recursive_fx_apply(value, tbptt_reduce_fx)	reduce_across_time if isinstance(value, list): value = torch.tensor(value) else: result[k] = tbptt_reduce_fx(value.float())
# todo: checks for being not outside of this repository </s> file_: str	rm_url def rm_url(self, file_, url): Parameters url: str self._run_annex_command('rmurl', annex_options=[file_] + [url])
# todo: i can't manage the import issue, can you? </s> else:	LogTensor def __repr__(self) -> str: if hasattr(self, "child"): return type(self).__name__ def on(self, tensor):
if self._ndim == 3: # todo: use hasz </s> return d.value	getX cs = lgeos.GEOSGeom_getCoordSeq(self._geom) d = c_double()
# todo: something a bit less heavy than eval </s> ret.extend(term)	flatten ret = [] for term in terms: else: ret.append(term)
# todo: replace xrange (could fail with 32-bit python 2.x). </s> if not self.len:	__irshift__ n -- the number of bits to shift. Must be >= 0. if n < 0: raise ValueError("Cannot shift an empty bitstring.") if n == 0:
# todo: verify set_fields </s> if cookie is none:	flowmod priority = 0 # self.dp.lowest_priority if inst is None: cookie = self.flow_cookie flags = 0
# todo: handle external images </s> def jsonify(self, app: sanic) -> dict:	Template@73 if path.exists(): return path return { "name": self.name,
# @todo: copy relevant parts of translate toolkit internally to avoid external dependencies </s> return postrings	merge_strings for st in extra: postrings.append(st)
# todo: remove this asap. </s> new_items = []	_make_unicode if isinstance(entity, str): return unicode(entity) for item in entity: new_items.append(self._make_unicode(item))
# todo: recursive </s> args = convert_args(args)	resolver return wrap(source, info, **args)
# todo: should actually be implemented by annexrepo </s> with open(_file, 'a') as f:	initiate lgr.debug("Initiating protocoling." "cd %s; vim %s" f.write(self.HEADER) os.chmod(_file, 0755)
# todo consolidate to base class </s> else:	CarbonEventLoop while not self.has_exit: if self._force_idle: duration = kEventDurationForever if carbon.ReceiveNextEvent(0, None, duration,
# todo: check that body contains link to dashboard and email prefs. </s> cls.joeadmin = {'id': joeadmin.id,	setup_class ckan.tests.CreateTestData.create() cls.app = paste.fixture.TestApp(pylons.test.pylonsapp) 'apikey': joeadmin.apikey,
# todo - log details about the test </s> svip=dict(required=false, type='str', default=none)	SolidFireConnection self.argument_spec.update(dict( skip=dict(required=False, type='str', default=None, choices=['mvip', 'svip']), )) self.module = AnsibleModule(
# avdn: todo </s> self.assertequal(all([true, 1, none, 'a']), false)	testAll def testAll(self): self.assertEqual(all([True, 1, '', 'a']), False) self.assertEqual(all([True, 1, False, 'a']), False)
# todo: also deal with empty directories </s> return path.replace("\\", "/")	to_remote_path
# todo: replace by plugins_info.move_to_end('coretasks') for python 3 </s> found in ``directory``	find_directory_plugins def find_directory_plugins(directory): for _, abspath in _list_plugin_filenames(directory): yield handlers.PyFilePlugin(abspath)
# todo(tsileo): also update following (it's in the object) </s> if flags:	task_process_new_activity flags.update(flag(MetaKey.PUBLISHED, activity.published)) set_inbox_flags(activity, flags) DB.activities.update_one({"remote_id": activity.id}, {"$set": flags}) app.logger.info(f"new activity {iri} processed")
raise mpdnotimplemented # todo </s> result = self._find(type, what)	_findadd @register(r'^findadd "(?P<type>(album|artist|title))" "(?P<what>[^"]+)"$')
# todo: implement versioning on all subclasses </s> subclasses.extend(get_subclasses(sc))	get_subclasses subclasses = [] for sc in cls.__subclasses__(): return subclasses
# todo: avoid using default index? </s> (3, 3)	levshape ('c', 'z')], ) internal = self._internal result = internal._sdf.agg(*(F.countDistinct(c) for c in internal.index_scols)).collect()[0]
# todo: update the tolerance after the ci moves to torch 1.10 </s> return [x["array"] for x in speech_samples]	_load_datasamples speech_samples = ds.sort("id").filter( lambda x: x["id"] in [f"1272-141231-000{i}" for i in range(num_samples)]
# todo: check nd reply is valid </s> self.sock.connect(self.faucet_event_sock)	setup_valve self.valve = valve_factory(dp)(dp, 'test_valve', self.notifier) self.valve.update_metrics(self.metrics)
# todo: test with ion/ioff </s> vm.add_magic_command("dataexplore", handle_dataexplore)	load_plugin vm.add_magic_command("de", handle_dataexplore) vm.add_value_tweaker(tweak_value)
# todo bind a log_cb to this config ^^ </s> def id(self):	id @property
# todo 0.24: raise a valueerror instead of a warning </s> split. you can make the results identical by setting ``random_state``	GroupShuffleSplit Notes ----- to an integer. return super().split(X, y, groups)
# todo[k]: remove when t853 is properly fixed. </s> thread_path = '/threads/{}'.format(thread_id)	test_event_generation sync_data = api_client.get_data('/delta?cursor={}'.format(cursor)) assert len(sync_data['deltas']) == 1 api_client.put_data(thread_path, {'add_tags': ['foo']}) sync_data = api_client.get_data('/delta?cursor={}'.format(cursor))
sort_by = none if 'sort_by' not in parameters else parameters['sort_by'][0]  # todo check integer! </s> for index in picked_torrents:	SearchEndpoint picked_torrents = random.sample(range(0, num_torrents - 1), random.randint(20, num_channels - 1)) picked_channels = random.sample(range(0, num_channels - 1), random.randint(5, min(20, num_channels - 1))) torrent_json = tribler_utils.tribler_data.torrents[index].get_json() torrent_json['type'] = 'torrent'
# todo: use widgets.dialog </s> if self._editor_component and self.is_focused():	OnTreeSelection self._editor.open(self.tree.get_selected_datafile_controller())
#todo: search recursively under stage.path instead of only within </s> self.tests.append((identifier, passed, output))	addTest
# todo: replace with "yield from" when dropping python 2. </s> "pageurl": self.url,	ConnectCast "rtmp": smil["base"], "playpath": video, "live": True })
# todo: empty. </s> with pytest.raises(assertionerror):	test_expand_context assert func(**ctx("foo", "bar", "baz")) == 6 ctx = Context({"one": 1, "two": 2, "three": 3}, History("Obj", "meth")) ctx("one", one="two")
# todo(brett.cannon) implement </s> def mock_implicit_hooks():	mock_implicit_hooks
# todo - diff </s> response = self.client.get('/r/1')	testReviewDetail0 self.assertEqual(response.status_code, 301)
# @todo: set metadata on file: org, location, disaster, date </s> ]	custom_postp s3.actions = [dict(label=s3_str(T("Open")), _class="action-btn", return output
# todo: call out to the ceph cluster to check the </s> fqdns = self.ceph_ctl.get_server_fqdns()	_get_version def _get_version(self): version = self.api.get("server/{0}".format(fqdns[0])).json()['ceph_version'] log.debug("version = %s" % version)
pass  # todo(nnorwitz): impl </s> public_symbols[name] = symbol	WarningHunter public_symbols = {} for name, symbol in primary_header.public_symbols.iteritems(): declared_only_symbols = dict.fromkeys(public_symbols, True) using_values = []
# todo: handle situations where share is password protected </s> return self._session['servername']	getServerName
raise  # todo: what if our seed node fails verification? </s> return response	EvilMiddleWare response = mock_client.post("http://localhost/node_metadata".format(mock_client), verify=False,
#todo: handle common wiki templates for type guessing </s> def _read_field(self, node):	WikiTable val = self._read_field(f).strip(' \n') if val: yield val if isinstance(node, Template): if node.name == 'refn':
# todo make more robust (timeout low? server returns error?) </s> except keyerror:	delete_payment try: with self.lock: return self.storage.put('lightning_payments', self.payments)
# todo: verify </s> self.assertequals(len(binds), 1)	test_remove_repo_cleanup self.test_bind() manager = factory.consumer_bind_manager() manager = factory.repo_manager() manager.delete_repo(self.REPO_ID)
# todo: test without file </s> return df.a.nunique()	test_nunique def test_impl(n): df = pd.DataFrame({'A': np.arange(n)}) hpat_func = hpat.jit(test_impl) n = 1001
# todo: error handling </s> book.data = data	get_or_create_book if not response.ok: response.raise_for_status() if user and user.is_authenticated: book.added_by = user
# todo: look this up in one query </s> return result.rowcount == 1	delete_playlist_by_mbid WHERE playlist.mbid = :playlist_mbid with ts.engine.connect() as connection:
# todo also check for motion codec parameter support </s> return _motion_detected.get(camera_id, false)	is_motion_detected
# todo(phawkins): we currently set dtype=false because we aren't as </s> for axis in set(range(-len(shape), len(shape))) | set([none])))	testCountNonzero jtu.format_shape_dtype_string(shape, dtype), axis), "shape": shape, "dtype": dtype, "axis": axis} def testCountNonzero(self, shape, dtype, axis): rng = jtu.rand_some_zero()
# todo: order of attributes is not assured; allow for any order. </s> s = converter.parse(testprimitive.directions31a)	testBracketA def testBracketA(self): from music21 import converter raw = fromMusic21Object(s) self.assertEqual(raw.count('<bracket'), 4)
# todo(john sirois): this hacks around a direct but undeclared dependency </s> def say_hello():	do_test_thrift SourceRoot.register('src/python', PythonBinary) self.create_file(relpath='src/python/test/main.py', contents=dedent(""" print(' '.join(VALID_IDENTIFIERS)) binary = self.make_target(spec='src/python/test',
# todo: what about '_type'? </s> return self._addoccurrences(fossil, obj, self._fromdt, self._todt)	_postprocess
#todo: check login_required? </s> api_type = "file"	_summary_hda_dict 'type'  : < name of the dataset >, 'url'   : < api url to retrieve this datasets full data >, encoded_id = trans.security.encode_id( hda.id ) return {
# todo discont: use offsets instead (note need for int conversion) </s> key_offset= dt.lower().find(keymap[t].lower())	__generate_input_and_label s  = indent+'    <input id="%s%s" type="radio" name="%stype" value="%s" %s/>' % (prefix, t, prefix, t, dstr) s += '<label for="%s%s">' % (prefix, t) else: key_offset = -1
# todo(juice): maybe it would be ok to extend the test to validate </s> def _verify_rpc_cast(self, exp_msg):	_verify_rpc_cast
# todo: remove summary when bug 862603 lands. </s> region=region, gaia=gaia, mobile=mobile, tablet=tablet,	_get_query def _get_query(region, gaia, mobile, tablet, filters=None, new_idx=False): filter_overrides=filters, new_idx=new_idx).facet('category')
# todo: assert metrics. </s> int i = 4;	test_get_touched_functions int i = 3; } }""", unit=False,
# todo: this currently aligns based on phrases, not words </s> bbox_vert_aligned_right(bbox_from_span(c[i]), bbox_from_span(c[0]))	is_vert_aligned_right return (all([c[i].is_visual() and
# todo: why do we have the --branch and --single-branch tags here, this causes problems </s> dep = deployment(cfg)	enable_cors cfg = DeploymentConfig(env.cfg_label, env.cfg_path)
fname = fnames[0]  # todo handle multiple notebooks </s> parser = argparse.argumentparser()	diff@31 parser.add_argument('notebook', nargs='*') args = parser.parse_args()
# todo: remove hardcoded http </s> sent = send_campaign_email_subscriber(campaign.email, subscriber, site, connection)	send_campaign campaign.email.enable_click_tracking() with get_connection() as connection: if sent: subscriber.create_activity(ActivityTypes.SENT, email=campaign.email)
# todo: obtain path lock or make operation atomic in sqlite </s> self.api.upload(file_to_upload)	_clean_path@12 return self.state_store.set_cleaning(path) if self.state_store.is_cleaning: self.state_store.remove(path)
# todo: set content_length </s> if not hasattr(self, '_accepted_renderer'):	accepted_renderer @property self._perform_content_negotiation() return self._accepted_renderer
# @todo: build better caption rather than just using raw comments </s> else:	inv_track_item_total_weight return current.messages["NONE"] if quantity is not None and weight is not None: return current.messages["NONE"]
# todo: use different flag than .reentrant </s> colorsorter._init_state()	_suspend state._cur_shapelist = ColorSorter._cur_shapelist state.sphereLevel = ColorSorter.sphereLevel return
# todo: write the wavelet transform </s> sj = s0 * 2 ** (dj * np.arange(0, j + 1))	scales J = int((1 / dj) * np.log2(self.N * self.dt / s0))
# todo: automate this (by lookup from nn). </s> action_space=env.action_space,	TestIMPALAAgentFunctionality architecture="large", environment_spec=environment_spec, internal_states_space=IMPALAAgent.standard_internal_states_space, execution_spec=dict(
# todo render mock data before response, support more functions </s> return "null"	none_parser
# :todo: implement test. </s> def test_fail_bundles_wrong_type(self):	FindTransactionsRequestFilterTestCase self.skipTest('Not implemented yet.') def test_fail_unexpected_parameters(self): self.skipTest('Not implemented yet.') def test_fail_bundles_contents_invalid(self):
# todo: trigger via dummy audio? </s> self.core.playback.play(self.tl_tracks[0])	test_pause_selects_dummy1_backend self.core.playback.pause() self.playback1.pause.assert_called_once_with()
# todo: reformat or delete </s> camera.elevation = -50	sawyer_pusher_camera_upright camera.lookat[0] = 0 camera.lookat[1] = 0.85 camera.azimuth = 270 camera.trackbodyid = -1
# todo: fire_switch_state_change(self, channel_index, old_switch_state, switch_state, from_myself) </s> def get_clients(self):	get_clients
# todo: rewrite tests </s> self._url_to_body(self.component.deep_url),	test_component_url assert_equal(
# todo: the following was copy/pasted from the histogram viewer, maybe </s> self._last_viewer_state = {}	reset_cache def reset_cache(self):
#@todo: remove in 0.4.10 </s> def _log(self, level, plugintype, pluginname, messages):	Hook def __init__(self, core, manager): super(Hook, self).__init__(core, manager) return super(self.__name__, self)._log(level, plugintype, pluginname.replace("Hook", ""), messages)
# todo: will removed when django 1.7 will be deprecated </s> return super(referencesfield, self).formfield(**kwargs)	ReferencesField setattr(cls, self.name, HStoreReferenceDescriptor(self)) def formfield(self, **kwargs): def get_prep_lookup(self, lookup, value): if isinstance(value, dict):
# todo: bytes vs str </s> def _route(f):	route self.url_map.append((url, f, kwargs)) return f
# todo what happens with the background thread here? </s> return self._recv_thread is not none and \	_on_read_thread threading.get_ident() == self._recv_thread.ident
# todo: remove when transition to python3 complete </s> def __neg__(self):	__neg__
body = {}  # todo: not clear what this is supposed to be </s> zone = existing.zone.name[:-1]	_apply_Delete def _apply_Delete(self, ns1_zone, change): domain = existing.fqdn[:-1] _type = existing._type
# todo: enable once there's a user.avatar property returning a wrapper with avatar-style methods </s> 'path': truncate_path(categ.getcategorypathtitles(), 30, false)	get_related_categories 'categ': categ, 'favorite': categ in favorites, } return OrderedDict(sorted(res.items(), key=operator.itemgetter(0)))
# todo this might not cover all cases </s> permissions['read'] = true	_set_s3_permissions @staticmethod def _set_s3_permissions(permissions, name): if name == 'WRITE' or name == 'FULL_CONTROL': permissions['write'] = True
# todo: for backward compatibility only, remove if not used anymore </s> args = {}	get_vm vm = self.module.params.get('vm') if not vm: args['projectid'] = self.get_project(key='id') vms = self.cs.listVirtualMachines(**args)
# todo do a check for the flip condition </s> return false	reference_invariant for vertex in range(len(self.vertices)): if any(vertex not in tri for simplex in self.simplices: if any(simplex not in self.vertex_to_simplices[pt]
# todo: wait for an event instead of spinning. </s> class wavehdr(structure):	WAVEHDR
# todo fix. </s> self.asserttrue(cmp_result, self.__print_contexts(ctx_init, x86_ctx_out, reil_ctx_out))	test_cmova cmp_result = self.__compare_contexts(ctx_init, x86_ctx_out, reil_ctx_out) if not cmp_result:
# todo: add and store preprocessing errors. </s> file_object (dfvfs.fileio): file-like object that contains the artifact	LinuxDistributionPlugin def _ParseFileData(self, knowledge_base, file_object): Args: value data. Raises:
# todo: check if integerctype, not just ctype.arith (floats, etc.) </s> elif type1_promo.signed == type2_promo.signed:	_promo_type type2_promo = ctypes.integer if type2.size < 4 else type2 if type1_promo == type2_promo: return max([type1_promo, type2_promo], key=lambda t: t.size)
# todo: add standard commands </s> if self.interface.content:	on_resize self.interface.content.refresh()
oldsize = self.size # todo: remove </s> return struct.unpack('>i', stream.read(4))[0]	read_uint
# todo(seemuch): remove this contrib import </s> return tf.reshape(x[0, :], (batch_size,))	_strip_first_and_last_dimension
# todo: allow_stderr_warning is used for the --build deprecation, </s> script.environ["pip_test_fail_build_wheel"] = "1"	test_pep517_no_legacy_cleanup @pytest.mark.network def test_pep517_no_legacy_cleanup(script, data, with_wheel): res = script.pip( 'install', '-f', data.find_links, to_install,
# todo: this property is only used by the mvpformindicatorpillow </s> nature=responsenature.submit_error,	get_failure_response def get_failure_response(doc): return OpenRosaResponse( status=201,
# todo: stderr=_stderr_file, </s> os.environ["authfile"] = os.environ["xauthority"] = filename	_setup_xauth self._old_xauth = {} self._old_xauth["AUTHFILE"] = os.getenv("AUTHFILE") cookie = xauth.generate_mcookie() xauth.call("add", self.new_display_var, ".", cookie)
# todo(dave) implement </s> resp_quotas[resource] = getattr(project_quotas_model, resource)	_extract_project_quotas :return: Python dict containing quota information resp_quotas = {} return resp_quotas
# todo: replace with copy and copy_file </s> origin = repo.create_remote('origin', remote_url)	create_remote def create_remote(repo, remote_url): origin.fetch() return origin
# todo no need for .view(1, -1) </s> seq_ends = mask.long().sum(dim=0) - 1	_compute_score for i in range(1, seq_length): score += self.transitions[tags[i - 1], tags[i]] * mask[i] last_tags = tags[seq_ends, torch.arange(batch_size)] score += self.end_transitions[last_tags]
else fn)  # todo: change to jscommand </s> webelement = element()	fn def fn(element: Element):
# todo: consider adding some better error handling for bad/failed requests. </s> if req.status_code == 404:	_get_user_hubspot_id ), params={'hapikey': api_key}, return None req.raise_for_status()
# todo: remember that we are now out of sync and try again </s> _update_device_from_client_ips(device, ips)	get_device ips = yield self.store.get_last_client_ip_by_device( devices=((user_id, device_id),) defer.returnValue(device)
# todo test cachetag </s> cut1 = cut(table, 'foo')	test_cut [u'B', u'3', u'7.8', True], ['D', 'xyz', 9.0], expectation = [['foo'], ['A'],
# todo: remove this ``expectedfailure`` </s> def test_unsuccessful_read_transaction(self):	test_unsuccessful_read_transaction with self.assertNumQueries(3): try:
# todo(stephenfin): fix these various bugs in a follow-up </s> return sa.string(length=39).with_variant(	InetSmall dialects.postgresql.INET(), 'postgresql',
# todo: hack: this is papering over a bug elsewhere. </s> return self.reply_to == is_dead	is_dead @property
# todo check if lus can be more than one token </s> with codecs.open(results_file, 'rb', 'utf-8') as f:	read_full_results def read_full_results(results_file): h = HTMLParser.HTMLParser() results = csv.DictReader(f) fe_amount = 0
# todo: somehow caused by circular import under python3 refactor </s> self.stopserver()	killServer
# todo: remove one day </s> return bucket, blob	_parse_gcs_url else: bucket = parsed_url.netloc
# todo uncomment the actual test below after we have implemented the l1 attack </s> adv_acc = adv_label.eq(ori_label).sum().to(torch.float)\	test_attack_strength sanity_checks=False) _, ori_label = self.model(self.normalized_x).max(1) / self.normalized_x.size(0) self.assertLess(adv_acc, .1)
raise notimplementederror # todo </s> cur = self._connection.cursor()	get_storage_properties return [prop[0] for prop in cur.description]
# todo: handle marker? </s> if alias:	describe_alias try: alias = _find_alias(functionname, name, keys = ('Name', 'FunctionVersion', 'Description') return {'alias': dict([(k, alias.get(k)) for k in keys])}
# todo: remove me when auto_now_add is enabled (post-migration) </s> user.add_unconfirmed_email('')	test_add_blank_unconfirmed_email with pytest.raises(ValidationError) as exc_info:
# todo: scale and translation could be merged into a single network </s> return self.z_placeholder	add_backward_preprocessing_ops
# todo add stuff from will's pull req </s> if coin_name not in removed:	blockchain_assert_coin_consumed Checks coin consumed conditions Returns None if conditions are met, if not returns the reason why it failed return Err.ASSERT_COIN_CONSUMED_FAILED
pass # todo </s> entries = timepiece.entry.objects.date_trunc(trunc)	check_truncs def check_truncs(self, trunc, billable, non_billable): self.make_entries(user=self.user) for entry in entries: if entry['billable']:
# # fixme: # todo: remove me </s> redis_crawler.sadd('blacklist_{}'.format(service_type), line)	load_blacklist redis_crawler.delete('blacklist_{}'.format(service_type)) lines = f.read().splitlines() except Exception: pass
# todo: once per second, reinspect any projects marked dirty. </s> self.refresh_module_info(filename)	refresh_all_module_info files_in_dir = list_files_in_dir_recursively(base_dir) haskell_source_files = [x for x in files_in_dir if x.endswith('.hs')] end_time = time.clock() log('total inspection time: {0} seconds'.format(end_time - begin_time))
# todo: finish this. </s> if entry.editable:	_GDriveFS normalized_entry = entry_clause[0] entry = entry_clause[0] effective_permission |= 0222 stat_result = { "st_mtime": entry.modified_date_epoch }
# todo: test for first revision on last page. </s> res = self.app.get(offset)	test_purge offset = url_for(controller='revision', action='purge', id=None)
assert study_id == 0  # todo(akiba) </s> assert study_id == 0	set_study_param_distribution self.param_distribution[param_name] = distribution
# todo: let users specify a base mac address </s> return ["-monitor", "telnet:{}:{},server,nowait".format(self._monitor_host, self._monitor)]	_monitor_options def _monitor_options(self): else: return []
# todo: make an ascii-art bar </s> return "%.2fs" % s	render_time return "" s = float(data) if s >= 0.01: return "%dms" % (1000*s)
return self.name # todo: probably we should raise an exception here </s> parent_elem = _iuia.controlviewwalker.getparentelement(self._element)	parent @property def parent(self): if parent_elem: return UIAElementInfo(parent_elem)
# todo: proper content negotiation </s> if isinstance(resp, response):  # there may be a better way to test	dispatch_request meth = decorator(meth) self.validate_payload(meth) return resp representations = self.representations or {}
# todo: make it really async. </s> def parse_resource(self, encryptions):	TransparentDataEncryptions api = SqlManagementClient(credentials.credentials, credentials.subscription_id) return api.transparent_data_encryptions.get( return 'transparent_data_encryption_enabled', encryptions.status == "Enabled"
# todo: break this tuplet stuff into a helper function shared for <note>, <rest>, and <chord> </s> post = wrapgetbyidlocal(slurnum)	addSlurs newSlur.addSpannedElements(obj) post = True return post
# todo: validate triplet of states </s> over the entire subsystem's state space."""	expand_cause_repertoire return self.expand_repertoire(DIRECTIONS[PAST], purview, repertoire, new_purview)
# todo: check if format matches </s> comment = destination['description']	_create_database def _create_database(request, source, destination, start_time): use_default_location = destination['useDefaultLocation'] external_path = destination['nonDefaultLocation']
# todo: make this cleaner/faster </s> if platform == "linux" or platform == "linux2":	_what_os _platform = 'linux' elif platform == "darwin":
# todo / fixme : to be actually implemented later .... </s> res = user_permission_list(full=true)['permissions']	test_permission_remove_group_already_not_allowed def test_permission_remove_group_already_not_allowed(): assert res['blog.main']['allowed'] == ["alice"] assert res['blog.main']['corresponding_users'] == ["alice"]
# todo: check against cygwin before removing </s> if success_check == 0:	get_exit_value self.sendline(''' if [ $? = 0 ]; then echo 'SHUTIT''_RESULT:0'; else echo 'SHUTIT''_RESULT:1'; fi''') shutit.log('Checking exit value.',level=logging.DEBUG) shutit.log('Returning true.',level=logging.DEBUG) return True
#todo: does not keep case </s> self.assertequal(ret, '3')	test_num self.assertTrue(p.persistent_count is None) ret = p.num(3) p.num() ret = p.num("3")
# todo: cleanup </s> def createrepo(self):	createRepo repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "name", "testrepo0", self.token)
#todo: once package/file api are merge to contentapi, replace this check with global content_search </s> class content(command):	Content
# todo: self._line_structures is a work-around and this needs </s> u'unable to extract timestamp from iis log line with structure: '	_ParseLogLine time = structure.get('time', None) if not (date and time): u'{0:s}.').format(structure)) return
self.assertequal(end, 1) ## todo real = 0 </s> [unit]	test_2027_show_unit_for_oneshot_service root = self.root(testdir) systemctl = _cov + _systemctl_py + " --root=" + root Description=Testing A [Service]
# todo: refactor </s> else:	_get_ind_sub_slice f = loc['f'] args = [offset_var] def f(old_slice, offset):  # pragma: no cover return slice(old_slice.start - offset, old_slice.stop - offset)
# todo make fetch_result _not_ a pd.dataframe </s> error='table number must be at least 1'	test_table_index_under url = 'http:INVALID:URL'  # we should never even validate the URL fetch_result = fetch(url=url, tablenum=0)
# todo extend to inputs with shape (n_samples, 1) </s> allow_infinity=false,	test_filter_values_covered_by_single_interval@36 @given( filter_values=arrays(dtype=np.float, max_value=1e3), shape=integers(min_value=1, max_value=100)
#todo: check the data! </s> for i in p:	test_feed pipe_def = self._get_pipe_def("testpipe1.json") p = pipe2py.compile.parse_and_build_pipe(pipe_def, verbose=True) count += 1 self.assertTrue("the" in i.get('description'))
# todo: support ddof </s> def df_len_overload(df):	df_len_overload if len(df.columns) == 0:  # empty df return lambda df: 0
prng = randomstate() # todo: seed it </s> return (-0.5 * (spmv(a[2].data, a[2].indices, a[2].indptr, psi)	d1_current def d1_current(A, psi): Todo: cythonize, requires poisson increments - n1 ** 2 * psi))
# todo(b/178173737): use tf.math.segment_sum once a fast version is </s> weights = tf.minimum(one, weights)	_initialize_instrument_weights weights = tf.maximum( tf.math.divide_no_nan(one, float_times_last), return tf.unstack(weights, name='instrument_weights')
"""@todo: docs. contribution is welcome.""" </s> self.logger.close()	__del__@246
#todo: unit tests </s> dashboard_folder = find_dashboard(user)	dashboard @must_be_logged_in def dashboard(auth): dashboard_id = dashboard_folder._id return {'addons_enabled': user.get_addon_names(),
# todo: will need some code for the tabs active terms to work </s> self.windows.remove(window)	deregister_window dbg('Terminator::deregister_window: de-registering %s:%s' % (id(window), type(window))) else: err('%s is not in registered window list' % window)
# todo: use other libraries. </s> def get_size_of_pointer():	get_size_of_pointer
# xxx todo: read incrementally to reduce memory usage. </s> return len(zlib.compress(data, opts.compress_level))	get_compressed_length
#todo fixme: we need to check that we aren't adding a duplicate </s> item.addclaim(claim)	main@52 continue for claim in real_claims:
# todo(crcrpar): make this works </s> warnings.simplefilter('always', userwarning)	experimental_version def experimental(func): @functools.wraps(func) warnings.warn( "{} is experimental (from version: {}). "
# todo: fails because of missing svg support </s> def test_image_repeat_block():	test_image_repeat_block assert_pixels('image_page_repeat_block', 8, 2 * 8, table, ''' <style>
# todo: the assert below fails. </s> pytest.importorskip("torch")	test_model_initiation_fullpath def test_model_initiation_fullpath(): backend = {'model': 'pytorch_model.PyTorchModel', 'explainer': 'dice_pytorch.DicePyTorch'}
# todo: remove in version 3.10 </s> request.resolver_match = resolvermatch('get', (), {})	test_api_root_view_discard_default_django_model_permission ``_ignore_model_permissions`` attribute support. request = factory.get('/', format='json', response = api_root_view(request) self.assertEqual(response.status_code, status.HTTP_200_OK)
# todo(b/142684737): the multi-processing api might change. </s> statistics=statistics_gen.outputs['statistics'],	_create_pipeline@136 example_gen = CsvExampleGen(input=examples) statistics_gen = StatisticsGen(examples=example_gen.outputs['examples']) infer_feature_shape=False) validate_stats = ExampleValidator(
# todo verify </s> class error(exception):	Error
# todo: autosummon option to a specific channel </s> def on_stop(self, **_):	on_stop
# todo: make sure secret values are masked </s> access = config	PackConfigsController class PackConfigsController(ResourceController): supported_filters = {} def __init__(self):
# todo: use triple factory </s> trans_h = transh(triples_factory=self.factory)	test_trans_h self.assertIsNotNone(trans_h)
# todo: make test method </s> try:	test_gps def test_gps(): return event_loop() finally:
# todo: cleaning of facts should eventually become part of taskresults instead of vars </s> if any(((connection.supports_persistence and c.use_persistent_connections), connection.force_persistence)):	_get_connection if not connection: raise AnsibleError("the connection plugin '%s' was not found" % conn_type) self._play_context.timeout = connection.get_option('persistent_command_timeout') display.vvvv('attempting to start connection', host=self._play_context.remote_addr)
# todo: remove temporary workaround once https://github.com/python-babel/babel/issues/415 has been resolved. </s> self._show_always = false	UpdateCheckManager super().__init__(parent=parent) self._parent = parent self._update_level = 0 def check_update(self, show_always=False, update_level=0, callback=None):
if not config.testnet:  # todo </s> self.timestamp = tx['block_time']	Transaction self.gasprice = gasprice self.startgas = startgas def hex_hash(self): return '<None>'
# todo: implement it </s> hello, %(nickname)s (<a href=\"%(logout_url)s\">sign out</a>)	GoogleLoginHandler a = """
# todo is a division by moving avg factor needed for variance? </s> if consumer is not none:	batchnorm_to_affine@30 bn_input = producer.input[0] nodes_to_remove += [producer] if consumer.op_type == "Squeeze": bn_output = consumer.output[0]
#todo: implement xml support </s> cur.execute(	Tenants '../db/keystone.db')) con = sqlite3.connect(dbpath) "INSERT INTO tenants VALUES ('%s', '%s', %d)" % (tenant_id, tenant_desc, tenant_enabled))
#todo(qos): support all the optional parameters </s> context, network['id'])	_extend_network_policy_data policy = policy_object.QosPolicy.get_network_policy(
# todo: should use ".handle_quick_operation" action in the future </s> _bus.subscribe(post_type, func)	deco for e in [arg] + list(events): _bus.subscribe(f'{post_type}.{e}', func) return func
#todo: check if/where this is used; if not used externally - remove </s> self.save_surface_definitions_to_file()	remove_surface self.gui.remove_surface(surface) self.surfaces.remove(surface)
# todo: accept only exported keys </s> pass	state_keys
# todo: check the data! </s> u'commsallowance': u'9767', u'mileage': u'3358', u'mpmisc': u'20',	test_csv self.assertTrue(i == {u'FamilyNumOfJourneys': u'0', u'Member': u'Lancaster', u'MPOtherEuropean': u'0', u'FamilyTotal': u'0', u'OfficeRunningCosts': u'19848', u'MPOtherRail': u'233', u'title': u'Mr Mark Lancaster', u'description': u'Total allowances claimed, inc travel: 151619<br>Total basic allowances claimed, ex travel: 146282<br>Total Travel claimed: 5337<br>MP Mileage: 3358<br>MP Rail Travel: 1473<br>MP Air Travel: 0<br>Cost of staying away from main home: 22541<br>London Supplement: 0<br>Office Running Costs: 19848<br>Staffing Costs: 88283',
# todo: raise an invalidparameters instead and stop using cli_ui in gitlabform.gitlab </s> group_id,	delete_ldap_group_link data["id"] = group_id self._make_requests_to_api( method="DELETE", data=data,
# todo: move to "worklockeconomics" class #1126 </s> start_bid_date = now + (60 * 60)  # 1 hour	deploy_worklock@12 @pytest.fixture(scope="module", autouse=True) def deploy_worklock(testerchain, agency, test_registry, token_economics): end_bid_date = start_bid_date + (60 * 60) deposit_rate = 100
#todo: how to handle language change? clear and populate again? </s> def add_to_store(self, store, item):	add_to_store
# todo hack! include image digest, needed for the downstream notifications handler </s> return_object = str(err)	do_subscription_delete return_object = True httpcode = 200 return(return_object, httpcode)
# todo: fix with stubber / before send event </s> with mock.patch('botocore.endpoint.endpoint._send') as _send:	TestApiGateway 'exportType': 'swagger', 'accepts': 'application/yaml' _send.return_value = mock.Mock( status_code=200, headers={}, content=b'{}')
# todo: remove when bug 862603 lands. </s> price__gt=0))	from_search if (mobile or tablet) and not gaia: srch = srch.filter(app_type=amo.ADDON_WEBAPP_HOSTED) return srch
'i': ('i', [{'j': 'j'}]),  # todo: support true for cases when the value should simply be mapped into the field name? </s> >>> glom(target, {'a_d': path('a', 'd.e'), 'a_2': path('a', 2)})	Path Use this to wrap ints, datetimes, and other valid keys, as well as strings with dots that shouldn't be expanded. {'a_de': 'f', 'a_2': 3} def __init__(self, *path_parts):
# todo aggregation only supports # of docs matching </s> return [	order_by @property def order_by(self): (col.field, order) for sort_column_id, order in self._order_by
# todo: import refactor - figure out which group this needs :( </s> except:	getItem item = eos.db.getItem(id_, *args, **kwargs) else: logger.error("Could not get item: %s", identity) raise
# todo: fix this issue </s> )	TestClip samples = np.array( [[0.9, 0.5, -0.25, -0.125, 0.0], [0.95, 0.5, -0.25, -0.125, 0.0]], sample_rate = 16000 augmenter = Compose([Clip(a_min=-0.1, a_max=0.1, p=1.0)])
# @todo: move this to link table? </s> @param attr: controller attributes	S3CMS S3Summary @param method: the widget method @ToDo: Support comments if not current.deployment_settings.has_module("cms"):
# todo: check syntax, values? </s> if not self.response.parsed_hdrs.has_key('location'):	status303 def status303(self):        # See Other
# todo: if self.outputs["edge indices"].islinked: edgeindices = loft.calcedgeindices() </s> col = layout.column()	drawAdvanced col.prop(self, "splineDistributionType") col.prop(self, "surfaceDistributionType")
# # todo: delete when 0.4.0 is available </s> 0 = above viewport	_get_y_viewport Return the current y viewport. Note: > 1 below viewport (i.e. dead, falling down a hole) up to 5 indicates falling into a hole
# todo: header fields might vary across file types, thus prior sensing would be needed </s> if len(diffs) > 1:	diff_values if a and b != len(diffs)-1: if np.any(diffs[a] == diffs[b]): return {key: diffs}
# todo: default to 'next' when redoc 2.0.0 is released. </s> ...	register_converter app.url_map.converters['uuid'] = UUIDConverter api.spec.register_converter(UUIDConverter, 'string', 'UUID') api.register_blueprint(blp) Once the converter is registered, all paths using it will have their
# todo -- parallelize this </s> predictions = {}	SpeakerChangeDetection source='audio', device=self.device) protocol = get_protocol(protocol_name, progress=False, for current_file in getattr(protocol, subset)(): uri = get_unique_identifier(current_file)
# todo implement </s> else:	hook_WriteFile s = ql.uc.mem_read(lpBuffer, nNumberOfBytesToWrite) ql.stdout.write(s) try: f = ql.handle_manager.get(hFile).file
pass  # todo </s> pass  # todo	SpotifyPlaylistsProvider def __init__(self, backend): self._backend = backend def delete(self, uri): pass  # TODO
# todo: in the future we'll probably need to keep a request history </s> raise valueerror(	update_req_handler verify_key: Optional[VerifyKey] = None, ) -> None: "Can't process Request service without a given " "verification key" )
# todo : an "invalid campaign popup" </s> def __get_selected_map(self):	__get_selected_map
# todo: review why this is now unused </s> def watchedevents(self):	watchedEvents
# todo type_min/type_max </s> def test_impl(s):	test_series_map1 return S.map(lambda a: 2 * a) hpat_func = hpat.jit(test_impl)
# todo(jakevdp): remove when minimum jaxlib is has extension version 4 </s> self.add_option(name, default, int, args, kwargs)	DEFINE_integer
# todo(b/141131288): enable test once complex sort is supported on tpu. </s> for return_index in [false, true]	testUnique "return_counts": return_counts} for dtype in default_dtypes for return_inverse in [False, True] for return_counts in [False, True]))
# todo: add modified date </s> self._skip_hash_check = false	skip_hash_check self._skip_hash_check = True yield
# todo: extract real data length: </s> def __get_v(self):	__get_v
except exception as err:  # todo: what exception?! (socket error; authentication error; ...?) </s> :raises jsonapierror if http status is not in the 200 (ok) range	json_or_error @staticmethod def json_or_error(response): if not 200 <= response.status_code < 300: raise JsonApiError('API request failed with HTTP status %s: %s' %
#todo: unit tests </s> return {'addons_enabled': user.get_addon_names(),	dashboard dashboard_id = dashboard_folder._id
# todo: obviously incrementing the rows individually is bad. how </s> return token.doc.tensor[token.i]	get_token_vector_via_tensor
# todo: add doc string </s> except valueerror:	edge_index Numerical edge type index try: print("Warning: Edge key '{}' not found.".format(edge_type)) index = None
# todo: add test and check this more throughroughly. </s> ret = layer(inputs)	easy_apply def easy_apply(layer, inputs): Apply a layer to input[s]. except (TypeError, AttributeError): if len(inputs) != 1:
# todo: make this test real </s> view.template_path = 'foo.txt'	test_get_relative_template_location__template_path__full_path def test_get_relative_template_location__template_path__full_path(self): locator = self._make_locator() self.assertEquals(locator.get_relative_template_location(view), ('', 'foo.txt'))
# todo: wer計算するときに消していい？ </s> normalize=true)	do_eval_wer@102 str_pred = re.sub(r'[\'<>]+', '', str_pred) wer_mean += compute_wer(ref=str_true.split('_'), if progressbar: pbar.update(1)
# todo: combine with the 'canonicalization' that is part of the gemm optimizer. </s> _approx_eq.debug = 1	test10 def test10(self):
# todo return empty list if not loaded </s> spotify.error.maybe_raise(self.error)	did_you_mean def did_you_mean(self): suggestion exists. did_you_mean = utils.to_unicode( lib.sp_search_did_you_mean(self._sp_search))
# todo debug </s> ruleweekdaynew.weekday = int(weekdayitem.attrib[	parseRuleRecursively ruleWeekdayNew.time != "utc"): raise ValueError("No valid value for 'time' attribute " "weekday"]) if (ruleWeekdayNew.weekday < 0 or
# todo: update with misconfigurationexception when auto mode is removed in v1.3 </s> def monitor_op(self):	monitor_op @property
# todo: rewrite this to use dict in all code and not different independent vars </s> else:	printk with open("/proc/sys/kernel/printk", "w") as fpk: if enable: fpk.write("0")
# todo: collations are not supported, but the default ones needed </s> _prop_match(item, child) if child.tag == _tag("c", "prop-filter")	_comp_match return False filter_.remove(filter_[0]) else _comp_match(item, child, scope="component") for child in filter_)
raise exception  # todo (key not found in columns) </s> add a column with the given ``name`` and ``key``, which has the given	add_table ``columns``. Each element in ``columns`` may be either a string giving the name of the column, or a tuple containing the name of the column
# todo: use fsevents' sincewhen parameter instead of the current </s> del self.monitored_paths[path]	__remove_dir FSEventStreamStop(streamRef) FSEventStreamInvalidate(streamRef)
# todo: if not default behavior: have to specify in decorator (see design_problems.txt). </s> return (logits,) + tuple(self.call(self._graph_fn_get_parameters_log_probs, logits))	get_logits_parameters_log_probs def get_logits_parameters_log_probs(self, nn_output): Override get_logits_parameters_log_probs API-method to not use the state-value, which must be sliced.
# todo(mattrobenolt): remove servicedelegator check </s> and convert options into django settings that are	bootstrap_options def bootstrap_options(settings, config=None): required to even initialize the rest of the app. from sentry.options import load_defaults
# todo: remove the false when enabling the crawler. </s> elif opt in ("-v", "--verify"):	parse_options input_uri = arg elif opt in ("-h", "--help"): input_verify_exploit = True elif opt in ("-c", "--crawl"):
#todo: we may want to deal with error nicely </s> list_val.append((row['prefix'], row['prefix__destination'], row['retail_rate']))	export_rate final_result = request.session['final_rate_list'] list_val = [] data = tablib.Dataset(*list_val, headers=headers) if format_type == Export_choice.XLS:
raise notimplementederror  # todo </s> raise notimplementederror  # todo	Perturbation def generator(*args, **kwargs): raise NotImplementedError  # TODO generator = None return self.compute_generator(generator, verbose=verbose)
# todo: skips header parsing </s> model.read_abaqus_inp(abaqus_inp_filename)	read_abaqus def read_abaqus(abaqus_inp_filename, log=None, debug=False): return model
# todo: arrange </s> self.remote.modify_repo(repo, "name", "testrepo0", self.token)	createRepo @pytest.fixture def createRepo(self): self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token) self.remote.modify_repo(repo, "mirror_locally", "0", self.token)
#todo basket column. </s> file.seek(0)	count_lines i = -3 for _ in file:
# todo: fix in detectors. </s> center=(int(ellipse["center"][0]), int(ellipse["center"][1])),	draw_ellipse ): try: axes=(int(ellipse["axes"][0] / 2), int(ellipse["axes"][1] / 2)), angle=int(ellipse["angle"]),
# todo: this can be formulated more efficiently </s> batch = module.input0.size(0)	bias_diag_ggn num_classes = sqrt_ggn_out.size(2) out_pixels = module.output_shape[2] * module.output_shape[3]
# todo(nzw0301): remove the upper version constraint when the callback supports </s> "pyyaml",  # only used in `optuna/cli.py`.	get_install_requires "scipy!=1.4.0", "sqlalchemy>=1.1.0", ] return requirements
# todo generator </s> examples	Drop By default, the availability of at least one remote copy is verified, before file content is dropped. As these checks could lead to slow -------- Drop all file content in a dataset::
# todo: systemhistory_user_id </s> def logger(taskname, request_user, log_text):	Taskname taskname_name = models.CharField(max_length=50, unique=True) def __str__(self): stdlogger.info( request_user +
# todo check more types here </s> def rot_13(self) -> "chepy":	rot_13 self.output = codecs.encode(self.output, "rot_13") return self
# todo: varref should store a token instead of a string! </s> if tag == command_e.whileuntil:	SpanForCommand if tag == command_e.Subshell: node = cast(command__Subshell, UP_node) node = cast(command__WhileUntil, UP_node) return node.spids[0]  # while spid
# todo: askr, undocumented! </s> min( green, 63 )	LedCtrlChar blue   =  0 min( red, 63 ) max( green,  0 ) min( blue, 63 )
# todo: this is a hack to make a rule know </s> feature_str = feature_str.strip()	_create_feature_key feature_str += feature + " "
# todo: remove one day </s> deletes the cluster, including the kubernetes endpoint and all	delete_cluster retry: Retry = DEFAULT, timeout: float = DEFAULT worker nodes. Firewalls and routes that were configured during cluster creation are also deleted. Other Google Compute Engine
# todo: fix within gitpython or build a fully functional </s> count_cmd = ['git', 'count-objects', '-v']	count_objects @property count_str, err = self._git_custom_command('', count_cmd) count = {key: int(value)
# todo: fails because of missing svg support </s> </style>	test_image_repeat_block th { height: 4px } td { height: 2px } <table> <thead>
# todo: take into account /ipfs/(hash), first check if this is correct fmt </s> def get_cached_index_file_contents(path: str) -> str:	get_cached_index_file_contents return get_index_file_contents(path)
#todo has not (clc) </s> .and_then('inx')	test_load_sprite .and_then('LoadSpritesIntoPPU:') .and_then('LDA sprite, x') .and_then('CPX #4') #TODO it should be 4 .and_then('BNE LoadSpritesIntoPPU')
# todo move this to augmenters.size </s> img_0to1 = img[..., 0]  # depth map was saved as 3-channel rgb	quokka_heatmap size = img.shape[0:2] shape_resized = _compute_resized_shape(img.shape, size) img_0to1 = img_0to1.astype(np.float32) / 255.0 img_0to1 = 1 - img_0to1  # depth map was saved as 0 being furthest away
#todo publish don't write </s> factory = esmetransceiverfactory(self.config, self.global_options)	startWorker self.r_prefix = "%s@%s:%s" % (self.config['system_id'], self.config['host'], self.config['port']) log.msg("r_prefix = %s" % self.r_prefix) factory.loadDefaults(self.config) self.sequence_key = "%s_%s#last_sequence_number" % (self.r_prefix, self.config['smpp_offset'])
# todo: remove when new appium is out </s> appium webelement	create_web_element Overrides method in Selenium WebDriver in order to always give them
# todo: note that this won't work for nans </s> )	_flatten_fdir -1 + shape[1], -1 + 0, gotomap = dict(zip(dirmap, go_to)) for k, v in gotomap.items():
#self.resetacount() #@todo implement </s> i += 1	getInfo elif tmp[4] == "1": status = 2 else: status = 3 yield result
# todo: i should make sure to escape single quotes here </s> def send_null(self, key, safe=false):	send_null
# todo: finish this... </s> if elem.get( 'state' ) != state:	_assert_dataset_state errmsg = "Expecting dataset state '%s', but state is '%s'. Dataset blurb: %s\n\n" % ( state, elem.get('state'), elem.text.strip() ) errmsg += "---------------------- >> begin tool stderr << -----------------------\n"
# todo: remove need for --no-strict-optional </s> rv = os.path.splitext(file)[0].replace(os.sep, '.')	file_to_module if rv.endswith('.__init__'): rv = rv[:-len('.__init__')]
# todo: no testpath exercises this code... </s> def _mstime(self):	_msTime
# todo: the logic here for ion concentration setting is in two </s> returns (float): energy per atom	energy_per_atom @property def energy_per_atom(self): return self.energy / self.composition.num_atoms
# todo candidate for move to system/osi as not btrfs related </s> if (re.search(share_name + '$', line) is not none):	share_id out, err, rc = subvol_list_helper(root_pool_mnt) subvol_id = None subvol_id = line.split()[1] break
# todo: ... </s> self.assertidentical(s._protocol, none)	test_clientConnectionLost retry = self.patch_reconnector('retry') s = ReconnectingClientService(object(), object()) retry.assertCalledOnce()
# todo(danms) once libvirt has support for lxc hotplug, </s> self._create_domain_and_network(xml, instance, network_info)	resume_state_on_host_boot def resume_state_on_host_boot(self, context, instance, network_info): virt_dom = self._conn.lookupByName(instance['name'])
help='') # todo </s> pass	sop_cmd
# todo(b/197746608): finds a safer way of reconstructing the metric, </s> if isinstance(batch_input, collections.abc.mapping):	_KerasModel def predict_on_batch(self, x, training=True): return self._keras_model(x, training=training) inputs = batch_input.get('x') else:
# todo: implement </s> patches = []	terrain_defense_upgrade :rtype: list
# todo(elliot): what info do we need for this recipe type? </s> for i in range(0, len(recipes)):	create_existing_recipe_list cmd = "autopkg search -p %s" % app_name exitcode, out, err = get_exitcode_stdout_stderr(cmd) search_term = "%s.%s.recipe" % (app_name, recipes[i]["name"]) for line in out.split("\n"):
# todo: figure out how to best show this kind of warning to the </s> isinstance(obj, int) or	is_simple isinstance(obj, bytes) or isinstance(obj, string_types) or isinstance(obj, bool) or obj is None)
# todo: another solution should be used here. this is a hack for compatibility reasons. to resolve the gadget address calculation of segments of elf files have a different base address if calculated segment.virtualaddress - segment.offset </s> offset_tmp += match.start()	__gatherGadgetsByEnding to_return = [] match = re.search(ending[0], tmp_code) index = match.start() if offset_tmp % arch.align == 0:
# todo: take care of theano to keras port: </s> x -= offset	vgg16_preprocess else: shape = [1, 1, 1, 3] return X
# todo: this should be solved via plugins </s> alter_foreignkey_to_int('recipes_oldrecipearticleredirect', 'new_id')	Migration super(Migration, self).alter_self_foreignkeys(orm) alter_foreignkey_to_int('articles_articlecontents', 'article') def move_self_foreignkeys(self, orm): super(Migration, self).move_self_foreignkeys(orm)
# todo dm: once we do distributed launching, this needs to be done per node not per cluster </s> msg = "could not start node '%s' within timeout period of %s seconds." % (	_start_process return process else: node_name, InProcessLauncher.PROCESS_WAIT_TIMEOUT_SECONDS) logger.error(msg)
# todo: return errors in a universal way </s> return code_store.full_code()	compile_code ast_root = parser.parse(token_list) code_store = CodeStore()
# todo: implement this here </s> def load(self):	Classifier raise NotImplementedError def save(self): raise NotImplementedError
# todo: test multi-line lambdas </s> def _opcode(name):	_opcode
# todo: find a way to invalidate the system df cache. </s> columns: typing.list[str] = ['default']) -> none:	SQCommand def __init__(self, engine: str = '', hostname: typing.List[str] = [], start_time: str = '', end_time: str = '', self.ctxt = context.get_context() self._cfg = self.ctxt.cfg
# todo(yanase): which dtype should we use? float or float32? </s> def get_study_uuid_from_id(self, study_id):	get_study_uuid_from_id @abc.abstractmethod
# todo: copy doesn't really work as expected, i.e., the reference </s> loggers = set([])	Gogo messages < low_level               -> ignore low_level <= messages < high_level -> low_hdlr def __init__(self, name='root', high_level=None, low_level=None, **kwargs): Args:
if posix and not sunos:  # todo: sunos </s> def test_uids(self):	test_uids @skip_if_linux()
# todo: write units tests </s> energy limit of the si will be incluided	add_elements The symbol of the elements. include_pre_edges : bool Examples --------
# todo(@awav): check it </s> s.profiling.output_file_name = name	test_eachtime s.profiling.each_time = True s.profiling.output_directory = os.path.dirname(__file__) + '/each_time/' with gpflow.settings.temp_settings(s): m.compile()
# todo: for some reason on osx a unix socket cannot be </s> actually failing.	retry_before_failing return retry(exception=AssertionError, timeout=None, retries=retries)
# todo: catch and report error if possible </s> return self._securitydetails	securityDetails def securityDetails(self) -> Union[Dict, 'SecurityDetails']: Security details if the response was received over the secure
#todo resend message + throttling </s> request.session.delete_test_cookie()	verify_computer@122 if user and user.is_active: auth_login(request, user) return HttpResponseRedirect(redirect_to) except VerifiedComputer.DoesNotExist:
# todo: unittest </s> mapper = chainmapper([dataset.a.mapper,	BaseSearchlight if 'append' in dir(mapper): mapper.append(feat_sel_mapper) feat_sel_mapper]) results.a['mapper'] = mapper
print('warning: exception during driver init, {}'.format(ep.name))  # todo: use proper logger </s> return none	load_drivers driver_init = ep.resolve() except: try: driver = driver_init()
# todo change to check for error when the functionality changes. currently acts as though it doesn't exist </s> assert_equal(res.status_code, 200)	TestApiBaseViews class TestApiBaseViews(OsfTestCase): def test_root_returns_200(self):
# todo: raise an error if finaloutputslot has len=0.  that means the user didn't load a batch dataset into the project. </s> parser.add_argument('--option_config_file', help='a json file with various settings', required=true)	getArgParser def getArgParser(): parser = argparse.ArgumentParser( description="Ilastik Cluster Workload Launcher" ) parser.add_argument('--project', help='An .ilp file with feature selections and at least one labeled input image', required=True) parser.add_argument('--output_description_file', help='The JSON file that describes the output dataset', required=False)
# todo check the op returned a view </s> if self.call_time > 0:	summary_function print >> file, '  Total time spent in calling the VM %es (%.3f%%)' % ( self.vm_call_time, val) val = 100. - self.vm_call_time * 100 / self.call_time print >> file, '  Total overhead (computing slices..) %es (%.3f%%)' % (
# todo (@awaelchli): standardize this across all plugins in lightning and lite. related refactor: #7324 </s> pass	on_validation_start
#@todo: move to utils in 0.4.10 </s> return	log_debug if not self.pyload.debug:
# todo straya </s> if tmp_old.header_hash == self.height_to_hash[uint32(0)]:	find_fork_for_lca def find_fork_for_lca(self, new_lca: BlockRecord) -> uint32: tmp_old: BlockRecord = self.block_records[self.lca] return uint32(0) if tmp_old.height in self.height_to_hash:
# todo: this test actually isn't very useful right now, but it will make sense </s> class d(object):	D
# todo(tonyg/slamm): is this assertion correct? </s> args=('%d.example.com' % i, sleep_multiplier_seconds * i))	TrafficShaperTest processes = [ multiprocessing.Process( for i in range(num_requests)] total_timer = IntervalTimer()
# todo: add mode, state </s> self._engine.addfillpath(*args, **kwargs)	addFillPath
# todo: content-type is hard-coded but ideally should be retrieved; </s> raise typeerror("unknown route type '{}'"	_get_tuple_from_route elif isinstance(route, tornado.web.URLSpec): pattern, handler_class = route.regex.pattern, route.handler_class .format(type(route).__name__)) return pattern, handler_class
# todo(vek): need to pass context in for access to auth_token </s> def __init__(self, name, state):	InstanceInfo self.name = name assert state in power_state.valid_states(), "Bad state: %s" % state
# todo create correct test data </s> channel.translator,	test_all_asset assert token1.approve(channel.address, half_amount) is True channel1 = ABIContract( channel.address, default_key=tester.k1,
# todo(stevemar): assert returned fields </s> raw_output = self.openstack('object create ' + self.container_name	test_object_create + ' ' + self.OBJECT_NAME) items = self.parse_listing(raw_output)
# todo(yuriyz): change to 404 (bug 1200517) </s> pdict = dbutils.get_test_port()	test_update_byid extra = {'foo': 'bar'} response = self.patch_json('/ports/%s' % pdict['uuid'],
# todo: remove this - cura-4482 </s> return self.getqualitydefinitionid(self._global_container_stack.getbottom())	activeQualityDefinitionId if self._global_container_stack:
# todo: update once calculation_magic is implemented </s> assert store.get_lpq_projects() == set()	test_not_eligible_not_lpq def test_not_eligible_not_lpq(self, store) -> None:
# rbarlow_todo: convert this callrequest into a celery task call </s> :type call_report: pulp.server.dispatch.call.callreport	cancel_agent_request :param call_request: The call request that has been cancelled. :type call_request: pulp.server.dispatch.call.CallRequest task_id = call_report.call_request_id consumer_id = call_request.args[0]
# todo: assume the fixup size is four bytes, probably bad. </s> class testdidntrunerror(exception):	TestDidntRunError
# todo: assert </s> self.remote.save_repo(repo, self.token)	createRepo self.remote.modify_repo(repo, "name", "testrepo0", self.token) self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token)
# todo: implement </s> except:	on_feed_output def on_feed_output(self, feed): try: raise PluginError('make_html requires Cheetah template engine') config = feed.config['make_html']
# todo in python 2.7 and later, this should be </s> instance of a sqlalchemy model.	DeserializationException pass
kwargs['application'] = application.objects.get(client_id=credentials['client_id'])  # todo: this should be cached one day </s> try:	PreAuthorizationMixin def dispatch(self, request, *args, **kwargs): uri, http_method, body, headers = self._extract_params(request) scopes, credentials = server.validate_authorization_request(uri, http_method, body, headers) log.debug('Saving credentials to session, %r.', credentials)
except httperror as e:  # @todo ask for server instead </s> chosenserver = chosenserverlist[0]  # the first value, server	chooseBestServer chosenServerList = BestServerList[random.randrange(0, len(BestServerList) - 1)]
# todo(dolph): can be uncommented pending bug 968519 </s> self.identity_api.add_user_to_tenant(self.tenant_bar['id'],	test_delete_user_with_tenant_association 'name': 'fakeuser', 'password': 'passwd'} user['id']) self.identity_api.delete_user(user['id'])
# todo: expect_match should work with emit() </s> self.target_command = 'ex_new'	TokenNew super().__init__(params, TOKEN_COMMAND_NEW,
# todo should this be handled differently when there are multiple ratings? </s> return cls.add_identifier(data, key)	get_trakt_data 'title': item.title, 'year': item.year
# todo see if this can be better done in one query </s> else:	_order_by yield column.desc()
# todo: get rid of sleep hack!! </s> self.cm.create_scope('other-scope')	testCreateScope scopes = self.cm.get_all_scopes() self.assertIsNotNone([s for s in scopes if s.name == 'other-scope'])
# todo stop guessing </s> print 'running the command failed. try running it from the console. arguments for subprocess.call: {0}'.format(	run_lint_command lint_result = os.path.join(app_dir, 'lint-result.xml') call_result = subprocess.call([lint, app_dir, '--xml', lint_result], shell=True) [lint, app_dir, '--xml', lint_result]) return lint_result, app_dir, ignore_layouts
if isinstance(err, asyncio.cancellederror):  # todo: remove when updated to 3.8 </s> log.warning("no peers in dht, announce round skipped")	BlobAnnouncer@27 await self.node.joined.wait() await asyncio.sleep(60, loop=self.loop) continue self.announce_queue.extend(await self.storage.get_blobs_to_announce())
''' todo: change conditional to return on non-http responses </s> parser = argparse.argumentparser(description='interplanetary wayback (ipwb) indexer')	checkArgs def checkArgs(argsIn): Check to ensure valid arguments were passed to the indexer and provide parser.add_argument('-d', '--daemon', help='Location of ipfs daemon (default 127.0.0.1:5001)', default=IP+':'+PORT, dest='daemon_address') parser.add_argument('-o', '--outfile', help='Path of newly created CDXJ. Shows progress by default unless suppressed with -q')
# todo(pep612): fix for paramspectype </s> arg_types=arg_types,	apply_generic_arguments arg_types = [expand_type(at, id_to_type) for at in callable.arg_types] remaining_tvars = [tv for tv in tvars if tv.id not in id_to_type] ret_type=expand_type(callable.ret_type, id_to_type), variables=remaining_tvars,
# compare filesizes todo print analysis of this :) </s> else:	read_log log_file = os.path.join(DATA_ROOT, '%(login)s/%(project_id)s.process.log' % locals()) if os.path.exists(log_file): return ''
#change status of todo </s> notification = frappe.new_doc("notification")	test_invalid_condition def test_invalid_condition(self): notification.subject = "test" notification.document_type = "ToDo"
# todo: figure out way to paramaterize node['osd_ids'] for this test </s> assert socket("tcp://%s:6800" % node["address"]).is_listening	test_osd_listens_on_6800
# todo: should we concatenate preprocessed_s and preprocessed_last_s_prime? </s> super(impalaagent, self).define_api_methods(	define_api_methods "environment-stepper/actor-component/policy", "environment-stepper/actor-component/dict-preprocessor-stack"
# todo: implement </s> @inlinecallbacks	sketch def sketch(): Example demonstrating how an application would normally talk to the queue. def doWork(self): txn = self.__txn__
1  # todo: fill in identifier </s> total  - total wall time to run the function call (including subcalls)	print_stats formated_stack.append(formated_line) print('\n'.join(formated_stack)) cumm   - total wall time for the function itself (removing subcalls) single - time spent on a _single_ execution (average time, really)
# todo: this should be configurable. some people may want such </s> return soap_error.format(**{'faultcode':'client', 'cid':cid, 'faultstring':faultstring})	client_soap_error
# todo test </s> return hash((frozenset(self.nodes), self.current_state.tostring(),	__hash__ def __hash__(self):
# todo: add for morph targets data. </s> if context == 'node':	decompose_transition def decompose_transition(matrix, context, export_settings): translation = convert_swizzle_location(translation) rotation = convert_swizzle_rotation(rotation)
# todo: remove when #980 has been merged </s> qidx = ['low', 'med', 'high', 'veryhigh'].index(format['3sat_qualityname'])	_sortkey prefer_http = 1 if 'rtmp' in format['url'] else 0 return (qidx, prefer_http, format['video_bitrate'])
except exception:  # todo: refactor this... </s> requestsnetworkwrapper()	test_init
# todo: remove this once there's proper support in upstream jinja </s> return encode_if_unicode(fn(*args, **kwargs))	ensure_str def wrapper(*args, **kwargs):
# todo: find another way ... </s> if isinstance(box, boxes.imagemarkerbox):	compute_atomicbox_dimensions def compute_atomicbox_dimensions(box): image_marker_layout(box) if isinstance(box, boxes.ReplacedBox):
# todo: use the solution we implement once #134 gets fixed </s> else:	testBigIntPymssql def testBigIntPymssql(self): if sys.version_info >= (3, ): py_type = long in_val = 123456789
# todo: rate limiting </s> def get_request_content_type(self):	DjangoHttpTransportContext return self.req.path def get_request_method(self): return self.req.META['CONTENT_TYPE'] def get_path_and_qs(self):
# todo complete this method </s> self.wooov = imd.wooov(self._cc, t1, t2, eris, kconserv)	make_ip kconserv = self.kconserv t1, t2, eris = self.t1, self.t2, self.eris self.Wovoo = imd.Wovoo(self._cc, t1, t2, eris, kconserv) self.made_ip_imds = True
# todo: avoid building interpolant every time? </s> return self._epochs	epochs @property
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> authorityurl = sampleparameters['authorityhosturl'] + '/' + sampleparameters['tenant']	Test_AcquireTokenWithUsernamePassword "username" : "crwilcox@microsoft.com", "password" : "SUPER SECRET PASSWORD" resource = '00000002-0000-0000-c000-000000000000' # or 'https://management.core.windows.net/' cache = None # TODO: Make this a cache driver
# todo need copy? </s> for i in numpy.where(mesh.is_boundary_node)[0]:	newton_update cols.append((jac_plus - jac_minus) / (2 * eps)) matrix = numpy.column_stack(cols) matrix[2 * i + 0] = 0.0 matrix[2 * i + 1] = 0.0
# todo: signal to the user that they should reload their data! </s> if self._sync_token is none:	_messenger_queue_publish "encoding": "JSON", "entity_fbid": self._state.user_id, topic = "/messenger_sync_create_queue" payload["initial_titan_sequence_id"] = str(self._sequence_id)
# todo: simulate short dataset with known properties and use it </s> assert_array_almost_equal(erds.sa.orig_offset, [2.4])	test_erdataset assert_array_equal(erds.samples[0], np.repeat(np.arange(1,5), nfeatures)) assert_array_equal(erds.sa.orig_onset, [evs[0]['onset']]) assert_array_equal(erds.sa.time, [np.arange(2.5, 11, 2.5)]) erds = eventrelated_dataset(ds, evs, time_attr='time', match='closest')
# todo(b/186451541): reduce the number of calls to model_fn. </s> prev_loss = train_metrics['loss']	_run_test train_metrics = metric_outputs['train'] self.assertEqual(train_metrics['num_examples'], expected_num_examples)
# todo: this crashes if a feature is already named 'index'. </s> def list_models(cls):	list_models can be used for image classification. Returns
# todo: convert into a proper api to detect read-only layouts </s> self._value = not self._value	CheckBox new_event = self._frame.rebase_event(event) if event.buttons != 0: return return event
# todo: location </s> content = obj.get('content', '')	object_to_html Returns: string, the content field in obj with the tags in the tags field converted to links if they have startIndex and length, otherwise added to seen_ids = set() mentions = []
# todo: make this configurable </s> condition = condition_conjunction(conditions)	condition_for_cell conditions = self.conditions_for_cuts(cell.cuts)
# todo: the stuff </s> self.taskname = task.name	SimpleTaskPersistence class SimpleTaskPersistence(SimplePersistence): def __init__(self, task): @property def plugin(self):
# todo: docs and comments </s> header = "test:"	test@47 softmaxes = {} labels = {} i = 1 with torch.no_grad():
# todo (t65593688): this should be removed after </s> )	get_tensorizers field_names=["source_sequence", "dict_feat", "target_sequence"], ), src_tensorizer = TokenTensorizer.from_config( TokenTensorizer.Config(
# reasons why we said no. todo: allow configurable error messages </s> raise autherror(	DirectoryHandler ) else: 403, "This user is not permitted to create this alias", )
self.button_align_test = wx.button(self, label="align test")    # todo maybe align left? </s> self.button_save = wx.button(self, label="save")	WallpaperPanel self.sizer_settings_right = wx.BoxSizer(wx.VERTICAL) self.sizer_bottom_buttonrow = wx.BoxSizer(wx.HORIZONTAL) self.button_delete = wx.Button(self, label="Delete") self.sizer_setting_span_mode = wx.StaticBoxSizer(wx.VERTICAL, self, "Span mode")
# todo: requires special treatment? </s> cls._building_line_to_game_entity(building_line)	_process_game_entities for unit_line in full_data_set.unit_lines.values(): cls._unit_line_to_game_entity(unit_line)
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> "password" : "super secret password"	Test_AcquireTokenWithUsernamePassword "authorityHostUrl" : "https://login.windows.net", "clientId" : "04b07795-8ddb-461a-bbee-02f9e1bf7b46", # xplat's which is supposed to be in every tenant } authorityUrl = sampleParameters['authorityHostUrl'] + '/' + sampleParameters['tenant']
# todo(berrange): why do we bother converting the </s> xml = virt_dom.xmldesc(0)	resume_state_on_host_boot virt_dom = self._conn.lookupByName(instance['name'])
# todo: just a quick hack (screen is erased before scrolling begins). </s> max( red,  0 )	LedCtrlChar green *= 21 blue   =  0 min( green, 63 ) max( green,  0 )
accept_federated_only=self.federated_only)  # todo: 466 </s> def canonical_public_address(self, address_bytes):	canonical_public_address @canonical_public_address.setter
# todo: rate limiting </s> super(zmqmethodcontext, self).__init__(app, methodcontext.server)	ZmqMethodContext def __init__(self, app):
).consume()  # todo see issue 170 </s> run_cleanup_job('aws_import_ec2_instances_cleanup.json', neo4j_session, common_job_parameters)	cleanup_ec2_instances @timeit
# todo debug </s> self.type = none	RuleElement class RuleElement: self.triggered = False self.timeWhenTriggered = 0.0
# todo: attachments. </s> however, note that that function does some things that this one shouldn't,	_save_migrated_models See SubmissionPost.save_processed_models for ~what this should do.
if key.endswith('_state'): # todo: kludge </s> defaultcontenttype = 'application/octet-stream'	SpectrumResource def grrender(self, value, request): (freq, fftdata) = value
# todo(b/182621549): for sobol sequences, dimension should be known at graph </s> random_type_sample = randomtype.stateless	_mvnormal_pseudo_antithetic if random_type == RandomType.PSEUDO_ANTITHETIC: random_type_sample = RandomType.PSEUDO result = _mvnormal_pseudo( antithetic_shape,
# todo(piyush): current api-site doesn't contain this api description. </s> return service_client.responsebody(resp, body)	list_tenants resp, body = self.get('tenants') self.expected_success(200, resp.status)
# todo(vek): need to pass context in for access to auth_token </s> assert state in power_state.valid_states(), "bad state: %s" % state	InstanceInfo self.name = name
""" todo. """ </s> def media_next_track(self):	media_next_track
# todo: consider implement it through calling self.collected </s> "deprecated; use `browser.element('#foo').config.driver` style for the majority of cases",	parent @property def parent(self): DeprecationWarning) return self.get(Query('parent search context', lambda element: element().parent))
# todo: alot of stuff here </s> self.out_parsing = true	decode_out@74 if not self.out_parsing and d != Decoder.RESPONSE_MAGIC: logger.debug('No kamstrup request magic received, got: {0)'.format(data[0].encode('hex-codec')))
# todo remove this method. should be handled in importtask creation. </s> return	SingletonImportTask autotag.apply_item_metadata(self.item, self.match.info) def _emit_imported(self, lib): for item in self.imported_items(): plugins.send('item_imported', lib=lib, item=item)
# todo: figure out how to mock open </s> if (defaultorcustom == 'default'):	_helper_assert_path_isfile_not_present with patch('os.path.isfile', return_value=isfile) as mock_isfile: actual = None correct_dir = config.examples_dir actual = eg_util.has_default_entry_for_program(program, config)
# todo: remove str() when dropping support for py37. </s> str(self.connection.settings_dict['name'])]	DatabaseClient args = [self.executable_name,
# todo fixme insert and replace instead </s> if getattr(cls, '__origin__', none) == union:	_map_type r = tmap.get(cls, None) if r is not None: elems = cls.__args__ elems = [e for e in elems if e != type(None)]
# todo: refactor. </s> db.add_son_manipulator(autoreference(db))	test_auto_ref_and_deref @gen_test def test_auto_ref_and_deref(self): db.add_son_manipulator(NamespaceInjector()) a = {"hello": u"world"}
# todo: remove warning check once deprecated </s> data = {	TestFrameSindex @pytest.mark.skipif(not sindex.has_sindex(), reason="Spatial index absent, skipping") class TestFrameSindex: "A": range(5), "B": range(-5, 0),
# todo: test this </s> if is_commitment_signed:	_store_raw_msg_if_local_update chan.hm.store_local_update_raw_msg(raw_msg, is_commitment_signed=is_commitment_signed)
# todo: move to serializer </s> img = img.convert('rgb')	create_thumbnail os.makedirs(os.path.dirname(thumbnail_path)) except OSError: img.save(thumbnail_path, 'JPEG', quality=85)
# todo: ... </s> will return a tuple of (`metric_name`: str, `metric_value`: float)"""	wrap_xgboost_metric ------- eval_metric: Function def eval_metric(prediction, target): :meth:`xgboost.sklearn.XGBModel.fit`
# todo: change the way we do it (csv dialect may change, encoding </s> progress = progressbar(prefix="importing data", unit="bytes")	ImportDataCommand encoding = "utf-8"  # TODO: receive as a parameter timeout = 0.1  # TODO: receive as a parameter file_header = open_compressed(filename).readline().strip().split(",") table_schema = self.table.schema
# todo: does this import need to be delayed because </s> if self.get_option("mipgap") is not none:	ExtensiveFormAlgorithm print("Initializing ef solver with options=" +str(list(self.get_option("solver_options")))) if (self.get_option("mipgap") < 0.0) or \ (self.get_option("mipgap") > 1.0):
# todo counts as yes if vhnd was not available </s> ('ifa_tablets', "# of pregnant women who have received at least 30 ifa tablets"),	HealthStatus ('mother_reg', "# of Mothers of Children Aged 3 Years and Below Registered"), ('childrens', "# of Children Between 0 and 3 Years of Age Registered"), ('weight_once', "# of Pregnant Women Whose Weight Gain Was Monitored At Least Once"), ('weight_twice', "# of Pregnant Women Whose Weight Gain Was Monitored Twice"),
# todo this should depend on the error (even more granularity) </s> self.storage.put('lightning_payments', self.payments)	delete_payment del self.payments[payment_hash_hex] except KeyError: self.storage.write()
# todo: this also needs to trigger filter_specs.updateui to switch to </s> num_cols = self.tblpz.columncount()	_clear_table self.tblPZ.setHorizontalHeaderLabels(["Z", "P"]) self.Hmax_last = 1.0 for row in range(2): for col in range(num_cols):
# todo: i put dummy() to fix below, remove the comments after a while. </s> self.assertequal(fsa.tsub , 0) # no call_leave called	test_stop_in_middle self.assertEqual(fsa.ncall , 1) self.assertEqual(fsa.nactualcall, 0) self.assertEqual(fsb.ttot , 4)
# todo: optimizer state gets cast to fp16 and back to fp32 for </s> params = fp32_params	Adam p32 = torch.nn.Parameter(p.data.float()).to(p.device) p32.grad = torch.zeros_like(p32.data) self.fp32_param_groups = [] param_groups = list(params)
# todo assert cls.__tablename__ == '' </s> yield from dbcleanup_wrapper(session, s)	galaxy_session @pytest.fixture def galaxy_session(model, session, user):
# todo: log errors to log file </s> def get_user(self):	get_user
# todo: use optparse command options instead. </s> rel_path = path[prefix_length:]  # for example /subpackage/module.py	_get_module_names module_names = [] for path in paths: rel_path = os.path.splitext(rel_path)[0]  # for example /subpackage/module parts = []
# todo - fix this to be more efficient, so we don't parse the file twice </s> return field_name[:_max_field_name_lenth]	_truncate by mysql.  This is NOT smart enough to check for conflicts, so there could be issues if an xform has two very similar, very long, fields''' return field_name
# todo: alltoall not yet implemented on xla:cpu </s> x = onp.arange(prod(shape), dtype=onp.float32).reshape(shape)	testGradOfPsum def f(x): return lax.psum(x, axis_name='i') jtu.check_grads(f, (x,), 2, ["fwd", "rev"], 1e-2, 1e-2, eps=1.)
self.progressbar.set_text("sorting ... ") # todo(jflesch): i18n/l10n </s> self.wtree.get_object("menuitemscan").connect("activate", self._scan_next_page)	_connect_signals self.wTree.get_object("menuitemNew").connect("activate", self.new_document) self.wTree.get_object("toolbuttonNew").connect("clicked", self.new_document) self.wTree.get_object("toolbuttonScan").connect("clicked", self._scan_next_page) self.wTree.get_object("menuitemQuit").connect("activate", lambda x: self._destroy())
# todo: support other output fields </s> flat.extend(x)	flatten_list for x in nested:
# todo: deprecated - remove in version 0.10 </s> else:	_create_feature_key if self.ENABLE_FEATURE_STRING_COMPRESSION: compressed = zlib.compress(bytes(feature_str, "utf-8")) return feature_str
raise exception  # todo </s> primary key of the table. if it is desired that there be no primary	add_table the ``name`` matches that of an existing table with a different ``key``, a ``ValueError`` will be thrown. key, this can be achieved by creating the table manually, or with a custom query, and then creating the WillieDB object.
pass #todo fix imports </s> attrib = node.attr	visit_Attribute self.generic_visit(node)
# todo(vek): need to pass context in for access to auth_token </s> self.state = state	InstanceInfo def __init__(self, name, state): self.name = name
# todo: test coverage for this branch </s> if instance.subscribed:	subscribe_user instance = Subscription.objects.get_or_create( newsletter=my_newsletter, user=request.user already_subscribed = True elif confirm:
#todo kajak doesnt like my consumer-list -> research why? </s> if float(signal._min) != 0:	createSignal@68 value.set('slope',str(signal._factor)) if float(signal._offset) != 0: value.set('min',str(signal._min)) if float(signal._max) != 1:
#@todo: remove in 0.4.10 </s> raise notimplementederror	handleFree def handleFree(self, pyfile): else: self.fail(_("Required premium account not found"))
# todo: remove </s> return lookup_group_plugin(group_type).form_to_db_schema()	_form_to_db_schema
pass     # todo: </s> missed.append(package)	_missed if package not in list_of_installed:
# todo: remove in 21.08 </s> this method copies the cache from 'cache_audio_dir'	copy_cache@171 to TTS specific cache directory given by get_cache_directory()
# todo: handle index/keyerror here when we overrun a segment </s> if page_number < 1:	get_page_buffer logger.warning('unexpected page number requested: %d', page_number) offset = self.page_size * page_number
# todo: confirm is uri </s> :py:func:`schema <py2neo.neo4j.schema>`	schema @property def schema(self): if self.__schema is None: self.__schema = Schema(self.uri.string + "schema")
return response(status=400)  # todo </s> message = ["nodes", self.known_nodes.abridged_nodes_dict()]	send_nodes def send_nodes(subscriber):
# # todo: # fixme: </s> date_to = date_from	base64Decoded_page if not date_range: date_range.append(datetime.date.today().strftime("%Y%m%d")) else: date_from = date_from[0:4] + '-' + date_from[4:6] + '-' + date_from[6:8]
# todo(sdake) the parameters to delete operations are highly suspect </s> if expected_retval:	_fake_rpc_method def _fake_rpc_method(*args, **kwargs): self.fake_args = args return expected_retval
# todo: direct use of json['error'] is deprecated; use display_message('...', 'error') instead. </s> j_dic['modifications'] = []	enrich_json_with_base j_dic['entities'] = [] j_dic['events'] = [] j_dic['equivs'] = [] j_dic['infos'] = []
# todo: should be injected </s> function['name'] = function.pop('functionname')	parse_function @staticmethod return (function['name'], function)
# todo: try simply using all possible fields instead of extracting features manually. </s> ])	go@78 ])), ], X = extraction_pipeline.fit_transform(bugs) X, y = RandomUnderSampler(random_state=0).fit_sample(X, y)
# todo: make truly async </s> response = request.execute()	CloudSQLFacade@20 database_instances = [] request = self._cloudsql_client.instances().list(project=project_id) database_instances.extend(response.get('items', [])) request = self._cloudsql_client.instances().list_next(previous_request=request, previous_response=response)
# todo(vek): need to pass context in for access to auth_token </s> self.name = name	InstanceInfo class InstanceInfo(object): assert state in power_state.valid_states(), "Bad state: %s" % state self.state = state
# todo: try reconnecting? </s> def received(self):	received @property
#self.resetacount() #@todo implement </s> tmp = url.split("/")	getInfo ids = "" names = "" ids+= ","+tmp[-2] names+= ","+tmp[-1]
#todo: remove this transformation </s> continue	determine_all_nonschema_privileges all_reads = set() for schema, owner in dbcontext.get_all_schemas_and_owners().items(): writes, reads = determine_nonschema_privileges_for_schema(role, objkind, schema, dbcontext) all_writes.update(writes)
# todo(dcramer): ideally we could just send the signal to the subprocess </s> db.session.commit()	save_chunk ))
# todo: this method should not require the training data. </s> return self._compute_loss(features, labels, logits, params, mode)	Model devices=devices) def _loss_op(features, labels, params, mode, config): def _normalize_loss(num, den=None): if isinstance(num, list):  # Sharded mode.
# todo: tab completion for the local system. </s> write(ansi.cub())	cursor_back def cursor_back(self, adjust_internal_cursor=True): if self.cursor_position + 1 <= len(self.input_buffer): else: write(ansi.CUU() + ansi.CUF(context.window_size[1]))
# todo find out what is best used here! </s> probs = np.zeros([len(x), len(estimators)])	GPyClassifier def predict_proba(self, X): if self.estimators is None: for i, model in enumerate(estimators): probs[:,i] = model.predict(X)[0].flatten()
#todo - look at alphabet? </s> def test_str_find(self):	test_str_find
# todo: docstring </s> vsc_fid = int(args['frameid'])	on_setExpression@1020 @async_handler pyd_tid, pyd_fid = self.frame_map.to_pydevd(vsc_fid) safe_repr_provider.set_format(
# todo: reactivate after fixing alternative selection </s> }	test_system_importer_file_csv_form_based_location_form_filled data_dict = { 'systemstatus': systemstatus_id, file_dict = { 'systemcsv': SimpleUploadedFile(upload_csv.name, upload_csv.read()),
# todo add brief documentation what that means </s> return torch.cat((y1, y2), -1)	NICECouplingBlock y2 = x2 - self.G(x1_c) y2_c = torch.cat([y2, *c], 1) if self.conditional else y2 def jacobian(self, x, rev=False): return 0
# todo: else: req.warning('...') </s> self.log.info('uninstalling plugin %s', plugin_filename)	_do_uninstall plugin_path = os.path.join(self.env.path, 'plugins', plugin_filename) if not os.path.isfile(plugin_path): os.remove(plugin_path)
# todo :: move arbitray path construction to storagelayout object </s> else:	PartitionUploader logger.info( msg='Retrying send because of a Request Skew time', raise typ, value, tb @retry(retry_with_count(log_volume_failures_on_error))
# todo(developer): uncomment and set to a path to your audio file. </s> for i, result in enumerate(response.results):	transcribe_file_with_multilanguage@216 alternative_language_codes=[second_lang]) print('Waiting for operation to complete...') alternative = result.alternatives[0] print('-' * 20)
# todo debug </s> if (ruleweekdaynew.weekday < 0 or	parseRuleRecursively + "in weekday tag.") ruleWeekdayNew.weekday = int(item.attrib[ ruleWeekdayNew.weekday > 6): raise ValueError("No valid value for 'weekday' "
# todo: implement clock interrupt. </s> def __leaf_01(ql: qiling):	__leaf_01
# todo: avoid dummy and generate func here when inlining is possible </s> return lambda df: len(df._data[0])	df_len_overload def df_len_overload(df): if len(df.columns) == 0:  # empty df
#todo: algorithm is cryptfiltername, specified in the /cf dictionary </s> decodedstream = ''	dctDecode @return: A tuple (status,statusContent), where statusContent is the decoded PDF stream in case status = 0 or an error in case status = -1
# todo: remove in future release </s> cmd += ' --permanent'	remove_port if permanent:
# todo: we want to create a state group for this set of events, to </s> events,	_resolve_state_events if key[0] == EventTypes.Member: logger.debug("Resolving conflicted member lists %r", events) auth_events )
# todo: remove this as soon it is fixed in plaidml. </s> logger.debug("adding session batchsize: %s", batchsize)	add_session_batchsize def add_session_batchsize(self, batchsize):
# todo: limit/check colorcode </s> max( blue,  0 )	LedCtrlChar min( green, 63 ) max( green,  0 ) for i in range(81, 1, -10): for j in range(8):
# todo: simplify this via aliasing methods in `frappe.qb` </s> "name": name	delete_all_passwords_for try: frappe.db.delete("__Auth", { }) except Exception as e:
# self.skiptest("unfinished (bad functionality?)") # todo </s> cmd = "{systemctl} --all show zza.service"	test_2027_show_unit_for_oneshot_service self.assertTrue(greps(out, r"^SubState=")) self.assertTrue(greps(out, r"^UnitFileState=")) out, end = output2(cmd.format(**locals())) logg.info(" %s =>%s\n%s", cmd, end, out)
# todo: change to us-east-1 </s> service_config = {'regions': {'us-east-1': {}, 'us-west-1': {}}} #, 'cn-north-1': {}}}	test_get_sns_info def test_get_sns_info(self): get_sns_info(credentials, service_config, ['us-east-1', 'us-west-1'], 'aws') get_sns_info(credentials, service_config, ['us-east-1', 'us-west-1'], 'aws-us-gov')
# todo these args are locked and can not be changed </s> finally:	video_capture video_cap = cv2.VideoCapture(video_path) try: video_cap.release()
# todo: figure out, what's going on with v6 here! </s> assert_is_instance(ar, annexrepo, "annexrepo was not created.")	test_AnnexRepo_instance_from_clone @with_tempfile def test_AnnexRepo_instance_from_clone(src, dst): ok_(os.path.exists(os.path.join(dst, '.git', 'annex'))) with swallow_logs(new_level=logging.WARN) as cm:
# todo: do not require xml directly here. </s> def parse_label(self):	parse_label
# todo partially update stored playlists? </s> logger.debug(	track_message_changed u'Callback called: Message for track %d in playlist ' u'"%s" changed to "%s"', position, playlist.name(), message)
# todo: the following skipped suite and fixtures should be enabled </s> provider_name = 'vultr'	VultrProviderTests class VultrProviderTests(TestCase, IntegrationTests): domain = 'capsulecd.com' def _filter_headers(self):
# todo: use spoolup options to fetch main value </s> repspoolmax = container.getmodifieditemattr("repairmultiplierbonusmax")	handler@17 if "projected" in context: repAmountBase = container.getModifiedItemAttr("armorDamageAmount") repSpoolPerCycle = container.getModifiedItemAttr("repairMultiplierBonusPerCycle") repAmount = repAmountBase * (1 + calculateSpoolup(repSpoolMax, repSpoolPerCycle, cycleTime, SpoolType.SCALE, 1))
# todo - this is what i get back from kafka at the moment, clearly it's wrong </s> self.assertraises(	test_partition_error exceptions.UnknownTopicOrPartition, lambda: protocol.OffsetResponse(
# todo: avoid dummy and generate func here when inlining is possible </s> return lambda df: 0	df_len_overload if len(df.columns) == 0:  # empty df
raise exceptions.mpdnotimplemented  # todo </s> *musicpd.org, stored playlists section:*	playlistdelete@154 @protocol.commands.add('playlistdelete', songpos=protocol.UINT) ``playlistdelete {NAME} {SONGPOS}`` Deletes ``SONGPOS`` from the playlist ``NAME.m3u``.
# todo: need to end xa state here </s> class _oraclebinary_float(_oraclebinaryfloat, oracle.binary_float):	_OracleBINARY_FLOAT
# todo: remove temporary workaround once https://github.com/python-babel/babel/issues/415 has been resolved. </s> if self.tagger.autoupdate_enabled:	GeneralOptionsPage self.ui.server_port.setValue(config.setting["server_port"]) self.ui.analyze_new_files.setChecked(config.setting["analyze_new_files"]) self.ui.check_for_updates.setChecked(config.setting["check_for_updates"]) self.ui.update_level.clear()
# todo: some kind of value escape </s> self.counts[values] += 1	CounterMetric if values not in self.counts: self.counts[values] = 1 def fetch(self): return dict(self.counts)
# todo: enable output dtype selection. </s> cmd = re.sub(	calc@74 kwargs['count'] = len(parts) with rasterio.open(output, 'w', **kwargs) as dst: r'{(\d)\s*,\s*(\d)}', lambda m: 'sources[%d][%d]' % (
# todo: handle bidi </s> block_level_width(box, containing_block)	block_replaced_width def block_replaced_width(box, containing_block, device_size):
#todo - should the default be gapped(single_letter_alphabet) instead? </s> for record in records :	ClustalWriter alignment_length = None if alignment_length is None : alignment_length = len(record.seq)
# todo(tr3buchet) - remove comment in multi-nic </s> def get_public(self):	get_public
# todo: cartopy has had two formatters for a while but we use newer one </s> feat = getattr(self.projection, method)(ax=self)	_format_apply if getattr(self, f'_{name}', None):  # already drawn continue if isinstance(feat, (list, tuple)):  # list of artists? for obj in feat:
# todo: remove hardcoded ad-hoc behaviors. </s> return 4 if get_bits() == 32 else 8	get_size_of_pointer
# todo: do we need to skip config.add_slack variable here? </s> else (lambda v: true)	generate_norm1_norm_constraint discrete_only: Bool only optimize on distance between the discrete variables model_vars = list(filter(var_filter, model.component_data_objects(Var))) setpoint_vars = list(
raise skiptest  #todo: figure out why this randomly started failing. </s> eq_('text/html; charset=utf-8', response['content-type'])	test_content def test_content(self): eq_(200, response.status_code)
# todo: will probably need to make this configurable at some point </s> a reference to a secondary table in an aggregate table	SecondaryTableDefinition table_definition = models.ForeignKey(AggregateTableDefinition, on_delete=models.CASCADE, related_name='secondary_tables')
pass  # todo </s> }	ThreeDScene "camera_config": { "samples": 4, def begin_ambient_camera_rotation(self, rate=0.02): pass  # TODO
return 0.5, 1 #todo make a better angle here. </s> saferandom = 4	FIREFOX EDGE = 1 MEDIUM = 2
raise exceptions.mpdnotimplemented  # todo </s> subscribe to a channel. the channel is created if it does not exist	subscribe@18 def subscribe(context, channel): *musicpd.org, client to client section:* already. The name may consist of alphanumeric ASCII characters plus underscore, dash, dot and colon.
@unittest.skip('not written')  # todo: finish! </s> def test_simplenamespace(self):	test_SimpleNamespace @py3_only
# todo: clean up </s> return none  # default	pubsub_cache_size @pytest.fixture
# todo: add test </s> for key in kwargs.keys():	remove_extra def remove_extra(kwargs, flavor='lattice'): if key in stream_kwargs: kwargs.pop(key)
# todo: give a vanilla example </s> for appliance in predicted_power:	feca@26 ------- re: float representing Fraction of Energy Correctly Assigned appliance_energy_predicted = np.sum(predicted_power[appliance].values) total_energy_predicted = np.sum(predicted_power.values)
# todo figure this out </s> "qml-module-qtqml-models2",	QmlPlugin "qml-module-qtmultimedia", "qml-module-qtorganizer", "qml-module-qtqml-statemachine", "qml-module-qtquick-controls",
# urwid.text( " todotxt-machine ", align='center' ), </s> return true	selectable
# todo(b/141131288): enable complex-valued sorts on tpu. </s> slice = lambda x: lax.slice(x, starts, limits, strides)	testSliceGrad def testSliceGrad(self, shape, dtype, starts, limits, strides, rng_factory): rng = rng_factory(self.rng()) check_grads(slice, (operand,), 2, ["fwd", "rev"], eps=1.)
# todo: make this configurable? </s> return ( c.user and authorizer.is_authorized(c.user, action,	_has_purge_permissions action = model.Action.PURGE
# todo commit hash </s> manifest = json.load(f)	index_command directory = Path(args.directory) manifest_path = directory.joinpath("repos.json") repos = manifest.get("repos", []) pkgs: Dict[str, Any] = {}
# todo: vip1019 </s> def __repr__(self):	ListType self.count = count def eq(self, other): return repr(self.subtype) + '[' + str(self.count) + ']'
# look & feel todo:turn on.. </s> ret1= core.segmap(img1)	test_segment_input_output_spec_check@13 def test_segment_input_output_spec_check(): img1= cv2.imread('./fixture/real_proj/images/bw1.png') ret2= core.segmap(img2)
#todo - can we raise the error before the unit test function </s> pass	ComparativeTest class ComparativeTest(unittest.TestCase):
# todo: groupby executor, spawn many _as_completed coroutines </s> futures2, keys = unpack_remotedata(futures)	_gather @gen.coroutine keys = list(keys) yield All([self.futures[key]['event'].wait() for key in keys])
# todo(aron): move these client test cases to their own test class </s> test_log_csv_resp = self.client.get(self.api_test_log_csv_url + "?group_id=" + self.group.id)	test_api_auth_not_logged_in self.assertFalse(user_csv_resp.content, "Authorization error") attempt_log_csv_resp = self.client.get(self.api_attempt_log_csv_url + "?group_id=" + self.group.id) self.assertFalse(test_log_csv_resp.content, "Authorization error") exercise_log_csv = self.client.get(self.api_exercise_log_csv_url + "?group_id=" + self.group.id)
self.mdbx = maestral()  # todo: create or get daemon instead? </s> self.seticon(self.icon())  # reload icon	show_when_systray_available def show_when_systray_available(self): self.show() else:
# todo: fire_switch_state_change(self, channel_index, old_switch_state, switch_state, from_myself) </s> def get_clients(self):	get_clients
# todo: refactor. </s> def test_auto_ref_and_deref(self):	test_auto_ref_and_deref db = self.db db.add_son_manipulator(AutoReference(db))
#todo: unit tests </s> dashboard_id = dashboard_folder._id	dashboard def dashboard(auth): user = auth.user return {'addons_enabled': user.get_addon_names(), 'dashboard_id': dashboard_id
# @todo: remove this if in 0.6 </s> name=el.get('name'),	_to_image def _to_image(self, el): driver=self.connection.driver, extra={'serverId': el.get('serverId')})
# todo: convert position ordering according to data axistags </s> self.editor.imageviews[self.editor._lastimageviewfocus]._isrubberbandzoom = false	rubberBandZoom self.editor.imageViews[self.editor._lastImageViewFocus]._cursorBackup = self.editor.imageViews[self.editor._lastImageViewFocus].cursor() self.editor.imageViews[self.editor._lastImageViewFocus].setCursor(Qt.CrossCursor) self.editor.imageViews[self.editor._lastImageViewFocus].setCursor(self.editor.imageViews[self.editor._lastImageViewFocus]._cursorBackup)
# todo: should this fail instead? </s> ),	test_few {'id': 1, 'name': 'spam'}, {'id': 3, 'name': ''}, ]) self.assert_received(self.debugger, [
# todo(developer): uncomment and set the following variables </s> from google.cloud import scheduler	delete_scheduler_job@63 from google.api_core.exceptions import GoogleAPICallError client = scheduler.CloudSchedulerClient()
# todo(sbdchd): move to queries </s> not_mergeable = auto()	MergeabilityResponse OK = auto() NEEDS_UPDATE = auto() WAIT = auto()
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> def callback(err, tokenresponse):	Test_AcquireTokenWithUsernamePassword resource = '00000002-0000-0000-c000-000000000000' # or 'https://management.core.windows.net/' cache = None # TODO: Make this a cache driver print(tokenResponse) self.fail("Not implemented")
# todo: this should be done on evoked </s> event_id, tmin, tmax = 1, -0.11, 0.15	test_dics_epochs@77 raw.info['bads'] = ['MEG 2443', 'EEG 053']  # 2 bads channels left_temporal_channels = mne.read_selection('Left-temporal')
irregular_dims = ['time', 't']  # todo: get irregular dims from dataset_type </s> def just_time(ds):	_get_group_by_func if hasattr(group_by, '__call__'): return group_by try: return ds.time
# todo: actually test this once i figure out how to do this in py.test </s> if species.startswith(genus.capitalize()):	in_genus_list def in_genus_list(species, genus_list): return True return False
# todo make this configurable </s> else:	sender_to_nick_and_color user = room.users[sender] nick = user.display_name nick = shorten_sender(sender) nick_color_name = W.info_get("nick_color_name", nick)
# todo get function data </s> def get_parent_until(self, *args):	Exec class Exec(object): def __init__(self, base): return self.base.get_parent_until(*args)
# todo: test with multiple responses </s> if e in dont_test:	test_all_estimators estimators = all_estimators() clf = LDA() continue with warnings.catch_warnings(record=True) as w:
# todo task python? </s> from -- string from where to start the input	pipe_fetchpage@48 _INPUT -- not used since this does not have inputs. conf: to -- string to limit the input token -- if present, split the input on this token to generate items
#todo: use invalidation time </s> values = (none, # database will handle this	dump_forward The caller is responsible for ensuring from_id is a unique and correct ID Params: MessageFwdHeader Telethon object forward.date.timestamp(), forward.from_id,
# todo: weather data source changed, temporarily disable, will enable it later when new data source is available. </s> transfer_time = self._decision_strategy.transfer_time	_on_action_received if executed_number > 0: station.bikes = station_bikes - executed_number transfer_evt = self._event_buffer.gen_atom_event(evt.tick + transfer_time, CitiBikeEvents.DeliverBike, payload)
# todo: instead of discarding pending jobs, maintain them </s> conn, addr = yield sock.accept()	scheduler_server logger.debug('Scheduler at %s:%s', addrinfo.ip, self.scheduler_port) sock.listen(32) Task(self.scheduler_req, conn, addr)
#todo load this from somewhere </s> "memory %s" % mem_usage if mem_usage else none	print_process "elapsed %s" % hms(start_elapsed), "exp. remaining %s" % remaining, ] print >> log.v5, ", ".join(filter(None, info))
# todo: verify logic for create -- we shouldn't 'annexify' non-annexified </s> pass	_EarlyExit
# todo: test 2b: planilha deployed mais atualizada que total (deployed) </s> continue	row_with_sorted_columns row_dates = set() for key in row.keys(): label, day, month = key.split("_") row_dates.add(f"2020-{int(month):02d}-{int(day):02d}")
# todo find out what is best used here! </s> cs = configurationspace()	get_hyperparameter_search_space name="thetaL", lower=1e-5, upper=1e-3, default=1e-4, log=True) thetaU = UniformFloatHyperparameter( cs.add_hyperparameter(nugget) cs.add_hyperparameter(thetaL)
# todo(dspasovski): fix this. </s> type=amo.addon_webapp)	get_new_cat def get_new_cat(self):
asynchronous=false, # todo: (true) when jconnor fixes </s> units,	Content } args = [ options, ]
# todo(lbragstad): move this test to tests/test_v3_assignment.py </s> self.delete('/roles/%(role_id)s' % {	test_delete_role 'role_id': self.role_id})
# todo: check for valid values </s> self.syllabic = 'single'	Lyric self.syllabic = 'middle' else: # assume single def _getMX(self): Returns an mxLyric
# todo: convert into a proper api </s> self._value = not self._value	CheckBox new_event = self._frame.rebase_event(event) if event.buttons != 0: return return event
# todo: check arp reply is valid </s> self.config_file = os.path.join(self.tmpdir, 'valve_unit.yaml')	setup_valve def setup_valve(self, config): self.faucet_event_sock = os.path.join(self.tmpdir, 'event.sock') self.logfile = os.path.join(self.tmpdir, 'faucet.log')
# todo(solitude): remove this. </s> def activity_log(request, userid):	activity_log all_apps = request.amo_user.addons.filter(type=amo.ADDON_WEBAPP) return jingo.render(request, 'account/activity.html', {'log': _get_items(None, all_app)})
#todo: remove this transformation </s> all_reads.update(reads)	determine_all_nonschema_privileges all_writes.update(writes)
@unittest.skip('not written')  # todo: finish! </s> @py3_only	test_SimpleNamespace def test_SimpleNamespace(self): raise NotImplementedError
# todo update this code once keras > 2.0.4 is released </s> return self.labeling_.predict(	SequenceLabeling Returns ------- sequence, batch_size=batch_size, verbose=verbose)
# todo: move to base class </s> max_x = max(arr2)	getNodesRect if any([len(arr1) == 0, len(arr2) == 0, len(arr3) == 0, len(arr4) == 0]): return None min_y = min(arr3) max_y = max(arr4)
# todo hmm... ok, need to document reload() </s> class user_config:	user_config
# todo: move this to an integration test? </s> recoverable_signature = bytes(signature) + bytes([1])	make_recoverable_signature pubkey_bytes = coincurve.PublicKey.from_signature_and_message(recoverable_signature, data_hash, hasher=None) \ .format(compressed=False) return recoverable_signature
# todo: federation should provide one method to send, </s> profile = profile.objects.get(guid=guid, user__isnull=false)	receive_task profile = None if guid: except Profile.DoesNotExist: logger.warning("No local profile found with guid")
# todo debug </s> raise valueerror("no valid value for 'time' attribute "	parseRuleRecursively ruleMonthdayNew.time = str(item.attrib["time"]) if (ruleMonthdayNew.time != "local" and + "in monthday tag.") ruleMonthdayNew.monthday = int(item.attrib[
common_path=prefix,  # todo: add key? </s> return autoresolve_generic(base, decisions, strategies)	autoresolve_cells
# todo: make an ascii-art bar </s> return "%.1fms" % (1000*s)	render_time if s >= 0.01: return "%dms" % (1000*s) return "%dus" % (1000000*s)
# todo: take this out later </s> or os.path.exists(os.path.join(root, dir, 'scripts', 'python.exe'))):	egg_info_path dirs.remove(dir) for dir in list(dirs): dirs.remove(dir) if dir == 'test' or dir == 'tests':
# todo(ytknzw): add more specific assertion with the test case. </s> def fail_objective(_: trial) -> float:	fail_objective
# todo - update this to use content-disposition instead of file_name </s> url = url + ("received_count=%s" % received_count)	download_xforms received_count = django_model.objects.count() if url.find("?") == -1: url = url + "?" print "Hitting %s" % url urllib.urlretrieve(url, to_file)
# todo - temporary workaround while yt bug not fixed </s> return ''	GetThumbFromSnippet try: return snippet['thumbnails']['high']['url']
# todo(yuriyz): change to 404 (bug 1200517) </s> response = self.patch_json('/ports/%s' % pdict['uuid'],	test_update_byid def test_update_byid(self): pdict = dbutils.get_test_port() {'extra': extra}) self.assertEqual(response.content_type, 'application/json')
# todo: this is a jump. </s> vi_cmd_data['motion']['command'] = 'vi_big_m'	vi_big_m def vi_big_m(vi_cmd_data): vi_cmd_data['motion']['args'] = {'mode': vi_cmd_data['mode']} return vi_cmd_data
# todo : documentation pending </s> for key, value in response.cookies.items():	get_confirm_token if key.startswith('download_warning'): return value
# todo: check output </s> prob.set_solver_print(level=2)	test_hierarchy_iprint g2.linear_solver = ScipyIterativeSolver() g2.linear_solver.precon = LinearBlockGS() prob.setup(vector_class=PETScVector, check=False) output = run_model(prob)
# todo implement </s> db = current.db	PayPalAdapter @param pe_id: the subscriber PE ID @returns: the record ID of the newly created subscription s3db = current.s3db sptable = s3db.fin_subscription_plan
# todo: handle name clashing </s> self._skip_hash_check = false	skip_hash_check self._skip_hash_check = True yield
# todo (jack): add script to dump msmarco to ext_host:ext_port </s> proxy.kwargs['ext_host'],	main@25 query)) _2 = time.perf_counter() proxy.kwargs['ext_port'], query))
# todo: configurable timeout </s> self._verify_client(client)	RunSaltAPIHandler @tornado.web.asynchronous def post(self): self.disbatch(client)
# todo wtf is that/?? </s> plot_file(jj)	plot_all def plot_all():
# hack to support saving/loading pytorch models. todo: improve </s> next(context.gen)	Model if hasattr(self, '_layers'): contexts = [layer.use_params(params) for layer in self._layers] yield if backup is not None:
# todo make this not terrible </s> return json_response(look_up_app_json(domain, app_id))	get_app_api @cloudcare_api def get_app_api(request, domain, app_id): except RemoteAppError: raise Http404()
# todo: non-numeric columns should be ignored automatically </s> df = pd.dataframe({'a': np.arange(n), 'b': np.arange(n)**2})	test_iloc4 return df.iloc[[1,4,9]].B.values hpat_func = hpat.jit(test_impl) np.testing.assert_array_equal(hpat_func(df, n), test_impl(df, n))
# todo: add for all fields </s> continue_system_importer_file_csv = false	check_row@64 if not row[model.csv_column_system - 1]:
# todo email submitter </s> class meta:	ReviewableMixin return self.__machine.accept(*args, **kwargs) def reviews_reject(self, *args, **kwargs): abstract = True
# todo: prune_services=false in future release </s> try:	service old_protocols = set(_current_protocols) - set(protocols) for protocol in new_protocols: __salt__['firewalld.add_service_protocol'](name, protocol) except CommandExecutionError as err:
# todo: handle non-normalizable combining chars. probably need to use </s> hintstyle = int(hintstyle)	get_font_information hinting = hinting.lower() == 'true'
# todo in python 2.7 and later, this should be </s> instance of a sqlalchemy model.	DeserializationException pass
# todo: consider tag.blend_clipping_elements. </s> viewport[3] - viewport[1], viewport[2] - viewport[0], values.shape[2]	paste def paste(viewport, bbox, values, background=None): ) view = np.full(shape, background) if background else np.zeros(shape)
# todo: review </s> dict[]: tasks	tasks @property def tasks(self): return self["tasks"]
# todo: remove at some point </s> return self.best_match(name, year)	smart_match def smart_match(self, name): name, year = self.parse_name(name)
# todo: remove this log </s> "rotation_quaternion": "rotation",	__gather_path "location": "translation", "rotation_axis_angle": "rotation", "scale": "scale", "value": "weights"
# todo: ideally we'd know whether this was a folder </s> def check_repo_root(cls, location):	check_repo_root @classmethod
# todo: #154 - some auto-updater logic? </s> return deployer	get_deployer def get_deployer(self): port = self.rest_information()[0].port
# todo alert? </s> raise keyerror(agent_uuid)	agent_name agent_path, agent_name, agent_name + '.dist-info') if os.path.exists(dist_info):
if lang is none:  # todo: remove in v8 </s> path[-1] += extension	_add_extension def _add_extension(self, path, extension):
# todo error on missing levels </s> markers = {}	_LinePlotter if data["style"].isnull().all(): style_levels = [None] else: style_levels = categorical_order(data["style"])
# todo: include regression tests for when tvtk is installed </s> res = dist_ident.run()	test_ident_distances res = dist_ident.run() yield assert_equal, res.outputs.distance, 0.0 yield assert_equal, res.outputs.distance, 0.0 os.chdir(curdir)
# todo: should modify to parallel execution. </s> )	beaver_populate id=id_at_location, data=obj, print("Result", result) node.store[id_at_location] = result  # type: ignore
raise mpdnotimplemented # todo </s> def _findadd(self, type, what):	_findadd @register(r'^findadd "(?P<type>(album|artist|title))" "(?P<what>[^"]+)"$')
# todo: add cn to domains? </s> reporter = zope.component.getutility(interfaces.ireporter)	_recovery_routine_with_msg def _recovery_routine_with_msg(self, success_msg): :param str success_msg: message to show on successful recovery reporter.add_message(success_msg, reporter.HIGH_PRIORITY)
w, h = tiledsurface.n, tiledsurface.n # todo: support for other sizes </s> return	open_last_cb def open_last_cb(self, action): if not self.confirm_destructive_action(): return
# todo: since these a delete request, shouldn't use request body. put pointer </s> return {	_serialize_node_search def _serialize_node_search(node): :param Node node: Node to serialize 'id': node._id, 'title': node.title,
# todo: old requirement, remove in future versions? </s> res = self.app.get(url, follow_redirects=true)	test_01_front_page @with_context def test_01_front_page(self): dom = BeautifulSoup(res.data) err_msg = "Top users should not be shown to anonymous users"
# todo replace with to_bytes() when available in utils.py </s> :param value: the value that is to be encrypted	_encrypt_value base method to encrypt a value - uses one slot id to encrypt a string :param value: byte string :param slot_id: slot of the key array
# todo: retrieve old days count </s> def mod_log(self) -> modlog:	mod_log @property
# todo: if "pkg" in facts["inspections"] then use pkgcopier. </s> robo_print("generating %s recipe..." % recipe["type"])	generate_jss_recipe "the app and try again, using the .app file itself as input." % recipe["type"]) if prefs.get("FollowOfficialJSSRecipesFormat", False) is True: keys["Identifier"] = ("com.github.jss-recipes.jss.%s" %
args.get('thread_config'),  # todo deprecate </s> args.get('cli'), args.get('msi'), args.get('service_principal'), args.get('file_auth'), args.get('tenant_id'),	main@27 run(args.get('provider'), args.get('profile'), args.get('subscription_id'), args.get('client_id'), args.get('client_secret'),
>>> from torch import nn  # todo: import nn for all doctests </s> the function that will be invoked every time you want to validate that a module is compatible	register_module_validator@24 validator_class: type = DEFAULT_MODULE_VALIDATOR, ): for training with Opacus. You may supply your own validator_class that holds the registry of VALIDATORS.
#todo - reconsider this </s> self.tree.bind("<double-button-1>", self.on_double_click, "+")	OutlineFrame self.columnconfigure(0, weight=1) self.rowconfigure(0, weight=1) self.tree.column('#0', anchor=tk.W, stretch=True) self.tree.heading('#0', text='Item (type / line)', anchor=tk.W)
# todo: check if valid cdxj here before returning </s> return get_index_file_contents(path)	get_cached_index_file_contents @lru_cache()
# todo: break out into separate view / template </s> 'can_edit' : node.is_contributor(user) and not node.is_registration,	get_node_permission def get_node_permission(node, user): return {
# todo: figure out way to paramaterize node['osds'] for this test </s> port = "680{}".format(x)	TestOSDs )).is_listening def test_osds_listen_on_cluster_network(self, node, Socket): assert Socket("tcp://{address}:{port}".format( address=node["cluster_address"],
# todo: non-json response contents </s> def result(self, timeout=none):	FutureAdapter This adapter must be implemented by all bravado clients such as FidoClient or RequestsClient to wrap the object returned by their 'request' method. Must implement a result method which blocks on result retrieval. :param timeout: maximum time to wait on result retrieval. Defaults to
# todo: flag to expressionreplacementvisitor to only replace </s> return m	kaug cloneModel=True, tee=False, keepfiles=False, solver_options=None): m = sensitivity_calculation('kaug', instance, paramSubList, perturbList,
## \todo there should really be a method to map from plug to parameter. </s> plugvaluewidget = gafferui.stringplugvaluewidget( parameterhandler.plug() )	StringParameterValueWidget if multiLine : plugValueWidget = GafferUI.MultiLineStringPlugValueWidget( parameterHandler.plug() ) with IECore.IgnoredExceptions( KeyError ) : if parameterHandler.parameter().userData()["UI"]["password"].value :
# todo: should use input_axes here, but the workflow always gives out </s> '*', 'pixelclassification' + testcase)	_test_pixel_classification @timeLogged(logger) def _test_pixel_classification(self, testcase, input_axes): if not os.path.exists(project_file): raise IOError('project file "{}" not found'.format(
#todo: check for continous or discrete, only continuous supported right now </s> ------	hsvd H = hsvd(sys) The Hankel singular values are the singular values of the Hankel operator.  In practice, we compute the square root of the eigenvalues of the matrix formed by taking the product of the observability and controllability gramians.  There are other (more efficient) methods based on solving the Lyapunov equation in a particular way (more details soon). sys : a state space system Outputs
# todo: this is wrong. they must be motions. </s> vi_cmd_data['can_yank'] = true	vi_big_c def vi_big_c(vi_cmd_data): vi_cmd_data['populates_small_delete_register'] = True if vi_cmd_data['count'] == 1:
for node in pynode.gltf.scene.nodes.values(): # todo if parent is in another scene </s> blendernode.set_parent(pynode, obj, parent)	BlenderNode if not (pynode.mesh and pynode.mesh.skin is not None): pynode.set_transforms(obj, parent) pynode.mesh.blender_set_mesh(mesh, obj) for child in pynode.children:
# todo: create a queue for all orders and make it auto-complete when all the orders are processed </s> def learn(self, *args, **kwargs):	learn
# todo: drop me after the domain-allocation switch, as this method </s> data = self.data	_data_buffer @property ctype = numpy_to_ctypes(data.dtype) cpointer = ctypes.cast(int(data.grid.get_raw_storage_buffer()),
# xxx/todo: remove this when sdk 1.4.3 is released </s> search paths and errors.'),	Command help='Prints verbose debugging messages to the console while running.'), make_option('--debug_imports', action='store_true', default=False, make_option('--clear_datastore', action='store_true', default=False, help='Clears the datastore data and history files before starting the web server.'),
# todo(dcramer): we're selecting source twice which is a waste of resources </s> def get_stream_channels(self):	get_stream_channels
# todo: we need a better way to create model instances and stay compatible with </s> return a the count query for the model type	get_count_query A ``query(self.model).count()`` approach produces an excessive subquery, so ``query(func.count('*'))`` should be used instead.
self.end(266) #todo# too long? </s> if not os.path.exists(docker_socket): self.skiptest("docker-based test")	test_6133_run_default_services_from_single_service_saved_container def test_6133_run_default_services_from_single_service_saved_container(self): after it has been restarted from a commit-saved container image. images = IMAGES image = self.local_image(COVERAGE or IMAGE or CENTOS)
# todo: kill this </s> def cache_single(filename):	cache_single
# todo: rewrite tests </s> self._url_to_body(self.component.deep_url),	test_component_url assert_equal(
# todo: update cache </s> args:	ApplicationService if namespaces: self.namespaces = namespaces event(Event): The event to check. Returns:
# todo(samueldmq): change the below to get_head_action for </s> def add_routes(self, mapper):	Public tenant_controller = controllers.TenantAssignment() mapper.connect('/tenants',
# todo: raise warning if computed output is already in cache. </s> elif hasattr(step, 'transform'):	Model step.fit(*Xs, *ys) if hasattr(step, 'predict'): output_data = step.transform(*Xs) else:
# todo replace all that with ssh-copy-id </s> {'title': 'setting up ssh keys',	initSequences def initSequences(controller): 'functions':[installKeys]} ]
# todo: make legacy detection non-reliant on side </s> with open(self.filepath, 'r') as config:	read_config def read_config(self): return config.read() except (OSError, IOError), e:
# todo: estimate fees </s> return hdwalletkey(self.parent_id, session=session)	parent
# todo: exit codes are currently ignored on windows. </s> def lmagic(line):	lmagic @register_line_magic
# todo: use value_op for this type of retrieval instead </s> fargs = itt.product(nin_rng, bsz_rng)	pytest_generate_tests bsz_rng = [32, 64] if 'basic_bnargs' in metafunc.fixturenames: metafunc.parametrize('basic_bnargs', fargs)
'''todo: add docs''' </s> def discrete_columns(self):	discrete_columns return [x for x in self.columns if x['type'] == 'DiscreteColumn']
# todo: exact match. </s> offset = self.page_size * page_number	get_page_buffer def get_page_buffer(self, page_number): if page_number < 1: return self.buf[offset:offset + self.page_size]
limit = 20  # todo: change to setting </s> self.assertequal(comment_md, '<p><img class="comment-emoji" src="%(static)sspirit/emojis/airplane.png">, '	test_markdown_emoji comment = ":airplane:, :8ball: :bademoji: foo:" md = Markdown(escape=True, hard_wrap=True) '<img class="comment-emoji" src="%(static)sspirit/emojis/8ball.png"> ' ':bademoji: foo:</p>' % {'static': settings.STATIC_URL, })
# todo add installation logic for torch </s> metric=data.get('metric'),	experiment_from_dict time_finished=data.get('time_finished'), info=info if any(info) else data.get('info'), pythonver=data.get('pythonver'), max_duration=data.get('max_duration')
# todo: implement subdomains for slate tensors </s> col = 0	eigen_tensor row, col = index except ValueError: rshape = expr.shapes[0][row] rstart = sum(expr.shapes[0][:row])
# todo: in 0.6.0 change this to "disabled": false </s> }]	test_enabled_missing "name": "test-properties", "proto": "udp", }) expected = self._tabs("""package openvpn
# todo: this switch between 64 and 128 is a hack for now. we should have a separate cli option for size </s> help="model directory. a directory containing the trained model \	ConvertImage action=FullPaths, dest="model_dir", you wish to process. Defaults to 'models'") parser.add_argument('-a', '--input-aligned-dir',
# todo(datapipe-1509|abrar): currently we have </s> if value is none:	process_bind_param return None return json.dumps(value, separators=self.separators)
# todo: delete from application_services_regex where id=this service </s> if namespaces:	ApplicationService self.token = token if url: self.namespaces = namespaces def is_interested(self, event):
# todo: fix clone issue </s> pred_proba = self.clf.predict_proba(self.x_test, method='linear')	test_prediction_proba_linear assert (pred_proba.min() >= 0) assert (pred_proba.max() <= 1)
# todo(iceboy): projection. </s> async def on_close(self):	RecordMainConnection@63 rdoc['udoc'], rdoc['pdoc'] = await asyncio.gather( user.get_by_uid(rdoc['uid']), problem.get(rdoc['domain_id'], rdoc['pid'])) bus.unsubscribe(self.on_record_change)
# todo(#362): add server authentication with thrift 0.12. </s> sasl_client.setattr('host', host)	get_transport import sasl  # pylint: disable=import-error def sasl_factory(): sasl_client.setAttr('service', kerberos_service_name) if auth_mechanism.upper() in ['PLAIN', 'LDAP']:
# todo : documentation pending </s> if key.startswith('download_warning'):	get_confirm_token def get_confirm_token(response): return value return None
# todo: checking for executability is a hack; use file extension </s> self._output_dir = os.path.join(	_setup_output_dir def _setup_output_dir(self): self._get_local_tmp_dir(), 'output') if not os.path.isdir(self._output_dir):
# @todo this needs to be using domain fronting to defeat censorship </s> fp = os.path.join(dirpath, f)	dir_size total_size = 0 for dirpath, dirnames, filenames in os.walk(start_path): if not os.path.islink(fp): total_size += os.path.getsize(fp)
"""@todo add progressbar for multisite. ensure the other one is hidden first.""" </s> def capture_sys_output():	capture_sys_output caputure_out, capture_err = StringIO(), StringIO() current_out, current_err = sys.stdout, sys.stderr
# todo: add classname if bound method </s> spec = getfullargspec(function)	get_all_arg_names possible = spec.args + [spec.varargs, spec.varkw] + spec.kwonlyargs all_args = [x for x in possible if x]
# todo check behavior when not loaded </s> def is_available(self):	is_available Will always return :class:`None` if the album isn't loaded. if not self.is_loaded:
# todo: determine actual time left. </s> else:	api_call return True
# todo replace with more general current_expression_attribute </s> matches = current_single_word_re.finditer(line)	current_single_word for m in matches: if m.start(1) <= cursor_offset and m.end(1) >= cursor_offset:
# todo(slaweq): change this to neutron floating ips and turn neutron </s> mock.call(active_server['id']),	test_wait_for_server self.assertEqual(2, mock_get_server.call_count) mock_get_server.assert_has_calls([ ]) self.assertEqual(2, mock_get_active_server.call_count)
# todo: below lines are commented out to ensure that </s> def limit(self, n):	MODMCompatibilityQuerySet return self.order_by(*sort_keys)
ridedatafileset(item=self).publish() #todo: use a more gentle message </s> def _to_os_style(self, path):	_to_os_style
# todo: we should should distinguish sub-subflows here </s> return storagepool.retrieve(parent_name, self.parent[parent_name])	parent_task_result def parent_task_result(self, parent_name): :param parent_name: name of parent task to retrieve result from
# todo: rf to use --batch where possible instead of splitting </s> self.cmd_call_wrapper._log_opts['outputs'] = true	_fake_exception_wrapper def _fake_exception_wrapper(self, options_): with swallow_logs(new_level=logging.ERROR) as cml: json_list = \
# todo(b/160795287): deprecate estimator based executor. </s> 'eval_spec': eval_spec,	trainer_fn return { 'estimator': estimator, 'eval_input_receiver_fn': eval_receiver_fn
total = db.get_count(query=query)  # todo(nsatterl): possible race condition? </s> mimetype = 'application/javascript'	jsonp if callback: data = str(func(*args, **kwargs).data) return current_app.response_class(content, mimetype=mimetype) else:
# todo: add 'downcast' when value parameter exists </s> return dataframe(internal)	set_index if inplace: self._internal = internal
# todo add validation tests </s> else:	validation_wrapper errors = validateLink(obj, *args, **kwargs) elif name == 'object_pose': log('This validation type is not defined!', 'ERROR') errors = []
# todo make this cancellable with is_cancellable_behavior </s> the robot and keep vector still between sdk control instances.  care must be taken when	ReserveBehaviorControl blocking background behaviors, as this may make Vector appear non-responsive. This class is most easily used via a built-in SDK script, and can be called on the command-line
#@todo: move to utils in 0.4.10 </s> return self.set_config(*args, **kwargs)	setConf def setConf(self, *args, **kwargs):
# todo(b/145514490): this is a bit heavy handed, there maybe caches where </s> def type_signature(self):	type_signature @property
# todo: replace with path transformation functions </s> _xl_app.screen_updating.set(false)	autofit_sheet num_rows = sheet.xl_sheet.count(each=kw.row) xl_range = get_range_from_indices(sheet.xl_sheet, 1, 1, num_rows, num_columns) if axis == 'rows' or axis == 'r': sheet.xl_sheet.rows[address].autofit()
assert len(config['sources']) == 1  # todo: merge multiple sources </s> tile_size=[storage['tile_size'][dim] for dim in crs.dimensions],	get_grid_spec storage = config['storage'] crs = CRS(storage['crs']) resolution=[storage['resolution'][dim] for dim in crs.dimensions])
#todo: remove expressions </s> def __init__(self):	WholeStepInvertedMordent InvertedMordent.__init__(self) self.size = music21.interval.Interval("M2")
# todo make this configurable </s> def kick(self, nick, date, message=true, extra_tags=[]):	kick
# todo allow repose (is not affected by createfrompose) </s> if posemats.shape[2] == 4:	setPose bone.matPose[:3,:3] = poseMats[bIdx,:3,:3] invRest = la.inv(bone.matRestGlobal) bone.matPose[:3,3] = poseMats[bIdx,:3,3] self.update()
# todo: this updates the resolution upon initialization and makes the </s> @gtktemplate.callback	ResolutionRow def _on_change_value(self, scale, scroll, value): scale.set_value(int(value - (value % 50))) def _on_delete_button_clicked(self, button): print("TODO: RatbagdProfile needs a way to delete resolutions")
# todo: udpoutgoing style buffer </s> def on_error(self, errcode):	on_error
pass  # todo: why ignore unicodedecodeerror? </s> for attrtag, attrvalue in elt.items():	addReferencedFile def addReferencedFile(docElt, elt): if attrTag in ("href", "src") and ( not localFilesOnly or
# todo: these 2 little simplifications can reduce test time by 30-40%, to do in test framework </s> assert "libb/0.1:{} - cache".format(package_id_arg) in client.out	_assert_recipe_mode client.run("create libb") client.run("create app")
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> "authorityhosturl" : "https://login.windows.net",	Test_AcquireTokenWithUsernamePassword def test_acquire_token_with_user_pass(self): sampleParameters = { "clientId" : "04b07795-8ddb-461a-bbee-02f9e1bf7b46", # xplat's which is supposed to be in every tenant "username" : "crwilcox@microsoft.com",
# todo: adjust dimension order for tf2 broadcasting </s> raises:	sparsemax Returns: Tensor, output of sparsemax transformation. Has the same type and ValueError: In case `dim(logits) == 1`. logits = tf.convert_to_tensor(logits, name="logits")
# :todo: implement test. </s> def test_fail_min_weight_magnitude_float(self):	ReplayBundleRequestFilterTestCase def test_fail_min_weight_magnitude_string(self): ``min_weight_magnitude`` is a string. ``min_weight_magnitude`` is a float. self.skipTest('Not implemented yet.')
# todo: discriminate between worksheet & workbook ranged names </s> return self.app.activeworkbook.worksheets(s).activate()	set_sheet
# todo: fix this in a cleaner way </s> ...                 ('d', factors(['a'], [2], [0.9, 0.1]), '0'),	TreeCPD ...                 ('B', 'C', '1'), ...                 ('C', factors(['A'], [2], [0.1, 0.9]), '0'), ...                 ('D', factors(['A'], [2], [0.4, 0.6]), '1')]) >>> tree.to_rule_cpd()
# todo: does this need to be made more efficient? </s> if children:	py2st else: lineno = 0  # default in Leaf return (typ,) + tuple(children) else:
# todo add </s> new_history.addtohistory(line)	read_history_file for line in f:
# todo configurable </s> "port": host.ssh_port,	paramiko_connection@14 "hostname": host.host, "username": host.username, } user_config = ssh_config.lookup(host.host)
except exception:  # todo - which exceptions? </s> for organism_element in element:	_parse_organismHost if organism_element.tag == NS + 'name': append_to_annotations("organism_host", organism_element.text)
# todo hotfix for unnecessary weights in old adapters </s> for sample in self.search_samples:	test_find_in_index with self.subTest(sample=sample): config = ADAPTER_CONFIG_MAP[sample[1]] if sample[1] else None
# todo: is there a nicer way to do this? if i add a new grep plugin i won't </s> pinst.end()	profile_me for pinst in self._plugins: pinst.grep( request, response )
#todo move this up to not be a nested method </s> def insert_to_legacy_db_feeder_queue(self,ooid,timestamp):	insert_to_legacy_db_feeder_queue
#todo - complete implementation of these apis </s> try :	_items ports = self.network_manager.get_all_ports(tenant_id, network_id) builder = ports_view.get_view_builder(req)
# todo: use nestedbuffers instead of saving by value </s> if directory.secondary_directory_address is not none:	_parse_entry_table for address in [psp_dir_one_addr, psp_dir_two_addr]: directory = Directory(self, address, firmware_type) secondary_directory = Directory(self, directory.secondary_directory_address, 'secondary') self.directories.append(secondary_directory)
# todo: clear last object inspector requests dictionary </s> def to_remote_path(path):	to_remote_path
# todo implement. </s> self.dataset_rdd = repartition(self.num_workers)	SparkModel self.dataset_rdd = rdd self.num_workers = num_workers nb_epoch          = parameters['nb_epoch'] batch_size        = parameters['batch_size']
async_pub['tag'],  # todo: fix </s> return a dict that will mimic the "functions" dict used all over salt.	functions_dict It creates a wrapper around the function allowing **kwargs, and if pub_data is passed in as kwargs, will re-use the JID passed in
# todo: check to make sure time points match </s> 'parameter ``t``: ', squeeze=true,	forced_response raise ValueError('Parameter ``T`` is mandatory for continuous ' 'time systems.') transpose=transpose) n_steps = T.shape[0]            # number of simulation steps
#todo: check the data! </s> count = 0	test_feed TODO: have these tests iterate over a number of test pipelines pipe_def = self._get_pipe_def("testpipe1.json") for i in p: count += 1
# todo assert cls.__tablename__ == '' </s> yield from dbcleanup_wrapper(session, s)	galaxy_session @pytest.fixture def galaxy_session(model, session, user):
# todo: unit tests </s> return {'addons_enabled': user.get_addon_names(),	dashboard user = auth.user dashboard_folder = find_dashboard(user) 'dashboard_id': dashboard_id,
pass # todo </s> frame.set_label_widget(label)	_add_with_frame frame = gtk.Frame() label = gtk.Label() frame.add(widget) self.vbox.add(frame)
# todo: make return values consistent across both *repo classes! </s> file_: str	rm_url def rm_url(self, file_, url): Parameters url: str self._run_annex_command('rmurl', annex_options=[file_] + [url])
# todo: move this into onnx main library </s> node = onnxnode(node)	TensorflowBackend @classmethod def run_node(cls, node, inputs, device='CPU'): device_option = get_device_option(Device(device)) input_tensors = []
# todo: this should be a separate test </s> self.assertequals(exc['type'], 'valueerror')	MiddlewareTestCase event = self.client.events.pop(0) self.assertTrue('sentry.interfaces.Exception' in event) self.assertEquals(exc['value'], 'hello world') self.assertEquals(event['level'], logging.ERROR)
# todo:@zhui add support for 'mean', 'max', 'min' function. </s> nodes), self.adj_src_index.view_eid(nodes)	successor else: if return_eids: else: return self.adj_src_index.view_v(nodes)
# todo(eric ayers) not really part of the test, just to detect the cache poisoning </s> self.assertin('dependencies', jl_text)	BuildsymsSanityTests def test_java_library(self): jl_text = '{0}'.format(self._syms['java_library']['defn']) self.assertIn('sources', jl_text)
# todo: funcbody </s> self.asserttrue(isinstance(node, parser.fieldexp))	testFieldExp node = p._field() self.assertIsNotNone(node) self.assertEqual('foo', node.exp.value.name._data)
# todo: this is a work around for infinite blocking wait in storage </s> collector_process: the collector process object (instance of	_GetCollectorStoragePorts def _GetCollectorStoragePorts(self, collector_process): MultiProcessCollectorProcess). Raises:
# todo: change config values instead of overwriting files on disk </s> summary="test case {0}".format(case_count + 1),	BaseAPIClient_TestCase testcase = tcms_api.TestCase( category=self.category, status=self.CASESTATUS) testcase.tags.add([random.choice(self.tags) for counter in range(10)])
# todo: duplicate + replace with psutil.getloadavg()? (available in 5.6.2) </s> sys.stdout.write("\t\t\t\"auto-cpufreq\" refresh in:{:2d}".format(remaining))	countdown def countdown(s): for remaining in range(s, 0, -1): sys.stdout.flush() time.sleep(1)
# todo move to another thread if we need to process messages </s> getter, setter = self.properties[interface][prop]	Get def Get(self, interface, prop):
# todo: stop ignoring these once we have proper handling for these messages. </s> lambda v: time.time() - v[0] > self.reply_timeout, self.active_requests)	TrieNodeRequestTracker self.missing: Dict[float, List[Hash32]] = {} def get_timed_out(self) -> List[Hash32]: for peer, (_, node_keys) in timed_out.items(): self.logger.debug(
# todo: createpropertyconditionex with propertyconditionflags_ignorecase </s> return conditions[0]	IUIA if len(conditions) > 1: return self.iuia.CreateAndConditionFromArray(conditions) return self.true_condition
#todo: define tests which check db contents </s> suite = unittest.testsuite(suite_list)	test_suite ] suite_list = map(unittest.defaultTestLoader.loadTestsFromTestCase, return suite
# todo(py3.7): add required=true </s> class mec16xxerror(glasgowappleterror):	MEC16xxError
# todo: may test with codecs.open passing an encoding </s> def test_export_to_csv_fobj(self):	PluginCsvTestCase rows.export_to_csv(utils.table, temp.name) table = rows.import_from_csv(temp.name) temp = tempfile.NamedTemporaryFile(delete=False) self.files_to_delete.append(temp.name)
# todo: remove this log once we find out what's causing oom </s> tasks = [	_get_version 'readthedocs.projects.tasks.update_docs_task', 'readthedocs.projects.tasks.sync_repository_task',
# todo: and netcdf writer will be more generic </s> return dateutil.parser.parse(time)	_parse_time if isinstance(time, compat.string_types):
# todo: find these references and ensure they are closed </s> if not is_binary:	blame info['committer'] + ' ' + info['committer_email']), committed_date=info['committer_date']) if line and line[0] == '\t': line = line[1:]
# todo(py3.7): add required=true </s> def test_build(self):	test_build @synthesis_test
# todo generator </s> content = [ap['path'] for ap in content_by_ds[ds_path]	Drop for ds_path in content_by_ds: ds = Dataset(ds_path) if ap.get('type', None) != 'dataset' or ap['path'] == ds.path] if not content:
# todo: summary hash for new current id </s> self.steps = [s.set_hyperparams(self.wrapped.get_hyperparams()) for s in self.steps]	set_hyperparams def set_hyperparams(self, hyperparams: HyperparameterSamples) -> BaseStep: return self
# todo - make this work on loop with more than two links </s> def make_fill_posts(bm, edges, **kwargs):	make_fill_posts
# todo: is region (lla | atn | odn | others?) important? </s> "no_auto_fg": true,	Mqtt "ct": "websocket", "mqtt_sid": "", "gas": None, "pack": [],
# todo: if first epsilon, repeat with smaller epsilons </s> return	_apply def _apply(self, a, epsilons=100, steps=10): image = a.original_image() min_, max_ = a.bounds()
# time.sleep(40)  # todo: should remove after polling get. </s> expected = op(value_1, value_2)	test_mpc_matmul_public@108 res = op(mpc_tensor_1, value_2) res.block_with_timeout(secs=40) assert (res == expected).all()
pass # todo </s> def collect_incoming_data(self, data):	collect_incoming_data
# todo: what actually raises valueerror in the following code? </s> return true	_GzipStreamFile def readable(self): return True
# todo: determine if this is object store safe and what needs to be </s> get check_complete response from the remote server.	raw_check_complete check_complete_response = self.__raw_execute_and_parse("check_complete", {"job_id": self.job_id}) return check_complete_response
# todo(slaweq): change this to neutron floating ips and turn neutron </s> error_server = fakes.fakeserver('1234', '', 'error')	TestRebuildServer Test that a server error while waiting for the server to rebuild raises an exception in rebuild_server. fake_floating_ip = fakes.FakeFloatingIP('1234', 'ippool', '1.1.1.1', '2.2.2.2',
# todo: confirm change made it to elasticserach </s> results = formes().run()	test_xform_pillow_couch FormProcessorInterface(domain=self.domain).save_processed_models([form]) self.pillow.process_changes(since=0, forever=False) self.assertEqual(1, results.total) form_doc = results.hits[0]
# todo: remove this monkeypatch once upstream class is fixed. </s> next_serial = int(strftime("%y%m%d00", localtime(time())))	_increment_serial if next_serial <= self.soa.rdata.serial: next_serial = self.soa.rdata.serial + 1
# :todo: implement test. </s> self.skiptest('not implemented yet.')	FindTransactionsRequestFilterTestCase def test_fail_empty(self): self.skipTest('Not implemented yet.') def test_fail_unexpected_parameters(self): self.skipTest('Not implemented yet.')
# todo: check degree minute second formats? </s> py = decimal.decimal	NumberType name = 'number' formats = ('default', 'currency')
# todo: keep map sorted chronologically </s> return scriptdirectory(	from_options @classmethod options.get_main_option('script_location'), options)
# todo: implement bip45/67/electrum/? </s> network = self.network.network_name	_get_account_defaults :type account_id: int :return: network code, account ID and DbKey instance of account ID key if account_id is None: account_id = self.default_account_id
# todo implement. </s> def train(self, parameters):	SparkModel self.spark_context = sc self.dataset_rdd = rdd self.dataset_rdd = repartition(self.num_workers) nb_epoch          = parameters['nb_epoch']
# todo at_list </s> indent(f, indent_level)	print_atl def print_atl(f, atl_block, indent_level):
#rnn_cell = tf.nn.rnn_cell.dropoutwrapper(rnn_cell, input_keep_prob=1.0, output_keep_prob=1.0)  # todo: custom values (warning: no dropout when testing !!!, possible to use placeholder ?) </s> with tf.name_scope('note_projection'):	Model name='bias', ) return tf.matmul(X, W) + b  # [batch_size, NB_NOTE] rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(self.args.hidden_size, state_is_tuple=True)  # Or GRUCell, LSTMCell(args.hidden_size)
# todo: assert metrics. </s> assert touched_functions == {("outer_func", 1, 6), ("inner_func", 3, 5)}	test_get_touched_functions [], [4],
delattr(self, "runtime_coef") # todo, better way to pass variables from initialiszer </s> self.reuse_scope_count = 0	reuse def reuse(self): self.reuse_context += 1
# todo: https://github.com/turicas/brasil.io/issues/210 </s> result['cidades'][city] = data	format_spreadsheet_rows_as_dict@17 elif city == 'Importados/Indefinidos': result['importados_indefinidos'] = data return result
# todo: then we can pull the descriptor out of the tile_spec </s> storage_units = create_storage_units(datasets, storage_type, executor=executor)	store_datasets for storage_type_id, datasets in storage_types.items(): storage_type = index.storage.types.get(storage_type_id) index.storage.add_many(storage_units)
# todo add locales </s> raise yunohosterror("main_domain_change_failed")	domain_main_domain _set_hostname(new_main_domain) except Exception as e: app_ssowatconf() if os.path.exists("/etc/yunohost/installed"):
# todo: large gains also expected when precalculating psi. </s> def should_continue(f, *args):	should_continue
# todo legacy method to be removed/refactored </s> from corehq.apps.orgs.models import organization	get_organizations def get_organizations(self):
# resume normal sphinx.ext.autodoc operation </s> return super(functiondocumenter, self).format_name()	SaltFunctionDocumenter if not self.objpath: return super(FunctionDocumenter, self).format_name() return self.module.__func_alias__.get(self.objpath[0], self.objpath[0])
# :todo: implement test. </s> 'tips': [	GetInclusionStatesRequestFilterTestCase TransactionId(self.trytes1), TransactionId(self.trytes2), TransactionId(self.trytes1), TransactionId(self.trytes2),
# todo[k]: fix this properly. </s> foreignkey(namespace.id, ondelete='cascade'),	Transaction class Transaction(MailSyncBase, HasPublicID): nullable=False) namespace = relationship(Namespace)
# todo (fpliger):   this handles pandas api change so users do not experience </s> a dict mapping between dimension and value for x_max, y_max, x_min, y_min	get_dim_extents def get_dim_extents(self): return {'x_max': max([renderer.x_max for renderer in self.comp_glyphs]), 'y_max': max([renderer.y_max for renderer in self.comp_glyphs]),
# todo remove me (das2 experiment) </s> self._communities[cid] = trackercommunity.join_community(dummymember(cid), self._my_member)	get_community try: return super(TrackerDispersy, self).get_community(cid, True, True) return self._communities[cid]
# todo: is that right? </s> es.index(doc, index, doc_type=document._meta.db_table,	index_doc es.index(doc, index, doc_type=Document._meta.db_table, id=doc['id'], bulk=bulk, force_insert=force_insert) id=doc['id'], bulk=bulk, force_insert=force_insert)
# :todo: implement test. </s> def test_fail_transaction_not_trytes(self):	ReplayBundleRequestFilterTestCase def test_fail_transaction_wrong_type(self): ``transaction`` is not a TrytesCompatible value. ``transaction`` contains invalid characters. self.skipTest('Not implemented yet.')
#@todo: move to utils in 0.4.10 </s> return self._log("debug", args)	log_debug def log_debug(self, *args):
# todo change to native framework call, when plex allows token in header </s> log.critical('missing section key in parameters')	viewstate 'Unknown error digesting the specified section was: %s' % str(e)) return req.set_status(412) req.finish('Missing section parameter')
# todo: just access the original event position, rather </s> self._scene_transform = tr	_set_scene_transform Called by subclasses to configure the viewbox scene transform.
# todo fix. </s> x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init)	test_cmova def test_cmova(self): asm = ["cmova eax, ebx"] cmp_result = self.__compare_contexts(ctx_init, x86_ctx_out, reil_ctx_out) if not cmp_result:
# todo: change logic to support "--submit" </s> else:	_HandleReservedIPs return []
# todo/fixme: are these correct.. </s> :math:`\boldsymbol{\mu}^t\mathbf{v}\boldsymbol{\mu}`	gaussian_logpdf yVmu : ndarray or double :math:`\mathbf{y}^T\mathbf{V}\boldsymbol{\mu}` logdet_V : ndarray or double Log-determinant of the precision matrix, :math:`\log|\mathbf{V}|`.
# todo: duplicate + replace with psutil.getloadavg()? (available in 5.6.2) </s> sys.stdout.write("\t\t\t\"auto-cpufreq\" refresh in:{:2d}".format(remaining))	countdown def countdown(s): for remaining in range(s, 0, -1): sys.stdout.flush() time.sleep(1)
# todo featureparams nameids </s> assert lst	mergeLangSyses assert all(l.ReqFeatureIndex == 0xFFFF for l in lst) self = otTables.LangSys()
# todo: revert this. </s> ...               obstime="2011/01/05t00:00:50", frame="heliocentric")	Heliocentric >>> import sunpy.coordinates >>> import astropy.units as u >>> sc <SkyCoord (Heliocentric: obstime=2011-01-05 00:00:50, observer=<HeliographicStonyhurst Coordinate (obstime=2011-01-05 00:00:50): (lon, lat, radius) in (deg, deg, AU)
start_time_system_s = none  # todo </s> return info_csv["capture software"] == "pupil mobile" and "data format version" not in info_csv	is_pupil_mobile_recording def is_pupil_mobile_recording(rec_dir: str) -> bool: info_csv = utils.read_info_csv_file(rec_dir) except KeyError: return False
"tabsize": 4,  # todo: fetch these from the project settings / global settings </s> sel.add(current_completion.region)	CompletionSnippetHandler try: sel = view.sel() view.run_command("insert_snippet", {"contents": insertText}) except Exception as err:
# todo: account for distance from mid </s> returns future midpoints for dataframe of book data	get_future_mid def future(timestamp): i = books.index.get_loc(timestamp+offset, method='nearest')
# todo only return results within uri roots given by ``uris`` </s> except keyerror:	LocalLibraryProvider def lookup(self, uri): try: logger.debug('Failed to lookup %r', uri) return []
# todo: cannot be loaded with plugins; improve this solution </s> field = field(name=name, type=type)	__read_convert_schema if name is not None: dtype = dataframe.index.get_level_values(index).dtype field.required = True schema.fields.append(field)
# todo: uncomment when adding support for literal hex bytes </s> except attributeerror as err:	test_setattr x.attr = 42
# todo: rewrite using six.b() </s> def of_basic_type(cls, val):	of_basic_type Returns true if val is of basic type :param val:
uploader.upload_file(file, container='export') # todo: right container folder?! </s> for a in apps:	warm_cache stats.get_stats(id, app.config.get('GEO')) apps_cached.append(id) warm_app(a['id'], a['short_name']) for page in pages:
# todo: avoid dummy and generate func here when inlining is possible </s> return lambda df: 0	df_len_overload if len(df.columns) == 0:  # empty df
return skiptest("test doesn't pass yet")  # todo(frostig) </s> self.assertallclose(ans, expected, check_dtypes=false)	testTransposeAndAddRank3 x = onp.reshape(onp.arange(8., dtype=onp.float32), (2, 2, 2)) expected = fun(x)
# todo better check would be if the node is linked to the output and actually used </s> for obj in depsgraph.objects:	get_obj_count_estimate def get_obj_count_estimate(depsgraph): try: for psys in obj.particle_systems:
# todo: non-numeric columns should be ignored automatically </s> df = pd.dataframe({'a': np.arange(n), 'b': np.arange(n)**2})	test_iloc4 return df.iloc[[1,4,9]].B.values hpat_func = hpat.jit(test_impl) np.testing.assert_array_equal(hpat_func(df, n), test_impl(df, n))
description = ''  # todo(wking): store descriptions </s> else:	_handler_repository_deleted self.pop(key) except KeyError: self.save()
# todo document </s> this is the distance from the subsystem's constellation to the null	conceptual_information concept.""" return constellation_distance(subsystem.constellation(), ())
).consume()  # todo see issue 170 </s> paginator = client.get_paginator('describe_load_balancers')	get_loadbalancer_data def get_loadbalancer_data(boto3_session, region): elbs = [] for page in paginator.paginate():
# todo scope the cleanup to the current project - https://github.com/lyft/cartography/issues/381 </s> bucket.time_created = {timecreated},	load_gcp_buckets bucket.location_type = {LocationType}, bucket.meta_generation = {MetaGeneration}, bucket.retention_period = {RetentionPeriod}, bucket.iam_config_bucket_policy_only = {IamConfigBucketPolicyOnly},
# todo: error handling </s> self.__backend.remove(triple)	__isub__ def __isub__(self, other): return self
# todo(kpy): this only works for subdomains that have a single fixed </s> self.redirect('/view', id=self.params.id, query=self.params.query)	View if self.params.subscribe: return self.redirect('/subscribe', id=person.record_id,
# todo: one day this can be removed (once all our users have updated) </s> old_data = utils.get_persistent_data(old_path)	migrate_symlinks old_path = os.path.join(G.COLAB_DIR, 'persistent.json') if not os.path.exists(old_path): data['workspaces'] = get_legacy_projects() data['recent_workspaces'] = old_data.get('recent_workspaces')
# todo new message here </s> factory.setdeliversmcallback(self.deliver_sm)	startWorker factory.setDisconnectCallback(self.esme_disconnected) factory.setSubmitSMRespCallback(self.submit_sm_resp) factory.setSendFailureCallback(self.send_failure) log.msg(factory.defaults)
# todo: xxx </s> test:	TestInputRSS class TestInputRSS(FlexGetBase): __yaml__ = """ rss: url: tests/rss.xml
# todo(mattjj): if we instead lower directly to lax.gather, we can probably </s> def sqrt(x):	sqrt x, = _promote_to_result_dtype(onp.sqrt, x) return power(x, _constant_like(x, 0.5))
# todo(nmigen-0.2): remove this </s> .format(field, rec_name(self), rec_name(subord)))	Record if field not in subord.fields: raise AttributeError("Cannot connect field '{}' of {} to subordinate {} " subord_items.append(subord.fields[field]) if isinstance(shape, Layout):
# todo this should be a return and printed elsewhere </s> plt.show()	plot_random_forest_feature_importance print('\nFeature importances saved in: {}'.format(source_path)) plt.close(figure)
# todo(tianjianlu): fails on a100 gpu. </s> return np.linalg.norm(actual - expected) / np.linalg.norm(expected)	_compute_relative_diff
# todo: remove in v1.2 </s> def fit(self, x, y):	KNeighborsClassifier n_jobs=n_jobs, ) Parameters ----------
# todo: worry about concurrency </s> with open(path, "r+b") as file:	create_dummy@36 self.api.upload(file, path) os.remove(path)
# todo(rbharath): there should be some automatic check to ensure that all </s> n_tasks = len(tasks)	test_multitask_data tasks = ["task0", "task1"] n_samples = 100 ids = np.array(["C"] * n_samples, dtype=object) X = np.random.rand(n_samples, n_features)
# todo ... </s> while len(found) < limit:	find_special_numbers def find_special_numbers(special_selector, limit=10): found = [] if special_selector(n): found.append(n)
# todo: change logic to c_leq based on benchmarking </s> var_values = list(v.value for v in fix_nlp.mindtpy_utils.variable_list)	handle_NLP_subproblem_other_termination if termination_condition is tc.maxIterations: config.logger.info( if config.add_integer_cuts: add_int_cut(var_values, solve_data, config)
# todo(ochang): remove this once migrated to python 3. </s> if platform.system() != 'windows':	test_worker_init signal.signal(signal.SIGINT, signal.SIG_IGN)
# todo handle algorithm </s> pass	list_permissions
# todo: reproduce and submit traceback to issue 41 </s> return the ip address (can be v4 or v6) of the client requesting this view.	MyIpView :param request: django request object :return: HttpResponse object
# todo: add location info </s> return 2  # bash gives 1 for invalid option; 2 is better	Shopt for name in opt_names: index = match.MatchOption(name) if not self.exec_opts.opt_array[index]: return 1  # at least one option is not true
# todo: a lousy way of propagating what will usually be </s> selinuxmode.enforcing: "enforcing",	ConfigureSELinuxTask SELINUX_CONFIG_PATH = "etc/selinux/config" SELINUX_STATES = { SELinuxMode.PERMISSIVE: "permissive" }
# todo model? </s> :param threshold: float, 0-1, default to 0.95. decided whether a range is stable. larger => more unstable ranges	one_step one step => cut, classifier, draw :param video_path: your video path :param frame_count: default to 5, and finally you will get 5 frames for each range :param compress_rate: before_pic * compress_rate = after_pic. default to 0.2
# todo(b/160795287): deprecate estimator based executor. </s> source = path_fn(working_dir)	copy_model path_fn = eval_model_path else: io_utils.copy_dir(source, dest) absl.logging.info('%s model copied to: %s.', tag.capitalize(), dest)
# todo(guillermooo): remove this by 1.0 </s> v.set_read_only(true)	open_default v.set_name('Dart Settings - Default (read-only)') v.set_syntax_file('Packages/JavaScript/JSON.tmLanguage')
# todo(stevemar): assert returned fields </s> items = self.parse_listing(raw_output)	test_object_create def test_object_create(self): raw_output = self.openstack('object create ' + self.CONTAINER_NAME self.assert_show_fields(items, OBJECT_FIELDS)
# todo: set optimum flags for platform+compiler combo, see </s> if version < version('2016.05.003'):	url_for_version def url_for_version(self, version): t = 'http://elpa.mpcdf.mpg.de/elpa-{0}.tar.gz' return t.format(str(version))
#todo: check this with robot </s> fids_raw = np.zeros([3, 3])	object_registration coords = coords_aux[mask] fids_dyn = np.zeros([4, 6]) for ic in range(0, 3): fids_raw[ic, :] = dco.dynamic_reference_m2(coords[ic, :], coords[3, :])[:3]
# todo: can be done faster by custom code </s> if i < self.line:	getMatchingText upstr = util.upper(ls[i].text) if upstr.startswith(text) and i != self.line: last = upstr for i in tcfg.autoCompList:
# todo(jk0): this will eventually need to take ssl into consideration </s> images.append(base_image_meta)	detail for image_meta in image_metas: if self._is_image_available(context, image_meta): return images
# todo: list is incomplete, to be completed for missing languages. </s> 'kn',	Family 'got', 'gn', 'ks', 'lb',
# todo: use optparse command options instead. </s> return module_names	_get_module_names parts.insert(0, package_name) module = ".".join(parts)
log_importance_weight = none  # todo: check the reason/behavior for this </s> _current_trace_replaced_variable_proposal_distributions = {}	_begin_trace _current_trace_execution_start = time.time() _current_trace = Trace()
raise exceptions.mpdnotimplemented  # todo </s> for remote songs. this change is volatile: it may be overwritten by	addtagid@447 *musicpd.org, current playlist section:* ``addtagid {SONGID} {TAG} {VALUE}`` tags received from the server, and the data is gone when the song gets removed from the queue.
raise mpdnotimplemented # todo </s> result = self._find(type, what)	_findadd @register(r'^findadd "(?P<type>(album|artist|title))" "(?P<what>[^"]+)"$')
# todo: requires special treatment? </s> cls._building_line_to_game_entity(building_line)	_process_game_entities for unit_line in full_data_set.unit_lines.values(): cls._unit_line_to_game_entity(unit_line)
# todo: verify this is a windows image </s> guid = (16 * '{:02x}').format(g0, g1, g2, g3, g4, g5, g6, g7, g8, g9, ga, gb, gc, gd, ge, gf)	PdbSigantureScanner if pdb_name in self._pdb_names: (g3, g2, g1, g0, g5, g4, g7, g6, g8, g9, ga, gb, gc, gd, ge, gf, a) = \ yield (GUID, a, pdb_name, data_offset + sig) sig = data.find(b"RSDS", sig + 1)
# todo: move instead of copy to save time? </s> file = os.path.abspath( file )	BaseJobRunner Return true, if the common prefix of both is equal to directory e.g. /a/b/c/d.rst and directory is /a/b, the common prefix is /a/b return os.path.commonprefix( [ file, directory ] ) == directory commands = job_wrapper.get_command_line()
# todo: remove when the time is right. </s> *args, **kwargs	generate_policy_parameters n: int = None, duration_periods: int = None, ) -> dict: Construct policy creation from parameters or overrides.
# todo: test for the _correct_ revision_id value. </s> assert detail.object_type == "resource", \	test_create_package if detail.object_id == package_created['id']: assert detail.object_type == "Package", str(detail.object_type) str(detail.object_type) elif detail.object_id == package_created['resources'][1]['id']:
# todo: need support mint and other distro based on ubuntu. </s> return cls.is_hardy() or cls.is_intrepid() or cls.is_jaunty()	is_supported_ubuntu @classmethod
# todo: currently mnn python binding have mem leak when creating mnn.tensor </s> else:	handle_prediction if len(anchors) == 5: assert len(prediction) == 1, 'invalid YOLOv2 prediction number.' if v5_decode: boxes, classes, scores = yolo5_postprocess_np(prediction, image_shape, anchors, len(class_names), model_input_shape, elim_grid_sense=True) #enable "elim_grid_sense" by default
#todo support host caches on multiple datastores </s> log.trace('updating dvs \'{0}\''.format(dvs_name))	update_dvs The updated config spec (vim.VMwareDVSConfigSpec) to be applied to the DVS. try: task = dvs_ref.ReconfigureDvs_Task(dvs_config_spec)
# todo: encode / escape key </s> upload_url = result['uploadurl']	ex_get_upload_url result = self.ex_get_upload_data(container_id=container_id)
pass  # todo - should this do something </s> change is a variable, if is false doesn't change anything, if is true show reconfigure window confirmation	on_acceptmgmtinterface_clicked def on_acceptmgmtinterface_clicked(self, widget, data=None): listmgmtinterfaces = self.builder.get_object("listmgmtinterfaces") treemgmtinterfaces = self.builder.get_object("treemgmtinterfaces")
# todo: test jacobian </s> mod = tradedfactormodel(data.portfolios, data.factors)	test_linear_model_time_series_kernel_smoke def test_linear_model_time_series_kernel_smoke(data):
# @todo: this has a chance to spam the user with notifications </s> def content(self):	content return { 'event': self.event,
# todo: log the reason? </s> self._delayedretrydeferred.cancel()	stopTrying Put a stop to any attempt to reconnect in progress. self.continueTrying = 0 self._delayedRetryDeferred = None if self._connectingDeferred is not None:
# todo: implement me </s> self.logmessage("%s %s" % (msg.__class__, vars(msg)), 4)	UserJoinedRoom def UserJoinedRoom(self, msg): if self.chatrooms is not None:
# todo: support aa </s> class activations:	Activations lh: Array po: Array
# todo: replace with something efficient </s> only a getter, no setter	meta @property def meta(self): return self._meta
# todo: seems weird to deal with here. implement this by registring some handler? </s> self._known_mirrored_classes.add(cls)	register_mirrored break if cls2 not in self._known_mirrored_classes: print('Dynamically defining class!', cls) js = cls.get_js()
# todo: remove mocking when storedfilenode is implemented </s> self._set_up_private_project_with_wiki_page()	_set_up_registration_with_wiki_page self.registration = RegistrationFactory(project=self.private_project, user=self.user) self.registration_wiki_id = self.registration.wiki_pages_versions['home'][0]
# todo use get_site_base_path </s> add_to_patch_log(tb)	execute_patch log(tb) import os block_user(False) if success:
# todo: for dev, store hash of .cpp and .h files on extension build inside version_dev, then when </s> focused = false	open_sim hwnds = [] win32gui.EnumWindows(callback, hwnds) while not focused: time.sleep(1)
# todo: pytest mark.parametrize once nose removed. </s> s = f.format("${foo}", foo="home")	test_dollar_formatter assert s == "12 $HOME"
# todo(yanase): check values </s> assert len(sigma) == 1	TestParzenEstimator weights_func=default_weights) assert len(weights) == 1 weights, mus, sigma = ParzenEstimator._calculate([-0.4, 0.4], -1., 1., prior_weight=1., consider_prior=consider_prior,
time.sleep(1)  # todo: avoid race conditions in other way </s> def test_csr_no_sans(self):	test_csr_no_sans
# todo(b/171936854): move all methods to non-experimental api. </s> return optimizer	configure_optimizer@28 if use_graph_rewrite: optimizer = tf.train.experimental.enable_mixed_precision_graph_rewrite(
# todo: this is all just debugging stuff and can be removed </s> raise valueerror("axis must be an integer")	right_rotate_covariance ndim is the number of axes for the Gaussian variable. For vector variable, ndim=1 and covariance is a matrix. if axis < -ndim or axis >= ndim: raise ValueError("Axis out of range")
# todo remove this sleep to reproduce http://tracker.ceph.com/issues/8107 </s> self.assertequal(pool, none)	_assert_visible self.assertNotEqual(pool, None) return pool
# todo: reenable </s> cmd = son([("getlasterror", 1)])	__last_error cmd.update(args) splitns = namespace.split('.', 1)
# todo(user): remove after 184 is out. </s> args:	RecordsPool flush_size_chars=_FILES_API_FLUSH_SIZE, ctx=None, filename: file name to write data to as string. flush_size_chars: buffer flush threshold as int.
# passamos por todos os lugares e nao atingimos o objetivo </s> def __lt__(self, other):	__lt__
cursor.execute("""select * from todo </s> priority = ", priority: %s" % pri	todo if pri == 0: priority = "" added_time = time.strftime(conf.humanTimestampFormat, time.localtime(int(added_at)))
# todo assert content of the template by matching expected template </s> super().__init__(scope, construct_id, **kwargs)	DummyStack class DummyStack(core.Stack): HeadNodeConstruct(self, "HeadNode", head_node)
# todo: unit test! </s> index: tuple of (node1_type, edge_type, node2_type)	edge_index def edge_index(self, edge_type): Return edge type index from the type tuple Returns: Numerical edge type index
# todo: this is not thread-safe! </s> reorg_cursor.execute('''select * from blocks where block_index=?''', (block_index,))	reorg reorg_necessary = False for block_index in range(last_block_index - 10, last_block_index + 1): block_hash_have = reorg_cursor.fetchall()[0]['block_hash'] if block_hash_see != block_hash_have:
# todo: is this a duplicate? </s> ----------	blend def blend(dataset, set_proba, **kwargs): Randomized blending of datasets in data according to parameters in conf set_probe : WRITEME Returns
# todo document </s> this is the distance from the subsystem's constellation to the null	conceptual_information concept.""" return constellation_distance(subsystem.constellation(), ())
# todo when would we use a replay memory without next-states? </s> scope="replay-memory",	ReplayMemory self, capacity=1000, next_states=True ):
# todo: remove the dirty variable once #2004 is pushed </s> return (event_key, event_value, dirty)	_ParseItem else: event_string = event_data.decode('utf-8')
loop=asyncio.new_event_loop(),  # todo: this doesn't work without this </s> self.web_client = webclient(	TestRTMClient_Issue_605 t.start() self.assertFalse(self.called) token=self.bot_token, run_async=False,
# todo: can we just remove the leading spaces from the </s> which then redirect to the main page.	back_to_main returns an HttpResponseRedirect back to the main page for the App Manager app with the correct GET parameters. page = None params = {}
# todo username </s> 'ceph-disk-prepare',	prepare_disk import subprocess subprocess.check_call( '--', disk,
# todo: auxiliary_vars </s> total_loss = tf.math.reduce_sum(losses)	Model def train_step(inputs, targets, auxiliary_vars): with tf.GradientTape() as tape: trainable_variables = ( self.net.trainable_variables + self.external_trainable_variables
# todo: use triple factory </s> self.assertisnotnone(trans_h)	test_trans_h def test_trans_h(self):
# todo: raise specific exception? </s> return wrapper	Site ap = self.access_points[access_point] return getattr(ap, method_name)(*args, **kwargs) open = deleguate_to_acces_point('open', True) search = deleguate_to_acces_point('search', True)
# todo: replace the pickle here with something else </s> results = []	foreach def foreach(layer): backprops = [] for X in Xs:
#todo todo todo todo todo todo todo todo todo </s> sizespec = univ.sequenceof.sizespec + constraint.valuesizeconstraint(1, 1024)	GeneralNames class GeneralNames(univ.SequenceOf):
# todo: check this, reconstruct might not work </s> (v(tmp[idx + 1]) - v(tmp[idx - 1]))	_ctr_fun if idx == 0:  # Needed since '-1' is considered a valid index in Python raise IndexError("list index out of range")
# todo: handle index/keyerror here when we overrun a segment </s> return self.buf[offset:offset + self.page_size]	get_page_buffer if page_number < 1: logger.warning('unexpected page number requested: %d', page_number)
# todo(vek): need to pass context in for access to auth_token </s> def __init__(self, name, state):	InstanceInfo self.name = name assert state in power_state.valid_states(), "Bad state: %s" % state
# todo: currently job processes are not maintained. perhaps it is better / safer approach, </s> client = self.clients[_job.compute_id]	job_request error = None try: compute = client.compute assert compute.scheduler_ip_addr == self.scheduler['ip_addr']
# todo(b/145514490): this is a bit heavy handed, there maybe caches where </s> return self._type_spec	type_signature @property
# todo(b/185726968): replace with shared v1 test_util. </s> model = keras.model(inputs=[i1, i2], outputs=outputs)	testStripPruningFunctionalModel x1 = layers.Dense(10)(i1) x2 = layers.Dense(10)(i2) pruned_model = prune.prune_low_magnitude(model, **self.params) stripped_model = prune.strip_pruning(pruned_model)
# todo refactor this </s> scoring='roc_auc',	clfreport clf = GridSearchCV(algo, param, n_jobs=cores) else:
# todo: unused now, but will be necessary to compute the adjoint </s> indices1 = tuple(sympy.symbols('%s1' % d) for d in self.grid.dimensions)	_interpolation_coeffs Returns ------- indices2 = tuple(sympy.symbols('%s2' % d) for d in self.grid.dimensions) indices = list(powerset(indices1))
# todo: we should slice and input that to the model. </s> self.categorical_speakers = load_pickle(os.path.join(self.output_dir, 'categorical_speakers.pkl'))	load_from_disk if not os.path.exists(file): return None self.kx_train = load_npy(os.path.join(self.output_dir, 'kx_train.npy')) self.kx_test = load_npy(os.path.join(self.output_dir, 'kx_test.npy'))
# todo(leonidbeynenson): think on _get_extra_compress_args </s> 'update_config': update_args,	convert_test_args template_folder = os.path.dirname(model_template_path) converted_args = { } converted_args.update(self.__map_args(args, self.test_out_args_map))
# todo: check that this works using lvm on luks </s> else:	printk f.write("4")
# todo: webext instrumentation doesn't support req_call_stack yet. </s> post_body = self.get_post_request_body_from_db(db)	test_record_post_data_x_www_form_urlencoded def test_record_post_data_x_www_form_urlencoded(self): encoding_type = "application/x-www-form-urlencoded" assert post_body == self.post_data_multiline
# todo: if the arrays could be drawn as shorts istead of floats, it </s> def title(self):	title
# todo: renable when regions are sorted out. </s> log.info('preview deleted: %s' % obj.pk)	obj_delete if not AppOwnerAuthorization().is_authorized(request, object=obj.addon): return super(PreviewResource, self).obj_delete(request, **kwargs)
raise tipgusnotfound # todo right error </s> complete_tip_dict = wb_tip_dict	whistleblower_get_single store.close() raise TipReceiptNotFound complete_tip_dict.update({ 'id' : requested_t.receipt })
#todo migrate to remove this hack </s> return redirect('/account')	auth_registerbeta
# todo(b/178225158): deprecate in favor of the reporting libray when ready. </s> return train_step.numpy() <= prev_train_step_value	is_train_step_the_same_or_behind def is_train_step_the_same_or_behind():
# todo: check that the performance measure is within some range </s> reported on the website, or other).	TestBaselines class TestBaselines(unittest.TestCase): Tests that the baselines in the benchmarks folder are running and def test_bottleneck0(self): Tests flow/benchmark/baselines/bottleneck0.py
## todo: # fixme: remove me </s> return self.get_all_pastes_domain(paste_parent)	get_last_crawled_pastes def get_last_crawled_pastes(self):
# todo: log exception </s> if result.endswith(' '):	scan@71 virusresults = re.findall("(?:\([^\)]*\) )?([^\s]+) (.+)\s+$", output, re.MULTILINE) results = [] result = result[:-1] result = result.split(' ')
# todo: kill this </s> def cache_single(filename):	cache_single
# todo: remove logging </s> user,	handle_spam refused_email, ) ALERT_SPAM_EMAIL, mailbox.email,
# todo: when repo.save_trial(trial) </s> def __init__(self):	SomeObserver self.events: List[Trial] = [] def on_next(self, value: Tuple[HyperparamsRepository, Trial]):
# todo(brett.cannon) implement </s> return []	mock_implicit_hooks
# todo: remove the following line when issue #71 (preserve the trajdataframe index during preprocessing operations) is solved. </s> if leaving_time:	_stops_trajectory minutes_for_a_stop, leaving_time, no_data_for_minutes, min_speed_kmh) stops = nparray_to_trajdataframe(stops, utils.get_columns(tdf), {}) stops.loc[:, 'leaving_datetime'] = leaving_times return stops
# todo: skips header parsing </s> return model	read_abaqus def read_abaqus(abaqus_inp_filename, log=None, debug=False): model = Abaqus()
# todo find out if this is good because of sparcity... </s> 'handles_nominal_values': false,	SupportVectorRegression return {'shortname': 'SVR', 'name': 'Support Vector Regression', 'handles_numerical_features': True, 'prefers_data_scaled': True,
ret = type(spec)() # todo: works for dict + ordereddict, but sufficient for all? </s> >>> glom(target, path('a', 2))	Path Use this to wrap ints, datetimes, and other valid keys, as well as strings with dots that shouldn't be expanded. 3 >>> glom(target, Path('a', 'd.e'))
# todo: should this move to case.rebuild? </s> pass	reset_state try: delattr(case, k) except AttributeError: logging.error(
# todo this should depend on the error (even more granularity) </s> self.storage.put('lightning_payments', self.payments)	delete_payment return
# todo force-exit taskgroup, to clean-up </s> for chan in self.channels.values()))/1000 if self.channels else 0	num_sats_can_receive def num_sats_can_receive(self) -> Union[Decimal, int]: with self.lock:
# todo: needs further implementation </s> return self.name	rendered_report_title @cached_property
# todo: update db, add new tx to db + update spend utxo's </s> self._dbwallet.balance = self._balance	updatebalance_from_serviceprovider self._balance = Service(network=self.network.network_name).getbalance(self.addresslist(account_id=account_id))
# todo: use r.op.span_id to print error with location </s> log('jobstate whendone %d', pid)	WhenDone
""" todo: better test here """ </s> def special_test_function(params, realm=none):	test_filter_params def test_filter_params(self): return 'OAuth ' + ','.join(['='.join([k, v]) for k, v in params]) self.assertEqual(special_test_function.__doc__, " I am a special test function ")
# todo: don't compute this realtime, store it in db </s> def main_user_changed_email(self):	main_user_changed_email
# weird problems happen in the parallel run -- todo - figure it out </s> def annex_worktree_size(self):	annex_worktree_size info = self.info return info['size of annexed files in working tree'] if info else None
# todo: expect_match should work with emit() </s> def options(self):	options @property
# todo round to f2dot14? </s> print("removing gx tables")	main@38 if not scalar: continue coordinates += GlyphCoordinates(var.coordinates) * scalar for tag in ('fvar','avar','gvar'): if tag in varfont:
# todo: fix this </s> _tag("cs", "getctag")]	propfind _tag("D", "owner"), _tag("D", "getetag"), multistatus = ET.Element(_tag("D", "multistatus")) collections = []
# todo use the faster method </s> super(command, self).handle_noargs(**options)	handle_noargs def handle_noargs(self, **options):
# todo: add option to preserve original key names </s> export_fields = _get_export_fields(table.field_names, fields_exclude)	schema max_rows=samples, **input_options, if export_fields is None: export_fields = table.field_names
marked[id(atom)] = atom # since marked means "it's been appended to the todo list" </s> one bond and have no other bonds.	ops_connected_Mixin def selectDoubly(self): selected atom through two or more non-overlapping sequences of if not self.selatoms: self.w.history.message(redmsg("Select Doubly: No atom(s) selected."))
# todo: consider a better home for this code </s> opcode_intercept.make_registrations()	_make_registrations@38 mathlib.make_registrations() randomlib.make_registrations() try: import icontract
# todo: remove args after modifying all dependent files </s> def set_model_params(self, model_parameters):	set_model_params pass
# todo: sorting the batch will result in various local metrics being broadcasted </s> default=48,	Seq2seqAgent agent.add_argument( '-attl', type=int, help='Length of local attention.',
(status, output) = commands.getstatusoutput(command) # todo: replace with subprocess call! </s> return new_task(instance_name, cls, self, **kwargs)	new_task
# todo: notify something here. </s> var = tkinter.stringvar()	create_variable elif type_from_name == 'double': var = tkinter.DoubleVar() else: var = vtype()
self.assertequal(end, 1) ## todo real = 0 </s> self.assertequal(end, 0)	test_2027_show_unit_for_oneshot_service cmd = "{systemctl} --all show a.service" out, end = output2(cmd.format(**locals())) self.assertTrue(greps(out, r"^Id=")) self.assertTrue(greps(out, r"^Names="))
# todo: the following dtypes are not currently supported. </s> index = ['row' + str(i) for i in range(1, a.shape[0] + 1)]	test_b64 if self.should_skip: return self.skip('pandas is not importable') columns = ['Col' + str(i) for i in range(1, a.shape[1] + 1)] df = pd.DataFrame(a, index=index, columns=columns)
# todo: test accessibility of {training_,}confusion{,s} of </s> data = getmvpattern(10)	testHarvesting transerror = TransferError(clfs['linear'][0]) cv = CrossValidatedTransferError(
# todo: this is a jump. </s> return vi_cmd_data	vi_big_m vi_cmd_data['is_jump'] = True vi_cmd_data['motion']['command'] = 'vi_big_m'
#todo - merge this with _write_multi_line method? </s> self._write_multi_line("source", \	GenBankWriter assert len(segment)==1, segment segment = segment[0] self._get_annotation_str(record, "source")) org = self._get_annotation_str(record, "organism")
# todo - verify contents </s> self.assertequal(response.status_code, 301)	testReviewDetail0 def testReviewDetail0(self):
# @todo: replace with a link to popup a datatable of the list of updates </s> args=[record_id, "organisation", organisation_id, "profile"],	org_name organisation_id = row["event_organisation.organisation_id"] return A(org_represent(organisation_id),
# todo: check if this is correct </s> kind = 0x100	Operand LITERAL = 1 RAW = 2
# todo: other types that can have series inside? </s> def resolve_argsort(self, ary, args, kws):	resolve_argsort resolver = ArrayAttribute.resolve_argsort.__wrapped__ sig = resolver(self, ary, args, kws)
# todo: we should raise exn:fail:contract </s> return prefab_key.short_key()	do_prefab_struct_key if not (isinstance(v, values_struct.W_Struct) and v.struct_type().isprefab): return values.w_false
# todo: finish </s> return_recordings.append(model_playlist.playlistrecording.parse_obj(recording.dict()))	insert_recordings recording.created = row['created']
# todo test cases </s> self._new_sensor_alert = sensor_alert	set_new_sensor_alert
# todo: allow "a, a, b" when typing "aaab" </s> remove = _add_hotkey_step(mapping, steps[state.index], suppress, lambda: none)	set_index mapping = {KEY_UP: lambda: (remove(), set_index(state.index+1))} else: state.remove_last = remove return False
# todo: log exception </s> ilo = curator.indexlist(self.es)	delete_index@505 def delete_index(self, index_prefix='metricbeat-', days=7): Delete index equal to or older than days. ilo.filter_by_regex(kind='prefix', value=index_prefix) ilo.filter_by_age(source='name', direction='older', timestring='%Y.%m.%d', unit='days', unit_count=days)
# steps = 0 # todo </s> def add(self, matrix):	Duplis self.exported_obj = exported_obj self.matrices = matrix self.matrices += matrix self.count += 1
# todo: create/clear alarm_data folder </s> self.actors.append(act)	setup_actors for actor in config.get("actors"): a = class_for_name(actor["module"], actor["class"])
# todo: create spatial index to speed up the clip </s> with rasterio.open(outfile_image, 'w', **kwargs) as dst:	main@44 'height': window.height, 'width': window.width, dst.write(patched_arr) outfile_label = Path(outfile_image).with_suffix('.png')
federated_only=self.federated_only,  # todo: 466 </s> self.known_metadata_dir = known_metadata_dir or constants.uninitialized_configuration	NodeConfiguration self.keyring_dir = keyring_dir or constants.UNINITIALIZED_CONFIGURATION self.known_nodes_dir = constants.UNINITIALIZED_CONFIGURATION self.registry_filepath = registry_filepath or constants.UNINITIALIZED_CONFIGURATION self.temp = temp
#todo: add some meaningful test </s> prodid:-//example inc.//example calendar//en	test_groupChange data_get_3 = """BEGIN:VCALENDAR VERSION:2.0 BEGIN:VEVENT UID:event1@ninevah.local
# todo: implement an external validation mechanism that can be omitted at runtime if desired. </s> assert self.key == gene2.key	crossover def crossover(self, gene2): assert isinstance(self, ConnectionGene) weight = self.weight if random() > 0.5 else gene2.weight enabled = self.enabled if random() > 0.5 else gene2.enabled
# todo ... </s> return false	cpreprocess_evaluate_ifdef arg = arg.strip() if not is_valid_defname(arg): return arg in state.macros
# see https://git-annex.branchable.com/todo/output_of_wanted___40__and_possibly_group_etc__41___should_not_be_polluted_with___34__informational__34___messages/ </s> 'wanted',	set_wanted out, err = self._run_annex_command(
# todo: add some kind of "ding" sound to all of these messages </s> return self.total_income - self.total_expenses	total_earnings @property def total_earnings(self):
""" todo: write desc here """ </s> db_table = 'profiles_email_notification'	Meta class Meta:
# todo add code </s> p.save()	test_creation p = Poll.objects.create(question = "lo lo", pub_date = timezone.now())
# todo: modify it to find the optimal elimination order </s> for node in working_factors:	query@38 working_factors[variable].append(phi) eliminated_variables.add(var) factors = working_factors[node] for factor in factors:
# todo remove get_media_references </s> if error:	back_to_main params = {} if edit: params['error'] = error args = [domain]
# todo(yuriyz): change to 404 (bug 1200517) </s> self.assertequal(response.content_type, 'application/json')	test_update_byid extra = {'foo': 'bar'} response = self.patch_json('/ports/%s' % pdict['uuid'], self.assertEqual(response.status_code, 200) result = self.get_json('/ports/%s' % pdict['uuid'])
# todo(shardy): remove when we no longer support essex </s> try:	authenticate@86 except (AttributeError, KeyError): logger.info("AWS authentication failure.") nova = client.Client(con.service_user, con.service_password, con.tenant, con.auth_url,
# todo: validate that the 'name' in the guide matches the name we're actually displaying. </s> "statename": statenames[state],	browse_state return { "state": state, "senators": get_senators(state), "representatives": get_representatives(state),
# todo check executions for dict contents </s> return filtered version of ``scope.get_defined_names()``.	_get_defined_names_for_position This function basically does what :meth:`scope.get_defined_names <parsing_representation.Scope.get_defined_names>` does.
'i': ('i', [{'j': 'j'}]),  # todo: support true for cases when the value should simply be mapped into the field name? </s> def __init__(self, *path_parts):	Path >>> target = {'a': {'b': 'c', 'd.e': 'f', 2: 3}} >>> glom(target, {'a_d': Path('a', 'd.e'), 'a_2': Path('a', 2)}) self.path_parts = path_parts
# todo unify </s> fh.write("begin nodes\n".encode("utf-8"))	_write_nodes assert not write_binary for k, x in enumerate(points):
# :todo: implement test. </s> self.skiptest('not implemented yet.')	GetBundlesRequestFilterTestCase def test_pass_compatible_types(self): Request contains values that can be converted to the expected def test_fail_empty(self): Request is empty.
# todo: clean up this event print out. we probably want something </s> display_output('{0}:'.format(fun), 'text', self.opts)	print_docs for fun in sorted(docs):
# todo: checking _unique_instances might be superfluous here </s> def res_filter(res):	res_filter
# todo: can this be optimized to avoid duplicating the anchors? </s> self.config.rpn_anchor_scales,	get_anchors self._anchor_cache = {} if not tuple(image_shape) in self._anchor_cache: self.config.RPN_ANCHOR_RATIOS, backbone_shapes,
# todo: 'flags_definition', 'spectral_definition'? </s> if all(shape):	reproject (resampling == RESAMPLING.nearest or no_fractional_translate(array_transform))): dydx = (int(round(array_transform.f)), int(round(array_transform.c))) assert src.dtype == dest.dtype assert source.nodata == dst_nodata
# todo: this is not thread‐safe! </s> reorg_cursor.execute('''delete from blocks where block_index>=?''', (block_index,))	reorg logging.warning('Status: Blockchain reorganisation at block {}.'.format(block_index)) break reorg_cursor.execute('''DELETE FROM transactions WHERE block_index>=?''', (block_index,)) purge(db, quiet=True)
return -1 # todo: followup after decision around returning none </s> def file_name_with_device(self) -> str:	_FILE_OBJECT name = "" if self._context.memory[self.vol.layer_name].is_valid(self.DeviceObject):
# todo add brief documentation what that means </s> assert len(input_dims) == 1, "can only use 1 input"	NICECouplingBlock def jacobian(self, x, rev=False): return 0 return input_dims
annot.annotation_metadata.annotator.email = "todo"  # todo </s> json.dump(jam, f, indent=2)	convert_JAMS annot = jam.sections.create_annotation() fill_section_annotation(json_file, annot) f.close()
# todo fix hack </s> return name	_get_name generator_id=%s; cursor = bdb.sql_execute(gather_data_sql)
"""todo: not implemented""" </s> >>> n(ser)	n def n(x): Example: 3 >>> df = pd.DataFrame({'x': ser})
# todo clean up, make configurable </s> parser.add_option("-c", "--config",  action="store", type="string", default="",   help="configuration file.")	parseCommandLineOptions parser = OptionParser(usage=usage, version=version)
replace = util.get_value(rule['replace'], kwargs) #todo use subkey? </s> _input -- source generator	pipe_strregex@26 def pipe_strregex(context, _INPUT, conf, **kwargs): Keyword arguments: kwargs -- other inputs, e.g. to feed terminals for rule values conf:
# todo: change 2312 by an always closed/non-http port </s> thread_names = [t.name for t in threading.enumerate()]	verify_threads_running self.assertIn('WorkerThread', thread_names) self.called_teardown_audit = True
#        todo: these are not in metadata. should they be? </s> if not other_comments:	smart_update other_comments = getattr(other, 'comments', '') if not my_comments: other_comments = '' if len(other_comments.strip()) > len(my_comments.strip()):
# todo: log exception </s> except subprocess.calledprocesserror as e:	scan@71 if local: try: output = e.output else:
if testname == "tests5": continue # todo </s> return self.runparsertest(innerhtml, input, expected, errors, treecls)	testFunc def testFunc(self, innerHTML=innerHTML, input=input,
# todo: implement </s> :param value: value used for patching the member.	terrain_defense_upgrade :type converter_group: ...dataformat.converter_object.ConverterObjectGroup :param line: Unit/Building line that has the ability. :type value: MemberOperator :param operator: Operator used for patching the member.
# todo(fsiddi): use proper exception filtering </s> registration_process.start()	start_worker def start_worker(): app.run(host='0.0.0.0') registration_process.join()
# todo: clean mixed precision api when tensorflow requirement is updated to >=2.4. </s> raise notimplementederror("this model can not restore v1 checkpoints")	map_v1_weights slots. Returns:
# todo assert responses, swipe down </s> "namespaceid": "nem",	test_nem_signtx_xem_as_mosaic "mosaics": [ { "name": "xem", },
# todo extend to nonbinary nodes </s> raise valueerror("connectivity matrix must be nxn, where n is the \	connectivity_matrix if cm.shape[0] != network.size:
# todo: update authors' num_sounds (when not handled via trigger) </s> return self.channels	get_channels_display elif self.channels == 2: return u"Stereo"
# todo pydocs </s> one = self.fetchone()	BigQueryCursor size = self.arraysize result = [] if one is None: break
# todo: use upstream implementation when available </s> def _rvs(self):	_rvs
# todo: append to current tree </s> elif type_from_name == 'boolean':	create_variable if vtype is None: if type_from_name == 'int': var = tkinter.BooleanVar() elif type_from_name == 'double':
# todo: check syntax, values? </s> self.setmessage('header-location', rs.redirect_without_location)	status303 def status303(self):        # See Other
# todo: replace all of this string templating with a function that accepts </s> try:	MonkeyDrops if not file_moved and os.path.exists(self._config["destination_path"]): os.remove(self._config["destination_path"]) shutil.move(self._config["source_path"], self._config["destination_path"]) LOG.info(
# todo: skipna should be implemented. </s> def _kdf(self) -> dataframe:	_kdf return self._ks._kdf
""" todo: documentation </s> raise notimplementederror('reduce(self, records)')	reduce
#     # todo: add an exception message </s> elif token == "s":	_parse_token parts["minute"] = int(value) elif token in ["ss", "s"]: value = value.ljust(7, str("0")) seventh_digit = int(value[6])
# todo: test with intercept </s> else:	test_all_estimators with warnings.catch_warnings(record=True) as w: if E in meta_estimators: e = E() clone(e)
# todo data alignment stuff </s> offset       = sparse.indices.byte_offset	get_data_from_sparse def get_data_from_sparse(gltf, sparse, type_, type_val=None): if type_ == "indices": component_nb = gltf.component_nb_dict['SCALAR'] elif type_ == "values":
assert oldsize == self.size, '%s: %d, %d' % (self.type, oldsize, self.size) # todo: remove </s> def read_uint(stream):	read_uint
# todo: verify exception type once those exists </s> assert false	test_dotifle @pytest.mark.xfail(reason='TODO')
#todo: this is just for backwards compatibility. it should be removed in v0.98 with p2.6 </s> return 0	ExtraIndicoFilter class ExtraIndicoFilter(logging.Filter): def filter(self, record): return 1
# todo: can cause an endless loop for single track repeat. </s> def get_current_tl_track(self):	get_current_tl_track
raise mpdnotimplemented # todo </s> def _findadd(self, type, what):	_findadd @register(r'^findadd "(?P<type>(album|artist|title))" "(?P<what>[^"]+)"$')
document_type='commcarecasesql',  # todo: should this be the same as the couch models? </s> producer.send_change(topics.form_sql, _change_meta_from_sql_form(form))	publish_form_saved
# todo: account for line widths and style </s> return [	sphere_3d_box face_color = self.face_color if face_color is not None: { "type": "sphere",
#todo: figure out why unicode sometimes causes an issue with loading after pickling </s> self.theta = self.theta[:, :, order]  # parameters defining the representation	Corex self.tcs = self.tcs[order]  # TC for each component self.alpha = self.alpha[order]  # Connections between X_i and Y_j def calculate_mis(self, theta, log_p_y): p_y = np.exp(log_p_y).reshape((-1, 1))  # size n_hidden, 1
#todo use calendar </s> with open("todolist.txt", "w+") as f:	writeToFile def writeToFile():
# todo: remove when support for django 1.4 is dropped </s> def _postgisadapter_prepare(self, conn):	_PostGISAdapter_prepare
# todo: remove this fallback logic with rally 1.0 </s> def __lt__(self, other):	ClusterHealthStatus RED = 1 YELLOW = 2 if self.__class__ is other.__class__: return self.value < other.value
# todo could this be vectorized? </s> return self	__isub__ def __isub__(self, other):
# todo more specific exception type? </s> values = variable.values.astype('o')	_nc4_values_and_dtype elif (variable.dtype.kind == 'U' or (variable.dtype.kind == 'S' and variable.dtype.itemsize > 1)): else: values = variable.values
# todo fixme : should we revoke all access tokens when application inactivated? seems likely. </s> current_user = self.request.user	UserNodes Q('is_deleted', 'ne', True) ) if current_user.is_anonymous(): auth = Auth(None)
except exception:  # todo: what could happen here? </s> log('warning', 'ws failed to create websocket connection. attempt {} of {}.'.format(attempt, max_attempts))	setup_websocket@177 except Exception:  # TODO: What could happen here?
# todo: endianness support </s> self.log.exception("error executing openocd command:")	execute_command except Exception, e:
# todo: add logger here </s> writeimage(resp.headers.get(	make_auth_screenshot f.close()
# todo: add 3ph loads </s> disco = np.where(ppc["bus"][:, 1] == none)[0]	_set_buses_out_of_service ppc["bus"][disco, VM] = np.nan ppc["bus"][disco, VA] = np.nan
#set limit to 25 --> currently hardcoded --> todo: add a setting for this ? </s> window.setproperty("emby.nodes.%s.path" %str(totalnodescount),"activatewindow(video,%s,return)"%path)	buildVideoNodesListing WINDOW.setProperty("Emby.nodes.%s.title" %str(totalNodesCount),label) WINDOW.setProperty("Emby.nodes.%s.type" %str(totalNodesCount),"favourites") WINDOW.setProperty("Emby.nodes.%s.content" %str(totalNodesCount),path) totalNodesCount +=1
# todo: implement toolpath.get_meta_data() </s> plugin_manager.import_plugins(ignore_names=["gtkconsole", "openglwindow"])	get_environment plugin_manager = pycam.Plugins.PluginManager(core=event_manager)
# todo: this should now raise an exception </s> result_before = model.predict(np.zeros((1, 3)))	test_model_predict_different model = dqn.ModelWrapper( state_axes=1, action_size=2, batch_size=3, model=small_model result_before = np.copy(result_before) result_after = model.predict(np.ones((1, 3)))
# todo(hartikainen): once tfp.bijectors.chain supports conditioning, </s> else:	ConditionalRealNVPFlow if _use_static_shape(x, event_ndims): event_shape = b.forward_event_shape(event_shape) event_shape = b.forward_event_shape_tensor(event_shape) event_ndims = tf.size(event_shape)
# :todo: implement test. </s> filter_type = gettrytescommand(mockadapter()).get_response_filter	GetTrytesResponseFilter skip_value_check = True def test_pass_transactions(self):
# todo: compare col/row widths before/after - not implemented yet </s> yield self.check_cell, param[0], param[1]	test_cell ('A3', test_date_2), ((3,1), test_date_2)]
# todo: add a bunch more here to ensure sane config file </s> allowed_values_string = '{' + ', '.join([ "'" + str(x) + "'" for x in allowed_values]) + '}'	verify_values def verify_values(self, key, allowed_values ): assert self.__dict__[key] in allowed_values, "config.%s='%s' (%s) is invalid.  Allowed values: %s" % (key, value, type(value), allowed_values_string)
# todo: remove anytime in 2016 </s> if self.drilldown_map and self.drilldown_map[0]['val'] == param_value_status_active:	FormsByApplicationFilter - Active Application (Application) - Deleted Application (Application-Deleted) labels = [ (_('Application Type'),
# todo: validate that liveactions for actionexec are all deleted. </s> def _create_custom_headers(self):	_create_custom_headers
# todo: proper content negotiation </s> return resp	dispatch_request self.validate_payload(meth) resp = meth(*args, **kwargs) representations = self.representations or {} mediatype = request.accept_mimetypes.best_match(representations, default=None)
# todo detect for typeerror: duplicate base class str, </s> self._name_context = name_context	ClassName class ClassName(TreeNameDefinition): def __init__(self, parent_context, tree_name, name_context, apply_decorators): self._apply_decorators = apply_decorators @iterator_to_context_set
#todo load this from somewhere </s> time_domain = len(self.parent.run_times) * sum([d.num_batches for d in self.alloc_devices])	print_process run_elapsed = time.time() - self.batch_start_time self.parent.run_times.append(run_elapsed) time_factor = 0.0 if time_domain == 0.0 else float(sum(self.parent.run_times)) / time_domain complete = float(self.batch_idx + self.num_alloc_batches) / len(self.parent.batches)
# todo - check and if we don't have category, take the only placement that exists in current site </s> if absolute_url:	full_url def full_url(self): return mark_safe('<a href="%s">url</a>' % absolute_url) return 'no url'
# todo: expect_match should work with emit() </s> return self.params ['file_name']	file_name @property
# todo: use slotssequenceelement to render this. </s> def render_share_problem(self, ctx, data):	render_share_problem
# todo: until we get it working. </s> else:	generate_jss_recipe jssimporter_arguments["extension_attributes"] = [{ "ext_attribute_path": "CFBundleVersionExtensionAttribute.xml" keys["Input"]["GROUP_TEMPLATE"] = "SmartGroupTemplate.xml" if "app_file" in facts:
pass  # todo </s> self.train_started = false	train_finish_epoch assert self.train_started assert len(self.forward_data_queue) == 0, "Not all forwardings were used?"
# todo fix. </s> self.asserttrue(cmp_result, self.__print_contexts(ctx_init, x86_ctx_out, reil_ctx_out))	test_cmova cmp_result = self.__compare_contexts(ctx_init, x86_ctx_out, reil_ctx_out) if not cmp_result:
# # todo: # fixme: </s> return render_template("base64decoded.html", l_64=b64_metadata, vt_enabled=vt_enabled, l_type=l_type, type=type, daily_type_chart=daily_type_chart, daily_date=daily_date,	base64Decoded_page l_type = r_serv_metadata.smembers('hash_all_type')
# todo: uncomment assert when #23880 is fixed </s> pony = models.get_model("test_runpythonatomic", "pony")	inner_method Pony.objects.create(pink=1, weight=3.55) raise ValueError("Adrian hates ponies.")
raise exceptions.mpdnotimplemented  # todo </s> def subscribe(context, channel):	subscribe@18 *musicpd.org, client to client section:* ``subscribe {NAME}``
# todo(mattjj): remove this logic when allreduce pred supported on cpu / gpu </s> device_assignment = tuple(d.id for d in devices)	parallel_callable raise ValueError("compiling computation that requires %s replicas, " "but %s devices were specified" compiled = built.Compile( compile_options=xb.get_compile_options(
# todo: warn/error: check if this var has units: assigning </s> set the lower bound for this variable.	setlb for vardata in self.values(): vardata.setlb(val)
# todo(#12314): add a good error about invalid data type. </s> user_interpreter_constraints: interpreterconstraints,	is_valid_for def is_valid_for( self, interpreter_universe: Iterable[str], ) -> bool:
#todo: note that i'm passing a dc to the fuzzablerequest and it's not </s> self.url, method='get', dc={'a': ['2'], 'b': ['cc']})	test_variants_false_diff_params_type fr_other = FuzzableRequest(
# todo: write tests </s> "when there is an otherwise-unspecified validity error that prevents parsing."	MeiValidityError pass
# todo special function redirection and __getattr__ redirection </s> return minpy.numpy.negate(self)	__neg__
# todo: this really shouldn't be in this class </s> self._print_jobs.append(print_job)	_addPrintJob return Logger.log("w", "Missing printer %s for job %s in %s", job.printer_uuid, job.uuid, [p.key for p in self._printers])
# todo: find a better way to enforce this. </s> op1_var = self._translate_src_oprnd(oprnd1)	_translate_stm def _translate_stm(self, oprnd1, oprnd2, oprnd3): assert oprnd1.size and oprnd3.size op3_var = self._translate_src_oprnd(oprnd3) where = op3_var
# todo: what does constructor of gitconfigparser, in case file doesn't exist? </s> submodules[line[1]]["initialized"] = not line[0].startswith('-')	get_all_submodules_dict lines = [line.split() for line in out.splitlines()] for line in lines: submodules[line[1]]["modified"] = line[0].startswith('+') submodules[line[1]]["conflict"] = line[0].startswith('U')
# todo: some regressors have extra options in their predict method, and they return a tuple of arrays. </s> output_data = step.transform(*xs)	Model if hasattr(step, 'predict'): output_data = step.predict(*Xs) else: raise TypeError('{} does not implement predict or transform!'.format(step.name))
# todo: do not use bare except </s> 'flattened chain')	flatchain columns=self.var_names) else: else: return None
# todo: do some more checks here. currently it only tests that they </s> self.remote_node2.nmt.state = 'operational'	test_start_two_remote_nodes time.sleep(0.1) slave_state = self.local_node.nmt.state time.sleep(0.1) slave_state = self.local_node2.nmt.state
# todo subject.cn from cert? </s> if '_' not in last:	TestMac key = sp_match.group(1) val = sp_match.group(2) last['_'] = {} akey = array_match.group(1)
# todo if the database is not found we should build it, otherwise just run the tests. </s> def analyze_policy(c):	analyze_policy c.run('python3 policy_sentry/bin/policy_sentry analyze policy-file --policy examples/analyze/explicit-actions.json')
# todo: handle agg_columns. </s> return _col(super(seriesgroupby, self).idxmin(skipna))	idxmin
# todo: check if subdirs is empty </s> except dropbox.exceptions.apierror as d:	Dropbox_Dropper try: logging.info("Dropbox: Trying to upload file %s to %s" % (file, dropbox_dir)) logging.error("Dropbox: API error: %s" % d) f.close()
# todo implement for stride != kernel_size </s> sl = x[..., y:y+my*py+ky:py, x:x+mx*px+kx:px]	stack_for_pool stack = fill_value*np.ones((ky, kx, *x.shape[:2], my+ky, mx+kx), dtype=x.dtype) for Y in range(ky): stack[Y, X, ..., :sl.shape[2], :sl.shape[3]] = sl return stack.reshape(-1, *stack.shape[2:]), (my, mx)
# @todo: display better tick labels for date range (e.g. 06/01 - 06/05) </s> data = data.drop('yyyy',1)	_parse_csv data['time']=timeindex data = data.set_index('time') return "", data
# todo: make the multiplication sign configurable </s> if den:	_dcgain_cont for j in range(self.inputs): num = self.num[i][j][-1] gain[i][j] = num / den else:
# todo: not all values have exact matches in flexget, need to update flexget qualities </s> % (parsedurl.scheme, parsedurl.netloc, config.get('port'),	CouchPotato json = task.requests.get(url).json() except RequestException: parsedurl.path)) entries = []
# todo: normalization for other languages </s> if int_number == number:	convert_number def convert_number(number, denominators): return int_number, 0, 1 frac_number = abs(number - int_number)
# todo: write this </s> point2(0,0),	bezier_points_variants def bezier_points_variants(): Point2(1,2), Point2(2, -1),
# todo unordered float </s> e.append(m2_expr.expraff(ir.irdst, dst_o))	jno n.zeroExtend(instr.mode), dst.zeroExtend(instr.mode)) return e, []
# todo make more robust - some folks will write .jsx and others </s> prop_names.insert(0, prop)	load_components@37 prop_names = componentData.get('props', {}).keys() for prop in default_props: if 'content' in prop_names: prop_names.remove('content')
# todo: also create an activity detail recording what exactly changed in </s> session = context['session']	resource_update def resource_update(context, data_dict): user = context['user'] id = data_dict["id"]
# todo: cache this - it's a big time-waster when libraries get big </s> conn_module.finalize(shutit)	finalize_target if mod.module_id == shutit.cfg['build']['conn_module']: conn_module = mod
raise notimplementederror # todo </s> for s in self.states_list:	resample_states def resample_states(self):
pass  # todo </s> full_layout[alias.lower()] = layout.layout[ref]	build_layout alias, ref, ref))
# todo: check if this different handling of none and '' has </s> return [	Console def _query(self, vector, args): result = self.vectors.get_result(vector, args) line.split('\x00') for line in result.strip('\x00').replace('\x00\n', '\n').split('\n')
# todo: use k-way merge instead of sort </s> for i in range(n_local):	parallel_sort@79 send_counts = np.zeros(n_pes, np.int32) recv_counts = np.empty(n_pes, np.int32) if node_id < (n_pes - 1) and key_arr[i] >= bounds[node_id]: node_id += 1
# todo: move this to pyresample </s> if isinstance(self.target_geo_def, (list, tuple)):	NativeResampler raise ValueError("Must either expand or reduce in both " "directions") test_func = max if expand else min target_geo_def = test_func(self.target_geo_def,
# todo(b/132888123): consider other options to avoid possible bugs here. </s> if not is_argument_tuple(type_spec):	pack_args_into_anonymous_tuple else: py_typecheck.check_type(type_spec, computation_types.NamedTupleType) raise TypeError( 'Parameter type {} does not have a structure of an argument '
# todo: add keep parameter </s> def agg(self, func_or_funcs, *args, **kwargs):	agg
# todo: backport the windows implementation </s> try:	_is_ipv6_enabled def _is_ipv6_enabled(): if socket.has_ipv6: sock = socket.socket(socket.AF_INET6, socket.SOCK_STREAM) sock.bind((HOSTv6, 0))
#todo implement extra options </s> = openappend[mathicsnonexamplefile]	OpenAppend >> OpenAppend[] = OutputStream[...] mode = 'a' stream_type = 'OutputStream'
# todo when dns server is ipv6 </s> results = []	build_address def build_address(address): address = address.strip('.') for label in labels: l = len(label)
# todo: it would be nice to set the size header here </s> def render(self, ctx):	Downloader self._name = name IFileNode(filenode) req = inevow.IRequest(ctx) gte = static.getTypeAndEncoding
pass # todo: explain </s> self.setmessage('header-location', rs.redirect_without_location)	status303 def status303(self):        # See Other
# todo: handle in cleaner way </s> torch.sum(self.mu @ self.p, 1) - torch.diag(self.o))**2	loss_inv_mu def loss_inv_mu(self, l2=0.0): loss_1 = torch.norm(self.Q - self.mu @ self.P @ self.mu.t())**2 return loss_1 + loss_2
# todo maybe we can figure out a version string </s> if install_result:	PackageManager ) ) return True return False
# todo(devcamcar): implement filter by user. </s> should throw an exception if the user has instances,	AdminController return user_dict(manager.AuthManager().create_user(name)) @admin_only volumes, or buckets remaining. manager.AuthManager().delete_user(name)
# population dynamics (todo: registry) </s> if self.n_steps % period == 0:	Simulator get_signal(self.signals, ct.insig)) for probe in self.model.signal_probes: tmp = get_signal(self.signals, probe.sig).copy() self.probe_outputs[probe].append(tmp)
# todo pick a runtime that is lightly loaded </s> def initiate(node, actor, **kwargs):	initiate
include_base = true  # todo: make option </s> return merge_render(base, local, remote, strategy)	make_inline_source_value else: raise ValueError("Invalid item patch op {}".format(re.op))
# todo: error checking </s> raise authenticationrequired()	add_card@85 :name: @todo :returns: @todo headers = {'Cookie': self._cookie, 'Accept': 'application/json'} response, content = self._client.request(
# todo: we change the type here, maybe we should change it earlier? </s> lower=lower, upper=upper)	KTRXFull coefficient_method=coefficient_method, date_array=date_array, def get_regression_coef_knots(self, aggregate_method='median'): self._set_aggregate_posteriors()
return wikidata_key  # todo </s> if not cover_id:	get_cover_url def get_cover_url(self, cover_blob, *_): {"url": "/img/entities/e794783f01b9d4f897a1ea9820b96e00d346994f"} return None return "%s%s" % (self.covers_url, cover_id)
# todo: should it end with a slash? </s> if self.poll():	wait_for_results delay = next - last time.sleep(delay) return 0
# todo(todd): exception (404) </s> for inst in self.instdir.all:	detail "servers": [] value["servers"].append(self.instance_details(inst)) return json.dumps(value)
# todo: support for multiple message versions </s> )	HashJob def update_keyword(self, kwdict): self.hash_keyword.update( def update_content(self, url): if self.content:
# todo check performance </s> if not (arange(self.ndim) == sort(hstack((rdims, cdims)))).all():	unfold rdims = setdiff1d(range(self.ndim), cdims)[::-1] elif cdims is None: raise ValueError( 'Incorrect specification of dimensions (rdims: %s, cdims: %s)'
@unittest.skip('not written')  # todo: finish! </s> @py3_only	test_SimpleNamespace def test_SimpleNamespace(self): raise NotImplementedError
# todo - fix meta.submission to point to real submission </s> desc = ""	_get_cursor filter[2] = unicode(filter[2]) to_append = " AND s.%s %s %s " % tuple( filter ) if sort_descending: desc = "desc"
# rbarlow_todo: convert this callrequest into a celery task call </s> task_id = call_report.call_request_id	cancel_agent_request :type call_request: pulp.server.dispatch.call.CallRequest :param call_report: The report associated with the call request to be cancelled. consumer_id = call_request.args[0] agent_manager = managers_factory.consumer_agent_manager()
# todo: spawn process which sets resource limits and then calls </s> handler = getattr(self, handler_name, self.unknown_command)	dispatch_request handler_name = 'handle_%s' % (command['cmd'],)
# todo: this is a temporal fix </s> defines the size of each block with vectors of the glyph image.	_create_feature_glyph feature : (N, D) ndarray The feature pixels to use. num_bins = feature.shape[2] block_image_temp = np.zeros((vbs, vbs))
# todo: the peer node url needs to be fixed. </s> request_body=channelupdateserializer,	ChannelViewSet except ObjectDoesNotExist: raise ResourceNotFound responses=with_common_response({status.HTTP_202_ACCEPTED: "Accepted"}), )
# todo: disclaimer!!! this is a temporary hack to escape from current "ddos attack" </s> return get_response(request)	block_suspicious_requests raise Ratelimited()
# todo add binary column (after dropping support for python 2.7) </s> self.assertequal(list(df.columns), ['__index__', 'a'])	test_reset_index self.assertEqual(list(df.columns), ['index', 'a']) df = pd.DataFrame({'a': [1, 2, 3, 4, 5]}) df = pd.DataFrame({'a': [1, 2, 3, 4, 5]}) with self.assertRaises(ValueError):
result = np.hstack((result, result_a))  # todo: https://github.com/tensorlayer/tensorlayer/issues/288 </s> parameters	flatten_list ---------- list_of_list : a list of list
# todo scope the cleanup to the current project - https://github.com/lyft/cartography/issues/381 </s> def cleanup_gke_clusters(neo4j_session, common_job_parameters):	cleanup_gke_clusters Delete out-of-date GCP GKE Clusters nodes and relationships :type neo4j_session: The Neo4j session object
# todo: convert to a python xml thing </s> self.on_actionsaveas_text_triggered()	on_actionSave_Text_triggered f.write(unicode(self.ui.text.toPlainText())) f.close()
# todo(mattjj): remove this special case, used for debugging on cpu </s> del master, consts, jaxpr, env	parallel_callable jaxpr, (pval, consts, env) = trace_to_subjaxpr(fun, master).call_wrapped(pvals) assert not env return partial(execute_replicated, compiled, pval, axis_size)
# todo: arrange </s> self.remote.modify_repo(repo, "mirror_locally", "0", self.token)	createRepo self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token)
# todo-me move sorting and add more sorting options </s> print("...deleted {playlist.title} for '{user}'."	delete_playlist elif jbop == 'mostPopularTv': if playlist.title.startswith('Most Popular TV'): .format(playlist=playlist, user=user)) except:
# todo(karita): make all scorers batchfied </s> if args.batchsize == 1:	recog_v2@81 token_list=train_args.char_list, pre_beam_score_key=None if args.ctc_weight == 1.0 else "decoder" non_batch = [k for k, v in beam_search.full_scorers.items() if not isinstance(v, BatchScorerInterface)] if len(non_batch) == 0:
singleton=false,  # todo: re-enable </s> opts,	roster def roster(opts, whitelist=None): Returns the roster modules tag='roster', whitelist=whitelist,
# todo this is a workaround since exceptions are currently not correctly stacked </s> def __tregex_compile(self, pattern):	__tregex_compile
# todo put an index.html in front of this bucket </s> existing_version_keys = yield effect(	publish_docs new_version_keys = yield Effect( ListS3Keys(bucket=configuration.dev_bucket, ListS3Keys(bucket=configuration.documentation_bucket, prefix=version_prefix))
# todo: handle other hosts </s> return nem	add_emane_interface nem = self.add_nem(platform, nem_name) self.add_parameter(nem, 'nemid', str(nemid))
# todo : real error </s> self.configuration_errors.extend(s.configuration_errors)	ComplexExpressionNode else: for s in self.sons: valid = False return valid
# todo fix. </s> cmp_result = self.__compare_contexts(ctx_init, x86_ctx_out, reil_ctx_out)	test_cmova asm = ["cmova eax, ebx"] ctx_init = self.__init_context() if not cmp_result: self.__save_failing_context(ctx_init)
# todo use proper tx, txindex, sibling.  should also test that </s> blockheaderbytes = getheaderbytes(version, hashprevblock, hashmerkleroot, time, bits, nonce)	testForkingPast time = blockJson['time'] bits = blockJson['bits'] res = self.c.storeBlockHeader(blockHeaderBytes) hashPrevBlock = int(blockJson['hash'], 16)
# todo(b/133761055): update all callers and make this an error condition to </s> eval_saved_model_path,	construct_fn def construct():  # pylint: disable=invalid-name start_time = datetime.datetime.now() include_default_metrics, additional_fetches=additional_fetches)
# todo: saved searches </s> category=category)	create_tag_class use_sort_as_name = False return partial(Tag, use_sort_as_name=use_sort_as_name, icon=icon,
# todo: logging </s> return self._verified	is_verified pkey.verify_update(sign_bytes) verify_result = pkey.verify_final(signature)
#todo - this should probably syslog </s> def emit_syslog(self, what):	emit_syslog
# todo(stephenfin): remove this in a future major version </s> server	RestoreServer for server in parsed_args.server: utils.find_resource(
# todo allow exporting poseunits </s> fbx_anim.writeobjectdefs(fp, action)	exportFbx@86 if config.useMaterials: fbx_material.writeObjectDefs(fp, meshes) fp.write('}\n\n') fbx_header.writeObjectProps(fp)
# todo(mordred) when this changes to rest, force interface=admin </s> except ironic_exceptions.clientexception:	get_nic_by_mac _tasks.MachineNodePortGet(port_id=mac))
pass # todo </s> def collect_incoming_data(self, data):	collect_incoming_data
# todo: this could be a property.  handle it! </s> setting to an empty list will disable todo checking.	set_todo_states a list[] of strings as its argument. The list can be split by '|' entries into TODO items and DONE items. Anything after new_todo_states = [] new_done_states = []
# todo: remove for all locales generated by the doc. </s> if rebuild:	reset_cache self.pod.podcache.reset() self._build_routing_map(inject=False)
# todo: implement </s> (default) or a billing address.	assign_address_to_request@37 passed request. This abstracts the difference between logged-in users and session-based guests.
# todo message </s> returns	_parse_argspec def _parse_argspec(args): ------- tags, mapping
# todo: must be implemented </s> def should_fetch_kindlegen():	should_fetch_kindlegen
# todo: ensure that if multiple flags are provided, the *last* one overrides </s> exec_opts.setshoptoption(opt_name, b)	Shopt else:
# todo we could reload the message </s> except valueerror:	Forward@99 try: self._input_sender = self._client.session.get_input_entity( pass return self._input_sender
# todo: rate should not have to be inversed </s> arbitrage_loop.insert(0, next_node)	retrace_negative_loop if next_node not in arbitrage_loop: arbitrage_loop.insert(0, next_node) arbitrage_loop = arbitrage_loop[arbitrage_loop.index(next_node):] return arbitrage_loop
# todo(cmaloney): test user provided parameters are present. all the </s> expanded_config = json.load(f)	test_load_expanded_config@26 def test_load_expanded_config(): assert 'master_quorum' in expanded_config
# todo: move to base class </s> n_rect = qtcore.qrectf(n.scenepos(), qtcore.qpointf(n.scenepos().x() + float(n.w), n.scenepos().y() + float(n.h)))	getNodesRect if activeGraphOnly: if n._rawNode.graph() != self.graphManager.activeGraph(): rectangles.append([n_rect.x(), n_rect.y(), n_rect.bottomRight().x(), n_rect.bottomRight().y()]) arr1 = [i[0] for i in rectangles]
# todo: revisit for potential behaviour / type checking. </s> attribute value.	RGB_Colourspace Parameters ---------- if value is not None: assert hasattr(value, '__call__'), (
"""todo: explain what this is testing </s> assert_true(_infer_dimension_(spect, n, p) > 2)	test_infer_dim_3 pca = PCA(n_comp=p) pca.fit(X)
# todo(b/148082271): remove this line once tft 0.22 is used. </s> outputs[_transformed_name(_label_key)] = inputs[_label_key]	preprocessing_fn outputs[_transformed_name(key)] = tft.scale_to_z_score(inputs[key])
# todo: no coverage here </s> else:	_col_is_part_of_mappings if self.secondary is None: return self.parent.mapped_table.c.contains_column(column) or \ return self.parent.mapped_table.c.contains_column(column) or \ self.target.c.contains_column(column) or \
# todo - fix this problem with bad imports from bitbucket </s> package.save()	Command commit, created = Commit.objects.get_or_create(package=package, commit_date=commit.committed_date) zzz += 1 for commit in get_bitbucket_commits(package): commit, created = Commit.objects.get_or_create(package=package, commit_date=commit["timestamp"])
# todo: make sure we only call get_transform if the transform for </s> return 2 - (k-1.0) / (m-1.0) if k >= m else 1.	_density_max
# todo(twd2): improve here: </s> raise error.invalidtokenerror(token.type_newmail, code)	UserNewmailWithCodeHandler async def get(self, *, code: str): tdoc = await token.get(code, token.TYPE_NEWMAIL) mail_holder_udoc = await user.get_by_mail(tdoc['mail']) if mail_holder_udoc:
# todo generator </s> check=check_argument,	Drop constraints=EnsureStr() | EnsureNone()), recursive=recursion_flag, if_dirty=if_dirty_opt, )
# todo(mjanusz): remove circular reference between canvas and seed policies. </s> def set_state(self, state):	set_state
# todo scope the cleanup to the current project - https://github.com/lyft/cartography/issues/381 </s> return []	get_gcp_instance_responses :param compute: The compute resource object :return: A list of response objects of the form {id: str, items: []} where each item in `items` is a GCP instance response_objects = [] for zone in zones:
raise notimplementederror()  # todo </s> :param int vendor_id: the device's vendor id (e.g. ``0x1038``).	MouseSettings :param int product_id: The device's product id (e.g. ````0xbaad``). :param dict mouse_profile: The mouse profie (``devices/*``).
# todo(okuta): check type </s> return a.std(axis=axis, dtype=dtype, out=out, ddof=ddof,	std Returns: cupy.ndarray: The standard deviation of the input array along the axis. keepdims=keepdims)
except (keyerror,valueerror): # todo, handle errordetail messages </s> class imagedoesnotexistsexception(exception):	ImageDoesNotExistsException
# todo: cp.get_file will also match cp.get_file_str. this is the </s> salt \* sys.reload_modules	reload_modules def reload_modules(): Tell the minion to reload the execution modules return True
# todo move this to spotify.luigi.hdfs </s> def pipe_reader(cls, input_pipe):	pipe_reader @classmethod
# todo: why is there a benchmark? </s> def action_space(self):	action_space @property
# todo use deepcopy() here </s> p1 = exterior[0]	_is_polygon_line @classmethod def _is_polygon_line(cls, exterior): angles = set() for p2 in exterior[1:]:
# todo: log. </s> self.evt.set()	poke if self.done is not None: self.map_ = self.done(self.map_)
# todo assert exit code != 0 </s> self.asserttrue(commit.child("file.txt").exists())	test_commit_volume self.assertTrue(commit.exists())
# todo: this might be too slow because of the addition </s> return a count of the number of datapoints for a time interval.	Timeseries return reduce(operator.add, rval.values()) return rval Returns an ordered dictionary like get(), but the values are integers rather than lists.
min_stake=0)  # todo: handle customized min stake here. </s> log.info(f'fetched external ip address from centralized source ({endpoint}).')	get_external_ip_from_centralized_source if ip:
# todo: fix this 405 method not allowed error </s> test views creation and registration	test_fab_views def test_fab_views(self):
# todo: same code as for batch gradient, but with sum_batch = true </s> def weight(self, module, grad_input, grad_output):	GradBase module, grad_input, grad_output, grad_out_vec, sum_batch=True) shape = module.bias.shape batch = grad_output[0].shape[0] grad_out_vec = grad_output[0].contiguous().view(batch, -1)
# todo: launch visitor on node </s> for statement in fc_ast.body:	SemBuilder fc_ast = parsed.body[0] body = [] if isinstance(statement, ast.Assign): src = self.transformer.visit(statement.value)
# todo: this pattern seems to repeat a lot, maybe we should have a sensible default? </s> return linter(config=config)	_get_linter@18 def _get_linter(dialect="ansi"):
self.entity_bin = serialize(value, to_bytes=true)  # todo: techdebt fix </s> def entity(self, value: any) -> none:	entity@43 @entity.setter
# todo legacy method to be removed/refactored </s> return filter(none, [organization.get_by_name(org) for org in self.organizations])	get_organizations def get_organizations(self):
except exception:  # todo: be specific </s> r = get(url, headers=headers)	get_insta_json def get_insta_json(url): headers = {"Accept-Language": "en"} json_start = r.text.find("window._sharedData") + 21 json_stops = r.text.find("</script>", json_start) - 1
# todo: get runname from model-dir </s> summary = tf.summary(value=[	evaluate_once@157 map_0_3, _ = calculate_map(output_per_batch, config.network.num_classes, 0.3) map_0_5, _ = calculate_map(output_per_batch, config.network.num_classes, 0.5) tf.Summary.Value(tag='val_loss', simple_value=val_loss), tf.Summary.Value(tag='mAP@0.3', simple_value=map_0_3),
## todo: # fixme: remove me </s> paste_parent = self.r_serv_onion.hget('onion_metadata:{}'.format(self.domain), 'paste_parent')	get_last_crawled_pastes return self.get_all_pastes_domain(paste_parent)
# todo: make sure the image is present or pull it </s> return ''.join(random.choice(letters) for _ in range(length))	random_word def random_word(length):
# todo: ask where to install the bootloader (if the user wants to install it) </s> def get_next_page(self):	get_next_page
# todo: we can't do this; this shells out for each selection change... </s> raise	_lookup_tree_cache break except OSError as e: tree_state = {} for entry in entries:
# todo: start here </s> running this job. should be safe to run this at any time, or multiple	_cleanup_local_tmp times. This particular function removes any local tmp directories
# todo: enable gpu tests on jenkins </s> prediction_length = 50	test_jitter_synthetic_gp@60 if ctx == mx.Context("gpu") and not check_gpu_support(): return context_length = 5 num_samples = 3
# todo(tdurakov): remove dict to object conversion once rpc api version </s> bdms = objects.blockdevicemappinglist.get_by_instance_uuid(	_get_host_volume_bdms instances = objects.InstanceList.get_by_host(context, self.host, use_slave=use_slave) context, instance.uuid, use_slave=use_slave) instance_bdms = [bdm for bdm in bdms if bdm.is_volume]
# todo: revise this when build deps are in dag_hash </s> spec = pkg.spec.concretized()	test_find for pkg in packages: if pkg.name.startswith('external'): installed_specs[spec.name] = spec self.layout.create_install_directory(spec)
# todo(b/155804245) sanitize the names so that they're valid python names </s> arguments.update(kwargs)	tfx_component_class_init instance_name = kwargs.pop('instance_name', None) arguments = {} base_component.BaseComponent.__init__( self,
#todo: add check/warning if theta or phi outside appropriate ranges </s> assert np.abs(m) <= l, 'absolute value of expansion coefficient must be <= l'	sph_harmonic ------- y : complex float
raise  # todo </s> def from_configuration_directory(cls, filepath: str):	from_configuration_directory @classmethod raise NotImplementedError
# todo: take care of theano to keras port: </s> else:	vgg16_preprocess def vgg16_preprocess(X): if X.shape[1] == 3: shape = [1, 1, 1, 3] offset = VGG16_OFFSET.reshape(shape)
# todo: has to be refactored. </s> return manager	get_manager global MANAGER  # pylint: disable=global-statement if not MANAGER:
# todo: implement @plist </s> "when there is an otherwise-unspecified validity error that prevents parsing."	MeiValidityError class MeiValidityError(exceptions21.Music21Exception):
# todo: assert metrics. </s> void func3() {	test_get_touched_functions "file.cpp", int i = 1; int i = 3; }
# todo: docs for this. </s> word = next(w for w in m.groups() if w is not none)	dollar_replace if word == "$": return "$"
# todo: move to base class </s> if n._rawnode.graph() != self.graphmanager.activegraph():	getNodesRect else: for n in self.getAllNodes(): continue n_rect = QtCore.QRectF(n.scenePos(), QtCore.QPointF(n.scenePos().x() + float(n.w), n.scenePos().y() + float(n.h)))
# todo: remove in v8 </s> for ft in flatten(t):	flatten for t in task:
# todo should we be using a python library for this? </s> return false	fetch_all cwd=self.files_dir) except subprocess.CalledProcessError as e: return True
# todo: check whether the graph execution is resolved correctly. </s> steps = {step.properties['name']: step for step in subgraph.steps}	test_pipeline_get_induced_subgraph def test_pipeline_get_induced_subgraph(description): pipeline = Pipeline.from_json(description) assert len(steps) == 3 assert steps['step-2']._children == [steps['step-4']]
# todo: currently mnn python binding have mem leak when creating mnn.tensor </s> tp_sorted = []	draw_plot_func if true_p_bar != "": Special case to draw in (green=true predictions) & (red=false predictions) for key in sorted_keys: fp_sorted.append(dictionary[key] - true_p_bar[key])
# todo: move part of this to card type. </s> if not self.name.text():	AddCardTypeDlg self.parent_type.addItem(card_type.name) self.card_types.append(card_type) self.OK_button.setEnabled(False) else:
# todo: change this when data is avaialable </s> obj['production']['oil']     = obj['production'].get('oil', 0.0) + float(item['gen_mw']) * 0.5 / sum_other	fetch_production@51 if iso == 'CAISO' and fuel_name == 'other': sum_other = 40 + 0.5 + 0.5 + 18 + 8 + 5 + 5 + 2 obj['production']['coal']    = obj['production'].get('coal', 0.0) + float(item['gen_MW']) * 0.5 / sum_other obj['production']['nuclear'] = obj['production'].get('nuclear', 0.0) + float(item['gen_MW']) * 8 / sum_other
pass  # todo </s> def is_connected(self) -> bool:	is_connected
# todo: remove with v1 deprecation </s> kwargs.pop('name')	Resource not self._is_full_v1_name(kwargs['name'])): kwargs.setdefault('query', []) return super(Resource, self).read( pk, fail_on_no_results=fail_on_no_results,
# todo -- get a list of these from the api </s> 'type': 'issue',	PhabricatorService pass extra = { } yield self.get_issue_for_record(issue, extra)
# todo actually it is already well tested in base calss actionlog </s> assert legacy_logs == new_logs	test_restart_log_loads_legacy_data getattr(restart_log_new, 'append_' + ev.ev_type.name)(ev.data) with open(log_file_path_new, 'r', newline='') as f:
# todo: needs input cleansing and validation </s> new_policy = drone_alice.grant(bob, label, m=m, n=n,	make_alice_control federated_only=True) except KeyError as e: expiration=expiration_time) import pudb; pudb.set_trace()
# todo not supported yet </s> def _varfunc():	_varfunc
#todo: how to use the same data for both transformers </s> b = net.constantfill([], ["b"], shape=[2, ], value=1.0, run_once=0, name="b")	test_fc@23 net = core.Net("net") X = net.GaussianFill([], ["X"], shape=[2, 2], mean=0.0, std=1.0, run_once=0, name="X") Y = X.FC([W, b], ["Y"], name="Y") workspace.ResetWorkspace()
# fix: https://github.com/certtools/intelmq/issues/1720 # todo: find better fix </s> else:	get_feed_by_filename return feedname, function
uploader.upload_file(file, container='export') # todo: right container folder?! </s> return true	warm_cache del os.environ['PYBOSSA_REDIS_CACHE_DISABLED'] else:
# todo: implement me </s> image = self.image.clone()	_test_gradcheck depth = self.depth.clone() depth = utils.tensor_to_gradcheck_var(depth)  # to var
# todo implement this function </s> :param setting_name: name of setting, could be volume, brightness	set_system_setting def set_system_setting(setting_name, setting_value): :param setting_value: value of setting :return:
# todo: try/except this call. </s> relpath = os.path.basename(path)	image_tag if(path is not None): if not os.path.exists(path): if(image.save("%s/qt_img%s.png" % (path,match.group("name")), "PNG")):
# todo: handle this </s> print "error :", error_type, " ->", error	on_client_error
# todo: this can be formulated more efficiently </s> sqrt_ggn = einsum('bijc->bic', sqrt_ggn)	bias_diag_ggn out_pixels = module.output_shape[2] * module.output_shape[3] sqrt_ggn = sqrt_ggn_out.view(batch, module.out_channels, out_pixels, return einsum('bic->i', (sqrt_ggn**2))
if not version_2_79_or_older():  # todo </s> if len(tools.common.get_armature_objects()) <= 1:	CustomPanel row = col.row(align=True) row.scale_y = 1.05 row = col.row(align=True) row.scale_y = 1.05
# todo find which file is being downloaded with this item. </s> def downloadfilestart(fileid):	downloadFileStart
raise notimplementederror # todo </s> return [prop[0] for prop in cur.description]	get_storage_properties def get_storage_properties(self):
# todo: also improve 'crash-start' detection (to reduce lag when server fails to start) </s> time.sleep(0.1)	auto_server_start@118 L.debug("It looks like Ramona server is not running - launching server") launch_server_daemonized() s = self.connect() if s is not None: break
# todo: remove in favor of a proper per-module selection </s> __focused_module = -1	Core class Core(): @staticmethod def add_module(module: UiModule, index=None) -> int:
# todo: error sound </s> return self.session.db.get_settlerlvl_of_building(building_id) <= \	_is_buildable def _is_buildable(self, building_id):
# todo: create xxx_failure test </s> self.asserttrue(check('test_space', failure_tests),	test_result_space_failure tests = exclude_from_resultlist(r, 'failure')
# todo: check ping response </s> self.faucet_event_sock, self.metrics, self.logger)	setup_valve self.registry = CollectorRegistry() self.metrics = faucet_metrics.FaucetMetrics(reg=self.registry) # pylint: disable=unexpected-keyword-arg self.notifier.start() dp = self.update_config(config)
pass # todo </s> class artist(testmodel):	Artist
# todo, pass also best score </s> entry point for ddp	ddp_train Args: process_idx:
# todo: bash path completion </s> '/' + cfg['build']['build_id'] +	record_config if cfg['build']['delivery'] in ('docker'): self.send_file(cfg['build']['build_db_dir'] + '.cfg', shutit_util.print_config(cfg))
# todo: we should throw here, i don't like this. </s> return cls.get_by_name(name)	_get_by_object @classmethod def _get_by_object(cls, object):
# todo: missing objects? </s> with connection.schema_editor() as schema_editor:	delete_table @classmethod schema_editor.delete_model(cls)
#todo, multipart raw submissions need further parsing capacity. </s> (since we don't know what xform was being posted to)	post@28 Just like play, if you specify a callback you get called, otherwise you get a generic response.  Callbacks follow if request.META['CONTENT_TYPE'].startswith('multipart/form-data'): instance = request.FILES['xml_submission_file'].read()
# todo: this is untested. </s> self._problems.append(e)	_VerifyHelper try: result = callback(connection, cert, error_number, error_depth, ok) return 0 else:
# todo allow other audio devices but the default. </s> self.command += ' reverb '	reverb if wet_only: self.command += ' -w '
# todo(sahid): we should never configure a driver backend for </s> conf.target_dev = vif.vif_name	_set_config_VIFOpenVSwitch def _set_config_VIFOpenVSwitch(self, instance, vif, conf, host=None): conf.net_type = "bridge" self._set_config_VIFPortProfile(instance, vif, conf)
raise notimplementederror #todo, implement! </s> tree = xmltreefromstring(f.read())	SetDefinition except: raise DeepValidationError("Unable to download " + url) except IOError: raise DeepValidationError("Unable to download " + url)
# todo: not for checkbox (should have checkbox class) </s> return self._html_output(	as_span normal_row=SPAN_TEMPLATE, error_row=u'%s',
# self.assertisnotnone(cursor.service_processing_time_in_millis)  # todo flaky test </s> lambda: cursor.execute(	test_cancel executor.submit(cancel, cursor) self.assertRaises( SELECT a.a * rand(), b.a * rand() FROM many_rows a
# todo check if result is in scope -> no evaluation necessary </s> names are returned.	_get_defined_names_for_position <parsing_representation.Scope.get_defined_names>` does. - If `position` is given, delete all names defined after `position`. :type     scope: :class:`parsing_representation.IsScope` :param    scope: Scope in which names are searched.
## todo: # fixme: remove me </s> for word in word_injection_suspect:	is_sql_injection temp_res = str.find(line, str.upper(word)) if temp_res!=-1: temp_res = str.find(line, str.upper(word)) if temp_res!=-1:
@pytest.mark.skip()  # todo: fix this </s> def test_issue_560_success(self):	TestWebClient self.async_client: WebClient = WebClient(token=self.bot_token, run_async=True) def tearDown(self): client = self.sync_client response = client.conversations_list(exclude_archived=1)
# todo: this completion may not be good, since it resets to 0 later. </s> history = build_history_buffer(time_steps, num_classes, notes_per_bar, style, prime_beats=false)	generate@75 def generate(model, time_steps, style, bars, inspiration=None): Generates a sequence if inspiration is not None: for i in range(NOTES_PER_BAR):
# todo: add longer frame data </s> header = required_header.copy()	testWsUtils self.assertEquals(sock._validate_header(header, key), False) del header["connection"] header["sec-websocket-accept"] = "something" self.assertEquals(sock._validate_header(header, key), False)
# todo - retrieve from config </s> return self.remove_member_from_group(user['dn'], group_cn, opts)	remove_user_from_group user = self.get_user_by_uid(user_uid, ['dn', 'uid', 'objectclass'], opts) if user is None:
# todo: refactor things like the augment_punct call </s> output_conllu = f"{tokenizer_dir}/{short_name}.{dataset}.gold.conllu"	build_combined_korean for dataset in ("train", "dev", "test"):
# todo return empty list if not loaded </s> did_you_mean = utils.to_unicode(	did_you_mean suggestion exists. Will always return :class:`None` if the search isn't loaded. lib.sp_search_did_you_mean(self._sp_search)) return did_you_mean if did_you_mean else None
#todo: fix auditor+south </s> for meta in phone_metas:	dashboard for phone in user_phones: phone_metas = time_bound_metadatas.filter(deviceid=phone.phone.device_id)\ user_date_map[meta.timeend.date()] += 1 program_totals_by_date[meta.timeend.date()] += 1
# todo, still to work out this </s> ('offset','<q=0'),	NDRUniConformantVaryingArray ) commonHdr64 = ( ('ActualCount','<Q=len(Data)'), )
# todo: need to kill db connections in order to drop database </s> yield app	app ) app_context = app.test_request_context() app_context.pop()
# todo scope the cleanup to the current project - https://github.com/lyft/cartography/issues/381 </s> response_objects.append(res)	get_gcp_instance_responses for zone in zones: req = compute.instances().list(project=project_id, zone=zone['name']) return response_objects
# todo: slow </s> return [vector[0], vector[1]]	WriteVector2D
# todo error reporting over the master event bus </s> except exception as exc:	ClearFuncs return dict(error=dict(name=exc.__class__.__name__, args=exc.args, log.error( 'Exception occurred in the runner system: {0}'.format(exc)
# steps = 0 # todo </s> self.count += 1	Duplis self.count = 1 def add(self, matrix):
# todo(devcamcar): how to assert this succeeded? </s> tenant_name='bar')	foo_client def foo_client(self): return self._client(username='FOO',
# todo: handle the `scoped` attribute </s> title=link.get('title'))	find_link_stylesheet_elements ): continue
# todo stub </s> (token,)	_get_as_id_txn def _get_as_id_txn(self, txn, token): cursor = txn.execute( ) res = cursor.fetchone()
#todo:  we make render response return a string if passed a string?? </s> if not error_handler: raise	route pylons.c.tg_errors = inv.error_dict pylons.c.tg_values = params if isinstance(error_handler, basestring): error_handler_absolute_url = urlparse.urljoin(controller_url, error_handler)
#todo: dataset/hda by id (from history) or check_ownership for anon user </s> api_type = "file"	_summary_hda_dict 'type'  : < name of the dataset >, 'url'   : < api url to retrieve this datasets full data >, encoded_id = trans.security.encode_id( hda.id ) return {
# todo: work out a nice fix for this failure. </s> before the next meaningful code segment.	Rule_L007 whether the last code segment was an operator or not. Anchor is our signal as to whether there's a problem. anchor = None if parent_stack and parent_stack[-1].is_type("expression"):
1  # todo: fill in identifier </s> calls=stat.ncall,	print_stats line = ProfileLine( recursion=recursion, cumulative=stat.tsub, total=stat.ttot,
# todo (server): process request, send response </s> self.ui.showerrormessage("failed to create the controlled room suffix for the following reason: {}.".format(errormsg))	controlledRoomCreationError
# todo: errors </s> elif isinstance(c, gtk.label):	set_label if isinstance(c, Gtk.Container): if set_label(c, message): c.set_markup(message) return True
# todo(amotoki): due to neutron bug 1378525, neutron disables </s> messages.success(request, msg)	UpdateForm **params) msg = _('Router %s was successfully updated.') % data['name'] return router except Exception:
# todo: implement purge manually or call it on the command line </s> self._repo.pull(remote)	checkout_remote_branch def checkout_remote_branch(self, remote, branch):
#todo: a single softmax'd vector?? </s> b = tensor.zeros_like(x[0,:])	crossentropy_softmax_1hot return crossentropy_softmax_1hot_with_bias(x, b, y_idx, **kwargs)
# todo this context is probably not right. </s> the return value will always be true.	py__bool__ def py__bool__(self): return True
# * todo heading 1 --> </s> todo.toggle_todo_state()	test_toggle_todo_with_no_heading def test_toggle_todo_with_no_heading(self): self.assertEqual(vim.current.buffer[0], '') Todo.toggle_todo_state()
# todo find out what is best used here! </s> n_estimators = constant("n_estimators", 100)	get_hyperparameter_search_space name="learning_rate", lower=0.0001, upper=1, default=0.1, log=True) subsample = UniformFloatHyperparameter( max_features = UniformFloatHyperparameter( "max_features", 0.5, 5, default=1)
## todo: remove shared kwargs </s> resources = self.apigclient.get_resources(restapiid=apiid)['items']	get_lambda_function_names def get_lambda_function_names(self, apiId, stage): for resource in resources: if 'resourceMethods' in resource:
# todo: handle winddownset event (see #1193) </s> contract_function = self.contract.functions.deposit(amount, lock_periods)	deposit_tokens @validate_checksum_address receipt = self.blockchain.send_transaction(contract_function=contract_function, sender_address=sender_address)
# todo cache? </s> df = as_dataframe(measurements(), schema=measurement)	dataframe dataframe().plot()
# todo: update consumer (agent) </s> dist = mgr.get_distributor(repo_id, distributor_id)	__distributor @return: The found model object. @raise MissingResource: when not found. if dist is None: raise MissingResource('/'.join((repo_id, distributor_id)))
raise skiptest("buggy")  # todo(mattjj): fix </s> return x + x.t	testTransposeAndAddRank3 def testTransposeAndAddRank3(self): x = onp.reshape(onp.arange(8., dtype=onp.float32), (2, 2, 2)) expected = fun(x)
# @todo: pheonix </s> self.populateskilltree()	delaySearch def delaySearch(self, evt): else: self.searchTimer.Stop()
# todo also test these! </s> else:	test_all_estimators with warnings.catch_warnings(record=True) as w: if E in meta_estimators: e = E() clone(e)
# todo: remove cache clearing once upstream issues regarding non-batch </s> return loss	closure bayes_optimizer.zero_grad() loss = -acquisition_function(candidates).sum()
#todo - mutableseq? </s> represent that with this class we set position as 1 and the	BetweenPosition o extension - The range to the other position of a boundary. This specifies a coordinate which is found between the two positions. extension as 1. def __init__(self, position, extension = 0):
# todo: handle more complex metric specifications and labeling </s> self.process_criteria_logic(item['ts'], val)	monitoring_data for item in data: val = self.get_value(item)
# todo(b/160795287): deprecate estimator based executor. </s> eval_spec = tf.estimator.evalspec(	trainer_fn train_input_fn, max_steps=trainer_fn_args.train_steps) serving_receiver_fn = lambda: _serving_input_receiver_fn(schema) eval_input_fn, steps=trainer_fn_args.eval_steps,
# todo: remove this method in v2.5 </s> )	remove_from_device partition=self.want.partition
# todo add cleanup </s> @_require_installed	DeploymentTests Tests for deploying applications. Similar to http://doc-dev.clusterhq.com/gettingstarted/tutorial/ def setUp(self): This is an alternative to
# todo: should prob be called before update? </s> self._hyperparams['ent_reg'] = ent_reg	set_ent_reg
# todo: remove this log statement when invoking this method on each iteration of the goal state loop (currently it is invoked only on a new goal state) </s> error = ("agent supported wire protocol version: {0} was not "	check_wire_protocol_version logger.info("Wire protocol version:{0}", PROTOCOL_VERSION) logger.info("Server preferred version:{0}", preferred) "advised by Fabric.").format(PROTOCOL_VERSION) raise ProtocolNotFoundError(error)
# todo per-sync cached results </s> def is_stopping():	SyncBase @staticmethod
# todo: avoid dummy and generate func here when inlining is possible </s> if len(df.columns) == 0:  # empty df	df_len_overload @overload(len)  # TODO: avoid lowering? return lambda df: 0 return lambda df: len(df._data[0])
if not is_old_django: # todo: remove when pre-csrf token templatetags are no longer supported </s> html = template.render(c)	test_uni_form_helper_inputs template = get_template_from_string(""" {% load uni_form_tags %} self.assertTrue('class="submit submitButton"' in html) self.assertTrue('id="submit-id-my-submit"' in html)
# todo:liberate - move this to a more generalized tag enhancement package? </s> data.file = scaled_file	ReplacingImageWithThumbField raise ValidationError(_('Cannot process image'))
#todo(cp16net): need to set the return code correctly </s> self._instance_router(mapper)	API def __init__(self): mapper = routes.Mapper() def _instance_router(self, mapper): instance_resource = InstanceController().create_resource()
# todo: modifiers </s> self.update()	_pyvis_update
#@todo: remove in 0.4.10 </s> for p in xrange(2, pages + 1):	handleMultiPages pages = int(m.group(1)) except: self.html = self.loadPage(p) self.package_links += self.getLinks()
# todo: handle fancy-index copies by allocating a buffer and </s> if isinstance(mode, basestring) and mode not in _iteration_schemes:	resolve_iterator_class raise ValueError("unknown iteration mode string: %s" % mode) elif mode in _iteration_schemes:
weigts (float): weights #todo: batched? </s> args:	cost@26 weigts (float): weights #todo: batched? return np.abs(circuit(*weights)-1)
#temporarily select a random music file to play. todo: replace with proper playlist </s> self._dobreak = true	breakLoop def breakLoop(self, returnValue = None): @param returnValue:
# :todo: implement test. </s> def test_fail_addresses_wrong_type(self):	FindTransactionsRequestFilterTestCase self.skipTest('Not implemented yet.') def test_fail_bundles_contents_invalid(self): self.skipTest('Not implemented yet.') def test_fail_addresses_contents_invalid(self):
# todo: 289 </s> signing_power = self._crypto_power.power_ups(signingpower)	generate_self_signed_certificate return signing_power.generate_self_signed_cert(self.stamp.fingerprint().decode())
# todo: s_vectors[sent_adr] += eps </s> lang_freq : str, optional	Average workers : int, optional Number of working threads, used for multithreading. For most tasks (few words in a sentence) Some pre-trained embeddings, i.e. "GoogleNews-vectors-negative300.bin", do not contain information about the frequency of a word. As the frequency is required for estimating the word weights, we induce
# @todo: pheonix </s> self.populateskilltree()	delaySearch def delaySearch(self, evt): else: self.searchTimer.Stop()
# todo: fix populus support this via an deploy argument </s> issuer = issuer(address=issuer_address)	main@84 print("Issuer verified contract is", link) else: print("Issuer contract is", issuer.address) print("Currently issued", issuer.call().issuedCount())
# todo(asalkeld) support versions </s> sh.monitor_services()	cfn_hup self._config = self._metadata["config"] s = self._config.get("services")
# todo generator </s> doc="path/name of the component to be dropped",	Drop path=Parameter( args=("path",), nargs="*", constraints=EnsureStr() | EnsureNone()),
# todo(tetsuro): remove this or condition when all </s> cols = [	_get_allocations_by_consumer_uuid consumer = sa.alias(_CONSUMER_TBL, name="c") project = sa.alias(_PROJECT_TBL, name="p") allocs.c.id, allocs.c.resource_provider_id,
# todo return xml fragment </s> return { 'service': urlquote_plus(service) }	LoginView def get_initial(self): service = self.request.GET.get('service')
# todo alert? </s> dist_info = os.path.join(	agent_name def agent_name(self, agent_uuid): agent_path = os.path.join(self.install_dir, agent_uuid) agent_path, agent_name, agent_name + '.dist-info') if os.path.exists(dist_info):
# todo: py3 branch fails here </s> 11	60636	60736	srr081241.13799221/1	0	+	test_remote_bam break results = pybedtools.BedTool(gen()).saveas() 11	60674	60774	SRR077487.5548889/1	0	+ 11	60684	60784	SRR077487.12853301/1	0	+
# todo: raise </s> tokens.skip(tokentype.symbol, "=")	_parse_style_name def _parse_style_name(tokens): if tokens.try_skip(TokenType.SYMBOL, "["): name = parse_string(tokens) tokens.skip(TokenType.SYMBOL, "]")
# todo: is this test is writing to the default system directory and ignoring updates to the passed filepath? </s> return filepath	custom_config_filepath @pytest.fixture(scope='module') def custom_config_filepath(custom_filepath):
# todo: log exception </s> }	add_tag@357 "lang": "painless", "params": { } }
# todo inference in python now missing postprocessing glue code </s> from detectron2.utils.analysis import _flatten_to_tuple	WrapModel outputs = outputs.get_fields()
# todo: clean up proposed streams here as well? -dan </s> return false	_cancel_hold notification = Notification('SIPInvitationChangedState', e.invitation, e.data) notification.center = notification_center return True
# todo unordered float </s> dst.zeroextend(instr.mode))	jno n = m2_expr.ExprId(ir.get_next_label(instr), dst.size) dst_o = m2_expr.ExprCond(of, e.append(m2_expr.ExprAff(meip, dst_o)) e.append(m2_expr.ExprAff(ir.IRDst, dst_o))
raise notimplementederror # todo </s> return [prop[0] for prop in cur.description]	get_storage_properties def get_storage_properties(self):
downsized_img = transform.resize(gray_img, (84, 84), mode='constant')  # todo: check resizing doesn't cause problems </s> gray_img = color.rgb2gray(state)  # todo: check image conversion doesn't cause problems	_state_to_tensor@9 downsized_img = transform.resize(gray_img, (84, 84), mode='constant')  # TODO: Check resizing doesn't cause problems return torch.from_numpy(downsized_img).float()  # Return 2D image tensor
# todo: make this more portable with shutil etc. </s> self.clone.call("commit", "-a", "-m", filename)	_createCommitNewFile phlsys_subprocess.run_commands( "touch " + os.path.join(self.path, filename))
# todo(andym): delete this once personas are migrated. </s> return cls._meta.get_field('default_locale')	get_fallback @classmethod
# todo don't- delete if track is on local nets </s> pivot_drawings.append(drawing)	get_drawings if bounding_box.Intersects(dwg_bb):
# todo append masters as named-instances as well; needs .designspace change. </s> warnings.warn("glyph %s has incompatible masters; skipping" % glyph)	_add_gvar allControls = [d[1] for d in allData] control = allControls[0] continue del allControls
# todo: log discarded bytes? </s> calculated_crc = crc16.crc16xmodem(''.join([chr(item) for item in message[:-2]]))	valid_crc def valid_crc(self, message): return supplied_crc == calculated_crc
# todo(guillermooo): implement a vs a register. </s> super().__setitem__(key.lower(), value)	MacroRegisters def __setitem__(self, key, value): if key in ('%', '#'): def __getitem__(self, key): if key in ('%', '#'):
# todo: support aggregation functions sum, count, etc. </s> assert isinstance(df_var, ir.var)	_get_renamed_df return df_var
# todo: move to base class </s> arr1 = [i[0] for i in rectangles]	getNodesRect continue n_rect = QtCore.QRectF(n.scenePos(), QtCore.QPointF(n.scenePos().x() + float(n.w), n.scenePos().y() + float(n.h))) arr2 = [i[2] for i in rectangles] arr3 = [i[1] for i in rectangles]
pattern = re.compile(".*-\s"+"(\[[\sx]\]).*") # pattern for a markdown todo-list () </s> db.session.rollback()	generate_fake try: db.session.commit()
# todo: order of attributes is not assured; allow for any order. </s> from music21.musicxml import testprimitive	testBracketA def testBracketA(self): s = converter.parse(testPrimitive.directions31a) raw = fromMusic21Object(s)
# todo: fix self.cursor_x >= w </s> self.comm = self.comm[:i]	inline_k_ctrl_k def inline_k_ctrl_k(self, xbegin, i, w):
# todo pydocs </s> return bigqueryhook(bigquery_conn_id=self.bigquery_conn_id)	get_db_hook
# todo - verify exit code is 0. using check_output() hangs, not sure why. tried shell=true which doesn't help </s> def test_quiet_mode(self):	test_quiet_mode
return deserialize(self.binary, from_bytes=true)  # todo: techdebt fix </s> self.binary = serialize(value, to_bytes=true)  # todo: techdebt fix	BinObject@20 def object(self, value):
# todo is this necessary? what if an error is raised within the generator? </s> _generator_to_fasta(	_sequence_to_fasta description_newline_replacement, max_width): def seq_gen(): seq_gen(), fh, id_whitespace_replacement=id_whitespace_replacement, description_newline_replacement=description_newline_replacement,
# todo is there a way to actually test that the creds work? </s> def build_conda_packages():	build_conda_packages @build_wrapper('conda')
# todo: hack </s> materials = self._container_registry.findinstancecontainers(id=container_id)	ThreeMFWorkspaceReader material_container_files = [name for name in cura_file_names if name.endswith(self._material_container_suffix)] for material_container_file in material_container_files: material_labels.append(self._getMaterialLabelFromSerialized(archive.open(material_container_file).read().decode("utf-8"))) if materials and not materials[0].isReadOnly():  # Only non readonly materials can be in conflict
# todo: modifiers </s> def _pyvis_update(self):	_pyvis_update
if dt_def is none:  # todo: check for errors </s> s = b[i]	_column_fillna_impl def _column_fillna_impl(A, B, fill):  # pragma: no cover if hpat.hiframes_api.isna(B, i): s = fill
# todo: how to handle not found authorname </s> @login_required	authenticate_google_drive @admin_required def authenticate_google_drive():
# todo: may test file contents </s> table = rows.import_from_csv(self.filename, encoding=self.encoding)	PluginCsvTestCase self.assertIs(rows.import_from_csv, rows.plugins.csv.import_from_csv) self.assertIs(rows.export_to_csv, rows.plugins.csv.export_to_csv) self.assert_expected_table(table) def test_import_from_csv_fobj(self):
raise notimplementederror # todo </s> for s in self.states_list:	resample_states def resample_states(self):
# todo: catch unacceptable types (str, dict, etc) to avoid errors for other.child below </s> def scalar_manager(self) -> typescalarmanager:	scalar_manager @property
# todo: temporary hack until they fix </s> link = f'<a class="image" href="{src}" target="_blank">'	test_extra_image_separated result, html = run(testdir) assert result.ret == 0 assert link in html assert os.path.exists(src)
# todo: this isn't going to work right.  when we do incremental </s> try:	index_post if es is None: es = elasticutils.get_es() es.index(post, index, doc_type=Post.ElasticMeta.type, id=post['id'], bulk=bulk, force_insert=force_insert)
# todo, this is not working if the action is not active (nla case for example) </s> self.__out_tangent = self.__set_indexed(value)	out_tangent @out_tangent.setter
# todo: use shape inference to figure out how large of an array </s> def transform_reduce(self, expr):	transform_Reduce
# todo remove above two lines </s> add(dataset=dataset(sys.argv[1]), path='fromproc.txt')	test_procedure_discovery@110 from datalad.api import add, Dataset with open(op.join(sys.argv[1], 'fromproc.txt'), 'w') as f: def test_procedure_discovery(path): ps = run_procedure(discover=True)
# todo (elliot): put this in the preferences. </s> prefs["recipecreatelocation"],	generate_jss_recipe facts["app_name"] + ".png") else: facts["app_name"].replace("/", "-"), facts["app_name"] + ".png")
raise notimplementederror # the below does most probably not work anymore todo </s> if not a_to_b and not b_to_a:	detail return "It does not differ from `{0}`.".format(b) elif not a_to_b:
raise exceptions.mpdnotimplemented  # todo </s> already. the name may consist of alphanumeric ascii characters plus	subscribe@18 *musicpd.org, client to client section:* ``subscribe {NAME}`` underscore, dash, dot and colon. raise exceptions.MpdNotImplemented  # TODO
#remove the already-done entry (this connects to the other todo, </s> if res[0]['confirms'] > 0:	restart_waiter res = jm_single().bc_interface.query_utxo_set(txid, includeconf=True) if not res[0]: break log.info("The last transaction is now in a block; continuing.")
# todo: other types like boolean </s> typ_val = _h5_typ_table[arr_typ.dtype]	get_type_enum_overload def get_type_enum_overload(arr_typ):
# todo: add broadcasting to get_rotation_matrix2d for center </s> return rotate(input, self.angle, self.center)	Rotate self.angle: torch.Tensor = angle self.center: Union[None, torch.Tensor] = center def __repr__(self): return self.__class__.__name__ + '(' \
return ""  # todo: followup after decision around returning none </s> pass	_FILE_OBJECT try: name += self.FileName.String return name
# todo move this to augmenters.size </s> imgaug.augmentables.heatmaps.heatmapsonimage	quokka_heatmap See :func:`imgaug.imgaug.quokka`. Returns Depth map as an heatmap object. Values close to ``0.0`` denote objects that are close to the camera. Values close to ``1.0`` denote objects
# todo(denero) fix user plumbing using @requires_authenticated_user </s> submission =  models.submission(parent=user_key, submitter=user,	SubmitNDBImplementation return list(models.Assignment.query().filter(by_name)) def create_submission(self, user, assignment, messages): assignment=assignment, messages=messages)
# todo update docstring </s> ]	runSSH command = [ b'ssh', if key is not None: command.extend([
# todo(elliot): what info do we need for this recipe type? </s> else:	create_existing_recipe_list for line in out.split("\n"): if search_term in line: print err sys.exit(exitcode)
# todo: the following skipped suite and fixtures should be enabled </s> provider = provider	VultrProviderTests provider_name = 'vultr' domain = 'capsulecd.com'
# todo: handle this case </s> self.indenter.correct_indentation(3)	test_returning_after_backslashes def test_returning_after_backslashes(self): self.assertEquals('c = \\\n    b\na', self.editor.get_text())
# todo: create xxx_failure test </s> tests = exclude_from_resultlist(r, 'failure')	test_result_space_failure p = op.join(app.config['ROOT'], 'tests/fixtures/ttf/Font-Light!.ttf') r = run_set(p, 'result') self.assertTrue(check('test_space', failure_tests), lookup('test_space', tests))
#todo: this can probably just be removed now? </s> return name	mash_attrib
# todo: what if there weren't enough contracts approved to distribute n kfrags?  we need to raise notenoughqualifiedursulas. </s> return msgpack.dumps(self.ids)	packed_payload
# todo remove comment parameter. </s> return cast_int(ord(value), size)	cast_char
# todo: add highlighting line </s> item = qstandarditem(function_name.replace('.', '_'))	update_functions functions_list[function.name] = function.address for function_name in sorted(functions_list.keys()): item.setData(function_addr, Qt.UserRole + 2) self.functions_list_model.appendRow([item])
# todo(lyarwood): test drivervolumeblockdevice.driver_detach in </s> self.requested_networks, none, self.security_groups, none)	test_build_networks_if_not_allocated mock_allocate.assert_called_once_with(self.context, instance,
tokenize_with_offsets=not use_sp_model,  # todo(b/181866850): drop this. </s> " ".join(["--{}={}".format(k, v) for k, v in flags.items()]))	_make_sp_model_file SentencePieceTrainer.Train(
# todo: remove unescape_entities when mako html safe comes in </s> delete_index(index)	delete_all @requires_search
#todo: fix auditor+south </s> program_map = sorteddict()	dashboard .filter(formdefmodel__domain=request.user.selected_domain) found_meta_ids = [] program_totals_by_date = {} for date in dates:  program_totals_by_date[date] = 0
# todo: extract to _tmp and then move in a single command so we </s> def is_extracted(self):	is_extracted return exists(self.path) and exists(self.stamp_path) \ and os.stat(self.stamp_path).st_mtime >= os.stat(self.path).st_mtime
"""todo doc me""" </s> >>> from petl import fields	fields def fields(table): >>> table = [['foo', 'bar'], ['a', 1], ['b', 2]] >>> fields(table)
# todo verify </s> pass	Error
# todo: add a 'comment' to the calculation </s> "the aida user {} is not configured more than once to use computer {}! "	get_authinfo aidauser.username, computer.hostname)) except MultipleObjectsReturned: "only one configuration is allowed".format( aidauser.username, computer.hostname))
'''todo: add docs''' </s> def discrete_columns(self):	discrete_columns return [x for x in self.columns if x['type'] == 'DiscreteColumn']
# todo: check arp reply is valid </s> self.tmpdir = tempfile.mkdtemp()	setup_valve self.config_file = os.path.join(self.tmpdir, 'valve_unit.yaml') self.faucet_event_sock = os.path.join(self.tmpdir, 'event.sock')
# todo: here we should check for the leverage based on the config value </s> for index, item in enumerate(self.sell_orders[base_asset]):	on_order_cancellation self.buy_orders[base_asset][index] = np.array([0, 0]) break if item[0] == order.qty and item[1] == order.price: self.sell_orders[base_asset][index] = np.array([0, 0])
# todo created_at? </s> res[dt] = subs	get_states subs = parse_file(f)
# todo curves </s> pass	hvcurveto args = next(it) yield ('rrcurveto', [0, args[0], args[1], args[2], args[3], 0]) if last_args: args = last_args
conv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1), # todo: change to kernel_size=1, padding=0? </s> candidate_size=candidate_size,	create_mobilenetv1_ssd_predictor config.center_variance, config.size_variance, nms_method=nms_method, sigma=sigma, device=device)
time.sleep(5)  # todo: for some reason, events do not trigger instantly </s> self.assertisnotnone(watermark)	test_pull_subscribe def test_pull_subscribe(self): subscription_id, watermark = self.account.inbox.subscribe_to_pull() self.account.inbox.unsubscribe(subscription_id)
# todo ... </s> arg = arg.strip()	cpreprocess_evaluate_ifdef if not is_valid_defname(arg): state.error("preprocessor: '" + arg + "' is not a valid macro name")
#todo: kvick we should rename 'short_circuit' to something like 'disable_service_start' </s> download_file(pkg_url, dst_file_path, context.package.get('timeout', 1))	_download_pkg pkg_url = context.package.arg dst_file_path = context.package.full_path
# todo: the stuff </s> def plugin(self):	SimpleTaskPersistence self.task = task self.taskname = task.name return self.task.current_plugin
return  #todo disabled for now, see #2151 for details </s> except runtimeerror:	sanity_checker def sanity_checker(string): try: return False else:
# todo: push stream to experiment exchange </s> return os.path.join(self.folder_name, 'polyaxon_requirements.txt')	_get_requirements_path if os.path.isfile(requirements_path):
# todo: remove force_masquerade parameter in future release </s> remove a specific port from a zone.	remove_port .. versionadded:: 2015.8.0 CLI Example:
# todo(b/148082271): remove this line once tft 0.22 is used. </s> for key in _vocab_feature_keys:	preprocessing_fn for key in _DENSE_FLOAT_FEATURE_KEYS: outputs[_transformed_name(key)] = tft.scale_to_z_score( outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary( _fill_in_missing(inputs[key]),
# todo other source types </s> null=true,	Plan help_text="The frequency with which a subscription should be billed." ) help_text="The number of intervals (specified in the interval property) between each subscription billing." )
# todo: add dirty file </s> cache_path = self.cache._get_path_or_dummy(fuse_path)	using_cache_path_or_dummy print(func, fuse_path, args, kwargs)
# todo: i can't manage the import issue, can you? </s> self.owner = owner	LogTensor the tensor is located. id: An optional string or integer id of the LogTensor. self.id = id self.child = None
raise notimplementederror  # todo </s> raise notimplementederror  # todo	Perturbation pass raise NotImplementedError  # TODO def generator(*args, **kwargs): raise NotImplementedError  # TODO
dot_product_threshold = 0.95 # todo(ntonci): add to parameters </s> for i in range(0, n_samples):	compute_dual_quaternions_with_offset n_samples = len(dq_B_H_vec) dq_W_E_vec = [] dq_B_H = dq_B_H_vec[i] dq_W_E = dq_W_B * dq_B_H * dq_H_E
# todo: make a tests that asserts that an observer can receive updates from the hyperparamsjsonrepository </s> self.events: list[trial] = []	SomeObserver class SomeObserver(_Observer[Tuple[HyperparamsRepository, Trial]]): def on_next(self, value: Tuple[HyperparamsRepository, Trial]): self.events.append(value)
# todo special case for b=c? </s> if a == m: return 0.0	Ka k = D[a][m]**2 + D[b][c]**2/4.0 - (D[a][b]**2 + D[a][c]**2)/2.0 k /= D[a][m]
# todo: proper distinction between text and bytes. </s> def set_state(self, state):	set_state
# todo: this can unnecessarily suspend the starting of a build, in </s> d.addcallback(get_brs)	BuilderControl brids[self.original.name], self.master.master.status) return d def rebuildBuild(self, bs, reason="<rebuild, no reason given>", extraProperties=None):
# todo (rtibbles): sort out the status of quizzes, and either reinstate them or remove them. </s> score = 0	PlaylistProgressParent else:
# todo: remove the check for deprecated check_docs after the extension has been removed </s> sys.exit(0)	cb_full_documentation def cb_full_documentation(self, option, optname, value, parser):
# todo: support auto-alignment, need a context object for this, eg: </s> self.stream.write("{:>{align}}       {:9} {} -> {}\n".format(	VarsDumper first = True for key, value in event.locals.items(): "", "vars" if first else ".",
# reasons why we said no. todo: allow configurable error messages </s> return self.store.get_room_events_max_id(room_id)	get_current_key_for_room
# todo?: self.assert_eq(kdf.loc['a':'o', 'b':'d'], pdf.loc['a':'o', 'b':'d']) </s> self.assert_eq(kdf.loc['a':'o', 'a'], pdf.loc['a':'o', 'a'])	test_loc2d_with_known_divisions kdf = ks.from_pandas(pdf) self.assert_eq(kdf.loc[['a'], 'A'], pdf.loc[['a'], 'A']) self.assert_eq(kdf.loc['a':'o', ['A']], pdf.loc['a':'o', ['A']]) self.assert_eq(kdf.loc[['n'], ['A']], pdf.loc[['n'], ['A']])
# todo : test with/without leave_one_out </s> corpus_df = pke.utils.load_document_frequency_file(str(corpus_df_file))	create_df corpus_df_file = tmp_path / name pke.utils.compute_document_frequency( return corpus_df, corpus_df_file
# todo: implement </s> with self._lock:	_check_audio self._no_audio_timer = None if not self.audio_was_received:
''' todo: test failing as of 2015/06/03 and needs to be completed. ''' </s> self.fail("not implemented")	Test_AcquireTokenWithUsernamePassword print(tokenResponse)
headers = csv_reader[0] # todo check size </s> result = download_to_file.asyncresult(notebook['uuid'])	_patch_status result.backend.store_result(notebook['uuid'], None, "SUBMITTED")
# todo remove in v8 </s> if record.levelno >= logging.warning:	StrictModeExceptionHandler def emit(self, record: logging.LogRecord) -> None:
# :todo: implement test. </s> self.assertnotequal(tryte[2], trit(-1))	test_init_mixed_types tryte = Tryte([Trit(1), 1, -1]) self.assertEqual(tryte[0], Trit(1)) self.assertEqual(tryte[2].value, -1)
#todo: maybe it could be better as a background task </s> def cleanupnonces(cls):	cleanupNonces expired = cls.query().filter(cls.expiration_date <= datetime.datetime.now()).fetch(keys_only=True) ndb.delete_multi(expired)
# todo: replace with below line when numba supports np.isin in nopython mode </s> result = pandas.series(self._data[idx])	hpat_pandas_series_getitem_idx_slice_impl **Test**: python -m hpat.runtests hpat.tests.test_series.TestSeries.test_series_iloc2
# todo: use correct priority instead of 0 </s> " to execopcode (%s)" % type(op))	Processor amount of time if not isinstance(op, opcodes.OpCode): lu_class = self.DISPATCH_TABLE.get(op.__class__, None) if lu_class is None:
# todo: remove this skip after fixing </s> rpolygon = visuals.regularpolygon(pos=(0., 0.), radius=0.4, sides=8,	test_regular_polygon_draw2@55 if sys.version[0] == 3: raise SkipTest color=(0, 0, 1, 1)) rpolygon.transform = transforms.STTransform(scale=(75, 100),
# todo add locales </s> logger.warning("%s" % e, exc_info=1)	domain_main_domain write_to_file("/etc/yunohost/current_host", new_main_domain) _set_hostname(new_main_domain) raise YunohostError("main_domain_change_failed") app_ssowatconf()
# todo in the future it's possible we'll want to break this out by action_type, in order to track </s> period.close_out(tx)	compute_consumption is_checkpoint = (base_action_type == 'stockonhand' and not is_stockout) if is_checkpoint: yield period period = ConsumptionPeriod(tx)
@deprecated_alias(net='prev_layer', end_support_version=1.9)  # todo remove this line for the 1.9 release </s> a unique layer name.	SubpixelConv1d act : activation function The activation function of this layer. Examples ----------
# todo: make pull request to get this custom vgg feature accepted </s> upsample_filter_np = bilinear_upsample_weights(upsample_factor,	FCN_32s@44 mean_centered_image = _mean_image_subtraction(image_float, [_R_MEAN, _G_MEAN, _B_MEAN]) number_of_classes) upsample_filter_tensor = tf.constant(upsample_filter_np)
# todo: check the status before chaging the port </s> </ribcl>""".format(username)	get_user </LOGIN>
# todo(emfree): remove after status overhaul. </s> auth_code = none	GmailAuthHandler return auth_response except OAuthError:
# todo: site-wide announcements. </s> request.user, locale=request.locale),	_kb_main 'is_watching_approved': ApproveRevisionInLocaleEvent.is_notifying( request.user, locale=request.locale), 'is_watching_approved_default': ApproveRevisionInLocaleEvent.is_notifying(
# todo use blenddata </s> pass	BpyStructProxy
1  # todo: fill in identifier </s> total  - total wall time to run the function call (including subcalls)	print_stats formated_stack.append(formated_line) print('\n'.join(formated_stack)) cumm   - total wall time for the function itself (removing subcalls) single - time spent on a _single_ execution (average time, really)
# todo: implement link-local handling in networkmanager backend and move this test into commontests() </s> addresses: ["172.16.5.3/20", "9876:bbbb::11/70"] # needed to bring up the interface at all	test_link_local_disabled renderer: %(r)s ethernets: link-local: []''' % {'r': self.backend, 'ec': self.dev_e_client}) self.generate_and_settle()
# todo check. </s> def _translate_hlt(self, tb, instruction):	_translate_hlt
# todo: need token </s> if node.tag == bool_expr_e.boolbinary:	_Eval return bool(s) raise NotImplementedError(op_id) op_id = node.op_id s1 = self._EvalCompoundWord(node.left)
# todo: add at least reflection tests before adding notimplemented version </s> ``playlistinfo [[songpos] | [start:end]]``	playlistinfo @protocol.commands.add('playlistinfo') def playlistinfo(context, parameter=None): Displays a list of all songs in the playlist, or if the optional argument is given, displays information only for the song
# todo: need to account for feature engineering here - probably need to have hardcoded test data rather than calculating splits here </s> model_initializer=xgbclassifier,	test_sentinels_optimization def test_sentinels_optimization(env_0): optimizer = GBRT(iterations=2) model_init_params=dict(objective="reg:linear", max_depth=Integer(2, 20), subsample=0.5), model_extra_params=dict(
# todo: this decompose is used because of cache </s> return self.sub(other, copy=false)	__isub__
except exception:  # todo - which exceptions? </s> if organism_element.tag == ns + 'name':	_parse_organismHost for organism_element in element:
preprocessed_state_space=spaces.floatbox(shape=(2,)),  # todo: remove once auto preprocessor space inference done. </s> print('policy weights: {}'.format(policy_weights))	TestApexAgent preprocessed_state_space=env.state_space )
# todo: cleanup </s> self.remote.save_repo(repo, self.token)	createRepo self.remote.modify_repo(repo, "name", "testrepo0", self.token) self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token)
# todo use base64 data </s> else:	DebugProbeServer self._is_running = False if isinstance(probe, SharedDebugProbeProxy): self._proxy = SharedDebugProbeProxy(probe) if port is None:
# todo: total_reward isn't always greater than 95 even with a working implementation </s> bias_init=neon.constantinit(),	small_model ), neon.Affine( activation=neon.Tanh(), axes=(action_axes, )
# todo: maybe change this later to push some more info, not just the </s> if not self.serverorclient and self.tunnel:	communication_win self.elist = [hEvent_sock] self.mlist = [0] hEvent_pipe = win32event.CreateEvent(None, 0, 0, None) # for reading from the pipe overlapped_pipe = pywintypes.OVERLAPPED()
# todo: add support for heteroskedasticsingletaskgp </s> scalar_state_dict = {	model_list_to_batched shape0 = _get_module(models[0], n).shape if not all(_get_module(m, n).shape == shape0 for m in models[1:]): s: p.clone() for s, p in models[0].state_dict().items() if s in scalars }
# todo add options to maodify the sorted by key and the header options </s> self.close()	do_quit print('Thank you for using Poseidon')
# todo: handle this </s> value = str(exception)	from_exception name = type(exception).__name__
1  # todo: fill in identifier </s> if stat.children is not none:	print_stats average=stat.tavg, ) for child in stat.children: if child.name.endswith('switch'):
# todo: keep # pylint: disable=fixme </s> copy.append(self.eventslideroffsets[:])	transferbuttonsto copy.append(self.eventslidervisibilities[:]) copy.append(self.eventslideractions[:]) copy.append(self.eventsliderfactors[:]) copy.append(self.eventquantifieractive[:])
# todo: preallocate! </s> return c1.x0[0] - c2.x0[0]	SortByX0 def mycmp(c1,c2): if c1.x0.size == 2: return c1.x0[1] - c2.x0[1] elif c1.x0.size == 3:
# todo: user info </s> from django.core.handlers.base import basehandler	_install_impl old_get_response = BaseHandler.get_response def sentry_patched_get_response(self, request):
# todo: use weight scaling factor if provided, xavier's default else </s> learning_rates = {}	DATrainer Takes a DenoisingAutoencoder object, a (symbolic) cost function which takes the input and turns it into a scalar (somehow), for parameter in model.params(): lr_name = '%s_lr' % parameter.name
#todo: consider if this should check if the store belongs to a </s> matcher.extendtm(self.unit_set.filter(state=obsolete))	get_matcher def get_matcher(self): from translate.search import match return matcher
start_time_synced_s = none  # todo </s> try:	is_pupil_mobile_recording def is_pupil_mobile_recording(rec_dir: str) -> bool: return info_csv["Capture Software"] == "Pupil Mobile" and "Data Format Version" not in info_csv except KeyError:
# todo improve precision </s> u = 0.6105540	__init__@38 self.degree = 1 r = 0.5878606 v = 0.1109710 A = 0.1856914
# todo: rewrite tests </s> 'startmonth': 'may',	TestUserProfile 'institution': 'another institution', 'department': None, 'startYear': '2001', 'endMonth': None,
# todo: expose from marshal </s> raw_timestamp = data[4:8]	_bytes_from_bytecode number, timestamp and source size along the way. If source_stats is None then skip the timestamp check. raw_size = data[8:12] if len(magic) != 4 or magic != imp.get_magic():
pass #todo: update pupil invisible recording to pupil capture v1.15 format </s> info_csv = utils.read_info_csv_file(rec_dir)	is_pupil_mobile_recording try: return info_csv["Capture Software"] == "Pupil Mobile" and "Data Format Version" not in info_csv
# todo action required that updates the endpoint </s> mac_endpoints = self.sdnc.endpoints_by_mac(device)	_get_endpoints ip_endpoints = self.sdnc.endpoints_by_ip(device) if len(ip_endpoints) > 0: if len(mac_endpoints) > 0: return mac_endpoints
# todo: why the reversal? </s> self.inds=[]	Disk print "Creating disk ..." self.verts=[] self.texcoords=[] self.ttype = GL_TRIANGLES
# todo index.json only if htmlsections in doc key.. </s> directories = {}	nav_directories for doc in self.wrapper.registered_docs(): if self.is_index_page(doc):
# todo: start here </s> times.	_cleanup_local_tmp def _cleanup_local_tmp(self): This particular function removes any local tmp directories added to the list self._local_tmp_dirs
#todo: refractor asap </s> query = project.objects.filter(cond)	get_project_list cond = Q(privacy=Project.PUBLIC) | Q(access__user=user, access__access__gt=Access.NO_ACCESS)
#todo enable again when farm is upgraded incl. the new offset calc </s> assert isinstance(reader, basereader)	test_reader_basic def test_reader_basic(reader):
# todo: better exception handling </s> res = results.mean()	print_results Dataframe with all the results
# todo(lyarwood): test drivervolumeblockdevice.detach in </s> self.requested_networks, none, self.security_groups, none)	test_build_networks_if_not_allocated mock_allocate.assert_called_once_with(self.context, instance,
# update new uniqueid kodi 17 - todo get uniqueid_id for updates from embydb </s> return false	compare_all if not self.compare_boxsets():
# todo_recorders - need to pass in parent info instead of none </s> level : int	_set_solver_print Control printing for solvers and subsolvers in the model. Parameters iprint level. Set to 2 to print residuals each iteration; set to 1 to print just the iteration totals; set to 0 to disable all printing
# todo: test for first revision on last page. </s> res = self.app.get(offset)	test_purge offset = url_for(controller='revision', action='purge', id=None)
# todo add the ability to `git reset --hard` the dataset tree on failure </s> yield get_status_dict(	__call__@151 try: ok_clean_git(ds.path) 'run', ds=ds,
# todo(jflesch): i18n / l10n </s> print "exception while trying to get the number of pages of '%s': %s" % (self.docid, e)	get_nb_pages except Exception, e:
breaks = {}   # todo: support more than one breakpoint per line </s> sock.sendto(ret.encode('utf-8'), sockaddr)	InfoBreakpoints breaks[int(m.group(2))] = int(fields[0]) ret = json.dumps(breaks)
# todo: verify that workid is the primary key someplace. </s> @wraps(thunk)	abstract The decorated function is abstract. @note: only methods are currently supported. def inner(cls, *a, **k): raise NotImplementedError(qual(cls) + " does not implement " +
# todo: test me. </s> self.view.run_command('maybe_mark_undo_groups_for_gluing')	eval_full_command vi_cmd_data = self.parse_action(vi_cmd_data) if not vi_cmd_data['is_digraph_start']: self.view.run_command('vi_run', vi_cmd_data) self.reset()
# todo: process the remaining tag types. </s> if captures['tag'] == '!':	_handle_match@77 captures['whitespace'] = '' print captures['name'] pass elif captures['tag'] in ['{', '&']:
# todo add inv parameter? </s> images_normalized, random_state_derived, parents + [self],	_augment_all_channels_histogram_equalization return self.all_channel_histogram_equalization._augment_images(
# todo: should allow multi_dimensional inputs/outputs </s> return mask	get_mask shape = x.get_shape() mask = checkerboard(shape[1:], parity=self.parity, dtype=dtype)
# todo: change to nih:sha-256; hashes </s> _logger.info(u"[provenance] added packed workflow: %s", path)	packed_workflow f.write(packed.encode("UTF-8"))
# todo(py3.7): add required=true </s> self.comb += [	UARTSubtarget in_fifo.we.eq(self.uart.rx_rdy), self.uart.rx_ack.eq(in_fifo.writable) self.uart.tx_data.eq(out_fifo.dout), out_fifo.re.eq(self.uart.tx_rdy),
# todo partially update stored playlists? </s> u'callback called: message for track %d in playlist '	track_message_changed def track_message_changed(self, playlist, position, message, userdata): u'"%s" changed to "%s"', position, playlist.name(), message)
# xxx todo alignement check </s> e = []	jno meip = mRIP[instr.mode] n = m2_expr.ExprId(ir.get_next_label(instr), dst.size)
raise  # todo: what if our seed node fails verification? </s> endpoint = 'https://{}/kfrag/{}/reencrypt'.format(work_order.ursula.rest_interface, id_as_hex)	send_work_order_payload_to_ursula def send_work_order_payload_to_ursula(self, work_order): payload = work_order.static_payload() return requests.post(endpoint, payload, verify=work_order.ursula.certificate_filepath)
# todo this thing is too wide </s> return engine == "luxcore" and (context.material and not context.material.luxcore.use_cycles_nodes)	LUXCORE_PT_material_presets @classmethod def poll(cls, context): def draw(self, context): layout = self.layout
# todo: prepopulate </s> push_mock.assert_called_with(bar)	test_sync_child_entities only_theirs_mock.assert_called() pull_mock.assert_called_with(DOMAIN, foo)
# todo(mriedem): remove this conversion when all neutronv2 apis are </s> 'ovs_interfaceid': 'ovs_interfaceid',	test_get_instance_nw_info_remove_ports_from_neutron 'address': 'mac_address', 'network': network_model, 'devname': 'devname'}]}} self._fake_get_instance_nw_info_helper(network_cache,
# todo: expect_match should work with emit() </s> def __init__(self, params, *args, **kwargs):	TokenCommandExit super().__init__(params, TOKEN_COMMAND_EXIT,
# todo(mordred) when this changes to rest, force interface=admin </s> except ironic_exceptions.clientexception:	get_nic_by_mac _tasks.MachineNodePortGet(port_id=mac))
# todo: there is probably a robuster way than a sleep. </s> @api.marshal_with(launch)	Launch Individual pipelines can only be launched once, therefore the UUID of a pipeline also uniquely identifies a launch. def get(self, pipeline_uuid): launch = models.Launch.query.filter_by(pipeline_uuid=pipeline_uuid).first_or_404(
# todo check </s> def id():	id @interfacedoc
# todo: create xxx_failure test </s> p = op.join(app.config['root'], 'tests/fixtures/ttf/font-light!.ttf')	test_result_space_failure r = run_set(p, 'result') failure_tests = r['failure']
# todo action required that updates the endpoint </s> if name_endpoint:	_get_endpoints eps = [] device = args.rsplit(' ', 1)[idx] eps.append(name_endpoint) return eps
1  # todo: fill in identifier </s> channel1, deposit1, [transfermessage.lock],	test_settlement channel0, deposit0, [],
# todo: verify the key is rsa </s> if emlen<len(digestinfo)+11:	EMSA_PKCS1_V1_5_ENCODE digest = hash.digest() digestAlgo = DerSequence([hash.oid, DerNull().encode()]) raise ValueError("Selected hash algorith has a too long digest (%d bytes)." % len(digest)) PS = "\xFF" * (emLen - len(digestInfo) - 3)
#create and insert todo on remote site </s> master_doc = self.insert_into_master(master, 'test delete sync')	TestNodeConfiguration self.assertEqual(local_doc.description, master_doc.description) def test_delete(self): local_doc = frappe.get_doc(master_doc.doctype, master_doc.name) self.assertNotEqual(local_doc, [])
# todo(akshakya): validate shapes of params. </s> grad = [torch.cat(g, 0) for g in grad]	_CvxpyLayerFn for j, p in enumerate(param_order): grad[j] += [to_torch(del_param_dict[p.id], info['dcanon_time'] = time.time() - start if not ctx.batch:
# todo: move log_pi and correction under self.log_pi_for() </s> if not self._squash: return 0	_squash_correction def _squash_correction(self, actions):
# todo: remove when old stats are removed </s> assert store0.data_tool.context is store0	test_data_tool_store assert data_tool.get() is None assert data_tool.get(store0.__class__) is StoreDataTool
# todo(py3.7): add required=true </s> pass	INA260Error
# todo: handle agg_columns. </s> return _col(super(seriesgroupby, self).idxmin(skipna))	idxmin
# todo? don't consider the empty set here </s> 'immediately.', context)	big_acmip return _null_ac_bigmip(context, direction) if not connectivity.is_weak(context.network.cm, context.node_indices): return _null_ac_bigmip(context, direction) cuts = _get_cuts(context)
# todo: remove this when fixed in: https://github.com/seleniumhq/selenium/issues/767 </s> self.login(self.browser, 'bad-username', 'bad-username')	test_auth_failures self.percy.snapshot(name='login fields required')
# todo: implement </s> def test_migration(self):	TestMigrations pass def test_migration_skew(self):
# todo proper error messages </s> "64-bit signed integer"	_Integer64_reader def _Integer64_reader(s):
# @todo: this has a chance to spam the user with notifications </s> main_io_loop.add_callback(client.write_message, msg)	push_message@14 return main_io_loop = app.instance.web_server.io_loop
# todo: this is not the best place to configure rank? why is rank not </s> )	evaluate@56 os.path.dirname(last_checkpoint.model_checkpoint_path))[-1] global_step = int( tf.logging.info('Evaluating global_step {}'.format( global_step))
# todo: add lon lats ! </s> 1000: load1000}	load_modis 500: load500,
self.assertequals(status, 200) # todo: 202 when asynchronous </s> self.populate()	test_get_bind manager = factory.consumer_bind_manager() bind = manager.bind(self.CONSUMER_ID, self.REPO_ID, self.DISTRIBUTOR_ID)
# todo: pandas returns dataframe, maybe return namedtuple instread of </s> s = b[i]	_column_fillna_impl def _column_fillna_impl(A, B, fill):  # pragma: no cover if np.isnan(s): s = fill
# todo(harlowja): the bug 1214083 is causing problems </s> " result: %(result)s") % {'state': state,	attach_debug_listeners 'flow': str(details['flow'])}) def task_log_change(state, details): 'flow': str(details['flow']), 'runner': str(details['runner']),
# todo: pytorch v0.4 has torch.where function </s> new_trn_img = np.copy(nrm_trn_img)	get_cifar_anomaly_dataset nrm_tst_lbl[:] = 0 abn_trn_lbl[:] = 1 new_trn_lbl = np.copy(nrm_trn_lbl)
# todo change when v4 web3.py will released </s> escrow.sample(quantity=100)  # waay more than we have deployed	test_select_ursulas assert len(miners) == 3 assert len(set(miners)) == 3
# todo: handle values and aggfunc options </s> return lambda df: len(df._data[0])	df_len_overload def df_len_overload(df): if len(df.columns) == 0:  # empty df
self._cloud_flow_complete_message.addaction("", i18n_catalog.i18nc("@action", "review your connection"), "", "", 1) # todo: icon </s> printer_type_identifiers = {	_onAddDevice def _onAddDevice(self, name, address, properties): cluster_size = int(properties.get(b"cluster_size", -1)) "9066": "ultimaker3", "9511": "ultimaker3_extended",
# todo(sirp): snet=false for now, however, if the instance of </s> path_parts = path.split('/')	_parse_swift_tokens except ValueError: creds, path = path.split('@') obj = path_parts.pop() container = path_parts.pop()
# todo: expose from marshal </s> elif len(raw_timestamp) != 4:	_bytes_from_bytecode raw_size = data[8:12] if len(magic) != 4 or magic != imp.get_magic(): raise EOFError("bad timestamp in {}".format(fullname)) elif len(raw_size) != 4:
engine = sel.bind  # todo: get engine from select </s> else:	append_iterator_to_table "Names in SQL table: %s\n" "Names from incoming data: %s\n" % names = discover(t).measure.names rows = (dict(zip(names, row)) for row in rows)
# act as if we were to install the packages in todownload </s> sys.exit(1)	doUtilYumSetup self._getSacks(archlist=archlist, thisrepo=sys.argv[3]) except yum.Errors.YumBaseError, msg:
#todo, multipart raw submissions need further parsing capacity. </s> instance = multipart_filename_error	get_instance_and_attachment@27 try: instance = request.FILES[MAGIC_PROPERTY].read() else: for key, item in request.FILES.items():
# :todo: implement test. </s> iota(self.adapter).sendtransfer,	SendTransferCommandTestCase self.assertIsInstance(
# todo - this isn't actually the correct way to set the vary header, </s> {'detail': 'method \'%s\' not allowed on this resource.' % self.method})	http_method_not_allowed def http_method_not_allowed(self, request, *args, **kwargs):
# todo: fix this to show all configs </s> template_name_suffix = '_create'	ProjectStageCreate class ProjectStageCreate(BaseGetProjectCreateView): Create/Add a Stage to a Project form_class = forms.StageCreateForm def form_valid(self, form):
# todo bucket-owner-read and bucket-owner-full-control </s> def notice(message):	notice
# todo...or leave it to user's default? </s> return {'object': obj,	event_to_activity event: dict, a decoded JSON event Returns: an ActivityStreams activity dict 'id': obj.get('id'), 'url': obj.get('url'),
assert 'not both' in res.stdout  # todo: stderr </s> with open(target_path, 'w') as f:	basic_target_path @pytest.fixture def basic_target_path(tmp_path): f.write(BASIC_TARGET) return target_path
# todo: check that stack trace is maintained. </s> assert numpy.all(val == [[1]])	test_local_join_1 s = join(1, a) f = function([a], s, mode=mode_opt) e = f.maker.fgraph.toposort() assert len([n for n in e if isinstance(n.op, Join)]) == 0
# todo: distributed search messages need to implemented </s> if self.chatrooms is not none:	UserJoinedRoom self.chatrooms.roomsctrl.UserJoinedRoom(msg) else:
# todo extend to nonbinary nodes </s> normalize=false)	raw_current_marbl return self._raw_current_marbl else: return self._raw_current_marbl
# todo(mriedem): consider skipping the microversion discovery </s> def initialize_connection(self, context, volume_id, connector):	initialize_connection try: connection_info = cinderclient(
# fixme todo we should verify we get a circuit_new event for </s> self.send("250-circuit-status=123 built purpose=general")	test_bootstrap_single_existing_circuit self.send("250+ns/all=") self.send(".") self.send("250 OK") self.send("250-stream-status=")
# todo: wobble </s> phi += pi	to_polar arctan2(-position[1], -position[0], out=phi)
# todo: vectorize with numpy </s> self.compile_script = ""	CppBuilder self.executable_path = "" self.code_gen_dir = "" def append_includes(self, library_path): self.include_paths.append(library_path)
# todo: avoid magic string value </s> if key_offset == -1:	__generate_input_and_label key_offset= dt.lower().find(keymap[t].lower()) else: s += '%s</label>' % escape(dt) else:
raise tornado.gen.return(notebook)  # todo py2: replace by return </s> or not exporter.exclude_input                   # keep cell if input not excluded	filter_empty_code_cells return ( cell.cell_type != 'code' or                     # keep non-code cells
# todo: hack </s> if type(container) == type(containerregistry.getinstance().getemptyinstancecontainer()):	ThreeMFWorkspaceWriter return True @staticmethod return  # Empty file, do nothing. file_suffix = ContainerRegistry.getMimeTypeForContainer(type(container)).preferredSuffix
# todo(ytknzw): add more specific assertion with the test case. </s> raise valueerror	fail_objective
# todo(mattjj): remove this special case, used for debugging on cpu </s> pvals = [partialval((aval, core.unit)) for aval in avals]	parallel_callable @lu.memoize with core.new_master(JaxprTrace, True) as master: jaxpr, (pval, consts, env) = trace_to_subjaxpr(fun, master).call_wrapped(pvals)
from _devbuild.gen.id_kind_asdl import id  # todo: fix circular dep </s> def usererrorstring(self):	UserErrorString
mock = create_mock_json('tests/resources/list_race_details.json')  # todo </s> assert self.scores.url == '%s%s' % (self.scores.client.api_uri, 'scores/json-rpc/v1')	test_url
# todo(mattjj): test that constants/literals are set up properly </s> vjp = lambda g: (g + x + y, g * x * 9.)	vjpfun def vjpfun(x, y): return out, vjp
# todo present sample rate configuration using source.get_sample_rates().values() </s> return bool(self.osmosdr_source_block.get_gain_mode(ch))	get_agc
# todo: project_id is tried to load with api_key. do not load it? </s> res = self.app.get(url + '?title=' + announcement.title  + '&body=' + announcement.body)	TestAnnouncementAPI res = self.app.get(url + "?title=wrongvalue") data = json.loads(res.data) data = json.loads(res.data) assert len(data) == 10, data
# todo: this shader is drawn over regular shader now, drawing only one should speed it up </s> l = len(m.group(0))	repl return str(fn).zfill(l)
#todo, is this the best way to handle </s> elif self.tracked_mobject is not none:	update_position def update_position(self): if self.position_update_func is not None: self.decimal_number_mobject.move_to(self.tracked_mobject.get_center() + self.diff_from_tracked_mobject)
# todo: we haven't read all the data, actually </s> class threadingtcpserver(socketserver.threadingmixin, socketserver.tcpserver):	ThreadingTCPServer
#set limit to 25 --> currently hardcoded --> todo: add a setting for this ? </s> try:	buildVideoNodesListing WINDOW.setProperty("Emby.nodes.%s.path" %str(totalNodesCount),"ActivateWindow(Video,%s,return)"%path) WINDOW.setProperty("Emby.nodes.%s.content" %str(totalNodesCount),path) etree.ElementTree(root).write(nodefile, xml_declaration=True) except:
# todo recover multiple tables at the same time. </s> def __isub__(self, other: any) -> any:	__isub__
#todo(nmakhotkin) we should use thing such a decorator here </s> executions = [execution]	Executions
# todo: check ping response </s> self.metrics = faucet_metrics.faucetmetrics(reg=self.registry) # pylint: disable=unexpected-keyword-arg	setup_valve self.table = FakeOFTable(self.NUM_TABLES) self.logger = valve_util.get_logger('faucet', self.logfile, logging.DEBUG, 0) self.notifier = faucet_experimental_event.FaucetExperimentalEventNotifier( self.faucet_event_sock, self.metrics, self.logger)
# todo(huangyp): make sure striding won't cross segment boundaries. </s> state: the initial state for streaming inference.	zero_state Returns:
# todo: add logging </s> result = fixcache[params]	fixlist def fixlist(params): except KeyError: result = tuple(fixparam[x] for x in params)
# todo: not implemented yet </s> return match_sets	check_consistency m = eq_text_neq_type_spans(ann_objs, restrict_types=restrict_types, ignore_types=ignore_types) if len(m) != 0:
# todo it is a bit hack-ish, is it possible to find a more generic fix? </s> return true	is_nvcc_available assert s[0]=='release' global nvcc_version except: p = os.path.join(config.cuda.root,'bin','nvcc')
# todo: scale and translation could be merged into a single network </s> def add_backward_preprocessing_ops(self):	add_backward_preprocessing_ops
# todo: handle marked with template </s> return '{}()'.format(self.__class__.__name__)	TZoneUTC def dst(self, dt): return ZERO
# todo - mux support </s> for sig in frame.signals:	recalcDLC frame.calcDLC() if "force" == strategy: if sig.getStartbit() + int(sig.signalsize) > maxBit: maxBit = sig.getStartbit() + int(sig.signalsize)
# todo: use shlex.quote as soon as a newer python version is available. </s> rvm_cmd = os.path.expanduser('~/.rvm/bin/rvm-auto-ruby')	RubocopCommand def run(self, edit): self.check_file(self.view.file_name()) quoted_file_path = pipes.quote(file_path) rubocop_cmd = rvm_cmd + ' -S rubocop ' + quoted_file_path
categories = category.objects.filter(status=1)  # todo: fix magic number </s> raise http404("post does not exist")	post_detail try: post = Post.objects.get(pk=pk) context = { 'post': post,
except exception:  # todo: refactor this... </s> requestsnetworkwrapper()	test_init
#todo: add method </s> data['title'] = title	onPlayBackStarted data['season'] = int(season) data['episode'] = int(episode) logger.debug("[traktPlayer] onPlayBackStarted() - Title: %s, showtitle: %s, season: %d, episode: %d" % (title, showtitle, season, episode)) else:
# todo(stephenfin): enable this once we drop use of </s> return {	list_opts@60 api_db_group: api_db_opts,
# todo: fails because of missing svg support </s> <thead>	test_image_repeat_block img { display: block } </style> <tr><th><img src="pattern.png"></th></tr> </thead>
# todo(b/134526360): xla doesn't support integer dots, so we emit a sum of </s> _check_shapelike('broadcast', 'sizes', sizes)	_broadcast_shape_rule def _broadcast_shape_rule(operand, sizes):
# todo: remove uris, replacing it with support in query language. </s> raise notimplementederror	Library :param uris: zero or more URI roots to limit the search to :type uris: list of strings or :class:`None`
# todo get tri-letter dimensionality from fit-transform as input shape </s> activation=self._params['activation_hidden'])(x)	_build_shared_model activation=self._params['activation_hidden'])(x) x_out = Dense(units=self._params['dim_fan_out'], return x_out
return none  #todo: fix logic above, inserting this just to fix warnings </s> part_number = block_number + 1	put_bytes_mpu_mp_shm False: reuse existing session :return: Multi-part upload response start = block_number*block_size end = (block_number+1)*block_size
# todo(sloria): test me </s> @must_not_be_registration	dropbox_config_put @must_have_addon('dropbox', 'node') def dropbox_config_put(node_addon, **kwargs):
chunk_size = 150 # todo: tune </s> line2node = map_line2node(cfunc, database_metadata, line2citem)	CoveragePainter
n)  # todo: access alice's private key inside this method. </s> return msgpack.dumps(self.ids)	packed_payload
# todo: remove verify ssl config when working without it. </s> 'netflow': net_flow if zone_key1 == sorted_zone_keys[0] else -1 * net_flow,	fetch_exchange@86 response_data = { 'sortedZoneKeys': '->'.join(sorted_zone_keys), 'source': 'demanda.ree.es', }
# todo: add json schema validation </s> _check_format(json_file, json_content)	load_json_file raise exceptions.FileFormatError(err_msg)
# todo: re-enable this when we bring back unsubscribe (bug 802379). </s> self.check_validation('your manifest must be less than %s bytes.' %	test_response_too_large max_webapp_size = settings.MAX_WEBAPP_UPLOAD_SIZE
# todo maybe this could be moved to trakt.media class? </s> def run(self):	Movie class Movie(Base):
# todo: see get_scale_factor() to choose 72 px on hidpi </s> if self._scroll_handler_id is not none:	scroll_handler_id @scroll_handler_id.setter self.disconnect(self._scroll_handler_id) self._scroll_handler_id = value
# todo: error handling? </s> self.collateral_kwargs["notebook_server_info"] = none	StopInteractiveSession self.collateral_kwargs["project_uuid"] = None self.collateral_kwargs["pipeline_uuid"] = None return False else:
# todo: check if releasing locks early still makes sense </s> hv_defs = cluster.simplefillhv(self.op.hypervisor, self.op.os_type, {})	_RevertToDefaults for name in self.op.hvparams.keys(): if name in hv_defs and hv_defs[name] == self.op.hvparams[name]:
node = self.__read_metadata(filepath=metadata_path, federated_only=federated_only)  # todo: 466 </s> with open(filepath, 'rb') as certificate_file:	__read_tls_public_certificate if not filepath and checksum_address is not None: filepath = self.generate_certificate_filepath(checksum_address) cert = x509.load_pem_x509_certificate(certificate_file.read(), backend=default_backend()) return cert
# todo: load state into here </s> if current_chunk:	map_schedule_onto_host_or_device elif isinstance(sched_item, Barrier): if sched_item.kind == "global": new_kernel_name = kernel_name_gen() new_schedule.extend(
# todo(b/158462888): use aggregete losses that works with replicas. </s> kl_cutoff_loss = self._kl_cutoff_coef * tf.square(kl_over_cutoff)	kl_cutoff_loss kl_cutoff = self._kl_cutoff_factor * self._adaptive_kl_target mean_kl = tf.reduce_mean(input_tensor=kl_divergence) if debug_summaries: tf.compat.v2.summary.scalar(
# todo: modifiers </s> self.update()	_pyvis_update
categories = category.objects.filter(status=1)  # todo: fix magic number </s> raise http404("post does not exist")	post_detail try: post = Post.objects.get(pk=pk) context = { 'post': post,
# todo: duplicate + replace with psutil.getloadavg()? (available in 5.6.2) </s> sys.stdout.flush()	countdown for remaining in range(s, 0, -1): sys.stdout.write("\r") time.sleep(1)
# todo: fix self.cursor_x >= w </s> return i	inline_k_ctrl_k def inline_k_ctrl_k(self, xbegin, i, w):
# todo: seems like a pandas' bug when fill_value is not none? </s> self.assert_eq(kdf.groupby("a").ffill(),	test_ffill 'D': [0, 1, 5, 4]}, columns=['A', 'B', 'C', 'D'], index=[0, 1, 2, 3]) kdf = koalas.DataFrame(pdf) pdf.groupby("A").ffill().drop('A', 1)) else:
# todo: make sure limits are deterministic then update this </s> def test_double_add_ignored(self):	test_double_add_ignored @pytest.mark.skip()
pass # todo: explain </s> self.setmessage('header-location', rs.redirect_without_location)	status303 def status303(self):        # See Other
# todo: support speedy mode for running the script </s> "._config" (generated by us), if present."""	rm_configs def rm_if_exists(f): if os.path.exists(f):
# todo: find a better random value </s> if prop.type == datetime.datetime:	AccessPoint item.modified = True return item return datetime.datetime.now() elif prop.type == datetime.date:
# todo test this </s> and stopping applications, opening up application ports and setting up	ChangeStateScript @implementer(ICommandLineScript) class ChangeStateScript(object): routes to other nodes. _deployer = Deployer()
# todo(yuriyz): change to 404 (bug 1200517) </s> result = self.get_json('/ports/%s' % pdict['uuid'])	test_update_byid self.assertEqual(response.status_code, 200)
# todo: we should throw here, i don't like this. </s> item.ref = resourcereference(pack=pack, name=name).ref	_get_all for item in result: pack = getattr(item, 'pack', None) return result
tol = 0.15  # todo(skye): can we be more precise? </s> self.assertraises(	testFftErrors ) self.assertRaises( ValueError, lambda: func(rng([2, 3], dtype=onp.float64), axis=[-3])) pass
# todo the following way of testing is highly sensitive to small changes </s> act_bit_width=none,	LFC self, num_classes=10, in_bit_width=None, in_ch=1,
# todo: clean up </s> return none  # default	pubsub_cache_size @pytest.fixture
# todo(kumar) remove this when validator is fixed, see bug 620503 </s> def validator(upload_id, **kw):	validator @task log.info('VALIDATING: %s' % upload_id) upload = FileUpload.objects.get(pk=upload_id)
# todo: take namespace into account, currently doesn't matter since </s> account_ns = account.namespace	top_level_namespaces user = db_session.query(User).join(ImapAccount)\ .filter_by(id=user_id).one() nses['private'].append(account_ns.cereal()) shared_nses = db_session.query(SharedFolder)\
# todo: support steps and times (motion blur) </s> world = 1 << 5	Change OBJECT = 1 << 2 MATERIAL = 1 << 3 REQUIRES_SCENE_EDIT = CAMERA | OBJECT | MATERIAL | VISIBILITY | WORLD REQUIRES_VIEW_UPDATE = CONFIG
#todo respect et mapped namespaces </s> if c == '&':	xmlesc matches = ('&', '<', '"', '>', "'") for c in text: text[cc] = '&amp;' elif c == '<':
# todo: handle timeout </s> raise valueerror("listeningport must be > 0 and <= 65535")	TCPClient if listeningPort <= 0 or listeningPort > 65535:
# todo extend to nonbinary nodes </s> return none	purview Returns: tuple[int]: The purview that the repertoire was computed over. return tuple(np.where(np.array(repertoire.shape) == 2)[0])
# todo: remove # pylint: disable=fixme </s> copy.append(self.extraeventsvalues[:])	transferbuttonsto def transferbuttonsto(self,pindex): copy = [] copy.append(self.extraeventsactions[:]) copy.append(self.extraeventsvisibility[:])
# todo: remove </s> is_removed=false,	unremoved return self.filter(Q(topic__category__parent=None) | Q(topic__category__parent__is_removed=False), topic__category__is_removed=False, action=0)
#todo - check annotation </s> alignments = list(alignments)	pairwise_alignment_check targets, alignments, local=True) : self.assertEqual(len(targets), len(alignments)) for target, alignment in zip(targets, alignments) :
# todo(hvy): whether a pruned trials should have an actual value can be discussed. </s> @property	BaseStudy Returns: A :class:`~optuna.structs.FrozenTrial` object of the best trial. def direction(self): Returns:
# todo(higumachan): remove this "if" section </s> 'but this version can not install keras(tensorflow) with pip.')	test_keras_pruning_callback_observation_isnan@61 def test_keras_pruning_callback_observation_isnan(): if not _available: study = optuna.create_study(pruner=DeterministicPruner(True)) trial = study._run_trial(func=lambda _: 1.0, catch=(Exception,))
# todo watch out because urllib.unquote will blow up on unicode text </s> def run (self):	Http self.server = HttpServer((host, port), HttpHandler) Backend.__init__(self, router) while self.running: if self.message_waiting:
# todo: edit once we get the properly labeled entity ids from nalanda </s> return self.entries.create(*args, **kwargs)	add_entry ) if num_entries > 0 and entries_to_move.exists():
# todo: think about moving this to model_eval mtry function </s> df_train.dropna(subset=[predictedcol], how='all', inplace=true)	DeploySupervisedModel if debug: print('\nTraining dataframe right after splitting it off:') if debug: print('\nTraining df after removing rows where pred col is NULL:')
# todo: enable non-windows methods in configure </s> else:	Package7Zip vcvars = tools.vcvars_command(self.settings) with tools.chdir("CPP/7zip"): env_build = AutoToolsBuildEnvironment(self) with tools.environment_append(env_build.vars):
# todo: there’s a vertical 0.5px shift on the second page </s> def to_pix(pixels_str):	to_pix
# todo: move to base class </s> else:	getNodesRect continue n_rect = QtCore.QRectF(n.scenePos(), QtCore.QPointF(n.scenePos().x() + float(n.w), n.scenePos().y() + float(n.h))) for n in self.getAllNodes(): if activeGraphOnly:
# todo check the op returned a view </s> val = 100	summary_function val = self.vm_call_time * 100 / self.call_time print >> file, '  Total time spent in calling the VM %es (%.3f%%)' % ( if self.call_time > 0: val = 100. - self.vm_call_time * 100 / self.call_time
# todo complete this method </s> return self	make_ip self.Wovoo = imd.Wovoo(self._cc, t1, t2, eris, kconserv) self.made_ip_imds = True
# todo(dcramer): this should respect rate limits/etc and use the normal </s> def is_enabled(self):	is_enabled
# todo: theme me </s> geom = gg_import('geom_' + l.geom.guide_geom)	create_geoms data = l.use_defaults(pd.DataFrame())[zeros] for ae in set(self.override_aes) & set(data.columns): params = l.geom.manual_aes.copy() params.update(l.stat.params)
# todo: temporary hack until they fix </s> assert link in html	test_extra_image_separated link = f'<a class="image" href="{src}" target="_blank">'
# todo: we need to pick the rank from `comm_shm`, not `comm`, </s> def _specialize_iet(cls, graph, **kwargs):	DeviceOpenACCNoopOperator class DeviceOpenACCNoopOperator(DeviceOpenMPNoopOperator): @classmethod options = kwargs['options'] if options['mpi']:
# todo: this is untested. </s> result = callback(connection, cert, error_number, error_depth, ok)	_VerifyHelper error_number = _lib.X509_STORE_CTX_get_error(store_ctx) error_depth = _lib.X509_STORE_CTX_get_error_depth(store_ctx) except Exception as e: self._problems.append(e)
# todo check error message </s> yield "/etc/yunohost/apps/%s/scripts/install" % app	app_expected_files yield "/var/www/%s/index.html" % app yield "/etc/yunohost/apps/%s/settings.yml" % app yield "/etc/yunohost/apps/%s/scripts/remove" % app yield "/etc/yunohost/apps/%s/scripts/backup" % app
return self.edited # fallback todo log? </s> def title(self) -> optional[str]:	title @property
# todo: custom exception (?) </s> return i;	_to_image i = NodeImage(id=el.get('id'), name=el.get('name'),
return 0.0 #todo - return nan or none here? </s> "did not correctly translate colour from floating point rgb tuple"	ColorsTest def t_color_conversions(self): translator = ColorTranslator() assert translator.int255_color((1, 75, 240)) == translator.translate((1, 75, 240)), \ "Did not correctly translate colour from integer RGB tuple"
# todo: add test case </s> assert isinstance(2**50, numeric_types)	test_types assert isinstance(u'a', string_types) assert isinstance('a', string_types)
# todo need to send a browse request for the object to be populated </s> return bool(lib.sp_artist_is_loaded(self._sp_artist))	Artist return name if name else None @property def load(self, timeout=None): :param timeout: seconds before giving up and raising an exception
# todo: handle errors better </s> try:	Teams Create a new team. team = Team(**args) db.session.commit() except sqlalchemy.exc.IntegrityError as e:
# todo allow first arg to be string name, class type or func </s> which is supported by all augmenters. support for other dtypes	augment_image The image to augment. Channel-axis is optional, but expected to be the last axis if varies by augmenter -- see the respective augmenter-specific documentation for more details.
#todo?# self.asserttrue(greps(err, "unit zzz.service not for --user mode")) </s> cmd = "docker cp {testdir}/zzb.service {testname}:/etc/systemd/system/zzb.service"	test_6133_run_default_services_from_single_service_saved_container sx____(cmd.format(**locals())) cmd = "docker cp {testdir}/zza.service {testname}:/etc/systemd/system/zza.service" sh____(cmd.format(**locals())) cmd = "docker cp {testdir}/zzc.service {testname}:/etc/systemd/system/zzc.service"
# todo packages for cloudera not available on lucid yet, using karmic for the moment (beta 1) </s> for pname in config['pypi']:	_python_library_installer sudo("easy_install -U %s" % pname)
# todo: support more than just lists </s> self.visit(node.body)	LocalExtractor def visit_For(self, node): self.visit_in_assign(node.target) self.visit(node.orelse) def visit_withitem(self, node):
# todo: this is a good place to detect and react to a loop, </s> self.logger.info(	expire_hosts_from_vlan del vlan.host_cache[eth_src] self.logger.info( '%u recently active hosts on vlan %u', len(vlan.host_cache), vlan.vid)
# todo: move the logic somewhere else </s> usage: random_integers(random_state, size, low=0, high=1)	RandomStreamsBase to supplement the missing information. return self.gen(normal, size, avg, std, ndim=ndim) Sample a random integer between low and high, both inclusive. If the size argument is ambiguous on the number of
pass  # todo: implement. </s> self.old, self.lookups = old, lookups	ChainContextSubstStatement class ChainContextSubstStatement(Statement): def __init__(self, location, old_prefix, old, old_suffix, lookups): self.old_prefix, self.old_suffix = old_prefix, old_suffix def build(self, builder):
# todo: segwit stuff </s> except keyerror:	script_to_string sigs = ' '.join([to_hexstring(i) for i in data['signatures']]) try: scriptstr = SCRIPT_TYPES_UNLOCKING[data['script_type']] scriptstr = [sigs if x in ['signature', 'multisig', 'return_data'] else x for x in scriptstr]
# todo: warning? exception? </s> return td	parse_timedelta elif m.group(3) == "y": days = 365 * count
raise notimplementederror # todo </s> s.resample()	resample_states def resample_states(self):
# todo: remove when materialized paths are fixed in the payload returned from waterbutler </s> if page == comment.overview:	_update_comments_timestamp guid_obj = Guid.load(root_id) if guid_obj is not None: root_id = node._id auth.user.comments_viewed_timestamp[root_id] = datetime.utcnow()
# todo: documentation pending </s> "but the mode is currently set as %s. " % ('training by model.train()' if self.is_train else 'inference by model.eval()') +	_check_mode logging.warning("Training / inference mode redefined redundantly. Please EITHER use the argument `is_train` OR `Model.train()` / `Model.eval()` to define the mode.") else: "Please EITHER use the argument `is_train` OR `Model.train()` / `Model.eval()` to define the mode.")
# end todo </s> return einsum('bic->i', (sqrt_ggn**2))	bias_diag_ggn sqrt_ggn = sqrt_ggn_out.view(batch, module.out_channels, out_pixels, num_classes)
# todo: this implementation should be revisited </s> msg_variable_length_headers,	HRITMSGEpilogueFileHandler super(HRITMSGEpilogueFileHandler, self).__init__(filename, filename_info, filetype_info, msg_text_headers)) self.epilogue = {}
#todo: allow changing of default roles associated with history </s> cur_groups.sort()	HistoryController return trans.show_error_message( "You must specify at least one default group." ) cur_groups = [ assoc.group for assoc in history.default_groups ] if cur_groups != group_in: history.set_default_access( groups = group_in )
# todo(@awav): may need them for other models </s> def test_gaussian_mean_and_variance(ntrain, ntest, d):	test_gaussian_mean_and_variance X, Y = rng.randn(Ntrain, D), rng.randn(Ntrain, 1) Xtest, _ = rng.randn(Ntest, D), rng.randn(Ntest, 1)
# todo: move to ab callback </s> def on_client_error(self, error_type, error):	on_client_error
# todo debug </s> ruleminutenew.start = int(minuteitem.attrib[	parseRuleRecursively currentRule.elements.append(ruleElement) elif not minuteItem is None: "start"]) if (ruleMinuteNew.start < 0 or
# todo: add this back in once we've merged back the refactored users code </s> commcare_username = self.commcare_username	testLinkOrphanCommCareUser parent.save() couch_user_1 = parent.get_profile().get_couch_user() commcare_imei = 'imei' commcare_user_data = 'random'
# todo consolidate this and pr plotter into 1 function </s> plt.show()	plot_random_forest_feature_importance print('\nFeature importance plot saved in: {}'.format(source_path)) plt.close(figure)
#todo: call _update_node_rule_for_parents </s> def _update_node_rule_for_parents(self):	_update_node_rule_for_parents
#todo avoid doing this since a may be a different sparse type </s> return (postprocess(x), 0)	cgnr@86 if normr <= tol: if callback != None: z = M*rhat p = z.copy()
# :todo: implement test. </s> skip_value_check = true	GetTrytesResponseFilter class GetTrytesResponseFilter(BaseFilterTestCase): def test_pass_transactions(self): self.skipTest('Not implemented yet.')
# todo: code not valide as accountcode moved to his own model accountcode </s> except attributeerror:	sanitize_cdr_field Sanitize CDR fields try: field = '' return field
#todo: kvick we should rename 'short_circuit' to something like 'disable_service_start' </s> log.debug('downloading {0} to {1}'.format(pkg_url, dst_file_path))	_download_pkg def _download_pkg(self, context): pkg_url = context.package.arg download_file(pkg_url, dst_file_path, context.package.get('timeout', 1))
# todo: logging </s> tester_provider = contractprovider(provider_backend=web3, registrar=registrar, sol_compiler=solidity_compiler)	contract_provider @pytest.fixture(scope='module') yield tester_provider
# todo: is that right? </s> es = elasticutils.get_es()	index_doc def index_doc(doc, bulk=False, force_insert=False, es=None): from wiki.models import Document index = get_index(Document) try:
# todo: remove this when domain decomposition is merged </s> for key in self._output:	create_output_subelement def create_output_subelement(self): if not self._output is None: subelement = ET.SubElement(element, key) subelement.text = str(self._output[key]).lower()
#     # todo: add an exception message </s> elif token == "x":	_parse_token else: rounding = 0 parts["timestamp"] = int(value) elif token in ["ZZZ", "ZZ", "Z"]:
# todo: check pdf content? how? </s> url = urlparse.urljoin(h1.base_url, 'pattern.png')	test_html_parsing assert [child.tag for child in body] == ['h1', 'p', 'ul'] h1 = body[0] assert url.startswith('file:') assert url.endswith('weasyprint/tests/resources/pattern.png')
#todo: this should be reading from the wcs object </s> def instrument(self):	instrument @property
# todo: change logic to c_leq based on benchmarking </s> 'mindtpy unable to handle nlp subproblem termination '	handle_NLP_subproblem_other_termination add_int_cut(var_values, solve_data, config) else: 'condition of {}'.format(termination_condition))
# todo: how to handle not found authorname </s> @admin_required	authenticate_google_drive @app.route("/gdrive/authenticate") def authenticate_google_drive(): authUrl = gdriveutils.Gauth.Instance().auth.GetAuthUrl()
group_id='test-consumer',  # todo: what belongs here? </s> self._topic = topic	KafkaChangeFeed class KafkaChangeFeed(ChangeFeed): Kafka-based implementation of a ChangeFeed def iter_changes(self, since, forever): consumer = _get_consumer(self._topic)
# todo: use weight scaling factor if provided, xavier's default else </s> annealed = sharedx(conf['base_lr'], 'annealed')	DATrainer thislr = conf.get(lr_name, 1.) learning_rates[parameter] = sharedX(thislr, lr_name) self = cls(model=model, cost=cost, conf=conf, learning_rates=learning_rates, annealed=annealed,
# todo: clocksignal, resetsignal </s> reply = self._eval_nested_lists(request)	Simulator elif isinstance(request, tuple): self.evaluator.assign(*request) except StopIteration: exhausted.append(generator)
# todo: looks like xform has a add_bind method. look into it. </s> app.save()	Command self.stdout.write("Saving modified app...") app = form.get_app() self.stdout.write(form.source) self.stdout.write('command finished')
# todo: accept these via quirks? </s> is_setup -- true iff this transfer should begin with a setup token.	send_on_endpoint Sends a block of data on the provided endpoints. endpoint_number -- The endpoint number on which to send. blocking -- True iff this transaction should wait for the transaction to complete. data_packet_pid -- The data packet PID to use (1 or 0). Ignored if the endpoint is set to automatically
# todo(sloria): test me </s> return os.path.split(path.strip('/'))[1]	get_file_name
# todo refactor like in https://github.com/guardicore/monkey/pull/1528 because </s> return _setup_config_by_cmd_arg(island_args.server_config_path)	setup_server_config if island_args.server_config_path:
pass  # todo </s> self.wb1.sheets[0].range('a1').value = 123.	TestSheet def test_index(self): assert_equal(self.wb1.sheets['Sheet1'].index, 1) assert_equal(self.wb1.sheets[0].range('A1').value, 123.) def test_cells(self):
# * todo heading 1 --> </s> vim.current.window.cursor = (1, 0)	test_toggle_todo_with_no_heading Todo.toggle_todo_state() self.assertEqual(vim.current.buffer[0], '')
# todo: split into a function + context manager </s> os.environ.pop('datalad_log_level')	teardown_package if _test_states['loglevel'] is not None: lgr.setLevel(_test_states['loglevel']) else: os.environ['DATALAD_LOG_LEVEL'] = _test_states['DATALAD_LOG_LEVEL']
# todo: fix with stubber / before send event </s> self.region = 'us-west-2'	TestApiGateway class TestApiGateway(BaseSessionTest): def setUp(self): self.client = self.session.create_client( 'apigateway', self.region)
# todo: fix highlight handling </s> page = 1	PublioStore def search(self, query, max_results=20, timeout=60): br = browser() while counter: with closing(br.open('http://www.publio.pl/e-booki,strona' + str(page) + '.html?q=' + urllib.quote(query), timeout=timeout)) as f:
# todo what about "check_existing" ? </s> if target is none:	pre_save_struct def pre_save_struct(proxy: Proxy, bpy_struct: T.Struct, attr_name: str): return None if isinstance(target, T.ColorManagedViewSettings):
# todo: i think this should use '$ fileregions' </s> offset = self.page_size * page_number	get_page_buffer def get_page_buffer(self, page_number): if page_number < 1: return self.buf[offset:offset + self.page_size]
#todo: error checking </s> evaluation.message('unitvector', 'nokun', k, n)	UnitVector if n is None or k is None: return return def item(i):
# todo (t65593688): this should be removed after </s> bytetokentensorizer.config(column="source_sequence")	get_tensorizers ) if add_contextual_feat: ) initialize_tensorizers(
# todo: support for multiple message versions </s> else:	get_base_directory def get_base_directory(self): if self.opts.dest: return self.conf["general"].get("destination", "/tmp/")
# todo: remove in sopel 8 </s> 'event',	is_triggerable 'rule', 'find_rules', 'intents', 'commands',
pass  # todo </s> def test_activate(self):	TestSheet assert_equal(self.wb1.sheets[0].range('A1').value, 123.) def test_cells(self): self.wb1.sheets['Sheet2'].activate() assert_equal(self.wb1.sheets.active.name, 'Sheet2')
raise mpdnotimplemented # todo </s> result = self._find(type, what)	_findadd @register(r'^findadd "(?P<type>(album|artist|title))" "(?P<what>[^"]+)"$')
recording_software_version = none  # todo </s> return info_csv["capture software"] == "pupil mobile" and "data format version" not in info_csv	is_pupil_mobile_recording def is_pupil_mobile_recording(rec_dir: str) -> bool: info_csv = utils.read_info_csv_file(rec_dir) except KeyError: return False
# todo: read this from config </s> self.errata_last_state = new_state	render_errata_step def render_errata_step(self, progress_report): current_state = progress_report['importer']['errata']['state'] self._render_general_spinner_step(self.errata_spinner, current_state, self.errata_last_state, _('Importing errata...'), update_func)
# todo: trigger via dummy audio? </s> self.core.playback.play(self.tl_tracks[0])	test_pause_selects_dummy1_backend self.core.playback.pause() self.playback1.pause.assert_called_once_with()
# todo(kgriffs): measure initial time, and keep iterating until </s> def bench():	create_bench srmock = StartResponseMockLite() function = name.lower().replace('-', '_') app(env, srmock) if srmock.status != '200 OK':
# todo refactor with np.tile or sth </s> masks = {}	module_masks activations = self.activations[module][0] if isinstance(module, self.masked_modules):
assert study_id == 0  # todo </s> def __init__(self):	InMemoryStorage self.study_attrs = {} self.trials = []
# todo also check for motion codec parameter support </s> return _motion_detected.get(camera_id, false)	is_motion_detected
# todo: custom gremlin method </s> def put_edge(self, index_name, key, value, _id):	put_edge
# todo: must be implemented </s> pass	should_fetch_kindlegen
# todo(ytknzw): add more specific assertion with the test case. </s> def fail_objective(_: trial) -> float:	fail_objective
# todo: move this function somewhere else </s> def label_from_instance(self, instance):	DirectoryFormField return _(instance.pootle_path[len(translation_project.pootle_path):])
# todo: account for line widths and style </s> return [	sphere_3d_box face_color = self.face_color if face_color is not None: { "type": "sphere",
# todo: add test here as well </s> def test_wut():	test_wut
# todo delete? we should search for valid parser </s> return sorted(comps, key=lambda x: (x.name.startswith('__'),	Completion comp_dct[k] = new comps.append(new) x.name.startswith('_'), x.name.lower()))
help='') # todo </s> def sop_cmd(bool_eq_wrapper):	sop_cmd
# todo: remove save parameter </s> (user and self.has_permission(user, 'write')) or is_api_node	can_edit else: is_api_node = False
#@todo: move this and other methods out of this file , into a general </s> make a lighter color	_light_color lcolor = [0.0, 0.0, 0.0] lcolor[0] = 0.5 * (1.0 + color[0])
#todo: write a doc string for this method </s> def fill_chain(v):	fill_chain
codecs.decode('5050827d08000000d00745b2cf451b00', 'hex'),  # tcp random cmd_ack_ok todo: generate proper sequenced response </s> socket.return_value.recv.return_value = codecs.decode('d007fffc2ffb0000','hex') # tcp cmd_ack_ok	test_udp_connect def test_udp_connect(self, helper, socket): helper.return_value.test_ping.return_value = True # ping simulated zk = ZK('192.168.1.201') conn = zk.connect()
# todo: i believe this code here can be improved </s> return predicted_label	classify_instance indices = self.select(competences) votes = np.ma.MaskedArray(predictions, ~indices)
# todo: handle "other" </s> def to_sequence_relation(cls, elem):	to_sequence_relation return Tree.SequenceRelation( elem.get('type'), elem.get('id_ref_0'), elem.get('id_ref_1'),
# todo: change this to be architecture independent </s> if nativesize is 32:	get_adrs_mem nativeSize = get_native_size() if nativeSize is 16: return DbgDword(ea) if nativeSize is 64:
# todo: catch errors </s> please e-mail the information below to	except_handler out(error_template % """\ A problem has occurred, but it probably isn't your fault. <a href='mailto:red@redbot.org'>red@redbot.org</a> and we'll look into it.""")
# todo: i18n </s> configmanager.confighotkey.set_closure(self.show_configure_async)	init_global_hotkeys def init_global_hotkeys(self, configManager): logging.info("Initialise global hotkeys")
# todo: this sucks. do a real approximation with something like dvsa. </s> return state.solver.satisfiable(extra_constraints=(sc_constraint,)), sc_constraint	_attempt_write_nopsled def _attempt_write_nopsled(self, state, shellcode, start, nopsled_size): sc_data = state.memory.load(start, len(shellcode) + nopsled_size)
if self.scene.world != none and self.scene.world.node_tree != none and 'background' in self.scene.world.node_tree.nodes: # todo: parse node tree </s> i = triangle_index * 3	write_triangle def write_triangle(self, triangle_index, index_table):
# todo: figure out how to import pycache files </s> if not inspect.isclass(cls):	class_subclasses raise TypeError(repr(cls) + " is not a class.") for clazz in cls.__subclasses__():
# todo: chose a better hook position :) </s> env_vars = env_var + input + b"\x00" + (ql.path).encode() + b"\x00"	place_input_callback def place_input_callback(uc, input, _, data): ql.mem.write(ql.target_addr, env_vars)
# todo this should be more modular </s> yield dict(zip(keys, element))	_dict_product def _dict_product(self, d): keys = d.keys()
# todo; to change this to checkpoint_callbacks to include static ucr </s> ("commcarecase-deleted", commcarecase)	CouchCaseReindexerFactory iteration_key = "CouchCaseToElasticsearchPillow_{}_reindexer".format(CASE_INDEX_INFO.index) doc_provider = CouchDocumentProvider(iteration_key, doc_type_tuples=[ ]) return ResumableBulkElasticPillowReindexer(
# todo check if this always works? </s> bm = bmesh.new()	triangulate assert(obj.type == 'MESH') if apply_modifiers and obj.modifiers: bm.from_mesh(me) bpy.data.meshes.remove(me)
# todo: format the inputs' directory name </s> return self._portal_manifest.output_partition_num	_output_partition_num @property
# todo add assertions </s> p = poll.objects.create(question = "lo lo", pub_date = timezone.now())	test_creation p.save() self.assertEqual(Poll.objects.count(), 2) # Cause setup created one already
# todo: create a default location to save for the specific deployment </s> "name": "credit fraud",	model_deployments } model = { "categories": "", "repo_language": "python"
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
# todo: test this block </s> def html(self):	html for inclusion in <head>. Note:
# todo: use a contextmanager to ensure we always delete the callback from the list. </s> yield from self.wait_ping(node)	bond self.routing.remove_node(node) asyncio.ensure_future(self.populate_not_full_buckets()) logger.debug("bonding completed successfully with {}".format(node)) self.update_routing_table(node)
# todo debug </s> ruleelement.timetriggeredfor = float(	parseRuleRecursively ruleElement = RuleElement() ruleElement.type = "sensor" item.attrib["timeTriggeredFor"]) currentRule.elements.append(ruleElement)
# todo: revise exception taxonomy </s> packet_type_signature]))	parse_pubkey_bundle "types '{}' (see RFC4880 4.3. Packet Tags).".format(packet_type, [PACKET_TYPE_PRIMARY_KEY, PACKET_TYPE_USER_ID, except (PacketParsingError, IndexError) as e: raise PacketParsingError("Invalid public key data at position {}: {}."
# todo: we lose the response code, so we can't check this </s> % localfile)	test_GET_FILEURL_localfile_nonlocal localfile = os.path.abspath("web/GET_FILEURL_localfile_nonlocal") fileutil.make_dirs("web") d.addBoth(self.shouldFail, error.Error, "localfile non-local", "403 Forbidden",
# todo(qijun) the default decimal is 7, but numpy.dot and eigen.mul </s> if hasattr(self, attr_name):	OpTestMeta kwargs[out_name] = out_name scope.create_var(out_name).get_tensor() kwargs[attr_name] = getattr(self, attr_name) op = func(**kwargs)
# todo page_size = size of each result page </s> for policy in page:	search_all_iam_policies@27 response = client.search_all_iam_policies( scope, query=query, page_size=page_size) print(policy) break
# todo untested </s> finally:	get_serial_number _api.OPENSSL_free(hex_serial)
#todo: implement mp3 cd support </s> def initial_device_setup(self):	HAL except: common.log_exception() self.setup_cds_initial() self.setup_device_events()
# todo is this check necessary; this was an assertion which are disabled in <4000 which is good </s> _op(view, 'yank', register=register, linewise=linewise)	registers_op_yank
# todo log here </s> if factor['status'].lower() != 'active':	get_factor_id@44 if factor['provider'].lower() != 'okta' or \ factor['factorType'].lower() != 'push': not_active = True continue
# todo: match channel against [a-za-z0-9:._-]+ </s> *musicpd.org, client to client section:*	unsubscribe@30 @protocol.commands.add('unsubscribe') ``unsubscribe {NAME}`` Unsubscribe from a channel.
# todo(vek): need to pass context in for access to auth_token </s> assert state in power_state.valid_states(), "bad state: %s" % state	InstanceInfo self.name = name
# todo: detect how directories being present is being handled. </s> def drupal(self):	drupal @controller.expose(help='drupal-related scanning tools')
# todo(tobyboyd): remove eager flag when tf 1.0 testing ends. </s> raw_record: scalar tensor tf.string containing a serialized	parse_record_keras through preprocessing steps (cropping, flipping, and so on). This method converts the label to one hot to fit the loss function. Example protocol buffer. is_training: A boolean denoting whether the input is for training.
# todo(solitude): remove this. </s> all_apps = request.amo_user.addons.filter(type=amo.addon_webapp)	activity_log @login_required return jingo.render(request, 'account/activity.html', {'log': _get_items(None, all_app)})
# todo: consider filtering by location type </s> if messy_date_string:	clean_date cleaned_datetime = parse(messy_date_string) return cleaned_datetime.date()
# todo: error handling like numba callwrappers.py </s> c.pyapi.decref(typobj)	check_element_type ) c.pyapi.decref(typobj)
return # todo raise error </s> logger.debug(u'%s.seeked signaled', player_iface)	Seeked @dbus.service.signal(dbus_interface=PLAYER_IFACE, signature='x') pass
# todo(leofang): test newer rocm versions </s> @testing.for_all_dtypes()	test_fftn_orders def test_fftn_orders(self, dtype, enable_nd): for order in ['C', 'F']:
data = {}  # todo: do we need public key handling now? </s> :type name: string	Artist :param uri: artist URI :type uri: string :param musicbrainz_id: MusicBrainz ID :type musicbrainz_id: string
# todo better logging </s> try:	Connection def exec_command(self, command): pass scp = SCPClient(self.ssh.get_transport()) if f_type == 'config':
f="'f.${def3}.${def3}'"             #todo </s> out2 = output(cmd.format(**locals()))	test_6133_run_default_services_from_single_service_saved_container cmd = "docker exec {testname} systemctl enable zzc.service" sh____(cmd.format(**locals())) logg.info("\n>\n%s", out2) cmd = "docker commit -c 'CMD [\"/usr/bin/systemctl\",\"init\",\"zzc.service\",\"{cov_option}\"]'  {testname} {images}:{testname}"
# todo: actually kill the start/stop/restart/status command for 5.11 </s> self.collector.stop()	_do_restart if self.collector:
# # fixme: # todo: remove me </s> pass	load_blacklist for line in lines: redis_crawler.sadd('blacklist_{}'.format(service_type), line)
# todo: once the translation process is able to correctly extract </s> def __call__(self, data, untagged_key, param_key, param_value, value, locale_identifier=none):	UntagParamRegex class UntagParamRegex(object): def __init__(self, value): if not self.value: return False
# todo generator </s> assert(not completed)	Drop to_drop, refds_path=refds_path, for ds_path in content_by_ds: ds = Dataset(ds_path)
# todo: doc </s> if self.args.verbose:	predictTestset nbIgnored += 1 continue  # Back to the beginning, try again tqdm.write(predString) f.write(predString)
# todo (abhikpal, 2017-06-06) </s> print("log size is {} bytes".format(log_size.value))	log_info if log_message.value: if verbose: print("Shader source:") print(self._source.decode('utf-8'))
raise notimplementederror  # todo </s> raise notimplementederror  # todo	Perturbation generator = None return self.compute_generator(generator, verbose=verbose)
# todo(qingqing) : redirect c++ ostream to python stream. </s> counters/options for profiling by `config` argument. the default config	cuda_profiler programming interface. The profiling result will be written into `output_file` with Key-Value pair format or Comma separated values format. is ['gpustarttimestamp', 'gpustarttimestamp', 'gridsize3d', 'threadblocksize', 'streamid', 'enableonstart 0', 'conckerneltrace'].
# todo: if either ref or ref0 are not scalar and the output is </s> ----------	_setup_var_sizes def _setup_var_sizes(self, recurse=True): Compute the arrays of local variable sizes for all variables/procs on this system. recurse : bool Whether to call this method in subsystems.
# todo(b/132329316) remove when `xla.compile` allows tf.device(tpu). </s> with tf.device(none):	TPUStrategyTest def forward(): step = lambda: golden.create_all_variables(mod) variables_per_replica = forward() self.assertLen(variables_per_replica, golden.num_variables)
# todo: allow user to return an ordereddict </s> the default state initialization routine. it is not made a method	zero_state to ensure that the brick argument can be omitted. return tensor.zeros((batch_size, dim), dtype=theano.config.floatX)
#todo - introduce an annotated alignment class? </s> if identifier.find("/")<>-1 :	_identifier_split start_end = identifier.split("/",1)[1] if start_end.count("-")==1 :
# todo: docs and comments </s> feats[video_id]["label"].append(label.cpu())	extract_feats@22 sm = output[j] label = target[j] feats[video_id]["clip_id"].append(clip_id) sp = os.path.join(args.output_dir, "features.pth")
# todo: remove this method in v2.5 </s> partition=self.want.partition	remove_from_device def remove_from_device(self): resource = self.client.api.tm.gtm.datacenters.datacenter.load( ) resource.delete()
# todo: ensure encoding </s> update()	load_cached def load_cached(): return read()
#todo - parse it? </s> self.check("psiblast", applications.ncbipsiblastcommandline)	test_psiblast
# todo kill state dependency </s> if len(sel) > 0 and any([not s.empty() for s in sel]):	on_activated other_view = window.active_view_in_group(group) if other_view and other_view != view: sels_begin_at_pt = sel[0].begin() sel.clear()
# todo: should we only export keys with signing capabilities? </s> verifies the passed signature against the passed content using the	gpg_verify_signature def gpg_verify_signature(signature_object, pubkey_info, content): passed public key. The function selects the appropriate verification algorithm (rsa or dsa)
# naive implementation (todo) </s> with open(file_path, 'r') as content_file:	_get_normalized def _get_normalized(self, file_path): if file_path in self._normalized: content = content_file.read() normalized = self._normalize_script(content)
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
# todo: write code to add selected indirect virtual deps to </s> continue	_greedy_slots pkg.root, dep_str, pkg.use.enabled, parent=pkg, strict=True) blocker_atoms = (x for x in atoms if x.blocker) blockers[pkg] = InternalPackageSet(initial_atoms=blocker_atoms)
# todo: if !blocking... </s> try:	SharedLock assert not self.__is_owned(), "double acquire() on a non-recursive lock" if shared: if self.__nwait_exc > 0: self.__wait(self.__turn_shr)
# todo: compare to plain for loop through the labels </s> dataset.chunks == c))	getSamplesPerChunkLabel count[cc, lc] = N.sum(N.logical_and(dataset.labels == l,
# todo: determine why this even happens, as it shouldn't be possible </s> frame = round((milli / 1000.0) * (fps_num / fps_den)) + 1	secondsToTime day = day % 7
# todo: instead of hiding..., which may consume memory... why don't killing? </s> distance = distance / 100.0	_response_length_distance distance = distance % 100
raise exception('lol') #todo fixme </s> for l in self._listeners:	Settings def add_change_listener(self, l): self._listeners.append(l) l.setting_changed(name, old_value, new_value)
# todo(zshi) remove this check when classic drivers are removed </s> existing_text = fp.read()	store_agent_certificate if os.path.exists(existing_verify_ca): try: except EnvironmentError: with excutils.save_and_reraise_exception():
'value': t.text,  # only for easy identification during debugging. todo: delete </s> maxy = page['height'] * (1 - footer_ratio)	get_bodytexts def get_bodytexts(page): return list(filter(lambda t: t['top'] >= miny and t['bottom'] <= maxy, page['texts']))
# todo: work around this </s> import_completions.insert(0, (u"qualified", "qualified "))	get_special_completions if qualified_match: qualified_prefix = qualified_match.group('qualifiedprefix') return import_completions return None
# todo add options to modify the columns </s> print('thank you for using poseidon')	do_quit def do_quit(self, arg): self.close() return True
# todo: test logging messages. </s> if isinstance(message, exception) and ( len(self.messages) < 2 or self.messages[-2] is not message):	MockTransport self.states.append('running') def deliver(self, message): raise message def shutdown(self):
# todo: can we assume reverse=false? </s> return queryset	most_common ).order_by('-num_times') if min_count:
1  # todo: fill in identifier </s> for stat in ordered_stats:	print_stats return ''.join(line) highest_time = 0 for line in _stack(stat): highest_time = max(highest_time, line.average)
# todo: prepare this above </s> self.lazywhere = lazyloader.lazywhere	LazyLoadInstance for key, value in lazyloader.binds.iteritems(): self.params[key] = row[key] def __call__(self): return self.mapper.select(self.lazywhere, **self.params)
# todo: we need to insert a linebreak here, but there is no </s> if line >= self.get_line_count():	MeldBuffer class MeldBuffer(meld.util.sourceviewer.srcviewer.GtkTextBuffer): __gtype_name__ = "MeldBuffer" return self.get_end_iter() return self.get_iter_at_line(line)
# todo: find a better random value </s> if default is default_parameter:	AccessPoint try: item = results.next() raise ItemDoesNotExist return default
# todo: remove once typeshed supports literal types </s> _winapi.setnamedpipehandlestate(self.connection,	IPCClient raise IPCException("The connection is busy.") else: _winapi.PIPE_READMODE_MESSAGE, None,
# todo: fix circular imports </s> raise valueerror('must pass either `auth` or `user`')	can_edit :param User user: User object to check :returns: Whether user has permission to edit this node. if auth and user: raise ValueError('Cannot pass both `auth` and `user`')
# todo disconnect pub/sub </s> def msg_sub(self, msg):	msg_sub
# todo: add at least reflection tests before adding notimplemented version </s> tracklist_slice = protocol.range(parameter)	playlistinfo if parameter is None or parameter == '-1': start, end = 0, None start, end = tracklist_slice.start, tracklist_slice.stop tl_tracks = context.core.tracklist.tl_tracks.get()
# todo(pkilambi): process the output as needed </s> try:	service_delete @staticmethod def service_delete(uuid): out, err = utils.trycmd('kubectl', 'delete', 'service', uuid) if err:
# todo: remove in 1.4 </s> the url including script_name and path_info, but not query_string	path_url @property bpath_info = bytes_(self.path_info, self.url_encoding) return self.application_url + url_quote(bpath_info, PATH_SAFE)
# todo: adjust for dhcpv6 </s> utils.set_virt_disk_driver(self, driver)	set_virt_disk_driver def set_virt_disk_driver(self, driver: str): Setter for the virtual disk driver that will be used.
# todo: perhaps unify all data collection in one single context. </s> self.settings.vi['expecting_user_input'] = value	expecting_user_input @expecting_user_input.setter
# todo(hub-cap):fix this ugly hack! </s> run_as_root=true)	load_mysqld_options def load_mysqld_options(): try: arglist = re.split("\n", out)[1].split() args = {}
# todo: this can be removed for cartopy > 0.14.3 </s> return xoff * self.spacing, yoff * self.spacing	_handle_location if is_string_like(location): location = self.location_names[location]
)  # todo: figure out the number of frames independent of 3d detection </s> self.notify_all({"subject": "eye_process.should_stop", "eye_id": eye_id})	Offline_Pupil_Detection else: return 0.0 self.eye_video_loc[eye_id] = None def recent_events(self, events):
# todo: add the parts to the music << >> </s> return self._printer	printer
# todo: gpt </s> else:	printk with open("/proc/sys/kernel/printk", "w") as fpk: if enable: fpk.write("0")
#todo: does not keep case </s> p.num()	test_num ret = p.num("3") self.assertEqual(p.persistent_count, 3) ret = p.num(count=3, show=1) self.assertEqual(p.persistent_count, 3)
# todo: refactor common tests for all models, e.g. shape checking </s> trans_h = transh(triples_factory=self.factory)	test_trans_h self.assertIsNotNone(trans_h)
# todo: a possibility to call different wine binaries </s> return getattr(args, name, false)	flag def flag(name): Convenience function for accessing boolean flags in args.
# xxx todo: real error handling, as this is probably going to </s> create_root_dir()	PrepareSystemForInstallationTask def run(self):
# todo: warn if field has_choices but not in table.filtering </s> except programmingerror:  # does not exist	Command with transaction.atomic(): try: pass finally:
# @todo: crud strings </s> t = current.t	ContactTracingModel names = ("disease_contact", ) db = current.db define_table = self.define_table
# :todo: implement test. </s> self.skiptest('not implemented yet.')	GetTransactionsToApproveRequestFilterTestCase Request is empty, so default values are used for all parameters. self.skipTest('Not implemented yet.') def test_fail_depth_float(self): self.skipTest('Not implemented yet.')
# todo xxx: bug 593055 </s> r = self.client.get(url, follow=true)	TestEmailChange self.assertEqual(self.user_profile.email, 'jbalogh@mozilla.com') url = reverse('users.emailchange', args=[self.user.id, self.token, eq_(r.status_code, 200) u = User.objects.get(id=self.user.id).get_profile()
# todo: why don't we return a arraywithunit? </s> return self.__class__(np.array(self).__neg__(),	__neg__ unit_type=self.unit_type, unit=self.unit)
# todo: remove </s> pass	_render_paginator try: del query_dict[page_var] extra_query = "" if query_dict:
# todo.before_release: hack for reproducing the exact results we have in </s> logger.record_tabular('finalgoaldistancemin', np.min(goal_dists))	Pusher2dEnv logger.record_tabular('FinalGoalDistanceMax', np.max(goal_dists))
# todo: higher dimensions? happens often in statistics </s> that is checked is defined at the top of sage/interfaces/r.py.	available_packages OUTPUT: list of strings .. note:: EXAMPLES:: sage: ap = r.available_packages()   # optional - internet
pass  # todo: implement this </s> name = unicode(c.data(0, name_role).tostring())	set_state self.verticalScrollBar().setValue(state['pos']) for parent in self.categories.itervalues(): if name in state['selected']: c.setSelected(True)
and not self.allow_all_insecure):  # todo: remove after release </s> self._base_url = self.url	base_url if base is not None and base.get("href"): self._base_url = base.get("href") return self._base_url
# todo(tsileo): handle tombstone </s> return bleach.clean(html, tags=allowed_tags)	clean_html
# todo: pytest.warns is not supported until pytest >= 2.8.0, whose </s> def start(self):	CallCollectorMixin def setup(self): self._log_call(('setup')) self._log_call(('start')) super(CallCollectorMixin, self).start()
# todo: min() and max() patches do nothing useful at present. just remove? </s> return (self // other, self % other)	__divmod__
# todo: specific exceptions, useful error reporting </s> if len(node.children) == 0:	generate_node_html_lines t   = node.formtype() indent = " "*6*depth lines.append(indent+'<div class="item">') lines.append(indent+'  <div class="item_content">')
# todo: get rid of this after we remove reconfig. </s> finite difference form for check, can be "forward", "central", or "backward". leave	set_check_partial_options method : str Method for check: "fd" for finite difference, "cs" for complex step. undeclared to keep unchanged from previous or default value. step : float
# todo: write tests for handle_status </s> self.curkey['fingerprint'] = args[9]	fpr def fpr(self, args):
# todo: fix clone issue </s> assert (pred_proba.min() >= 0)	test_prediction_proba_linear pred_proba = self.clf.predict_proba(self.X_test, method='linear')
# todo: replace with "yield from" when dropping python 2. </s> yield stream	Euronews if urlparse(url).path.endswith("m3u8"): streams = HLSStream.parse_variant_playlist(self.session, url) else: name = source.get("label", "vod")
# todo: check if a valid ipfs hash </s> return get_index_file_contents(path)	get_cached_index_file_contents @lru_cache()
# xxx todo - do not duplicate code!!! </s> for k, v in sorted(d.items(), key=lambda x: x[0]):	print_results if isinstance(v, dict): print(prefix + k + ":")
# todo: return proper searchable iterator </s> data = self._to_send_data()	wave def wave(self, first: bool = True) -> str: Args: data["action_type"] = "ma-type:user-generated-message" data["lightweight_action_attachment[lwa_state]"] = (
#todo, multipart raw submissions need further parsing capacity. </s> instance = request.files['xml_submission_file'].read()	post@28 a different signature as play, only passing in the document (since we don't know what xform was being posted to) else: instance = request.raw_post_data
# todo: check if we can use orm to do that </s> :return: list of last opinions	PublishableContentManager return published def get_last_opinions(self): :rtype: list home_number = settings.ZDS_APP['opinions']['home_number']
# todo: flip this around when statemutability is output instead </s> {}	test_extract_sigs_ignores_imports def test_extract_sigs_ignores_imports(): @public def foo() -> uint256:
pass  # todo(zcd) </s> data = np.ones(out_tensor.shape(), dtype=np.float64)	__set_tensor__ out_dtype = out_tensor.dtype() if data is None: elif out_dtype == core.DataType.FP32: data = np.ones(out_tensor.shape(), dtype=np.float32)
# todo: switch to split tokenizing? much faster </s> null=self.word_dict[self.word_dict.null_token],	DrqaAgent if len(examples) == 0: return batch_reply cuda=self.opt['cuda']) if 'labels' in observations[0]:
# todo: deleting remote folders involves reimplementing </s> ),	trash_or_confirm@49 secondary=_( "This remote location does not support sending items " buttons=[ (_("_Cancel"), Gtk.ResponseType.CANCEL),
# todo: other types than can have series inside: list, set, etc. </s> s = fill	_column_fillna_impl if np.isnan(s):
# todo: add more checks here? </s> def _is_compressed(self, swf_document):	SWFParser if magic in ('FWS', 'CWS'): return True :param swf_content: The SWF file. :return: True if the SWF is compressed
# todo: handle case where end of sysex is reached too </s> names.remove('time')	put_byte@65 names = list(typeinfo.names) if opcode <= 0xf0: if len(names) == len(data): args = {}
# todo add verbose output </s> self._localedir = new_localedir	LocalizationModel@8 return self._localedir @localedir.setter
try: # todo: fix this. if not in the scanning workbench, _drawmachine() fails. </s> self._mousex = -1	OnMouseLeave
# todo(rakhmerov): why is it here? this module is too generic. </s> return _get_greenlet_local_storage()[var_name]	get_thread_local def get_thread_local(var_name): if not has_thread_local(var_name):
# todo: if py3k, override unpickler.find_class(). </s> except exception as e:	MtimeDB d = mypickle.load() except SystemExit: try: d = json.loads(_unicode_decode(content,
# todo verify permission type for the provided resource type </s> :type user_db: :class:`userdb`	user_is_admin def user_is_admin(user_db): Return True if the provided user has admin rule, false otherwise. :rtype: ``bool`` return user_has_role(user_db=user_db, role=SystemRole.ADMIN)
# todo: use rule in raw table not default chain policy </s> if module in self._module_refcount:	handle_modules self._module_refcount.setdefault(module, 0) self._module_refcount[module] += 1 self._module_refcount[module] -= 1 if self._module_refcount[module] == 0:
# self.assertisnotnone(cursor.query_planning_time_in_millis)  # todo flaky test </s> def cancel(c):	test_cancel @with_pandas_cursor() time.sleep(random.randint(5, 10)) c.cancel()
# todo: remove in v2.8 </s> return request.user.is_authenticated	has_permission def has_permission(self, request, view): if not settings.LOGIN_REQUIRED:
# todo tests </s> if not isinstance(when, datetime):	ActionLogData@13 class ActionLogData: raise TypeError( "'when' should be 'datetime' or 'str', got: {}"
# todo: support multiple selections </s> if exact:	Marks else: win, view, rowcol = _MARKS.get(name, (None,) * 3) rowcol_encoded = ':'.join(str(i) for i in rowcol) else:
# todo: add primitive_type info in debug info later. </s> add_gradient(arr, grad)	_cumulate_gradient _logger.fatal('Number of gradients does not match.') for sub_arr, sub_grad in zip(arr, grad):
# todo(paul): why does 'event' not have a 'user' object? </s> }	get_messages "chunk": [e.get_dict() for e in events], "start": pagin_config.from_token.to_string(), defer.returnValue(chunk)
# todo(vek): need to pass context in for access to auth_token </s> assert state in power_state.valid_states(), "bad state: %s" % state	InstanceInfo self.name = name
raise notimplementederror()  # todo </s> setting_name,	MouseSettings "The %s device has no '%s' setting" % ( ) )
# todo(mlavalle) this notification should be updated to publish when </s> return [	get_address_groups pager = base_obj.Pager(sorts, limit, page_reverse, marker) address_groups = ag_obj.AddressGroup.get_objects( self._make_address_group_dict(addr_group, fields) for addr_group in address_groups
# todo: remove once elasticsearch v6.x is deprecated. </s> 'dynamic_templates': [{	ElasticsearchOutputModule mappings = self._DEFAULT_MAPPINGS if self._raw_fields: 'strings': { 'mapping': {
# todo: in the future we should be able to choose different providers </s> s = self.recv_packet()	__raw_stor_file read_len = len(read_data) self.__send_smb_packet(SMB.SMB_COM_WRITE_RAW, 0, 0, tid, 0, pack('<HHHLLHLHH', fid, read_len, 0, write_offset, 0, 0, 0, 0, 59), '') if self.isValidAnswer(s,SMB.SMB_COM_WRITE_RAW): self._sess.send_packet(read_data)
# todo: rewrite tests </s> assert_equal(self.user.timezone, 'america/new_york')	TestUserProfile url = api_url_for('update_user', uid=self.user._id) self.app.put_json(url, payload, auth=self.user.auth) def test_update_user_locale(self): assert_equal(self.user.locale, 'en_US')
pass # todo </s> def add_directory(name=str):	add_directory
# todo deprecate? </s> if rpath is not none:	pocket @property def dal_module(self): from .core.common import import_dir return import_dir(rpath, '.dal')
# todo: figure out if we really need to override these methods, or if there is a  bug in the default </s> self._loaddata(data[0])	reload def reload(self): self.initpath = '/playlists/{0!s}'.format(self.ratingKey)
# todo: with git <= 2.3 keep old mechanism: </s> return self.repo.active_branch.name	git_get_active_branch
# todo(mattjj,phawkins): improve this implementation </s> try:	wraps @curry fun.__name__ = namestr.format(fun=get_name(wrapped)) fun.__module__ = get_module(wrapped)
# todo: warn on failure to delete? </s> key_offset= dt.lower().find(keymap[t].lower())	__generate_input_and_label s  = indent+'    <input id="%s%s" type="radio" name="%stype" value="%s" %s/>' % (prefix, t, prefix, t, dstr) s += '<label for="%s%s">' % (prefix, t) else: key_offset = -1
# todo maybe return suitable values for the last property </s> tokens = self.tokens = []	Analyzer self.column = column = cursor.position() - block.position() self.text = text = block.text()[:column] for t in tokeniter.tokens(cursor.block()): if t.end > column:
# todo(sloria): test me </s> file_obj = dropboxfile.find_one(q('node', 'eq', node) & q('path', 'eq', path))	dropbox_render_file @must_have_addon('dropbox', 'node') def dropbox_render_file(path, node_addon, auth, **kwargs): client = get_node_addon_client(node_addon) rev = request.args.get('rev')
# todo: test require restart </s> % example3_test_zone)	TestMigrateDNSSECMaster assert wait_until_record_is_signed( self.replicas[1].ip, example3_test_zone, timeout=200 assert wait_until_record_is_signed( self.replicas[0].ip, example3_test_zone, timeout=200
# todo: remove in v1.2 </s> metric="minkowski",	KNeighborsClassifier algorithm="auto", leaf_size=30, metric_params=None, n_jobs=None,
# todo: allow for defining custom path param options in the </s> def localize_path(cls, pod_path, locale):	localize_path multi-file localization.""" pod_path = cls.clean_localized_path(pod_path, locale)
# todo do a proper mro resolution. currently we are just listing </s> from jedi.evaluate.context.typing import annotatedsubclass	define_generics def remap_type_vars(): for type_var in self.list_type_vars():
# todo: check error location </s> 'nest': graphqlfield(datatype(), resolver=lambda *_: data())	get_fields def get_fields(self): return {
# #todo make sure we switch to correct units for machine when saving file </s> self.hidecolumn(num+2)	_hal_init for num in range(0, 9): if num in (INFO.AVAILABLE_AXES_INT):
# :todo: implement test. </s> iota(self.adapter).sendtransfer,	SendTransferCommandTestCase self.assertIsInstance(
# @todo: extend entity_types within the template </s> args = filename,	doc_image_represent if not filename: return current.messages["NONE"] ), ),
# todo: use a better measure then first tab </s> else:	toggle_cursor args and kwargs are used to construct which.""" if self.current_cursor == which: self.set_cursor(which, *args, **kwargs)
# todo(phawkins): enable test after jaxlib 0.1.22 is released. </s> return lsp_special.logsumexp(array_to_reduce, axis, keepdims=keepdims)	lax_fun
# todo: replace with "yield from" when dropping python 2. </s> return	_get_rtmp_stream swf = swfdecompress(res.content) match = _rtmp_re.search(swf) res = http.get(match.group(1)) rtmp, playpath = rtmpparse(res.text)
return cursor_offset, line #todo not implemented </s> if cursor_offset == 0:	backspace @on('\x7f') @on('KEY_BACKSPACE') return cursor_offset, line if not line[:cursor_offset].strip(): #if just whitespace left of cursor
# todo: this operation will cause damage to disk data which should be limited </s> self.__system("/usr/sbin/service ix-inadyn quietstart")	_restart_dynamicdns self.__system("/usr/sbin/service inadyn restart")
# todo: direct use of json['error'] is deprecated; use display_message('...', 'error') instead. </s> j_dic['events'] = []	enrich_json_with_base def enrich_json_with_base(j_dic): j_dic['offset'] = 0 j_dic['triggers'] = [] j_dic['modifications'] = []
# todo(user): remove after 184 is out. </s> f.write(str_buf)	RecordsPool (_FILES_API_MAX_SIZE, len(str_buf))) start_time = time.time() if self._ctx: operation.counters.Increment(
# todo: warning </s> def handle_incident(self, icd):	handle_incident
# todo(israt) :add reduce func to suport the following reduce op </s> assert g.device == f.ctx()	create_test_heterograph assert g.idtype == idtype
# todo: add back: </s> 0.35832712500000063	to_datetime >>> timeit.timeit( ...    lambda: repr(ks.to_datetime(s, infer_datetime_format=True)), >>> timeit.timeit( ...    lambda: repr(ks.to_datetime(s, infer_datetime_format=False)),
pass  # todo </s> config = {	ThreeDScene "camera_config": { "samples": 4,
# todo. optionally sort on birthdate </s> ('gallery', _('gallery'), self.use_gallery),	display_nav_links ('individuals', _('Individuals'), True), ('sources', _('Sources'), True), ('download', _('Download'), self.report.inc_download), ('contact', _('Contact'), self.report.use_contact),
# todo(yanase): remove number from system_attrs after adding trialmodel.number. </s> def test_get_study_id_from_name_and_get_study_name_from_id(storage_mode):	test_get_study_id_from_name_and_get_study_name_from_id with StorageSupplier(storage_mode) as storage: function_name = test_get_study_id_from_name_and_get_study_name_from_id.__name__
# todo: refactor me, please! </s> self.__altered_path = folder_path	ImageProcessing folder_path = os.path.join(self.__altered_path, folder_name) if (not os.path.isdir(folder_path)): path = self.__altered_path self.__image_steps = [self.__input_path] + [
options['taskid'] = none # todo </s> provies content management on a consumer	ConsumerContentManager def install(self, id, units, options={}): Install content on a consumer.
# todo: use unshare() here </s> proc = subprocess.popen(cmd, stdin=subprocess.pipe)	mk_dm cmd = 'dmsetup create --'.split() + [devname] if readonly: proc.communicate(table.encode('ascii')) assert proc.returncode == 0
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
# todo: move to base class </s> continue	getNodesRect for n in self.getAllNodes(): if activeGraphOnly: n_rect = QtCore.QRectF(n.scenePos(), QtCore.QPointF(n.scenePos().x() + float(n.w), n.scenePos().y() + float(n.h))) rectangles.append([n_rect.x(), n_rect.y(), n_rect.bottomRight().x(), n_rect.bottomRight().y()])
print("got result: >%s<" % buffer) # todo remove. </s> while len(result) > 0 and not self._do_quit:	Receiver try: result = "-NO DATA-"  # so len(result) is > 0 result = conn.recv(BLOCK_SIZE) result_buffer += result
# todo : documentation pending </s> if key.startswith('download_warning'):	get_confirm_token def get_confirm_token(response): return value return None
# todo: should be able to use lib.godot_string_new_with_wide_string directly </s> cls = type(instance)	pybind_instance_notification @ffi.def_extern() def pybind_instance_notification(instance_handle, notification): for parentcls in inspect.getmro(cls): try:
raise notimplementederror # todo </s> raise notimplementederror # todo	Site raise NotImplementedError # TODO def remove(self, item):
# todo: this procedure would leave a clean dataset, but `run` cannot handle dirty </s> 'code',	test_basics@41 ok_clean_git(ds.path) ds.config.add( where='dataset') ds.config.add(
# todo: remove the following line when issue #71 (preserve the trajdataframe index during preprocessing operations) is solved. </s> del lat_lng_dtime_other[i + 1 + imax]	_filter_array continue Dr, Dt = sum(DrDt[:imin, :]) lX = len(lat_lng_dtime_other) else:
return  # todo return placeholder "[loading]" artist? </s> tracks = filter(none, tracks)	to_playlist if username is not None and sp_playlist.owner.canonical_name != username: name = '%s by %s' % (name, sp_playlist.owner.canonical_name) return models.Playlist( uri=sp_playlist.link.uri,
# todo check the op returned a view </s> val = self.vm_call_time * 100 / self.call_time	summary_function print >> file, '' val = 0 print >> file, '  Total time spent in calling the VM %es (%.3f%%)' % ( self.vm_call_time, val)
#     todo </s> return	chmod def chmod(self, path, mode): cache_path = self.cache.get_path_or_dummy()
# todo check if this is getting updated </s> for package in self.packages:	test_initial_data self.assertTrue(package.repo)
# todo split up into several dependent tests -- need to check how this </s> else:	make_single_fclayer_modelwrapper model.set_tensor_datatype("weights", wdt) if binary_xnor_mode: model.set_initializer("weights", W) if T is not None:
# todo: verify that the post request was received </s> self.addnewcrawlui()	setupNewCrawl def setupNewCrawl(self, evt): self.newCrawlTextCtrl.SetFocus()
# todo fix. </s> self.__save_failing_context(ctx_init)	test_cmova x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) cmp_result = self.__compare_contexts(ctx_init, x86_ctx_out, reil_ctx_out) self.assertTrue(cmp_result, self.__print_contexts(ctx_init, x86_ctx_out, reil_ctx_out))
# todo use csv module </s> contact=contact)	registration@32 contact.save() backend = Backend.objects.get(name=backend_name) connection.save() return HttpResponseRedirect(
# todo: move this to a task queue </s> self.redis.hdel(key, self.read_prefix + self.public)	set_public else:
# todo docstring </s> d.addcallback(applications_from_units)	discover_node_configuration return applications
# todo: implement auto-dtype method in general parameters </s> self.generate_subshells(include_pre_edges)	add_elements self.mapped_parameters.add_node('Sample') self.mapped_parameters.Sample.elements = list(self.elements)
if self._ndim == 3: # todo: use hasz </s> lgeos.geoscoordseq_getx(self._cseq, i, byref(temp))	xy y = array('d') temp = c_double() x.append(temp.value) lgeos.GEOSCoordSeq_getY(self._cseq, i, byref(temp))
# todo: try/catch </s> for actor in config.get("actors"):	setup_actors a = class_for_name(actor["module"], actor["class"]) act = a(actor["id"], actor["params"])
# todo(reedwm): remove manual casts once mixed precision can be enabled with a </s> x = layers.dense(num_classes, name='fc1000')(x)	trivial_model@36 x = layers.Lambda(lambda x: backend.reshape(x, [-1, 224 * 224 * 3]), name='reshape')(img_input) x = backend.cast(x, 'float32') x = layers.Activation('softmax')(x)
# todo: determine proper template to use. </s> exitcode = proc.returncode	get_exitcode_stdout_stderr args = shlex.split(cmd) proc = Popen(args, stdout=PIPE, stderr=PIPE) return exitcode, out, err
# todo: reuse code in ..diaggn.conv2d to extract the diagonal </s> batch, channels, pixels, classes = (	separate_channels_and_pixels def separate_channels_and_pixels(self, module, sqrt_ggn_out): into module.input0.size(0), module.out_channels,
#todo: test me </s> def free(self):	free
# todo: test this </s> return 'initial'	get_value def get_value(style, name): Return the value of a property as a string, defaulting to 'initial'. values = style[name] if hasattr(values, 'value'):
# todo: this should be abstracted into a property/method or something </s> raise improperlyconfigured, 'you need to register at least one template for page before the admin code is included.'	_needs_templates @classmethod def _needs_templates(cls):
# todo workaround for https://bugzilla.mozilla.org/show_bug.cgi?id=1411264 </s> def click(self):	Email return self.root.text
# todo: add cntk </s> "partial derivates."	gradients backend = K.backend() if backend == "theano":
# todo: raise exception with preferred method </s> self.base = req.url	RequestError super(RequestError, self).__init__(message) self.req = req self.error = req.text
return s #todo return partial result instead of giving up </s> self._set_cursor_offset(len(self.current_line), reset_rl_history=false)	down_one_line self._set_current_line(tabs_to_spaces(self.rl_history.forward(False, search=self.config.curtsies_right_arrow_completion)),
# todo the calls to sleep were added in an attempt to make this tests </s> 'mine.send',	test_send ) self.assertTrue( ['grains.items'], minion_tgt='minion',
#todo pliki specjalne </s> def mod(self, *args):	_pathdec args = list(args) try:
# todo check argument kinds </s> sbuiltin = nearest_builtin_ancestor(s.type)	is_overlapping_types if t.type in s.type.disjoint_classes: return False if tbuiltin in sbuiltin.mro or sbuiltin in tbuiltin.mro: return True
# todo: this assert is probably not valid in all cases. </s> def runpytest(self, *args, **kwargs) -> pytest.runresult:	runpytest
# todo: support grouping and stacking at the same time </s> def xmax(self):	xmax @property
# todo: each dp learns independently. an edge dp could </s> ofmsgs.extend(self._add_controller_learn_flow())	_add_default_flows ofmsgs.extend(self._add_vlan_flood_flow())
# todo(ochang): remove this once migrated to python 3. </s> signal.signal(signal.sigint, signal.sig_ign)	test_worker_init def test_worker_init():
# todo: move 'hardcoded' coordinate specs (name, units, etc) into tile_spec </s> return time	_parse_time def _parse_time(time): if isinstance(time, compat.string_types):
# todo(crcrpar): annotate this correctly. </s> .. note::	experimental_version "The interface can change in the future".format(func.__name__, version)) if func.__doc__ is not None: Added in version {} as experimental feature. The interface can change in the future. return func(*args, **kwargs)  # type: ignore
# todo: add notification related to command-line options for </s> let each of the plugins add its own command line arguments, which	add_plugin_args may or may not be displayed as help topics. for name, plugin_ep in plugins.iteritems():
# todo: revise this when finat gets dual evaluation </s> return i / order	vtk_interval_local_coord elif i == order: return 1.0
# todo(tfmot): renable once savedmodel preserves step again. </s> model = keras_test_utils.build_simple_dense_model()	_get_pretrained_model self._train_model(model, epochs=1) return model
# todo make sure this works </s> pass	createGroup
# todo github.com/clusterhq/flocker/pull/897#discussion_r19024474 </s> },	test_moving_data u"nodes": { node_1: [application], } flocker_deploy(self, deployment_config, application_config)
# todo debug </s> def __init__(self):	RuleElement self.type = None self.triggered = False
# todo add weight regularization (l2) </s> if training is none:	SparseDropout class SparseDropout(tf.keras.layers.Dropout): training = tf.keras.backend.learning_phase() to_retain_prob = tf.random.uniform(
# todo -- make sure it's always a scale </s> self.__dict__[key] = value	__scale__ if key == "default" and key in vars(self): self.default.set(value) return def __getitem__(self, key):
# todo: must be implemented </s> def should_fetch_kindlegen():	should_fetch_kindlegen
# todo: verify logic for create -- we shouldn't 'annexify' non-annexified </s> class _earlyexit(exception):	_EarlyExit
# todo change to check for error when the functionality changes. currently acts as though it doesn't exist </s> res = self.app.get('/api/v2/')	TestApiBaseViews class TestApiBaseViews(OsfTestCase): assert_equal(res.status_code, 200)
# todo: handle /dev/null (windows equivalent?) for new or deleted files </s> if before.endswith('.ipynb') or after.endswith('ipynb'):	show_diff@58 def show_diff(before, after): If we are diffing a notebook, show the diff via nbdiff. nbdiffapp.main_diff(before, after) else:
# todo: the following reproduces the old behavior of </s> afinal.append(numpy.polyval(p, 1.0))	calc_afinal p = [1] for j in range(len(cp) - 1): return afinal
# todo only do these things if status is true </s> routing_key, my_obj = item	format_rabbit_message def format_rabbit_message(self, item): the message should be item = (routing_key,msg) self.logger.debug('rabbit_message:{0}'.format(my_obj)) my_obj = json.loads(my_obj)
# todo(frostig): might the following work? </s> return pmin_p.bind(x, axis_name=axis_name)	pmin Returns: An array with the same shape as ``x`` representing the result of an
# todo: still in progress </s> def __init(self):	__init
# todo find out what is best used here! </s> uniformfloathyperparameter(name="lambda_2", log=true,	get_hyperparameter_search_space lower=10 ** -10, upper=10 ** -3, default=10 ** -6)) lower=10 ** -10, upper=10 ** -3, default=10 ** -06))
# :todo: implement test. </s> def test_fail_depth_float(self):	ReplayBundleRequestFilterTestCase def test_fail_depth_string(self): ``depth`` is a string. ``depth`` is a float. self.skipTest('Not implemented yet.')
# todo(nnorwitz): enable test. </s> class('bling')]	testTemplateWithMultipleTemplateArgsMid self.assertEqual(1, len(result)) types = [Class('Foo'), self.assertEqual(Class('Bar', templated_types=types), result[0])
raise mpdnotimplemented  # todo </s> obtain a list of all channels. the response is a list of "channel:"	channels@43 def channels(context): *musicpd.org, client to client section:* lines. raise MpdNotImplemented  # TODO
# todo transfer headers, and authenticated proxies: not sure how to do it in chrome yet </s> self.driver.ensure_add_cookie({'name': c.name, 'value': c.value, 'path': c.path,	transfer_session_cookies_to_driver for c in [c for c in self.cookies if domain in c.domain]:
# todo document me </s> @rtype: l{pulp.server.db.model.user} instance or none	_check_username_password_ldap @param username: the login of the user @type password: str or None @return: user corresponding to the credentials ldap_uri = "ldap://localhost"
)  # todo </s> res["rooms_section"]["rooms"].sort(key=lambda e: e.get("order", 0))	get_group_summary res["users_section"]["users"].sort(key=lambda e: e.get("order", 0))
# xxx lp: todo: verify key format </s> with open(filename, 'wt') as fp:	Metablock def __repr__(self): return canonicaljson.encode_pretty_printed_json(attr.asdict(self)) fp.write("{}".format(self))
#self.assertequal(  # todo: fix </s> url = reverse('app3:demo-url')	test_namespace_url_reverse self.assertEqual('/app3/demo/url', url)
# todo factor this out entirely </s> raise ioerror	load_plugin raise IOError if not os.path.exists(plugin_json_path) or not os.path.isfile(plugin_json_path):  # noqa log.info("Loading plugin {0}".format(plugin_json_path)) with open(plugin_json_path, 'r') as json_file:
# todo(kpy): remove support for legacy urls in mid-january 2012. </s> self.request.method = 'get'	head self.serve() self.response.clear()
# todo: test this method </s> raise forms.validationerror("extra code may not contain text outside tags (advanced use only).")	MetaDataForm def clean_extra(self): value = self.cleaned_data['extra'] return value
# todo: axis = 1 </s> returns	set_index Whether to append columns to existing index. inplace : bool, default False ------- DataFrame
# .. todo:: report an error to the user </s> for myitem in self.lstkeywords.selecteditems():	on_pbnRemove_clicked None. Raises: self.lstKeywords.takeItem(self.lstKeywords.row(myItem))
#ack = self.serial_port.read() # todo: use ack </s> except serial.serialexception:	Disconnect try: if self.serial_port.isOpen(): sys.stderr.write("Error closing the port {0}".format(self.serial_name))
# todo: this is untested. </s> return name	_get_name if name._name == _ffi.NULL: 1/0
model=pke.supervised.kea())  # todo: fix doc for model param </s> pke.utils.compute_document_frequency(	create_df def create_df(corpus_dir, tmp_path, name='corpus_df.gz'): str(corpus_dir), str(corpus_df_file), extension='txt', n=1) corpus_df = pke.utils.load_document_frequency_file(str(corpus_df_file))
# todo - gitlab issue #27 - only write cost data of controllable and in service elements </s> if el == "gen":	_make_objective@173 sign_corr = 1 for el in pd.unique(costs.element_type): idx = gen_idx elif el == "sgen":
# todo remove compatibility shims for anki 2.1.46 and lower. </s> deck_config = deckconfig(anki_dict)	DeckConfig decks = collection.decks get_conf = decks.get_config if hasattr(decks, 'get_config') else decks.getConf deck_config._update_fields() return deck_config
# integer case, todo: bool, date etc. </s> s = b[i]	_column_fillna_impl def _column_fillna_impl(A, B, fill):  # pragma: no cover if hpat.hiframes_api.isna(B, i): s = fill
# todo: switch _ignore_connection_aborted for _ignore_transmission_error, or provide retry mechanism </s> return {1: 'netmon results not saved in current database format'}	SessionInfo return {1: 'procmon results not saved in current database format'} @property @property def fuzz_node(self):
# todo proper error message </s> shape = batch_input_shape	SimpleSeq2Seq if isinstance(depth, int): depth = (depth, depth) elif input_shape: shape = (batch_size, ) + input_shape
# todo: re-enable custom_objects </s> def __init__(self, optimizer, additional_updates):	AdditionalUpdatesOptimizer super(AdditionalUpdatesOptimizer, self).__init__() self.optimizer = optimizer
# todo: remove compatability hook </s> if "__future__" not in ln:	get_executables for ln in fIn: if ln.strip(): break fOut.write(ln)
# todo(danms): remove this legacy fallback when secure rbac </s> def get_metadef_objects(self):	get_metadef_objects
return user.affiliation  # todo: update </s> continue	create_personal_data_fields missing = set(PersonalDataType) - existing for pd_type, data in PersonalDataType.FIELD_DATA: field = RegistrationFormPersonalDataField(registration_form=regform, personal_data_type=pd_type, is_required=pd_type.is_required)
# todo: not all messages have running status </s> spec = spec_by_status[status_byte]	read_message def read_message(infile, status_byte, peek_data, delta, clip=False): except LookupError: raise IOError('undefined status byte 0x{:02x}'.format(status_byte))
# todo: arrange </s> def createrepo(self):	createRepo repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "name", "testrepo0", self.token)
# todo: this should match on the app_label as well as the model name to avoid potential duplicate names </s> if unit == length_unit_foot:	to_meters return length if unit == LENGTH_UNIT_CENTIMETER: return length * 0.3048 if unit == LENGTH_UNIT_INCH:
# todo: -------------------------------------------------------------------- </s> may be in .git/config datalad.id	get_id def get_id(self):
# todo: evaluate history </s> x = dense(16)(x)	test_cdqn@53 action_input = Input(shape=(nb_actions,), name='action_input') observation_input = Input(shape=(1,) + env.observation_space.shape, name='observation_input') x = Activation('relu')(x) x = Dense(((nb_actions * nb_actions + nb_actions) / 2))(x)
# todo: per node message function </s> def readout(self):	readout
# todo allow user to edit a txt file in blender which contains the description or take readme? </s> return "" + self.indentation * self.indent	ind def ind(self): hierarchy.
# todo: check for expected warnings. </s> m.db.pool.shutdown()	_run_master reactor.callLater(0.01, d.callback, None) yield d
#todo: dont unfold all, but allow enum_all() to work </s> self.generate_context_menu()	tree_on_menu def tree_on_menu(self, id_dlg, id_ctl, data='', info=''):
# xxx todo </s> logging.getlogger(linkcheck.log).addhandler(handler)	Configuration logging.config.fileConfig(filename) handler = linkcheck.ansicolor.ColoredStreamHandler(strm=sys.stderr) if debug is not None: self['debug'] = True
# todo also check for motion codec parameter support </s> def is_motion_detected(camera_id):	is_motion_detected
# todo: remove at some point </s> current_packages = host.fact.rpm_packages	rpm info = host.fact.rpm_package(source) exists = False if ( info['name'] in current_packages
oldsize = self.size # todo: remove </s> def read_uint(stream):	read_uint
# @todo this is out because of the issue noted in the code. we'll </s> eq_(user['username'], 'admin',	test_account_password_failure params=params, status=403) "Should have a username of admin {0}".format(user)) ok_('error' in user,
#     todo </s> os.chmod(cache_path, mode)	chmod cache_path = self.cache.get_path_or_dummy()
# todo: check success summary the same way. </s> expected = """	test_reason_without_protocol def test_reason_without_protocol(): We deny to use Failure('reason') in stories defined without Failure("'foo' is too big") can not be used in a story without failure protocol. Function returned value: ReasonWithSimple.two
self.button_help = wx.button(self, label="help")                # todo maybe align left? </s> self.parent_tray_obj = parent_tray_obj	WallpaperPanel def __init__(self, parent, parent_tray_obj): wx.Panel.__init__(self, parent) self.sizer_main = wx.BoxSizer(wx.VERTICAL) self.sizer_top_half = wx.BoxSizer(wx.HORIZONTAL) # wallpaper/monitor preview
# todo: remove this when distributed materials are merged </s> string += '=\t{0: <12} [{1}]\n'.format(percent, percent_type)	_repr__ percent = self._nuclides[nuclide][1] percent_type = self._nuclides[nuclide][2] string += '{0: <16}\n'.format('\tElements') for element in self._elements:
time.sleep(1)  # delay, for last.fm latency. todo can this be removed later? </s> port = 1234	TestPyLastNetwork self.assertEqual(msg, "Unauthorized Token - This token has not been issued") def test_proxy(self): self.network.enable_proxy(host, port) self.assertTrue(self.network.is_proxy_enabled())
# todo: test coverage of this branch </s> elif confirm:	subscribe_user )[0] if instance.subscribed: instance.subscribed = True instance.save()
# todo(nnorwitz): it would be good to warn about this. </s> self.filename = filename	Include class Include(Node): def __init__(self, start, end, filename, system): self.system = system def __str__(self):
# todo fix. </s> asm = ["cmova eax, ebx"]	test_cmova ctx_init = self.__init_context() x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init)
# todo: link user's osf account with orcid </s> 'status': 'success',	unconfirmed_email_remove user.clean_email_verifications(given_token=given_token) user.save() 'removed_email': json_body['address'] }, 200
# todo: create xxx_failure test </s> self.asserttrue(check('test_space', failure_tests),	test_result_space_failure tests = exclude_from_resultlist(r, 'failure')
# todo necessary? </s> return [{"id": name + str(i), "x": x, "y": y, "type": "priority"}]	new_node
#todo: remove the below </s> query = q_revoke_default.format(grantor, schema, privilege, self.object_kind.upper(), self.rolename)	revoke_default self.sql_to_run.append(query)
# todo: this function should probably move to somewhere in casexml.apps.stock </s> yield child	commtrack_nodes def commtrack_nodes(node): for child in node: else: for e in commtrack_nodes(child):
# todo: proper java error? </s> raise notimplementederror()	call_static_float_method @native_method
#todo tuplet: add variables for if it's the start of a tuplet </s> cannot manually modify beams	getBeaming Note: So far only VexFlow's automatic beaming is supported
# todo: test for the _correct_ revision_id value. </s> elif detail.object_id == package_created['resources'][0]['id']:	test_create_package assert detail.activity_type == "new", str(detail.activity_type) if detail.object_id == package_created['id']: assert detail.object_type == "Resource", \ str(detail.object_type)
# todo: dynamically add/remove adapters </s> :param error: indicates an error (boolean)	_setupCallback def _setupCallback(self, result, error=False, **kwargs): if not super()._setupCallback(result, error=error, **kwargs): return
# todo: cronjob (celery task) to delete stale tokens </s> user = authenticate(user=user)	select_email user=user, provider=external_service, auth_login(request, user) messages.success(
# todo: detect the zygote and run 'art-on' automatically. </s> args:	handle_switches args: Parsed arguments. sysroot: Local sysroot path.
#todo: log all non-http errors to stderr </s> code=404, output='object does not have field "%s".' % key	get_object_item return val.field(key) except KeyError:
# todo: refactor to not use a try/except </s> class x(object):	X
# todo: require an api key on the basic auth header </s> id = db.column(db.integer, primary_key=true)	Release - Mark this release as abandoned. - Show me all active releases for this build by unique name in order creation_date = db.Column(db.DateTime, default=datetime.datetime.utcnow) name = db.Column(db.String(500))
# todo lib </s> return self._items_renamed	items_renamed @property
# no other choice fixme todo </s> self.model = kmeans(self.k, max_iter=1, init=self.centroids, n_init=1)	sklearnfit self.didsklearnfit=1
# todo(vek): need to pass context in for access to auth_token </s> def __init__(self, name, state):	InstanceInfo self.name = name assert state in power_state.valid_states(), "Bad state: %s" % state
#todo - check annotation </s> str(f_seq))	GenBankTests self.assertEqual(len(fa_record.seq), len(f_seq))
# todo: this test requires manifold access, see: t88318502 </s> def testpanopticfpn(self):	TestCaffe2Export self._test_model("COCO-Detection/retinanet_R_50_FPN_3x.yaml")
# todo: check that the performance measure is within some range </s> tests flow/benchmark/baselines/bottleneck2.py	TestBaselines Tests flow/benchmark/baselines/bottleneck1.py bottleneck1_baseline(num_runs=1, sumo_binary="sumo") bottleneck2_baseline(num_runs=1, sumo_binary="sumo") def test_figure_eight(self):
# todo debug </s> raise valueerror("no valid tag was found.")	parseRuleRecursively ruleElement.element = ruleSecondNew currentRule.elements.append(ruleElement) elif (currentRoot.tag == "and" or currentRoot.tag == "or"):
# todo: flush logs to generate a log per favorite event, so we can link </s> time.sleep(poll_frequency_s)	Start streams[key] = Stream(name=key.name()) streams[key].source = sources[key]
# todo: test this block </s> note:	html @property def html(self): * 'heading' and 'subheading' should not be included. * Be careful not to try to get the full html inside this template.
# todo username </s> '--add-key={monitor_secret}'.format(	create_mon keyring, '--create-keyring', monitor_secret=monitor_secret, ),
#    todo: maybe we're being too generic in this isinstance? </s> if not isinstance(error, socket.error):	get_socket_exception_reason def get_socket_exception_reason(error): :param error: The socket.error exception instance return EUNKNSERV = -2      # Name or service not known error
# todo(brett.cannon) implement </s> return []	mock_implicit_hooks
# todo: fix with stubber / before send event </s> def setup(self):	TestApiGateway super(TestApiGateway, self).setUp() self.region = 'us-west-2'
# todo: error </s> return true	handle_timeout_sustain
# todo: implement this </s> self.writestarted.emit(self)	requestWrite file_handler: Optional[FileHandler] = None, **kwargs: str) -> None:
# todo: hanlde multiple keys (index args) </s> return lambda df: 0	df_len_overload if len(df.columns) == 0:  # empty df
# todo(b/161952382): replace with keras premade models and </s> tf.cast(tf.zeros_like(taxi_fare), tf.int64),	preprocessing_fn tips = _fill_in_missing(inputs[_LABEL_KEY]) outputs[_transformed_name(_LABEL_KEY)] = tf.where( tf.cast( tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))
#todo load this from somewhere </s> if log.verbose[5]:	print_process time_factor = 0.0 if time_domain == 0.0 else float(sum(self.parent.run_times)) / time_domain complete = float(self.batch_idx + self.num_alloc_batches) / len(self.parent.batches) mem_usage = self.device_mem_usage_str(self.alloc_devices) info = [
self.router.send_multipart([address, '', payload])  # todo, send job id </s> hash(x)	ishashable def ishashable(x): return True except TypeError:
# todo: self.assertfalse(prop.is_valid(np.bool8(true))) </s> self.assertfalse(prop.is_valid(bar()))	test_Instance self.assertFalse(prop.is_valid([])) self.assertFalse(prop.is_valid({})) self.assertFalse(prop.is_valid(Baz()))
pass  # todo </s> else:	_onAddDevice cluster_size = int(properties.get(b"cluster_size", -1)) if cluster_size > 0: device = LegacyUM3OutputDevice.LegacyUM3OutputDevice(name, address, properties) self._discovered_devices[device.getId()] = device
# todo check checksum match </s> def __init__(self, namespace, concurrency=50, error_file=none):	Checker self.pool = GreenPool(concurrency) self.error_file = error_file
# todo: check num strings and support nan </s> ]	StringArrayPayloadModel members = [ ('index_offsets', types.CPointer(offset_typ)), models.StructModel.__init__(self, dmm, fe_type, members)
pass  # todo </s> assert self.train_started	train_finish_epoch def train_finish_epoch(self): assert len(self.forward_data_queue) == 0, "Not all forwardings were used?" assert self.is_forwarding_finished, "Forwarding not finished?"
# todo: lacp timeout configurable. </s> return valve_packet.packetmeta(	parse_rcv_packet eth_dst = eth_pkt.dst vlan = self.dp.vlans[vlan_vid] data, pkt, eth_pkt, port, vlan, eth_src, eth_dst, eth_type)
# todo: better scoring algorithm </s> sentence = re.sub(r'\w+', '', sentence)	format_sentence def format_sentence(sentence):
self.assertfalse(greps(err, "unit zzz.service not for --user mode")) #todo </s> execstart=/usr/bin/testsleep 111	test_6133_run_default_services_from_single_service_saved_container Description=Testing C [Service] [Install] WantedBy=multi-user.target""")
# todo: make these http requests asynchronous. not easy since we don't </s> object = {	Twitter id = tweet.get('id') if not id: 'objectType': 'note', 'published': self.rfc2822_to_iso8601(tweet.get('created_at')),
# todo: determine if this puts the case properties in the expected order. </s> hidden_value_path = question.value + "-" + option.value	Command existing_subcases = {c.name:c for c in form.actions.subcases} for question in questions: hidden_value_text = option.label if hidden_value_path not in question_dict:
# todo: check if we can avoid py3 specific here </s> level={true: logging.debug,	_log_err self.log("stderr| " + line.rstrip('\n'),
# todo: deprecate </s> it in subclass. here is an example::	save_authorization_code def save_authorization_code(self, code, request): client = request.client
# todo implement. </s> super(sparkmodel, self).__init__(keras_model, data,	SparkModel class SparkModel(DistributedModel): def __init__(self, sc, rdd, keras_model, data, optimizer, optimizer, master_port) self.spark_context = sc
# todo(piyush): current api-site doesn't contain this api description. </s> return service_client.responsebody(resp, body)	update_snapshot_status url = 'snapshots/%s/action' % str(snapshot_id) resp, body = self.post(url, post_body)
# todo(piyush): current api-site doesn't contain this api description. </s> body = json.loads(body)	list_tenants def list_tenants(self): resp, body = self.get('tenants') return service_client.ResponseBody(resp, body)
# todo debug </s> self.element = none	RuleElement self.triggered = False self.timeWhenTriggered = 0.0
# todo: write a unit test </s> return false	_function_matcher try: return matcher_func(node) return match
# todo: more tests </s> args=(true, "just 'expr' or 'expr is true'"))	ComparisonTest node = test_utils.extract_node("True == foo") message = Message('singleton-comparison', with self.assertAddsMessages(message): self.checker.visit_compare(node)
# todo: test for the _correct_ revision_id value. </s> details_length_before = len(model.session.query(	test_create_package Test that correct activity stream item and detail items are emitted when a new package is created. model.activity.ActivityDetail).all()) before = datetime.datetime.now()
# todo: triage </s> pdf_page_from_image_out = none	exec_page_sync if options.clean: preprocess_out = preprocess_clean(preprocess_out, page_context) if not options.lossless_reconstruction: visible_image_out = preprocess_out
# todo: create xxx_failure test </s> self.asserttrue(check('test_space', failure_tests),	test_result_space_failure r = run_set(p, 'result') failure_tests = r['failure'] lookup('test_space', tests))
# todo: use pabot options here </s> return []	DynamicTestItem variables.append("DYNAMICTEST:"+self.name) options['variable'] = variables def contains(self, other): return self == other
# todo(ihrachys): replace with port.create() once we get an object </s> class qospolicybasetestcase(object):	QosPolicyBaseTestCase
raise notimplementederror # the below does most probably not work anymore todo </s> if not a_to_b and not b_to_a:	detail return "It does not differ from `{0}`.".format(b) elif not a_to_b:
# todo assert exit code != 0 </s> dvol.parseoptions(args + ["-p", self.tmpdir.path,	test_commit_volume volume = self.tmpdir.child("foo") volume.child("branches").child("master").child( "commit", "-m", "hello from 30,000 ft"]) commitId = dvol.voluminous.getOutput()[-1]
# todo: add logging </s> result = tuple(fixparam[x] for x in params)	fixlist try: result = fixcache[params] fixcache[params] = result return result
# todo: create/clear alarm_data folder </s> for actor in config.get("actors"):	setup_actors a = class_for_name(actor["module"], actor["class"]) act = a(actor["id"], actor["params"])
# todo is this redundant now we have --dumptab? </s> users, _, resume = win32net.netwkstauserenum(wpc.conf.remote_server, 1 , resume , 999999 )	dump_loggedin print "\n[+] Logged in users:" try: for user in users: print "User logged in: Logon Server=\"%s\" Logon Domain=\"%s\" Username=\"%s\"" % (user['logon_server'], user['logon_domain'], user['username'])
# todo: add option for attentive reader </s> varscope.reuse_variables()	boe_reader_model targets = placeholders["answers"] with tf.variable_scope("embedders") as varscope: support_embedded = nvocab(support) print('TRAINABLE VARIABLES (only embeddings): %d' % get_total_trainable_variables())
# todo: remove? </s> self.case = none	StockReportParser self.domain = domain self.verified_contact = verified_contact u = verified_contact.owner if domain.commtrack_enabled:
raise notimplementederror  # todo... </s> array, start_idxs, batch_lens, beam_width = node.inputs	infer_shape beam_width = T.cast(beam_width, dtype="int64") array_shape, start_idxs_shape, batch_lens_shape, beam_width_shape = input_shapes
#todo: calculate _net_workarea for the monitor represented by </s> def __init__(self):	QuickTile dbus.service.Object.__init__(self, sessBus, '/com/ssokolow/QuickTile')
remote.fetch()  ### todo: show progress </s> repo = repo(path)	_update_directory dir_type = "bookmark" if is_bookmark else "directory" long_name = dir_type + ' "' + BOLD + path + RESET + '"' except exc.NoSuchPathError: print(ERROR, long_name, "doesn't exist!")
# todo: the one_hot=true is only necessary because one_hot=false is </s> self.train = mnist(which_set = 'train', one_hot=true)	setUp@9 skip_if_no_data()
# todo : the section permission here should be ql_x86_a_priv_3, but i do n’t know why it can only be set to ql_x86_a_priv_0. </s> if type(register_str) == str:	set_register register_str = self.get_reg_name(register_str) return self.ql.uc.reg_write(register_str, value)
# todo: remove when support for django 1.4 is dropped </s> 'a'	popattr >>> popattr(B, 'a', None) 'a' val = getattr(obj, attr, default) try:
# todo(vek): need to pass context in for access to auth_token </s> def __init__(self, name, state):	InstanceInfo self.name = name assert state in power_state.valid_states(), "Bad state: %s" % state
os.chdir(dest_dir)  # todo: error checking </s> self.argv_stack.append(argv)	Push def Push(self, argv): self.top = {}
# todo: verify that we need this for-loop </s> if not rep == ignore:	_repn_ tmp = {} for key in self._sections: tmp[key] = rep return tmp
# todo: remove </s> logger.error(error_tb)	process_handler_error self.inc_count('error-%s' % ex.__class__.__name__.lower()) if error_tb: else: logger.error('Error in %s function' % func_name,
# todo check the op returned a view </s> print >> file, 'scan op profiling (', self.name, ')'	summary_function return print >> file, '' else: print >> file, 'Scan Op profiling'
# todo(mierdin) will parameters always be here? fix this and the heinous thing below </s> if raw_inquiry.get('children'):	InquiriesController ) inquiries = [] continue new_inquiry = self._transform_inquiry(raw_inquiry)
# todo: do we need to skip config.add_slack variable here? </s> sum(abs(v_model.value-v_setpoint.value)	generate_norm1_norm_constraint norm_constraint_blk.abs_reformulation.add( expr=v_model - v_setpoint.value <= norm_constraint_blk.L1_slack_var[idx]) for v_model, v_setpoint in zip(model_vars, setpoint_vars)) norm_constraint_blk.sum_slack = Constraint(
# todo: allow iteration over regionfile self. (thus: for chunk in regionfile('region.mcr'): ... ) </s> length = none	parse_chunk_headers compression = compression[0] chunk_status = self.STATUS_CHUNK_OUT_OF_FILE compression = None chunk_status = self.STATUS_CHUNK_OUT_OF_FILE
# todo: test more stuff! </s> cls.tiktorch_net.configure(	setup_class num_output_channels=2 ) window_size=[192, 192], num_input_channels=1,
# todo find out what's wrong </s> data = [	StroudSecrest A = 3/5 B = 2/75 (A, numpy.array([[0.0, 0.0, 0.0]])), (B, fsd(3, (nu, 1))),
# todo: use dataopdict/tuple's new `map` method. </s> processor_stack.reset()	setup_preprocessor inputs=build_space ), action_space=None) return processor_stack else:
# todo: rewrite tests </s> self._url_to_body(self.component.url),	test_component_url def test_component_url(self): assert_equal(
# todo(brett.cannon) implement </s> def mock_implicit_hooks():	mock_implicit_hooks
# todo(rosmaita): bug #1745003 </s> def _pre_upgrade_ocata_expand01(self, engine):	_pre_upgrade_ocata_expand01
# todo: detect other runtimes </s> app = sanitize_app_name(app)	deploy_app @piku.command("deploy") @argument('app') do_deploy(app)
# todo: change to deprecationwarning in version 1.1 </s> val = str(val)	fset if val: if isinstance(val, (list, tuple, dict)): req.environ[key] = val or None
# todo: replace with specific error when exceptions are refactored </s> def left(self, value):	left @left.setter
# todo: merge with config_check_pre_system_cron </s> stop_system_importer_file_csv_run = true	config_check_run@158 else: if not os.path.isfile(csv_import_file): else: if not os.access(csv_import_file, os.R_OK):
# due to exponentiation being involved there is some fp error. todo: arrange to be able to assert the range, or duplicate the computation, instead of using exact constants </s> self.assertequal('foo', self.s.get_status())	TestAPRSStation self.s.receive(self.__message([ Status('foo')
# todo(b/163904067): mimic defun' behavior and reset the step seed to </s> return tf.cumsum(x, axis=axis, exclusive=exclusive)	CumSum Raises: ValueError: if the input axis is invalid. rank = GetRank(x) if not isinstance(rank, int) and axis != -1:
# todo(eric_k): unicorn@1.0.2rc1 doesn't like writing to </s> return globals()["uc_arm_reg_" + reg_name]	_to_unicorn_id def _to_unicorn_id(self, reg_name): elif self._cpu.arch == CS_ARCH_ARM64: return globals()["UC_ARM64_REG_" + reg_name]
# todo: where to store config? </s> if request.user.id:	process_transfer def process_transfer(request, transfer_uuid): transfer = models.Transfer.objects.get(uuid=transfer_uuid) transfer_path = transfer.currentlocation.replace(
# todo: refactor accordingly when v3 websocket api is released </s> def order_book_class(self) -> bittrexorderbook:	order_book_class @property
# todo: if we're going to implement ttl, it will be here. </s> except keyerror:	public_key return self._crypto_power.public_keys[key_class]
#todo fixme: this is a very crude way of dupe checking </s> else:	addClaims@35 for claim in claims: if claim.getID() in item.get().get('claims'): pywikibot.output('Adding %s --> %s' % (claim.getID(), claim.getTarget().getID())) item.addClaim(claim)
# todo: replace with a getter </s> if not isjson(line):	JsonValidatorTool def check_multiline_json(self): self.f.seek(0) if isJson(line.replace("'", '"')): die(self.invalid_json_msg_single_quotes)
#  todo: test </s> (	handler@18 def handler(fit, ship, context): "Command Burst", ),
# todo: union types don't work with scalar types </s> )	_resolve_type@35 value.field_name, str(type(self)), _type.types
# todo(releasesv2): add health data </s> self.create_release(project=self.project, version="1.0")	OrganizationReleasesV2Test self.browser.snapshot("organization releases v2 - no access") def test_list(self): self.browser.get(self.path) self.browser.wait_until_not(".loading")
# todo: what happens to sysex messges here? </s> pass  # all ok	Parser is a bytearray or bytes of data to parse. Returns the number of pending messages. elif isinstance(mididata, bytes): mididata = bytearray(mididata)
# todo: speedup by allocating the denominator directly instead of constructing it by sum </s> def half(tens, second):	get_minibatch_features abs_dif = tf.reduce_sum(tf.abs(tf.expand_dims(activation,3) - tf.expand_dims(tf.transpose(activation, [1, 2, 0]), 0)), 2) mask = 1. - big m, n, _ = tens.get_shape() m = int(m)
# todo: revert this. </s> carrington coordinates.	Heliocentric Earth. obstime: SunPy Time Examples --------
# todo: test pdb files with dna and rna too: </s> yield from atomiterator(pdb_id, structure)	CifAtomIterator if not pdb_id: warnings.warn("Could not determine the PDB ID.", BiopythonParserWarning)
# todo: document </s> :type: [:class:`keyvalue`] - a list of values"""	PyHKey return [PyHKey(self, n) for n in  res] @property res = [] with ExpectWindowsError(259):
annot.annotation_metadata.validation_and_reliability = "todo" #todo </s> for possible_annot in xrange(3):	create_JAMS return jam = jams.Jams() if os.path.isfile(os.path.join(path, "textfile" + str(possible_annot+1) + ".txt")):
# todo share code with check_argument_count in checkexpr.py? </s> if sym.node.fullname() in ('typing.generic',	analyze_typevar_declaration sym = self.lookup_qualified(unbound.name, unbound) if sym is None: 'typing.AbstractGeneric'): tvars = List[str]()
# todo fix. </s> asm = ["cmova eax, ebx"]	test_cmova ctx_init = self.__init_context() x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init)
if np.any(coulomb_log):  # todo: something </s> particles=particles,	Knudsen_number .. [1] https://en.wikipedia.org/wiki/Knudsen_number path_length = mean_free_path(T=T, z_mean=z_mean, V=V,
# todo change when v4 web3.py will released </s> chain.wait.for_receipt(tx)	test_pre_deposit tx = escrow.transact().initialize() chain.wait.for_receipt(tx) owner = web3.eth.accounts[1] tx = escrow.transact({'from': creator}).preDeposit([owner], [1000], [10])
# todo: comment in visit_print also applies here </s> return ret	fix_function_overloads ret.append(current_overload[0]) elif len(current_overload) > 1:
# todo: handle multiple skip stacks </s> replaced_box_width.without_min_max(box, containing_block)	block_replaced_width @handle_min_max_width block_level_width.without_min_max(box, containing_block)
# todo clarify api - this should be pretty limited to support mainly confirming </s> ellipsis: str = "...",  # todo: cleanup @ redesign	_truncate_hex lines: int = TEXT_MAX_LINES, width: int = MONO_HEX_PER_LINE, ) -> Iterator[str]: ell_len = len(ellipsis)
# todo remove </s> types = evaluator.goto_definition(name)	get_params_for_module except IndexError: continue if compare.base in [t.base for t in types if hasattr(t, 'base')]: evaluator.eval_trailer(types, trailer)
raise skiptest("buggy")  # todo(mattjj): fix </s> expected = fun(x)	testTransposeAndAddRank3 def fun(x): return x + x.T ans = _parallelize(fun)(x) self.assertAllClose(ans, expected, check_dtypes=False)
elif key_type == unicode:  # todo: change to 'str' on python3 </s> def field_types(self):	field_types @property
#todo: create a failed test if a dependency didn't install? </s> self.tests.append((buildid, passed, buildinfo))	addTest
# todo(zchee): configurable and refactoring </s> def __init__(self, vim):	Source Base.__init__(self, vim) self.name = 'jedi'
# todo: when sharing moves into the store this should be replaced </s> return "direct-%s-%s" % (shareehome.resourceid(), sharercollection.resourceid(),)	directShareID
# todo: systemhistory_user_id </s> "|taskname_name:" + str(taskname.taskname_name)	Taskname request_user + log_text +
# todo(developer): uncomment and set the following variables </s> print("making batch prediction... ")	batch_predict_bq@93 response = client.batch_predict(bigquery_input_uri=bq_input_uri, bigquery_output_uri=bq_output_uri, response.result()
:class:`goless.channelclosed` will be raised. (#todo) </s> self.maxsize = size	BufferedChannel def __init__(self, size): assert isinstance(size, int) self.values_deque = _collections.deque() self.waiting_chan = _be.channel()
# todo debug </s> seconditem = currentroot.find("second")	parseRuleRecursively monthdayItem = currentRoot.find("monthday") hourItem = currentRoot.find("hour") counter = 0 if not orItem is None:
# todo(jeremydw): thread pool. </s> self.secret = secret	GoogleCloudStorageDeployment def __init__(self, bucket, access_key=None, secret=None): self.bucket = bucket def get_url(self): return 'http://{}/'.format(self.bucket)
# todo(iceboy): check if the user attended the contest. </s> @base.require_csrf_token	ContestDetailHandler @base.require_priv(builtin.PRIV_USER_PROFILE) @base.require_perm(builtin.PERM_ATTEND_CONTEST) @base.sanitize async def post_attend(self, *, tid: objectid.ObjectId):
# todo: union types don't work with scalar types </s> is_optional = true	get_graphql_type_for_annotation@58 types = annotation.__args__ non_none_types = [x for x in types if x != type(None)] graphql_type = get_graphql_type_for_annotation( non_none_types[0], field_name, force_optional=True
# todo: implement meaningful test </s> target = ['tracklist', 'mode', 'volume', 'history']	test_export_coverage result = Core._config_to_coverage('volume') self.assertEqual(result, ['volume']) self.assertEqual(result, target) result = Core._config_to_coverage('last')
# todo: use closest instead of conditioning on single entry </s> s = surt.surt(urir, path_strip_trailing_slash_unless_empty=false)	showTimeMap @app.route('/timemap/<regex("link"):format>/<path:urir>') indexPath = ipwbConfig.getIPWBReplayIndexPath() cdxjLinesWithURIR = getCDXJLinesWithURIR(urir, indexPath)
# todo: include urls explicitly in desc format </s> return true	persists
# todo: refactor </s> self.iterator(mode='sequential', batch_size=batch_size, num_batches=none, topo=none)	get_batch_design method inherited from Dataset
# todo isolate this test </s> assert locale('en', 'us').scientific_formats[none].pattern == '#e0'	test_scientific_formats_property
# todo: provide a kernel which will describe how coordinates are extruded. </s> def test_firedrake_extrusion_assemble(family, degree):	test_firedrake_extrusion_assemble assert integrate_assemble_p0(family, degree) < 1.0e-14
# todo: send finished </s> a = class_for_name(actor["module"], actor["class"])	setup_actors def setup_actors(self): act = a(actor["id"], actor["params"]) self.actors.append(act)
tol = 0.15  # todo(skye): can we be more precise? </s> def testffterrors(self, inverse):	testFftErrors {"testcase_name": "_inverse={}".format(inverse), "inverse": inverse} rng = jtu.rand_default() name = 'ifft' if inverse else 'fft'
# todo: fixme for photos 4 </s> def _latitude(self):	_latitude return self._info["latitude"]
# todo make "master" not hard-coded, fetch it from some metadata </s> ]	VoluminousOptions subCommands = [ ["init", None, InitOptions, "Create a volume and its default master branch"], def postOptions(self):
# todo: extend to inputs with shape (n_samples, 1) </s> min_value=-1e10,	test_equal_interval_length filter_values=arrays(dtype=np.float, elements=floats(allow_nan=False, max_value=1e10), shape=array_shapes(min_dims=1, max_dims=1,
#todo: this loop is pretty slow .. (parellize) </s> return self.mesigmaderiv(grad*u).t * (grad*v) + ky**2*self.mnsigmaderiv(u).t*v	getADeriv if adjoint:
# todo move to common? </s> day = as_date(d)	is_holiday def is_holiday(d: DateIsh) -> bool:
# todo: give a vanilla example </s> \\frac{\\sum_n y}{\\sum_{n,t} y},	feca@26 .. math:: fraction = \\frac{\\sum_n \\hat{y}}{\\sum_{n,t} \\hat{y}} \\right )
time.sleep(1)  # delay, for last.fm latency. todo can this be removed later? </s> self.assertequal(title, "test title")	test_track_title_prop_caps def test_track_title_prop_caps(self): track = pylast.Track("test artist", "test title", self.network)
# todo(mordred) when this changes to rest, force interface=admin </s> except ironic_exceptions.clientexception:	get_nic_by_mac try: return self.manager.submit_task( return None
# todo: remove this when we depend on genshi >= 0.5 </s> link = {'href': href, 'title': title, 'type': mimetype, 'class': classname}	add_link linkset = req.chrome.setdefault('linkset', set()) if linkid in linkset: links = req.chrome.setdefault('links', {}) links.setdefault(rel, []).append(link)
# todo: test without file </s> n = 1001	test_nunique hpat_func = hpat.jit(test_impl)
# todo: make this configurable </s> @classmethod	MachineApplication required options: user def get_authentication_item(cls, token_type,
# todo(harlowja): the bug 1214083 is causing problems </s> _notify_failure(e)	get_scheduler_flow _error_out_volume(context, db, volume_id, reason=e) except Exception as e: _log_failure(e) _error_out_volume(context, db, volume_id, reason=e)
# todo: distinguish between text elements with actual whitespace </s> return {'object': obj,	event_to_activity Returns: an ActivityStreams activity dict 'id': obj.get('id'), 'url': obj.get('url'),
# todo: require tests </s> def jac_mat_prod(self, module, g_inp, g_out, mat):	ZeroPad2dDerivatives in_features = in_channels * in_x * in_y return mat_unpad.view(batch, in_features, num_cols) batch, in_features, num_cols = mat.size() mat = einsum('bic->bci', (mat)).contiguous()
# todo remove comment parameter. </s> return cast_int(ord(value), size)	cast_char
#todo need to hook up any inputs here </s> pipe['modules'][util.pythonise(module['id'])] = module	_parse_pipe pipe['graph'] = {} pipe['wires'] = {} pipe['graph'][util.pythonise(module['id'])] = [] if module['type'] == 'loop':
# todo(b/155239129): used list_physical_device in `setup` for gpu tests. </s> self.assertisinstance(v, tf.tensor)	test_to_representation_for_tf_variable v = eager_tf_executor.to_representation_for_type( tf.Variable(10, dtype=tf.int32), {}, self.assertEqual(v.numpy(), 10) self.assertEqual(v.dtype, tf.int32)
# todo: it seems that yahoo! converts relative links to </s> print i	pipe_fetchpage@41 for i in items: if context and context.verbose: print "--------------EOF item data ----------------" yield {"content": i}
# todo watch out because urllib.unquote will blow up on unicode text </s> msg = self.next_message()	Http def run (self): while self.running: self.server.handle_request()
# todo add binary column (after dropping support for python 2.7) </s> self.assertequal(list(df.columns), ['index', 'a'])	test_reset_index def test_reset_index(self): df = pd.DataFrame({'a': [1, 2, 3, 4, 5]}) df = pd.DataFrame({'a': [1, 2, 3, 4, 5]}) reset_index(df, index_label='__index__')
self.assertequals(status, 200) # todo: should be 202 </s> self.assertequals(status, 200)	test_get_bind self.REPO_ID, self.DISTRIBUTOR_ID) self.assertTrue(body is not None) self.assertEquals(body['consumer_id'], self.CONSUMER_ID)
# todo. check if build_url_fname can be used. </s> @param: notelist -- list of notes	dump_notes def dump_notes(self, notelist): if not notelist: return "&nbsp;"
raise notimplementederror #todo, implement! </s> return none	title if 'title' in self.metadata: return self.metadata['title'] else: return self._title
# todo: deprecate `pages` in favor of `page_limit` since it is less confusing </s> dict_writer.writerows(list_of_posts)	write_posts_to_csv with open(filename, 'w') as output_file: dict_writer = csv.DictWriter(output_file, keys)
# todo: arrange </s> def createrepo(self):	createRepo repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "name", "testrepo0", self.token)
# todo: os.path doesn't make sense here as it's os-dependent </s> pass	FileSystemException class FileSystemException(Exception):
common_path=prefix,  # todo: add key? </s> def autoresolve_cells(base, decisions, strategies):	autoresolve_cells
# xxx todo register a failure handler that reverses the local state </s> nses = {'private': [], 'shared': [] }	top_level_namespaces def top_level_namespaces(self, user_id): well as all the shared folder rows. with session_scope() as db_session: user = db_session.query(User).join(ImapAccount)\
# todo: console auth doesn't have support for handling unknown providers </s> user, domain = email_address.split('@')	handler_from_email@37 def handler_from_email(email_address): email_address = user + '@exchange.mit.edu' provider = provider_from_address(email_address)
#todo same issue with batch_size </s> raise exception("invalid graph type")	create_graph elif graph_type == 'generator': tf_graph.create_generator(graph)
# todo add locales </s> if new_main_domain not in domain_list()["domains"]:	domain_main_domain from yunohost.tools import _set_hostname if not new_main_domain: raise YunohostError("domain_name_unknown", domain=new_main_domain) operation_logger.related_to.append(("domain", new_main_domain))
# todo: tokenize attribute strings properly </s> whether the 'protein_coding' biotype annotation is among its values.	pandas_series_is_biotype def pandas_series_is_biotype(series): Hackishly infer whether a Pandas series is either from return 'protein_coding' in series.values
#todo - cookies? </s> 3. fall back and replace all unicode characters	get_unicode_from_response Tried: 1. charset from content-type tried_encodings = [] encoding = get_encoding_from_headers(r.headers)
# => todo: allow for passing a branch </s> based on the current working directory""",	Update args=("--dataset", "-d",), doc=""""specify the dataset to update. If constraints=EnsureDataset() | EnsureNone()), merge=Parameter(
# todo: this wait=false can be problematic! </s> return s	getcustomservice if object_id in self.customservices: for s in self.customservices[object_id]: return service
# todo: may test with codecs.open passing an encoding </s> rows.export_to_csv(utils.table, temp.file)	PluginCsvTestCase def test_export_to_csv_fobj(self): temp = tempfile.NamedTemporaryFile(delete=False) table = rows.import_from_csv(temp.name) self.assert_expected_table(table)
# todo: arrange </s> self.remote.modify_repo(repo, "mirror_locally", "0", self.token)	createRepo self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token)
# todo: see get_scale_factor() to choose 72 px on hidpi </s> self._scroll_handler_id_value = value	_scroll_handler_id def _scroll_handler_id(self, value): if self._scroll_handler_id_value is not None:
rec_dict._proxy._handle.close() #todo - better solution </s> handle.close()	test_duplicates_to_dict handle = open("Fasta/dups.fasta", "rU") iterator = SeqIO.parse(handle, "fasta")
# todo add support for more diverse obs_spec and action_spec </s> def _obs_concat(obs_list):	_obs_concat
# todo: passing a broker client around isn't thread-safe </s> msg = {'action': hot_deploy.create, 'package_id': package_id}	notify_new_package can deploy a new package).
#todo: allow a,c,m recovery for unknown values </s> return gmpy.fsqrt(n)	sqrt
# todo: attributes should be freed </s> pangocairo.pango_cairo_show_layout_line(	show_first_line if hinting: pangocairo.pango_cairo_update_layout(context, pango_layout.layout) context, next(pango_layout.iter_lines()))
# todo: remove seaborn dependencies </s> ax.autoscale()	update_line ax.relim()
# todo: replace </s> mean_rank = np.mean(ranks)	compute_mean_rank ranks_object_based, _ = _compute_metrics(all_entities=all_entities, kg_embedding_model=kg_embedding_model, triples=triples, corrupt_suject=False, device=device) stop = timeit.default_timer() log.info("Evaluation took %s seconds \n" % (str(round(stop - start))))
# todo: investigate why this fails </s> 'cannot save an obsolete model', second.save)	test_simple_foreign_key_between_mutable_models to=first_model_def.model_ct) second.first = first second = SecondModel.objects.get() second.first = first
recording_uuid = none #todo </s> try:	is_pupil_invisible_recording def is_pupil_invisible_recording(rec_dir: str) -> bool: return info_csv["Capture Software"] == "Pupil Invisible" and "Data Format Version" not in info_csv except KeyError:
# todo: remove dependency on legacy_examples </s> 'add_four.add_two.add_one_2',	test_compile@10 res = coalesce_execution_steps(plan) assert set(res.keys()) == { 'add_four.add_two_2.add_one', 'add_four.add_two_2.add_one_2',
#todo: remove expressions </s> self.size = music21.interval.interval("m2")	WholeStepInvertedMordent class WholeStepInvertedMordent(InvertedMordent): def __init__(self):
# todo: this type conversion seems to be bottle neck </s> q_values = self.fc2(features)	QFunc v_values = self.fc3(features) q_values = v_values + (advantages - tf.reduce_mean(advantages, axis=1, keepdims=True)) return q_values
# todo: raise unrecognized operator error </s> for node in graph_def.node:	create_neon_graph@46 name_to_op = {} var_names = [] print(node.name) if node.op not in known_ops:
# todo: remove? </s> return false	StockReportParser def looks_like_prod_code(self, code): try: except ValueError: return True
# todo: extend </s> print('all variables (embeddings + model): %d' % get_total_variables())	boe_reader_model print("OUTPUT SHAPE " + str(output.get_shape())) logits, loss, predict = predictor(output, targets, options["answer_size"]) return logits, loss, predict
# todo: uncomment these, when #1217 is fixed </s> finder = packagefinder([find_links], ["http://pypi.python.org/simple"])	test_finder_priority_file_over_page def test_finder_priority_file_over_page(): link = finder.find_requirement(req, False) assert link.url.startswith("file://")
# todo(ytknzw): add more specific assertion with the test case. </s> def fail_objective(_: trial) -> float:	fail_objective
# todo: remove this after we create the contents web service and directories are </s> model = self.notebook_manager.copy_notebook(copy_from, copy_to, path)	_copy_notebook path, copy_from, path, copy_to or '', self.set_status(201) self._finish_model(model)
# todo: os.path.expandvars, os.path.expanduser? is not needed here, isn't it? always? </s> "path", metavar='file', nargs='+',	setup_parser def setup_parser(parser): help="path or pattern describing what to get")
# todo: check syntax </s> if not self.response.parsed_hdrs.has_key('location'):	status303 def status303(self):        # See Other
# todo: should this result be 0 instead of nan? </s> pass	compute_histogram
# todo: if update_atlas: introduce charts via self._atlas </s> sage: m.coframes()	coframes sage: TM.coframes() [Coordinate coframe (R^2, (dx,dy))] [Coordinate coframe (R^2, (dx,dy)), Coframe (R^2, (e^0,e^1))] sage: U = M.open_subset('U', coord_def={c_cart: x^2+y^2<1}) # unit disk
# todo: we're reading the config file twice. </s> ):	split_args_in_optional_and_positional previous = args[i - 1] if (not arg.startswith("-")) and ( positions.append(i) positionals = [arg for i, arg in enumerate(args) if i in positions]
# todo(amotoki): due to neutron bug 1378525, neutron disables </s> log.info(msg)	UpdateForm msg = _('Failed to update router %s') % data['name']
# xxx todo register a failure handler that reverses the local state </s> for shared_ns in shared_nses:	top_level_namespaces nses['private'].append(account_ns.cereal()) shared_nses = db_session.query(SharedFolder)\ nses['shared'].append(shared_ns.cereal()) return nses
# todo: replace `funcname` with func.__name__? </s> >>> @check_relativistic	check_relativistic Examples -------- ... def speed(): ...     return 1 * u.m / u.s
# todo: remove debug statements after fixing in-toto/in-toto#171 </s> <side effects>	gpg_verify_signature The content to be verified. (bytes) <Exceptions> None. <Returns>
# todo: work out a way to set this based on the timespan of the data. </s> log.debug(f'reading {kwargs["filepath"]} failed with the following exception:\n{e}')	is_datasource_for with h5netcdf.File(kwargs["filepath"], mode="r", **cls._netcdf_read_kw) as f: return "XRS" in f.attrs["summary"].astype("str") return False
except exception as error:  # todo: be specific </s> with self._mutex:	add_job def add_job(self, job):
# todo: try mlp rather than bilinear </s> return tf.get_collection(tf.graphkeys.global_variables, self.scope)	get_variables
# todo: score is the negative of the distance </s> :return:	TransE :param h_embs: :param r_embs: score = - torch.sum(torch.abs(h_embs + r_embs - t_embs)) return score
return # todo </s> newnode = nodes.continue()	visit_continue return newnode
# todo(lbragstad): sleeping after the response status has been checked </s> time.sleep(1)	IdentityUsersTest user_id=user_id, new_pass=old_pass,
except exception:   # todo </s> logging.warning('status: blockchain reorganisation at block {}.'.format(block_index))	reorg block_hash_have = reorg_cursor.fetchall()[0]['block_hash'] if block_hash_see != block_hash_have: break if not reorg_necessary: return last_block_index + 1
pass  # todo: 实盘需要检查 </s> for key in list(self.env.orders_pending_mkt_dict):	check_orders_pending_with_mkt for order in self.env.orders_pending_mkt_dict[key]: if self.send_signal(order):
field = column.field.replace('#', '') # todo: is this line needed? </s> return u"m{module.id}_{detail_type}".format(module=module, detail_type=detail_type)	detail
# todo: better default date format </s> for label, df in dictionary.iteritems():	proportion_of_energy_submetered good_days_list = [] def get_kwh_per_day_per_chan(dictionary): kwh_per_day = usage_per_period(df.icol(0), freq='D', max_dropout_rate=max_dropout_rate)['kwh']
# todo: look this up in one query </s> playlist_mbid: the mbid of the playlist to delete	delete_playlist_by_mbid def delete_playlist_by_mbid(playlist_mbid: str): Returns: True if the playlist was deleted, False if no such playlist with the given mbid exists
# todo add typeerror if params are given. </s> return compiled.builtin	parent @common.safe_property
# todo: add logging in case the post fails. </s> agent_id = random.randint(0, len(agents))	battle def battle(name, user, num_times, config): try: agents.insert(agent_id, "docker::%s/%s" % (user, name)) infos = []
# todo: add option to only show error runs </s> 'run': run,	RunController line_cache = db_linecache.DBLineCache(request.db, run_id) return { 'trace': trace, 'getline': line_cache.getline,
# todo: decide on replace= behavior, see #903 </s> f.trap(nosuchchilderror)	_maybe_create def _maybe_create(f):
persist=false  # todo: add log persistence </s> return none	_get_requirements_path requirements_path = os.path.join(self.repo_path, 'polyaxon_requirements.txt') if os.path.isfile(requirements_path):
# todo(b/182621549): for sobol sequences, dimension should be known at graph </s> mean,	_mvnormal_pseudo_antithetic covariance_matrix=None, scale_matrix=None,
# todo: get the real security group of launch in here </s> self.set_state(instance.shutdown)	_wait_for_shutdown timer.stop() d.callback(None) timer.stop() d.callback(None)
1  # todo: fill in identifier </s> channel1 = token_manager1.partneraddress_channel[app0.raiden.address]	test_settlement token_manager1 = app1.raiden.managers_by_token_address.values()[0] assert token_manager0.token_address == token_manager1.token_address deposit0 = channel0.balance deposit1 = channel1.balance
# todo maybe include the _request here too? </s> self.network, "27",	get_images raise WSError(
# todo: consolidate?? </s> handler for /run requests	RunSaltAPIHandler @tornado.web.asynchronous def post(self):
# todo: remove this skip after fixing </s> border_color=(0, 1, 1, 1))	test_circle_draw@20 assert_image_equal("screenshot", 'visuals/circle2.png') gloo.clear() c.draw_visual(ellipse) assert_image_equal("screenshot", 'visuals/circle3.png')
# todo(termie): we should probably return not founds instead of none </s> resp = c.get_tenants(user_id=self.user_foo['id'])	IdentityApi def test_get_tenants(self): token = self._login() data = json.loads(resp.body) self.assertDictEquals(self.tenant_bar, data[0])
pass  # todo </s> nick_id = self.execute('select nick_id from nicknames where slug = ?',	_get_nick_id This identifier is unique to a user, and shared across all of that user's aliases.""" [slug]).fetchone() if nick_id is None:
#@todo: remove in 0.4.10 </s> try:	handleMultiPages m = re.search(self.PAGES_PATTERN, self.html) pages = int(m.group(1))
# :todo: implement test. </s> self.assertnotequal(tryte[2], trit(-1))	test_init_mixed_types tryte = Tryte([Trit(1), 1, -1]) self.assertEqual(tryte[0], Trit(1)) self.assertEqual(tryte[2].value, -1)
# todo: match cciss* somehow </s> try:	_revert_elevator def _revert_elevator(self): for dev in glob.glob(self._options["elevator_devs"]): f = open(dev, "w") f.write("cfs")
# todo: parlist, dots, block </s> node = p._field()	testFieldExp def testFieldExp(self): self.assertIsNotNone(node) self.assertEqual(1, p._pos)
# todo:   seems like they could be combined </s> return none	FormRepeater try: query.append(("app_id", self.payload_doc(repeat_record).app_id)) url_parts[4] = urllib.urlencode(query) return urlparse.urlunparse(url_parts)
# todo debug </s> + "attribute in monthday tag.")	parseRuleRecursively if (ruleMonthdayNew.monthday < 1 or ruleMonthdayNew.monthday > 31): ruleElement = RuleElement() ruleElement.type = "monthday"
# todo: this is a jump. </s> vi_cmd_data['motion']['args'] = {'mode': vi_cmd_data['mode']}	vi_big_m def vi_big_m(vi_cmd_data): vi_cmd_data['is_jump'] = True return vi_cmd_data
# todo: data could be chunked, proxy needs to handle this </s> return server	Proxy self.port = server.server_port logger.info('{0} proxy server started, listening on {1}, proxy for: ({2}, {3}) using {4} decoder.' def handle(self, sock, address): session = conpot_core.get_session(self.proxy_id, address[0], address[1])
# todo docstring </s> applications = []	discover_node_configuration instances. d = self._gear_client.list() for unit in units: applications.append(Application(name=unit.name))
# todo: decode all field to string </s> :return: total power and a second dictionary with all other measures	jtop return {} def _total_power(self, dpower): :rtype: dict, dict total_name = ""
1  # todo: fill in identifier </s> line[7::7] = len(line[7::7]) * '.'	print_stats return stack def _line(recursion): return ''.join(line) highest_time = 0
# todo: use weight scaling factor if provided, xavier's default else </s> self = cls(model=model, cost=cost, conf=conf,	DATrainer learning_rates[parameter] = sharedX(thislr, lr_name) iteration = sharedX(0, 'iter') learning_rates=learning_rates, annealed=annealed, iteration=iteration, minibatch=minibatch)
# todo stub </s> def metrics_identity(evaluator_identity: detectionevaluator) -> dataframe:	metrics_identity return evaluator_identity.evaluate()
# todo read2 is silently discarded </s> if read1.match is none:	Demultiplexer self.files = dict() def __call__(self, read1, read2=None): if self.untrimmed_outfile is None and self.untrimmed_path is not None: self.untrimmed_outfile = xopen(self.untrimmed_path, 'w')
# todo: redo using locally established collection </s> collection.add_handle(handle, handle.name)	test_add_handle_to_collection local_master.git_fetch(collection.name) handle = HandleRepo(h_path, name="MyHandle") local_master.git_fetch(collection.name) ok_clean_git(local_master.path, annex=False)
# todo: handle fancy-index copies by allocating a buffer and </s> -----	ForcedEvenIterator StopException When _base_iterator reachs the end of the dataset Uneven batches may be discarded and StopException will be raised without having iterated throught
# todo deal with cached user dict here </s> user = couchuser.get(user_id)	_run_reports for user_id in rep.user_ids:
# todo: fix this, this is one of the few cases where using the config </s> :param references: list of tests references to be resolved and	_make_test_suite_loader def _make_test_suite_loader(self, references): transformed into test factories :type references: list of str
# todo: replace with isinstance(expr, bindabletypes) </s> if value is none:	_add_dependencies value = key else:
1/0  # conflict resolution todo </s> if list_a[uid] != status[uid][0] and list_b[uid] != status[uid][1]:	get_actions@64 actions.append(('upload', uid, 'b', 'a')) else: 1/0  # conflict resolution TODO elif list_a[uid] != status[uid][0]:  # item update in a
# todo(dtroyer): remove tenant_id when we clean up the sdk refactor </s> parser.add_argument(	_add_additional_network_options '--provider-network-type', metavar='<provider-network-type>',
# todo some complication with -1 label </s> new_params = est.get_params()	test_estimators_overwrite_params set_random_state(est) params = est.get_params() for k, v in params.items(): assert_false(np.any(new_params[k] != v),
# todo: 289 </s> signing_power = self._crypto_power.power_ups(signingpower)	generate_self_signed_certificate def generate_self_signed_certificate(self):
# todo: deprecated, remove in 1.5.0 </s> if not username in self._users.keys():	get_user_setting raise UnknownUser(username) user = self._users[username]
# todo: match channel against [a-za-z0-9:._-]+ </s> def unsubscribe(context, channel):	unsubscribe@30 *musicpd.org, client to client section:* ``unsubscribe {NAME}``
# todo: arrange </s> self.remote.modify_repo(repo, "mirror_locally", "0", self.token)	createRepo self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token)
# todo: remove this after we create the contents web service and directories are </s> def save_notebook_model(self, model, name, path=''):	save_notebook_model
# todo: remove in sopel 8 </s> 'rule',	is_triggerable ) forbidden = any(hasattr(obj, attr) for attr in forbidden_attrs) 'find_rules', 'search_rules',
# todo need more daa </s> assert len(data) >= 1	test_sub_projects_by_repo response = requests.get('http://localhost:5000/api/unstable/repo-groups/24/repos/21477/sub-projects') data = response.json() assert data[0]["sub_project_count"] > 0
pass # todo: explain </s> if not self.response.parsed_hdrs.has_key('location'):	status303 def status303(self):        # See Other
# todo: docstring </s> eval_name += '.' + s	__get_variable_evaluate_name@800 else: eval_name += '[{}]'.format(int(s)) return eval_name
# todo: choose one from the following two </s> return true	available def available(self, exception_flag=True): TODO: For now, it is always available. However, sub-solvers may not
except exception:  # todo: be specific </s> headers = {"accept-language": "en"}	get_insta_json url = url.replace("https://", "http://") r = get(url, headers=headers)
continue  # todo should we store relations? </s> else:	messages_from_raw@405 raise e log.info("Success!") raise Exception("Unknown encoding scheme:" + str(encoding)) new_part.data_sha256 = sha256(data_to_write).hexdigest()
# todo adding and removing tracks as if this was a regular list </s> self._sp_playlist, [t._sp_track for t in tracks], len(tracks),	reorder_tracks spotify.Error.maybe_raise(lib.sp_playlist_reorder_tracks(
# todo: redundancy between all gaze mappers -> might be moved to parent class </s> self.g_pool.quickbar.remove(self.button)	deinit_gui if self.button:
# todo: use <meta charset /> </s> self.data.append(self.tagreplace)	handle_starttag
# todo is pexpect thread safe, e.g. could we be blocked on this </s> handle = int(hxstr[0], 16)	BluetoothLeDevice if pnum == 0: after = self.con.after except pexpect.TIMEOUT: pass
# todo; not sure what's wrong here. possible bug? </s> words = ["i", "like", "cheese", "."]	test_matcher_no_match doc = get_doc(matcher.vocab, words) assert matcher(doc) == []
# todo ... </s> while len(found) < limit:	find_special_numbers def find_special_numbers(special_selector, limit=10): found = [] if special_selector(n): found.append(n)
# todo: show in display_problems() </s> if pkg2 in discard_pkgs:	_greedy_slots continue for j in range(i + 1, len(greedy_pkgs)): continue if blockers[pkg1].findAtomForPackage(pkg2, modified_use=self._pkg_use_enabled(pkg2)) or \
# todo: this should disappear once the image package is refactored </s> :type: (n_dims,) ndarray	centre The geometric centre of the Image - the subpixel that is in the middle. return np.array(self.shape, dtype=np.double) / 2
# todo: move trusted to be autodetected inside resource </s> returns:	describe_resource@21 expand? (bool): if `True` it will expand the metadata nostats? (bool): if `True` it not infer resource's stats Resource: data resource resource = Resource(source, trusted=True, **options)
'expiration': (maya.now() + datetime.timedelta(days=3)).iso8601(),  # todo </s> assert 'plaintexts' in bob_response_data['result']	test_character_control_lifecycle@190 response = bob_control_test_client.post('/retrieve', data=json.dumps(bob_request_data)) assert response.status_code == 200 for plaintext in bob_response_data['result']['plaintexts']: plaintext_bytes = b64decode(plaintext)
# todo: deprecate </s> if self.request_token_url:	authorize_access_token request_token = self._fetch_request_token() params = request.args.to_dict(flat=True)
# todo: support all tzinfo subclasses by calling utcoffset() </s> sign = '-'	tzname def tzname(self, dt=None): sign = '+' return "%s%d:%d" % (sign, self.offset / 60, self.offset % 60)
dataarray = datasection.data.uint64s # todo implement 32 bit </s> usage = "usage: %prog [options] todo description here :]"	generate_option_parser def generate_option_parser():
# todo: workaround, remove it when xgboost is fixes </s> return pickle.load(f)	load_obj def load_obj(name):
# todo: log exception </s> query = {	delete_by_task_id@504 "query": { "term": {
# todo: read xml lazily? </s> return xml_from_string(xml_to_string(element)).xpath(xpath, namespaces=namespaces)	xpath
# todo: specific exception </s> objects.  returns a list of searchmatchset objects, one for each	check_consistency def check_consistency(ann_objs, restrict_types=[], ignore_types=[], nested_types=[]): checked criterion that generated matches for the search. match_sets = []
# todo: check correctness </s> def _get_leaves(self, x, stack):	_get_leaves
# todo(twilson) we can remove this when we require ovs>=2.12.0 </s> schema = 'ovn_southbound'	MetadataAgentOvnSbIdl def __init__(self, chassis=None, events=None, tables=None): connection_string = config.get_ovn_sb_connection()
self.last_recv = self.targets[0].recv(10000)  # todo: remove magic number (10000) </s> return 100  # todo select count(*) from cases -- but watch out for partially finished case	SessionInfo return 100  # TODO upgrade database format to store this info @property def test_case_data(self, index): Args:
# todo: candidate for move to system/hdparm </s> return is_mounted(mnt_pt)	is_share_mounted def is_share_mounted(sname, mnt_prefix=DEFAULT_MNT_DIR):
# todo: consider un-hardcoding this and plumbing pool_maxsize to requests.adapters.httpadapter. </s> self.log.debug("v2_playbook_on_handler_task_start")	v2_playbook_on_handler_task_start self.v2_playbook_on_task_start(task, False, handler=True)
#todo(chris): implement service_catalog </s> respect cache headers. to avoid stale data, then, we append a little	HTTPClient def _munge_get_url(self, url): Munge GET URLs to always return uncached content. bit of nonsense onto GET parameters; this appears to force the data not to be cached.
# todo: separate agent model. </s> self.traj_infos = traj_infos	obtain_samples self.agent_inputs = agent_inputs
# todo: this needs refactoring </s> for genus in genus_list:	in_genus_list if species.startswith(genus.capitalize()): return True
# todo(lyarwood): test drivervolumeblockdevice.driver_detach in </s> def test_build_networks_if_not_allocated(self, mock_get, mock_allocate):	test_build_networks_if_not_allocated @mock.patch.object(manager.ComputeManager, '_allocate_network') instance = fake_instance.fake_instance_obj(self.context, system_metadata={},
# todo: errors </s> elif isinstance(c, gtk.label):	set_label if isinstance(c, Gtk.Container): if set_label(c, message): c.set_markup(message) return True
# todo: group by storage type also? </s> dims = ('longitude', 'latitude')	group_storage_units_by_location stacks = {} for su in sus:
return none # todo </s> 'playlist: 0',	_status 'random: 0', 'single: 0', 'playlistlength: 0', 'xfade: 0',
# todo: implement </s> self.uciok.set()	_uciok
# todo: make sure 'feature' contains only capitals and underlines </s> pass	set_package_name
# todo: factor out the logging? </s> def __add_subscription(self, subscribe_fn):	__add_subscription
pass # todo </s> if tool_id in all_tools:	_get_tool def _get_tool(self, tool_id): return all_tools[tool_id] else:
# todo: action value doesn't exist for beta </s> self.start_tests(name='no horizon estimate')	test_no_horizon_estimate reward_estimation = dict(horizon=0, discount=0.99, estimate_horizon=False) self.unittest(reward_estimation=reward_estimation)
# todo: write this method if possible </s> res = self.compose_request('rawtx', txid, {'format': 'hex'})	getrawtransaction def getrawtransaction(self, txid):
# todo: winexe calls hang and the test fails by timing out. the same </s> 'user_data': self.copy_file('windows-firewall.ps1'),	test_win2012r2_winrm self.override_profile_config( 'ec2-win2016-test', 'win_installer': self.copy_file(self.INSTALLER), 'winrm_ssl_verify': False,
raise notimplementederror # the below does most probably not work anymore todo </s> if not a_to_b and not b_to_a:	detail return "It does not differ from `{0}`.".format(b) elif not a_to_b:
# todo add in/out fifo contributions </s> def get_output_datatype(self):	get_output_datatype
expr,  # todo rethink this circular import </s> _label_counter += 1	_generate_label def _generate_label(name: str) -> str: return f"label{_label_counter}"
for people in annos:  # todo : speed up with affine transform </s> divs = np.linspace(0, len(data), thread_count + 1)	threading_data t.start() threads.append(t) divs = np.round(divs).astype(int) results = [None] * thread_count
# todo: remove this backward compatibility code (in_proj_weight) </s> bias = bias[2 * self.embed_dim:]	in_proj_v if bias is not None:
pass #todo: show multi select menu </s> def begin(self):	SelectionTool class SelectionTool(NavigationTool): super(SelectionTool, self).begin() game.main.onEscape = game.main.showPause
# todo: waffle here </s> return (self._allows_public_posting() or	allows_posting_by has_perm(user, 'forums_forum.post_in_forum', self))
# todo: move this into demo/slow-completion.sh </s> raise args.usageerror('oshc: missing required subcommand.')	OshCommandMain try: action = argv[0] if action not in SUBCOMMANDS: raise args.UsageError('oshc: Invalid subcommand %r.' % action)
# todo: check error location </s> 'test': graphqlfield(test_type),	get_fields def get_fields(self): 'nest': GraphQLField(DataType(), resolver=lambda *_: Data())
# fixme: todo </s> options_map: a parser's option_map.	execute_print Args: query: An instance of a compiled Print statemnet. file: The output file to print to. if print_stmt.from_clause is not None:
# todo: consult best practices for python and twisted logging. </s> json.dump(top.state_to_json(), f)	noteDirty def noteDirty(): pass
# todo: handle errors better </s> must be used to receive a recaptcha secret key for post /users/ form.	UserSignupForm def get(self): Get signup form keys. return {"recaptcha_server_key": "TODO"}
# todo : look for alive and killing </s> return static_file(path, root=os.path.join(bottle_dir, 'htdocs'))	declare_common_static def declare_common_static(self): @route('/static/:path#.+#')
#todo: write log file. </s> except keyerror:	get_user_url try: if a_tag["href"].find("http://www.zhihu.com/people/") == 0: pass return user_url
# todo: this is a hack to make a rule know </s> def prev_action_states(self) -> list[text]:	prev_action_states return [PREV_PREFIX + a for a in self.action_names]
# todo: check the crs </s> for col in range(columns):	make_mask_seg@36 rows = img_src.meta['height'] // height if drop_last else img_src.meta['height'] // height + 1 columns = img_src.meta['width'] // width if drop_last else img_src.meta['width'] // width + 1 outfile_image = os.path.join(outpath, Path(image_file).stem+'_'+str(row)+'_'+str(col)+Path(image_file).suffix) window = Window(col * width, row * height, width, height)
# todo: use sqlalchemy objects to do this </s> card.timerstarted = float(req.data.get('timestarted', time.time()))	answer_card ease = int(req.data['ease']) card = col.getCard(card_id) col.sched.answerCard(card, ease)
# temporary hack to get pf api working. todo - remove </s> return response	daily_report return HttpResponseBadRequest("No schema matches 'resolution'") response = read(request=request, ids=[ formdefs[0].pk ], \
@unittest.skip('not written')  # todo: finish! </s> def test_simplenamespace(self):	test_SimpleNamespace @unittest.skip('not written')  # TODO: finish! raise NotImplementedError
# todo should we pass? </s> self.tables[request_type], self.tables[request_type], location,	retrieve_most_recent_cached_data " Location = ? ORDER BY REQUEST_TIME DECS LIMIT ?",
# todo: find a better solution </s> dialog.exec_()	invokeContributionsDialog def invokeContributionsDialog(parent):
# todo (jconnor 2013-01-22) make this configurable </s> multi_handle.setopt(pycurl.m_maxconnects, self.max_concurrent)	_build_multi_handle def _build_multi_handle(self): multi_handle.setopt(pycurl.M_PIPELINING, DEFAULT_MULTI_PIPELINING) multi_handle.handles = [self._build_easy_handle() for i in range(self.max_concurrent)]
# todo: test with unicode strings and non-ascii chars </s> self.assertequal(message, decrypted)	StringTest print "\tEncrypted: %s" % encrypted decrypted = rsa.decrypt(encrypted, self.priv) def test_sign_verify(self): message = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
# todo remove backwards compatability fix in a future version </s> old_file = os.path.join(sublime.packages_path(), 'user', '.vintageousrc')	_migrate_rcfile new_file = os.path.join(sublime.packages_path(), 'User', '.neovintageousrc') if os.path.exists(old_file):
# todo(pebaz): contents.extend([search down the '.'s!!]) </s> def load_module(self, name):	Nimporter return self print(self.__class__.__name__, 'could not find', fullname, path, f'{module}.nim')
# todo handle color, scatter, etc </s> return matplotlibplot(theme)	_backend_to_plot_obj def _backend_to_plot_obj(backend, theme=None): if backend == 'cufflinks' or backend == 'plotly': return CufflinksPlot(theme)
# todo(haoyuzhang): understand slowdown of setting learning phase when </s> example protocol buffer.	parse_record_keras This method converts the label to one hot to fit the loss function. Args: is_training: A boolean denoting whether the input is for training. dtype: Data type to use for input images.
# todo implement this </s> self.get_adb().shell("sqlite3 %s \"update '%s' set value='%s' where name='%s'\""	change_settings :param name: settings name to set :param value: settings value to set % (db_name, table_name, value, name)) self.get_settings()
# todo(mierdin): note that this will always return true if rbac is not enabled </s> (reject if invalid)	InquiriesController 1. Retrieve details of the inquiry via ID (i.e. params like schema) 2. Determine permission of this user to respond to this Inquiry 4. Update inquiry's execution result with a successful status and the validated response 5. Retrieve parent execution for the inquiry, and pass this to action_service.request_resume
# todo: check complex data types possible for series for dataframes set column here </s> return lambda df: len(df._data[0])	df_len_overload def df_len_overload(df): if len(df.columns) == 0:  # empty df
#todo : multi parent intelligence </s> return len(self.children) != 0	has_child
pass  # todo: replace this </s> return self._particle_symbol	particle @property def particle(self) -> str:
# todo(rbharath): this should be modified to contain a cluster split so </s> splitters = {	load_pdbbind_grid reload=True): if featurizer == 'grid': 'index': deepchem.splits.IndexSplitter(), 'random': deepchem.splits.RandomSplitter(),
# todo(b/186451541): reduce the number of calls to model_fn. </s> prev_loss = train_metrics['loss']	_run_test train_metrics = metric_outputs['train'] self.assertEqual(train_metrics['num_examples'], expected_num_examples)
# todo(benjy): some more elegant way to coordinate how tasks claim targets. </s> interpreter = self.select_interpreter_for_targets(self.context.targets())	PythonRun raise TaskError('Multiple targets specified: %s' % ', '.join([repr(t) for t in target_roots])) binary = target_roots[0] with self.temporary_pex_builder(interpreter=interpreter, pex_info=binary.pexinfo) as builder: chroot = PythonChroot(
# todo: this procedure would leave a clean dataset, but `run` cannot handle dirty </s> ds.clean()	test_basics@41 'datalad_test_proc', where='dataset') ok_file_has_content(op.join(ds.path, 'fromproc.txt'), 'hello\n') ok_clean_git(ds.path)
# todo: give users the ability to specify this via their profile </s> results = cls.storage.db.find({'emails':email})	find_by_email def find_by_email(cls, email):
# todo: some type checking that a is invertible hence a valid key </s> return self.key_space().nrows()	block_length
#todo: milestones = get_milestones(target_url) </s> response = urllib.request.urlopen(req)	send_post_request req.add_header("Authorization", b"Basic " + base64.urlsafe_b64encode(username.encode("utf-8") + b":" + password.encode("utf-8"))) req.add_header("Content-Type", "application/json") json_data = response.read() return json.loads(json_data.decode("utf-8"))
#@todo: remove in 0.4.10 </s> self.html = self.loadpage(p)	handleMultiPages except: pages = 1 self.package_links += self.getLinks()
#  todo: test </s> fit.modules.filtereditemincrease(lambda mod: mod.item.group.name in	handler@18 ( "Command Burst",
pass  # todo </s> assert_equal(len(self.wb1.sheets[0].names), 1)	TestSheet assert_equal(self.wb1.sheets[0].name, 'NewName') def test_names(self): def test_book(self): assert_equal(self.wb1.sheets[0].book.name, self.wb1.name)
# todo 2.0: do not delete them, only set is_deleted on parent </s> if not hasattr(self, "_showids"):	getShowIds self._showIds=False return self._showIds
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
## todo: # fixme: remove me </s> for word in word_list:	is_sql_injection if len(temp_res)>0: result.append(temp_res) temp_res = str.find(line, str.upper(word)) if temp_res!=-1:
# todo: if worker does not execute preprocessing, next state is not preprocessed here. </s> else:	setup_preprocessor return processor_stack
report_config = {}  # todo port to fooddata.from_request </s> @memoized	data_providers def data_providers(self): return [
# todo: how to handle not found authorname </s> authurl = gdriveutils.gauth.instance().auth.getauthurl()	authenticate_google_drive @login_required @admin_required return redirect(authUrl)
except exception:  # todo: be specific </s> headers = {"accept-language": "en"}	get_insta_json url = url.replace("https://", "http://") r = get(url, headers=headers)
# todo implement this effectively </s> def run_full_test_suite(c):	run_full_test_suite @task
if isinstance(err, asyncio.cancellederror):  # todo: remove when updated to 3.8 </s> match = await self.db.get_address(accounts=wallet.accounts, address=address)	Ledger@730 def add_account(self, account: Account): self.accounts.append(account) if match: for account in wallet.accounts:
# todo in python 2.7 and later, this should be </s> instance of a sqlalchemy model.	DeserializationException class DeserializationException(Exception):
raise notimplementederror #todo </s> pass #todo	Span
# todo: move texture export to individual formats? this is practically smurf </s> with open(os.path.join(os.path.dirname(defs.__file__), "robotlib.yml"), "r") as f:	generateLibEntries return [("None",) * 3] + [(entry,) * 3 for entry in yaml.load(f.read())]
# todo: the following skipped suite and fixtures should be enabled </s> provider = provider	VultrProviderTests provider_name = 'vultr' domain = 'capsulecd.com'
# todo: import that elsewhere </s> sendvariable = _control.execqueue.socket.sendvariable	shareConstant@61 @ensureAtomicity def shareConstant(**kwargs): for key, value in kwargs.items(): if hasattr(value, '__code__'):
# assert wrapped.type == 'multipolygon' todo: same as above </s> for points in points_list:	test_geobox [(-148.2697, -35.20111), (-149.31254, -35.20111), (-149.31254, -36.331431), (-148.2697, -36.331431), (148.2697, -35.20111)], polygon = geometry.polygon(points, crs=geometry.CRS('EPSG:3577')) resolution = (-25, 25)
# todo(b/144127474): remove this manual cleanup once tf.wrap_function(...) </s> def type_signature(self):	type_signature @property
# todo(jjma): find a better way of describing this error to user. </s> self._input_base_path, splits)	testInvalidDate with self.assertRaisesRegexp(ValueError, 'Retrieved date is invalid'):
# todo: log </s> client = self.get_arguments('client')[0]	RunSaltAPIHandler Handler for /run requests @tornado.web.asynchronous self._verify_client(client) self.disbatch(client)
# todo this needs to be done on the content but seems to be a non-trival </s> res_items = [content]	pipe_fetchpage@47 if split_token != "": res_items = content.split(split_token) if context.verbose: print "FetchPage: found count items:",len(res_items)
# todo: look in other supported bumpversion config locations </s> return '{version} ({release_date})'.format(	title @property version=self.version, release_date=self.release_date
await self._stream.reset()  # todo: specify error code </s> def __aiter__(self):	__aiter__
# todo: remove at some point </s> return self.host_data[hostname]	get_host_data def get_host_data(self, hostname):
# todo: remove in v.0.6 </s> cov = covariance()	TestCovariance class TestCovariance(MetricTestCase): cov.fit(self.iris_points) csep = class_separation(cov.transform(self.iris_points), self.iris_labels)
# todo: implement me </s> (depth, image,), raise_exception=true)	_test_gradcheck depth = utils.tensor_to_gradcheck_var(depth)  # to var image = utils.tensor_to_gradcheck_var(image)  # to var
raise mpdnotimplemented # todo </s> result = self._find(type, what)	_findadd @register(r'^findadd "(?P<type>(album|artist|title))" "(?P<what>[^"]+)"$')
# todo: refactor, move to utils </s> field_dict[field.name] = field	get_field_dict for field in self.formdefinitionfield_set.all():
# todo: create xxx_failure test </s> tests = exclude_from_resultlist(r, 'failure')	test_result_space_failure p = op.join(app.config['ROOT'], 'tests/fixtures/ttf/Font-Light!.ttf') r = run_set(p, 'result') self.assertTrue(check('test_space', failure_tests), lookup('test_space', tests))
# todo check </s> def set_metadata(self, metadata):	set_metadata pass
# todo(sbauza): the current placement noauthmiddleware returns a 401 </s> return ()	fake_initialize_connection if volume_id == CinderFixture.SWAP_ERR_NEW_VOL:
# todo: rewrite tests </s> schools = [{	TestUserProfile for i, job in enumerate(jobs): assert_equal(job, res.json['contents'][i]) 'institution': 'an institution', 'department': 'a department',
# todo: may use sys.stdout.encoding if output_file = '-' </s> elif '^' + field_name in new_field_names:	_get_field_names for field_name in table_field_names: if field_name in new_field_names: result.append('^' + field_name) return result
# todo: the one_hot=true is only necessary because one_hot=false is </s> assert topo.ndim == 4	test_topo def test_topo(self):
# todo(slaweq): change this to neutron floating ips and turn neutron </s> def test_wait_for_server(self, mock_get_server, mock_get_active_server):	test_wait_for_server @mock.patch.object(shade.OpenStackCloud, "get_active_server") Test that waiting for a server returns the server instance when its status changes to "ACTIVE".
# todo: remove next if due specific hack </s> continue	set_target_dependency_packages if targets_file_path is None:
raise notimplementederror # todo </s> s.resample()	resample_states def resample_states(self):
# todo: should be injected </s> async def fetch_all(self, credentials, regions=none, partition_name='aws', targets=none):	RegionsConfig@13 def __init__(self, service, child_config_type): self._service = service facade = AwsFacade() for region in await facade.build_region_list(self._service, regions, partition_name):
# todo: support non-numericals like string </s> return len(df_arr)	_run_call_len def f(df_arr):  # pragma: no cover
# todo(jflesch): update keyword index </s> self.__worker = worker	ActionStartWorker Start a threaded job def __init__(self, worker): def do(self): SimpleAction.do(self)
# todo: use utils.serialize_user </s> 'url': result[split_id+'_url'],	create_result if result[split_id+'_public']: nest[split_id] = { 'highlight': lit or nest.get(split_id)['highlight'] if nest.get(split_id) else None, 'wiki_link': wiki_link,
# todo: replace all of this string templating with a function that accepts </s> except oserror:	MonkeyDrops os.utime( self._config["destination_path"], (ref_stat.st_atime, ref_stat.st_mtime) LOG.warning("Cannot set reference date to destination file") monkey_options = build_monkey_commandline_explicitly(
# todo: arrange </s> self.remote.modify_repo(repo, "mirror_locally", "0", self.token)	createRepo repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "name", "testrepo0", self.token) self.remote.save_repo(repo, self.token)
# todo: get actual error message </s> operation_name,	_ValidationException HTTPStatusCode=404, ),
# todo add assertions </s> self.assertequal(poll.objects.count(), 2) # cause setup created one already	test_creation def test_creation(self): p = Poll.objects.create(question = "lo lo", pub_date = timezone.now())
# todo(ralexstokes) look at better way to handle once we have fork choice in place </s> def is_sync_peer_selected(self) -> bool:	is_sync_peer_selected @property
# todo add options to maodify the sorted by key and the header options </s> 'stop recording and exit:  quit'	do_quit print('Thank you for using Poseidon') self.close()
# todo: remove in v8 </s> yield ft	flatten else: for t in task:
# todo: validate </s> yield logger	formatted_logger logger = logging.getLogger('test') handler.setFormatter(logging.Formatter('%(levelname)s : %(message)s')) logger.removeHandler(handler)
# todo: optionally enable multiple selection </s> return datum	tableView_objectValueForTableColumn_row_ 'label': str(value), 'icon': icon,
# todo: should make sure that all the shapes conform here, </s> return expr	transform_Reduce
"name": drug,  # todo: case_name? </s> if messy_date_string:	clean_date cleaned_datetime = parse(messy_date_string) return cleaned_datetime.date()
# todo: perform appropriate postgres1 conversion between python datetime/mxdatetime </s> colspec += " serial"	PGSchemaGenerator def get_column_specification(self, column): colspec = column.name else: colspec += " " + column.type.get_col_spec()
# todo use bincount! </s> all uppercase and lowercase iupac protein characters are supported.	Protein Sequence Notes @classproperty def nondegenerate_chars(cls):
'label': {}, # todo fill with a localized key </s> ret[attr][language] = unicode(dict_l[attr])	localization_set ret[attr] = {}
# todo: error if row_identifier is none </s> self._connection = none	_Close def _Close(self): self._cursor = None
# todo: configurable timeout??? </s> override this method if you wish to handle the decoded data	_handle_decoded_payload differently. self.syndic_cmd(data)
#todo add server-side input validation here (currently validated on client) </s> 'type': action.action_type,	action_to_json def action_to_json(action): 'keyword': action.keyword, 'name': action.action_name,
# todo check if result is in scope -> no evaluation necessary </s> return true	_is_name_break_scope return False elif isinstance(par, pr.Import) and par.is_nested():
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
# todo(twd2): improve here: </s> class usernewmailwithcodehandler(base.handler):	UserNewmailWithCodeHandler @base.require_priv(builtin.PRIV_USER_PROFILE) @base.route_argument
gc.collect()  # todo: see first comment above </s> ok_(isinstance(c(test_path), type(test_path)))	test_EnsureDataset test_path = opj("some", "path")
# todo ... </s> def addchild(self, child):	addChild
return 0  # todo </s> return self._gid	realid @pyqtProperty(str, notify = initialized)
# todo: timeline is global, get rid of it </s> with open(dst, 'wb+') as out_file:	copy_mustache src = os.path.join(os.path.dirname(__file__), 'mustache.html') dst = os.path.join(kw['output_folder'],'mustache.html') data = in_file.read().replace('{{first_post_data}}', first_post_data) out_file.write(data)
# todo: that’s a memory leak </s> pango_font = glyph_item.item.analysis.font	show_first_line num_glyphs = glyph_string.num_glyphs offset = glyph_item.item.offset hb_font = pango.pango_font_get_hb_font(pango_font) font_hash = hb_face = harfbuzz.hb_font_get_face(hb_font)
# todo: try/catch </s> self.actors.append(act)	setup_actors for actor in config.get("actors"): a = class_for_name(actor["module"], actor["class"])
# todo(jflesch): python 3 problem </s> check the spelling in the text, and compute a score. the score is the	check_spelling number of words correctly (or almost correctly) spelled, minus the number of mispelled words. Words "almost" correct remains neutral (-> are not
# todo: log exception </s> reader = output.split('\n')	scan@103 reader = None metadata = {} for row in reader: row = row.split('\t')
#todo fixme: we should provide an option to create the page </s> real_claims = list()	main@45 claims.append(arg) if len(claims) % 2 != 0: c = 0 while c != len(claims):
# todo: double-check dates </s> return  # exit	stake@183 painting.paint_staking_confirmation(emitter=emitter, ursula=STAKEHOLDER, elif action == 'divide': if staking_address and index is not None:
pass # todo </s> f = conn.send('foobar')	test_send_connecting def test_send_connecting(conn): assert f.failed() is True assert isinstance(f.exception, Errors.NodeNotReadyError)
# todo: if compilation failed, we can't proceed; handle this. </s> self.refresh_module_info(filename)	refresh_all_module_info files_in_dir = list_files_in_dir_recursively(base_dir) haskell_source_files = [x for x in files_in_dir if x.endswith('.hs')] end_time = time.clock() log('total inspection time: {0} seconds'.format(end_time - begin_time))
# todo: test with/without collection_dir and with/without df </s> pke.utils.compute_document_frequency(	create_df def create_df(corpus_dir, tmp_path, name='corpus_df.gz'): str(corpus_dir), str(corpus_df_file), extension='txt', n=1) corpus_df = pke.utils.load_document_frequency_file(str(corpus_df_file))
# todo make it callable </s> else:	BceKgeLoss def loss(self, scores, labels=None): if labels is None: self.loss(scores.view(-1), labels.view(-1))
# todo(vek): need to pass context in for access to auth_token </s> assert state in power_state.valid_states(), "bad state: %s" % state	InstanceInfo class InstanceInfo(object): def __init__(self, name, state): self.state = state
# todo(b/131363314): the reference executor should support generating </s> tf_poly_sub = tf_computation(lambda x, y: foo(x, y, false))	test_py_and_tf_args def foo(x, y, add=True): return x + y if add else x - y self.assertEqual(tf_poly_add(2, 1), 3) self.assertEqual(tf_poly_add(2., 1.), 3.)
# todo: fix this for opengl core </s> f._platform_event_data = []	_event_wrapper def _event_wrapper(f): f._platform_event = True f._platform_event_data.append(data) return f
# todo: may use sys.stdout.encoding if output_file = '-' </s> sys.exit(1)	_get_field_names if diff: missing = ', '.join(['"{}"'.format(field) for field in diff]) else: result = []
# todo: allow child connections </s> if i.type == 'd':	distrib_branch_level log.add_msg_contents(msg) if not self.has_parent: have a parent, stop connecting to the others. """ if i.conn != msg.conn.conn:
# todo: remove when no longer checking for alias. </s> return bin_path in os.environ['path'].split(':')	has_bin_in_path
# todo(guillermooo): use tokens to identify requests:file. </s> v.erase_regions('dart.errors')	erase_errors v = sublime.active_window().active_view()
# todo: not tested against real openmrs instance </s> def get_how_to_search_patients(search_string):	get_how_to_search_patients
# todo: the button should be styled within inputspecs </s> self.inputpz.showzpk()	InputTabWidgets self.inputSpecs.load_all_specs() self.inputInfo.showInfo() self.sigFilterDesigned.emit() # pyFDA -> plot_all.updateAll
# todo: total hack below. implement more principled formatting. </s> summary_metadata.plugin_data.plugin_name = 'text'	GinConfigSaverHook if self._summarize_config: md_config_str = self._markdownify_operative_config_str(config_str) summary_metadata.plugin_data.content = b'{}' text_tensor = tf.make_tensor_proto(md_config_str)
# todo: unit tests </s> 'dashboard_id': dashboard_id,	dashboard dashboard_folder = find_dashboard(user) dashboard_id = dashboard_folder._id
# todo! don't use tostring(not unique for large arrays) </s> return hash((self.tpm.tostring(),	__hash__ self.current_state, self.past_state,
# todo(sdake) the parameters to delete operations are highly suspect </s> self.fake_kwargs = kwargs	_fake_rpc_method def _fake_rpc_method(*args, **kwargs): if expected_retval: return expected_retval
# todo: test coverage of this branch </s> newsletter.on_site, slug=newsletter_slug	subscribe_user @login_required def subscribe_user(request, newsletter_slug, confirm=False): ) already_subscribed = False
# todo(mordred) when this changes to rest, force interface=admin </s> except ironic_exceptions.clientexception:	get_nic_by_mac try: return self.manager.submit_task( return None
# todo: change this to use assertsetequal: </s> self.asserttrue('<field name="title">example doc ☃ 1</field>' in doc_xml)	test__build_doc 'popularity': 10, } self.assertTrue('<field name="id">doc_1</field>' in doc_xml) self.assertEqual(len(doc_xml), 152)
raise exceptions.mpdnotimplemented  # todo </s> same as prio, but address the songs with their id.	prioid@353 ``prioid {PRIORITY} {ID...}``
# todo: find a way to not depend on a specific font </s> line = make_text(u'some text')	test_line_with_any_width @SUITE.test width, _height = line.get_size() line = make_text('some some some text some some some text')
# todo non-spatial features, policy </s> state = nn.relu(self.fc1(state))	Simple spatial_action = self.spatial_policy.forward(state) print(screen.size(), minimap.size()) value = self.fc2(state) return spatial_action, value
# todo: go to "blazemeter" section for these settings by default? </s> proj_name = self.parameters.get("project", self.settings.get("project", none))	__get_test_id def __get_test_id(self, token): if not token: if isinstance(proj_name, (int, float)): proj_id = int(proj_name)
#todo(cp16net): need to set the return code correctly </s> mapper = routes.mapper()	API class API(wsgi.Router): super(API, self).__init__(mapper) self._instance_router(mapper)
# todo: jrk: chunking times points needs to be simplified </s> if (not isinstance(train_time, float) and	GeneralizationAcrossTime ------- fig : instance of matplotlib.figure.Figure not (isinstance(train_time, (list, np.ndarray)) and np.all([isinstance(time, float) for time in train_time]))):
#todo: add the whole build log? it could be several thousand </s> def addtest(self, buildid, passed=true, buildinfo=none):	addTest
# todo: we are losing information about tables which are views here </s> super(command, self).run_from_argv(argv)	run_from_argv def run_from_argv(self, argv):
# todo: since multiclass is done internally - we need to check </s> " classes. make sure that it is what you intended to do" )	__libsvm if self.clf.model.nr_class != 2: warning("You are estimating sensitivity for SVM %s trained on %d" % svcoef = N.matrix(self.clf.model.getSVCoef()) svs = N.matrix(self.clf.model.getSV())
# todo assuming one_hot as default for now </s> read all the data into memory, apply the preprocessor,	apply_preprocessor then reassign table array. x_ = self.X[:]
# # todo: should be able to just access the api from qml. </s> self.__additional_components_view = self._application.createqmlcomponent(path, {"manager": self})	_createAdditionalComponentsView if not plugin_path: return if not self.__additional_components_view: Logger.log("w", "Could not create ui components for UM3.")
# todo: save state right here </s> elif isinstance(sched_item, enterloop):	map_schedule_onto_host_or_device if isinstance(sched_item, RunInstruction): current_chunk.append(sched_item) loop_end = loop_bounds[i] inner_schedule = []
#todo: handle arc segments </s> return [round(a[0]+r*cos(phi),4),round(a[1]+r*sin(phi),4)]	pol2car
# todo: these should always be unicodes </s> def _only_zinc_compileable(self, target):	_only_zinc_compileable
# todo: make more reliable, modularize </s> properties = {	BaselineGradientAnalyzer "name": "BaselineGradient", "show_as": "rgb",
# todo: this test requires manifold access, see: t88318502 </s> @unittest.skipif(not torch.cuda.is_available(), "cuda not available")	TestCaffe2Export ts_model.save(os.path.join(d, "model.ts")) def testMaskRCNN(self): def testMaskRCNNGPU(self): self._test_model("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml", device="cuda")
# todo: find why this doesn't work with this set to 0 </s> assert tf.test.is_gpu_available()	gradient_memory_measure_mb def gradient_memory_measure_mb(): tf.reset_default_graph() tf.set_random_seed(1)
# todo: select correct protocol based on alpn (?) </s> self.expected_form_out = "relative"	process_connect_request self.expected_form_in = "relative"
# todo remove in v8 </s> def inject_dependency(self, target, dependency):	inject_dependency
# todo_recorders - need to pass in parent info instead of none </s> run the solver.	LinearRunOnce This is done without iteration or norm calculation. SOLVER = 'LN: RUNONCE' Parameters ----------
# todo : pytest.mark.parametrise once nose is gone. </s> pass	SB
# todo: this regex could change based on project req format </s> logging.info("importing document '{}'...".format(prefix))	new_document @param parent: parent document's prefix (if one will exist) @return: imported Document try: document = tree.new_document(path, prefix, parent=parent)
# todo: run in built-in terminal </s> if os.path.exists(picked):	UnpackWindow self, "Pick directory", QtCore.QDir.currentPath()) error_msg(self, "This directory already exists", 'warning') else:
# todo: scale and translation could be merged into a single network </s> assert mask.get_shape() == shape[1:]	get_mask def get_mask(self, x, dtype): shape = x.get_shape() return mask
# todo: this logic also exists in self.use_as_template() </s> :returns: whether user has permission to edit this node.	can_edit Must specify one of (`auth`, `user`). :param Auth auth: Auth object to check if not auth and not user: raise ValueError('Must pass either `auth` or `user`')
result = np.vstack((result, result_a))          # todo: https://github.com/tensorlayer/tensorlayer/issues/288 </s> list_of_list : a list of list	flatten_list Input a list of list, return a list that all items are in a list. Parameters Examples --------
# todo: fails because of missing svg support </s> table4, = wrapper4.children	test_ph_tables assert wrapper2.style['border_collapse'] == 'collapse' assert wrapper3.children[0].style['border_left_style'] == 'hidden' assert table4.style['border_top_style'] == 'outset' assert table4.style['border_top_width'] == 10
# todo: add .data and .grad to syft tensors </s> return false	module_is_missing_grad for p in model.parameters(): if p.grad is None:
"""the line which triggered the message"""#todo elaborate </s> if attr == 'reply':	JenniWrapper self.bot = jenni def __getattr__(self, attr): return (lambda msg: self.bot.msg(sender, origin.nick + ': ' + msg))
# todo: use lt and rt in profile as well </s> for b in triggers:	create_binding_buttons w = self.builder.get_object("bt" + b.name) if w: w = self.builder.get_object("bt" + b) if w:
# todo unordered float </s> n.zeroextend(instr.mode),	jno meip = mRIP[instr.mode] n = m2_expr.ExprId(ir.get_next_label(instr), dst.size) dst.zeroExtend(instr.mode)) e.append(m2_expr.ExprAff(meip, dst_o))
# todo: is this safe? </s> return self.intf( intf ).isup()	intfIsUp def intfIsUp( self, intf=None ):
assert ts.nanosecond == 0 # todo: handle nanosecond (timestamps.pyx) </s> retval2 = convert_datetimestruct_to_datetime(10, arg2ref, myref(out))	parse_datetime_str arg2ref = myref(arg2) retval = parse_iso_8601_datetime(arg0, arg1, arg2ref, myref(arg3), myref(arg4)) return integer_to_dt64(out)
pass # todo </s> when versioning is turned on.	test_data_relation_without_version pass # TODO
# todo(mnaser): uncomment this in patch resolving the issue </s> def _setup_compute_service(self):	_setup_compute_service
# todo query = 'query statement' </s> scope, query=query, page_size=page_size)	search_all_iam_policies@26 from google.cloud import asset_v1 client = asset_v1.AssetServiceClient() for page in response.pages: for policy in page:
# todo: validate schema </s> schedule = schedule.copy()	_schedule_by_id def _schedule_by_id(schedule_id): for schedule in manager.config.get('schedules', []): schedule['id'] = schedule_id return schedule
# todo: add rotary inertia </s> omit_set_map.add((nid, int(compi)))	get_omit_set comp = omit.components for nid in omit.ids: elif omit.type == 'OMIT': for nid, comp in zip(omit.ids, omit.components):
# todo: remove in 21.08 </s> download_audio(cache_audio_dir, cache_text_path)	main@187 cache_text_dir = os.path.dirname(cache_audio_dir) cache_text_path = os.path.join(cache_text_dir, 'cache_text.txt') copy_cache(cache_audio_dir)
#todo make more readable </s> days = hours / 24	format_duration def format_duration(self, duration): hours %= 24 minutes = duration % 60
# todo: arrange </s> self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token)	createRepo def createRepo(self): repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "mirror_locally", "0", self.token) self.remote.save_repo(repo, self.token)
# todo check response </s> result = self.diagnostics.read_data_by_identifier([0x01])	DiagnosticsOverIsoTpTestCase def test_create_iso_14229_1(self): self.assertIsInstance(self.diagnostics, iso14229_1.Iso14229_1, "Failed to initialize ISO-14229-1") self.assertIsInstance(result, bytearray, "Did not receive response") print("Result:", list(map(hex, result)))
# todo: use vendorid and productid </s> with open(path, 'w') as modprobe:	post_install def post_install(self, dest_dir): modprobe.write("options %s %s\n" % (self.KMS, self.KMS_OPTIONS))
#todo: issue warning that this is an unsafe operation, but doing it cause user insists </s> if self.selinux_mls_enabled():	selinux_initial_context def selinux_initial_context(self): context.append(None) return context
# todo: add the rest of the api actions here and call them directly from the api controller </s> self.deserializers.update({	add_deserializers def add_deserializers(self): }) self.deserializable_keyset.update(self.deserializers.keys())
# todo(nakago): check why tolerance is high </s> return nfp(out_dim=out_dim)	model @pytest.fixture
# todo: apply _is_zero attribute </s> examples::	_del_derived def _del_derived(self): r""" sage: M = FiniteRankFreeModule(ZZ, 3, name='M') sage: t = M.tensor((2,1), name='t')
# todo: skips header parsing </s> model.read_abaqus_inp(abaqus_inp_filename)	read_abaqus def read_abaqus(abaqus_inp_filename, log=None, debug=False): return model
# todo: error handling </s> self.__backend.remove(triple)	__isub__ def __isub__(self, other): return self
# todo: remove this assert when we're confident the code is correct </s> return channel_df[pd.timestamp(start_datetime):pd.timestamp(end_datetime)]	filter_datetime_single start_datetime = channel_df.index.values[0] if end_datetime is None:
# todo: check how to be writeable only from same group </s> except socket.timeout:	loop self.tegra.open(interval=500) elif datagram == "stop": pass
# todo debug </s> self.type = none	RuleElement class RuleElement: self.triggered = False self.timeWhenTriggered = 0.0
# todo testing </s> async_call.send(none, self.i_am, address, device_id, max_apdu_len,	i_am_callback def i_am_callback(address, device_id, max_apdu_len, seg_supported, vendor_id)
# todo: this is untested. </s> error_depth = _lib.x509_store_ctx_get_error_depth(store_ctx)	_VerifyHelper cert = X509.__new__(X509) cert._x509 = _lib.X509_STORE_CTX_get_current_cert(store_ctx) try: result = callback(connection, cert, error_number, error_depth, ok)
# todo: supports blocking queries and all consistency modes </s> def ttl_pass(self, check_id):	Check class Check(object): def __init__(self, agent): Mark a local TTL check as passing. return self.agent.http.get(
assert ts.nanosecond == 0 # todo: handle nanosecond (timestamps.pyx) </s> arg1 = len(str)	parse_datetime_str @numba.njit(locals={'arg1': numba.int32, 'arg3': numba.int32, 'arg4': numba.int32}) def parse_datetime_str(str): arg2 = PANDAS_DATETIMESTRUCT() arg3 = np.int32(13)
# todo: must be implemented </s> def should_fetch_kindlegen():	should_fetch_kindlegen
# todo: weather data source changed, temporarily disable, will enable it later when new data source is available. </s> date = self._parse_date(row=row)	_parse_row wh = self._weather(row=row) temp_str = row["Avg Temp"]
# truffle todo: revert </s> def _ishidden(path):	_ishidden
# todo should also include meta-chunk-hash </s> self._check_bad_headers(	test_bad_chunkhash def test_bad_chunkhash(self): self._check_bad_headers( 32, bad_headers={'x-oio-chunk-meta-chunk-hash': 'xx'})
# todo: make truly async </s> def __init__(self):	__init__@10
# todo: create xxx_failure test </s> lookup('test_space', tests))	test_result_space_failure failure_tests = r['failure'] tests = exclude_from_resultlist(r, 'failure')
# todo: add flex option to the implementation </s> if self.interface.content:	winforms_resize for content in self.interface.content: content.refresh()
# todo: adapt units_qs once we allow filtering </s> if unitstate == 'untranslated':	get_step_query if unitstates: state_queryset = units_queryset.none() state_queryset = state_queryset | units_queryset.filter(state=UNTRANSLATED) elif unitstate == 'translated':
time.sleep(.0015) # todo: tune </s> def repaint(self):	CoveragePainter self._painting_worker.start() self._director.coverage_switched(self.repaint) Paint coverage defined by the current database mappings self._priority_paint()
todo: factorize once #1457 is merged. </s> acq_time = get_cds_time(days=tline['days'], msecs=tline['milliseconds'])	_add_scanline_acq_time tline = self._get_acq_time_hrv() else: add_scanline_acq_time(dataset, acq_time)
# todo(thowe): there is a general smell here that this code is </s> return	run_connection print("Connection: %s" % conn) for flavor in conn.compute.flavors():
# todo: maybe[int] and maybe[simple_sum] are invalid </s> self._emitstrfunction(sum, sum_name, depth)	VisitCompoundSum if not self.pretty_print_methods: return
# todo: replace with stream-changed </s> return self.current_tl_track	get_current_tl_track
# todo -- this must not block long (!) </s> either introspection to the documentation of the function, or the	Worksheet_eval evaluate or introspect the cell is ignored. If the cell contains either 1 or 2 question marks at the end (not documentation of the function and the source code of the function respectively.
# todo: in 0.6.0 change this to "disabled": false </s> option dev 'tap0'	test_enabled_missing config openvpn 'test_properties' option ca 'ca.pem' option dev_type 'tap' option dh 'dh.pem'
# todo: re-enable this when https://github.com/ironlanguages/ironpython2/issues/10 is fixed </s> val.x = 2	test_module_exceptions if isinstance(val, type) and issubclass(val, Exception): if "BlockingIOError" not in repr(val): self.assertEqual(val.x, 2) else:
# todo improve logged output </s> 'failed to get vent collector statuses' + str(e))	get_vent_collectors self.logger.debug(
# todo: remnants from rllab -> gym conversion </s> self.sim.data.qpos.flat[self.joint_inds],	Pusher2dEnv return self._get_obs() def _get_obs(self): self.sim.data.qvel.flat[self.JOINT_INDS], self.get_body_com("distal_4"),
# todo(ls): revert this loop to "yield from" </s> the message indicating which exception occurred is always the last	format_exception_only SyntaxError exceptions, it emites several lines that (when printed) display detailed information about where the syntax string in the output. if self.exc_type is None:
# todo(mriedem): move to objectlistbase.__init__ for empty lists. </s> try:	detail servers = self._get_servers(req, is_detail=True) except exception.Invalid as err:
# todo: remove references to _attachments once all forms have been migrated to riak </s> if calced_props['cp_last_form'] is none:	update_calculated_properties } if calced_props['cp_first_form'] is None: del calced_props['cp_last_form'] if calced_props['cp_300th_form'] is None:
# todo: this is quadratic complexity </s> def set_pexpect_child(key,child):	set_pexpect_child
print("why would this happen?")  # todo </s> return self.rec(expr.child)	map_bitwise_not
# steps = 0 # todo </s> self.count += 1	Duplis self.count = 1 def add(self, matrix):
# todo(akhmerov): implement. </s> task1_db.output	test_on_task_result 'result': 'Hey', 'task': {'task1': 'Hey'} ) exec_db = db_api.get_execution(exec_db.id)
# todo(b/159180073): clean raise after fixing dataset reduce. </s> graph_def, init_op)	_get_wrapped_function_from_comp init_op = comp.tensorflow.initialize_op if init_op: def _import_fn(): return tf.import_graph_def(
# todo: filter values. </s> if ctx.needs_input_grad[1]:	SpSpMM m, k, n = ctx.m, ctx.k, ctx.n indexA, valueA, indexB, valueB, indexC = ctx.saved_variables indexB, valueB = transpose(indexB, valueB, k, n) _, grad_valueA = mm(indexC, grad_valueC, indexB, valueB, m, n, k)
# todo: handle errors better </s> return team	Teams except sqlalchemy.exc.IntegrityError as e: db.session.rollback()
# todo: this would be a good candidate for refactoring into a testcase subclass shared across backends </s> self.assertequal([result.id for result in results], [	test_boost searcher = self.raw_whoosh.searcher() self.assertEqual(len(searcher.search(self.parser.parse(u'*'), limit=1000)), 2) 'core.afourthmockmodel.1', 'core.afourthmockmodel.3',
#todo print to stderr </s> if links == none:	scrape_legislation@32 soup = BeautifulSoup(doc)#this gives us an index page re_str = "\w\w*\d\d\d\d\d.htm" yield for link in links:
time.sleep(40)  # todo: should remove after polling get. </s> mpc_1_2_3 = op(mpc_1_2, mpc_2_3)	test_tensor_abstraction_subsets@72 time.sleep(40)  # TODO: should remove after polling get. mpc_2_3 = op(tensor_pointer_2, tensor_pointer_3) time.sleep(40)  # TODO: should remove after polling get. exp_res_1 = op(data_1, data_2)
# todo: deprecation 3.1 </s> return complex_fun(x)	wrapped_fun if iscomplex(x): return jax_fun(x)
# todo: capture stdout for both the test assert and docs embedding </s> newton.options['iprint'] = -1	test_feature_iprint_neg1@33 prob.setup(check=False) prob['y1'] = 10000 ln_scipy.options['iprint'] = -1 prob.run_model()
# todo legacy method to be removed/refactored </s> from corehq.apps.orgs.models import organization	get_organizations def get_organizations(self):
# todo: figure out a dynamic way of doing this </s> self._init_task.cancel()	cog_unload self._init_task.add_done_callback(lambda _: self.scheduler.cancel_all())
# todo(b/132329316) remove when `xla.compile` allows tf.device(tpu). </s> self.assertempty(tested_modules & no_checkpoint_whitelist)	CoverageTest if issubclass(cls, snt.Module): all_sonnet_types.add(cls) self.assertEqual(tested_modules | no_checkpoint_whitelist, all_sonnet_types)
# todo: we should probably have a special folder just for header </s> if cython is none:	CythonExtensionManager libraries=None, compiler=None, raise ImportError('Cython is not available') code = deindent(code)
# todo - restore this when issue __ is fixed. </s> prob.driver = om.doedriver(om.fullfactorialgenerator(levels=3))	test_full_factorial model.add_design_var('x', lower=0.0, upper=1.0) model.add_design_var('y', lower=0.0, upper=1.0) prob.driver.options['run_parallel'] = True prob.driver.options['procs_per_model'] = 1
""" todo: check this description </s> n = f.func_name	declared def __init__(self, f): self.f = f trigger_functions[n] = f def __call__(self, *args):
#todo - use a context manager here once we drop python 2.6 </s> self.assertalmostequal(coordinates[3, 0],  2.5440202962223761)	test_pca self.assertAlmostEqual(coordinates[1, 1], -0.10647619705157851) self.assertAlmostEqual(coordinates[2, 0],  3.1453186907749426) self.assertAlmostEqual(coordinates[3, 1],  0.20633980959571077) self.assertAlmostEqual(coordinates[4, 0],  2.4468278463376221)
# todo: should change to 'bytes' on python3 </s> self.assertnotequal(before, after)	test_table_order_by before = [row.birthdate for row in self.table] self.table.order_by('birthdate') self.assertEqual(sorted(before), after) self.table.order_by('-birthdate')
# todo: remove this - cura-4482 </s> return self.getqualitydefinitionid(self._global_container_stack.getbottom())	activeQualityDefinitionId if self._global_container_stack:
# todo - this needs to be a timezone un-aware timestamp </s> return patch("h.accounts.schemas.models.user")	user_model @pytest.fixture
# todo(philday): add support for timeout (clean shutdown) </s> def host_power_action(self, host, action):	host_power_action
# todo(laigd): remove this check when 312743821 is in the release. </s> padded_x = py_utils.padortrimto(x, y.shape, pad_val=0)	test2DStaticShape with self.session(use_gpu=False, graph=tf.Graph()): x = tf.random.normal(shape=(3, 3), seed=123456) self.assertEqual(padded_x.shape.as_list(), [4, 6]) real_x = self.evaluate(padded_x)
#todo(ziad): use a more sophisticated proxy </s> def auth_filter(app):	auth_filter
# todo: log in browser as a session </s> def login_get():	login_get @response(template_file='account/login.html')
# todo files listed here may not belong to the given camera </s> if camera_config.get('@proto') != 'v4l2':	cleanup_movies logging.debug('cleaning up movies...') for camera_id in config.get_camera_ids(): continue preserve_movies = camera_config.get('@preserve_movies')
raise notimplementederror #todo </s> pass #todo	Span
# todo candidate for move to system/osi as not btrfs related </s> if (re.search(share_name + '$', line) is not none):	share_id out, err, rc = subvol_list_helper(root_pool_mnt) subvol_id = None subvol_id = line.split()[1] break
# todo catch error here </s> def get_value(self, valuename):	get_value
# todo resource arns may contain wildcards, e.g. arn:aws:iam::*:role/admin -- </s> session.run(	load_policies ON CREATE SET r.firstseen = timestamp() SET r.lastupdated = {aws_update_tag} ingest_policy, ARN=policy["Arn"],
# todo: safe labels </s> condition = and_(*self.conditions_for_cuts(self, cell.cuts))	condition_for_cell def condition_for_cell(self, cell): if not cell: return condition
# todo test </s> def parser_keys(self):	parser_keys
# todo: look into nonce prefix </s> self.send_cmd.msg_send_pubkey(sender, key)	svr_msg_request_pubkey with open("deadchat.cfg", "wb") as configfile: self.config.write(configfile) self.chatlog_print("Received id key request from " + sender)
# todo implement. </s> self.dataset_rdd = repartition(self.num_workers)	SparkModel self.dataset_rdd = rdd self.num_workers = num_workers nb_epoch          = parameters['nb_epoch'] batch_size        = parameters['batch_size']
# todo(yanase): implement maximization. </s> def system_attrs(self):	system_attrs @property
# todo: hack, this may be called differently in other agents (replace by root-policy). </s> optimizer.step, policy_vars, loss, loss_per_item, q_values_s, actions, rewards, terminals,	define_api_methods ) else: qt_values_sp, q_values_sp, importance_weights )
# todo: some message needed? </s> if attributes is none:	create_span def create_span(directory, document, start, end, type, attributes = {} else:
# todo: fix this upstream </s> (	root_down return ( (not endpoint.live) or (not str(endpoint.status).startswith("2")) and (not str(endpoint.status).startswith("3"))
# todo: remove </s> def _form_to_db_schema(group_type=none):	_form_to_db_schema
# todo: find out why pls and cca fail. ransac is random </s> estimator = estimator()	test_estimators_overwrite_params 'PLSSVD', 'GaussianProcess']): continue if hasattr(estimator, 'batch_size'): estimator.batch_size = 1
# todo: use upstream implementation when available </s> return random.uniform(self._random_state, shape=self._size)	_rvs
#doc [todo: get material from docstring of same method in assembly.py] </s> self.obj_with_glselect_name[glname] = obj	alloc_my_glselect_name glname = _new_glselect_name()
# todo: should this piece of data be global instead of local to each buffer? </s> def expecting_user_input(self, value):	expecting_user_input self.settings.vi['expecting_user_input'] = value
)  # todo </s> user_id=g_user_id,	get_group_summary yield self.attestations.verify_attestation( attestation, ) valid_users.append(entry)
# todo put this into the regression metric itself </s> return self._sign * self._score_func(y_true, y_pred, **self._kwargs)	_ThresholdScorer sample_weight=sample_weight, **self._kwargs)
# todo: maybe foreginkyes should be taken from freeze orm </s> alter_foreignkey_to_int('articles_articlecontents', 'article')	Migration } def alter_self_foreignkeys(self, orm): alter_foreignkey_to_int('recipes_oldrecipearticleredirect', 'new_id') def move_self_foreignkeys(self, orm):
# todo check </s> pass	set_metadata @interfacedoc
# todo: remove </s> except keyerror:	_render_paginator query_dict = context["request"].GET.copy() try: pass extra_query = ""
# todo: +kwargs </s> commit = stdout.strip()	get_last_commit_hash files, ['git', 'log', '-n', '1', '--pretty=format:%H'], return commit except CommandError as e:
# todo: remove this line when counters are supported in build.py </s> raise invalidvalues()	validate_content_token if len(args) > 1: args[1] = args[1].lower() return (name, args)
# todo proper error messages </s> <dd>gives the type of operating system running mathics.	OperatingSystem class OperatingSystem(Predefined): <dl> </dl> >> $OperatingSystem
# todo: remove check once pytorch avoids a copy for this case </s> def __init__(self, args, params):	FairseqAdam Important note: this optimizer corresponds to the "AdamW" variant of Adam in its weight decay behavior. As such, it is most closely super().__init__(args) fused_adam_cls = get_fused_adam_class()
# todo: cleanly remove clipboard code if it is no longer needed </s> widgets = {	NewFolderForm class Meta(object): model = Folder 'name': widgets.AdminTextInputWidget,
# todo: udpoutgoing style buffer </s> def on_error(self, errcode):	on_error
# todo: really dirty. figure out a better way. </s> return all_actions	_apply_actions_limit current_num_actions = sum(x for _, x in actions_mapping.items()) limit = LIMITS_MAPPING[action_type] remaining_num_actions = limit - current_num_actions if remaining_num_actions < 0:
# todo proper error messages </s> last_word = word	reader continue if stream.io.seekable(): word = "" yield last_word
# todo support multi-discrete actions </s> log_probs = action_pd.log_prob(mus) - torch.log(1 - actions.pow(2) + 1e-6).sum(1)	calc_v_targets else: mus = action_pd.sample() q1_preds = self.calc_q(states, actions, self.q1_net) q2_preds = self.calc_q(states, actions, self.q2_net)
# todo: to be removed in v2.8.0 </s> return self.model_from_path_setting('flat_menu_model')	FLAT_MENU_MODEL_CLASS @property
# todo: do_cert? </s> def supported_protocol(result):	supported_protocol
# todo: i'm not quite clear on the difference between </s> adjusted_bytes.append(byte)	read_message if byte > 127: adjusted_bytes.append(127) data_bytes = adjusted_bytes else:
# todo: do data augmentation </s> dtype=tf.float32)	_rotate def _rotate(): rotate_angle = tf.random_uniform([], minval=-rotate_angle_max, rotate_angle = tf.div(tf.multiply(rotate_angle, math.pi), 180.) rotated_image = tf.contrib.image.rotate([image], [rotate_angle],
# todo: approximate output size properly </s> def write_send_buff(shuffle_meta, node_id, i, val, data):	write_send_buff
# todo: should allow multi_dimensional inputs/outputs </s> shape = x.get_shape()	get_mask mask = checkerboard(shape[1:], parity=self.parity, dtype=dtype) assert mask.get_shape() == shape[1:]
end_tok = y.data[0, -1] # todo </s> cat_labels[e, :len(l)] = l	end_pad_concat for e, l in enumerate(labels):
# todo this makes self variables non-breakable. wanted? </s> <parsing_representation.scope.get_defined_names>` does.	_get_defined_names_for_position def _get_defined_names_for_position(scope, position=None, start_scope=None): Return filtered version of ``scope.get_defined_names()``. - If `position` is given, delete all names defined after `position`. - For special objects like instances, `position` is ignored and all
# ensure_authorized_to('create', announcement) # todo: uncoment? </s> title=gettext("manage global announcements"))	announcement def announcement(): announcement = announcement_repo.get_all_announcements() return handle_content_type(response)
# todo - add some tests to this response </s> def test_list(self):	TestReportTests response = self.client.get(self.list_url, filters, format='json') self.assertEqual(response.status_code, 200) response = self.do_list() self.assertEqual(len(response), 0)
# todo: unit tests </s> for child in reversed(node.nodes):	get_dashboard_nodes def get_dashboard_nodes(node, auth): if child is not None and not child.is_deleted and child.resolve().can_view(auth=auth) and node.can_view(auth): rv.append(child)
# todo: return return value </s> @wraps(f)	_keywords_only accidentally get passed the thing they're decorating as their first argument. def g(self, **kw): return f(self, **kw)
# todo have no idea if is cdecl or stdcall </s> return params['dest']	hook_memcpy import traceback print(traceback.format_exc())
# todo: remove this line when the time is right. </s> emitter = setup_emitter(general_config)	derive_policy_pubkey @option_config_file @group_general_config ALICE = character_options.create_character(emitter, config_file, general_config.json_ipc, load_seednodes=False) return ALICE.controller.derive_policy_encrypting_key(label=label)
# todo : pytest.mark.parametrise once nose is gone. </s> class sb(sa):	SB
# todo: kernels need some sort of structured form </s> def transforms(self):	transforms @property
pass # todo </s> def collect_incoming_data(self, data):	collect_incoming_data
##todo: not saving lines yet </s> embeddings_in_memory=embeddings_in_memory, out_path=out_path)	_calculate_evaluation_results_for eval_mini_batch_size: int, out_path: Path = None): mse = metric.mean_squared_error() mae = metric.mean_absolute_error()
# todo: check the data! </s> self.asserttrue(count > 0)	test_csv u'CentralITProvision': u'1223', u'StaffCoverAndOtherCosts': u'0', u'firstName': u'Mr Mark', u'MPOtherAir': u'0', u'MPOtherMileage': u'0',
"""todo doc me""" </s> return the header row for the given table. e.g.::	fields >>> from petl import fields >>> table = [['foo', 'bar'], ['a', 1], ['b', 2]]
# @todo: remove this if in 0.6 </s> name=el.get('name'),	_to_image def _to_image(self, el): driver=self.connection.driver, extra={'serverId': el.get('serverId')})
# @todo: display better tick labels for date range (e.g. 06/01 - 06/05) </s> fp = open(filepath, 'rb')	_parse_csv return "", read_csv(fp, sep=",", index_col=0, parse_dates=True)
# todo: implement </s> elif key in (qt.key_shift, qt.key_control, qt.key_alt, qt.key_meta):	Completer if ev.text()[-1:] > " " or ev.key() in (Qt.Key_Backspace, Qt.Key_Delete): self.showCompletionPopup() return True elif (ev.matches(QKeySequence.MoveToNextChar) or
# todo: kickoff syncing process with this peer </s> return self.listen_maddr.encapsulate(multiaddr(f"/p2p/{self.peer_id}"))	listen_maddr_with_peer_id @property
# todo: checks for being not outside of this repository </s> parameters	rm_url @normalize_path ---------- file_: str
# todo change affinity on osx/linux </s> try:	kill proc.kill() except psutil.NoSuchProcess: process.kill() except psutil.NoSuchProcess:
# todo: this is lazy, we should only reconfigure the drone(s) who are actually </s> 'private_key': private_key}))	_handle_command_genkeys def _handle_command_genkeys(self, name): private_key, publickey = self._get_zmq_keys(name)
# todo: fix with stubber / before send event </s> }	TestApiGateway 'stageName': 'bar', 'exportType': 'swagger', with mock.patch('botocore.endpoint.Endpoint._send') as _send: _send.return_value = mock.Mock(
# todo: is incref required? </s> ):	check_element_type with c.builder.if_then( cgutils.is_null(c.builder, typobj), c.builder.store(cgutils.true_bit, errorptr) loop.do_break()
# todo enable when it will not crash </s> worker = aptworker(self.get_toplevel(),	do_action_for_app self.update_action_button(self.UNINSTALLING_ACTION) elif action_id == self.UPDATE_ACTION: finish_handler=self.on_update_work_finished, data={'parent': self})
# todo support startblock, endblock </s> else:	_processTxEvents for tx_id, er in copy(self._tx_ids).items(): if self._filtered: self.handle_full_tx(block, tx_id, er)
#time = "todo" </s> jam = jams.jams()	create_JAMS fill_global_metadata(jam, lab_file) annot = jam.sections.create_annotation()
# todo?: does not match two subsequent variables or strings, such as  "start" # foo # bar # "end"  or  "start" # "end". </s> post = '"' if m.group('post') is not '"' else ''	repl pre  = '"' if m.group('pre')  is not '"' else ''
# todo: kernels need some sort of structured form </s> def transforms(self):	transforms @property
# todo: check types </s> getvalues = c.pyapi.object_getattr_string(val, "values")	unbox_timestamp_series @unbox(TimestampSeriesType) return unbox_array(types.Array(dtype=types.NPDatetime('ns'), ndim=1, layout='C'), getvalues, c)
# todo: delete this </s> return db_transaction.lastrowid	do_save def do_save(db_transaction): row = (data_payment_rate, ManagedEncryptedFileDownloader.STATUS_STOPPED, stream_hash)
# todo per-sync cached results </s> children = []	SyncBase class SyncBase(Base): plex = PlexInterface trakt = TraktInterface
# todo: hints </s> '.cfg', shutit_util.print_config(cfg))	record_config self.send_file(cfg['build']['build_db_dir'] + '/' + cfg['build']['build_id'] +
# todo: ... </s> return os.path.join(conf['dump_path'], os.path.basename(file_name))	get_dump_path
# todo: reformat or delete </s> camera.trackbodyid = -1	sawyer_pusher_camera_upright camera.lookat[2] = 0.45 camera.elevation = -50
# todo: error handling? </s> of a pipeline also uniquely identifies a launch.	Launch @api.response(404, 'Launch not found') class Launch(Resource): @api.doc('get_launch') @api.marshal_with(launch)
# todo: remove in v2.8 </s> c.roles.set([device_roles[i]])	setUpTestData ) c.regions.set([regions[i]]) c.platforms.set([platforms[i]]) c.tenant_groups.set([tenant_groups[i]])
# todo: use pybossa uploader! only for debugging: </s> except:	make_onefile_memzip try: import zlib mode= zipfile.ZIP_STORED zipf = zipfile.ZipFile(memzip, 'w', mode)
# todo: some streams were redirected, we need to manually work them </s> try:	areAdminRightsElevated processToken = HANDLE() if not OpenProcessToken(pid, TOKEN_READ, ctypes.byref(processToken)): elevated, elevatedSize = DWORD(), DWORD() if not GetTokenInformation(processToken, TokenElevation, ctypes.byref(elevated),
# todo: add pi_stack and cation_pi to feature_types (it's not trivial </s> [protein_file])	GridPoseScorer raise ValueError("feat not defined.") def score(self, protein_file, ligand_file): dataset = NumpyDataset(X=features, y=None, w=None, ids=None) score = self.model.predict(dataset)
# todo: look at args for remotedata </s> yield gen.sleep(1 - (time() - start))	delete_intermediates del delete_keys[:] k += len(_keys) center_stream.close() center_done.set()
# todo archive schema and return </s> logging.info('end extract')	extract@42
# todo(sano): deal with maximize task. </s> return study.study_uuid	get_study_uuid_from_id def get_study_uuid_from_id(self, study_id): session = self.scoped_session()
# todo: will work only if table.fields is ordereddict </s> max_sizes[field_name] = length	_max_column_sizes for field_name, value in zip(header, row): length = len(value) return max_sizes
# todo: remove </s> log.debug(u'registered core blueprint: {0!r}'.format(blueprint[0]))	_register_core_blueprints module = loader.find_module(name).load_module(name) for blueprint in inspect.getmembers(module, is_blueprint):
# todo: remove this when we depend on genshi >= 0.5 </s> link = {'href': href, 'title': title, 'type': mimetype, 'class': classname}	add_link linkset = req.chrome.setdefault('linkset', set()) if linkid in linkset: links = req.chrome.setdefault('links', {}) links.setdefault(rel, []).append(link)
#todo could use original filename to verify this </s> client = self.wc	_assert_get_song (GM has no native get for songs, just list). :param client: a Webclient or Musicmanager songs = client.get_all_songs() found = [s for s in songs if s['id'] == sid] or None
# todo: this is ugly use of exceptions; is there a better way to track whether in a given type of request? </s> pass	StoredObject @with_proxies(proxied_members, get_cache_key)
# todo: check for error 'toomanyrules' </s> raise targetgroupnotfounderror()	deregister_targets if target_group is None:
# todo: valid and invalid values for the rest of the user model's fields. </s> user['password'] = 'xxx'  # this password is too short.	test_user_update_with_short_password def test_user_update_with_short_password(self): with nose.tools.assert_raises(logic.ValidationError) as context: helpers.call_action('user_update', **user)
# todo: what should we do here, tell user the repo </s> if request.user.org:	group_members Add group members. member_name_str = request.POST.get('user_name', '') for member_name in member_list: mail_sended.send(sender=None, user=request.user.username,
# todo: break this tuplet stuff into a helper function shared for <note>, <rest>, and <chord> </s> affected elements (``@m21slurstart`` to the element at the start of the slur and	addSlurs creating a :class:`Slur` object, adding the affected objects to it, and storing the :class:`Slur` in the ``slurBundle``. ``@m21SlurEnd`` to the element at the end). These attributes hold the ``id`` of a :class:`Slur` in the ``slurBundle``, allowing :func:`addSlurs` to find the slur and add
# todo(dcramer): we want to be less aggressive on disabling domains </s> pass	DomainBlacklisted
# todo: do i need this? </s> def get_cutting_plane_from_surface_elements_x_cone(self):	MeshTools raise RuntimeError('not implemented; %r' % element.type) V += v raise NotImplementedError('hasnt been implemented') def get_cutting_plane_from_surface_elements(self):
# todo: make sure values are actually adding/etc together! </s> res = p.shape	test_parameter_shape p = Parameter(var)
# todo verify produced answer </s> self.linear_features = modulelist()	CNV in_quant_type = get_quant_type(in_bit_width) stats_op = get_stats_op(weight_quant_type) self.conv_features.append(get_act_quant(in_bit_width, in_quant_type)) for i, out_ch, is_pool_enabled in CNV_OUT_CH_POOL:
# todo: for logs </s> rpn_cls_prob = tf.reshape(rpn_cls_prob, [-1, 2], name='rpn_cls_prob_flatten')  # todo(debug): shape: (6417, 2)	Dataset rpn_labels = tf.cast(tf.reshape(rpn_labels, [-1]), tf.int32, name='rpn_labels')  # TODO(debug): shape: (57753,) labels = tf.boolean_mask(rpn_labels, labels_not_ignored, name='labels') cls_prob = tf.boolean_mask(rpn_cls_prob, labels_not_ignored)
fwd_from=none,  # todo select from the database </s> raise	dump_forward except sqlite3.IntegrityError as error: self.conn.rollback()
# todo remove once minimum required matplotlib version reaches 2.0 </s> x_pos = (event_frac - int(event_frac)) * self.width	time2xy y_pos = self.extreme_values.shape[0] - int(event_frac) - 0.5
# todo test this </s> a command to get a node into a desired state by pushing volumes, starting	ChangeStateScript @implementer(ICommandLineScript) and stopping applications, opening up application ports and setting up routes to other nodes.
# todo: cleanup and deprecate worker_address in config files, leaving only checksum_address </s> message = "removed all stored node node metadata and certificates"	forget_nodes self.node_storage.clear()
# todo: remove once elasticsearch v6.x is deprecated. </s> 'type': 'keyword',	ElasticsearchOutputModule 'ignore_above': ( self._ELASTIC_ANALYZER_STRING_LIMIT), }, },
continue  # todo: log this </s> schema['enabled'] = config_lib.boolean()	get_config_schema def get_config_schema(self): :returns: :class:`~mopidy.config.schema.ExtensionConfigSchema` return schema
return deserialize(self.mechanism_bin, from_bytes=true)  # todo: techdebt fix </s> id = column(integer(), primary_key=true, autoincrement=true)	Mechanism@26 class Mechanism(Base): mechanism_bin = Column(LargeBinary(3072), default=None) @property
# todo: ... </s> returns a list of child entities that don't have cchq_case_id set	get_children_only_theirs dhis2_api = Dhis2Api(settings.DHIS2_HOST, settings.DHIS2_USERNAME, settings.DHIS2_PASSWORD) return dhis2_api.gen_instances_with_unset('Child', 'cchq_case_id')  # TODO: Or would that be 'CCHQ Case ID'?
# :todo: implement test. </s> self.assertnotequal(tryte[2], trit(-1))	test_init_mixed_types self.assertEqual(tryte[1], Trit(1))
# todo: handle output diffing with plugins? </s> assert a.output_type == b.output_type	diff_single_outputs predicates=None, differs=None): "DiffOp a pair of output cells." if a.output_type in ("execute_result", "display_data"): di = MappingDiffBuilder()
@unittest.skip('not written')  # todo: finish! </s> raise notimplementederror	test_SimpleNamespace @unittest.skip('not written')  # TODO: finish! @py3_only
# xxx todo fix earley to maintain correct order </s> b: ("ab"|/[^b]/)+	test_anon g = Lark(r"""start: B
# todo: support ps fault-tolerance </s> self._var_to_ps[v.name] = string_to_id(	init_ps_var_partition ps_vars = {} for v in self._non_embed_vars.values(): v.name, len(self._ps_stubs) )
# todo: test me. </s> self.enter_normal_mode()	eval_full_command elif self.mode != MODE_NORMAL:
# todo(yifanmai): merge _setup_if_needed into setup </s> start_time = datetime.datetime.now()	construct_fn def construct_fn(model_load_seconds_callback: Callable[[int], None]): result = load.EvalSavedModel( eval_saved_model_path,
# todo(cmaloney): good exception catching, etc </s> start_response('200 ok', [('content-type', 'text/html')])	wsgi_app@10 subscriber.handle_event(json.load(env['wsgi.input']))
# todo: check if we can avoid py3 specific here </s> level={true: logging.debug,	_log_err def _log_err(self, line, expected=False): if line: False: logging.ERROR}[expected])
# todo: make this collection_type </s> $link: 0	test_workflow_run_output_collection_mapping - tool_id: collection_creates_pair state: - tool_id: collection_paired_test state:
# todo: assert </s> self.remote.modify_repo(repo, "mirror_locally", "0", self.token)	createRepo self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token)
raise exceptions.mpdnotimplemented  # todo </s> ``subscribe {name}``	subscribe@18 @protocol.commands.add('subscribe') def subscribe(context, channel): Subscribe to a channel. The channel is created if it does not exist already. The name may consist of alphanumeric ASCII characters plus
# todo: dispatch on name to validate content (string, number, tuple) </s> return func	_StyleDeclarations def reg(func): for prop in properties: return reg def __call__(self, property, value):
# todo ... </s> self.grid.addwidget(child.nativeguiobject, 0, 0)	addChild
# todo: provide a kernel which will describe how coordinates are extruded. </s> def test_firedrake_extrusion_assemble(family, degree):	test_firedrake_extrusion_assemble assert integrate_assemble_p0(family, degree) < 1.0e-14
# todo(sbauza): the current placement noauthmiddleware returns a 401 </s> return ()	fake_initialize_connection def fake_initialize_connection(self, context, volume_id, connector): return {}
# todo: add initialization </s> orthogonalty_constraint = torch.abs(orthogonalty_constraint)	compute_loss orthogonalty_constraint_denominator) orthogonalty_constraint = (orthogonalty_constraint_numerator / orthogonalty_constraint_denominator) - ( orthogonalty_constraint = torch.sum(orthogonalty_constraint) soft_constraints = self.weightning_soft_constraint * (entity_constraint + orthogonalty_constraint)
# todo: make sure config exists in /etc </s> work_id=share.work_id,	save_work message_id = random.randint(1, 1e5) msg_factory = message_factory.SwirlMessageFactory() enonce2=share.enonce2, otime=share.otime,
#todo - there could be room for both a responsiveness check and a valid </s> synthesized_cname_info.servers_clients[(server, client)].append(response)	create_or_update_cname_from_dname_info synthesized_cname_info.servers_clients[(server, client)] = []
recording_uuid = none #todo </s> except keyerror:	is_pupil_mobile_recording info_csv = utils.read_info_csv_file(rec_dir) try: return False
return user.address  # todo: update </s> else:	create_personal_data_fields if section is None: section = RegistrationFormPersonalDataSection(registration_form=regform, title='Personal Data') existing = {x.personal_data_type for x in section.children if x.type == RegistrationFormItemType.field_pd} missing = set(PersonalDataType) - existing
# todo: remove unescape_entities when mako html safe comes in </s> :return: stripped string	strip_html def strip_html(unclean): :rtype: str return bleach.clean(unclean, strip=True, tags=[], attributes=[], styles=[])
# todo: remove this ``expectedfailure`` </s> pass	test_unsuccessful_read_transaction data1 = list(Test.objects.all()) raise ZeroDivisionError self.assertListEqual(data1, []) with self.assertNumQueries(1):
# update new uniqueid kodi 17 - todo get uniqueid_id for updates from embydb </s> if self.should_stop():	compare_all views += self.emby_db.getView_byType('mixed') log.info("Media folders: %s", views) return False if not self.compare_movies(view):
# todo: look at the model to see which revision is last. </s> res = self.app.get(offset)	test_purge offset = url_for(controller='revision', action='purge', id=None)
#todo check that fn is callable </s> parameters	jacobi ---------- A : {csr_matrix, bsr_matrix}
# todo-blocker (rtibbles): hook into unit settings/front end parameterization to replace '8'. </s> def get_exercise_logs(request):	get_exercise_logs @allow_api_profiling Given a list of exercise_ids, retrieve a list of video logs for this user. data = simplejson.loads(request.raw_post_data or "[]")
# todo make this a private api </s> address = result.get('address', {})	GeocodeFarm places = [] for result in results.get('RESULTS'): latitude = coordinates.get('latitude', None) longitude = coordinates.get('longitude', None)
# todo: adjust the childrenrect for axis, labels and legends </s> def setyrlabels(self, labels):	setYRlabels
# todo-me return object </s> pass	delete_playlist print("...Deleted {playlist.title} for '{user}'." .format(playlist=playlist, user=user))
raise notimplementederror  # todo </s> def generator(*args, **kwargs):	Perturbation raise NotImplementedError  # TODO def perturbate(self): raise NotImplementedError  # TODO def compute(self, X, batch_size=32, verbose=0):
# @todo: move this field from this core table </s> current.response.s3.crud_strings[tablename] = storage(	S3PersonEducationModel s3_comments(), *s3_meta_fields()) title_create = ADD_IDENTITY, title_display = T("Education Details"),
pass # todo </s> def collect_incoming_data(self, data):	collect_incoming_data
# @todo: remove these when we're certain that s3_get_foreign_key finds these properly </s> @param as_string: represent each id as string	_get_all_items Get a list of the record IDs of all import items for the the given upload ID item_table = S3ImportJob.define_item_table() upload_table = self.upload_table
# todo: exc_info. </s> def rewind(self):	rewind
# todo: find a better random value </s> def save(self, item):	AccessPoint "on %s relation" % lazy_prop.relation) return lambda: (list(remote.search(conditions)),) This method has to be overriden. raise NotImplementedError("Abstract method")
#todo change to native framework call, when plex allows token in header </s> user = none	playlistsV3 @classmethod def LIST(self, req, *args): if args != None: if len(args) > 0:
# todo verify results </s> yield self.render_resource(self.rsrc, b"/")	DisabledV3RootResource self.rsrc.reconfigResource(self.master.config) @defer.inlineCallbacks self.assertRequest( content=unicode2bytes(
# todo this is a bit jankey to be honest </s> apps.clear_cache()	PluginConfig if apps_changed: apps.app_configs = OrderedDict() apps.populate(settings.INSTALLED_APPS)
# todo implement for all channels </s> payload = {'togglex':{'channel':channel, 'onoff':1}}	turn_on_channel return self._execute_cmd("SET", "Appliance.Control.ToggleX", payload)
# todo fix this </s> exam_root = etree.fromstring(patient_xml)	get_case_id This is the case_id if it's extracted, assumed to be in the PatientID However, there's a nonzero chance of them either forgetting to scan it case_id = exam_root.find("PatientID").text if case_id == '(_No_ID_)':
# todo: refactor this method. </s> @return: x	__isub__ def __isub__(self, y): (Optional) Convenience operator to unregister y from x @rtype Component or Manager if y.manager is not y:
# todo(crcrpar): support botorch v0.4.0. </s> "tqdm",	get_install_requires "packaging>=20.0", "scipy!=1.4.0", ] if sys.version_info[:2] > (3, 8):
# todo: show message. </s> archive.extractall(target_path)	_extractArchive Logger.log("d", "Removing current data in location: %s", target_path) shutil.rmtree(target_path) return True
# todo: send email here? </s> return response(status=service.http_status_from_exception(e))	WorksheetContentApi worksheet = service.worksheet(uuid) return Response(worksheet)
sage: k.pari_nf() # not tested # todo: pari-2.13.0 </s> for row in range(r):	minkowski_embedding sqrt2 = f.roots(R)[1][0] d = {} d[(row,col)] = places[row](B[col]) for i in range(s):
# todo: must be implemented </s> pass	should_fetch_kindlegen
# todo: do we need to skip config.add_slack variable here? </s> for v_model, v_setpoint in zip(model_vars, setpoint_vars))	generate_norm1_norm_constraint expr=v_model - v_setpoint.value <= norm_constraint_blk.L1_slack_var[idx]) rhs = config.fp_norm_constraint_coef * \ norm_constraint_blk.sum_slack = Constraint( expr=sum(norm_constraint_blk.L1_slack_var[idx] for idx in norm_constraint_blk.L1_slack_idx) <= rhs)
@jtu.skip_on_devices("tpu")  # todo(mattjj, pfau): fails on tpu. </s> {"testcase_name":	testSlogdet "_shape={}".format(jtu.format_shape_dtype_string(shape, dtype)), "shape": shape, "dtype": dtype, "rng_factory": rng_factory}
pass # todo </s> def title(self, p_title):	title self.titleedit.edit_text = p_title
# todo: add a named tuple/dict version </s> ]	ControllerState ('mainY', c_float), ('cX', c_float), __repr__ = toString __hash__ = hashStruct
# todo: check / store delta. </s> rotation = mathutils.quaternion((rotation[1], rotation[2], rotation[3], rotation[0]))	decompose_transition translation = convert_swizzle_location(translation) rotation = convert_swizzle_rotation(rotation) return translation, rotation, scale
# todo also check for motion codec parameter support </s> def is_motion_detected(camera_id):	is_motion_detected
# todo: this check may hide a bug a should be removed. </s> elif default == "yes":	query_yes_no valid = {"y":True, "n":False} if default is None: prompt = " [Y/n] " elif default == "no":
# todo(ytknzw): add more specific assertion with the test case. </s> raise valueerror	fail_objective
# todo need copy? </s> x[kk, kxy] += eps	newton_update for kk in range(X.shape[0]): for kxy in [0, 1]: jac_plus = jac_uniform(MeshTri(X, cells)) X = X_orig.copy()
return {} # todo </s> log.info("share from '%s' has been rejected by the pool" % worker_name)	_on_submit if result == True: log.info("Share from '%s' has been accepted by the pool" % worker_name) request.write(self.json_response(msg_id, result)) request.finish()
# todo: fixme-  assumes only one topic (next two lines) </s> self.application_id = _application_id	StreamTask class StreamTask: def __init__(self, _task_id, _application_id, _partitions, _topology_builder, _consumer, _producer): self.partitions = _partitions self.topology = _topology_builder.build()
# todo: test event is in response </s> 'end': datetime.datetime(2008, 1, 5, 9, 0, tzinfo=pytz.utc),	TestViews data = { 'title': 'Recent Event', 'end_recurring_period': datetime.datetime(2008, 5, 5, 0, 0, tzinfo=pytz.utc), 'rule': self.rule,
# todo(philday): add support for timeout (clean shutdown) </s> def host_power_action(self, host, action):	host_power_action
'''todo: add docs''' </s> return [x for x in self.columns if x['type'] == 'discretecolumn']	discrete_columns @property
# todo set only zero order </s> self.n_components = nm.prod(self.shape)	_setup_shape What is shape used for and what it really means. Does it represent shape of the problem? self.val_shape = self.shape
"deaths": "",  # todo: fix </s> input_format = "%d%m%y"	PtBrDateField2
#todo: check for continous or discrete, only continuous supported right now </s> raise valueerror, "oops, the system is unstable!"	gram@252 D,V = np.linalg.eig(sys.A) for e in D: if type=='c': print "controllable"
# todo: not actually sure this can ever happen. </s> self.flush_account()	ftp_REIN
# todo: django 2.0: remove </s> characters (ascii letters, digits, -, _). to be used in url.	nick_validator def nick_validator(value: str) -> None: if not 2 <= len(value) <= 60: raise ValidationError('The nick must be between 2 and 60 characters long.')
#        todo: need more info about log in procedure in game </s> page_size = 13	searchAuctions league=None, club=None, position=None, nationality=None, playStyle=None, start=0, page_size=16): elif page_size > 50:  # server restriction page_size = 50
# todo: this is repeated from flowdir </s> 1 + shape[1],	_flatten_fdir 0 - shape[1], 1 - shape[1], 0 + shape[1], -1 + shape[1],
1  # todo: fill in identifier </s> total  - total wall time to run the function call (including subcalls)	print_stats formated_stack.append(formated_line) print('\n'.join(formated_stack)) cumm   - total wall time for the function itself (removing subcalls) single - time spent on a _single_ execution (average time, really)
# todo: implement the shit herein instead of collectionrepo </s> this transformation of a filename to a handle's key may change.	_filename2key return self.name + '/' + branch + '/' + fname
# todo: test this </s> self.lnworker.save_channel(chan)	_store_raw_msg_if_local_update chan = self.lnworker.channels[channel_id]  # type: Channel chan.hm.store_local_update_raw_msg(raw_msg, is_commitment_signed=is_commitment_signed)
# todo(yamahata): creating volume simultaneously </s> :param volume_id: volume id	remove_volume def remove_volume(self, context, volume_id): self.volume_manager.remove_compute_volume(context, volume_id)
# todo: for now, circumnavigate the detached head issue. </s> raise skiptest("todo")	test_publish_default_target
# todo: the orderer node url needs to be fixed. </s> "core_peer_localmspid": msp_id,	ChannelViewSet CELLO_HOME, org_name) peer_node = Node.objects.get(id=peers[0]) "CORE_PEER_TLS_ROOTCERT_FILE": "{}/{}/peers/{}/tls/ca.crt".format( dir_node, org_name, peer_node.name + "." + org_name),
# todo: fix clone issue </s> assert (pred_proba.min() >= 0)	test_prediction_proba_linear pred_proba = self.clf.predict_proba(self.X_test, method='linear')
#todo_ismeal_quesataion: i prefer get_* for getters </s> def set_api_token(self, token):	set_api_token
# todo: create xxx_failure test </s> self.asserttrue(check('test_space', failure_tests),	test_result_space_failure r = run_set(p, 'result') failure_tests = r['failure'] lookup('test_space', tests))
# ^ todo: uncomment once federated ursula status pages work and remove skip (below) </s> flask_server=server,	test_render_ursula_status_page_with_known_nodes@32 server = Flask("ursula-status") status_page = UrsulaStatusPage(ursula=ursula, route_url='/') dash_duo.start_server(status_page.dash_app)
# todo: given trial, a repo, and an observer </s> self.events: list[trial] = []	SomeObserver class SomeObserver(_Observer[Tuple[HyperparamsRepository, Trial]]): def on_next(self, value: Tuple[HyperparamsRepository, Trial]): self.events.append(value)
# todo: refactor linodeexception, args[0] should be error_id </s> 'port': item['port'], 'weight': item['weight']}	_to_record def _to_record(self, item, zone=None): Build a Record object from the item dictionary. type = self._string_to_record_type(item['TYPE']) record = Record(id=item['RESOURCEID'], name=item['NAME'], type=type,
# todo: add safety tests so that when something fails it fails with a good error </s> for trans in additional_transformations:	transformation trans()
# todo: configurable timeout </s> def slow(self, container_names, state):	slow
# todo: deal with scroll position </s> x, y, w, h = group.generalsettings.getpossize()	setupSidebarGroup def setupSidebarGroup(self): group = Group((0, 0, 0, 0)) group.feaVarTabs = Tabs((0, h + 6, 0, 0), ["Features", "Variations", "Options"]) return group
# todo: finish </s> raise exception.notimplementederror	post_validate@66
# fixme: todo </s> posting.price is not none)	is_special pos = posting.position return (pos.lot.currency == CCY_FIFO and
# todo: deal with error </s> height = none	Glyph class Glyph(GLIFGlyph):
# todo: use the device specific template here </s> self.report_progress = report_progress	set_progress_reporter
# todo cid_map?? </s> checkins = get_checkins()	test_checkins@7 assert len(checkins) > 100 assert any('Victoria Park' in c.summary for c in checkins)
#todo: redo this with html parser instead of regex </s> if tag == 'title':	handle_starttag self.state = 'title' if tag == 'magnet' and self.state == 'matched':
# todo[yanndupis]: get rid of these torch references when extending hook_args </s> tensor.child = chain	detail tensor = sy.MultiPointerTensor(owner=worker, id=tensor_id) if chain is not None: return tensor
# todo do something with temp </s> for s in self.states_list:	resample_states s.resample(temp=temp)
# todo: extend to other types </s> class quantiletype(abstracttemplate):	QuantileType @infer_global(quantile) def generic(self, args, kws): assert not kws
# todo: experiment with when to apply conv </s> return f	pitch_bins_f bins = tf.tile(bins, [NUM_OCTAVES, 1, 1]) bins = tf.reshape(bins, [tf.shape(x)[0], time_steps, NUM_NOTES, 1])
# todo: re-enable for hardware </s> self.verify_all_stack_up()	test_tunnel_path_rerouted self.one_stack_port_down(self.port_map['port_3']) src_host, other_host, dst_host = self.net.hosts[:3]
# todo ... </s> if not is_valid_defname(arg):	cpreprocess_evaluate_ifdef def cpreprocess_evaluate_ifdef(state, arg): state.error("preprocessor: '" + arg + "' is not a valid macro name") return False
## todo: # fixme: remove me </s> paste_parent = self.r_serv_onion.hget('onion_metadata:{}'.format(self.domain), 'paste_parent')	get_last_crawled_pastes return self.get_all_pastes_domain(paste_parent)
# todo: check that the birth date is not in the future </s> year = 1900 + int(number[0:2])	_get_birth_date month = int(number[2:4]) % 50 % 20 day = int(number[4:6])
# todo make os portable for windows users </s> def putabsent(self, filename):	putabsent
# todo: distinguish between urllib and urllib2 contracts </s> raise notimplementederror	go_away
# todo: if the user supplied the full nonsymbolic image_shape and </s> gpu_x, gpu_y, *coords))]	local_gpu_incsubtensor set_instead_of_inc=node.op.set_instead_of_inc)(
# todo: fix signature of zip() in typeshed. </s> otherwise, return none.	find_partial_type_ref_fast_path def find_partial_type_ref_fast_path(self, expr: Expression) -> Optional[Type]: if not isinstance(expr, RefExpr): return None
#todo - handle this with a pipe? </s> self.assert_(str(alignment[0].seq).replace("-","") \	pairwise_alignment_check raise AssertionError("%s vs %s or %s" \ % (alignment[1].id , target.id, target.name)) in query_seq) self.assert_(str(alignment[1].seq).replace("-","").upper() \
# todo: why are we uncommenting html? </s> return value.encode('utf-8').decode('unicode_escape').replace('\\/', '/')	get_next_page value = match.groups()[0]
# todo: logs which are observably relevant should be sent to the client (e.g. the warning of refusing to have more receivers active) </s> pass	noteDirty def noteDirty(): with open(stateFile, 'w') as f:
# todo: enumerate the axes in a message </s> date_str = time.strftime('%a, %d %b %y %h:%m:%s gmt',	LmValidate "If Last-Modified is present, see if it will validate." def __init__(self, red): time.gmtime(red.parsed_hdrs['last-modified'])) req_hdrs = red.req_hdrs + [
# todo: check if page are exists </s> secret_file = '../credentials.secret'	TestConfluenceAuth class TestConfluenceAuth(unittest.TestCase): def test_confluence_auth(self): Keep the credentials private, the file is excluded. There is an example for credentials.secret See also: http://www.blacktechdiva.com/hide-api-keys/
# todo: optimize </s> logger.info('on_setup')	on_setup
# todo: remove this - cura-4482 </s> return self.getqualitydefinitionid(self._global_container_stack.getbottom())	activeQualityDefinitionId if self._global_container_stack:
# todo: progress +kwargs </s> return self.repo.active_branch.name	git_get_active_branch
# todo: ... </s> dhis2_api = dhis2api(settings.dhis2_host, settings.dhis2_username, settings.dhis2_password)	get_children_only_theirs Returns a list of child entities that don't have cchq_case_id set
# todo: (longer term) rather than abort, reject this candidate </s> )	make_install_req_from_dist global_options=parent.global_options, hashes=parent.hash_options ireq.satisfied_by = dist return ireq
# todo: dump content out for debugging in the future. </s> def get_local_driver():	get_local_driver
# todo: remove with neuralnet.__setstate_050__ </s> assert len(net_fit.history) == net_fit.max_epochs	test_history_correct_shape
# todo: verify that workid is the primary key someplace. </s> raise notimplementederror(qual(cls) + " does not implement " +	abstract @classmethod @wraps(thunk) thunk.func_name) return inner
parts.append(fmt[f:f_next])  # todo backslash-escapes, at least \n </s> if arg.o:	Shopt if b is None: raise NotImplementedError  # Display options exec_opts.SetOption(opt_name, b) else:
# todo: error detection </s> global mongodb	__connect mongodb = Connection('localhost', 27017)['cobbler']
pass # todo </s> all_tools = self.image.window.tools	_get_tool if tool_id in all_tools: return all_tools[tool_id]
# todo: uncomment this line once we're on django 1.11 </s> args = []	batch_delete_items if args is None and kwargs is None: raise Exception("You need to specify some filter options!") elif kwargs is None: kwargs = {}
# todo what should the swissnum _actually_ be? </s> def test_version(self):	test_version The client can return the version. version = yield self.client.get_version()
# todo: hide and stop spinner </s> print("is not an ap or is already connected")	Wireless else: if (not self.nmwidget.is_row_an_ap() or self.nmwidget.is_row_connected()): else: self.next_normal = False
# todo add code </s> p = poll.objects.create(question = "lo lo", pub_date = timezone.now())	test_creation p.save() self.assertEqual(Poll.objects.count(), 2) # Cause setup created one already
# todo debug </s> self.type = none	RuleElement class RuleElement: self.triggered = False self.timeWhenTriggered = 0.0
v_plus = self.probe_dtz_no_ep(board) # todo: change to probe_dtz </s> for i in range(1, 6):	filenames for j in range(i, 6): for k in range(j, 6): for j in range(i, 6): for k in range(j, 6):
# todo: typing for pb </s> addrs = [multiaddr(binascii.hexlify(addr)) for addr in peer_info_pb.addrs]	from_pb peer_id = PeerID(peer_info_pb.id)
# todo: set cookie </s> pass	api_view_settings del safe_settings['HTTP_Interface_IP'] del safe_settings['HTTP_Port'] self.my_sender('application/json', bytes(json.dumps(safe_settings), 'utf-8'))
# todo: start here </s> shutil.rmtree(self._local_tmp_dir)	_cleanup_local_tmp if self._local_tmp_dir: log.info('Removing temp directory %s...' % self._local_tmp_dir) except OSError as e: log.exception(e)
#todo test this </s> return self.rec(expr.child)	map_bitwise_not
# todo: use cli.output.write </s> data = data.replace('\r', '\n')	create_key_registry def _(event): data = event.data event.current_buffer.insert_text(data) if data[-1] == "\n":
assert study_id == 0  # todo </s> assert study_id == 0  # todo	InMemoryStorage assert study_id == 0  # TODO return copy.deepcopy(self.trials[trial_id]) return copy.deepcopy(self.trials)
# todo: lacp timeout configurable. </s> vlan = self.dp.vlans[vlan_vid]	parse_rcv_packet PacketMeta instance. eth_src = eth_pkt.src port = self.dp.ports[in_port] return valve_packet.PacketMeta(
# todo: we probably don't need this? just to be safe </s> norm_groups = []	FP16_OptimizerMMTLModified def step(self, closure=None): Not supporting closure. skip = False for i, group in enumerate(self.fp16_groups):
# todo: action value doesn't exist for beta </s> reward_estimation = dict(horizon=10, discount=0.99, estimate_horizon=false)	test_no_horizon_estimate self.unittest(reward_estimation=reward_estimation)
# todo: distinguish between text elements with actual whitespace </s> 'id': obj.get('id'),	event_to_activity an ActivityStreams activity dict obj = self.event_to_object(event, rsvps=rsvps) 'url': obj.get('url'),
# todo: change the frontend to pass seconds instead. </s> return {'sub': 'email', 'email': email, 'exp': id_token_expiration_timestamp}	userinfo_mock
# :todo: raises systemerror on python 2.6, </s> self.assertequal(list(out), [50, 53, 56, 184, 188, 192])	test_undo_filter out = reader.undo_filter(3, cp(scanline), cp(scanprev)) self.assertEqual(list(out), [40, 42, 45, 99, 103, 108])
# todo: remove this once uses have migrated to that new interface. </s> explicit_project_id = arg_project or os.environ.get(environment_vars.project)	monkeypatch_bq specified_credentials = kwargs.get('credentials') has_bigquery = get_integrations().has_bigquery() if explicit_project_id: kwargs['project'] = explicit_project_id
# todo: this is untested. </s> error_number = _lib.x509_store_ctx_get_error(store_ctx)	_VerifyHelper def wrapper(ok, store_ctx): cert = X509.__new__(X509) error_depth = _lib.X509_STORE_CTX_get_error_depth(store_ctx) try:
# todo: not clear that this is even used. </s> name,	EngagementEdge super().__init__("grapl:EngagementEdge", name, None, opts) self.role = LambdaExecutionRole(name, opts=pulumi.ResourceOptions(parent=self)) args=PythonLambdaArgs( handler="lambdex_handler.handler",
# todo: remove </s> for key in params.keys():	Synchroniser "server_deck_read_only": None, "server_allows_media_upload": None} def set_partner_params(self, partner_params): value = params.get(key) if value.lower() == "true":
# todo(termie): optimize this call at some point and put it into the </s> return {}	noop
# todo use deepcopy() here </s> angles.add(int(angle * 1000))	_is_polygon_line angle = ia.angle_between_vectors(vec_down, vec)
# todo color </s> for command_name in all_commands:	split_command_args :param command: redis command string, with args :param all_commands: full redis commands list if re.match(r"\s*{}( .*)?$".format(command_name), upper_raw_command): l = len(command_name)
# todo: use google-diff-patch-match library to diff the sources? </s> "compare source and outputs of cells x,y exactly."	compare_cell_source_and_outputs if x["cell_type"] != y["cell_type"]: return False
direct = false  # todo: test on undirect, but too long atm </s> ok_(exists('1'))	d1_basic_checks ok_file_under_git('1', '1 f.txt', annexed=True) ok_file_under_git(opj('1', 'd', '1d'), annexed=True)
# todo: do the computation without the 'sr' enforcement </s> defined by	riemann Return the Riemann curvature tensor associated with the metric. This method is actually a shortcut for ``self.connection().riemann()`` .. MATH:: R(\omega, u, v, w) = \left\langle \omega, \nabla_u \nabla_v w
# todo check </s> def set_metadata(self, metadata):	set_metadata pass
# todo: also use backward pass </s> embeddings = nvocab.embedding_matrix	get_selective_model@196 initializer=initializer) else: sentences_embedded = [tf.nn.embedding_lookup(embeddings, sentence) for sentence in sentences]
# todo: figure out a way to actually log this information without </s> if blen < f['hlen']:	decode_hybi f['length'] = b2 & 0x7f if f['length'] == 126: return f # Incomplete frame header (f['length'],) = unpack_from('>xxH', buf)
# todo: implement </s> real_path = path if relative_to is none else os.path.join(relative_to, path)	remove_file gio.File(real_path).delete()
# todo: add this back in once we've merged back the refactored users code </s> self.assertequals(commcare_users_count, 1)	testLinkOrphanCommCareUser couch_user_2 = CouchUser.get(couch_user_2.get_id) # refresh the couch user self.assertEquals(len(couch_user_2.commcare_accounts),0)
# todo: tighten foolscap schema to require exactly 32 bytes. </s> return [ s[i:i+interfaces.hash_size] for i in range(0, len(s), interfaces.hash_size) ]	str2l
# todo(harlowja): is there a better way to do this?? </s> fs['/d'] = 'd'	test_set_get_ls def test_set_get_ls(self): fs['/c'] = 'c' fs['/d/b'] = 'db'
# todo check validity </s> def get_id(self):	get_id
# todo find if genesis block can be non-zero. why does 'earliest' option even exist? </s> block = self._chain.get_block_by_hash(block_hash)	getBlockByHash def getBlockByHash(self, block_hash_hex, include_transactions): assert block.hash == block_hash block_dict = block_to_dict(block, self._chain, include_transactions)
# todo(tsileo): handle tombstone </s> return bleach.clean(html, tags=allowed_tags)	clean_html
# todo: duplicate checking </s> pass	VPPLicenseCursor
# todo: how to check it? meybe we can omit this test </s> f_result = ngt.make_transformer().computation(f_ng)()	test_constant importer = C2Importer() importer.parse_net_def(net.Proto(), verbose=False) assert(np.ma.allequal(f_result, workspace.FetchBlob("Y")) and f_result[0] == val)
# todo : pytest.mark.parametrise once nose is gone. </s> alt_dirs = paths.get_ipython_dir()	test_filefind def test_filefind(): t = path.filefind(f.name, alt_dirs)
# todo: do this when we want to switch off ctrl-c </s> module_id + '\n')	print_modules '        ' + str(cfg[module_id]['shutit.core.module.build']) + '    ' + return string
# todo add support for dynamically picking version of </s> else:	create_partitions timeout = timeout_ms, validate_only = validate_only raise NotImplementedError( "Support for CreatePartitions v{} has not yet been added to KafkaAdmin."
# todo: create a lint plugin for that to enforce using the helper </s> case=case, sortkey=i * 10)	setUpTestData TestCaseRunFactory(assignee=cls.tester, tested_by=cls.tester, run=cls.test_run_1, build=cls.build, for i, case in enumerate((cls.case_4, cls.case_5, cls.case_6), 1)]
# todo implement. </s> def _train(self, rdd, nb_epoch, batch_size, verbose, validation_split):	SparkModel self._train(self.dataset_rdd, nb_epoch, batch_size, verbose, validation_split)
last = 50 if 'last' not in parameters else int(parameters['last'][0])  # todo check integer! </s> channel_json = tribler_utils.tribler_data.channels[index].get_json()	SearchEndpoint torrents_json.sort(key=lambda result: result[sort_by] if sort_by in result else None, reverse=not sort_asc) channels_json = [] channel_json['type'] = 'channel' channels_json.append(channel_json)
# todo remove get_media_references </s> with the correct get parameters.	back_to_main @login_and_domain_required def back_to_main(req, domain, app_id=None, module_id=None, form_id=None, unique_form_id=None, edit=True, error='', page=None, **kwargs): This is meant to be used by views that process a POST request, which then redirect to the main page. The idiom for calling back_to_main used in this file is
# todo: decompose=true </s> wp = pywt.waveletpacketnd(data=x, wavelet='db1', mode='symmetric')	test_accessing_node_atributes_nd def test_accessing_node_atributes_nd(): assert_allclose(wp['aa'+'ad'].data, np.zeros((2, 2)) - 4, rtol=1e-12) assert_(wp['aa'+'ad'].path == 'aa'+'ad')
# todo: move to base class </s> if activegraphonly:	getNodesRect rectangles = [] if selected: if n._rawNode.graph() != self.graphManager.activeGraph(): continue
# todo: no idea of how to check correct logging via any kind of assertion yet. </s> cmd = 'echo testing real run > %s' % tempfile	test_runner @with_tempfile def test_runner(tempfile): ret = runner.run(cmd) assert_equal(0, ret, "Run of: %s resulted in exitcode %s" % (cmd, ret))
# todo verify layer exists in geoserver? </s> return self.__class__.class_protocol()	get_protocol
# todo: this needs test coverage. </s> return h5py.file(self._filename, mode)	h5_file
# todo(sbauza): remove the service_id filter in a later release </s> result = _ec2_instance_get_query(context).\	ec2_instance_get_by_id @require_context filter_by(id=instance_id).\ first()
# todo: it would be more helpful just to quietly recreate the data source config from get params </s> pass	_get_initial
# todo make this a private api </s> result.	GeocodeFarm longitude)``, or string as ``"%(latitude)s, %(longitude)s"``. :param bool exactly_one: Return one result or a list of results, if :param int timeout: Time, in seconds, to wait for the geocoding service to respond before raising a :class:`geopy.exc.GeocoderTimedOut`
# todo: assert </s> self.remote.modify_repo(repo, "name", "testrepo0", self.token)	createRepo @pytest.fixture def createRepo(self): self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token) self.remote.modify_repo(repo, "mirror_locally", "0", self.token)
# todo: cronjob (celery task) to delete stale tokens </s> request,	select_email user = authenticate(user=user) auth_login(request, user) "You have been successfully registered and logged in.") else:
raise valueerror("bucket might not exist")  # todo: create custom exception for easier handling </s> bucket.objects_enumerated = true	S3Service for item in page['Contents']: obj = s3BucketObject(key=item['Key'], last_modified=item['LastModified'], size=item['Size'])
# todo: remove this once all dependent code has been cleaned up </s> param_quantizer.reset_encoding_stats()	reset_encodings self.output_quantizers[0].reset_encoding_stats() self.output_quantizers[0].encoding = None param_quantizer.encoding = None
# todo: test, to be sure it doesn't mess things up </s> with open(template % i, "wb") as f:	_initialize_seeds l.debug("initializing seeds %r", self.seeds) template = os.path.join(self.in_dir, "seed-%d") f.write(seed)
# todo(phawkins): enable when there is an lu implementation for gpu/tpu. </s> compare_orthogonal(nq[..., :k], lq[..., :k])	testQr q1 *= phases self.assertTrue(onp.all(norm(q1 - q2) < 30)) self.assertTrue(onp.all(norm(onp.eye(k) - onp.matmul(T(lq), lq)) < 5)) if not full_matrices and m >= n:
# @todo: make a lookup table in diseasedatamodel: </s> t = current.t	ContactTracingModel "disease_contact_person", ) db = current.db crud_strings = current.response.s3.crud_strings
self.pos_emb = posencoding(max_seq_len * 10, d_model) # todo: *10 fix </s> subsequent_mask = torch.from_numpy(subsequent_mask).byte()	get_attn_subsequent_mask assert seq.dim() == 2 attn_shape = [seq.size(0), seq.size(1), seq.size(1)] if seq.is_cuda: subsequent_mask = subsequent_mask.cuda()
# todo docstring </s> :returns: a ``deferred`` which fires with a list of ``application``	discover_node_configuration def discover_node_configuration(self): instances. d = self._gear_client.list()
#invalidate any overlapping cached value #todo extract remaining valid bits </s> @param src: source operand.	CMOVP @param dest: destination operand.
# todo. check if build_url_fname can be used. </s> self.get_note_format(	dump_notes db = self.report.database ul = Html("ul") db.get_note_from_handle(notehandle)) for notehandle in notelist)
# todo: command+c for mac </s> try:	_get_version with open(join_path(dirname(__file__), "VERSION")) as fp: return StrictVersion(fp.read().strip())
# todo: connect to actual event in playlist </s> return variant('as', 'audio/musepack;application/musepack;application/x-ape;audio/ape;audio/x-ape;audio/x-musepack;application/x-musepack;audio/x-mp3;application/x-id3;audio/mpeg;audio/x-mpeg;audio/x-mpeg-3;audio/mpeg3;audio/mp3;audio/x-m4a;audio/mpc;audio/x-mpc;audio/mp;audio/x-mp;application/ogg;application/x-ogg;audio/vorbis;audio/x-vorbis;audio/ogg;audio/x-ogg;audio/x-flac;application/x-flac;audio/flac'.split(';'))	SupportedMimeTypes @property
# todo(lyarwood): remove the following in 16.0.0 pike </s> encryptors.get_volume_encryptor(self.connection_info, **encryption)	VolumeEncryptorInitTestCase def test_get_direct_encryptor_log(self, log): encryption = {'control_location': 'front-end', encryption = {'control_location': 'front-end', 'provider': 'nova.volume.encryptors.luks.LuksEncryptor'}
#todo - introduce an annotated alignment class? </s> return (name, start, end)	_identifier_split name = identifier.split("/",1)[0]
# todo: this is not the most efficient approach - refactor this functionality into util </s> pack_uid = resource_db.get_pack_uid()	APIUIDMixin resource_db = self.to_model(self)
raise notimplementederror  # todo </s> self.command._done()	communicate self.next_command = command def previous_command_finished(_): self.command, self.next_command = self.next_command, None if self.command is not None:
constant_liar: bool = false,  # todo(hvy): remove default value and fix unit tests. </s> weighted_above /= weighted_above.sum()	_sample_categorical_index weights_above = self._weights(len(above)) counts_above = np.bincount(above, minlength=upper, weights=weights_above) log_likelihoods_above = TPESampler._categorical_log_pdf(samples_below, weighted_above) return int(
#todo(cp16net): need to set the return code correctly </s> path = "/{tenant_id}/instances"	API self._instance_router(mapper) def _instance_router(self, mapper): mapper.resource("instance", path, controller=instance_resource)
# todo ... </s> if not is_valid_defname(arg):	cpreprocess_evaluate_ifdef def cpreprocess_evaluate_ifdef(state, arg): state.error("preprocessor: '" + arg + "' is not a valid macro name") return False
# xxx todo </s> 'colorreal':    "default",	Configuration 'colorparent':  "default", 'colorurl':     "default", 'colorbase':    "default", 'colorvalid':   "default",
# todo error #248 </s> auth=(settings.github_username, settings.github_password)	GitHubHandler username, repo_name = package.repo_name().split('/') r = requests.get( ) if r.status_code == 200:
#todo 接上 qa bot </s> self.domain_similarity,self.speech_domain,self.speech_matchee = res	rule_match Return: a boolean value, to indicate that this match makes sense or not. res,self.last_path = self.console.rule_match(speech, best_only=True) self._set_root_domain() if self.domain_similarity < threshold:
#todo add types and return and rtype </s> self._set_raw()	generate_docs self._set_params()
# todo: use a bytearray? </s> break	iter_lines raise line += c else: yield line
# todo(termie): this stuff should probably be moved to middleware </s> return {}	noop
# todo: account for conversion if tz other than gmt not specified </s> if not ipfsjson:	getIPFSAPIPort ipfsJSON = readIPFSConfig() ipfsAPIPort = basename(ipfsJSON['Addresses']['API'])
# todo add binary column (drop support for python 2.7) </s> reset_index(df, index_label='__index__')	test_reset_index df = pd.DataFrame({'a': [1, 2, 3, 4, 5]})
#todo: add index to conc_call_agg </s> call_date = date_today + datetime.timedelta(seconds=delta_duration)	Command numbercall = 10 for i in range(0, int(no_of_record)): delta_call = random.randint(-2, 2) numbercall = numbercall + delta_call
# todo: remove getattr when https://github.com/rtfd/readthedocs.org/pull/3339 got merged </s> def venv_path(self):	venv_path
#todo show this </s> clone = self.clone()	after_fork :param User user: :param bool save: clone.owner = fork if save:
hda.dataset.job_id = job.id  # todo: can't add attr to dataset in __init__(). why? </s> yield executionslice(job_index, param_combination, dataset_collection_elements)	ToolExecutionTracker if not completed_job: for dataset_collection_element in dataset_collection_elements.values():
#todo: issue warning that this is an unsafe operation, but doing it cause user insists </s> context.append(none)	selinux_initial_context if self.selinux_mls_enabled():
# todo: write me </s> self.bucket.delete_key(key_name+'-lock')	unlock def unlock(self, layer, coord, format):
# todo(kan-bayashi): documentation and type hint </s> **spline_kwargs	piecewise_rational_quadratic_transform@18 min_bin_width=min_bin_width, min_bin_height=min_bin_height, ) return outputs, logabsdet
# todo: implement me! </s> return render_to_response('accounts/username_reminder.html', dict(form=form, sent=true), context_instance=requestcontext(request))	username_reminder if form.is_valid(): user = form.cleaned_data["user"] else: form = UsernameReminderForm()
# todo: should this raise ioerror? </s> data = bytearray()	parse_hexdump for line in hexdump.splitlines(): data += bytearray.fromhex(line.split('#')[0])
status = offline # todo: all the cases </s> def query_room(self, room):	query_room
# todo: remove after py2.5 deprecation </s> self.assertequal('gi|16080617|ref|np_391444.1|', hsp.query_id)	test_tab_2226_tblastn_001 self.assertEqual(1, len(hit)) hsp = hit[0] self.assertEqual(34.88, hsp.ident_pct) self.assertEqual(43, hsp.ali_len)
# todo(guillermooo): further restrict valid register names. </s> raise valueerror('invalid register key: %s' % key)	MacroRegisters super().__init__(*args, **kwargs) def __setitem__(self, key, value): super().__setitem__(key.lower(), value) def __getitem__(self, key):
# todo(b/114938612): eventually remove this override. </s> with beam.pipeline() as p:	TaxiUtilsTest legacy_metadata = dataset_metadata.DatasetMetadata( dataset_schema.from_feature_spec(feature_spec)) with tft_beam.Context(temp_dir=os.path.join(working_dir, 'tmp')): examples = (
# todo: once/if we have gpu and language labels then we might be </s> typer.echo(f"{img:<44}: {stdout}")	echo_extensive_versions img, entrypoint=["printenv", "ORCHEST_VERSION"] )
# todo: use invalidation time </s> the caller is responsible for ensuring from_id is a unique and correct id	dump_forward Params: MessageFwdHeader Telethon object Returns: ID of inserted row"""
assert study_id == 0  # todo(akiba) </s> assert study_id == 0	set_study_param_distribution def set_study_param_distribution(self, study_id, param_name, distribution):
# todo: move this test elsewhere. </s> selenium.get(self.live_server_url + reverse('member-login'))	MemberFrontTests TODO: This is definitely way too slow. Fasten this. selenium = self.selenium username = find_element('.content-container input#id_username') password = find_element('.content-container input#id_password')
# todo: replace with copy function </s> :type: int	n_landmark_groups def n_landmark_groups(self): r""" return self.landmarks.n_groups
raise  # todo </s> registrar_file.write(json.dumps(registrar_data))	_write_registrar_file registrar_file.seek(0)
#todo fixme: we need to check that we aren't adding a duplicate </s> raise valueerror  # or something.	main@52 continue claims.append(arg) real_claims = list() c = 0
output = []  # todo should this be user defined? </s> for name in self.cls.fields.keys():	_remove_fields def _remove_fields(self): fields as attrs from the new cls being generated if getattr(self.cls, name, None): delattr(self.cls, name)
# todo: this is a jump. </s> vi_cmd_data['motion']['args'] = {'mode': vi_cmd_data['mode']}	vi_big_m def vi_big_m(vi_cmd_data): vi_cmd_data['is_jump'] = True return vi_cmd_data
#todo add movement callbacks to target if movable </s> print target_ref(), 'has refs:'	check_target_ref print "Z's dead baby, Z's dead" return gc.collect() gc.collect()
# xxx todo </s> ('notificationhookproc', c_void_p),	GdiplusStartupOutput class GdiplusStartupOutput(Structure): ('NotificationUnhookProc', c_void_p)
# todo add something like this in the future, its cleaner than the </s> yield next(star_module.get_filters(search_global))	iter_star_filters def iter_star_filters(self, search_global=False):
# todo pass this to json shcema validation </s> @api.response(501, description='movie identifier not allowed', model=default_error_schema)	MovieListMovieAPI return {} @api.validate(model=input_movie_list_id_schema, description=movie_identifiers_doc) @api.doc(description='Sent movie identifiers will override any existing identifiers that the movie currently holds') def put(self, list_id, movie_id, session=None):
# todo deal with pylint error in cleaner fashion than this </s> move_win(uploaded_file.name, newpath)	move_to_temp_dir_and_restore_filename newdirpath = tempfile.mkdtemp() newpath = os.path.join(newdirpath, origfilename) else: shutil.move(uploaded_file.name, newpath)
# todo(b/134950354): test embedding column for non-eager mode only for now. </s> tensor_spec.tensorspec([5], tf.float32)	test_error_raised_if_missing_preprocessing_layer encoding_network.EncodingNetwork( input_tensor_spec=[ ], preprocessing_layers=[
# todo(b/130724878): these conversions should not be needed. </s> model: a modelweights structure, containing tensors or variables.	Obj @attr.s(cmp=False, frozen=False) class Obj(object): optimizer_state: A list of Tensors or Variables, in the order returned by optimizer.variables().
# todo add the ability to `git reset --hard` the dataset tree on failure </s> log_stderr=false,	__call__@151 cmd, log_online=True, expect_stderr=True, expect_fail=True,
# todo: allow answer to repeat previous or go back after a mistake </s> u"languages are uncapitalized."	wiktionaryMismatch page.site.nocapitalize): pywikibot.output( % (page, self.originPage)) return True
# todo: @sbharadwajj implement and test </s> raise notimplementederror	Conv1DDerivatives def _weight_jac_mat_prod(self, module, g_inp, g_out, mat): raise NotImplementedError
# todo: move this function to user.query </s> hashed_password = password	_set_password if isinstance(password, unicode): password_8bit = password.encode('UTF-8')
'path': upload.filename,  # todo path? </s> @decorators.must_have_addon('figshare', 'node')	figshare_get_project def figshare_get_project(*args, **kwargs): node, figshare = figshare_get_context(**kwargs)
reference_doctype='todo' and reference_name='{0}'""".format(todo.name)) </s> "content": "test communication",	TestFeedbackTrigger "reference_doctype": "ToDo", "reference_name": todo.name, "subject": "Test Communication", "doctype": "Communication"
# todo: check syntax </s> self.setmessage('header-location', rs.redirect_without_location)	status303 def status303(self):        # See Other
# todo(vek): need to pass context in for access to auth_token </s> assert state in power_state.valid_states(), "bad state: %s" % state	InstanceInfo self.name = name
# todo test errors </s> c = [1, 2, 3]	test_append_1d eq(e.dtype, z.dtype) eq((10,), z.chunks) f = np.append(e, c) z.append(c)
#todo: add method </s> logger.debug("[traktplayer] onplaybackstarted() - playing a non-library 'episode' - %s - s%02de%02d - %s." % (data['showtitle'], data['season'], data['episode'], data['title']))	onPlayBackStarted data['title'] = xbmc.getInfoLabel('VideoPlayer.Title') if year.isdigit(): elif year and not season and not showtitle: self.type = 'movie'
# todo time complecity </s> break	split_command_args l = len(command_name) input_command = command[:l] else: raise InvalidArguments(r"`{command} is not a valide Redis Command")
# todo(john sirois): https://github.com/pantsbuild/pex/issues/1059 </s> return url	_calculate_package_index_args url_info = urlparse.urlparse(url) if "http" == url_info.scheme: if indexes is not None: if len(indexes) == 0:
# todo remove? </s> class alreadyevaluated(frozenset):	AlreadyEvaluated
# todo fix. </s> cmp_result = self.__compare_contexts(ctx_init, x86_ctx_out, reil_ctx_out)	test_cmova asm = ["cmova eax, ebx"] ctx_init = self.__init_context() if not cmp_result: self.__save_failing_context(ctx_init)
# todo: test </s> not exist	get_ifmod def get_ifmod(self, aug_conf_path, mod): ifMods = self.aug.match(aug_conf_path + "/IfModule/*[self::arg='" + mod + "']") if len(ifMods) == 0:
# todo(zhengzhenyu): handle this when the api supports creating </s> result = _destroy_in_db(context, orphan_buildreq.id)	delete_build_requests_with_no_instance_uuid _get_build_requests_with_no_instance_uuid(context, count)) done = 0 if result: done += 1
# todo: may test with codecs.open passing an encoding </s> self.files_to_delete.append(temp.name)	PluginCsvTestCase self.assert_expected_table(table) def test_export_to_csv_filename(self): rows.export_to_csv(utils.table, temp.name) table = rows.import_from_csv(temp.name)
# todo map relationship backreferences using the django names </s> except keyerror:	get_class return self.Base.classes[DjangoModel._meta.db_table]
# todo: переделать механизм pairs </s> return sum(score(arr[i-1], arr[i]) for i in range(1, len(arr)))	string_score
# todo(andym): use mock appropriately here. </s> return urlparse(url).query.find('modified') > -1	is_url_hashed
#@todo: remove in 0.4.10 </s> except:	handleMultiPages try: m = re.search(self.PAGES_PATTERN, self.html) pages = 1 for p in xrange(2, pages + 1):
# todo(henry-nash): we should issue an exception here since if </s> return '%s/%s/%s' % (endpoint, 'v3', path.lstrip('/'))	base_url endpoint = super(V3Controller, cls).base_url(context, 'public') if not path:
# todo: unit test for this check </s> return self.x + self.width	x2 @property
# todo(jaypipes): remove once the pci tracker is always created </s> self.rt.compute_node.id = 42  # has to be a number, not a mock	set_cn_id
# todo use location </s> if self.next_token_ == "locked":	parse_def_anchor_ glyph_name = self.expect_name_() self.expect_keyword_("COMPONENT") locked = True self.advance_lexer_()
# todo: take namespace into account, currently doesn't matter since </s> nses['shared'].append(shared_ns.cereal())	top_level_namespaces for shared_ns in shared_nses:
# todo: make the min-max values a setting? </s> if track.album and track.album.name:	set_metadata taglist[gst.TAG_ARTIST] = ', '.join([a.name for a in artists]) if track.name: taglist[gst.TAG_ALBUM] = track.album.name event = gst.event_new_tag(taglist)
# todo: remove in 21.08 </s> with open(pho_file, "r") as cachefile:	load_phonemes@454 key + ".pho") if os.path.exists(pho_file): phonemes = cachefile.read().strip() return phonemes
# todo(mattjj,phawkins): improve this implementation </s> return tuple(vjp(ct, ans, *primals) if vjp else ad_util.zeros_like_jaxval(x)	vjpfun def vjpfun(ct):
# todo -- make sure more stringent and parse each kext in-memory so we only allow whitelist from .text </s> ret = true	is_64bit_capable x86_64_flag = obj.Object("int", offset = x86_64_flag_addr, vm = addr_space) ret = x86_64_flag == 1 return ret
# todo remove </s> class alreadyevaluated(frozenset):	AlreadyEvaluated
# todo(termie): optimize this call at some point and put it into the </s> return {}	noop
# todo separate msg </s> print("connect successful!")	xpub_xsub_proxy backend_subs.bind(self.url2) print("Try: Proxy... CONNECT!")
# todo(himkt): remove `nltk` after solving </s> "numpy",	get_install_requires "cliff", "cmaes>=0.8.2", "packaging>=20.0", "scipy!=1.4.0",
# todo(kan-bayashi): documentation and type hint </s> min_derivative=default_min_derivative,	piecewise_rational_quadratic_transform@18 tail_bound=1.0, min_bin_width=DEFAULT_MIN_BIN_WIDTH, ): if tails is None:
# todo federate replies also when that is available in federation layer </s> content.refresh_from_db()	render_content try: content.render()
# todo: probably better to work out why they are occurring, but imo the </s> )	test_compatible dist, version_info=(3, 6, 5), assert not len(caplog.records)
# todo test </s> return hash((frozenset(self.nodes), self.current_state.tostring(),	__hash__ self.past_state.tostring(), self.network))
# todo: calculate mu-sigma for f1, accuracy, and roc_auc and make it selectable </s> if self.num_classes == 2:	load_data_from_objects self.num_classes = len(np.unique(self.trainY)) assert self.num_classes > 1 self.judgment_metric = 'f1_scores' else:
# todo(solitude): when the migration of data is completed, we </s> form.save()	payments_upsell extra={'addon': addon, 'amo_user': request.amo_user}) return redirect('submit.app.payments.paypal', addon.app_slug) return jingo.render(request, 'submit/payments-upsell.html', {
# todo: this should take a vector </s> def me(self):	Me Edge inner product matrix if getattr(self, '_Me', None) is None:
# todo(john-wood-w) allow until plugin validation is added. </s> excep.invalidobject,	test_should_raise_with_mixed_case_wrong_payload_content_type def test_should_raise_with_mixed_case_wrong_payload_content_type(self): self.secret_req['payload_content_type'] = 'TeXT/PlaneS' self.validator.validate, self.secret_req,
# todo col_type.python_type contains the type that </s> return self.sa_mapper.identity_key_from_instance(self)	sa_identity_key @property
# todo: more unittests </s> return unittest.makesuite(dochelperstests)	suite
# todo: check concurrent streams count and maybe wait </s> self.method = method	CallDescriptor class CallDescriptor: _, _, self.method_name = method.name.split('/') def __bind__(self, channel, method):
pass # todo </s> self.input_buffer.append(data)	collect_incoming_data
# todo: this check is to maintain backwards compatibility with the old way of creating </s> def _form_to_db_schema(self, package_type=none):	_form_to_db_schema
# todo: handle url == none </s> def keypress(self, size, key):	SelectableText class SelectableText(urwid.Text): def selectable(self): return key
weights[:,:-3] /= (bm * 3.0) # todo: use exact number of channels. </s> if j < 2 and i > 1: continue	Model net['main']    = net['conv5_4'] net['map'] = InputLayer((1, 3, None, None)) # TODO: This should not always be 3, could be 4 or 1. suffix = '%i_%i' % (j+1, i+1) if i == 0:
# todo: fix broken buildpack / example app </s> self._test_example('example-clojure-ring')	test_clojure_ring
# todo: currently we are assuming that 'on error resume next' is being </s> if (self.test_range):	Case_Clause_Atomic if (tokens[0] == "Else"): self.case_val = ["Else"] self.case_val = [tokens.lbound, tokens.ubound] else:
# todo: (step 11) gcs directory where kfp outputs are recorded </s> config=runner_config, pod_labels_to_attach=pod_labels	run@74 pod_labels = kubeflow_dag_runner.get_default_pod_labels() pod_labels.update({telemetry_utils.LABEL_KFP_SDK_ENV: 'tfx-template'}) ).run( pipeline.create_pipeline(
# todo: make these more configurable </s> send an http request to the rapidsms router, via the ajax app (which	request@49 must be running for this to work), and return a tuple containing the returned HTTP status, content-type, and body.
# todo: namelist, explist </s> self.assertisnotnone(node)	testFieldExp def testFieldExp(self): p = get_parser('foo') self.assertEqual(1, p._pos) self.assertTrue(isinstance(node, parser.FieldExp))
# todo care for mro stuff (multiple super classes) </s> return the builtin scope as parent, because the arrays are builtins	parent def parent(self):
return false  # todo: 2.0 return none </s> validation.check_boolean(mute)	MixerController def set_mute(self, mute): :class:`True` to mute, :class:`False` to unmute. if self._mixer is None: return False  # TODO: 2.0 return None
# todo: move install_time away from app_setting </s> domain	app_makedefault Redirect domain root to an app Keyword argument: from yunohost.domain import domain_list if not _is_installed(app):
# todo(okuta): check type </s> .. seealso:: :func:`numpy.std`	std size one. Returns: return a.std(axis=axis, dtype=dtype, out=out, ddof=ddof, keepdims=keepdims)
# todo: implement fs_type in dfvfs and remove this implementation </s> self.offset = 0	FileStatEvent size: The file size in bytes. fs_type: The filesystem this timestamp is extracted from. self.size = size self.allocated = allocated
# todo: hack </s> url = 'http:' + url	InputHtml title = str(title).strip() if not title: continue elif not url.startswith('http://') or not url.startswith('https://'): url = urlparse.urljoin(pageurl, url)
# todo: allow other formats? </s> instances for this page.	PodcastadminController :returns: podcasts podcasts = DBSession.query(Podcast)\ .options(orm.undefer('media_count'))\
#todo(chris): implement service_catalog </s> self.apikey = apikey	HTTPClient def __init__(self, user, apikey, projectid, auth_url, timeout=None): super(HTTPClient, self).__init__(timeout=timeout) self.projectid = projectid self.auth_url = auth_url
#todo: put the numpy.hstack() call in _load_and_unpack class to lazily load </s> else:	sr_loop if is_real: if unpack: LpqR = Lpq LpqI = numpy.zeros_like(LpqR)
self.setup() # todo: perhaps, remove this to pass path in context </s> return self[new_starting_step_index:], data_container	_load_pipeline_checkpoint step = self[new_starting_step_index] if isinstance(step, BaseCheckpointStep):
# todo: logs which are observably relevant should be sent to the client (e.g. the warning of refusing to have more receivers active) </s> with open(statefile, 'w') as f:	noteDirty json.dump(top.state_to_json(), f) pass
# todo: change logic to c_leq based on benchmarking </s> 'mindtpy unable to handle nlp subproblem termination '	handle_NLP_subproblem_other_termination add_int_cut(var_values, solve_data, config) else: 'condition of {}'.format(termination_condition))
# todo: checking the cause of the large deviation </s> struct.make_supercell([2, 2, 2])	generate_Si_cluster coords = [[0, 0, 0], [0.75, 0.5, 0.75]] lattice = Lattice.from_parameters(a=3.84, b=3.84, c=3.84, alpha=120, beta=90, gamma=60) mol = Molecule.from_sites(struct) XYZ(mol).write_file(os.path.join(test_dir, "Si_cluster.xyz"))
# todo: implement non-zero js </s> :param int tind: time index	ProblemTDEM_b solType = 'b' #: Type of the solution, in this case the 'b' field surveyPair = SurveyTDEM :rtype: scipy.sparse.csr_matrix :return: A
logo_url = ''  # todo: add logo url </s> 'fields': fields,	serialize_event_to_context@17 'title': event_context.subject_action, 'title_link': url, 'mrkdwn_in': ['text'], 'footer_icon': logo_url,
# todo: clean code </s> environment.	learner multiprocessing.Lock to lock other processes. It must be released after process is done. policy_fn: Method object to generate an explorer.
# todo fix. </s> ctx_init = self.__init_context()	test_cmova def test_cmova(self): x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) cmp_result = self.__compare_contexts(ctx_init, x86_ctx_out, reil_ctx_out)
# todo: refactor accordingly when v3 websocket api is released </s> def order_book_class(self) -> bittrexorderbook:	order_book_class @property
# todo if update_variable_bounds = false, this will not work as intended. </s> pass	FBBTException
# todo: add_messages_to_json? </s> j_dic['triggers'] = []	enrich_json_with_base j_dic['offset'] = 0 j_dic['entities'] = [] j_dic['modifications'] = [] j_dic['equivs'] = []
# todo: use pybossa uploader! only for debugging: </s> os.environ['pybossa_redis_cache_disabled'] = env_cache_disabled	warm_cache if env_cache_disabled is None: del os.environ['PYBOSSA_REDIS_CACHE_DISABLED'] return True
# todo: create xxx_failure test </s> self.asserttrue(check('test_space', failure_tests),	test_result_space_failure tests = exclude_from_resultlist(r, 'failure')
# todo: remove this method in v2.5 </s> )	remove_from_device partition=self.want.partition
# todo: get data init to work with tf_function compile #428 </s> np.random.random((10, 3, 4)),	WeightNormalizationTest loss='mse', experimental_run_tf_function=False) np.random.random((10, 3, 2)), epochs=3,
# todo: improve this. </s> elif mode == mode_visual:	ViBigL if s.b >= target: return sublime.Region(s.a + 1, target) if s.b >= target: new_target = utils.next_non_white_space_char(view, target)
@skipif('device-openmp')  # todo: still unsupported with openmp, but soon will be </s> assert len(trees) == 2	TestCodeGeneration eq = Eq(u.forward, u.dy.dy) op = Operator(eq) assert trees[0][0] is trees[1][0] assert trees[0][1] is not trees[1][1]
# todo: better to use an inotify method that doesn't conflict with eventlets. </s> self._handler_datapath_up(ryu_dp)	handler_connect_or_disconnect ryu_event (ryu.controller.event.EventReplyBase): DP reconnection. ryu_dp = ryu_event.dp else: self._handler_datapath_down(ryu_dp)
# todo: we also want to use pruned results </s> raise notimplementederror	BaseStorage raise NotImplementedError @abc.abstractmethod @abc.abstractmethod def get_all_trials(self, study_id):
raise  # todo </s> return self.parser.format_usage()	get_usage
# security todo - we _should_ be computing sha1(m), but we don't because that's the api. </s> if not self.has_private():	_DSAKey def has_private(self): return hasattr(self, 'x') raise error("No private key") if not (1L < k < self.q):
# todo: assert </s> self.remote.modify_repo(repo, "mirror_locally", "0", self.token)	createRepo self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token)
time.sleep(5)  # todo: for some reason, events do not trigger instantly </s> self.assertisnotnone(watermark)	test_pull_subscribe self.assertIsNotNone(subscription_id)
# todo: check to make sure time points match </s> inc = int(round(dt / sys_dt))	forced_response tout, yout, xout = sp.signal.dlsim(dsys, np.transpose(U), spT, X0) tout = tout + T[0] tout = T            # Return exact list of time steps yout = yout[::inc, :]
# todo: make it really async. </s> self.database_name = database_name	__init__@15 def __init__(self, resource_group_name, server_name, database_name): self.resource_group_name = resource_group_name
# todo: fill these in </s> pairs = [(b.name, b.index) for b in _builtins if b.kind == kind]	GenBuiltinLookup groups = collections.defaultdict(list) for name, index in pairs:
todo = atomlist # list of atoms we must still mark and explore (recurse on all unmarked neighbors) </s> env.history.message(cmd + msg)	ops_connected_Mixin cmd = greenmsg("Select Connected: ") if not self.selatoms: return if not atomlist:
# todo: allow setting a placeholder dom element, or any widget parent </s> from flexx import ui	Label class Label(Widget): Example: class App(ui.App): def init(self):
return user.address  # todo: update </s> setattr(field, key, value)	create_personal_data_fields for key, value in data.iteritems():
# todo: if the procpool has been exhausted this will block. </s> self._channel.close()	kill for c in self._consumers: c.cancel()
# todo: wrap backend call in error handling. </s> returns a :class:`mopidy.models.tltrack` or :class:`none`.	get_current_tl_track return self._current_tl_track
# todo: how to check it? meybe we can omit this test </s> f_result = ngt.make_transformer().computation(f_ng)()	test_constant importer = C2Importer() importer.parse_net_def(net.Proto(), verbose=False) assert(np.ma.allequal(f_result, workspace.FetchBlob("Y")) and f_result[0] == val)
# todo: need to cleanup the named argument mess before it is possible. </s> if star_dict_arg is not none:	CPythonExpressionFunctionCall self.setPositionalArguments( positional_args + tuple( constant_nodes ) ) self.setStarListArg( None ) if star_dict_arg.isExpressionMakeDict(): pass
# todo sk: select standby db if necessary </s> def db_for_read(self, model, **hints):	db_for_read
# todo?: self.assert_eq(kdf.loc['j':'q', 'b':'a'], pdf.loc['j':'q', 'b':'a']) </s> columns=list('abcde'))	test_loc2d_with_known_divisions def test_loc2d_with_known_divisions(self): pdf = pd.DataFrame(np.random.randn(20, 5), kdf = ks.from_pandas(pdf) self.assert_eq(kdf.loc[['a'], 'A'], pdf.loc[['a'], 'A'])
# todo ditto </s> if self.registered_apps.has_key(app_name.name):	_get_validation_errors if form_errors: validation_errors.extend(form_errors) apps_used = [] app = self.registered_apps[app_name.name] apps_used.append(app)
# todo(kevinbenton): this test should do something </s> pass	TestKeystoneAuth class TestKeystoneAuth(PecanFunctionalTest): def test_auth_enforced(self): response = self.app.get('/', expect_errors=True)
# todo remove hardcoded path </s> for token in doc:	add_el "The main character in Doug's novel is called Arthur Dent." doc = nlp(text) print("token", token.text, token.ent_type_, token.ent_kb_id_) print()
# todo multi-level import non-breakable </s> this function basically does what :meth:`scope.get_defined_names	_get_defined_names_for_position def _get_defined_names_for_position(scope, position=None, start_scope=None): <parsing_representation.Scope.get_defined_names>` does. - If `position` is given, delete all names defined after `position`.
# todo(ntamas): there are more possibilities; we could </s> else:	__isub__ self.delete_edges(other) elif isinstance(other, _igraph.EdgeSeq): return NotImplemented return self
@unittest.skip('not written')  # todo: finish! </s> def test_simplenamespace(self):	test_SimpleNamespace @unittest.skip('not written')  # TODO: finish! raise NotImplementedError
# todo: check error location </s> 'test': graphqlfield(test_type),	get_fields def get_fields(self): 'nest': GraphQLField(DataType(), resolver=lambda *_: Data())
# todo: refactor this to be more uniform across sources </s> settings["network"] = self.network	activate_source self.g_pool.plugins.add(NDSI_Source, args=settings)
# todo(wangqun):this method implement will be added after this </s> image_found = api_utils.get_openstack_resource(cli.glance().images,	validate_image def validate_image(cli, image): image, 'images') except (glance_exception.NotFound, exception.ResourceNotFound):
# todo: performance can be improved here by substituting expensive </s> return tf.nn.conv2d(x, flipkernel(k), name=name,	conv2d_flipkernel def conv2d_flipkernel(x, k, name=None):
# todo check if result is in scope -> no evaluation necessary </s> types = unite(	_name_to_types types += finder.find(filters, attribute_lookup=False) elif isinstance(node, tree.TryStmt): evaluator.execute(t, param.ValuesArguments([])) for t in exceptions
# todo: non-numeric columns should be ignored automatically </s> np.testing.assert_array_equal(hpat_func(df, n), test_impl(df, n))	test_iloc4 hpat_func = hpat.jit(test_impl) n = 11
@jtu.skip_on_devices("tpu")  # todo(phawkins): re-enable </s> self.asserttrue(onp.all(samples < hi))	testRngRandint self.assertTrue(onp.all(lo <= samples))
# todo: has some issues with datetime and sqlite </s> def test_sponsor_api(self):	test_sponsor_api
# todo here we could apply a "patch" to the native metadata, if desired </s> else:	get_dataset_identifier 'annex', 'uuid', default='') if not dsid: dsid = '_:{}'.format(ds.path.replace(os.sep, '_')) return dsid
# todo: if the input was compressed, compress the output? </s> each alignment block in an axt file contains three lines: a summary line and 2 sequence lines.	Axt Determines whether the file is in axt format axt alignment files are produced from Blastz, an alignment tool available from Webb Miller's lab Blocks are separated from one another by blank lines. The summary line contains chromosomal position and size information about the alignment. It
# todo pseudo code: </s> pass	Seeked @dbus.service.signal(dbus_interface=player_interface, signature='x')
# todo: extend to other types like datetime? </s> return signature(types.float64, *args)	QuantileType def generic(self, args, kws): assert not kws
#todo: check for continous or discrete, only continuous supported right now </s> ------	hsvd H = hsvd(sys) The Hankel singular values are the singular values of the Hankel operator.  In practice, we compute the square root of the eigenvalues of the matrix formed by taking the product of the observability and controllability gramians.  There are other (more efficient) methods based on solving the Lyapunov equation in a particular way (more details soon). sys : a state space system Outputs
# todo: candidate for move to system/hdparm </s> mnt_pt = mnt_prefix + sname	is_share_mounted def is_share_mounted(sname, mnt_prefix=DEFAULT_MNT_DIR):
# todo: how do i make the __iter__ thread safe? </s> for r in cursor:	__iter__@158 def __iter__(self): obj = cPickle.loads(r[0]) yield obj
# todo: lets turn skipifmock into an annotation </s> result = self.cb.get(self.key, getoptions(with_expiry=true))	test_get_with_expiry def test_get_with_expiry(self):
# todo: update consumer </s> dist = mgr.get_distributor(repo_id, distributor_id)	__distributor @return: The found model object. @raise MissingResource: when not found. if dist is None: raise MissingResource('/'.join((repo_id, distributor_id)))
# todo: use self.translate() </s> coherence of the fourier basis.	mu @property def mu(self): Is computed by :func:`compute_fourier_basis`. return self._check_fourier_properties('mu', 'Fourier basis coherence')
# todo(vek): need to pass context in for access to auth_token </s> assert state in power_state.valid_states(), "bad state: %s" % state	InstanceInfo class InstanceInfo(object): def __init__(self, name, state): self.state = state
# todo: when no longer supporting python 2.4 use finally: </s> self.end_col, self.strand_col, self.default_strand, \	GFFIntervalToBEDReaderWrapper them to BED format. def parse_row( self, line ): fix_strand=self.fix_strand ) interval = convert_gff_coords_to_bed( interval )
# todo: test jacobian </s> mod = tradedfactormodel(data.portfolios, data.factors)	test_linear_model_time_series_kernel_smoke def test_linear_model_time_series_kernel_smoke(data):
# todo: remove verify ssl config when working without it. </s> if not gomera_data:	fetch_island_data@26 return gran_canaria_data elif zone_key == 'ES-CN-IG': raise ParserException(zone_key, "Gomera not response") else:
# self.assertnotequal(end, 0) todo </s> self.asserttrue(greps(out, r"^names="))	test_2027_show_unit_for_oneshot_service logg.info(" %s =>%s\n%s", cmd, end, out) self.assertEqual(end, 0) self.assertTrue(greps(out, r"^Description=")) self.assertTrue(greps(out, r"^MainPID="))
# todo(harlowja): should we be a little more cautious about </s> self._logbook = self._catalog.create_or_fetch(self)	logbook if self._logbook is None:
time.sleep(1)  # delay, for last.fm latency. todo can this be removed later? </s> self.assertequal(title, "test title")	test_track_title_prop_caps def test_track_title_prop_caps(self): track = pylast.Track("test artist", "test title", self.network)
# todo parse out args instead of all endpoints </s> return endpoints	show_ignored for endpoint in sdnc.endpoints: if endpoint.ignore:
# todo: move to trim operator </s> active = bpy.context.scene.sequence_editor.active_strip	EditCrossfade self.report({'WARNING'}, "You need to be in the Video Sequence Editor to use this tool. \ Operation cancelled.") if active.type != "GAMMA_CROSS": effect = find_effect_strips(active)
# todo: support steps and times (motion blur) </s> is_viewport_render, use_instancing, transform)	ObjectCache2 mesh_key = self._get_mesh_key(obj, use_instancing) transform = None  # In viewport render, everything is instanced self.exported_meshes[mesh_key] = exported_mesh redefine_objs_with_these_mesh_keys.append(mesh_key)
# todo - can we support this? </s> self.s = seq(str(self.s), iupac.iupacunambiguousdna())	test_with_alt_alphabet self.test_simple()
pass  # todo </s> @option_poa	inspect @option_config_root @option_registry_infile @option_ignore_solidity_version def inspect(general_config, provider_uri, config_root, registry_infile, deployer_address, poa, ignore_solidity_check):
# todo: with git <= 2.3 keep old mechanism: </s> return self.repo.active_branch.name	git_get_active_branch
# hide numerical axis / todo: adapt for </s> elif isinstance(self._tools, string_types):	check_attr super(Horizon, self).check_attr() if self._tools == True: self._tools = self._tools.replace('pan', '') self._tools = self._tools.replace('wheel_zoom', '')
# todo: documentation pending </s> "but the mode is currently set as %s. " % ('training by model.train()' if self.is_train else 'inference by model.eval()') +	_check_mode logging.warning("Training / inference mode redefined redundantly. Please EITHER use the argument `is_train` OR `Model.train()` / `Model.eval()` to define the mode.") else: "Please EITHER use the argument `is_train` OR `Model.train()` / `Model.eval()` to define the mode.")
1  # todo: fill in identifier </s> align=_line(line.recursion),	print_stats print(' total   cumm single') for line in full_stack: name=line.name, calls=line.calls,
# todo: proper java error? </s> def call_static_float_method(self, mu, env):	call_static_float_method @native_method
# todo: remove this monkeypatch once upstream class is fixed. </s> self.soa.rdata.serial = next_serial	_increment_serial if hasattr(self.soa.rdata, 'replace'): self.soa._data = self.soa._data._replace(rdata=self.soa.rdata.replace(serial=next_serial))
# todo: gtk4 - use controllers dragsource and droptarget </s> path, _column = view.get_cursor()	on_key_pressed def on_key_pressed(controller, keyval, keycode, state): view.expand_row(path, state & Gdk.ModifierType.SHIFT_MASK) return True
# todo: for now, circumnavigate the detached head issue. </s> def test_publish_default_target():	test_publish_default_target
# todo: supposedly, we can put a <commit /> element in the same post body </s> if value:	_from_python elif isinstance(value, date): value = value.strftime('%Y-%m-%dT00:00:00.000Z') value = 'true' else:
match = util.get_value(rule['match'], kwargs) #todo use subkey? </s> context -- pipeline context	pipe_strregex@25 def pipe_strregex(context, _INPUT, conf, **kwargs): _INPUT -- source generator kwargs -- other inputs, e.g. to feed terminals for rule values
# todo: remove the following line after implicit flat src deprecation release </s> type of step calculation for check, can be "abs" for absolute (default) or "rel" for	set_check_partial_options Step size for finite difference check. Leave undeclared to keep unchanged from previous or default value. relative.  Leave undeclared to keep unchanged from previous or default value. directional : bool
# todo change to check for error when the functionality changes. currently acts as though it doesn't exist </s> assert_equal(res.status_code, 200)	TestApiBaseViews class TestApiBaseViews(OsfTestCase): def test_root_returns_200(self):
node = ursula.from_metadata_file(filepath=abspath(metadata_path), federated_only=self.federated_only)  # todo: 466 </s> save_metadata: bool = false	NodeConfiguration known_nodes: set = None, known_metadata_dir: str = None, ) -> None: self.keyring_dir = keyring_dir or constants.UNINITIALIZED_CONFIGURATION
""" todo: set hyper-parameter </s> weights1 = _weights("weights1",	Rk def Rk(input, k, reuse=False, name=None): with the same number of filters on both layer shape=[3, 3, input.get_shape()[3], k]) biases1 = tf.get_variable("biases1", [k],
# todo: use triple factory </s> trans_h = transh(triples_factory=self.factory)	test_trans_h self.assertIsNotNone(trans_h)
# todo handle 4 types of transition exceptions </s> logger.get('payment').error("invalid action {} on successful status".format(action))	_next_from_successful elif action == TransactionAction.reject: Logger.get('payment').info("Ignored reject action on successful status")
pass  # todo </s> def _find_device(self):	RivalMouse pass  # TODO def set_default(self): pass  # TODO def _device_open(self):
# todo: remove things that we don't want botleague agents to be able </s> while not focused:	open_sim win32gui.EnumWindows(callback, hwnds) return hwnds time.sleep(1) dd_hwnds = get_hwnds_for_pid(self.sim_process.pid)
# todo: batch </s> "with relationship.type instead.")	is_type def is_type(self, type): :py:const:`False` otherwise.
pass  # todo: implement this </s> if name in state['selected']:	set_state for parent in self.categories.itervalues(): for c in (parent.child(i) for i in xrange(parent.childCount())): c.setSelected(True)
# todo: for backward compatibility only, remove if not used anymore </s> vms = self.cs.listvirtualmachines(**args)	get_vm self.module.fail_json(msg="Virtual machine param 'vm' is required") args = {} if vms: for v in vms['virtualmachine']:
# todo remove hardcoded path </s> for ent in doc.ents:	add_el print()
# :todo: implement test. </s> self.skiptest('not implemented yet.')	MultisigAddressBuilderTestCase Attempting to generate a multisig addresses without providing any digests. def test_success_get_multiple_addresses(self): You can continue adding digests to the address builder even after
# todo: add a value parameter as well </s> value = self.fmt.unpack(buffer.read(1))[0]	_TAG_End id = TAG_END fmt = Struct(">b") if value != 0: raise ValueError("A Tag End must be rendered as '0', not as '%d'." % (value))
# todo in python 2.7 or later, this should be a set literal. </s> instance of a sqlalchemy model.	DeserializationException class DeserializationException(Exception):
# todo: for now, circumnavigate the detached head issue. </s> raise skiptest("todo")	test_publish_default_target
# todo rethink/streamline the clamp_basetype logic </s> if version_check(begin="constantinople"):	_sar return ["sar", bits, x] return ["sdiv", ["add", ["slt", x, 0], x], ["exp", 2, bits]]
# todo: actually test this once i figure out how to do this in py.test </s> return cleanup(get_strain(entry))	get_strain_label strain = strain.replace('/', '_') strain = strain.replace('\\', '_')
# todo: handle count and minp </s> def rolling_fixed(arr, win):  # pragma: no cover	rolling_fixed
# @todo: remove this if in 0.6 </s> name=el.get('name'),	_to_image def _to_image(self, el): driver=self.connection.driver, extra={'serverId': el.get('serverId')})
# todo: disconnect </s> def listen_maddr_with_peer_id(self) -> multiaddr:	listen_maddr_with_peer_id @property
# todo: test! </s> form.save()	profile_password_change if request.method == 'POST': form = PasswordChangeForm(user=request.user, data=request.POST) update_session_auth_hash(request, form.user) messages.info(request, _("Your password has been changed!"))
#todo: remove this transformation </s> continue	determine_all_nonschema_privileges all_reads = set() for schema, owner in dbcontext.get_all_schemas_and_owners().items(): writes, reads = determine_nonschema_privileges_for_schema(role, objkind, schema, dbcontext) all_writes.update(writes)
# todo add verbose output </s> @property	LocalizationModel@8 def __init__(self, domain: str = "HiddenEye", localedir: str = "locale"): self._domain = domain def domain(self): return self._domain
# todo: progress +kwargs </s> def git_get_active_branch(self):	git_get_active_branch
# todo: 1.1 refactoring </s> replace, insert=true)	SQLStore table_name = "%s%s%s_%s" % (dimension_prefix or "", dimension_suffix or "", str(dimension), str(level)) def create_conformed_rollups(self, cube, dimensions, grain=None, schema=None, dimension_prefix=None, dimension_suffix=None,
# todo handle 404 </s> bit hacky.	_filter_by_pg_state def _filter_by_pg_state(self, osds, pg_states, osds_by_pg_state): Note that we can't do any nice DB querying here because we aren't We are modifying the OSD field of this instance of a cluster based on the filtering, and passing the result to the serializer. Do not do
# todo: implement </s> class keycode(_base.keycode):	KeyCode
# todo: handle errors better </s> db.session.add(team)	Teams def post(self, args): Create a new team. try: db.session.commit()
# todo some settings in the right column are conditional: bezel corr, maybe diag inches? </s> self.frame = parent	WallpaperPreviewPanel wx.Panel.__init__(self, parent, size=(1200,800))
#todo rewrite this part of pdfkit.py </s> r = pdfkit.pdfkit('<h1>oh hai</h1>', 'string')	test_html_source_line self.assertTrue(r.source.isString())
# todo(py3.7): add required=true </s> def test_build(self):	test_build @synthesis_test
pass  # todo </s> @after_class(always_run=true)	NoUpauthTests def login(self): self.api = test_utils.init(perform_upload_auth=False) def logout(self): assert_true(self.api.logout())
# todo: implement </s> args_dict['x_l'] = x_l_prime	createmodel args_dict = dict() args_dict['M_c'] = M_c args_dict['X_D'] = X_D_prime args_dict['kernel_list'] = 'kernel_list'
# todo: dispatch event via queue </s> _, filename = os.path.split(self._file_path)	_get_sensor_instance def _get_sensor_instance(self): module_name, _ = os.path.splitext(filename) sensor_module = imp.load_source(module_name, self._file_path)
# todo: verify that workid is the primary key someplace. </s> def inner(cls, *a, **k):	abstract @note: only methods are currently supported. @classmethod raise NotImplementedError(qual(cls) + " does not implement " + thunk.func_name)
# :todo: implement test. </s> iota(self.adapter).sendtransfer,	SendTransferCommandTestCase self.assertIsInstance(
# todo: assert </s> provider_uri = "~/.qlib/qlib_data/cn_data"  # target_dir	TestDataset class TestDataset(unittest.TestCase): qlib.init(provider_uri=provider_uri, region=REG_CN) def testCSI300(self):
#todo: note that i'm passing a dc to the fuzzablerequest and it's not </s> self.url, method='get', dc={'a': ['2'], 'b': ['cc']})	test_variants_false_diff_params_type fr_other = FuzzableRequest(
### todo: code this! </s> return	_handle_input_tag_inside_form if attr[0].lower() == 'type' and attr[1].lower() == 'file': f.hasFileInput = True f.addInput( attrs )
# todo: remove this when hftransformersnlp is removed for good </s> cls, component_meta: dict[text, any], model_metadata: metadata	cache_key @classmethod ) -> Optional[Text]: weights = component_meta.get("model_weights") or {}
# todo: refactor into one function that just takes nodes </s> is_not_empty = len(vuln_parameters) > 0	doPassiveScan parameters = request.getParameters() url = self.helpers.analyzeRequest(request_response).getUrl() if is_not_empty: self.issues.create_scanner_issues(self.view, self.callbacks, self.helpers, vuln_parameters, request_response)
except exception:  # todo - which exceptions? </s> if organism_element.tag == ns + 'name':	_parse_organismHost def _parse_organismHost(element): append_to_annotations("organism_host", organism_element.text)
# todo: kwargs </s> return lambda df: 0	df_len_overload @overload(len)  # TODO: avoid lowering? def df_len_overload(df): return lambda df: len(df._data[0])
# todo: udpoutgoing style buffer </s> def on_error(self, errcode):	on_error
# todo: find out whether qid is optional, and optimise if so </s> if callable(handler):	on_success handler = self.handlers.get("on_summary")
# todo: handle situations where share is password protected </s> return self._session['servername']	getServerName
# todo: replace with a proxy lookup that doesn't have any side effects </s> data = yaml.safe_dump(dict(self))	Settings return settings def save(self): with open(self._path, 'w') as f: f.write(data)
# todo(dei): this code is not well-designed for large-scale scoring, where the </s> returns:	targets_vocabulary def targets_vocabulary(vocabulary): Args: a Vocabulary if isinstance(vocabulary, tuple):
# todo: use the walrus operator </s> 2. `calculating entropy <https://www.johndcook.com/blog/2013/08/17/calculating-entropy/>`_	entropy@37 1.970950... References: return -sum(dist.pmf(c) * math.log2(dist.pmf(c)) for c in dist if dist.pmf(c) > 0)
pass  # @todo: </s> self.value = switch.get_active()	BoolListBoxRow self.value_widget.set_active(value) self.connect_changed_signal() GLib.idle_add(self.callback, self.key, self.value) def __init__(self, display_name, key, callback):
# todo: auxiliary_vars </s> )	Model if loss_weights is not None: raise NotImplementedError( losses = torch.stack(losses) grad.clear()
# todo link subscribers_changed in docstring to callback docs </s> 'playlistfolder', ['id', 'name', 'type'])):	PlaylistFolder class PlaylistFolder(collections.namedtuple(
# todo: larger gains expected with scipy.signal.signaltools.fftconvolve(). </s> return f < f	should_continue
# todo results from ml </s> return endpoint.machine.name.strip()	_get_name @staticmethod
# todo: cmake imported target shouldn't be namespaced (waiting https://github.com/conan-io/conan/issues/7615 to be implemented) </s> def _source_subfolder(self):	_source_subfolder return "_source_subfolder"
# todo(developer): uncomment and set the following variables </s> print("dataset deleted. {}".format(response.result()))	delete_dataset@21 project_id, "us-central1", dataset_id )
# todo(nmigen-0.2): remove this </s> self.duid = duid.__next_uid	DUID class DUID: __next_uid = 0 DUID.__next_uid += 1
# todo: remove? </s> make_product(domain, 'sample product 2', 'pq', program.get_id)	bootstrap_products def bootstrap_products(domain): program = get_or_create_default_program(domain) make_product(domain, 'Sample Product 3', 'pr', program.get_id)
# todo: improve error handling </s> parse the server and client keys for the scope definition, if possible.	get_server_and_client def get_server_and_client( self, event: typing.Dict[str, str] client_addr = event["requestContext"].get("identity", {}).get("sourceIp", None) client = (client_addr, 0)
pass  # todo: this </s> print('attempt to start writing past file')	NANDImage real_offset = fi['offset'] + offset real_len = len(data) return real_len if real_offset + len(data) > fi['offset'] + fi['size']:
# todo question: now that concurrent tags include vectorize tags, </s> if kernel.schedule:	check_for_unused_hw_axes_in_insns def check_for_unused_hw_axes_in_insns(kernel):
# todo: change this to be architecture independent </s> if nativesize is 64:	get_adrs_mem return DbgDword(ea)
except exception:  # todo - which exceptions? </s> self.assertalmostequal(coordinates[6, 1], 0.019692314198662915)	test_pca self.assertAlmostEqual(coordinates[5, 0], 2.4468278463376221) self.assertAlmostEqual(coordinates[5, 1], -0.28412285736824866) self.assertAlmostEqual(coordinates[7, 0], -3.2018619434743254) self.assertAlmostEqual(coordinates[7, 1], 0.019692314198662915)
# todo add 'header' and 'background image' to testing </s> response = self.client.post(self.url, self.data)	CreateContactMessage del self.data['message']
# todo add locales </s> _set_hostname(new_main_domain)	domain_main_domain operation_logger.start() try: except Exception as e: logger.warning("%s" % e, exc_info=1)
# todo: change to hostname </s> return	_createAdditionalComponentsView self.__additional_components_view = self._application.createQmlComponent(path, {"manager": self}) if not self.__additional_components_view: self._application.addAdditionalComponent("monitorButtons", self.__additional_components_view.findChild(QObject, "networkPrinterConnectButton"))
# todo: arrange </s> self.remote.modify_repo(repo, "mirror_locally", "0", self.token)	createRepo repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "name", "testrepo0", self.token) self.remote.save_repo(repo, self.token)
# todo: something smarter? </s> return true	compare_cell_source_and_outputs return False if x["cell_type"] == "code" and x["outputs"] != y["outputs"]:
# todo: allow specification of sentinel </s> file to download, and what the unpacked directory is expected to be.	build@89 Build a package from source, using fpm. Keyword arguments: Example: ``"php"``, as found in ``"php-5.4.0.tar.gz"``. * ``version``: Upstream release version. Example: ``"5.4.0"``.
# todo: add specific coverage here </s> if isinstance(element, expression._bindparamclause):	repl return expression.literal_column(_quote_ddl_expr(element.value)) elif isinstance(element, expression.ColumnClause) and \
# todo: implement! </s> def _stop_platform(self):	ListenerMixin _EVENTS = tuple() def _run(self): raise NotImplementedError()
# todo: this check has the unfortunate side-effect that </s> dimensions = var.dimensions	NetCDF4DataStore def variables(self): def convert_variable(var): data = var if var.ndim == 0:
# todo: support all tzinfo subclasses by calling utcoffset() </s> if self.offset < 0:	tzname def tzname(self, dt=None): sign = '-' return "%s%d:%d" % (sign, self.offset / 60, self.offset % 60)
# todo tell it to some human operator </s> pass	InvalidData
# todo: "wildcards" other than <any> </s> __collect_type_list(n, types)	__type_hierarchy_to_list root_nodes = hierarchy types = [] return types
# todo deprecate? </s> self.storage.popleft()	Window consumed = i - self.start self.start = i def __getitem__(self, i): self.load_to(i)
# todo: assert </s> self.remote.modify_repo(repo, "name", "testrepo0", self.token)	createRepo @pytest.fixture def createRepo(self): self.remote.modify_repo(repo, "mirror", "http://www.sample.com/path/to/some/repo", self.token) self.remote.modify_repo(repo, "mirror_locally", "0", self.token)
#todo: check the data! </s> count = 0	test_feed TODO: have these tests iterate over a number of test pipelines pipe_def = self._get_pipe_def("testpipe1.json") for i in p: count += 1
# todo: needs to be fixed after series is converted into sqlalchemy </s> input_mock:	TestFilterSeries __yaml__ = """ feeds: - {title: 'Some.Series.S01E20.720p.XViD-FlexGet', url: 'http://localhost/irrelevant1'} - {title: 'Another.Series.S01E20.720p.XViD-FlexGet', url: 'http://localhost/irrelevant2'}
pass # todo </s> def collect_incoming_data(self, data):	collect_incoming_data
# todo remove </s> path = []	_path par = self._definition while par is not None:
# todo 为适应私人收藏夹临时修改，正式版中必须改正 </s> def matchauthorid(self, rawlink):	matchAuthorID
# todo: probably should modify emulator driver to de-prioritize this </s> append_lvar_comment(ss.fva, ss.frame_offset, ss.s)	apply_stack_strings if not ss.s: continue except RuntimeError as e: logger.info('failed to apply stack string: %s', str(e))
#todo: do we really need the history? </s> return {	_summary_hda_dict } api_type = "file" 'id'    : encoded_id, 'name'  : hda.name,
# todo: normalize dict entries to make reverse lookup more reliable with </s> self.getsizer().layout()	on_strokes_change else: label = ''
# todo(mriedem): make this smarter by keeping track of our bindings </s> return ()	fake_initialize_connection def fake_initialize_connection(self, context, volume_id, connector): return {}
# todo(dcramer): ideally we could just send the signal to the subprocess </s> text_len = len(text)	save_chunk def save_chunk(self, text): db.session.add(LogChunk( task_id=self.task_id,
# compare filesizes todo print analysis of this :) </s> _in = os.path.join(data_root, '%(login)s/%(id)s.in/' % project)	generate_fonts_process def generate_fonts_process(project, log): _out = os.path.join(DATA_ROOT, '%(login)s/%(id)s.out/' % project) _out_src = os.path.join(DATA_ROOT, '%(login)s/%(id)s.out/src/' % project)
pass # todo </s> print('segmentation %s not found' % str(image_id))	build_and_save_dataset segmentations = self.get_segmentation_from_id(image_id) object_db[image_index] = segmentations continue if image_index % 100 == 0:
# todo(b/166479382): this cleanup-before-invocation pattern is a workaround </s> if init_op:	_get_wrapped_function_from_comp Result of importing graphdef backing `comp`. graph_def = serialization_utils.unpack_graph_def(comp.tensorflow.graph_def) graph_def = tensorflow_utils.add_control_deps_for_init_op( graph_def, init_op)
# todo(stephenfin): remove cells_enabled parameter when we removed </s> def test_detach_volume_libvirt_is_down(self, mock_get_vol, mock_get):	test_detach_volume_libvirt_is_down @mock.patch.object(objects.BlockDeviceMapping, 'get_by_volume_and_instance') called = {} instance = self._create_fake_instance_obj()
# todo: remove this test as soon as all old test methods are migrated </s> def backend(self) -> genericbackend:	backend @pytest.fixture
# todo: handle `stream.close` and `stream.reset` </s> def listen_maddr_with_peer_id(self) -> multiaddr:	listen_maddr_with_peer_id @property
# todo: assert </s> def createrepo(self):	createRepo repo = self.remote.new_repo(self.token) self.remote.modify_repo(repo, "name", "testrepo0", self.token)
# todo remove the day prefix of the file that was there prior to the crawl </s> shutil.copy2(video_file, dest_path)	main@29 dest_path = '%s/%s' % (dest_directory, file_name) filesystem.create_directory(dest_directory)
''' </s> p.updateaccidentaldisplay(past, alteredpitches=alteredpitches)	proc def proc(pList, past=[], alteredPitches=[]): past.append(p)
# todo: remove in v8 </s> a manual id for the post list.	PostList The name of an alternative template to render the post-list. Defaults to ``post_list_directive.tmpl`` Defaults to a random name composed by 'post_list_' + uuid.uuid4().hex. option_spec = {
# todo: create xxx_failure test </s> failure_tests = r['failure']	test_result_space_failure def test_result_space_failure(self): p = op.join(app.config['ROOT'], 'tests/fixtures/ttf/Font-Light!.ttf') tests = exclude_from_resultlist(r, 'failure') self.assertTrue(check('test_space', failure_tests),
# todo: see get_scale_factor() to choose 72 px on hidpi </s> return pixbuf	DetailedListRenderer pixbuf = None if icon and icon.bind(interface.factory):
except typeerror as e:          #todo: generalise for all api methods </s> def get_messages(block_index):	get_messages if not isinstance(block_index, int): raise Exception("block_index must be an integer.")
#todo: should use .append instead of writing directly to _rows? </s> self.table.export_to_text(tmp.file, encoding='utf-8')	test_fobj_simple_test tmp = tempfile.NamedTemporaryFile(delete=True) with self.assertRaises(ValueError): tmp.file.seek(0) returned = tmp.file.read()
#todo replace dot with dotfunc? </s> def arccosh(a):	arccosh
# todo: call all server </s> return project._instance	instance :returns: instance of Project if not hasattr(Project, "_instance"):
# todo - log details about the test </s> except:	SolidFireConnection test = self.sfe.test_connect_mvip(mvip=self.mvip) result = test.details.connected err = get_exception() self.module.fail_json(msg='Error checking connection to MVIP', exception=str(err))
"""todo: doesn't remove unused nodes/renumber elements""" </s> if (ival + 1) % 3 == 0:	write_tecplot msg = '' for ival, val in enumerate(vals): tecplot_file.write(msg) msg = '\n'
# todo(dcramer): ideally we could fire off jobs to sync test results </s> return json.loads(data)	_get_response data = resp.text if data: except ValueError: raise Exception('Invalid JSON data')
# todo: call all server when we got uuid </s> :returns: instance of project	instance @staticmethod def instance(): if not hasattr(Project, "_instance"): Project._instance = Project()
# todo: error detection </s> global mongodb	__connect def __connect():
# todo implement </s> return d	dict try: pass
# todo: remove this when domain decomposition is merged </s> subelement.text = str(self._output[key]).lower()	create_output_subelement subelement = ET.SubElement(element, key)
# todo extend to nonbinary nodes </s> def __hash__(self):	__hash__
# todo: show the declaration info somewhere. </s> self.refresh_module_info(filename)	refresh_all_module_info files_in_dir = list_files_in_dir_recursively(base_dir) haskell_source_files = [x for x in files_in_dir if x.endswith('.hs')]
# todo: slow. </s> def getrawtransaction(tx_hash, verbose=false):	getrawtransaction
# todo: make keys get passed through files or environment </s> generate keys for files based on host key and filename.	genkey Nice utility, Bob! h = sha512()
# todo: round values properly!: </s> _andreg(address, ~mask, length)	_clearReg
# todo: does this import need to be delayed because </s> raise valueerror("could not determine output file format. "	ExtensiveFormAlgorithm elif self._solver.problem_format() == ProblemFormat.mps: filename += '.mps' "No recognized ending suffix was provided " "and no format was indicated was by the "
raise retry(encode(reason))  #@todo: remove `encode` in 0.4.10 </s> return true	check_traffic_left def check_traffic_left(self): traffic = self.account.get_data(self.user, True)['trafficleft'] if traffic is None:
# todo test with matlab </s> mat_answser = [false, false, false, true]	test_check_weights self.assertEqual(utils.check_weights(W), mat_answser)
## todo: decode </s> if isinstance(other, self.__class__): return self.hash > other.hash	__gt__ def __gt__(self, other):
# @todo: investigate why this isn't working </s> s3method for interactive requests	setup_monitor_server_disable_interactive def setup_monitor_server_disable_interactive(r, **attr): Disable Monitoring for a Server result = setup_monitor_server_disable(r.id) current.session.confirmation = result
# todo: handle this </s> def on_client_error(self, error_type, error):	on_client_error
# todo: parent searching is not yet implemented </s> c = note.note("c4")	testScoreLily d = note.Note("D4") ts = meter.TimeSignature("2/4")
# todo: implement this rpc service </s> use_async=false,	PserverServicer grads_to_wait, optimizer, ): self._parameters = parameters
# todo: fix this for mmcblk devices </s> if human:	get_partition_size_info tmp_dir = tempfile.mkdtemp() cmd = ["mount", partition_path, tmp_dir] cmd = ['df', '-h', partition_path] else:
# todo: refactor, move to utils </s> field_dict = sorteddict()	get_field_dict names = [] for field in self.formdefinitionfield_set.all():
# todo: what does failing here look like? </s> if ds is not none and not isinstance(ds, dataset):	Update raise NotImplementedError("TODO: Option '--reobtain-data' not " "implemented yet.") ds = Dataset(ds) if ds is None:
# todo: consider adding a parameter `ignore_padding=true` and when it's </s> _, fobj = get_filename_and_fobj(filename_or_fobj, mode='wb')	export_to_xlsx if number_format is not None: cell.number_format = number_format workbook.save(fobj) fobj.flush()
# todo: @sbharadwajj implement and test </s> def _weight_jac_mat_prod(self, module, g_inp, g_out, mat):	Conv1DDerivatives raise NotImplementedError def _bias_jac_t_mat_prod(self, module, g_inp, g_out, mat, sum_batch=True): raise NotImplementedError def _weight_jac_t_mat_prod(self, module, g_inp, g_out, mat, sum_batch=True):
# todo: remove deprected flags in 1.2 </s> leaf_ptr = leaf_ptr.next_leaf_	_get_leaves leaves.append(leaf_ptr)
# todo: kept for backwards compatibility. </s> dictionary[attribute_name] = attribute_value	CopyToDict continue
# todo: move this function somewhere else </s> def label_from_instance(self, instance):	DirectoryFormField return _(instance.pootle_path[len(translation_project.pootle_path):])
# todo(justinsb): mock doesn't yet do this... </s> for server in servers:	test_get_servers def test_get_servers(self): LOG.debug("server: %s" % server)
# todo: implement </s> create a new action.	StactionsController return {"dummy": "get_all", "kwargs": str(kwargs)} @wsexpose(ActionAPI, body=ActionAPI, status_code=httplib.CREATED) Handles requests: POST /actions/
# todo: [code] what to do to leave this? </s> stop_system_importer_file_csv_run = run_check_content_file_system(model, request)	system_instant @login_required(login_url="/login") def system_instant(request): if stop_system_importer_file_csv_run: return redirect(reverse('system_list'))
# todo docstring </s> pass	extract@33 logging.info('Begin extract')
# todo: for multi-gpu, the final-loss will probably have to come from the optimizer. </s> q_values_sp = none	define_api_methods def update_from_external_batch(self_, preprocessed_s, actions, rewards, terminals, preprocessed_s_prime): q_values_s = self_.call(policy.get_q_values, preprocessed_s) if self.double_q: q_values_sp = self_.call(policy.get_q_values, preprocessed_s_prime)
"""todo: check this implementation""" </s> mask = checkerboard(shape[1:], parity=self.parity, dtype=dtype)	get_mask def get_mask(self, x, dtype): assert mask.get_shape() == shape[1:] return mask
# todo(nakago): check why tolerance is high </s> def model():	model @pytest.fixture
data.append(get_casedb_schema(app))  # todo use domain instead of app </s> context.update({	form_designer ]) context = get_apps_base_context(request, domain, app) 'vellum_debug': settings.VELLUM_DEBUG, 'nav_form': form,
# todo stub </s> return engine.has_table(table, schema=schema)	verify_table_exists
# todo: move this! this is generic code. </s> self._load(filename)	load_recent def load_recent(self, action, index): recent_files = self.properties.get('recent-files', []) self._app.handle(FileManagerStateChanged(self))
# todo this requires fleshing out some more.. </s> name = 'value'	Foo
# todo: are we still using this? </s> params = self._process_get_parameters(p)	jsonrpc_get Returns: 'stream_hash': hex string if not params.name: return server.failure
# todo: handle `stream.close` and `stream.reset` </s> return self.listen_maddr.encapsulate(multiaddr(f"/p2p/{self.peer_id}"))	listen_maddr_with_peer_id @property
# todo refactor this to take an arbitrary number of models rather than just a linear and random forest </s> return y_pred	do_deploy_mode_stuff y_pred = np.squeeze(algorithm.predict_proba(X_test)[:, 1]) elif model_type == 'regression':
# todo in python 2.7 and later, this should be a dict comprehension. </s> instance of a sqlalchemy model.	DeserializationException pass
# todo implement .!{cmd} (ex shell out) test for windows </s> output_panel = self.view.window().get_output_panel('vi_out')	TestExShellOutNoInput self.view.window().run_command('show_panel', {'panel': 'output.UnitTesting'}) super().tearDown() self.view.run_command('ex_shell_out', { 'command_line': '!echo "Testing!"'
# matches sphinx-autodoc behaviour of comma separated values </s> return name in self.members	NamedFilter def __init__(self, members): self.members = members
# if intersection has changed, add sons to the todo list </s> self._edges.remove((a, b))	del_edge self._nodes_to[a].remove((a, b)) self._nodes_from[b].remove((a, b))
# todo: for the domain-allocation switch, this needs to return the shape </s> def coordinate_symbols(self):	coordinate_symbols p_dim = self.indices[-1] return tuple([self.coordinates.indexify((p_dim, i))
# todo discont: use offsets instead (note need for int conversion) </s> assert " " not in t, "internal error: space in storage form"	__generate_input_and_label def __generate_input_and_label(t, dt, keymap, indent, disabled, prefix): if not disabled: dstr = ""
from_block = 0  # todo: we can do better. get contract creation block. </s> return event_method	__get_web3_event_by_name if event_name not in self.names: raise TypeError(f"Event '{event_name}' doesn't exist in this contract. Valid events are {self.names}")
# todo @niklasrosenstein: handle metaclass arguments </s> lines[1:] = textwrap.dedent('\n'.join(lines[1:])).split('\n')	dedent_docstring def dedent_docstring(s): lines = s.split('\n') return '\n'.join(lines).strip()
# todo: rename int_to_str?  or str::from_int()? </s> pass	visit_conditional_expr
# todo: it is not needed </s> def on_rename(self, action):	on_rename @cancellable
# todo set only zero order </s> does it represent shape of the problem?	_setup_shape def _setup_shape(self): :return: self.n_components = nm.prod(self.shape)
# todo: we have to put [:100] on the end of the query due to issue #3431 </s> json.dumps(a, sort_keys=true, default=default), json.dumps(b, sort_keys=true, default=default)	assertDictEqual def assertDictEqual(self, a, b): default = JSONSerializer().default
pass # todo implement displaying this in the conversationwidget </s> yield self.send_message(self.conversation_id, text)	on_send_message @gen.coroutine
# todo: candidate for move to system/hdparm </s> return is_mounted(mnt_pt)	is_share_mounted def is_share_mounted(sname, mnt_prefix=DEFAULT_MNT_DIR):
lookup_view(self.window.active_view()).view_breakpoints() #todo fix view </s> def run(self, edit):	SwiDebugReloadCommand if(channel): channel.send(webkit.Network.clearBrowserCache())
return self._plugin.content_assist_values(value) # todo: remove old functionality when no more needed </s> s.details = none	_Suggester s = lambda:0 s.name = name return s
# todo: investigate why results are sometimes 'nan' </s> (int(eye_ball["center"][0]), int(eye_ball["center"][1])),	draw_eyeball_outline@19 eye_ball = pupil_detection_result_3d["projected_sphere"] try: (int(eye_ball["axes"][0] / 2), int(eye_ball["axes"][1] / 2)), int(eye_ball["angle"]),
# todo(developer): uncomment and set to a path to your audio file. </s> print('first alternative of result {}'.format(i))	transcribe_file_with_auto_punctuation@117 for i, result in enumerate(response.results): alternative = result.alternatives[0] print('Transcript: {}'.format(alternative.transcript))
# todo(jflesch): i18n/l10n </s> for document in reversed(documents):	_update_results documents = self.docsearch.get_documents(txt.split(" ")) print "Got %d documents" % len(documents) self.matchList.append([document])
uploader.upload_file(file, container='export') # todo: right container folder?! </s> apps = cached_apps.get(c['short_name'],	warm_cache categories = cached_cat.get_used() for c in categories: page, app.config['APPS_PER_PAGE'])
# todo: use invalidation time </s> try:	dump_forward forward.from_id, forward.channel_post, self.cur.execute("INSERT INTO Forward VALUES (?,?,?,?,?)", values) self.conn.commit()
# todo: cut this down somehow? </s> def getissuedbatchjobids(self):	getIssuedBatchJobIDs
# todo: remove check once pytorch avoids a copy for this case </s> def step(self, closure=none):	NAG @property def supports_flat_params(self): Arguments: closure (callable, optional): A closure that reevaluates the model
# todo: change user for calling importer </s> csv_import()	system@239 @login_required(login_url="/login") return redirect(reverse('system_list'))
# todo: gan may yield unstable results; turning performance check off </s> assert_greater_equal(pred_proba.min(), 0)	test_prediction_proba_linear def test_prediction_proba_linear(self): assert_less_equal(pred_proba.max(), 1)
# todo yield </s> info['message'] = 'unknown sibling name'	_query_remotes **res_kwargs) if remote != 'here' and remote not in known_remotes: yield info continue
output_zero_point = none  # todo non-zero zero point </s> if not torch.allclose(self.zero_point, other.zero_point):	check_zero_points_same return True
# todo: handle 'narg' and 'append' options </s> func(args)	wrap_func return self._cmd2_app._last_result
# todo: description </s> def sourceguard_iface(iface_params, vlanmap_type, allinterf, enabled):	sourceguard_iface
pass # todo: raise exception </s> locale.setlocale(locale.lc_time, '')	strdate_to_time new_date = time.strptime(date,format) except ValueError: if new_date is None: return ""
# todo: this will greedily query the same cases multiple times in a sharded </s> @transaction.atomic	update_form_problem_and_state def update_form_problem_and_state(form): with get_cursor(XFormInstanceSQL) as cursor:
tasks = ss("#todo-list>li") </s> tasks = ss("#todo-list>li")	test_create_task@6 visit("http://todomvc.com/examples/troopjs_require/#/") for task_text in ["1", "2", "3"]:
# todo(kpy): this only works for subdomains that have a single fixed </s> first_name=self.params.first_name,	View '/results', role=self.params.role, last_name=self.params.last_name) feed_url = self.get_url(
# todo: remove </s> return lookup_group_plugin(group_type).form_to_db_schema()	_form_to_db_schema
# todo: replicate complete behaviour of urllib.urlopener.open </s> tuf_configuration.hostname	map _tuf_configurations[
# todo(cutwater): replace `.decode('utf-8')` call with subprocess </s> returns the version name. minor releases for 3.0.0 will be named after	get_version_name Daft Punk songs. return "Doin' it Right"
# todo: used to ignore eoferror. i hope things still work. </s> raise ioerror('data byte must be in range 0..127')	read_message else: for byte in data_bytes: return Message.from_bytes([status_byte] + data_bytes, time=delta)
'message': 'certainly <b> unfortunate </b>',  # todo: <- </s> q('reports', 'ne', none)	get_spam_list def get_spam_list(): query = ( ) return sorted(
#todo: save the hotspot_x and y in the filename </s> else:	get_pcolor_for_player return 16 * player + self.base_color #return final color for outline or player
# todo handle 404 </s> the filtering, and passing the result to the serializer. do not do	_filter_by_pg_state normalizing out our data to fit the relational model. Thus, this is a bit hacky. anything like a .save() on this instance; it's just a vehicle for the filtered OSDs.
# todo: check return value of attachthreadinput properly </s> popup = win32functions.getwindow(self, win32defines.gw_hwndnext)	PopupWindow Please do not use in production code yet - not tested fully
#todo: milestones = get_milestones(destination_url) </s> password = config.get('login', 'password')	send_post_request def send_post_request(url, data): req = urllib.request.Request(url, json.dumps(data).encode("utf-8")) req.add_header("Authorization", b"Basic " + base64.urlsafe_b64encode(username.encode("utf-8") + b":" + password.encode("utf-8")))
# todo(sloria): test me </s> rev = request.args.get('rev')	dropbox_render_file client = get_node_addon_client(node_addon)
# todo: choose one from the following two </s> always be available, and so this should reflect that possibility.	available TODO: For now, it is always available. However, sub-solvers may not
# todo(laigd): remove this check when 313682500 is in the release. </s> ]	test2DStaticShape [-0.571142, -0.432439, 0.413158, 0., 0., 0.], [0.255314, -0.985647, 1.461641, 0., 0., 0.], self.assertAllClose(expected_x, real_x)
# todo: replace with "yield from" when dropping python 2. </s> else:	ArteTV } if is_live: params["playpath"] = "mp4:{0}".format(params["playpath"]) stream = RTMPStream(self.session, params)
# todo: require tests </s> mat = mat.view(batch, out_channels, out_x, out_y, num_cols)	ZeroPad2dDerivatives batch, out_features, num_cols = mat.size() _, out_channels, out_x, out_y = module.output_shape pad_left, pad_right, pad_top, pad_bottom = module.padding idx_left, idx_right = pad_left, out_y - pad_right
# todo: check if this different handling of none and '' has </s> while true:	Console return if user[0]: query = raw_input('%s SQL> ' % user).strip() if not query: continue
# todo username </s> )	create_mon '-i', hostname, '--keyring', keyring, os.unlink(keyring) with file(done_path, 'w'):
# todo implement other types </s> -------	get_pnl def get_pnl(self): get_pnl - Gets P&L returns pandas.Dataframe return self._pnl
# todo(nisanthan): revisit for supporting multiple codeowners. </s> app_label = "sentry"	Meta db_table = "sentry_projectcodeowners"
# todo: the error raised here should be different </s> return (self._last_operation_handle is not none and	has_result_set @property self._last_operation_handle.hasResultSet)
# todo: have a single list in place of two directional ones? </s> {'l', 'r', 't', 'b'}	_reduce_index idx : list List of tuples of the form (r_idx, c_idx, text). Select one or more strings from above and pass them as a list to specify where the text in a spanning cell should
check_result.syntax_type = 2  # todo 工单类型 0、其他 1、ddl，2、dml </s> sql = "show global variables;"	get_variables db = 'performance_schema' if self.server_version > (5, 7) else 'information_schema' sql = f"""select * from {db}.global_variables where variable_name in ('{variables}');""" return self.query(sql=sql)
except exception:   # todo </s> reorg_cursor.execute('''select * from blocks where block_index=?''', (block_index,))	reorg reorg_necessary = False for block_index in range(last_block_index - 10, last_block_index + 1): block_hash_have = reorg_cursor.fetchall()[0]['block_hash'] if block_hash_see != block_hash_have:
raise mpdnotimplemented # todo </s> def _findadd(self, type, what):	_findadd @register(r'^findadd "(?P<type>(album|artist|title))" "(?P<what>[^"]+)"$')
# todo remove this constraint </s> vs = [vs]	df_filter def df_filter(df, **kwargs): for k, vs in kwargs.items(): df = df[getattr(df, k).isin(vs)] return df
# todo(qos): register the callbacks to properly manage </s> pass	register_rpc
# todo: find a better way to identify if it is a parameter schema </s> return (v for item in instance for v in _validators.enum(	enum_validator def enum_validator(validator, enums, instance, schema): arrays, like query parameter (collectionFormat: multi) validator, enums, item, schema)) return _validators.enum(validator, enums, instance, schema)
# todo: implement shape inference for xladynamicslice </s> def _concatenate(*operands, dimension=none):	_concatenate
# todo: clean up py3k support </s> def emit(self, record):	NullHandler pass
# todo: raise alert for connection failure </s> return self.__register_secret_data(key, conn)	register_secret_data @private def register_secret_data(self, key, conn_data=None):
# todo(stephenfin): remove this as it's related to nova-network </s> instance.obj_reset_changes(['fault'])	fill_faults instance.fault = None
# todo: serialization of new version realtions is disabled </s> expected = {	test_relations_serialization recid_v2, record_v2 = deposit_v2.fetch_published() depid_v1, deposit_v1 = deposit_resolver.resolve(depid_v1_value) "version": [ {
# todo(mierdin): note that this will always return true if rbac is not enabled </s> (reject if invalid)	InquiriesController 1. Retrieve details of the inquiry via ID (i.e. params like schema) 2. Determine permission of this user to respond to this Inquiry 4. Update inquiry's execution result with a successful status and the validated response 5. Retrieve parent execution for the inquiry, and pass this to action_service.request_resume
).consume()  # todo see issue 170 </s> elbs = []	get_loadbalancer_data def get_loadbalancer_data(boto3_session, region): client = boto3_session.client('elb', region_name=region, config=_get_botocore_config()) for page in paginator.paginate(): elbs.extend(page['LoadBalancerDescriptions'])
# todo: there's a race with the initial "output" event. </s> {port_num!r},	TestsBase from ptvsd.debugger import debug debug( {debug_id!r}, {debug_options!r},
# todo: windows git class integration </s> auth=httpbasicauth(globalvars.github_username, globalvars.github_password),	GitManager "head": branch, "base": "master"} data=json.dumps(payload)) log('debug', response.json())
# todo: rebalance if output distributions are 1d instead of 1d_var </s> for item in items_list:	_process_df_build_map def _process_df_build_map(self, items_list): col_var = item[0].name assert col_var in self.str_const_table
# todo(b/150147476, b/150024785): fix tf.function in tf1 crash. </s> def wrapped_func(*args):	TfFunctionIfEagerDecorator self.func_kwargs = kwargs def __call__(self, func): if not hasattr(tf.compat.v1, "executing_eagerly_outside_functions" ) or tf.compat.v1.executing_eagerly_outside_functions():
# todo add verbose output </s> def localedir(self, new_localedir):	LocalizationModel@8 @localedir.setter
# todo: переделать механизм pairs </s> def string_score(arr):	string_score
# todo verify permission type for the provided resource type </s> :rtype: ``bool``	user_is_admin :type user_db: :class:`UserDB`
# todo - verify contents </s> response = self.client.get('/r/1')	testReviewDetail0 def testReviewDetail0(self):
# todo: remove this if/when we support rh mode. </s> return base_name.split(".")[0]	_attribute_from_filename def _attribute_from_filename(filename):
# todo stub </s> return evaluator_identity.evaluate()	metrics_identity @pytest.fixture  # type: ignore
# var_params = config.get_variable_params()  # todo: better way? </s> return path(str(path_template).format(tile_index=tile_index,	get_filename def get_filename(path_template, tile_index, start_time): start_time=start_time.strftime(date_format)))
# todo(ut) monkey patch </s> return self.data.get(key)	getChanged if default is not None and key not in self.data: self.data[key] = default
# todo(stubexecutor): customize self.stubbed_component_ids to replace components </s> base_stub_executor.basestubexecutor	get_stub_launcher_class stub_launcher.stubbed_component_map = dict(stubbed_component_map) for component_id in stubbed_component_ids: stub_launcher.test_data_dir = test_data_dir return stub_launcher
# todo: should be injected </s> self.facade._set_session(credentials)	IAM self.facade = AWSFacade() self.service = 'iam' await self._fetch_children(self, {})
# todo: check against plural_rules[lang]['nplurals'] </s> if not trans:	twtranslate break except KeyError: raise TranslationError("No English translation has been defined for TranslateWiki key %r" % twtitle) if code_needed:
# todo: implement </s> def _uciok(self):	_uciok
# todo: enable admin tests </s> def celery_beat(ctx, level='debug', schedule=none):	celery_beat cmd = 'celery beat -A framework.celery_tasks -l {0} --pidfile='.format(level) if schedule:
# todo remove in 4.0 </s> def __nonzero__(self):	__nonzero__
raise notimplementederror # todo </s> for s in self.states_list:	resample_states s.resample()
pass # todo </s> else:	_get_tool all_tools = self.image.window.tools if tool_id in all_tools: self.image.window.prompt_message(True, "Error: no tool " + tool_id)
#todo: python2 specific, remove </s> '-v', '--version', action='version',	get_args help='files to process', ) version='EXIF.py Version %s on Python%s' % (__version__, sys.version_info[0]), help='Display version information and exit'
# todo: remove this hack asap </s> 'lang': "en-us"	check_connection if not is_paired(): payload = { } ws.emit(Message("recognizer_loop:utterance", payload))
# todo(huangtianhua): remove this method when bug #1479641 is fixed. </s> update_args['description'] = prop_diff.get(self.description)	handle_update def handle_update(self, json_snippet, tmpl_diff, prop_diff): update_args = {} if self.NAME in prop_diff: update_args['name'] = prop_diff.get(self.NAME)
# todo use properties here to infer mechanism and purview from </s> return self._expand_repertoire('past', mechanism, purview, repertoire,	expand_cause_repertoire def expand_cause_repertoire(self, mechanism, purview, repertoire, cut): cut)
# todo: remove when #980 has been merged </s> return (qidx, prefer_http, format['video_bitrate'])	_sortkey def _sortkey(format): qidx = ['low', 'med', 'high', 'veryhigh'].index(format['3sat_qualityname'])
# because it's being configured too late -- bad! todo refactor! </s> nearest_project_dir = get_nearest_project_dir(args)	move_to_nearest_project_dir os.chdir(nearest_project_dir)
# todo unordered float </s> dst_o = m2_expr.exprcond(of,	jno e = [] meip = mRIP[instr.mode] n.zeroExtend(instr.mode), dst.zeroExtend(instr.mode))
#  todo: resolve symlinks etc </s> if s[start].isdigit():	_split_version_components start = 0 while start < len(s): while end < len(s) and s[end].isdigit(): end += 1
# todo: don't write them? is *much* slower on re-load (~3x) </s> self.update_module()	test__load__ def test__load__(self): with self.assertRaises(KeyError): self.loader[self.module_key + '2']
# todo (jian): double check why typing is crashing </s> data: union[dict, list]	ParamItem class ParamItem(NamedTuple):
# todo thread safe </s> def __init__(self,nodeid):	TheDevice class TheDevice(Device): This is the actual device object that you will use to get information from your real hardware Device.__init__(self,nodeId) self.counter=0
#todo make more easy n1, n2, n3 </s> self.tabs["collector"]["package_view"].setcontextmenupolicy(qt.customcontextmenu)	init_tabs self.tabs["collector"]["l"].addWidget(self.tabs["collector"]["package_view"], 0, 0) self.tabs["collector"]["l"].addWidget(toQueue, 1, 0) self.tabs["queue"]["view"].setContextMenuPolicy(Qt.CustomContextMenu) self.tabs["log"]["l"] = QGridLayout()
# todo: write this method if possible </s> res = self.proxy.getrawtransaction(txid)	getrawtransaction return res
# todo(dcramer): ideally we could just send the signal to the subprocess </s> text_len = len(text)	save_chunk def save_chunk(self, text): db.session.add(LogChunk( task_id=self.task_id,
# @todo: investigate why this isn't working </s> redirect(url(f="server"))	setup_monitor_server_disable_interactive S3Method for interactive requests result = setup_monitor_server_disable(r.id)
# todo: dump to file </s> return regex.match(value) is not none	regexp if type(expr) != str: expr = str(expr)
# todo: migrate to new tilegrid format via library. </s> def isenum(segtype, tag):	isenum
# todo: test coverage of this branch </s> return render_to_response(	subscribe_user 'newsletter': my_newsletter, 'action': 'subscribe' "newsletter/subscription_subscribe_user.html", env, context_instance=RequestContext(request))
# todo this needs to be done on the content but seems to be a non-trival </s> if context.verbose:	pipe_xpathfetchpage@44 from lxml import etree root = etree.HTML(content) print "XPathFetchPage: found count items:",len(res_items) for res_item in res_items:
# todo(mattjj): re-enable </s> mass = 0.9	testMomentumVectorInverseTimeDecayStaircaseSchedule step_sched = optimizers.inverse_time_decay(0.1, 3, 2., staircase=True)
# todo: use the xmlrpc-c type indicators instead / additionally </s> finally:	RtorrentXmlRpc elif hasattr(result, "__iter__"): result = '\n'.join(i if isinstance(i, basestring) else pformat(i) for i in result) if tmp_import and os.path.exists(tmp_import): os.remove(tmp_import)
# todo: prompt </s> rstr += le.border ()	redraw rstr = u'' rstr += pager.border() rstr += le.refresh () return rstr
# todo: provide more informative errors </s> try:	make_alice_control def create_policy(): Character control endpoint for creating a policy and making bob_pubkey = bytes.fromhex(request.args['bob_encrypting_key']) label = bytes.fromhex(request.args['label'])
# todo: remove this function </s> if key in self.hosts:	__getitem__@110 'Use of Inventory[<host_name>] is deprecated, ' 'please use `Inventory.get_host` instead.' return self.hosts[key] raise NoHostError('No such host: {0}'.format(key))
# todo: avoid use of _objects_by_key_id method </s> acckey = qr.first()	_get_account_defaults filter_by(wallet_id=self.wallet_id, purpose=self.purpose, depth=3, network_name=network) if account_id is not None: if len(qr.all()) > 1: _logger.warning("No account_id specified and more than one account found for this network %s. "
# todo: add at least reflection tests before adding notimplemented version </s> else:	playlistinfo the entire playlist if parameter is None or parameter == '-1': tracklist_slice = protocol.RANGE(parameter) start, end = tracklist_slice.start, tracklist_slice.stop
# :todo: implement test. </s> super(sendtransfercommandtestcase, self).setup()	SendTransferCommandTestCase class SendTransferCommandTestCase(TestCase): self.adapter = MockAdapter() def test_wireup(self):
# todo: save additional models </s> if protected_methods is none:	Authentication class Authentication(object): protected_methods = ['POST', 'PUT', 'DELETE'] self.protected_methods = protected_methods
# todo: this check may hide a bug a should be removed. </s> "default" is the presumed answer if the user just hits <enter>.	query_yes_no def query_yes_no(question, default="no"): Ask a yes/no question via raw_input() and return their answer. It must be "yes" (the default), "no" or None (meaning an answer is required of the user).
# todo: preallocate! </s> class k(object):	SortByX0 return c1.x0[0] - c2.x0[0] return c1.x0[1] - c2.x0[1] def __init__(self, obj, *args): self.obj = obj
# todo: unit test this </s> prop_type: int,	get_property def get_property(self, win: Union[Gdk.Window, Wnck.Window, int], empty: Any=None): :param win: A GTK or Wnck Window object or a raw X11 window ID.
# todo: funcname, funcbody </s> self.asserttrue(isinstance(node, parser.fieldexp))	testFieldExp self.assertEqual(1, p._pos)
# todo: consider returning an empty {} rather than raising </s> eq_(unique1.issuperset(unique2), false)	test_issuperset eq_(super_.issuperset(sub_), True) eq_(twin1.issuperset(twin2), True) eq_(unique2.issuperset(unique1), False) not_an_identity_set = object()
#todo redownload and verify against original? </s> found = [s for s in songs if s['id'] == self.song.sid] or none	_assert_get_song def _assert_get_song(self, sid): (GM has no native get ability for songs, just list).""" assert_is_not_none(found) assert_equal(len(found), 1)
# todo test me ! </s> return dictionary.get(key)	dict_get@6 @register.filter def dict_get(dictionary, key): except: return []
pass # todo </s> def collect_incoming_data(self, data):	collect_incoming_data
#todo: this is identical to regex in line 33 of subscribe.py! </s> def _handle_autoreply(bot, event, command):	_handle_autoreply autoreplies_list = bot.get_config_suboption(event.conv_id, 'autoreplies') if autoreplies_list:
# todo: only allow this if the task is still in error state </s> return u'<task %s>' % self.func	__repr__
# todo: add for morph targets data. </s> translation = convert_swizzle_location(translation)	decompose_transition def decompose_transition(matrix, context, export_settings): translation, rotation, scale = matrix.decompose() rotation = convert_swizzle_rotation(rotation) scale =  convert_swizzle_scale(scale)
# todo: enable specificity beyond hostname (e.g. include scheme, port) </s> raise notimplementederror	go_away
# todo: this causes a blank window to be shown. </s> self.interface.factory.not_implemented('app.current_window()')	current_window
# xxx todo </s> def input_integer(self, token):	input_integer
# todo: handle timeout </s> def listeningport(self, listeningport):	TCPClient return self.__listeningPort @listeningPort.setter if listeningPort is None: raise TypeError("ListeningPort cannot be None")
# todo: matthewp, profile this transfer </s> logits : ``torch.floattensor``, required.	sequence_cross_entropy_with_logits in the sequence. This allows loss computations for models which use padding. Parameters A ``torch.FloatTensor`` of size (batch_size, sequence_length, num_classes) which contains the unnormalized probability for each class.
# todo: in the future you can also add the possibility to synchronize from a chosen profile </s> def view_esn(self, pathitems=none):  # pylint: disable=unused-argument	view_esn
# todo: remove this method in v2.5 </s> resource = self.client.api.tm.gtm.datacenters.datacenter.load(	remove_from_device name=self.want.name, partition=self.want.partition
# todo(ringw): fix barline detection here. </s> score_reader.read_system(page.system[1])	testIMSLP39661_keySignature_CSharpMinor def testIMSLP39661_keySignature_CSharpMinor(self): page = engine.OMREngine().run(_get_imslp_path('IMSLP39661-000.png')).page[0] treble_sig = score_reader.score_state.staves[0].get_key_signature() self.assertEqual(treble_sig.get_type(), musicscore_pb2.Glyph.SHARP)
# todo: establish what's the increment of ctc decoder (how many new letters might be there) [rough estimate] </s> self.logzero = -10000000000.0	E2E self.report_cer = False self.report_wer = False self.loss = None self.acc = None
# todo: this doesn't always work. for some scores where a part uses more than one clef, more </s> pass	MeiValidityError class MeiValidityError(exceptions21.Music21Exception):
# todo(shardy): remove when we no longer support essex </s> nova.authenticate()	authenticate@86 proxy_tenant_id=con.tenant_id, service_type=service_type, return nova
# todo: test me @jmcarp </s> sha=sha,	github_branch_widget rendered = github_branch_template.render( branches=[each.name for each in branches], owner=owner, repo=repo
# todo: this needs to be deferred but for now we hard code </s> else:	buildFilter filter = None elif len(converted) == 1: operand = ("|" if operand == "or" else "&") filter = "(%s%s)" % (operand, "".join(converted))
# todo: 验证 localport 有效性 </s> sys.exit(0)	main@102 args = parser.parse_args() if args.version: config = Config(None, None, None, None, None) if args.c:
# todo not portable, redo. </s> self.assertalmostequal(2.0, np.std(collected), places=1)	TestExplorations for _ in range_(1000): test.test("get_noise", fn_test=lambda component_test, outs: collected.append(outs)) np.random.seed(10) input_ = nn_output_space.sample(size=3)
pass  # todo </s> @after_class(always_run=true)	NoUpauthTests def login(self): self.api = test_utils.init(perform_upload_auth=False) def logout(self): assert_true(self.api.logout())
# todo: skipped due to gh-4436 </s> assert_raises(valueerror, ds.create_sibling_ria, 'ria+file:///some/where',	test_invalid_calls def test_invalid_calls(path): ds = Dataset(path).create() name='some', storage_name='some')
# todo: getattr doesn't return by default members </s> except valueerror:	visit_callfunc self._check_uninferable_callfunc(node) try: return num_positional_args += implicit_args
# todo: error detection </s> mongodb = connection('localhost', 27017)['cobbler']	__connect def __connect():
# todo: wrap backend call in error handling. </s> returns a :class:`mopidy.models.tltrack` or :class:`none`.	get_current_tl_track def get_current_tl_track(self):
#todo delete backward compatibility check after some versions </s> counter += 1	process_web_assets self.identify_scans_to_process() if self.scans_to_process.shape[0]: r = app[1] self.logger.info('Processing {}/{}'.format(counter, len(self.scans_to_process)))
# todo: launch visitor on node </s> codeobj = compile(fixed, '<string>', 'exec')	SemBuilder body=body, decorator_list=[])]) ctx = self.ctx.copy() eval(codeobj, ctx)
# todo: handle this case properly </s> return phlgit_log.make_revisions_from_hashes(self, hashes)	make_revisions_from_hashes from any of 'hashes'. :hashes: a list of commit hash strings
# todo results from p0f </s> return endpoint.machine.name.strip()	_get_name @staticmethod
raise notimplementederror  # todo ... </s> exit()	_at_exit_handler def _at_exit_handler(): if not isExited: print("All threads:") import Debug
# todo: fix this assertionerror: eol while scanning string literal </s> x = bytearray([1,2,3])	test_setattr def test_setattr(self): try: x.attr = 42
# todo: also preserve __module__, __name__ and a few other important attrs </s> self.stats = pstats.stats(profile())	FuncProfile if isinstance(self.sort, str): self.sort = (self.sort, ) self.ncalls = 0 self.skip = skip
# todo: support multiple sourcestamps </s> defer.returnvalue(path_to_authzfail(req))	StopBuildActionResource authz = self.getAuthz(req) res = yield authz.actionAllowed(self.action, req, self.build_status) return b = self.build_status
# forward all other methods. todo(l.zou): could use a proxy to automate these </s> def apply_gradients(self, *args, **kwargs):	apply_gradients
# todo: will be replaced with exception in future releases </s> return self.to_json(), {self._media_name: self.media}	convert_input_media def convert_input_media(self): if util.is_string(self.media):
#todo: why can't we read emboss's swiss output? </s> self.assertequal(str(target.seq).upper(), \	pairwise_alignment_check in str(target.seq).upper()) else : str(alignment[1].seq).replace("-","").upper()) return True
# todo(b/148082271): remove this line once tft 0.22 is used. </s> learning_rate=learning_rate)	_build_keras_model dnn_hidden_units=hidden_units,
# todo: make test method </s> return event_loop()	test_gps def test_gps(): droid.startLocating() finally: droid.stopLocating()
raise invalidpocketleveldbworldexception()  # todo maybe try convert/load old pe world? </s> :param box pymclevel.box.boundingbox	deleteChunksInBox def deleteChunksInBox(self, box): :return: None logger.info(u"Deleting {0} chunks in {1}".format((box.maxcx - box.mincx) * (box.maxcz - box.mincz),
# todo: somehow caused by circular import under python3 refactor </s> self.stopserver()	killServer
"meta.deleted": false,  # todo(tsileo): retrieve deleted and expose tombstone </s> return bleach.clean(html, tags=allowed_tags)	clean_html
# todo: total hack below. implement more principled formatting. </s> summarize_config: whether to save a summary of the operative config that	GinConfigSaverHook base_name: The base name (name excluding path and extension) of the file where this hook will write the operative config. Also used as the will be loaded by TensorBoard and displayed in its "Text" tab. summary_writer: A tf.summary.FileWriter object to use for writing
# todo implement project_out and uncomment this </s> optimisation, update_step, eps)	ImageLucasKanade image, residual, transform, interpolator,
# todo: floats </s> block_level_width(box, containing_block)	block_replaced_width def block_replaced_width(box, containing_block, device_size):
# todo: make these http requests asynchronous. not easy since we don't </s> if screen_name is none:	Twitter def get_actor(self, screen_name=None): Args: url = API_CURRENT_USER_URL else:
# todo: this is an ugly workaround. remove after refactoring the player and titlebar. </s> self.status_label = self.ui.get_object("status_label")	Titlebar self.progress_scale.set_increments(30.0, 60.0) self.progress_scale.set_visible(False) self.update_progress_bar = self.ui.get_object("update_progress_bar") self.throbber = self.ui.get_object("spinner")
## todo: # fixme: remove me </s> line = str.upper(line)	is_sql_injection def is_sql_injection(url_parsed): return re.search(SQLI_REGEX, url_parsed, re.I) is not None
# todo(blee): this should be derived from the dataset size. </s> self._summary_writer.flush()	_WriteSummaries if value.HasField('simple_value'): tf.logging.info('%s summary on checkpoint@%d %s = %.8g', job_name,
# todo private access.. </s> and metaclass.get_root_context().py__name__() == 'django.db.models.base':	get_metaclass_filters def wrapper(cls, metaclasses): for metaclass in metaclasses: return [_new_dict_filter(cls)] return func(cls, metaclasses)
# todo: we currently use the same data for test and validation. </s> def should_generate_eval_dataset(self) -> bool:	should_generate_eval_dataset @abc.abstractmethod
# todo: warn/error: check if this var has units: assigning </s> for vardata in self.values():	setlb def setlb(self, val): vardata.setlb(val)
@skipif('device-openmp')  # todo: still unsupported with openmp, but soon will be </s> assert trees[0][0] is trees[1][0]	TestCodeGeneration op = Operator(eq) trees = retrieve_iteration_tree(op) assert trees[0][1] is not trees[1][1]
# todo: rework resolver install system to log and report what has been done. </s> galaxy.queue_worker.send_control_task( trans.app, 'reload_tool', noop_self=true, kwargs={ 'tool_id': id } )	reload def reload( self, trans, id, **kwd ): GET /api/tools/{tool_id}/reload message, status = trans.app.toolbox.reload_tool_by_id( id ) return { status: message }
# todo(zaneb): ensure parameters can be formatted for xml </s> params = self.data.items()	args def args(self): return dict((k, v) for k, v in params if k not in self.PARAMS)
# todo: temporary work around to issue #225 on github </s> ['v[t, x, y, z] + w[t, x, y, z] + 5.0']),	test_xreplace_constrained_time_varying @pytest.mark.parametrize('expr,expected', [ ('Eq(u, v*w*4.*ti0 + ti1*v)',  # more ops ['4.0*v[t, x, y, z]*w[t, x, y, z]']),
# todo autoescape context </s> def massaman(function, file_name):	dashboard register = template.get_library("apps.webui.templatetags.dashboard") register.tags.update({ position : massaman(f, path) }) a template file (for making fake templatetags). Code is poached from the InclusionNode class in __init__.py
pass # todo </s> return t == 'bool'	is_bool
# todo: must be implemented </s> pass	should_fetch_kindlegen
#todo print appropriate error message </s> <dd>registers multiple elements ($elem1$, ...) and their corresponding converter functions ($conditionalfunction1$, ...) in addition to the $defaultfunction$.	RegisterImport <dt>'RegisterImport["$format$", $defaultFunction$]' <dd>register '$defaultFunction$' as the default function used when importing from a file of type '"$format$"'. <dt>'RegisterImport["$format$", {"$conditionalFunctions$, $defaultFunction$, "$elem3$" :> $postFunction3$, "$elem4$" :> $postFunction4$, ...}]' <dd>also registers additional elements ($elem3$, ...) whose converters ($postFunction3$, ...) act on output from the low-level funcions.
# todo implement support for this </s> specular = materialdata['specularcolor']	exportSDFMaterial diffuse = materialdata['diffuseColor'] tagger.attrib('diffuse', '{0} {1} {2} {3}'.format( tagger.attrib('specular', '{0} {1} {2} {3}'.format( specular['r'], specular['g'], specular['b'], 1.0))
# todo(sloria): test me </s> if item['is_dir']:	build_dropbox_urls def build_dropbox_urls(item, node): return { 'upload': node.api_url_for('dropbox_upload', path=path),
federated_only=federated_only,  # todo: 289 </s> port=self.rest_information()[0].port)	get_deployer deployer = self._crypto_power.power_ups(TLSHostingPower).get_deployer(rest_app=self.rest_app,
raise notimplementederror  # todo </s> :rtype: bool	have_more_data def have_more_data(self, session): :param tf.Session session: assert self.current_dataset_name return True
# todo(vek): need to pass context in for access to auth_token </s> def __init__(self, name, state):	InstanceInfo self.name = name assert state in power_state.valid_states(), "Bad state: %s" % state
# todo(benjaoming) for 0.15, remove this </s> def validate_username(username):	validate_username
# todo: with git <= 2.3 keep old mechanism: </s> def git_get_active_branch(self):	git_get_active_branch
data_source_name='case-sql',  # todo: this isn't really needed. </s> def publish_form_saved(form):	publish_form_saved
# todo: handle errors from _common_run? </s> add("--enroll-autorenew", default=none, action="store_true",	_paths_parser help=config_help("cert_path")) add("--chain-path", default=flag_default("chain_path"), help=config_help("enroll_autorenew")) return parser
# todo: support complex </s> def test_choose_conv_method_int(self, dtype):	test_choose_conv_method_int a = testing.shaped_arange((10,), cupy, dtype) b = testing.shaped_arange((5,), cupy, dtype)
# :todo: implement test. </s> self.assertequal(	MultisigAddressBuilderTestCase builder.add_digest(self.digest_1) builder.add_digest(self.digest_2) addy, Address(
# todo: template requires address otherwise it throws an exception </s> 'product': line.product,	test_view_change_order_line_quantity order=order_with_items_and_stock).comment == ( 'Changed quantity for product %(product)s from' 'old_quantity': line_quantity_before_quantity_change, 'new_quantity': 2}
# todo do a proper mro resolution. currently we are just listing </s> super(classname, self).__init__(parent_context, tree_name)	ClassName class ClassName(TreeNameDefinition): self._name_context = name_context self._apply_decorators = apply_decorators
# todo: log exception </s> except exception as e:	delete@485 id=report_id ) return False
# todo: should assert that domain exists here but this breaks tests </s> def get_all_owner_ids_from_group(group):	get_all_owner_ids_from_group
# todo: additional treatment for "too many arguments"? although </s> def get_last_commit_hash(self, files):	get_last_commit_hash paths""" try:
# todo: handle fancy-index copies by allocating a buffer and </s> >>> newforcedevenclass = as_even(sequentialsubsetiterator)	ForcedEvenIterator >>> even_iterator = NewForcedEvenClass(dataset_size=100, ...     batch_size=30, num_batches=None) >>> even_iterator = NewForcedEvenClass(dataset_size=100, ...     batch_size=30, num_batches=None)
# todo: test training too </s> shared_resources = sharedresources(vocab(), {})	test_multi_support_fixed_class_inputs data_set = [ (QASetting("Where is the cat?", ["the cat is on the mat."]), [Answer("mat")]) input_module = MultiSupportFixedClassInputs(shared_resources) input_module.setup_from_data(data_set)
# todo: specific exception handling </s> def get_event_types(self):	get_event_types
# todo: actually read newtablename. </s> conn = psycopg2.connect('dbname=sgeadmin user=sgeadmin')	delete_chain def delete_chain(self, tablename, chain_index): cur = conn.cursor() cur.execute("SELECT tableid FROM preddb.table_index WHERE tablename='%s';" % tablename)
# todo: remove logging </s> "transactional/spam-email.txt",	handle_spam mailbox.email, f"Email from {contact.website_email} to {alias.email} is detected as spam", alias=alias, website_email=contact.website_email,
continue  # todo: 168 check version and update if required. </s> status_code=404, headers=headers)	provide_treasure_map response = Response("No Treasure Map with ID {}".format(treasure_map_id),
# todo(dolph): can be uncommented pending bug 968519 </s> self.identity_api.create_user('fake', user)	test_delete_user_with_tenant_association user = {'id': 'fake', 'name': 'fakeuser', self.identity_api.add_user_to_tenant(self.tenant_bar['id'], user['id'])
# todo add test </s> return tuple([recursive_negate(v_) for v_ in v])	recursive_negate elif isinstance(v, iap.StochasticParameter): return iap.Multiply(v, -1) elif isinstance(v, list): return [recursive_negate(v_) for v_ in v]
# todo: make a function and more generic (move to psutil) </s> get_avail_gov = s.getoutput("cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_available_governors")	avail_gov get_gov_state = s.getoutput("cpufreqctl --governor") gov_state = get_gov_state.split()[0]
# todo support multiple backends </s> :param name: name of the new playlist	StoredPlaylistsController self.backends[0].stored_playlists.playlists = playlists def create(self, name): :type name: string :rtype: :class:`mopidy.models.Playlist`
# todo: boulder messes up content-type #56 </s> new_authz_uri=new_authz_uri,	_regr_from_response return messages2.RegistrationResource( body=messages2.Registration.from_json(response.json()), terms_of_service=terms_of_service)
# todo: log exception </s> z = zipfile.zipfile(fname)	_main@994 for fname in parsedlist: if zipfile.is_zipfile(fname): if PY3: args.password = bytes(args.password, 'utf-8')
if result.was_accepted:  # todo: here, we need to assess the result and see if we're actually good to go. </s> return msgpack.dumps(self.ids)	packed_payload
self.my_sender('text/cache-manifest', bytes(manifest, 'utf-8')) # todo: cache-control/last-modified headers </s> del safe_settings['http']	api_view_settings try: del safe_settings['Root_Check'] del safe_settings['DNS_Interface_IP'] del safe_settings['DNS_Port']
# todo: semi-bounded -> exponential distribution. </s> def get_action_layer_output(self, nn_input, internal_states=none):	get_action_layer_output Args: nn_input (any): The input to our neural network.
# todo(b/160795287): deprecate estimator based executor. </s> eval_batch_size = 10	trainer_fn - eval_spec: Spec for eval. - eval_input_receiver_fn: Input function for eval. train_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda trainer_fn_args.train_files,
# todo ... </s> v = state.vars["v"]	test_parse_var_decl_body def test_parse_var_decl_body(): assert isinstance(v.body, CStatement) value = v.body._leftexpr
# todo fix. </s> ctx_init = self.__init_context()	test_cmova def test_cmova(self): x86_ctx_out, reil_ctx_out = self.__run_code(asm, 0xdeadbeef, ctx_init) cmp_result = self.__compare_contexts(ctx_init, x86_ctx_out, reil_ctx_out)
# todo(mordred): special casing auth_url here. we should </s> _auth_update(cloud, our_cloud)	_get_base_cloud_config name)) if 'auth' not in cloud: if 'cloud' in cloud: del cloud['cloud']
# todo make sure this works </s> pass	createGroup
# todo: needs more testing </s> return 0.75	false_probability
# todo(inf) commented out lines were only in fnv branch </s> file_menu.links.append(file_redate())	InitBSALinks file_menu = MenuLink(_(u'File..')) file_menu.links.append(File_Duplicate()) file_menu.links.append(balt.UIList_Delete()) file_menu.links.append(SeparatorLink())
raise notimplementederror # todo </s> if p < pval:	assert_populations_eq_means _,p = stats.two_sample_t_statistic(pop1,pop2)
# todo: 判断返回结果，处理异常 </s> assets = asset.objects.all()	perm_role_push header_title, path1, path2 = "系统角色", "角色管理", "角色推送" role_id = request.GET.get('id') asset_groups = AssetGroup.objects.all() if request.method == "POST":
pass  # todo </s> parser = argparse.argumentparser()	get_parser Arqument parser for CLI. functions : :class:`argparse._SubParsersAction` parser.add_argument( "-p",
# todo pseudo code: </s> pass	Seeked @dbus.service.signal(dbus_interface=player_interface, signature='x')
except exception:  # todo - which exceptions? </s> for organism_element in element:	_parse_organismHost if organism_element.tag == NS + 'name': append_to_annotations("organism_host", organism_element.text)
# todo: can only set before calling go() </s> lb.insert(end, item)	addListItem def addListItem(self, title, item): items = lb.curselection() if len(items) > 0: lb.selection_clear(items)
# todo add read lock </s> data_dir = settings.data_folder_path	init_data_files data_files = [] data_files_size = 0 if os.path.isdir(data_dir): for file in os.listdir(data_dir):
raise exceptions.mpdnotimplemented  # todo </s> reads messages for this client. the response is a list of "channel:"	readmessages@57 def readmessages(context): *musicpd.org, client to client section:* and "message:" lines. raise exceptions.MpdNotImplemented  # TODO
# todo should we instead pick the best result, instead of just the first? </s> }	get_trakt_data 'year': item.year
#todo: rewrite this without get_block_data() </s> self.refine_by = 2	AMRVACDataset super(AMRVACDataset, self).__init__(filename, dataset_type, units_override=units_override) def _set_code_unit_attributes(self):
pass # todo: raise exception </s> return ""	strdate_to_time continue locale.setlocale(locale.LC_TIME, '') return time.mktime(new_date)
# todo: returning httpresponse seems dirty. see if it can be </s> ``form_template_name`` field, or the ``form_template`` attr defined on	get_form_template def get_form_template(self, request): the page type model. :param django.http.HttpRequest request:
# todo: remove this </s> def locator(self):	locator return {'locationId': self.name}
# todo instead of 3*t, use log_sf </s> dur_distn = self.dur_distns[self.stateseq_norep[-1]]	HSMMStatesEigen dur = self.durations[-1]
if util.pythonise(pipe['wires'][wire]['tgt']['moduleid']) == module_id and pipe['wires'][wire]['tgt']['id'] == '_input' and pipe['wires'][wire]['src']['id'].startswith('_output'): # todo? this equates the outputs </s> embed = module['conf']['embed']['value']	_parse_pipe pipe['modules'][util.pythonise(module['id'])] = module pipe['graph'][util.pythonise(module['id'])] = [] pipe['modules'][util.pythonise(embed['id'])] = embed pipe['graph'][util.pythonise(embed['id'])] = []
1  # todo: fill in identifier </s> stack = [line]	print_stats total=stat.ttot, average=stat.tavg, if stat.children is not None: for child in stat.children:
1  # todo: fill in identifier </s> total=line.total,	print_stats align=_line(line.recursion), name=line.name, cumulative=line.cumulative, avg=line.average,
# todo: ensure the same callback logic as in set_settled </s> def channel_address(self):	channel_address @property
# todo(tlashchova): remove this method when mistralclient>1.0.0 is used. </s> def _cron_trigger_name(self):	_cron_trigger_name
# todo: how do we know something has changed? </s> return handle(handlerepobranchbackend(self, branch))	get_handle
# todo: parse the field contents </s> ae(r'xxx \y a', {'text':'xxx', 'yomi':'a'})	test_xe ae = lambda x, y: self.assertEqual(parse_xe(x, None), y) ae(r'"some name"', {'text':'some name'})
# todo fix the fold to allow any number of dispositions </s> "{}{} peaks at {:,} on day {}{}".format(	chart_descriptions else: on += 1  # 0 index issue labels[col], suffix,
# todo error handling. </s> ['git', 'show', hash_array[0]],	merge@44 ) nb_local = parser.parse(local_show.stdout) stdout=subprocess.PIPE )
assert study_id == in_memory_storage_study_id  # todo(akiba) </s> def get_study_uuid(self, study_id):	get_study_uuid
# todo support domain delegation, which will allow us to set a sub-account to execute as. we can then </s> service hook connection.	_authorize@34 def _authorize(self): connection_info = self.get_connection(self.conn_id) connection_extras = connection_info.extra_dejson
# todo: implement this method </s> def cb_wrapper(uv_signal_t, signum):	cb_wrapper
# todo check if for each column, all rows have equal-index series </s> if self.sparse_output_:	TSColumnTransformer Supports input types (X): list of numpy arrays, sparse arrays and DataFrames return sparse.hstack(Xs).tocsr() if self.preserve_dataframe and (pd.Series in types or pd.DataFrame in types):
# todo(b/138406006): remove the narrower dependency for pyarrow </s> 'tensorflow>=1.14,<2',	make_required_test_packages 'apache-airflow>=1.10,<2', 'kfp>=0.1.18,<0.2; python_version >= "3.0"', 'tzlocal>=1.5,<2.0',
content=content,  # todo(tsileo): handle markdown </s> return bleach.clean(html, tags=allowed_tags)	clean_html
# todo: we should probably have a special folder just for header </s> msvcrt.locking(f.fileno(), msvcrt.lk_rlck,	CythonExtensionManager lock_file = os.path.join(lib_dir, module_name + '.lock') with open(lock_file, 'w') as f: os.stat(lock_file).st_size) else:
# todo: does it get closed properly after process gets killed? </s> raise connectionfailedexception(msg)	_resolve_executable msg = "Executable '%s' not found. Please check your configuration!" % executable if not executable.startswith("/"):
# todo: try /usr/bin/curl instead? </s> characters (>)."""	html_decode html_chars = ( ("'", "&#39;"),
"""todo: document and test""" </s> keyword argument:	get_str_attribute or_else -- value to return if the entity does not have the give attribute. (default None)
# todo make this smarter about more complex configurations (backup original values, etc) </s> self.logger.warning('unable to mirror {0} on {1}, mirror port not defined on that switch'.format(	Parser ok = False else: str(port), str(switch_found))) ok = False
# todo: determine proper template to use. </s> out, err = proc.communicate()	get_exitcode_stdout_stderr def get_exitcode_stdout_stderr(cmd): args = shlex.split(cmd) exitcode = proc.returncode return exitcode, out, err
# todo: need a more specific colour here; conflict is wrong </s> self.toggle_scroll_lock(not button.get_active())	on_lock_button_toggled
# todo(b/123952794): migrate to v2 function. </s> yield	_subTest with self.subTest(sub_test_str):
# todo: filter according to what names the current file has in scope. </s> for filename in haskell_source_files:	refresh_all_module_info "Rebuild module information for all files under the specified directory." files_in_dir = list_files_in_dir_recursively(base_dir) self.refresh_module_info(filename)
#add to favorites tag --> todo translated label for favorites ? </s> if setid == none:	addBoxsetToKodiLibrary elif artwork[update_type] != '': setartsql = "INSERT INTO art(media_id, media_type, type, url) VALUES(?,?,?,?)" setssql="INSERT INTO sets (idSet, strSet)  values(?, ?)" cursor.execute(setssql, (None,strSet))
# todo(kan-bayashi): need to make more smart way </s> def zero_state(self, hpad):	zero_state
# todo: write this in human </s> except exception:	resize_image size = max_size, max_size try: exif = None if exif is not None:
# todo: automate this (by lookup from nn). </s> return_ += r	_calc_mean_return returns = list() return_ = 0.0 if t: returns.append(return_)
# todo(dcramer): once this goes live in production, we can kill the pickle path </s> '&'.	_make_key model._meta, md5_text(
# todo: log exception </s> continue	scan@71 while file not in filelist and result: file = file + ' ' + result.pop(0) result = result[-1] results.append((file, result))
# todo !!! </s> if col not in self.include:	convert_element if not isinstance(col, expression.ColumnElement): return None return None if self.exclude is not None:
# todo: handle base64 data another way? </s> if ot != y["output_type"]:	compare_output_approximate def compare_output_approximate(x, y): "Compare type and data of output cells x,y approximately." return False xkeys = set(x)
# todo(vek): need to pass context in for access to auth_token </s> def __init__(self, name, state):	InstanceInfo self.name = name assert state in power_state.valid_states(), "Bad state: %s" % state
# todo: better parallel sort test </s> a3 = pd.concat([df1, df2])	test_concat_str def test_impl(): df1 = pq.read_table('example.parquet').to_pandas() return (A3.two=='foo').sum() hpat_func = hpat.jit(test_impl)
#todo: remove expressions </s> invertedmordent.__init__(self)	WholeStepInvertedMordent def __init__(self):
# todo: simple_test_builtin should this as status=2. </s> span_id = word_.spanidfromerror(e)	_ValToArithOrError raise else: self.errfmt.PrettyPrintError(e, prefix='warning: ') return i
# todo merge sort on large dataset!!! </s> def __init__(self, source, spec, missing=none):	CutView self.source = source self.spec = spec
# todo: strs->index() has a redundant check for (i < 0) </s> raise assertionerror()	OldValue else: val = value.Str(s) return val
# todo(ntonci): add a check for small motion </s> dq_w_b = dq_b_w.inverse()	compute_dual_quaternions_with_offset def compute_dual_quaternions_with_offset(dq_B_H_vec, dq_H_E, dq_B_W): n_samples = len(dq_B_H_vec) for i in range(0, n_samples): dq_B_H = dq_B_H_vec[i]
# todo: this is highly inefficient if more properties are accessed; </s> def ratio(self):	ratio return self._ratio
# todo: fix self.cursor_x >= w </s> self.comm = self.comm[:i]	inline_k_ctrl_k return i
# todo: log exception </s> config.set(modname, key, str(conf[key]))	_rewrite_config@456 continue Config.add_section(modname) Config.add_section('main') for key in DEFAULTCONF:
# xxx todo </s> @type msg:  str	addNote def addNote(self, msg): @param msg: Note text. self.notes.append(msg)
# * todo heading 1 --> </s> vim.current.window.cursor = (1, 0)	test_toggle_todo_with_no_heading Todo.toggle_todo_state() self.assertEqual(vim.current.buffer[0], '')
# todo, i would prefer to query if the language was found... </s> returns:	update_model_in_repo_based_on_filename Args: model: the model to be added. If the model the filename of the model added to the repo if model._tx_filename is None:
# todo: remove </s> else:	Spider yield None else: if not self._preprocess_task(task): continue
# todo: kwargs </s> return lambda df: len(df._data[0])	df_len_overload def df_len_overload(df): if len(df.columns) == 0:  # empty df
# todo check the dataset </s> args.tf_manager, args.runners,	translate@34 _, response_data = run_on_dataset(
# todo pass zk node version to make sure we still own this node </s> while true:	checker time.sleep(120) if not self._running:
# todo: replace this with a simpler environment where we can actually test if it finds a solution </s> x = activation('linear')(x)	test_ddpg@57 x = Dense(16)(x) x = Activation('relu')(x) critic = Model(input=[action_input, observation_input], output=x) memory = SequentialMemory(limit=1000)
# todo tests for this </s> parsed_version = parse_version(version)	get_installable_version def get_installable_version(version): Get the version string of the latest version of Flocker which can be return parsed_version.installable_release
# todo: see collections </s> return handle(self.get_handles_data()[name][1])	get_handle
# todo: tf2.0 not stable, cannot import tensorflow.contrib.eager.python.saver </s> return value	get_confirm_token if key.startswith('download_warning'):
# todo: add for morph targets data. </s> return translation, rotation, scale	decompose_transition rotation = convert_swizzle_rotation(rotation) scale =  convert_swizzle_scale(scale)
# todo(karita) use torch.no_grad here </s> def zero_state(self, batchsize):	zero_state
# todo: bytes vs str </s> val = int(s.group(1), 16)	unquote_plus def unquote_plus(s): return chr(val) return qu_re.sub(decode, s.replace("+", " "))
# todo: remove this older form of error handling. </s> def boilmain(main_argv):	BoilMain
# todo remove </s> par = self.var.parent	parent @common.safe_property @memoize_default() if isinstance(par, Class) and par == self.instance.base \ or isinstance(par, pr.Class) \
# todo: explicitly commit files by name </s> exitcode, output, error = run_command(["bzr", "version"])	is_available def is_available():
# todo(hartikainen): once tfp.bijectors.chain supports conditioning, </s> return fldj	ConditionalRealNVPFlow if event_ndims_ is not None: event_ndims = event_ndims_ return log_det_jacobian def _inverse_log_det_jacobian(self, y, **condition_kwargs):
# todo: disconnect </s> def listen_maddr_with_peer_id(self) -> multiaddr:	listen_maddr_with_peer_id return self.listen_maddr.encapsulate(Multiaddr(f"/p2p/{self.peer_id}"))
# todo: check whether the name is already used or not </s> clean_nodes(material.node_tree.nodes)	add_material@544 if use_nodes and make_node_tree_empty:
# todo(tr3buchet): fix function call after refactor </s> log.debug(_("injecting file path: '%s'") % path)	_inject_onset_files % onset_files) onset_files = [] self.inject_file(instance, path, contents)
# todo: make sure requesting user is owner of the build_id </s> runs = db.relationship(	Release build_id = db.Column(db.Integer, db.ForeignKey('build.id'))
# todo: if build and the following command fails "podman inspect -t image <image_name>" then run build </s> move port forwarding from containers to dst (a pod or a infra container)	move_port_fw move_list(dst, containers, "ports")
# todo sp: updatable attributes ? </s> raise attributeerror("dbextra {} does not exist".format(	del_extra "The extras of a node can be set and deleted " "only after storing the node") key)) return DbExtra.del_value_for_node(self.dbnode, key)
# todo imageio single frame seek seems slow. look into this </s> retval = {"frame_fullname": "{}.png".format(filename),	process_video for i in range(self.count): idx = i + 1 "frame_name": filename, "frame_extension": ".png"}
# todo: move to config check </s> else:	system@124 if not os.path.isdir(model.csv_import_path): error_logger(csv_import_username.username, " SYSTEM_IMPORTER_FILE_CSV_CRON_PATH_NOT_EXISTING") if not os.access(model.csv_import_path, os.R_OK): error_logger(csv_import_username.username, " SYSTEM_IMPORTER_FILE_CSV_CRON_PATH_NO_READ_PERMISSION")
# todo: boto3 call can fail with botocore.exceptions.clienterror, </s> self.log.debug('dns upsert response: %s', pformat(response))	SpinnakerDns response = self.r53client.change_resource_record_sets( HostedZoneId=zone_id, return dns_elb
# todo: move this to either settings.py or the sql configuration. </s> if request.method == 'get':	search_form def search_form(request): Render the news search form. debug = "GET" search_form = SearchForm(request.GET)
#todo eval hook </s> readers.setdefault(f.__name__, f)	__reader def __reader(f):
data_source_name='ledger-v2',  # todo: this isn't really needed. </s> domain=case.domain,	change_meta_from_sql_case document_subtype=case.type,
# todo only do these thing if status is true </s> self.logger.debug('routing_key:{0}'.format(routing_key))	format_rabbit_message routing_key, my_obj = item self.logger.debug('rabbit_message:{0}'.format(my_obj)) if routing_key == 'poseidon.algos.decider': self.logger.debug('decider value:{0}'.format(my_obj))
# todo -- parallelize this </s> for _ in range(10):	SpeakerChangeDetection upper_alpha = 1. best_alpha = .5 * (lower_alpha + upper_alpha) current_alpha = .5 * (lower_alpha + upper_alpha) peak = Peak(alpha=current_alpha, min_duration=0.0,
# todo: remove this. </s> iter_agr_total_update = iter_agr_total.assign_add(1)	_create_counter trainable=False)
# todo: why the reverse order? </s> return from_polar_rad(math.radians(direction), magnitude)	from_polar
# todo: migrate new article ids to articlecontents </s> def alter_self_foreignkeys(self, orm):	Migration 'source_id': 'source_id', 'description': 'perex', super(Migration, self).alter_self_foreignkeys(orm) alter_foreignkey_to_int('articles_articlecontents', 'article')
# todo: this size isn't calculating correctly currently, but it should be included in the json </s> return false	Verify Returns: bool: True if valid. False otherwise. bc = GetBlockchain() if not bc.ContainsBlock(self.Index):
# todo(b/131315065): remove the comment above when the csv decoder no longer </s> "min_batch_size": desired_batch_size,	GetBeamBatchKwargs if desired_batch_size is None: return {} "max_batch_size": desired_batch_size,
# todo: retry instead? </s> logger.info('handling events resources with persist:%s', persist)	handle_events_resources @celery_app.task(name=RunnerCeleryTasks.EVENTS_HANDLE_RESOURCES) logger.info(payload)
# todo: add support of tuple (row_offset, col_offset) </s> reduction: if true performs averaging of 8 outputs, otherwise - summation.	flips_deaugment Deaugment input tensor (output of the model) assuming the input was flip-augmented image (See flips_augment). Args: Returns: Tensor of [B, C, H, W] shape.
# todo: model only from our models </s> dataset_test = get_dataset(args, transform_test, "val")	test_main@142 if os.path.isfile(args.val_file): metadata = torch.load(args.val_file) dataset_test.video_clips.compute_clips(args.num_frames, 1, frame_rate=15) test_sampler = UniformClipSampler(
# todo: axis should support 1 or 'columns' either at this moment </s> index_map = [(column, column) for column in keys]	set_index if append: index_map = self._internal.index_map + [(column, column) for column in keys] index_columns = set(column for column, _ in index_map) columns = [column for column, _ in index_map] + \
# todo optimize </s> else:	CalcFiniteHorizonOptimalInput@16 tm=np.linalg.matrix_power(A, ii)*B if tmp is None: tmp =np.c_[tm,tmp] for ii in np.arange(i,N-1):
eprint('❗ ' + x) # todo yellow? </s> config = 'config'	Modes class Modes:
# :todo: implement test. </s> self.skiptest('not implemented yet.')	ReplayBundleRequestFilterTestCase self.skipTest('Not implemented yet.') def test_fail_transaction_not_trytes(self): def test_fail_depth_null(self): ``depth`` is null.
data = json.load(open(input_fn))  # todo do we support multiple arguments here? </s> parser.add_argument('input_fn')	line_plot_cmdline parser.add_argument('--display', action='store_true', help='Display plot') parser.add_argument('--outfile', default='line_plot.png', type=str, help='Output file to store results (default: %(default)s)') kwargs = vars(parser.parse_args()) line_plot(**kwargs)
# todo: serialize the policy </s> return response(str(e), status=500)	make_alice_control SigningPower: None}, federated_only=True) new_policy = drone_alice.create_policy(bob, label, m, n, federated=federated_only)
# todo save the error to the plugin </s> logger.info(f'found {len(plugins)} active plugins')	_activate_plugins :param force_reload: force reload base apps, defaults to False :type force_reload: bool, optional self.activate_integration_globalsettings(plugins) self.activate_integration_app(plugins, force_reload=force_reload)
#todo: check if/where this is used; if not used externally - remove </s> self.gui.remove_surface(surface)	remove_surface self.surfaces.remove(surface) self._update_ui()
# todo - find a workaround for this </s> psutil.kill_process(proc.pid)	test_kill_process def test_kill_process(self):
# todo find out if this is good because of sparcity... </s> min_samples_split = uniformintegerhyperparameter(	get_hyperparameter_search_space "max_features", 0.5, 5, default=1) max_depth = UniformIntegerHyperparameter( name="min_samples_split", lower=2, upper=20, default=2, log=False) min_samples_leaf = UniformIntegerHyperparameter(
#todo: check the data! </s> count = 0	test_feed TODO: have these tests iterate over a number of test pipelines pipe_def = self._get_pipe_def("testpipe1.json") for i in p: count += 1
# todo: remove the append lock once append like ops are thread safe </s> del lock	test_locked_repr lock = self.locktype() lock.acquire()
# todo: skipped due to gh-4436 </s> eq_(len(ds.repo.whereis('one.txt')), 2)	_test_bare_git_version_2 ds.repo.enable_remote('bare-git') ds.repo.init_remote('ora-remote', options=init_opts) assert_status( 'ok',
# todo: non-numeric columns should be ignored automatically </s> df = pd.dataframe({'a': np.arange(n), 'b': np.arange(n)**2})	test_iloc4 n = 11
# todo error on missing levels </s> size_levels = categorical_order(data["size"])	_LinePlotter size_levels = [None] sizes = {} if slims is None: smin, smax = 1, 3
## todo: # fixme: remove me </s> paste_parent = self.r_serv_onion.hget('onion_metadata:{}'.format(self.domain), 'paste_parent')	get_last_crawled_pastes def get_last_crawled_pastes(self):
# todo(user): remove after 184 is out. </s> if self._ctx:	RecordsPool if self._ctx: operation.counters.Increment( operation.counters.Increment( COUNTER_IO_WRITE_MSEC,
# todo: fix this </s> return self._encoder.decode(int_values)	ints2str if not self._encoder: raise ValueError(
# todo: reactivate after fixing alternative selection </s> form = artifactexporterspreadsheetcsvconfigform()	test_artifact_exporter_spreadsheet_csv_config_artifactlist_artifact_sha256_form_label self.assertEqual(form.fields['artifactlist_artifact_sha256'].label, 'Export SHA256')
#todo generate the labels for the dict automatically from labels </s> value : tuple	get_obssumm_file time_range : A TimeRange or time range compatible string Returns Return a tuple (filename, headers) where filename is the local file name under which the object can be found, and headers is
# todo: must be implemented </s> def should_fetch_kindlegen():	should_fetch_kindlegen
# todo manage tangent? </s> kf.interpolation = 'constant'	set_interpolation if interpolation == "LINEAR": kf.interpolation = 'LINEAR' elif interpolation == "CUBICSPLINE": kf.interpolation = 'BEZIER'
# todo: find 1 or 2 utxo's with exact amount +/- self.network.dust_amount </s> _logger.warning("no account_id specified and more than one account found for this network %s. "	_get_account_defaults qr = qr.filter_by(account_id=account_id) acckey = qr.first() "Using a random account" % network) if not account_id and acckey:
if norm_groups[i] == -1:  # todo: early break </s> self._update_scale(skip)	FP16_OptimizerMMTLModified if norm_groups[i] == -1:  # TODO: early break skip = True return self.optimizer.step(
# todo generator </s> metavar="path",	Drop dataset=dataset_argument, path=Parameter( doc="path/name of the component to be dropped", nargs="*",
# todo: this is horrible </s> self.window.run_command('floobits_join_room', {	FloobitsPromptJoinRoomCommand def run(self, room=''): self.window.show_input_panel('Room URL:', room, self.on_input, None, None) 'room_url': room_url, })
# todo(b/155997704) clean this up once tfx_bsl makes a release. </s> return _csvtoexample	GetInputSourceToExamplePTransform
# todo: this doesn't seem necessary; test passes without it </s> watcher = watcher()	test_watch_ignore watcher.watch(tmpdir + '/*', ignore=lambda o: o.endswith('.ignore')) assert watcher.examine() == (None, None)
# todo what is the performance of this like? </s> section = cls.get_section(key, cache_id=cache_id)	get_directories @classmethod if section is None: return []
# todo add test for this </s> arr_aug = np.rot90(arr, k_i, axes=(1, 0))  # adding axes here rotates clock-wise instead of ccw	_augment_arrays_by_samples input_dtype = arrs.dtype if input_was_array else None arrs_aug = [] if keep_size and arr.shape != arr_aug.shape and resize_func is not None: arr_aug = resize_func(arr_aug, arr.shape[0:2])
# todo: support multiple. </s> returns podcast series id of edited podcast series	edit_podcast_series :param podcast_id: A podcast series id (hint: they always start with 'I'). :param subscribe: Subscribe to podcast. mutate_call = mobileclient.BatchMutatePodcastSeries update_mutations = mutate_call.build_podcast_updates([
# todo: don't assume that content is octetstring </s> def decorator(*args, **kwargs):	verify_cms_signers - TypeError if *Content-Type* header is not "application/pkcs7-signature" - SigningError if any signer on the CMS content is not valid. if request.headers['Content-Type'] != "application/pkcs7-signature": raise TypeError("verify_cms_signers expects application/pkcs7-signature, got: {}".format(
# :todo: implement test. </s> def test_pass_happy_path(self):	ReplayBundleRequestFilterTestCase class ReplayBundleRequestFilterTestCase(BaseFilterTestCase): filter_type = ReplayBundleCommand(MockAdapter()).get_request_filter Request is valid. self.skipTest('Not implemented yet.')
# todo: replace this with future on-the-fly-api-components. </s> del self.terminals_buffer[env_id]  # = []	reset_env_buffers del self.internals_buffer[env_id]  # = [] del self.rewards_buffer[env_id]  # = []
# todo: should be threaded </s> @staticmethod	TLSCipherSuiteChecker result, err = process.communicate() parsed = self.parse_nmap_outpt(result, err) def parse_nmap_outpt(result, err): if err:
#exclude watched items --> currently hardcoded --> todo: add a setting for this ? </s> cursor.execute(setartsql,(setid,"set",update_type,artwork[update_type]))	addBoxsetToKodiLibrary cursor.execute(setupdateartsql,(artwork[update_type],"set",setid,update_type)) elif artwork[update_type] != '': return True
# todo: calculate mu-sigma for f1, accuracy, and roc_auc and make it selectable </s> pr_curve_auc = auc(pr_recalls, pr_precisions)	get_metrics_binary else: pr_precisions, pr_recalls, pr_thresholds = precision_recall_curve(y_true, y_pred_prob, pos_label=1) roc_curve_auc = auc(roc_fprs, roc_tprs) results = dict(accuracy=accuracy,
# todo: should this be a glomerror of some form? probably </s> however flattening an integer itself will raise an exception:	flatten Because integers support addition, we actually have two levels of flattening possible: >>> flatten(target, init=int, levels=2) >>> target = 3 >>> flatten(target)
# truffle todo(ls): revert this loop to "yield from" </s> smod = self.exc_type.__module__	format_exception_only yield _format_final_exc_line(None, self._str) return if smod not in ("__main__", "builtins"): stype = smod + '.' + stype
assert study_id == 0  # todo(akiba) </s> assert study_id == 0	set_study_param_distribution def set_study_param_distribution(self, study_id, param_name, distribution):
# todo: log exception </s> output = sshexec(host, list2cmdline(cmdline), port=port, username=user, key_filename=conf["key"])	scan@71 else: host, port, user = conf["host"] except Exception as e: return None
raise notimplementederror # todo </s> return open(filename, 'rb')	file_opener def file_opener(filename): return _opener
# todo: import that elsewhere </s> if len(elementnames) != len(set(elementnames)):	wrapper@28 elementNames = list(itertools.chain(*(elem.keys() for elem in elements.values())))
# todo: check how those names are constructed and may be at least count the number of created object files in addition to that comparison </s> return ofpipeline(*args,  **kwargs)	_pipeline def _pipeline(*args, **kwargs):
# todo: test this block </s> if not super(seo_content_type_choices, self).__len__():	_populate_content_types def _populate_content_types(self):
# todo: implement logic for computing the host alias </s> self._load_result(result, 'skipped', **kwargs)	v2_runner_on_skipped
# todo: handle other resolve strategies </s> if not conflict:	ThreeMFWorkspaceReader if stacks: conflict = True instance_container_files = [name for name in cura_file_names if name.endswith(self._instance_container_suffix)] for instance_container_file in instance_container_files:
# todo: configurable timeout??? </s> override this method if you wish to handle the decoded data	_handle_decoded_payload differently. self.syndic_cmd(data)
# todo(piyush): current api-site doesn't contain this api description. </s> uri = '/subnetpools'	create_subnetpools post_data = {'subnetpool': kwargs} return self.create_resource(uri, post_data)
# todo(kevinbenton): remove after bug/1666493 is resolved </s> fixed_ip_list.append({'subnet_id': subnet['id'],	_test_fixed_ips_for_port {'address': fixed['ip_address'], 'id': subnet['id']}) 'ip_address': fixed['ip_address']}) else:
# todo(patrick, suraj, anton) - it's surprising that "non-padded/non-numpified" padding </s> if numpify:	prepare_inputs_for_common floats_list((x, self.feature_size)) for x in range(self.min_seq_length, self.max_seq_length, self.seq_length_diff) speech_inputs = [np.asarray(x) for x in speech_inputs] return speech_inputs
# todo: remove these deprecated properties we are off es 1 </s> es_settings = copy(es_settings)	_get_es_settings for setting in disallowed_settings_by_es_version[settings.ELASTICSEARCH_MAJOR_VERSION]: es_settings['index'].pop(setting)
n = 100 # todo: iir: more intelligent algorithm needed </s> the impulse respons	update_view place holder; should update only the limits without recalculating
#todo: get darknet class number from class file </s> best_iou = k.max(iou, axis=-1)	loop_body def loop_body(b, ignore_mask): true_box = tf.boolean_mask(y_true[l][b,...,0:4], object_mask_bool[b,...,0]) ignore_mask = ignore_mask.write(b, K.cast(best_iou<ignore_thresh, K.dtype(true_box))) return b+1, ignore_mask
# todo(ihrachys): replace with network.create() once we get an object </s> _test_class = policy.qospolicy	QosPolicyBaseTestCase
# todo: aio core is separate from transport </s> if 'proxy' in self.minion.opts:	ProxyMinion self.shutdown() def shutdown(self): self.minion.opts['proxyobject'].shutdown(self.minion.opts) logger.info('The proxy minion is shut down')
# :todo: implement test. </s> self.skiptest('not implemented yet.')	FindTransactionsRequestFilterTestCase def test_fail_approvees_wrong_type(self): self.skipTest('Not implemented yet.')
st = manager.get_stat(path)  # todo: errors </s> id: module id	get_cache_names def get_cache_names(id: str, path: str, cache_dir: str, pyversion: Tuple[int, int]) -> Tuple[str, str]: path: module path (used to recognize packages) cache_dir: cache directory
# todo: maybe this should be under a lock? </s> def instance(cls, *args, **kwargs):	instance
logfile = open('logs/exceptions.log', 'a') #todo: make not hardcoded </s> except exception, e:	create_logdir def create_logdir(): print >> sys.stderr, 'There was a problem creating the logs directory.' print >> sys.stderr, e.__class__, str(e)
# todo the actual brodcast </s> return web.json_response(self._client_identifier)	ValidatorAPIHandler@104 self._client_identifier = construct_trinity_client_identifier() @get(APIEndpoint.node_version) @get(APIEndpoint.genesis_time) async def _get_genesis_time(self, request: web.Request) -> web.Response:
# todo: remove when transition to python3 complete </s> return frd(-self.fresp, self.omega)	__neg__
# todo(laigd): remove this check when 312743821 is in the release. </s> x = tf.random.normal(shape=(3, 3), seed=123456)	test2DStaticShape def test2DStaticShape(self): y = tf.zeros(shape=(4, 6)) padded_x = py_utils.PadOrTrimTo(x, y.shape, pad_val=0)
# todo: deprecation warning </s> def delete(self, request, *args, **kwargs):	DestroyAPIView Concrete view for deleting a model instance.
# todo: systemhistory_user_id </s> request_user +	Taskname return self.taskname_name def logger(taskname, request_user, log_text): log_text + " taskname_id:" + str(taskname.taskname_id) +
# todo: figure out a way to set __name__ for partial object, update_wrapper </s> processor function for the join stage.	process_join def process_join(interface, state, label, inp, task, full_join, label_fn, Note that each key in the 'inp' is orgnized as: key = (where_index, join_column, other_columns)
# todo: test 2a: planilha total mais atualizada que deployed (total + deployed) </s> label, day, month = key.split("_")	row_with_sorted_columns for key in row.keys(): if not key.startswith("confirmados"): row_dates.add(f"2020-{int(month):02d}-{int(day):02d}") new = {"municipio": row["municipio"]}
# todo: end remove hosts/when block </s> def get_kwarg(key, default=none):	get_kwarg
self.ops_config = none  # todo </s> while 1:	_thread_spin_monitor time.sleep(0.05) if not self.ctx.tracing or self is not self.MAIN_INSTANCE:
# todo: action value doesn't exist for beta </s> self.unittest(reward_estimation=reward_estimation)	test_no_horizon_estimate reward_estimation = dict(horizon=0, discount=0.99, estimate_horizon=False) self.unittest(reward_estimation=reward_estimation)
# todo: will be replaced with exception in future releases </s> return self.to_json(), {self._media_name: self.media}	convert_input_media def convert_input_media(self): if util.is_string(self.media):
# todo: differentiate between tags assigned to the instance and a m2m field for tags (ex: configcontext) </s> user = user.objects.create_user(username=username)	create_test_user def create_test_user(username='testuser', permissions=list()): for perm_name in permissions: app, codename = perm_name.split('.')
false)  # todo: check if this should be secure </s> sale_price = item.get_price_per_item(discounts=self.discounts)	item_sale_price def item_sale_price(self, item):
# @todo implement threading here. </s> issue['created_on'], "%y-%m-%d %h:%m:%s"))),	annotations return dict([ self.format_annotation( issue['permalink'],
# todo: allow partial prase complete </s> break	process_events def process_events(context): while True: api.process_events() time.sleep(1.0/30)
# todo: old requirement, remove in future versions? </s> res = self.app.get(url, follow_redirects=true)	test_01_front_page self.signout res = self.signin(email=self.root_addr, password=self.root_password) dom = BeautifulSoup(res.data) self.signout()
## todo : add layers </s> pass	testGAN
# todo see #682 detailedlist should have a _selection attribute + selection property like tree </s> pass	after_on_refresh
# todo only do these thing if status is true </s> self.logger.debug('routing_key:{0}'.format(routing_key))	format_rabbit_message routing_key, my_obj = item self.logger.debug('rabbit_message:{0}'.format(my_obj)) if routing_key == 'poseidon.algos.decider': self.logger.debug('decider value:{0}'.format(my_obj))
# todo verify results </s> responsecode=501,	DisabledV3RootResource {"data": None, "errors": [{"message": "graphql not enabled"}]} )
# todo: parse automatically the 'swap' method </s> qs = getattr(parts, 'query', none)	video_id @property def video_id(self): if qs: video_id = parse_qs(qs).get('v', None)
# todo make proper json-ld definition </s> and concatenation of sequence-type fields into csv strings	_meta2index_dict return { ds_defs.get(k, k):
# todo: make sure this openssl command works everywhere, maybe we should use a text_base64_decode? </s> def package_upgrade_yum():	package_upgrade_yum
# todo this pipeline may drop nulls in prediction rows if impute=false </s> self._advanced_trainer.pipeline = pipeline	__init__@16 grain_column, verbose)
# todo: split name and email address </s> content = content.encode()	attachments for attachment in self.__email.iter_attachments(): content = attachment.get_content() to_return.append((attachment.get_filename(), BytesIO(content))) return to_return
self.assertequal(self.todolist.count(), 3)  # force won't delete subtasks </s> command = deletecommand(["-e", "@test"], self.todolist, self.out, self.error, none)	test_expr_del1 command.execute() result = "Removed: a @test with due:2015-06-03\nRemoved: a @test with +project\n"
# todo: this test requires manifold access, see: t88318502 </s> self._test_model("coco-detection/retinanet_r_50_fpn_3x.yaml")	TestCaffe2Export def testMaskRCNNGPU(self): self._test_model("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml", device="cuda") def testPanopticFPN(self): self._test_model("COCO-PanopticSegmentation/panoptic_fpn_R_50_3x.yaml")
# todo: remove when postgres migration is done </s> returns a client for either the new elasticsearch index (if the 'postgres'	_get_client_or_legacy_client def _get_client_or_legacy_client(request): feature flag is on) or the legacy index (if the flag is off). func = _get_client if request.feature('postgres') else _legacy_get_client
# todo: @sbharadwajj implement and test </s> def ea_jac_t_mat_jac_prod(self, module, g_inp, g_out, mat):	Conv1DDerivatives return True def get_unfolded_input(self, module): raise NotImplementedError def _jac_mat_prod(self, module, g_inp, g_out, mat):
# todo: find out why pypy 1.8 with close_fds=true triggers </s> try:	_unregister def _unregister(self): Unregister from the scheduler and close open files. self.vartree.dbapi.aux_get(self.settings.mycpv, ["EAPI"]) except KeyError:
# todo: check error location </s> return {	get_fields 'test': GraphQLField(test_type), 'nest': GraphQLField(DataType(), resolver=lambda *_: Data())
# todo keep playback.state unchanged </s> def seeked(self, position):	Seeked logger.debug(u'%s.Seeked signaled', PLAYER_IFACE) pass
# todo: we lose the response code, so we can't check this </s> % localfile)	test_GET_FILEURL_localfile_nonlocal localfile = os.path.abspath("web/GET_FILEURL_localfile_nonlocal") fileutil.make_dirs("web") d.addBoth(self.shouldFail, error.Error, "localfile non-local", "403 Forbidden",
# todo(dcramer): once we migrate to job plans we can remove this </s> 'got create_build task without job plan: %s', build_id)	create_build@28 try: if not job_plan: backend = JenkinsBuilder( app=current_app,
#@todo: remove in 0.4.10 </s> for p in xrange(2, pages + 1):	handle_pages pages = int(re.search(self.PAGES_PATTERN, self.html).group(1)) except Exception: self.html = self.load_page(p) self.links += self.get_links()
